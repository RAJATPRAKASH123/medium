story_url,bodyText
https://medium.com/@renebuest/the-pitfalls-of-cloud-connectivity-107bdede50ca?source=search_post---------0,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 1, 2016·3 min read
The continuous shift of business-critical data, applications and processes to external cloud infrastructures is not only changing the IT operating concepts (public, private versus hybrid) for CIOs but also the network architectures and integration strategies deployed. As a result, the selection of the location to host your IT infrastructure has become a strategic decision and a potential source for competitive advantage. Frankfurt is playing a crucial role as one of the leading European cloud connectivity hubs.
The digital transformation is playing an important part in our lives today. For example, an estimated 95 percent of all smartphone apps are connected to services hosted on servers in data centers located around the world. Without a direct and uninterrupted connection to these services, metadata or other information, apps are not functioning properly. In addition, most of the production data, needed by the apps, is stored on systems in a data center, with only a small percentage cached locally on the smartphone.
Many business applications are being delivered from cloud infrastructures today. From the perspective of a CIO a reliable, high-performance connection to systems and services is therefore essential. This trend will only continue to strengthen. Crisp Research estimates that in the next five years around a quarter of all business applications will be consumed as cloud services. At the same time, hybrid IT infrastructure solutions, using a mix of local IT infrastructure connected and IT infrastructure located in cloud data centers, are also becoming increasingly popular.
The ever-increasing data volumes further increases the requirement of reliable, high-performance connectivity to access and store data and information any place, any time. Especially in case business-critical processes and applications are located on cloud infrastructure. For many companies today, failure to offer their customers reliable, low-latency access to applications and services can lead to significant financial and reputation damage, and represents a significant business risk. Considering that the quality of a cloud services depend predominantly on the connectivity to and performance of the back end, cloud connectivity is becoming the new currency.
“Cloud connectivity” could be defined technically as latency, throughput and availability:
In simple terms, cloud connectivity could be defined as the enabler of real-time access to cloud services any place, any time. As a result, connectivity has become the most important feature of today’s data centres. Connectivity means the presence of many different network providers (carrier-neutrality) as well as a redundant infrastructure of routers, switches, cabling, and network topology. CIOs are therefore increasingly looking at carrier-neutrality as a pre-requisite, facilitating a choice between many different connectivity options.
In the past 20 years a cluster of infrastructure providers for the digital economy has formed in Frankfurt, which facilitates companies to effectively and efficiently distribute their digital products and services to their customers. These providers have crafted Frankfurt as a the German capital of the digital economy delivering a wide range of integration services for IT and networks, and IT infrastructure and data centre services. More and more service providers have understood that despite the global nature of a cloud infrastructure, a local presence is crucial. This is an important finding, as no service provider that is seriously looking to do business in Germany, can do so without a local data center. Crisp Research predicts that all major, international service providers will build or expand their cloud platforms in Frankfurt within the next two to three years.
Against this backdrop, Crisp Research has researched the unique features of Frankfurt as an international hotspot for data centres and cloud connectivity. The whitepaper, titled “The Importance of Frankfurt as a Cloud Connectivity Hub” is available for download now: http://www.interxion.com/de/branchen/cloud/die-bedeutung-des-standorts-frankfurt-fur-die-cloud-connectivity/download/
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@renebuest/the-significance-of-frankfurt-as-a-location-for-cloud-connectivity-cd90090ea65e?source=search_post---------1,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rene Buest
Aug 1, 2016·1 min read
Due to continuous relocation of business-critical data, applications and processes to external cloud infrastructures, the IT-operating concepts (public, private, hybrid), as well as network architectures and connectivity strategies are significantly changing for CIO’s. On the one hand, modern technology is required to provide applications in a performance-oriented, stable and secure manner, on the other hand, the location is significantly decisive for optimal “Cloud-Connectivity“.
Against this background, Crisp Research assesses the role of Frankfurt as data center location and connectivity hub with this strategy paper.
The strategy paper can be downloaded free of charge under “The significance of Frankfurt as a location for Cloud Connectivity“.
Tags: Cloud, Cloud Computing, Connectivity, Data Center
Originally published at analystpov.com.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
Gartner Analyst covering Infrastructure Services & Digital Operations. These are my own opinions.
"
https://medium.com/@insync/sync-your-external-or-network-drive-to-google-drive-with-plug-n-sync-2d6c374e8ac2?source=search_post---------2,"Sign in
There are currently no responses for this story.
Be the first to respond.
Insync
Jan 29, 2015·3 min read
Network and external drives are the lifelines for anyone dealing with serious data — be it for backup, expanding computer storage or sharing data around. These have become indispensable storage devices of our modern computing environment. As a syncing service, we’ve always striven to achieve seamless syncing across these devices without forcing the user to re-organize her data. This is one of the reasons that our “Add to Insync” feature is so well-received.
With “Add to Insync”, syncing any file or folder outside your main Google Drive folder is as easy as right clicking it and selecting “Add to Insync”. What happens in the background is that Insync adds a link to such a file or folder in your main Drive folder and keeps track of these additions.
However, “Add to Insync” worked horribly for network and external drives. For example, when the added file or folder went missing, such as when an external drive got disconnected, Insync would pause syncing for the entire account, not just for the disconnected external drive.
So we addressed it!
Now, if any file or folder added to Insync goes missing, Insync notifies you, pauses syncing for that file or folder but continues to sync the rest.
We call this feature “Plug-n-sync” and it’s awesome!
Consider the following scenarios:
Syncing to that folder resumes automatically as soon as you plug back that external drive.
Plug-n-sync now allows you to integrate your external or network drive with Google Drive easily. With this, you get to keep your current folder hierarchy as is — no need to move everything under one folder just so you can sync with the cloud.
Plug-n-sync is now available in Insync 1.1.4.
Happy plug-n-syncing with Insync :-)
Do more with cloud storage. Do more with Google Drive.
15 
2
15 
15 
2
Do more with cloud storage. Do more with Google Drive.
"
https://medium.com/javarevisited/external-configuration-in-microservices-spring-cloud-config-9c925f64749f?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
Let’s take a look at the overall microservice architecture that we have built as part of this spring cloud series.
If you have not followed this Spring Cloud series from the beginning, check this index page and bookmark it if needed — Spring Cloud Tutorials.
When we build a service, there are many values that we don’t want to hardcode and rather read from property files, for instance, endpoints, encrypted values, etc.
If we rely on the property files that are part of the application jar then for each configuration change we will have to rebuild the jar which is a tedious job. So instead, we can add all the properties in an external system like git, svn, etc, and just access them from our services and also enable the services to refresh dynamically when a property is added or edited in the external system.
To implement and test an external config server configuration, just 2 services are needed— Config Server and Config Client but in this tutorial, we will continue using the existing services that were mentioned in the previous articles. (Clone the projects mentioned in the conclusion section of Open Feign Tutorial).
Config Server
Now let’s generate a new Spring Boot project from the spring initializr site and add config-server and actuator dependencies in the pom file.
In the application.yml file, add the following properties
8888 is the port where our config server will run and the value provided in the “spring.cloud.config.server.git.uri” is the path to the git repository in your local system. This path could also be the Github web repository URL but for convenience, we have created a local repository under the config folder and added a yml file inside that, we will get into that later.
Config Client
Next, let’s use the existing subject service as a config client. Add the maven dependency in the pom file.
In the application.yml file, add the config server endpoint
management.endpoints…. property is added to access the refresh endpoint of the actuator over HTTP call.
In the resource class of subject service, we have added a property — @Value(“${message:Hello default}”), that will be accessed from the external file.
Now, both our server and client are ready. We need to add an external config file. As mentioned above, we will create a folder named config in the HOME directory (or anywhere you prefer, just make sure to update the URL in config server’s yml file). Inside the config folder, initialize git using the “git init” command. Create a file named subject.yml and add the following content to it
Run the registry, config-server, student, and subject services and hit the GET API in the student service
http://localhost:8093/api/student/info
Dynamically Refreshing the code to read the latest from config file
Edit the subject.yml file and call the refresh API provided by the actuator. As our service, subject, is running on port 8094, we will call the actuator refresh API on that port using the POST Curl
The response would be 200 OK.
Now, hit the student API again at http://localhost:8093/api/student/info and you will get the following output.
Congratulations!! We just finished the external config tutorial as well as the Spring Cloud Tutorial with all the key components included in a microservice architecture.
Complete code for the config server is available over Github. If you have already cloned subject service for previous tutorials, please do take an update.
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
10 
10 claps
10 
A humble place to learn Java and Programming better.
Written by
Author | Programmer | Area of Interest — Software Design & Architecture, Network Protocols & Communication, Space Technology | https://rubykv.com/
A humble place to learn Java and Programming better.
"
https://medium.com/@alibaba-cloud/mysql-external-based-replication-in-alibaba-cloud-b48b5b0ac9b8?source=search_post---------4,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jun 1, 2020·6 min read
By Ankit Kapoor, Database Senior Solution Architect
In this MySQL based article we are going to discuss about:
In order for you to fully understand the concepts presented in this blog, you should already be well aware about:
External based replication is one of the architecture of replication’s topology between an [ApsaraDB RDS] and on-premises Database server. In this architecture we can have RDS as master and on-prem DB as slave. We can also construct RDS as slave with on-prem DB as its master. In this article we will consider this architecture for MySQL and how can we deploy it in our production and staging environment. In this article, I am going to have Master on RDS MySQL 5.7 and slave on MySQL 8.0 version & reason behind this is to know what are the errors we can face while setting up this architecture and how can we resolve it. Having plain setup is simple and will not produce any errors.
Please note that currently we don’t support external based replication of architecture in which RDS is slave with on-prem DB server as Master.
Main purpose of having this architecture may vary for different business requirement. One of the business requirements I have seen is where we want one of our database servers to act as warehouse or OLAP and we want it to self-managed. In some cases COST can also be counted as a factor to adopt such architecture.
Main purpose of having RDS as a slave is because we want to keep our data safe and thus we opt for managed services. In times of crash recovery or backup recovery or to generate BI -report we can use this RDS and doesn’t worry about the monitoring or managing it.
I have replicated below scenario where I wanted to have below architecture:
1. Login to your RDS console https://rdsnext.console.aliyun.com/
2. Go to Products and locate Relational Database Services.
3. Click on create instance.
4. Make ensure that you setup below configuration:
5. Click next.
6. You will land to Instance configuration page.
7. Make ensure that you have your VPC and vswitch must be setup already.
8. Click on next and you will land to confirm order page.
9. Tick Terms and Services and confirm your order.
10. Wait for few minutes until your RDS instance gets ready.
11. Once ready click on the instance and you will land to below page:
12. Click on Configure Whitelist.
13. Click on Create Whitelist and add the IP of your on-prem server.
14. For testing purpose, I have made this RDS as public and put 0.0.0.0 in whitelisting.
15. Make ensure that public endpoint of this RDS must exist so that you can connect from outside VPC. To apply for the public end point, you can follow steps mentioned in below link: https://www.alibabacloud.com/help/doc-detail/26128.htm
16. Create Privileged account for RDS MySQL . For this please follow below link: https://www.alibabacloud.com/help/doc-detail/87038.htm
We will need this in creating replication user.
17. Connect to the RDS via DMS or MySQL client on ECS or on your local machine. For DMS, you can login via console only.
18. Once RDS is setup, please install MySQL on your local machine. You can follow Oracle guide on this. It is solely depend on you how to install MySQL . Either via RPM, binary files or DMZ package.
19. Once Database has been started, please enable gtid mode and put below configuration in your cnf file. I am pasting a sample config file for this:
20. Login to your RDS DB server.
21. Take the backup of RDS database using mysqldump. This has
22. been done to make ensure that all transactions are in GTID mode.
23. Restore it on your on-prem DB server.
24. Once done, please configure the master at on-prem DB server :
25. This error has been introduced because of the different versions between master and slave. Let’s see how can we resolve it
As per error, table structure seems to be different. We can fix this error by having similar structure at both end and then run start slave . I have pasted my output . Make ensure that you should take data from source too.
26. Hence this will start the external replication.
We don’t support Classic file based replication currently. But if you have standalone RDS MySQL instance then you can set classic file based replication with RDS as master or slave.
RDS as slave is not supported currently via GTID based but you can achieve this by using DTS tool.For more information you can visit below documentation:
https://www.alibabacloud.com/help/product/26590.htm?spm=a3c0i.126076.1204981.3.6295155aFo7R9z
Alibaba Cloud ApsaraDB RDS for MySQL is a stable, reliable, and scalable online database service. Based on Alibaba Cloud distributed file system and high-performance SSD storage, ApsaraDB RDS for MySQL features disaster tolerance, backup, recovery, monitoring, and migration capabilities to facilitate database operations and maintenance. To learn more, visit the official product page
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@Nico_g/how-apis-are-changing-software-as-we-know-it-fc9dfdf8f16?source=search_post---------8,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nicolas Garnier
Aug 30, 2015·4 min read
This article was originally published on the Mailjet BlogBy Nicolas Garnier
Remember when “the cloud” simply referred to random external servers or even the internet as a whole? Over the past few years, the evolution of “the cloud” has led to the birth of new business models such as Software-as-a-Service (SaaS).
SaaS is great for the end-user, making services which would otherwise be unaffordable, accessible to small companies and private individuals. But as a developer, I find SaaS especially interesting in the way it’s helped reinvent something that has been around for decades: APIs. Marc Andreessen, entrepreneur and co-founder of Netscape, said in his “Software is eating the World” speech from 2011, “With lower start-up costs and a vastly expanded market for online services, the result is a global economy that for the first time will be fully digitally wired”. The “digitally wired” expression says it all, I love it. What’s the best way to wire the digital economy? It’s obviously APIs, which by definition express how two pieces of softwares are connected and interact.
“APIs are the building blocks of the digital economy”, Laura Merling, VP Ecosystems and Solutions, AT&T. (source)
The cool thing about being a developer — what drove me to learn coding on my own — is the ability to build & hack whatever you can. Today is really the best time to do so, because you’re not on your own anymore. You’ll find the support of online and offline communities everywhere you look, from program-and-answer forums like Stackoverflow (Worldwide Alexa ranking of 56) to offline events and hackathons, whether it’s your local hackathon or Techcrunch Disrupt. In addition to the support of these communities, we also now have plenty of tools and resources at hand. For developers, if you try to think of a tool — something you can use to easily achieve a goal — APIs quickly come to mind. The hottest APIs are always from services like Yo, Twilio or Venmo. Now why is that?
APIs are awesome in the way they empower you to set up rich features you would have struggled developing on your own, either because of the complexity, the time needed, and often both. APIs enable you to delegate or outsource what’s not critical to your business while keeping control. Whether you want to integrate a transactional emailing feature to communicate with your users or a rich search experience, going the easy way with a SaaS like Mailjet for emails or Algolia for search will always be a smarter move than trying to reinvent the wheel. It’s very likely that there’s already a SaaS product out there that does exactly the feature you have in mind, with just a simple integration. That’s why some SaaS are becoming more like APIs-as-a-Service. They cut your time to market while offering you a rich, reliable, affordable and scalable service with a few lines of code.
APIs are the new libs. They definitely change the way we build software, shortening but at the same time enriching the code we write. We can often even reuse the wrappers provided by the service we want to integrate and tailor them to fit our needs.
And why not think outside the box to innovate here? What if we take things one step further and imagine APIs as drag-and-drop building blocks that don’t require writing code? This might seem unthinkable, but it’s actually already possible, with API connectors such as Zapier and IFTTT. Need to generate Todoists tasks from your Google Calendar? Easy! Need to automatically save your Gmail attachments to Drive? Just as easy!
Your reaction might be, “Yeah, yeah, yeah. Cool. But this is limited to triggered actions so we’re missing a lot of what an API has to offer. And we’ve got plenty of time before services like Twilio become accessible to non-developers.” But actually not. With the help of a service such as Blockspring, any non-developer can use Twilio, extract data from the US government or build a dataviz. I agree with Blockspring’s point that APIs are for the end-user too, and the fact that they’ve just raised $3.4 million from Andreessen Horowitz and SV Angel (source) tend to prove this right.
Don’t get me wrong, though. I’m not saying that developers will become obsolete, or that everyone will be able to easily pick up a developer job without training. At least not today. I’m saying that while the complexity of software is increasing, it’s more accessible thanks to APIs. And it’s a great thing. It also allows us, as developers, to spend our time focusing on what matters most to what we’re building and benefiting from APIs. From a business perspective, this means one thing: of course any SaaS company should seriously consider providing an API, but if it does, it should do it right with a great Developer Experience. Developers indeed expect a few things from an API: documentation, support, community, standards-compliance architecture style (REST). Those are not just important because they’ll make developers who use your API happy, they’re actually necessary for adoption and productivity.
Making email cool @mailjet @mjmlio #emailgeeks
21 
21 
21 
Making email cool @mailjet @mjmlio #emailgeeks
"
https://medium.com/oracledevs/query-logic-implementation-in-vbcs-for-adf-bc-rest-a8e3c3a03b00?source=search_post---------9,"There are currently no responses for this story.
Be the first to respond.
Oracle Visual Builder Cloud Service allows to define external REST service connections. In this post I will explain how to implement query logic against such service. Connection is defined for ADF BC REST service.
Wizard provides option to add query parameters, both static and dynamic. I have set one static parameter onlyData=true, to return data only from the service. Also I have created multiple dynamic parameters, the one used in this use case — q parameter. This parameter accepts query expression to filter data. Later in VBCS action chain, I will assign value to this parameter and service will be re-executed to bring filtered data:
Search form elements will be assigned with page scope variables, to hold user query input. On search button click, VBCS action chain will be invoked to read these values and update query parameter. Page scope variables:
Variables firstNameQueryVar and lastNameQueryVar are assigned to search form fields, here is example:
Search button invokes action chain:
Action chain does two things — calls JS function to construct query parameter and then assigns returned value to rest service query parameter to execute search:
JS function is mapped to accept input parameters from search form input fields:
JS function code — parameters are joined into ADF BC REST query string:
JS function result is mapped with page scope variable — result is assigned to this variable:
REST service query parameter q variable is assigned with this value. Once value changes, query is automatically re-executed:
In my next post I will explain how to implement filtering and pagination with transformation function, on top of service connection:
VBCS sample application code is available on GitHub (if you download ZIP from GitHub, make sure to extract it and create new archive including extracted content directly, without top folder).
Originally published at andrejusb.blogspot.com on September 19, 2018.
A community for developers by developers.
2 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2 claps
2 
Written by
TensorFlow Certified Developer | https://github.com/katanaml
Aggregation of articles from Oracle engineers, Groundbreaker Ambassadors, Oracle ACEs, and Java Champions on all things Oracle technology. The views expressed are those of  the authors and not necessarily of Oracle.
Written by
TensorFlow Certified Developer | https://github.com/katanaml
Aggregation of articles from Oracle engineers, Groundbreaker Ambassadors, Oracle ACEs, and Java Champions on all things Oracle technology. The views expressed are those of  the authors and not necessarily of Oracle.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/private-access-to-gcp-apis-through-vpn-tunnels-ab0217918e38?source=search_post---------10,"There are currently no responses for this story.
Be the first to respond.
—
10/3/19: Please instead see the official solution here:
cloud.google.com
—
This tutorial demonstrates how to use APIs for Google Cloud Platform (GCP) services from an external network, such as your on-premises private network or another cloud provider’s network. This approach allows your on-premises servers that are connected to your private network to access GCP services without using public IP addresses.
GCP customers often have workloads spanning cloud proividers and their on-premises datacenters connected via VPN. In many of those situations, customers need to access various Google Cloud APIs where arbitrary outbound traffic from any system is restricted or acutely controlled.
This not a problem while the workload is running on GCP: accessing GCP APIs from within a VPC is directed towards Google-internal interfaces where those services resides. When accessing these same APIs from other restricted cloud providers or even your own datacenter the situation is a bit different: customers need to either enumerate and selectively allow wide Google API IP ranges or apply the same treament to the traffic as if the workload is within GCP: send the traffic securely through the VPN Tunnel.
This article is a baseline walkthrough of how to setup an strongswan ipsec tunnel to Google and then access GCP APIs and your VMs residing on Google.
VPC Service Controls are also enabled to demonstrate how to lock down specific API access to just authorized projects. That is, once the private API is enabled, you can optionally lock down access to APIs like Google Cloud Storage such that it is only accessible via the tunnel. This is an optional security measure you can employ to further restrict access.
The following diagram summarizes the overall architecture that you create in this tutorial.
Connect the two networks through a VPN tunnel and set the routing to resolve and emit GCP APIs through that tunnel.
Notes:
This tutorial uses the following billable components of Google Cloud Platform:
You can find the full git repo of this article here:
github.com
In the tutorial, we will setup the following remote an local networks.
Local:
GCP:
The steps outlined below sets up the following (in order):
This section details setting up the project on GCP.
It is advised to run all these commands in the same shell to avoid resetting environment variables.
First export some variables (feel free to specify your own projectIDs, ofcourse):
Create project that will host the VPN gateway and VPC API access (substitute the projectID with your own; this article uses GCP_PROJECT_NAME in the examples below)
Export the envionment variables for the project and network region to setup
Create the custom network within this project that will host the VPC and private API access:
This VM is used for testing connectivity from the remote VM to GCP. This VM will not have a public IP allocated and is accessible via VPC>
Verify no public address is allocated:
This example sets up another GCP project to ‘simulate’ a remote network. This project does NOT use Cloud VPN and instead terminates the VPN traffic directly on a VM. In reality, you can setup any cloud provider VPN or appliance while this article shows raw, low-level ipsec configuration
As before, the following command uses a project called ONPREM_PROJECT. In your case, you should setup this project as (for example): ONPREM_PROJECT-[randomcharacters]
Export some environment variables and substitue the variables with your project names:
Configure a new custom network with a specific CIDR range. You can configure any range but for consistency with the ipsec configuration below, the CIDR ranges below are used.
Configure firewall rules on the ‘simulated’ network to allow ipsec traffic inbound and internal traffic to the privateIP ranges for GCP.
This VM will host the ipsec tunnel and provide DNS resolution for the onprem network.
Since our onprem network is yet another GCP project, setup a route via gcloud
Normally, you would use BGP or directly set routetables but on GCP, these routes need to get setup on control plane.
Setup another VM instance on the onprem network that will send traffic and DNS resolution through the VPN Gateway (instance-1)
In the end, you should have two VMs setup on your onprem project:
NOTE public_IP of ONPREM VPN Gateway) is: >>>> 35.192.118.145 <<<<
Create VPN using Cloud Console.
Use the following specifications for the VPN. The remote peer IP is the IP address of the VPN host VM we setup previously 35.192.118.145
Use a new shared secret (eg: $SHARED_SECRET = python -c ""import uuid; print str(uuid.uuid4())"")
NOTE public_ip of GCP VPN GATEWAY that gets allocated. In this article, its: 35.184.203.133
NOTE THE VPN GATEWAY PUBLIC IP >>>> 35.184.203.133 <<<<*
export the ip as an environment variable for later use
On the GCP network, allow traffic inbound from your onprem network range 192.168.0.0/20
Configure the “ONPREM” VPN gateway
Remember to replace leftid= with the IP address of the VPN Gateway VM on $ONPREM_VPN_IP
Remember to replace right= with the IP address of the VPN Gateway VM on $GCP_VPN_IP
then
Configure strongswan secrets:
In the current example, the values would be:
Configure ipsec.conf:
Remember to replace leftid= with the IP address of the VPN Gateway VM on $ONPREM_VPN_IP
Remember to replace right= with the IP address of the VPN Gateway VM on $GCP_VPN_IP
Verify tunnels are created
2f. [Remote] Add next-hop route thorugh VPN Gateway
Verify the tunnels are up:
Open up a new window in instance-1, and run ip xfrm monitor.
You should see ipsec traffic through the gateway vm instance-1
Private access for Google APIs from remote systems requries a special CNAME map:
www.googlapis.com: CNAME=restricted.googleapis.com --> 199.36.153.4/30
On remote gateway VM (instance-1), open up two new shells.
In one window, run ip xfrm monitor, In another window, access the GCE VM and a private API:
Also verify the traffic through to 199.36.153.7 is via the tunnel
The session with ip xfrm monitor shows activity which indicates data sent via tunnel.
The following sequece will configure and test DNS resolution and direct connectivity to:
On remote gateway VM (instance-1):
Note ONPREM_PROJECT, the settings in your file will be different.
NOTE: GCP VMs overrides certain host entries likes /etc/resolv.conf so you may need to reset this if you leave the 'onprem' VMs running.
This should resolve to the IPs provided locally with CNAME and resolve to the 199.36.153. range
Note the IP address you connected to: 199.36.153.5
At this point, we have verified GCS API calls through the tunnel
We have verified the remote gateway sends traffic to GCP via the tunnel. We will now configure another host ‘on prem’ which will also send its traffic through to GCP via this gateway:
Normally, routes are added directly if using BGP or static route:
However, since this tutorial simulates ‘onprem’ on GCP, add routes via gcloud instead:
Verify API traffic is through the tunnel by running ip xfrm monitor on the VPN Gateway host (eg, instance-1 on project=ONPREM_PROJECT_NAME)
At this point, the VPN connectivity to GCP APIs and VM is established via the tunnel. Optionally enable API access to GCP services such that it must go through a trusted project (in our case, via the tunnel).
See Service Perimeters as well as Cloud IAM Roles for Administering VPC Service Controls.
First enable a security perimeter such that GCS access is only available via project=gcp-project:
Note: once you enable this, all acess to GCS for the given bucket is blocked except via this specific project.
on a host ‘on prem’ force traffic for www.googleapis.com to not go through the tunnel. Do this buy making the DNS resolve to the external address. On instance-2, comment the name resolution entry:
Then attempt to access GCS:
(note the address is outside of the tunnel route: 74.125.70.95)
Now add in name resolution such that the traffic transits the gateway and tunnel:
(note the address resolved is 199.36.153.4)
When you have completed this tutorial, delete your project to avoid incurring further costs.
Google Cloud community articles and blogs
58 
1
58 claps
58 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/uptime-99/kubernetes-hpa-autoscaling-with-custom-and-external-metrics-da7f41ff7846?source=search_post---------11,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Jessica G
Jul 23, 2018·7 min read
Autoscaling deployments in Kubernetes is more exciting since HorizontalPodAutoscaler can scale on custom and external metrics instead of simply CPU and memory like before.
In this post I will go over how HPAs work, whats up with the custom and external metric API, and then go through an example where I configure autoscaling an application based on external Nginx metrics.
How the Horizontal Pod Autoscaler Works
HPAs are implemented as a control loop. This loop makes a request to the metrics api to get stats on current pod metrics every 30 seconds. Then it calculates if the current pod metrics exceed any of it’s target values. If so, it increases the number of deployment objects. I think this doc on the autoscaler’s autoscaling algorithm is a great read.
Essentially the HPA controller get metrics from three different APIs: metrics.k8s.io,custom.metrics.k8s.io, and external.metrics.k8s.io. Kubernetes is awesome because you can extend its API and that is how these metric APIs are designed. The resource metrics, the metrics.k8s.io API, is implemented by the metrics-server. For custom and external metrics, the API is implemented by a 3rd party vendor or you can write your own. Currently I know of the prometheus adapter and custom stackdriver adapter that both implement custom and external metrics API. Check out these k8s docs on the topic for details.
Here is an example I created that scales a Kubernetes deployment running on a GKE cluster with metrics from Stackdriver. However, I do not use the metrics that Stackdriver has by default, but rather I ship external metrics from Nginx metrics into Stackdriver, then use those metrics to scale my app. Below I describe the steps I took to accomplish this. This repo has example code for this.
Setup steps:
My goal is to add a horizontal pod autoscaler that will scale my deployment based on a Nginx external metrics that the HPA gets from Stackdriver.
Here are the high-level steps I took to accomplish this:
References:
Here are more detailed version of those same steps:
You can see that the external metrics API and the custom metric API are not available yet.
Lets fix that.
2. Deploy the External Metric server. Here I use the stackdriver adapter, but alternatively there is the Prometheus Adapter too. The Custom Metrics Stackdriver Adapter is an implementation of Custom Metrics API and External Metrics API using Stackdriver as a backend. This service makes stackdriver metrics available at a k8s API endpoint.
Following the steps from the google docs, I deployed the stackdriver adapter:
Check that the Stackdriver deployment was successful and that the custom and external metric APIs are now available:
Thats pretty cool how many custom metrics are available for use. However I want to use an external metric from Nginx metrics. So I need to get nginx setup to send its metrics to stackdriver so that those will be available as well.
3. I deployed Nginx ingress controller with the official helm chart. Then configured Nginx Ingress Controller to send its metrics to stackdriver. Luckily, Nginx Ingress controller already has a route /metrics at port10254 that exposes a bunch of metrics in prometheus format ( here is an example curl request to the nginx metrics endpoint to see a list of what metrics are exposed).
Also, stackdriver supports uploading additional metrics in prometheus format. In order to do this, I deployed the prometheus-to-stackdriver sidecar with the Nginx Ingress Controller deployment. This sidecar scrapes metrics and sends them to stackdriver.
Using this example of how to create the sidecar, I added this prometheus-to-sd container to the nginx-ingress-controller deployment, configuring the — source with the port and route of the metrics:
I could check now that the nginx external metrics we available in Stackdriver by navigating to the Stackdriver metrics dashboard:
Also I could check that the nginx metrics are now available at the kubernetes external metric api endpoint now. For example, I can retrieve the value of nginx_connections_total.
4. Here is an example helm chart of a sample nodejs app I deployed. Now that the external and custom metrics are available to use, I can create the horizontal pod autoscaler to scale my example nodejs application based on any of the nginx metrics. For example, lets say I wanted to scale up the app when there were more than one active connections to nginx. I can create an HPA that will increase the replica count of the Deployment example-nodejs-app when the metric nginx_connections_total increase beyond the targetValue of 1.
The HPA shows there is one current connection to nginx, so the replica count is 1. If the nginx connection increases, so will the pod replicas. While scaling on nginx connection counts may not be the best metric to scale on, its a pretty cool example of how all this works.
Resources:
If you enjoyed this story, clap it up!
uptime 99 is a ReactiveOps publication about DevOps, containers, and everything cloud. If you have an article you’d like to submit, email us!
Live simply. Program stuff.
See all (107)
333 
6
333 claps
333 
6
We collect, curate, and publish articles on everything cloud, kubernetes, open source, and security.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/workday-engineering/large-message-handling-with-kafka-chunking-vs-external-store-33b0fc4ccf14?source=search_post---------12,"There are currently no responses for this story.
Be the first to respond.
By Adam Kotwasinski, Senior Software Development Engineer, Workday
Multiple services in Workday use Kafka as a messaging bus (https://kafka.apache.org/uses), streaming various types of data. Unfortunately, Kafka imposes a limit on the size of the payload that can be sent to the broker (compared to RabbitMQ, that does not have such a limit). Removing the limit would allow the business layer code to solve the business problem, without Kafka-specific behaviours leaking into application code.
If the message is larger than the value accepted by the broker, the Kafka producer returns this exception:
Large payloads can be split into multiple smaller chunks that can be accepted by brokers.
The chunks can be stored in Kafka in the same way as ordinary (not-chunked) messages. The only difference is that the consumer would need to keep the chunks and combine them into the real message when all chunks have been collected.
The chunks in the Kafka log can be interwoven with ordinary messages.
For example, for a message chunked into three chunks, the consumer could have already consumed one of the chunks:
After the consumer receives more messages, we can see that the second chunk has been received:
After the third and final chunk is received, the message can finally be combined and returned to the end user, and the cache can be cleaned up:
The chunk size is derived from the Kafka producer configuration (max.request.size).
Each of the chunks get sent together with this metadata:
Chunk payload format
Sending messages
The main classes for this functionality are message producer and message chunker.
The code generating the chunking payloads (together with the above headers) is relatively simple. Given the payload and maximum allowed size, we split it into roughly payload.size/chunkSize number of chunks:
The offset returned by send methods must point to the lowest offset assigned to all of the chunks. Any other result would make it impossible to seek to the first chunk:
The Kafka producer API allows the user to compute the message’s partition from the message key. The result is based on the number of partitions currently hosted in the cluster (DefaultPartitioner in Kafka 1.0). Right now, the implementation sends the chunks to partition 0 if the partition was not specified. In future versions, we might want to change it by reading the partitions returned by the first chunk send operation, and sending other chunks to the same partition.
Otherwise, we could end up with a situation where consumers consuming from a single partition would never manage to receive a full message, as chunks would be spread across partitions:
Receiving messages
The main classes for this functionality are abstract consumer and message chunker.
When a Kafka message containing a chunk is received, it is kept locally and not returned to the user (as one would see no benefit in getting just a part of the payload). Only when all chunks have been collected are they assembled together into a single message and returned to the user.
Cleaning up this store might be needed after offset-changing operations, as seeking into a position between chunks could have returned a full message, when the user intended to seek to a later point (after the head of the message). Example: for a six chunk message, we already have received chunks 1, 2, and 3. After seeking to position three again, we’d have consumed chunks: 3 (again), 4, 5 and 6 (the new ones). This means that all chunks have been received, while chunks 1 and 2 were received before the seek operation, and should have not been made available to the user. In the previous diagrams, the offset to seek for would be `N+1`.
Currently the chunk store is implemented as a Java in-memory map.
Group management poses a challenge for chunk-based solutions. It is possible that a consumer group could get rebalanced, while some of the chunks have already been received by the old consumer. So, we would end with one (old) consumer storing beginning chunks, and the other (new) consumer receiving the remaining ones. The potential solutions for this are:
Instead of sending a large payload over Kafka, we could try a different approach — store the real payload in an external data store, and only transfer the pointer to that data. The receiver would then recognize this type of pointer payload, transparently read the data from the external store, and provide it to the end user.
Pointer envelope payload format
Generic IoC
The sending part would then be responsible for saving the payloads and sending the pointer through Kafka:
So long story short, the messages in Kafka would refer to the entries in the external store:
The external store would need to implement three methods: write, read, and rollback (in case of failed transactions):
The main class for this functionality is message chunker:
The receiving part would then be responsible for recovering the payloads:
In case of Kafka-send failures, it is necessary to rollback the external store. The payload generated by the store during write operation is provided as an argument to the `rollback` method. It is then the external store’s responsibility to do any necessary cleanup:
kafka-over-redis
In this implementation, we have used Redis as an external data store, but other type of persistent storage (like RDBMS) could be used too. The implementation is present as redis-backed external store.
To uniquely recognize payloads, each Redis key is based on the original message UUID and Kafka physical name of destination:
With Redis, the maximum payload size that can be stored as a single entry is 512 MB. Payloads larger than 512 MB need to be saved as multiple entries.
The key is just the (above computed) keyStr with segment number. Right now the expiry policy has to be maintained separately for Redis and Kafka (see `this.dataExpiry` below).
Since we could have saved the payload as more than one entry in Redis, we need to provide the number of segments in the response payload:
The diagram below shows how 3 messages could be stored, with 2, 3, or 4 segments each:
Receiving the real payload is just reading N segments with the key extracted from the received argument:
The received segments then get merged together and returned to the parent client.
A rollback step is very similar to a read step. Instead of reading payloads, they are deleted.
This table summarizes the differences:
The Workday Technology Blog is a collaboration of…
270 
3
270 claps
270 
3
Written by
Please visit our career site at https://www.workday.com/en-us/company/careers.html
The Workday Technology Blog is a collaboration of engineers, product managers, and designers at Workday to share and discuss best practices, lessons learned, and innovative solutions for the next wave of technology products.
Written by
Please visit our career site at https://www.workday.com/en-us/company/careers.html
The Workday Technology Blog is a collaboration of engineers, product managers, and designers at Workday to share and discuss best practices, lessons learned, and innovative solutions for the next wave of technology products.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/forecasts-in-snowflake-facebook-prophet-on-cloud-run-with-sql-71c6f7fdc4e3?source=search_post---------13,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Felipe Hoffa
May 14, 2021·8 min read
The goal for this post is to build a function you could use within Snowflake to forecast time series. A great open source tool for this is Facebook Prophet, and we just need a way to use it within our Snowflake environment. This is easy, with Snowflake’s ability to run external functions — hence we only need to host an instance of Prophet and add the necessary plumbing to end up with a prophetize(timeseries) function within Snowflake.
Let’s start with a demo, any time series in Snowflake will do: For example, the temperature around New York City in the Newark airport since 2018:
Then we can call our function prophetize() (see below how to create it) by aggregating the previous time series into an array with dates and values:
And that’s it. What we get back is an array with predictions. An easy way to visualize these results is to combine the values of the previous two queries:
Interesting notes on the above:
Now let’s check the details on how to connect a function in the Snowflake SQL world prophetize() to Facebook Prophet running on Cloud Run.
The requirements.txt to build this container are straightforward:
As the Dockerfile:
And this is main.py, a basic web server that parses incoming arrays into a dataframe that Prophet uses to forecast an arbitrary number of periods. Then it returns a serialized array with a forecast and uncertainty intervals that Snowflake will receive:
If we want to build and run this container on Google Cloud run, we need to run:
Building the image on Cloud Build is slow the first time, as it takes time to compile Prophet — but this cloudbuild.yaml makes it fast with an image cache on further builds:
One of the main goals I had behind this project was to celebrate that Snowflake now supports external functions for GCP. Hence my choice to deploy on Cloud Run.
Now, to run external functions through GCP, we need to set up a connection from Snowflake to Google API Gateway, and from API Gateway to Cloud Run.
First we need a gateway.yaml for API Gateway to know that it will act as a proxy to the service we deployed on Cloud Run:
Then you can follow the GCP docs to create an API Gateway with this configuration. Oh, and make sure to replace the values above with your own service URLs.
This is how I connected the dots on Snowflake, to create an integration with API Gateway:
And that’s all you need, now you can call the just minted prophetize() as in any other query in Snowflake:
Which gives results like:
You might have noticed many URLs in my configs above — now that you’ve seen them, you might want to start calling my functions from your accounts. That would be fine, but I’d rather protect them.
Snowflake makes this easy. Once you create the integration above, a service account for GCP will be automatically provisioned. You can get its value with describe integration prophet_test — and then use that service account to update the gateway.yaml so no one else can call it:
Then follow these Snowflake docs to update your GCP API Gateway with the above secured config.
Note that this GCP service account is provisioned by Snowflake regardless of what cloud you are using to host your Snowflake account. In this case I ran this whole demo on Snowflake on AWS, and it was able to call the GCP services effortlessly.
Meanwhile in Cloud Run, make sure to stop allowing unauthenticated invocations. With this only calls authorized through the API Gateway will be served:
Facebook Prophet is a versatile tool with many levers and ways to tune:
Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.
Read more from:
Knoema already has the valuation of multiple crypto coins in the Snowflake Marketplace, thus building a time series forecast with Prophet is straightforward:
The more you play with Prophet, the more you’ll see that the forecast will depend heavily on what values you feed it, and how you tune it. In this case, the forecasts varied widely depending on what starting date I used for the time series — starting in 2016 meant Prophet would observe a more complex pattern that feeding it data starting in 2018, and so forth.
Best part for me? I can pull all this off without leaving my comfy Snowflake SQL web UI:
I’m Felipe Hoffa, Data Cloud Advocate for Snowflake. Thanks for joining me on this adventure. You can follow me on Twitter and LinkedIn, and check reddit.com/r/snowflake for the most interesting Snowflake news.
github.com
medium.com
Data Cloud Advocate at Snowflake ❄️. Originally from Chile, now in San Francisco and around the world. Previously at Google. Let’s talk data.
See all (1,656)
149 
2
Thanks to Kent Graziano and Sri Chintala. 
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
149 claps
149 
2
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@harshithdwivedi/how-disabling-external-ips-helped-us-cut-down-over-80-of-our-cloud-dataflow-costs-259d25aebe74?source=search_post---------14,"Sign in
There are currently no responses for this story.
Be the first to respond.
Harshit Dwivedi
Aug 29, 2019·4 min read
Tl;dr : Set --usePublicIps=false in the execution parameter on your Dataflow pipeline.
For some context, we’re making a real-time data aggregation pipeline which guarantees that you get your web/app user data made available to you in near-real time (5–6 secs).
Think of it like Google Analytics, but on steroids!
Our entire infrastructure is built on Google Cloud Platform and we are using the following products within the GCP family :
To give you a scale of things we’re trying to do, here’s a screenshot outlining the average traffic we handle from a single property of our friends over at Z1Media :
The challenge here is obviously to keep the throughput and reliability of the system as high as possible while keeping the costs to a minimum.
We recently switched to BigQuery loads instead of streaming data directly into BigQuery which helped us cut down the costs by a fraction of 30% per day as Loading Files into BigQuery is free and we kept the data loading frequency to a bare minimum so that the data is near-realtime in nature.
Enabling BigQuery File Loads requires you to write the data to a Google Cloud Storage bucket where BQ can load the data from. To see if this would incur us any costs, we quickly looked at the docs to see the ingress/egress price for GCS.
Turns out that the Ingress/Egress data to and from Google Cloud Storage within products in the same region was Free and lucky for us, both Cloud Dataflow and BigQuery were in the same region of Tokyo (yay!).
Within a day of this new setup in place, we were overjoyed by the cost reduction until we saw a new pricing metric being added to our billing (which was more than what streaming inserts used to cost us).
Googling for what Carrier Peering means gave us no meaningful results either; the official docs mentioned that the Carrier Peering was to be used when you wanted to access G-suit products from within Google Cloud, which is something we weren’t doing.
On searching for questions on Stackoverflow and reddit, we revisited the docs for compute engine which stated that this ingress/egress pricing was free and a minute info caught our attention.
Egress to Google products (such as YouTube, Maps, Drive), whether from a VM in GCP with an external IP address or an internal IP address
Internal IP address was something we assumed that our dataflow would be using since we nowhere specified it to use an external IP address; and since we didn’t want our data to be available to anyone else apart from BigQuery we didn’t need to have an external IP in the first place!
Turns out that by default, Dataflow will enable external IPs and you need to pass a flag --usePublicIps=false while executing your Dataflow pipeline that disables it if you want to do so.
But just passing the flag isn’t enough as our pipeline failed immediately when we tried to start it with the following message :
Workflow failed. Causes: Subnetwork ‘’ on project z1media network ‘default’ in region asia-northeast1 does not have Private Google Access, which is required for usage of private IP addresses by the Dataflow workers.
Turns out that by default, accessing other Google Cloud services via internal IP is not allowed; so to fix this; we went to VPC network from the Cloud Dashboard and selected asia-northeast1 since that was where all of our products were situated in.
From within there, we simple enabled the option to allow “Private Google Access” and that was it!
And that was it!
We ran our pipeline after making these changes and everything was back to normal without affecting our workflow.
If you are working at a high growth company and want your data made available to you as soon as it’s created; take a look at https://roobits.com/ and we might be what you are looking for!
Thanks for reading! If you enjoyed this story, please click the 👏 button and share to help others find it! Feel free to leave a comment 💬 below.
Have feedback? Let’s connect on Twitter.
Has an *approximate* knowledge of many things. https://aftershoot.co
See all (33)
312 
1
312 claps
312 
1
Has an *approximate* knowledge of many things. https://aftershoot.co
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/envoy-external-authorization-server-envoy-ext-authz-helloworld-82eedc7f8122?source=search_post---------15,"There are currently no responses for this story.
Be the first to respond.
I’ve been working with Envoy Proxy for sometime and covered a number of ‘hello world’ type of tutorials derived from my own desire to understanding it better (i tend to understand much more by actually rewriting in code and writing about; it helps reinforce).
Recently, wanted to understand and use the external authorization server since i specialize in authn/authz quite a bit for my job. In digging into it earlier today, i found a number of good samples that this post is based on:
But as with anything I do, I gotta try it out myself from scratch an reverse engineer…otherwise its it doesn’t hold object permanence for me. This post is about how to get a basic “hello world” app using envoy.ext_authz where any authorization decision a envoy request makes is handled by an external gRPC service you would run. You can pretty much offload each decision to let a request through based on some very specific rule you define. You ofcourse do not have to use an external server for simple checks like JWT authentication based on claims or issuer (for that just use Envoy's built-in JWT-Authentication). Use this if you run Envoy directly and wish to make a decision based on some other complex criteria not covered by the others.
This tutorial runs an an Envoy Proxy, a simple http backend and a gRPC service which envoy delegates the authorization check to. You can take pretty much anything out of the original inbound request context (headers, etc) to make a allow/deny decision on as well as append/alter the response headers)
Before we get started, a word from our sponsors …here are some of the other references you maybe interested in
Well…its pretty straight forward as you’d expect
Steps 2,3 is encapsulated as a gRPC proto external_auth.proto where the request response context is set:
What that means is our gRPC external server needs to implement the Check() service..
The source code for this repo at:
github.com
Anyway, lets get started. You’ll need:
The backend here is a simple http webserver that will print the inbound headers and add one in the response (X-Custom-Header-From-Backend).
The core of the authorization server isn’t really anything special…i’ve just hardcoded it to look for a header value of ‘foo’ through…you can add on any bit of complex handling here you want.
The envoy confg settings describe th ext-authz in focus here:
The moment you start envoy, it will start sending gRPC healthcheck requests to the backend. That bit isn’t related to authorization services but i thouht it’d be nice to add into envoy’s config. For more info, see the part where the backend requests are made here in this the generic grpc_heal_proxy
Thats it…but realistically, you probably would be fine with using Envoy’s built-in capabilities or with Open Policy Agent or even Istio Authorization. This repo is just a demo of stand-alone Envoy.
If you want to add a custom metadata/header to just the authorization server that was not included in the original request (eg to address envoy issue #3876, consider using the attribute_context extension
In the configuration above, if you send a request fom the with these headers
Client:
External Authorization server will see an additional context value sent ""x-forwarded-host"" which you can use to make decision.
Finally, the backend system will not see that custom header but all the others you specified
SO/git Issues
Google Cloud community articles and blogs
104 
2
104 claps
104 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@duhroach/internal-ip-vs-external-ip-performance-76f15a650356?source=search_post---------16,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Jul 27, 2017·4 min read
“We’re not getting the throughput on our VMs. I think the bald guy in those videos is lying.”
Calls with customers are fun.
Google has a fantastic crew of dedicated people to help you get to the bottom of your cloud problems. I was lucky enough to sit in on a call with “Gecko Protocol” a B2B company offering a custom, light-weight networking protocol built for gaming and other real-time graphics systems.
They reached out to our fantastic support team since they were seeing lower-than-expected throughput for their backend machines which were responsible for transferring and transcoding large video & graphics files.
Here’s the graph they shared with us:
Truth is, yes, those are a lot smaller numbers than I’d expect. Let’s dig in a bit more and see what’s going on.
I grabbed the configuration data from their engineering director, and duplicated a test on my side of the fence. Since they were testing throughput, we can test it with iPerf. Following the same thing we did in the other article, we simply need to setup iPerf on the boxes, designate one of them as a server, and point the 2nd one at it.
1.95GB / sec was much higher than what Gecko Protocol was seeing in their graphs. Just to sanity check some things, I jumped on a quick video chat with their engineering team, and tried to get them to reproduce this test.
After about 20 minutes of “I still don’t see the same numbers” the reason for the problem suddenly appeared.
While setting up their tests, the engineer driving the setup made one disconnect in our discussion. He was testing the external IP of the server, while I was testing the internal IP.
I switched over to testing the external IP in my tests, and got the same results as Gecko Protocol was, much slower.
We see the difference is 1.066 gb / sec between using internal vs. external IPs in this test.
At this note, the team quickly scrambled : One of their engineers realized they were using external IPs for all their backends, even when transferring data within the same zone; and with this difference in throughput, it’s clear to see a bottleneck.
While Gecko Protocol was fixing up their target IPs, I decided to run another test. Something was wrong with the numbers we were looking at, since I know from experience that the networking latency between instances on the same zone should be significantly higher; not to mention that the Gecko Protocol group was seeing about ~1.7 gb/ sec, but our tests were topping out at ~1.06 gb / sec.
Having just found a throughput problem with the Dobermanifesto group, I decided to check a higher core instance and see if that would get us closer to the numbers they were seeing.
Sure enough, running a 16vCPU machine, doing same-zone transfer, on an external IP showed exactly the bandwidth that GeckoProtocol was seeing:
When we switched over to using the internal IP for the same test, on the larger machine, the bandwidth went through the roof:
The difference was 14.21 Gbits/sec between the internal & external IPs, using the right CPU configuration and same-zone transfer.
For Gecko Protocol, this small debugging session resulted in their video and graphics data transfer improving by ~14x. Which is immense, considering their offering is built on performance backend services for high-performance compute scenarios.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
94 
94 
94 
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
"
https://medium.com/hacking-and-slacking/building-complex-features-in-mongodb-cloud-632069407fa2?source=search_post---------17,"Sign in
There are currently no responses for this story.
Be the first to respond.
Todd Birchard
Dec 14, 2018·7 min read
Friends, family, and most importantly, strangers: I approach you today with a tale of renewed inspiration. After loudly broadcasting my own confusion and mediocre ability to actually implement an effective cloud via MongoDB Stitch, my ineptitude has been answered with an early Christmas…
"
https://blog.searce.com/configure-external-master-for-cloudsql-replicate-vm-mysql-to-cloudsql-1f0e059e1cc6?source=search_post---------18,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Bhuvanesh
Jul 26, 2018·5 min read
In our previous blog, we have explained that to configure external replica for a CloudSQL instance. Here we are going to see how to configure external master for CloudSQL. Unlike native replication or the method which we explained in our previous blog, setting up external master for CloudSQL is pretty different way. Lets see how we can achive this.
cloud.google.com
Note: The communication between the VM and CloudSQL is not Private. Because the CloudSQL is not a part of VPC.
Make the below configuration changes in mysql conf file.
Restart the MySQL to apply the changes.
Run the below command to take the backup. (while restoring you can use native .sql or .gz file).
Recently we encountered an issue while setting up this replication. Then we got to know if your dump has triggers or views, then the dump will not be imported into CloudSQL. So please ignore the views in the dump file. And this error also not visible for you in StackDriver logs.
You can assign service account to the VM and start upload the backup file using gsutil.Refer the below link for using gsutil command line interface.
cloud.google.com
Note: How this works?
Your backup file contains the Binlog file name and its position and GTID. So once the restoration has been done, CloudSQL will automatically try to establish the replication from the next GTID. If you are not using GTID replication then it’ll start replicating from the binlog position.
In the first step, we have allowed mysql port to public. Because we don’t know the slave IP address until its created. Now we have the IP.
Go to FireWall rules and add the slave IP to access it in 3306 port. Once this added remove the public rule(0.0.0.0/0).
medium.com
From our past 2 blogs, we have learned that GTID replication is good while migrating data to CloudSQL. Our Next target manually setup replication between two CloudSQL instances. So stay tune.
Hope you are going to play this, give some claps if it helped.
Less Talk, More Data | https://thedataguy.in
See all (503)
34 
1
34 claps
34 
1
Searce is a niche’ cloud-native technology consulting company, specializing in modernizing infra, app, process & work leveraging Cloud, Data & AI. We empower our clients to accelerate towards the future of their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/how-to-setup-aws-lambda-in-scala-without-any-external-library-9b1e754d29a5?source=search_post---------19,"There are currently no responses for this story.
Be the first to respond.
You have 1 free member-only story left this month. Sign up for Medium and get an extra one
(Originally published at https://edward-huang.com)
AWS Lambda has been widely used to trigger function as a service in a serverless architecture. It is an event-driven computing platform. Lambda runs when an event triggers it. It executes the function that loads into the system.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.searce.com/configure-external-listener-for-always-on-availability-groups-in-gcp-e1ae1c9632d1?source=search_post---------20,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bhuvanesh
Aug 8, 2019·9 min read
If you are dealing with Always on availability groups, then you are familiar with the Listener. Its like an endpoint for your Primary Server in availability groups. It’ll swap the DNS to the elected primary during the failover. So you don’t need to change anything in your connection string. You can add or remove SQL Server nodes at any time.
Its better you can read the explanation from Microsoft.
Your Availability Group can contain replicas that are on-premises only, Azure only, or span both on-premises and Azure for hybrid configurations. Azure replicas can reside within the same region or across multiple regions using multiple virtual networks (VNets). The steps below assume you have already configured an availability group but have not configured a listener.
Before moving into deep, I want to recommend that, Privately accessible listener are good. But never use the publically accessible listeners. In my case, just showing that its possible not only on Azure.
We have done a SQL Server migration recently from On-Prem to GCP. 90% of the app servers are already in GCP. But few application servers are still on the Datacenter. But those servers should access the Primary Server. And we have a site to site VPN between GCP and On-Prem. So instead of changing something on all the app servers, we decided to use an External Listener with private IP only.
From configuring AD to Always-on availability groups, I skipped here. You can find a lot of blogs for that. So Im just showing after the always-on availability group has been created.
Once you configured WSFC, you should assign an IP address for that. We need to reserve this IP and the listener IP(listener you are going to create in a while). So reserve these IP address from your subnet.
We are going to create this Listner from Failover Cluster not from SSMS.
Now we need to add the Probe port to the cluster resource. Open Powershell anyone on the Cluster node and run this command. Please change the parameters based on your system.
Validate the configuration.
On availability group, create a dependency for the listener. So this Listener IP and the probe port will be handled by the primary node.
Failover Cluster Manager → Roles → AG name → Resources → AG name → Properties → Dependencies.
To prevent the connection timeout, we need to adjust the RegisterAllProversIP and HostRecordTTL.
If you have launched all the SQL servers in a single Zone, then in Instance group we can add all the SQL servers. Else for each SQL server, we need to create instance group. Here my both primary and secondary SQL servers are in the same zone. So one instance group is fine.
The above screenshot, Under the Healthy its 1/2 means the Primary is listening 1444 port. So if you connect your applications, with this IP, it’ll route the request to the current primary.
NOTE: Make sure you need to create a firewall rule which allows GCP’s network services should talk to your SQL VM’s with 1444 port.
You can download this software to test the failover.
If you want to access from this LB, you need to connect to SQL server with the port 5222.
You can configure Read Only routing without any issues. But you need to make sure you have given 1433 port for read-only URL. Else it’ll not connect.
We may solve 99% problems, but we are using some creak kind of things here to access the listener from outside. So, for now, I found two issues.
Generally, its a best practice to use native things, instead of doing these kinds of funny things. But it was only available on Azure, now we implemented this on GCP. Private Load balancers are good, but don’t use Public one. And these steps will not work on AWS. We’ll try this on AWS and post it soon.
Happy Always on :)
Less Talk, More Data | https://thedataguy.in
88 
1
88 claps
88 
1
Searce is a niche’ cloud-native technology consulting company, specializing in modernizing infra, app, process & work leveraging Cloud, Data & AI. We empower our clients to accelerate towards the future of their business.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/routing-from-kubernetes-to-external-vms-using-the-ambassador-api-gateway-a-terraformed-playground-4faead9b021d?source=search_post---------21,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
At Datawire, we are seeing more organizations migrating to their “next-generation” cloud-native platform built around Docker and Kubernetes. However, this migration doesn’t happen overnight. Instead, we see the proliferation of multi-platform data centers and cloud environments where applications span both VMs and containers. In these data centers the Ambassador API gateway is being used as a central point of ingress, consolidating authentication, rate limiting, and other cross-cutting operational concerns.
This article is the first in a series on how to use Ambassador as a multi-platform ingress solution when incrementally migrating applications to Kubernetes. We’ve added sample Terraform code to the Ambassador Pro Reference Architecture GitHub repo which enables the creation of a multi-platform “sandbox” infrastructure on Google Cloud Platform. This will allow you to spin up a Kubernetes cluster and several VMs, and practice routing traffic from Ambassador to the existing applications.
I’ve written previously about using an edge proxy or gateway to help with a migration from a monolith to microservices, or a migration from on premises to the cloud. Ambassador can act as an API gateway or edge router for all types of platform, and although it was designed and built to run exclusively on Kubernetes, it is trivial to configure traffic routing from the cluster to external network targets, such as endpoints within VPNs or virtual private clouds (VPCs), cloud services, cloud load balancers, or individual VMs. If you have network access to the endpoint, then Ambassador can route to it.
Our Ambassador Pro Reference Architecture GitHub repo contains several folders that provide documentation and examples to help you understand how best to use all of the features that Ambassador supports, like rate limiting and distributed tracing. There is also a “cloud-infrastructure” folder that contains the necessary Terraform code and scripts to spin up a sample multi-platform VM / Kubernetes infrastructure using Google Cloud Platform (GCP). The resulting infrastructure stack is show below:
The Terraformed infrastructure example provided in the Ambassador Reference Architecture repo will create a simple regional network in GCP with a Kubernetes (GKE) cluster and several VM-based services deployed behind (publicly addressable) load balancers. The application deployed on the VMs has been taken from my “Docker Java Shopping” example of a very simple e-commerce shop, and this consists of two Java services using Spring Boot and one using Dropwizard.
Deploying Ambassador within the Kubernetes cluster enables the simplification of ingress for the entire network, and also allows the engineering team to centralise and standardise the management of the this gateway. Centralising operations of the gateway and edge of the network provides many benefits, such as the reduction of “authentication sprawl” and the ability to standardise cross-cutting concerns such as TLS termination or pass-through, context-based routing (e.g. using Filters to route based on HTTP headers), and rate limiting.
After cloning the reference architecture repo, navigate to the folder containing the GCP Terraform code and you will find a README with step-by-step instructions required to replicate our configuration. Be aware that spinning up this infrastructure will cost you money if you are outside of your GCP free trial credit:
Once you have everything configured and have run terraform applysuccessfully (which may take several minutes to complete), the infrastructure shown in the diagram above will have been created within your GCP account. You will also see some outputs from Terraform that can be used to configure your local kubectl tool, and also set up Ambassador.
The first output, with the name gcloud_get_creds, can be run to configure your local kubectl to point to the newly Terraformed Kubernetes cluster e.g. from the output above, I would run at my local terminal:
You can now install Ambassador into the cluster by following the Getting Started instructions, or following the quick-start in the README. Once the gateway is up and running and you have obtained the external GCP load balancer IP for the Ambassador Kubernetes Service, you can now deploy an Ambassador Mapping that routes to a GCP load balancer that is located outside of the Kubernetes cluster. I’ve deliberately kept the network routingand firewall rules simple with the current infrastructure, but future iterations of this tutorial will introduce more challenging configurations.
The Terraform output named shopfront_ambassador_config provides Kubernetes configuration that can be copy-pasted into a YAML file and applied into cluster. You should then be able to access the Shopfront service that is running on a VM (and communicating with other upstream services also running on VMs), via the Ambassador IP and the associated mapping e.g.: http://{AMBASSADOR_LB_IP}/shopfront/
If all goes well you should be able to see the following in your browser:
This is just the beginning of a range of tutorials we will present about over the coming months. We are keen to add more complexity, for example, creating network segments with peered VPCs and more complicated firewall rules, and we will also be looking to demonstrate using Kubernetes ExternalNameservices and Consul Connect to implement a multicluster service mesh for the implementation of full end-to-end TLS.
When you’ve finished experimenting with the Terraformed infrastructure, don’t forget to delete this and clean up, or otherwise you could be facing a unexpected GCP invoice!
This article and associated multi-platform data center example have been designed to help engineers migrating applications from VMs to a Kubernetes cluster. Ambassador is often used as a central point of ingress for the entire estate, and this allows the consolidation of authentication, rate limiting, and other cross-cutting operational concerns.
We will continue to iterate on the example infrastructure code, and also plan to support for additional cloud platforms like Digital Ocean and AWS. Please do reach out to me if you have any particular requests for cloud vendors or complicated routing scenarios.
As usual, you can also ask any questions you may have via Twitter (@getambassadorio), Slack or raise issues via GitHub.
This article was originally published on the getambassador.io blog.
ITNEXT is a platform for IT developers & software engineers…
68 
Thanks to Kiarash Irandoust. 
68 claps
68 
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Director of DevRel @ambassadorlabs | News Manager @InfoQ | Chair @QConLondon | Biz-Dev-Ops
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mohitkumarsrivastav/application-development-on-salesforce-app-cloud-89b4098476cf?source=search_post---------22,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohith Shrivastava
May 21, 2016·4 min read
When I first started working with force.com platform as a developer back in 2011 the only way to surface salesforce data or external data on the UI was using Visualforce and prior to that s-Controls were very popular way to expose data to the browser and that year saw the decline for the s-Controls and eventually s-Controls were not supported .
Visualforce(almost I could relate to JSP) and apex (sort of Java) seemed to be right and powerful approach back in 2011–2012 to build customized application pages and it remained to be an excellent approach .Although there was concept of using visualforce components(breaking page into multiple components) it was less understood by salesforce developers and most ended writing one monolithic page with a single apex class with large no of lines increasing complexity to debug the code.There was no way to enforce developers to use components as building blocks to complete page.
Year 2013–2014 saw some great improvements in JavaScript and SPA apps seemed to excite end users .With the Javascript remoting made available by the platform ,allowed the apex code to return JavaScript objects back on to the visualforce pages and this allowed developers to leverage libraries like Jquery and Bootstrap to build rich UI .This means visualforce was used just as a container .Also salesforce community cloud maturing faster there was need to provide Rich user Interface to the users and hence the plain visualforce tags provided by the visualforce became less exciting when it boiled to building mobile friendly ,high permanent responsive visualforce pages.
Year 2015 saw some Javascript frameworks like Ember ,Angular ,React and many more maturing faster and using Javascript remoting with these framework was widely adopted and quickly provided Javascript developers and UX designers and engineers a huge role in building UI rich application .Thanks to the visualforce container ,it allows you to embed Javascript and HTML and CSS acting like a webserver .2015 also saw rise of component frameworks like polymer ,React .Re imagining entire web page as components seems like a very logical approach .Also the year where lightning component framework made huge progress and to help admins there were builders introduced by salesforce to just drag and drop the components built by devs .Component framework also provided mechanism to split an application page into no of files allowing easier to debug and build and bundle .
So now we are in state of problem specially for new developers on the platform and those who are new to Javascript world (like me ) should we learn lightning component framework or Visualforce or new Javascript frameworks like Angular,React ,Polymer ,Ember and list goes on …?And challenge for architects who build ISV apps ,what is the ideal approach with all these lightning ,Javascript frameworks or Visualforce pages ??
The answer is it really depends on lot of other questions that one should ask before choosing what to pick
One thing that one needs to be very careful is about no of API calls your app will consume once its build .Salesforce has excellent REST and SOAP support but you need to be really thinking of an optimal approach and not consume much .One way to do this use visualforce containers with Javascript Remoting (Javascript remoting consumes no API calls .So this is great approach for ISV apps using frameworks ).No of API calls consumed counts against the cost .
If you are an ISV and trying to host a lightning component or an app the problems are too many .Right from choosing which frameworks (Visualforce (pure)or visualforce+frameworks or lightning components)to use ,security and writing optimal code to consume less API’s can be pretty challenging .That’s were a PDO (Product Development Outsource) come handy .They have excellent team with all skill-sets plus they have walked the path many times to help you get market faster .PDO also from their expertise they have excellent libraries to accelerate the build process .Never walk alone when it comes to building applications on salesforce platform ,it might look simple but since its a one vast platform it requires experience and expertise .
Lead Developer Advocate @Salesforce, Author “Learning Salesforce Lightning Application Development” & “Learning Salesforce Einstein”
56 
1
56 claps
56 
1
Lead Developer Advocate @Salesforce, Author “Learning Salesforce Lightning Application Development” & “Learning Salesforce Einstein”
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/external-authorization-server-with-istio-1159b21682bb?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
Tutorial to setup an external authorization server for istio. In this setup, the ingresss-gateway will first send the inbound request headers to another istio service which check the header values submitted by the remote user/client. If the header values passes some criteria, the external authorization server will instruct the authorization server to proceed with the request upstream.
The check criteria can be anything (kerberos ticket, custom JWT) but in this example, it is the simple presence of the header value match as defined in configuration.
In this setup, it is important to ensure the authorization server is always (and exclusively) called by the ingress gateway and that the upstream services must accept the custom JWT token issued by the authorization server.
To that end, this configuration sets up mTLS, RBAC and ORIGIN authentication. RBAC ensures service->service traffic flows between the gateway, authorization server and the upstream systems. Each upstream service will only allow ORIGIN JWT tokens issued by the authorization server.
This tutorial is a continuation of the istio helloworld application.
11/25/21: Updated for example to NOT use an actual service account. Instead, use the istio built gen-jwtpy in JWT issuers
3/20/21: Updated for istio 1.9: Integrate external authorization system (e.g. OPA, oauth2-proxy, etc.) with Istio using AuthorizationPolicy. Part of the upgrade is to use the v3 API (go-control-plane/envoy/config/core/v3, go-control-plane/envoy/service/auth/v3)
github.com
The following setup uses a Google Cloud Platform GKE cluster and Service Accounts certificates to issue the custom JWT tokens by the authorization server. We are using GCP service accounts for the authorization server JWTs simply because each service account on GCP has a convenient public JWK url for validation.
On any GCP project, setup env vars.
You can use the following prebuilt containers for this tutorial if you want to.
If you would rather build and stage your own, the Dockerfile for each container is provided in this repo.
The images we will use here has the following endpoints enabled:
salrashid123/svc: Frontend service
salrashid123/besvc: Backend Service
salrashid123/ext-authz-server: External Authorization gRPC Server
To build your own, create a public dockerhub images with the names specified below:
Create a 1.19+ GKE cluster (do not enable the istio addon GKE provides; we will install istio 1.9.1 manually)
After all the services are in running mode, get the GATEWAY_IP
Deploy the baseline application without the external authorization server
Verify traffic for the frontend and backend services. (we’re using jq to help parse the response)
If you would rather run this in a loop:
Kiali Dashboard
If you want, launch the kiali dashboard (default password is admin/admin). In a new window, run:
First we need to setup the auth* configs to use a convenient JWT/JWK issuer istio provides (you can use any jWT issuer, ofcourse; this is just a demo…do not use this in production!!!)
Use Istio’s sample JWT issuer script
Istio provides a convenient JWT issuer, JWK and script that you can use to for authentication
For example, following script will issue a JWT in the following form and our external authz server will use this to reissue certs
You can also see that its kid key-id is visible too ""DHFbpoIUqrY8t2zpA2qXfCmr5VO5ZEr4RzHU_-envvQ""
and with a JWK
NOTE: we will not be issuing these JWTs. The external authorization server will use the private key to reissue a JWT intended for a given service.
Apply the preset environment variables to ext_authz_filter.yaml:
This will cause a ‘deny’ for everyone since we specified some headers that cannot be met (since we didnt’ even deploy the authzserver in the first place that’d issue the JWT we just declared above!)
Edit mesh-config
add at the top:
Apply the authz config
The static/demo configuration here uses two users (alice, bob), two frontend services (svc1,svc2) one backend service with two labled versions (be, version=v1,version=v2).
The following conditions are coded into the authorization server:
The net effect of that is alice can view svc1, bob can view svc2 using ORIGIN authentication.
As Alice:
If you want to view the authz logs
You should see some debug logs as well as the actual reissued JWT header
note JWT headers include cliams and audiences
As Bob:
As Carol
note, it seems the traffic from the gateway to the authorization server isn’t correctly detected to be associated with the ingress-gateway (maybe a bug or some label is missing)
The configuration also defines Authorization policies on the svc1-> be traffic using BOTH PEER and ORIGIN.
This is done using normal RBAC service identities:
Note the from->source->principals denotes the service account svc1 runs as.
THis step is pretty unusual and requires some changes to application code to forward its inbound authentication token.
Recall the inbound JWT token to svc1 for alice includes two audiences:
This means we can use the same JWT token on the backend service if we setup an authentication and authz rule:
The RequestAuthentication accepts a JWT token signed by the external authz server and must also include the audience of the backend (which alice's token has). The second authorization (redundantly) rule further parses out the token and looks for the same.
Istio does not automatically forward the inbound token (though it maybe possible with SIDECAR_INBOUND->SIDECAR_OUTBOUND forwarding somehow...)...to achieve this requres some application code changes. The folloing snippet is the code within frontend/app.js which take the token and uses it on the backend api call.
4/27/20: update on the comment ""(though it maybe possble with SIDECAR_INBOUND->SIDECAR_OUTBOUND forwarding somehow...)"" Its not; envoy doens't carry state from the filters forward like this. You need to either accept and forward the header in code as shown below:
Or configure istio to make an OUTBOUND ext_authz filter call. The external authz filter will return a new Authorization server token intended for ust svcb.
You will also need to set allowed_client_headers so that the auth token returned by ext-authz server is sent to the upstream (in this case, upstream is svcb)
I think the config would be something like this:
(ofcourse changes are needed to ext-authz server as provided in this repo..)
Note: i added both ORIGIN and PEER just to demonstrate this…Until its easier forward the token by envoy/istio, i woudn’t recommend doing this bit..
Anwyay, to test all this out
Sample output
-Alice
Alice’s TOKEN issued by the authorization server includes two audiences:
Which is allowed by backend services RequestAuthentication policy.
Bob’s token does not include the backend service
Which means the RequestAuthentication will fail. Bob is only allowed to invoke svc2 anyway
Carol’s token is allowed to invoke svc1 but does not include the issuer to pass the RequestAuthentication policy
If you would rather run these tests in a loop
At this point, the system is setup to to always use mTLS, ORIGIN and PEER authentication plus RBAC. If you want to verify any component of PEER, change the policy to change the service account that is the target service authorization policy accepts and reapply the config.
Change either the settings RequestAuthentication or AuthorizationPolicy depending on which layer you are testing
then reapply the config and access the backend as alice
Finally, the external server is attached to the ingress gateway but you could also attach it to a sidecar for an endpoint. In this mode, the authorization decision is done not at the ingress gateway but locally on a service’s sidecar. To use that mode, define the EnvoyFilter workloadLabel and listenerType. eg:
If you do this, you will have to setup PEER policies that allow the service to connect and use the authorization server.
You can debug issues using these resources
To set the log level higher and inspect a pod’s logs:
If you want to use OIDC JWT authentication at the ingress gateway and then have that token forwarded to the external authz service, apply the RequestAuthentication policies on the ingress gateway as shown in the equivalent Envoy configuration here. You can generate an id-token using the script found under jwt_client/ folder.
Google Cloud community articles and blogs
68 
1
68 claps
68 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.searce.com/configure-external-replica-for-cloudsql-replication-from-cloudsql-to-vm-754a4ff00469?source=search_post---------24,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bhuvanesh
Jul 25, 2018·5 min read
CloudSQL is fully managed MySQL / PostgreSQL database system. CloudSQL reduces the workload for DBAs and anyone can easily manage even without a DBA. In many cases, people wants to have a replica of their production database for Testing purpose or even some other purpose. CloudSQL provides the flexibility to have external replicas on VM or On-prem. In this blog we are going to configure external replica for CloudSQL.
CloudSQL 2nd generation support GTID based replication and binlog file based replication. But GTID is more consistent. But 1'st generation only support binlog replication.
A global transaction identifier (GTID) is a unique identifier created and associated with each transaction committed on the server of origin (master). This identifier is unique not only to the server on which it originated, but is unique across all servers in a given replication setup. There is a 1-to-1 mapping between all transactions and all GTIDs. — From MySQL Documentation
Whenever we start a transaction, the Master will assign a GTID for the transaction with the below format.
server_UUID:transaction_id
Eg: If you are running the first time after the GTID enabled then the ID is 1 and the server uuid something like 3E41FA47–71CA-11E1–9E33-C80AA9429562.
So the GTID is 3E41FA47-71CA-11E1-9E33-C80AA9429562:1
Slave will execute the next transaction based on this GTID. So there is no need for mention Binlog file name and its position.
To understand more about GTID, please refer the below link.
dev.mysql.com
cloud.google.com
cloud.google.com
Note: The communication between the VM and CloudSQL is not Private. Because the CloudSQL is not a part of VPC.
Sometimes you will get the below error.
This means the command executed on the Master and slave already has its own password. A deep blog about this issue here.
Hope, this is useful for who all are needs an external slave or migrate from CloudSQL. In our next blog, we are going to explain how to replicate MySQL VM to CloudSQL.
medium.com
Happy Replication :)
Less Talk, More Data | https://thedataguy.in
24 
2
24 
24 
2
Searce is a niche’ cloud-native technology consulting company, specializing in modernizing infra, app, process & work leveraging Cloud, Data & AI. We empower our clients to accelerate towards the future of their business.
"
https://medium.com/google-cloud/external-api-resources-for-google-deployment-manager-8508c3cd4c2f?source=search_post---------25,"There are currently no responses for this story.
Be the first to respond.
To quote from the Deployment Manager (DM) documentation, “Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud Platform resources for you.” …is not entirely true. Sure, you can provision disks, VMs, GKE clusters, PubSub, and so on (think AWS Cloud Formation). Why that statement isn’t accurate is because DM can also manage provisioning an External API Resource. What I mean by that is you an perform the same CRUD operations to perform some task on an external resource from within a DM template.
That capability is pretty powerful..think about it: not only can you set up basic GCP resources, you can also perform some task against an arbitrary API server running somewhere else!.
Imagine that part of your DM template requires creating a VM and then creating some other resource off GCP and coordinating the output of one into the other..you can do that now with DM’s Type-Providers. Custom Type Providers allows you to define your own API server for provisioning tasks outside of GCP resources.
What this article shows is a basic REST app server and a DM template which creates and deletes a ‘resource’ off GCP
Deployment Manager ‘knows’ about your API server by consuming a swagger 2.0 specification describing your API that you need to provide to it during setup.
In this example, a very simple REST API “TODO Server” runs on AppEngine (python + Flask-RestPlus) that acts as the CRUD external API
The REST library i used here automatically furnishes a swagger 2 specification file as derived from code annotations. The API server is also protected by a static, hardcoded key which is also transmitted by DM during its operations.
Finally, the output of of the ‘add-task’ operation is shown in the output so you can take actions based on that or apply them elsewhere.
You can find the full sample here on github
github.com
The sample API server is the basic “Flask-RestPlus” API server which automatically creates a swagger file DM can consume
So, the first step is to deploy the application to GAE (assuming you set a Google Cloud Account and installed the gcloud SDK:
The step above will deploy the “TODO” server to version ‘dm-api’ and will not promote the app to default.
Test the APP is available (ofcourse substitute YOUR-PROJECT everywhere below for your real one):
via curl to see the raw API:
(should show nothing the first time, (as expected))
Now that the API server is deployed, edit
custom_type_provider.yaml and set the swagger_url parameter to your GAE endpoint's swagger file.
swagger_url: https://dmapi-dot-YOUR-PROJECT.appspot.com/swagger.json)
Then run the deployment
Once it completes, you should see
What that means is DM performed the CREATE actions defined in your template but not the DELETE ones yet (just setup its config so when you delete it)
Now delete the deployment alltogether
(if your API server is down during the operation for whatever reason try using the --delete-policy=ABANDON flag…if you do, its suggested to run the next deployment with n --create-policy=CREATE as shown here)
You can capture the output of your own API (the parts it returns in the API call, by referencing them directly in the template or as outputs)
eg, the output may look like:
There!. Whats shows in how to manage a trivial API server from within DM…you can use this to run your own swagger2.0 api server and even transmit security headers to help with its provisioning of external systems.
If you are transmitting secrets over, please manage it carefully from within the DM template. It maybe tricky to figure out the API specifications you need so the sample FLASK app provided here can also be run on plain GCE instances (just comment out the very last sections in main.py). The very last set section there will display ALL the headers recieved by your API server..its great for debugging.
Good luck!
Its recommended to follow the Google API Design Guide when setting up the API server
In the example here, the (Create, List) operations occur at the collection /todo while the (DELETE,GET,PUT) happens on the resource: /todo/{id}.
When defining the DM template use the GET resource as the TYPE-Provider:
The library used has a very nice UI for testing too which you can access at the root path of your API server
You may also want to use Cloud Endpoints if running elsewhere or developing a new API!
Google Cloud community articles and blogs
17 
17 claps
17 
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/cloud-security/cross-account-aws-iam-roles-with-external-ids-and-mfa-4ef2a18bdd27?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
When 2nd Sight Lab performs cloud penetration tests or cloud security assessments for customers, we ask for credentials with a specific set of permissions to analyze accounts to see if they have any security issues. We look at whether the account is following best practices, review architectural issues, can quickly query the network and IAM access, and check places where developers may be storing secrets in code. Some customers specifically request a penetration test, rather than an assessment or architecture review, because they require one to maintain compliance with PCI, HIPAA, or some other regulation.
We usually combine an assessment of cloud infrastructure with a web application assessment or penetration test because a vulnerable application is one of the gateways into your cloud account. We report external web application vulnerabilities on an assessment. On a penetration test, we attempt to exploit vulnerabilities to demonstrate whether the vulnerabilities provide account or data access. We usually also test the internal functionality of applications. If the developers or DevOps team has configured things correctly, the permissions we obtain should expose no sensitive data. If we can access sensitive data via a misconfiguration or vulnerability, that is one of the things we would let customers know in our report.
Our customers provide different credentials depending on what type of assessment or penetration test they are requesting. They may provide web application credentials to perform testing on web applications to see if one customer can access an account belonging to another customer, escalate privileges to an administrative account. We also use internal credentials to test whether then can obtain access to the host or other things in the cloud environment that should not be accessible. For an AWS account, they can give us credentials via an AWS IAM access key to review the security of the AWS account configuration. As long as these keys are created for a short term (our engagements are most often 2–4 weeks) and deactivated or deleted immediately after, the risk should be low.
However, a better approach is to use a cross-account role. That limits the access to people who have the required access from our 2nd Sight Lab account, rather than anyone on the Internet who can obtain an AWS access key. This post covers access to the AWS accounts using AWS IAM roles. Handling the secure transport of web application credentials or other shared credentials like AWS access keys is not included here. We cover that in our instructions provided to customers, our cloud security class, and may provide that in a future blog post.
Requirements for creating an AWS IAM role for an external account:
Create or select an IAM policy
You can create or select a policy in IAM by choosing policies on the left of the IAM dashboard and either searching for and using a specific policy, or creating a new one. The SecurityAudit policy allows a user to review security in the account. 2nd Sight Lab requests a few other permissions to check for access to secrets and other misconfigurations but limited to read-only access.
Assign the policy to a role and add a trust policy that gives the external account access.
On the AWS IAM console, click Roles. Then click the Create role button.
On the next screen, choose Another AWS account. Enter the AWS account number you want to give permissions to access your account. You may want to include an external ID that you provide to the person in the other account (securely). You may also want to require that the person has authenticated via MFA. Then click Next: Permissions.
Note that the external account assigns MFA to the user that assumes the role, and AWS requires that user to login using MFA via the console and programmatically if you select this option. The following page provides more information explaining why you might want to require an external ID:
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html
On the next screen, select the role you created, or choose a built-in role. Below, I am searching for the built-in auditor role. Check the box next to it and click Next:Tags
For our purposes, we are not going to enter any tags. Click Next:Review.
Enter a name and description. Click Create Role.
Now click on your new role name.
Click on the Trust Relationships tab. Here you can see permissions granted to the other account. The screenshot below redacted the account number, which you would see next to the label This account. Click on Show policy document.
That is the trust policy that gives access to the other AWS account (account number redacted below.)
Note that the above policy gives access to root in the external account. You probably want to change that to a specific user or group of users. In our case, we give access to a user named pentester in the external account. Note that the ARN (a unique identifier for something in an AWS account) changes to include the word “user” when assigning the ARN. Unfortunately, it does not appear that we can add an ARN for a group, so every user would need to be added individually.
Set up access in the external account to assume the role
In the external account (in our case, the account owned by 2nd Sight Lab who is performing the cloud penetration test or assessment), go to the IAM dashboard click Policies on the left menu and Create Policy.
On the next screen, click the JSON tab. Enter a policy, as shown. We would enter the customer account number in the policy where indicated below. If the external role requires MFA added a condition to require MFA in this policy as well. You might want to do this in any case. Click Review policy.
On the next screen, create a name and description. Then click Create policy.
Best practice would be to assign this policy to a user in a group. On the IAM dashboard, click Groups on the left. Then click Create New Group.
Enter a Group Name and click Next Step.
Select your new policy and click Next Step.
Click Create Group.
Click on your new group in the console.
Click on the Users tab and then Add Users to Group. If you don’t have an existing user for this purpose, you can create a new one and add it to the group.
Choose the user you want to add to the group. Click Add users.
Now you have a user that can assume the role and perform work in the external account. Note that when adding an external ID, the user can only take programmatic actions and cannot use the AWS console. Stay tuned for more information on this and other cloud security topics.
Teri Radichel — Follow me @TeriRadichel
© 2nd Sight Lab 2020
____________________________________________
Want to learn more about Cloud Security?
Check out: Cybersecurity for Executives in the Age of Cloud.
Cloud Penetration Testing and Security Assessments
Are your cloud accounts and applications secure? Hire 2nd Sight Lab for a penetration test or security assessment.
Cloud Security Training
Virtual training available for a minimum of 10 students at a single organization. Curriculum: 2nd Sight Lab cloud Security Training
Have a Cybersecurity or Cloud Security Question?
Ask Teri Radichel by scheduling a call with IANS Research.
____________________________________
2020 Cybersecurity and Cloud Security Podcasts
Cybersecurity for Executives in the Age of Cloud with Teri Radichel
Teri Radichel on Bring Your Own Security Podcast
Understanding What Cloud Security Means with Teri Radichel on The Secure Developer Podcast
2020 Cybersecurity and Cloud Security Conference Presentations
RSA 2020 ~ Serverless Attack Vectors
AWS Women in Tech Day 2020
Serverless Days Hamburg
Prior Podcasts and Presentations
RSA 2018 ~ Red Team vs. Blue Team on AWS with Kolby Allen
AWS re:Invent 2018 ~ RedTeam vs. Blue Team on AWS with Kolby Allen
Microsoft Build 2019 ~ DIY Security Assessment with SheHacksPurple
AWS re:Invent and AWS re:Inforce 2019 ~ Are you ready for a Cloud Pentest?
Masters of Data ~ Sumo Logic Podcast
Azure for Auditors ~ Presented to Seattle ISACA and IIA
OWASP AppSec Day 2019 — Melbourne, Australia
Bienvenue au congrès ISACA Québec 2019 — Keynote — Quebec, Canada (October 7–9)
Cloud Security and Cybersecurity Presentations
White Papers and Research Reports
Securing Serverless: What’s Different? What’s Not?
Create a Simple Fuzzer for Rest APIs
Improve Detection and Prevention of DOM XSS
Balancing Security and Innovation with Event-Driven Automation
Critical Controls that Could have Prevented the Target Breach
Packet Capture on AWS
Cybersecurity in a Cloudy World
52 
52 claps
52 
Cybersecurity in a Cloudy World
Written by
Cloud Security Training and Penetration Testing | GSE, GSEC, GCIH, GCIA, GCPM, GCCC, GREM, GPEN, GXPN | AWS Hero | Infragard | IANS Faculty | 2ndSightLab.com
Cybersecurity in a Cloudy World
"
https://medium.com/@alibaba-cloud/simplify-complexity-with-alibaba-clouds-network-infrastructure-services-cbf4fe9a79b2?source=search_post---------28,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 9, 2018·4 min read
The cloud is everywhere. We all have access to online software products whose nuts and bolts live in external data centers, even if it’s just a search engine that we use. Yet it seems like only a moment ago that the daily grind involved bulky hardware, tripping over wires, and waiting for the engineers to visit. IT networking was about cables and switches, boxes and towers, heavy lifting and manual restarts, and keeping server rooms and management cool.
Then cloud companies sprang up offering to virtualize our on-premises IT services and move them to the cloud, including our network infrastructure. Since then, networking in the cloud has made what was once a manual and often stressful business, a reasonably trivial user interface experience.
Cloud networking simplifies and economizes small to very large network infrastructures. It provides solutions for the most basic requirements to vast network architectures. Alibaba Cloud has everything you need to run your successful online business in the cloud. Alibaba Cloud offers Virtual Private Cloud (VPC) solutions that include flexible network and subnet setups, load balancing, VPN and the NAT Gateway, network security, dedicated support, and much more.
Computer networks connect people to people, people to services, and machines to machines. Like people, network servers have addresses that allow them to communicate with each other. Server IP addresses are numerical labels mapped to website addresses (URLs) by domain name system (DNS) servers.
Virtual cloud networks are the same, except the processing is done at enormous data centers around the world and the low-level details of how it happens are irrelevant to the end user, who may or may not be a network administrator.
Alibaba Cloud simplifies setting up and managing IT infrastructure in the cloud. What may have once taken months and thousands of hours and vast teams to develop and implement can now be achieved at your desk in a few days or even hours.
Let’s look at a simple example of an Alibaba Cloud VPC. The image below shows a small company’s VPC infrastructure. They are running a few ECS server instances in the cloud. These are Alibaba Cloud Elastic Compute Service (ECS) instances. ‘Elastic’ means the server can be reconfigured at any time and you can have your servers grow or diminish in memory or processing power whenever you like.
The user clicks through to a public Internet IP address and the request goes to the router. The network administrator has configured an Alibaba Cloud NAT Gateway on the router which translates (or NATs) public IPs to the private subnet IPs in the VPC and routes the request to the correct ECS instance. Every Alibaba Cloud VPC comes with one configurable router and you can add as many switches and servers as you need.
As your business and online traffic grows, you might want to build another Alibaba Cloud VPC in another region to help with load balancing. You can connect them with an Alibaba Cloud VPN Gateway. These provide secure and reliable transfer tunnels for traffic flowing between private networks. Alibaba Cloud VPN Gateway is an out-of-the-box service designed for easy configuration and immediate function.
You can also manage your peak throughput with Alibaba Cloud’s substantially stress-tested Server Load Balancer. A server load balancer ensures high availability to applications which may experience sudden spikes in traffic such as a busy e-commerce site might on the 11–11 Global Shopping Festival or Black Friday, for example.
The Alibaba Cloud Server Load Balancer guarantees up to 99.9% availability and redistributes traffic automatically across all available regions. It will auto-scale up or down, depending on the load. It monitors and detects unhealthy ECS instances and immediately reroutes the traffic to healthy instances. It also protects your network from SYN flood and DDoS attacks. Like your ECS instances, Alibaba Cloud offers a Pay-As-You-Go model for Server Load Balancer and is up to 60% more cost-effective when compared to traditional load balancers.
If you are worried about network reliability, Alibaba Cloud Server Load Balancer is built to cope with extreme levels of high volume traffic. Each year in November, the Alibaba Cloud Server Load Balancer is put to the test during Alibaba’s annual 11–11 Global Shopping Festival, now four times bigger than Black Friday in the U.S. Alibaba relies on Server Load Balancer to provide uninterrupted shopper access by switching requests between data centers and transferring transactions to healthy and available servers in regions located up to over 1,000 kilometers away.
In the case of server failure, the public IP address never changes and the user is protected from backend performance issues. Server Load Balancer also has a host of configuration and algorithm options making your network design simple and flexible.
Furthermore, Alibaba Cloud offers you the freedom to architect your own cloud solution that can be as big and as detailed as you wish. If you start small and your business grows, scaling up is a matter of just a few clicks inside the Alibaba Cloud user console.
Along with cloud networking products and services, Alibaba Cloud offers Domain Name System (DNS) servers, Content Delivery Networks (CDN) for speedy delivery of your images and caches, and web application firewall (WAF) for protecting your sites from online attacks.
Alibaba Cloud’s products don’t stop there either. Out-of-the-box Big Data processing solutions that maximize ROI on the data lakes created by your growing VPC networks are also available. E-MapReduce, MaxCompute and DataWorks are examples of Alibaba Cloud products that offer massive offline data processing, analysis, and mining capabilities.
So if you’re considering moving all or part of your IT network infrastructure to the cloud, and are looking for a simple and cost-effective solution with a wide range of Big Data possibilities as well as dedicated support, click here to explore more Alibaba Cloud products.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloud66/bash-bonanza-part-5-external-commands-3eaae59db091?source=search_post---------29,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud 66
Nov 28, 2017·5 min read
Welcome back, fellow bashstronauts!
Having previously discussed how arrays work with bash, I’d now like to discuss some subtleties of running external commands in Bash — that is, commands that are not Bash builtins.
This is a particularly important topic since a lot of the time, Bash is used as a sort of glue between processes — connecting or redirecting their outputs and inputs, running them in the background, e.t.c.
A fair warning before we start — Bash does not come with particularly rich process management functionality. If you need to do anything complicated, consider another programming language.
With that out of the way let’s begin!
Running External Commands
When running external commands in a bash script, you have two choices — running them in the foreground, or in the background:
Running them in the foreground involves simply writing out the command. This will then run to completion, and then continue with the next command.
These can be chained together with semicolons — then they will run in sequence.
Running them in the background involves writing out the command and appending an ampersand to the end, in which case it will immediately execute in the background and the bash script will continue to the next command without waiting.
These can be chained with ampersands, in which case they will be put one after another to execute in the background (concurrently).
Imposing Order
Our previous output was in a random order, since the processes run in parallel. To avoid sounding like Yoda, we may want to enforce some order on our echoes with the wait builtin.
When running an external command in bash, it is run as a separate process — more on that later. Each process has a process ID (PID) associated with it, which is, for all intents and purposes, random. There is a limited amount of them, so when once process completes, the process ID will be made available for other processes. This is commonly known as PID recycling.
The PID of the last command run in the background is held in the $! variable. The wait builtin can then take the PID of a process as an argument, in which case it will wait until that process runs to completion before continuing. In this case, it will also return the exit code of that process.
It can also be run without arguments, in which case it will wait for all background processes to complete.
Since Bash 4.3, you also have wait -n available, which will wait for any single job to complete before continuing - so if you have five processes running in the background, it will only wait for one of them.
Signalling Processes
Given a PID, you can send a signal to the process with the kill builtin. There are various signals (see info kill for a list), and they are usually used to terminate the process - by default, the TERM signal is sent. However, any program can overwrite what will be done when they receive the signal, so it is implementation dependent. For example, there is usually a signal that long-running daemons will accept to reload the configuration, or increase the log level.
A notable exception is the KILL signal, which can not be captured by the application, and is what the OS uses to kill the process immediately. This means that the application signalled will NOT have the opportunity to clean up and exit gracefully. Absolutely do not use the KILL signal if you can avoid it.
When you first start using the previously discussed tools, you will encounter some possibly counter-intuitive behaviour.
For example, try running the following script:
Running kill on that PID, and then checking ps aux | grep [s]leep, you may be surprised to see that sleep is still running!
To understand this behaviour, we need to understand how bash runs external commands. It will use the fork system call to create an almost exact copy of itself (called a child process), run the exec system call in the child (which will change the currently running command to the one specified - for example /bin/sleep), and then run the wait system call from the parent (the one we called the external command from) in order to wait for the child process to complete, and to get its exit code.
Note that when running a command in the background, it will simply not run the wait system call in the parent - meaning that we have to do so manually.
This means that the script and /bin/sleep are now two almost autonomous processes. If you send a signal to the script, bash will NOT propagate the signal to the child by default. If you want that to happen, you will have to do that yourself!
One important thing that makes the parent and child not completely autonomous is that the parent knows the child PID, and under UNIX, is guaranteed that the PID will not be recycled until the wait system call is called from the parent. This makes it safe to send signals from the parent to the child - you can be sure that it is not another process receiving it. This also makes it extremely dangerous to do so if you are not the parent - there is no guarantee the PID was not recycled between when you obtained it and when you send the signal. You should avoid doing that at all costs!
Closing thoughts
Here we covered how Bash runs external commands in a fair amount of detail, and covered some common gotchas. As usual, I hope these will help you avoid the common pitfalls and long debugging sessions!
Originally published at blog.cloud66.com on November 28, 2017.
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
19 
19 
19 
DevOps-as-a-Service to help developers build, deploy and maintain apps on any Cloud. Sign-up for a free trial by visting: www.cloud66.com
"
https://medium.com/@openbom/how-to-use-openbom-reference-properties-and-link-external-data-to-bill-of-materials-da0cc546c467?source=search_post---------30,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Jul 18, 2016·2 min read
The Bill of Materials (BOM) is an absolutely essential source of data helping you identify everything you need to build a product. Moreover, today’s manufacturing companies (especially small ones and individual makers) rely on external information to design and manufacture products.
A practical example that comes to mind is a reference to a component specification. This information is available online on websites such McMaster, Digikey, and others. How does one bring this information into a BOM? Glad you asked. openBoM helps make it easy with the use of what we call a ‘Reference Property’. The following video gives you an idea how Reference Properties work in openBoM.
This, by the way, is only a first step of what we can do with external references in openBoM. In a future release, we will provide the ability to reference or link to cloud data storage tools such as Dropbox, Google Doc, and Box. Moreover, openBoM will include the ability to link your BOM to your cloud CAD storages. Would this be useful to you?
CAD is moving to the cloud and thus allow better integration of data. Read more in my recent article on Beyond PLM. openBoM’s aim is to help you better integrate data in the changing manufacturing cloud data environment.
In a future post, I will share more information of how openBoM will help integrate data between it and cloud CAD systems. Stay tuned.
Meantime, you can learn more about what openBoM can do for you here http://tutorials.openbom.com.
Best,
Oleg @ openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
"
https://medium.com/@alibaba-cloud/flutter-analysis-and-practice-same-layer-external-texture-rendering-b29bfeed3610?source=search_post---------31,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 17, 2020·8 min read
In 2013, we were working on a group video call project and multi-channel video rendering was a major performance bottleneck. This was because the high-speed, on-screen operation of each channel (PresentRenderBuffer or SwapBuffer displaying the result of the rendering buffer on the screen) consumed a lot of CPU and GPU resources.
At the time, we separated the render and on-screen operations and abstracted multiple channels into a rendering tree. We then traversed the tree and performed rendering. After rendering was completed, the Vertical Synchronization (VSync) signal triggers on-screen operations all at once, instead of doing so one by one. This greatly reduces performance overhead.
We considered rendering the entire UI using OpenGL to further reduce the performance overhead of animations, such as sound spectrum and breathing effects. on the UI. However, due to various limitations, we did not put it into practice.
Figure 2–7 shows a simple Flutter rendering framework.
LayerTree is a tree data structure output by Dart Runtime. Each leaf node on the tree represents a UI element such as buttons or images.
Skia is a cross-platform rendering framework released by Google. It uses OpenGL to render at the backend. However, its Vulkan support is limited and does not support Metal at all.
Shell is a part of the platform, which includes implementations for iOS and Android, such as EAGLContext management, on-screen operations, and external texture implementations.
As shown in Figure 2–7, each leaf node on LayerTree is traversed in the pipeline after Dart Runtime completes the layout and outputs LayerTree. Each leaf node eventually calls the Skia engine and completes the rendering of UI elements. After the traversal is complete, glPresentRenderBuffer (iOS) or glSwapBuffer (Android) is called to complete the on-screen operation.
Based on this basic principle, Flutter implements UI separation on the native and Flutter engine, so developers do not need to care about platform implementation when writing UI code, making cross-platform apps possible.
While Flutter is separated from native, the Flutter engine and native are also completely separated. It is difficult for Flutter to obtain images that use a lot of memory from the native side, such as camera frames, video frames, and album images. React Native and Weex can directly obtain such data through NativeAPI. However, Flutter cannot directly obtain such data due to its basic principles. In essence, the channel mechanism defined by Flutter is a message transmission mechanism for transmitting data, such as images. This inevitably causes high memory usage and CPU utilization.
To solve the problem, Flutter provides a special mechanism: external textures, as shown in Figure 2–8.
As shown in Figure 2–8, each leaf node represents a widget written in Dart. The last node is the TextureLayer. This node corresponds to the texture widget in Flutter, which is not the same thing as GPU textures. When a texture widget is created in Flutter, it means the data this widget is displaying must be provided by native.
The process of rendering the TextureLayer node on iOS (similar to Android, with a slight difference in acquiring textures) is:
The key question is where the externaltexture object came from.
As shown by the code, before the native side calls RegisterExternalTexture, an object that implements FlutterTexture must be created. This object is eventually assigned to the external texture. externaltexture is a bridge between Flutter and native. You can use it to obtain image data throughout the rendering process.
As shown in Figure 2–9, the carrier of data transmitted by Flutter and native is PixelBuffer, and the data source (such as the camera and player) on the native side writes the data to PixelBuffer. Flutter acquires the data from PixelBuffer, converts it into the OpenGLES texture, and submits it to Skia for rendering.
Using this process, Flutter can easily render all the data required by the native side. In addition to dynamic image data from cameras and players, the image data from other image widgets can also be rendered. In particular, if the native side has a large image loading library, it takes time and effort to implement the same thing using Dart on the Flutter side. This process seems to be the perfect solution for displaying large amounts of data from the native side using Flutter. However, there are still many issues.
Figure 2–10 shows the flow of processing video and image data in a project. To improve performance, GPU is usually used on the native side while Flutter uses the copyPixelBuffer API, which means the data is transmitted from GPU to CPU and then to GPU. The CPU-to-GPU memory swap is the most time-consuming operation, which uses more time than processing the entire pipeline.
The Skia rendering engine requires GPU textures and the output of native data processing is precisely that. Can we use that text directly? The answer is yes, if EAGLContext resources are shared. The EAGLContext indicates the context used to manage the current GL environments and ensure resource separation in different environments.
Figure 2–11 shows the thread structure of Flutter.
In general, Flutter creates four runners. A runner, similar to the Grand Central Dispatch (GCD) on iOS, is a mechanism for running tasks in a queue. In most cases, a runner corresponds to a thread. The following runners are related to this article: the GPU runner, the I/O runner, and the platform runner.
An OpenGL-based app thread design has one thread for loading resources (images and textures) and one thread for rendering. However, to ensure that the textures created by the loading thread are available for the rendering thread, both threads share the same EAGLContext. However, this is not secure. If multiple threads access the same object with locking, that impacts performance. Improper code handling may even cause deadlocks. Therefore, Flutter uses another mechanism for EAGLContext: the two threads use their own EAGLContext and share texture data with each other through ShareGroup (shareContext for Android.)
A native module that uses OpenGL will also create its own context under its thread. To deliver the texture data created by the contexts to Flutter and submit it to Skia for rendering, it is necessary to expose its ShareGroup and save the ShareGroup on the native side when Flutter creates two internal contexts. ShareGroup is used to create contexts on the native side. This enables texture sharing between native and Flutter, as shown in Figure 2-12.
This method of using external_texture has two advantages:
The preceding topic has introduced the basic principles and optimization policies of Flutter external textures. You may wonder why Flutter still uses Pixelbuffer when external textures are so good. To use textures, you need to expose the ShareGroup. This is equivalent to opening the GL environment of Flutter. If the environment is isolated, deleteFrameBuffer does not affect objects in other environments when deleteTexture operations are performed. However, if the environment is opened, these operations may affect objects that are part of the Flutter context. Therefore, the framework designer must ensure the isolation and integrity of the framework.
During the development, Xianyu encountered a strange problem. It took a long time to locate the reason why glDeleteFrameBuffer is called when setCurrentContext is not set for the main thread. As a result, Flutter's FrameBuffer is deleted by mistake, causing Flutter crash during rendering. Therefore, to use this solution, try not to perform GL-related operations on the main thread on the native side, and add setCurrentContext before calling functions for the operations.
In addition, most logic in this article uses iOS implementation as examples. The overall principles of Android are the same, with slight differences in implementation methods. The external texture of Flutter on Android is implemented through SurfaceTexture by replicating data from the CPU to the GPU. OpenGL on Android uses ShareContext instead of ShareGroup to transmit the context. In addition, GL on Android at the Shell layer is implemented using C++. Therefore, the context is a C++ object. To share this C++ object with the Java Context object on the native side of Android, call it at the Java Native Interface (JNI) layer.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/simplify-complexity-with-alibaba-clouds-network-infrastructure-services-2f1274301990?source=search_post---------32,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 22, 2018·4 min read
The cloud is everywhere. We all have access to online software products whose nuts and bolts live in external data centers, even if it’s just a search engine that we use. Yet it seems like only a moment ago that the daily grind involved bulky hardware, tripping over wires, and waiting for the engineers to visit. IT networking was about cables and switches, boxes and towers, heavy lifting and manual restarts, and keeping server rooms and management cool.
Then cloud companies sprang up offering to virtualize our on-premises IT services and move them to the cloud, including our network infrastructure. Since then, networking in the cloud has made what was once a manual and often stressful business, a reasonably trivial user interface experience.
Cloud networking simplifies and economizes small to very large network infrastructures. It provides solutions for the most basic requirements to vast network architectures. Alibaba Cloud has everything you need to run your successful online business in the cloud. Alibaba Cloud offers Virtual Private Cloud (VPC) solutions that include flexible network and subnet setups, load balancing, VPN and the NAT Gateway, network security, dedicated support, and much more.
Computer networks connect people to people, people to services, and machines to machines. Like people, network servers have addresses that allow them to communicate with each other. Server IP addresses are numerical labels mapped to website addresses (URLs) by domain name system (DNS) servers.
Virtual cloud networks are the same, except the processing is done at enormous data centers around the world and the low-level details of how it happens are irrelevant to the end user, who may or may not be a network administrator.
Alibaba Cloud simplifies setting up and managing IT infrastructure in the cloud. What may have once taken months and thousands of hours and vast teams to develop and implement can now be achieved at your desk in a few days or even hours.
Let’s look at a simple example of an Alibaba Cloud VPC. The image below shows a small company’s VPC infrastructure. They are running a few ECS server instances in the cloud. These are Alibaba Cloud Elastic Compute Service (ECS)instances. ‘Elastic’ means the server can be reconfigured at any time and you can have your servers grow or diminish in memory or processing power whenever you like.
The user clicks through to a public Internet IP address and the request goes to the router. The network administrator has configured an Alibaba Cloud NAT Gateway on the router which translates (or NATs) public IPs to the private subnet IPs in the VPC and routes the request to the correct ECS instance. Every Alibaba Cloud VPC comes with one configurable router and you can add as many switches and servers as you need.
As your business and online traffic grows, you might want to build another Alibaba Cloud VPC in another region to help with load balancing. You can connect them with an Alibaba Cloud VPN Gateway. These provide secure and reliable transfer tunnels for traffic flowing between private networks. Alibaba Cloud VPN Gateway is an out-of-the-box service designed for easy configuration and immediate function.
You can also manage your peak throughput with Alibaba Cloud’s substantially stress-tested Server Load Balancer. A server load balancer ensures high availability to applications which may experience sudden spikes in traffic such as a busy e-commerce site might on the 11–11 Global Shopping Festival or Black Friday, for example.
The Alibaba Cloud Server Load Balancer guarantees up to 99.9% availability and redistributes traffic automatically across all available regions. It will auto-scale up or down, depending on the load. It monitors and detects unhealthy ECS instances and immediately reroutes the traffic to healthy instances. It also protects your network from SYN flood and DDoS attacks. Like your ECS instances, Alibaba Cloud offers a Pay-As-You-Go model for Server Load Balancer and is up to 60% more cost-effective when compared to traditional load balancers.
If you are worried about network reliability, Alibaba Cloud Server Load Balancer is built to cope with extreme levels of high volume traffic. Each year in November, the Alibaba Cloud Server Load Balancer is put to the test during Alibaba’s annual 11–11 Global Shopping Festival, now four times bigger than Black Friday in the U.S. Alibaba relies on Server Load Balancer to provide uninterrupted shopper access by switching requests between data centers and transferring transactions to healthy and available servers in regions located up to over 1,000 kilometers away.
In the case of server failure, the public IP address never changes and the user is protected from backend performance issues. Server Load Balancer also has a host of configuration and algorithm options making your network design simple and flexible.
Furthermore, Alibaba Cloud offers you the freedom to architect your own cloud solution that can be as big and as detailed as you wish. If you start small and your business grows, scaling up is a matter of just a few clicks inside the Alibaba Cloud user console.
Along with cloud networking products and services, Alibaba Cloud offers Domain Name System (DNS) servers, Content Delivery Network (CDN) for speedy delivery of your images and caches, and web application firewall (WAF) for protecting your sites from online attacks.
Alibaba Cloud’s products don’t stop there either. Out-of-the-box Big Data processing solutions that maximize ROI on the data lakes created by your growing VPC networks are also available. E-MapReduce, MaxCompute and DataWorks are examples of Alibaba Cloud products that offer massive offline data processing, analysis, and mining capabilities.
So if you’re considering moving all or part of your IT network infrastructure to the cloud, and are looking for a simple and cost-effective solution with a wide range of Big Data possibilities as well as dedicated support, click here to explore more Alibaba Cloud products.
Reference:
https://www.alibabacloud.com/blog/Simplify-Complexity-with-Alibaba-Cloud%E2%80%99s-Network-Infrastructure-Services_p540787?spm=a2c41.11318760.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/pouchcontainer-engineering-quality-practice-edffa2296e56?source=search_post---------33,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 20, 2018·8 min read
As PouchContainer keeps iterating and improving functions, the project scale grows larger, attracting many external developers for project participation. Because coding habits vary among contributors, code reviewers shall pay attention to the coding style in addition to logic correctness and performance, because consistent code specification is a premise for keeping project code maintainable. In addition to a consistent coding style, the coverage rate and stability of test cases are also the project focus. How can we ensure each code update has zero impact on existing functions in a project without regression test items?
This article shares PouchContainer practices in coding style specification and Golang unit test cases.
PouchContainer is a project constructed using Golang. It uses shell scripts to complete automatic operations such as compiling and packaging. In addition to Golang and shell scripts, PouchContainer includes many Markdown documents to help users understand PouchContainer. The standard typography and correct spelling of the documents are the focus of projects. The following describes the tools and use cases of PouchContainer in terms of coding style specification.
Golang has simple syntax, and the complete CodeReview guide of the community from the start helps achieve a consistent coding style across many Golang projects and minimize disputes. Based on the conventions in the developer community, PouchContainer defines specific rules for developers to follow, so as to ensure code readability. For more information, read the code style rules.
However, it is difficult to keep a consistent coding style for projects solely based on written specification. Similar to other programming languages, Golang provides basic tool chains such as golint, gofmt, goimports, and go vet used to check and unify the coding style, making it possible to automate code review and subsequent processes. Currently, PouchContainer runs the preceding code check tools in CircleCI to check every pull request submitted by developers. If an error is returned, the code reviewer can reject review and code merge.
In addition to the tools provided by Golang, we can select third-party code check tools such as errcheck in open source communities to check whether developers have handled the errors returned by functions. However, these tools lack a consistent output format, making it difficult to normalize the outputs of different tools. Open source communities provide gometalinter to normalize various code check tools. The following combination is recommended:
A gometalinter package can be tailored to specific projects.
Despite powerful functions, shell scripts require syntax check to avoid potential and unpredictable errors. For example, unused variables may be defined. Though such variables do not affect the use of scripts, they may be a burden on project maintainers.
PouchContainer uses shellcheck to check the shell scripts of the current project. Take the preceding code as an example, shellcheck generates an alarm about unused variables. The shellcheck tool can identify the potential problems of shell scripts during code review to reduce the error probability during execution.
The current continuous integration task of PouchContainer scans the .sh scripts of the project and uses shellcheck to check the scripts one by one. For more information, read the shellcheck documentation.
Note: When shellcheck is needlessly rigorous, you can add comments to the project to disable checking in expected places, or disable a check item. For specific check rules, check this wiki.
As an open source project, PouchContainer attaches equal importance to documents and code, because documents are the optimal way users can understand PouchContainer. Documents are prepared using Markdown, and their typography and spelling are the project focus.
Written specification is not enough to avoid false negatives in document checking, just like in the case of code checking. Therefore, PouchContainer uses markdownlint and misspell to check the typography and spelling of documents. Such checking is as important as golint and is performed on each pull request in CircleCI. If an error is returned, the code reviewer can reject review or code merge.
The current continuous integration task of PouchContainer checks the typography and spelling of the Markdown documents in a project. For configuration details, read here.
Note: When markdownlint is needlessly rigorous, you can disable check items in the project. For specific check items, read the markdownlint documentation.
A unit test ensures the correctness of a single module. In a test pyramid, a unit test with wider coverage of more functions is more likely to reduce the debugging costs of integration testing and end-to-end testing. In a complex system, a longer link of task processing results in a higher cost of problem locating, especially problems caused by minor modules. The following lists the conclusions on how to compile Golang unit test cases in PouchContainer.
Simply put, a unit test is intended to determine whether the output of a function meets expectations based on a given function input. When a tested function has various input scenarios, we can organize test cases in Table-Driven mode. See the following code. Table-Driven uses arrays to organize test cases, and verify the correctness of functions by means of cyclic execution.
To debug and maintain test cases with ease, we can add auxiliary information to describe the current test. For example, when reference tests the input of punycode without adding punycode, the code reviewer or project maintainer may not know the differences between xn--bcher-kva.tld/redis:3 and docker.io/library/redis:3.
For a function with complex behaviors, one input is not enough for executing a complete test case. In TestTeeReader, for example, data reading is complete after TeeReader reads hello, world from the buffer, and further reading is expected to encounter an ""end-of-file"" error. Such a test case must be executed independently rather than using Table-Driven.
Simply put, if you copy a large portion of code when testing a function, in principle, the expected test code can be fully extracted and used to organized test cases in Table-Driven mode. Be sure to follow the “Don’t Repeat Yourself” rule.
Note: Table-Driven is recommended by the Golang community. For more information, this wiki.
Dependencies are frequently encountered during testing. For example, a PouchContainer client requires an HTTP server. However, such dependencies exceed the processing capability of units and fall in the integration test scope. How can we complete these unit tests?
In Golang, interface implementation belongs to the Duck Type. An interface can be implemented in various modes, provided that the implementation complies with the interface definition. If external dependencies are subject to interface-based constraints, such dependency behaviors are simulated in unit testing. The following describes two common test scenarios.
2.2.1 RoundTripper
PouchContainer client testing is used as an example. The PouchContainer client uses http.Client. http.Client uses the RoundTripper interface to execute an HTTP request. With this interface, developers can customize the logic of sending HTTP requests, which is the major reason why Golang provides full support for HTTP 2 on the original basis.
For the PouchContainer client, the test mainly verifies the input destination address and the query, and determines whether results are returned normally. Therefore, before testing, developers must prepare corresponding RoundTripper, which only determines whether the input meets expectations, but does not implement the actual service logic.
In the following code, PouchContainer newMockClient can receive custom request processing logic. In a case which tests image deletion, the developer configures the custom logic to verify the destination address and determine whether the HTTP method is DELETE. In this way, the expected functional test can be completed without starting the HTTP server.
2.2.2 MockImageManager
For the dependency between internal packages, such as the dependency of PouchContainer Image API Bridge on PouchContainer Daemon ImageManager, the dependency behavior is subject to interface-based constraints. To test the logic of Image Bridge, similar to RoundTripper, we only need to execute corresponding Daemon ImageManager, without having to start containerd.
2.2.3 Differences between ImageManager and RoundTripper
ImageManager and RoundTripper use the same simulation method, but the number of interface-defined functions varies between them. Normally, developers can manually define a structure that take methods as fields, as shown in the following code.
When interfaces are relatively large and complex, manual operation becomes a burden for developers during testing. To address this issue, the community provides automatic generation tools such as mockery to reduce the burden of developers.
In some cases, third-party services are the subject of dependency. For example, the PouchContainer client represents a typical case. Such testing can be completed using Duck Type. We can also register http.Handler and start mockHTTPServer to process requests. The preceding test method is cumbersome, and recommended for use only when testing cannot be completed by Duck Type, or it can be used in integration testing.
Note: In the Golang community, some attempts are made to complete monkeypatch by modifying binary code. We do not recommend using monkeypatch, and recommend that developers design and compile testable code.
Code style checking, unit testing, and integration testing must be performed by means of continuous integration during code review to help reviewers make accurate decisions. Currently, PouchContainer uses TravisCI or CircleCI and pouchrobot for code style checking and testing.
Reference:
https://www.alibabacloud.com/blog/pouchcontainer-engineering-quality-practice_593906?spm=a2c41.11896382.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@alibaba-cloud/best-practices-for-building-secure-global-networks-internal-and-external-10aab7e6f123?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 20, 2020·4 min read
By Alibaba Cloud Network
One of the major challenges to cybersecurity comes from access between networks in different regions. When connecting Alibaba Cloud network products in different regions, Cloud Enterprise Network (CEN) serves as a secure, global private network that provides high performance and low latency within Alibaba Cloud. By using CEN, you can establish private network connections between Virtual Private Cloud (VPC) networks in different regions, or between VPC networks and on-premises data centers. CEN supports automatic route distribution and learning, which speeds up network convergence, improves the quality and security of cross-network communications, and interconnects all network resources. With these benefits, CEN can help you build an extended enterprise-level network with cross-network communication capabilities. As the basic component for enterprise connectivity, CEN provides outstanding security. By using typical access control policies combined with cloud services such as Cloud Firewall and PrivateZone, CEN provides enterprises with comprehensive security protection.
As a cloud network, CEN first builds channels for private network intercommunication. Cloud enterprise networks built through CEN are fully private networks that do not need to expose public network entries. This significantly reduces their vulnerability to attacks from public networks, greatly decreasing security risks. You can define strict access control policies and customize rules to permit or deny specific traffic flows. Then, you can apply these access control policies to instances to achieve trusted communication. By implementing routing policies, you can filter route information and modify route attributes. This allows you to define cloud network intercommunication capabilities and configure a wide range of route control capabilities.
CEN access links support encrypted transmission to minimize the risks posed by intermediate links. The cloud network uses Smart Access Gateway (SAG) and establishes private encrypted channels between Alibaba Cloud access points. By rigorously preventing replay attacks and periodically updating keys, this ensures that user traffic is not tampered with or listened to on public network transmission paths. Cloud Firewall allows you to implement access control, traffic analysis, and post-event auditing in scenarios that require intercommunication over public networks and cross-VPC access.
PrivateZone is a private DNS resolution and management service based on Alibaba Cloud VPC environments. By accessing PrivateZone through CEN, you can prevent your business DNS from being exposed to a public network. This helps prevent DNS hijacking and domain name pollution.
Customers in any region around the world can access the same services over the Internet, but the access quality and experience vary greatly from one customer to another. Access links on the Internet are uncontrollable. Access requests hop between multiple nodes to reach the destination server, the intermediate nodes are not under control, and the request and response paths may be different. Each node a request passes through can affect performance by introducing delays or jitters. These nodes can also affect service quality due to congestion and packet loss.
To solve these problems, Alibaba Cloud’s Apsara Cloud Network Management launched Global Accelerator (GA), which is a global network acceleration service. Based on Alibaba’s high-quality BGP bandwidth and global network, this service can direct user traffic to nearby acceleration nodes and deploy applications across regions. This can reduce the negative impact of network issues such as latency, jitter, and packet loss on service quality, providing global users with a high-availability and high-performance network acceleration service.
GA integrates scheduling and acceleration capabilities to provide high-quality, high-performance, high-availability, secure, reliable, and easy-to-deploy network acceleration services. The integration of Anti-DDoS Pro and Web Application Firewall (WAF) provides tiered security protection for enterprise applications.
GA provides free protection against DDoS attacks with a rate of up to 2 to 5 Gbit/s. Linked with Anti-DDoS Pro, GA can defend against attacks from hundreds of Gbit/s. Requests from terminals are cleansed of DDoS traffic before entering the acceleration network, ensuring the continuous availability of Internet application services. This provides a highly secure cross-region acceleration solution for global mobile Internet service providers.
For web applications, GA integrates WAF. Linked with WAF, GA provides a highly secure cross-region acceleration solution for global web application providers based on cloud security and big data capabilities.
Finding ways to empower government and enterprise users with intelligent network capabilities will be a key strategy and breakthrough point for future network development. After cloud migration, users must adopt completely new network construction services and operation methods. This means many existing network service tools and systems need to be completely reconstructed. This also represents many new opportunities for the networking industry. Improving efficiency by using intelligent network services will be a major future trend. Given the ongoing development of its digital economy, China is gradually establishing its position as the center of the global digital economy. Whether Chinese enterprises go overseas or multinational enterprises run business in China, networks will become the basic infrastructure that connects branches inside and outside the country. This will make networks the factor that most directly affects productivity. For the Alibaba Cloud Network team, our ultimate mission is to simplify the networks.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@IBMDeveloper/ibm-cloud-hyper-protect-services-protect-your-organization-from-internal-and-external-threats-d7af9af88a18?source=search_post---------35,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Jun 14, 2019·1 min read
As a developer, you probably understand how important data security is — and this holds true whether you are a founder of the next great tech startup or part of a large enterprise team. Barely a month goes by without a high-profile story in the news about a data breach at a major company, and those are just the ones that were discovered and worth reporting on.
Regardless of company size, data protection is as relevant now as it’s ever been. More recently, we’ve even heard of creative compromises where organizations believed their sensitive data was secure, but since they didn’t secure data they didn’t believe was sensitive, they found themselves vulnerable to attack. Even beyond specific incidents, many organizations are bound by compliance requirements (such as PCI DSS, GDPR, and HIPAA). As more and more countries evaluate and implement their own requirements, conversations around keeping sensitive data secure continue to evolve. Why rely on policy when you can rely on technology?
You can read the full blog post on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://medium.com/@IBMDeveloper/ibm-cloud-hyper-protect-services-protect-your-organization-from-internal-and-external-threats-7cf9f00216c0?source=search_post---------36,"Sign in
There are currently no responses for this story.
Be the first to respond.
IBM Developer
Jun 14, 2019·2 min read
As a developer, you probably understand how important data security is — and this holds true whether you are a founder of the next great tech startup or part of a large enterprise team. Barely a month goes by without a high-profile story in the news about a data breach at a major company, and those are just the ones that were discovered and worth reporting on.
Regardless of company size, data protection is as relevant now as it’s ever been. More recently, we’ve even heard of creative compromises where organizations believed their sensitive data was secure, but since they didn’t secure data they didn’t believe was sensitive, they found themselves vulnerable to attack. Even beyond specific incidents, many organizations are bound by compliance requirements (such as PCI DSS, GDPR, and HIPAA). As more and more countries evaluate and implement their own requirements, conversations around keeping sensitive data secure continue to evolve. Why rely on policy when you can rely on technology?
This is where a solution that provides data-at-rest and data-in-flight protection can help developers easily build applications with highly sensitive data. To meet this need, IBM Cloud offers a suite of services collectively known as IBM Cloud Hyper Protect Services, which are powered by LinuxONE. These services give users complete authority over sensitive data and associated workloads (even cloud admins have no access!) while providing unmatched scale and performance; this allows customers to build mission-critical applications that require a quick time to market and rapid expansion.
You can read the full blog post on IBM Developer.
Originally published at https://developer.ibm.com.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
Open source, code patterns, tutorials, meet-ups, challenges, mentoring, and a global coding community — all in one place.
"
https://itnext.io/building-an-arm-kubernetes-cluster-ef31032636f9?source=search_post---------37,"You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
Since I released this article, I had more than 30k views and a lot of feedback. I recently wrote an updated article deploying Kubernetes 1.13 and some new additions. Please check out https://medium.com/@carlosedp/building-a-hybrid-x86-64-and-arm-kubernetes-cluster-e7f94ff6e51d and use this one as a reference for more details.
In this guide I wanted to share my experience on building a complete Kubernetes Cluster (with LoadBalancer, Ingress and external storage) with SBC’s (Single Board Computers like the Raspberry Pi and similar) using the Rock64 boards and the history behind it. There are a couple of lessons learned that might be of interest to have a fully integrated environment avoiding some problems I had on the way.
After I started playing with my first SBC, a Raspberry Pi 3, I deployed a full media server stack composed of Transmission, SickRage, CouchPotato, Plex and HTPCmanager(based on nice packages from HTPCGuides). After it worked, I started converting all those applications to Docker containers and automated the deployment using docker-compose files. Later I deployed a complete monitoring solution and management composed of Portainer, cAdvisor, Prometheus, Grafana and even Traefik to expose some apps to the internet as an ingress and HTTPS front-end (with Letsencrypt certificates).
I learned a lot on this process where I had to build many images to run these apps, compose the deployment and even setup a CI on Travis to automatically cross-build Docker images for AMD64/ARM32/ARM64.
But it didn’t stop there, after reading lots of articles on building home clusters with these the Pi, I absolutely wanted one. But a more powerful one.
I ended up choosing the Rock64 SBC from Pine64, a quad-core A53 ARM board that can run 64bit Linux, 4GB RAM and a eMMC connector to use this kind of memory instead of the slow-as-molasses SDCards used on the Pi. Here is my shopping list:
And it all arrived, ready for a weekend of fun.
As I was already planning the cluster, I did a brief IP Plan for my home network. This is important to set the DHCP ranges you use for clients, the LoadBalancer ranges and the fixed IPs.
At first, I downloaded the Debian Stretch image from Ayufan’s repository. He is the guy that maintains the most up-to-date Linux builds for these boards. I went with latest version (0.6.20 at the moment of writing) that was marked as a pre-release version (it crashed, more on this later).
Very straightforward, I flashed the image to the eMMC module using Etcher, on the eMMC-USB adapter, popped the card into the board, connected it thru the Serial-USB adapter (GND-GND, Rx-Tx, Tx-Rx) to my computer and set a serial console (Putty on Windows or screen on Mac/Linux) to the correct COM port and speed of 1500000. You can also connect an HDMI monitor and keyboard for instead of serial.
I went thru all basic configurations for the boards, hostnames, IPs, install Docker/Kubernetes utilities and etc (check the links to the gist and script of steps I used).
At first, it all went perfect but after installing Kubernetes and MetalLB (more on this later), I got a kernel crash instantly. Trying again with a previous 0.6.15 build but now with Ubuntu Xenial got me the same problem. Issue filled.
Avoiding being edgy, I downloaded the last Stable build, 0.5.15 for Debian Strech and all went fine so stick to this version for time being. After some time, I had the boards assembled and installed.
For the internal DNS server I first thought about running Bind on a container in the infrastructure but I found a “design decision” on Kubernetes that avoids having TCP and UDP ports in the same Service (DNS uses TCP and UDP on port 53). This might be mitigated by MetalLB in the future.
In the end, I decided to go to a more reliable way and flashed DD-WRT into my Netgear R7000P router to be able to configure it’s internal dnsmasq daemon. Easy task, all configured and just one line added to DD-WRT. Now my router is the internal DNS server and cache on my network.
In case you can’t deploy an internal DNS, map all domains that will be used for your applications on your computer “hosts” file pointing to the Traefik Ingress IP configured.
I followed the official kubeam guide:
This will give you a pretty functional Kubernetes Cluster with an overlay network that allows all pods on all nodes to communicate to each other but I added a few other modules to have a more complete cluster.
To follow the next tasks, it’s easier to clone my repo from GitHub.
In case you need to reset your deployment and start over (happened for me), you need to clean every rule kubernetes and Weave added to your nodes. I made a gist with the steps needed. Follow this to avoid your pods not talking to each other after a redeployment.
MetalLB is a Kubernetes load balancer that integrates directly to the API. This way you can request a Service with type: LoadBalancer and your service will have an IP allocated from a pool configured in MetalLB. Check their website for more info.
For installation and configuration, run the script install_metallb.sh and deploy the manifest kubectl apply -f metallb-conf.yaml in from 1-MetalLB dir. Adjust the IP CIDR on the manifest for your network.
Traefik is an ingress controller that can act as a reverse proxy/loadbalancer on the service layer of Kubernetes.
With Traefik, any application deployment can be exposed with it’s services on a subdomain on the network. Traefik was deployed itself with a LoadBalancer service type and a fixed IP. This IP was mapped as a wildcard on my internal DNS so all calls to *.internal.mydomain.com goes to this IP and Traefik forward to the correct pod based on the ingress rule. Use hosts file in case you don’t have a DNS. Ex:
There are two deployments of Traefik, one for internal ingress (on domain *.internal.mydomain.com) and one for external ingress (on domain *.cloud.mydomain.com). These deployments have two different IPs assigned by the load balancer. The separation is due to the external Traefik handles certificate generation on Letsencrypt.
For this you need to point your HTTP and HTTPS ports to the loadbalancer IP assigned on external Traefik using the PortForward feature on your router.
Adjust the config files on the 2-Traefik dir (mostly the configmaps and the LoadbalancerIP on services)to suit your domain and IPs and deploy all with kubectl apply -f traefik* If you will not use external access and don't need certificates for it, don't deploy the external-traefik* manifests.
To have data persistence on your applications, you need to create Persistent Volumes on Kubernetes. The most flexible way of getting this is having a StorageClass allowing your deployments to request volumes to store the data.
I went with the option of having an external disk connected via USB3 to one of my nodes (I chose kubenode1) and shared the volume over NFS on the node. Format and mount your external drive and install the NFS server(adjust your mount path and network):
Then, I used nfs-client-provisioner to create the StorageClass on Kubernetes pointing to the NFS server using the files on 3-NFS_Storage (adjust the IP and mountpoint on deployment-arm.yaml).
Check if your StorageClass is the default:
If not, use the command
Heapster and InfluxDB collects some statistics from your cluster and show them on the web GUI. The Dashboard allows the management of the cluster using a web console.
Install the manifests from 4-Dashboard and 5-Heapster-Influx directories. There is an option to deploy the dashboard externally with HTTPS and authentication, this depends on the external Traefik. Generate your authentication with the generate_auth.sh and apply the external-ingress.yaml manifest.
There you have a nice dashboard to manage your pods.
Since deploying the cluster, I migrated the solution to a more elaborate version using Prometheus-operator.
I made a new article for this with detailed explanation, please follow that guide as I will keep the information below only for historic matter:
https://medium.com/@carlosedp/creating-a-full-monitoring-solution-for-arm-kubernetes-cluster-53b3671186cb
As an option, you can deploy a nice monitoring stack going with Prometheus, Grafana and their supporting applications. I advise this is kinda heavy on resources but the visuals and customization are great. Deploy all manifests from the 6-Monitoring directory. Remember to adjust the ingress domains for grafana/ingress.yaml and prometheus/ingress.yaml.
After this, you need to configure the Prometheus datasource in Grafana (add data source with URL http://prometheus:9090) and import the dashboards from the dashboards dir in grafana.
Access it on your ingress domain: http://grafana.internal.mydomain.com.
From here, you have a fully functional and featured cluster ready to deploy your applications or play with different stacks. OpenFaaS, Helm and many more options all in your hands. Just remember that you need to find (or build) ARM32 or ARM64 images to deploy on this cluster.
All configuration scripts and manifests used to install are hosted in https://github.com/carlosedp/kubernetes-arm.
To keep up on news or send suggestions, follow me on Twitter @carlosedp
ITNEXT is a platform for IT developers & software engineers…
1K 
9
1K claps
1K 
9
Written by
Writing everything cloud and all the tech behind it. If you like my projects and would like to support me, check my Patreon on https://www.patreon.com/carlosedp
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Writing everything cloud and all the tech behind it. If you like my projects and would like to support me, check my Patreon on https://www.patreon.com/carlosedp
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://itnext.io/use-helm-to-deploy-openvpn-in-kubernetes-to-access-pods-and-services-217dec344f13?source=search_post---------38,"You have 1 free member-only story left this month. Sign up for Medium and get an extra one
During development it could be useful to access your applications (pods) inside your Kubernetes cluster without creating an external (public) endpoint. This can be achieved by using OpenVPN. We will use the Kubernetes package manager Helm to install OpenVPN inside our Kubernetes environment.
This tutorial was performed on Google Cloud which offers $300 free credit to get started with any GCP product.
I’ll explain how to set up Kubernetes on GCP. If you have already a full working Kubernetes cluster you can skip these steps.
The official Google Cloud documentation explains how to create a more advanced cluster than I’m explaining below, but this is out of scope for now.
Wait a few minutes till the cluster is up and running! Then click “Connect” to get the command which is necessary to configure kubectl CLI access.
Now we can already check the IP ranges. You are probably going to see different values.
Let’s keep those values in mind.Now we will configure Kubernetes dashboard access.
We will create a ServiceAccount as workaround to access the dashboard. This is explained in great detail in this blog. It’s a slight abuse of the ServiceAccount mechanism.
Now we can visit the dashboard on http://127.0.0.1:8001/ui.Paste the token which you copied before:
Access the Kubernetes dashboard:
We will deploy a very basic sample app in a namespace called ‘sample-app’. The deployment will contain 2 pods which run a web server (Nginx). We will create a Service above which will function as a load balancer above the pods. Pods can go down and up again which can lead to different pod IP’s while Services are immortal and a Service IP will never change.
There are 3 important types of services:
We do not want to make our Service publicly available. We will create a Service of type ClusterIP. The ClusterIP will be in the range of the servicesIpv4Cidr.
Verify by checking the dashboard:
Check the Service. There is no external endpoint (public IP) and it’s created inside the expected range.
How can we visit our pods without defining an external endpoint (public IP)? Here for we will use OpenVPN. We will install OpenVPN inside our cluster using Helm and connect with it. It will route all network traffic to kubernetes pods and services through the VPN. This will make it possible for us to access the ClusterIP of our Nginx Service inside our browser!
We will prepare persistent storage for our OpenVPN pod because we don’t want to loose data if the pod is scaled down and up again. New certificates are generated with each deployment. If persistence is enabled certificate data will be persisted across pod restarts. Otherwise new client certs will be needed after each deployment or pod restart.
I’ll prepare the storage in the Google Cloud Platform. Open a new terminal:
Create our Kubernetes namespace in which we will deploy the OpenVPN pod.
Write a template for the persistent volume claim: openvpn-pv-claim.yaml. This will claim the Google Cloud storage. The claim is called openvpn-data-claim.
Create the claim:
We can verify in our dashboard. Don’t forget to choose the right namespace.
Helm is the package manager for Kubernetes. It has two parts: a client ( helm ) and a server ( tiller ) Tiller runs inside your Kubernetes cluster.
Now we will use the Helm Chart to install OpenVPN. Here for we need to write a values.yaml which contains our configuration. It’s based on the values.yaml from the GitHub repository.
Add the following values. Define your OVPN_K8S_POD_NETWORK:
Start the Helm install. We will point to our values.yaml and we reference the chart stable/openvpn.
This can take some time because the Docker image needs to be pulled during the deployment.
Access the dashboard to check if the OpenVPN pod is up and running:
After the helm install some commands will pop up. Those commands need to be executed to create the OpenVPN certificates. Verify by printing the content of the environment variables.
If the output of the variables contains the right values, you can start creating the kubeVPN.ovpn.
A kubeVPN.ovpn file is created!
The file is also stored inside the OpenVPN pod. This pod is persistent so there should be no fear to loose the kubeVPN.ovpn.Before we’re going to install and configure our VPN client we will stop the kubectl proxy command. Execute it again after you’ve connected successfully with the VPN to access your dashboard.
Now we need to install a VPN client. Windows users can install the OpenVPN client. Tunnelblick is a good option for OSX users. Drag the .ovpn file inside Tunneblick and click connect:
Now we’re connected.Let’s check the output of ifconfig .
I have received an IP between the range which we defined inside the values.yaml during the helm install.
Let’s go back to the Nginx Service we’ve created in the sample app:
Copy the ClusterIP and paste it in your browser. Now you’re ‘load balanced’ between the two pods.
You can also access your pod without using the Service but by connecting to the pod IP.
It works! We can visit a service of type ClusterIP inside our browser. Remember the explanation about this type of Service: “Choosing this value (ClusterIP) makes the service only reachable from within the cluster”.
By using OpenVPN we are able to visit the Service in our browser without using an external endpoint! We can even ignore the Service and visit our pods directly. When you disconnect from the VPN it won’t be possible to visit the Service or pods anymore.
Hope you enjoyed it!
ITNEXT is a platform for IT developers & software engineers…
355 
6
355 claps
355 
6
Written by
AWS Community Builder | DevOps | Docker Certified Associate | 4x AWS Certified | CKA Certified | https://lvthillo.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
AWS Community Builder | DevOps | Docker Certified Associate | 4x AWS Certified | CKA Certified | https://lvthillo.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/google-cloud/istio-grpc-loadbalancing-with-gcp-internal-loadbalancer-ilb-1b058b517640?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
A couple weeks back I started looking at how to setup and expose an Istio service on GKE through a GCP Internal (and external) LoadBalancer. I did work a fair amount on gRPC with GKE (example at the end of this article), and my experience so far with Isito was just HTTP-based traffic. Coincidently, a different customer asked about how to setup a generic GCP-GKE Internal LoadBalancer for ingress traffic with Istio. After i got that bit working, i started tackling how to augment that with gRPC.
This article covers how to setup Istio on GKE, then expose an Internal (and External) LoadBalancer for gRPC traffic.
After setting this up, the gRPC traffic was (expectedly) automatically LoadBalanced _between_ pods in cluster. I expected this but to see Istio automatically loadbalance individual RPCs sent via one channel was really nice.
I’ve documented the steps here:
github.com
Anyway, lets get started (you can follow the git link above or to repeat here inline:
and most critically, set
(its not enabled by default on Istio 1.0.5 atleast)
What that’ll do is just give you the external and internalLB IP addresses (note them down; you’ll also find it on the GCP console as above)
The source code for the sample application is in the apps/ folder for this repo. Thats a simple app that creates one gRPC Channel to the server and on that one connection, sends 10 RPC requests.
The image you’re deploying is salrashid123/grpc_backend…you ofcourse don’t have to deploy that app as-is; you can look at the source then build and upload to your own repo!
Now that w’ere setup, lets test
You should see responses from different service addresses:
Each response shows the hostname/pod that handled the request. In this example, the responses come from different pods, round-robin (gRPC Loadbalancing!)
Ok, now we need to verify that we can connect via internal LB.
First create a GCP VM within the same VPC. SSH in and run.
Once in the VM, install Docker and run the following (remember to set the environment variable for $ILB_GATEWAY_IP
You should see responses from different service addresses:
If you deployed and setup Kiali, you should see traffic inbound from the ILB and external gateway:
So..thare you have it, gRPC loadbalacing from external and internal traffic to a service inside Istio. If you’re interested in generic GKE gRPC loadbalancing setups, please see the examples below.
Appendix
medium.com
medium.com
medium.com
medium.com
Google Cloud community articles and blogs
51 
51 claps
51 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/kubernetes-dns-proxy-with-services-d7d9e800c329?source=search_post---------40,"There are currently no responses for this story.
Be the first to respond.
When building an application it is common that you’ll need to interact with external services to complete your business goals. When working within a Kubernetes cluster you may be enjoying using internal service routing to have one of your services communicate with another one of your services. If you want to use the Kubernetes DNS server to mask external services so they look like internal services then you’re in luck — it is possible! This article will go through how to use the Kubernetes DNS Server to mask external services to look like internal services.
If you haven’t gone through or even read the first part of this series you might be lost, have questions where the code is, or what was done previously. Remember this assumes you’re using GCP and GKE. I will always provide the code and how to test the code is working as intended.
medium.com
There are many reasons you may want to use your Kubernetes DNS server to mask external services. One example is around changing endpoints. You may know that a service endpoint will change and rather than have your developers changing the endpoint in code we can have the developers point to an easy to use internal service endpoint no matter what is really happening.
Now you can change your external service endpoint at any time through Kubernetes YAML configuration files rather than going into your application code.
To create an External Name Proxy is super simple. Most of the work we need to do is in the Kubernetes yaml Files. In this case, we will create a new yaml file as you see below.
This configuration file creates a new Service of type ExternalName and points the external name to the url of our choosing.
Now with our external service configured we just need to call to the external service in our application code.
I’ve setup an experiment for you so that it is easy for you to see this in action. As I’ve done previously you can run a startup script to setup your Kubernetes Cluster in Google Cloud and deploy your application code.
Once your service is available you’ll be able you can hit the /foreign endpoint to see the results of the call. Depending on your cluster’s IP Address this would be http://[your cluster IP Address]/foreign.
Note: If you run the code you’ll get back an error as jonbcampos.com is hosted via Bluehost and therefore I don’t have direct IP Address access. You can always go change this value to whatever value works for your test.
With this test complete you are done! You can now use the Kubernetes DNS server (via kube-proxy) to call an external service just like it was an internal service.
Before you leave make sure to cleanup your project so you aren’t charged for the VMs that you’re using to run your cluster. Return to the Cloud Shell and run the teardown script to cleanup your project. This will delete your cluster and the containers that we’ve built.
medium.com
medium.com
itnext.io
medium.com
Jonathan Campos is an avid developer and fan of learning new things. I believe that we should always keep learning and growing and failing. I am always a supporter of the development community and always willing to help. So if you have questions or comments on this story please ad them below. Connect with me on LinkedIn or Twitter and mention this story.
Google Cloud community articles and blogs
37 
37 claps
37 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Excited developer and lover of pizza. CTO at Alto. Google Developer Expert.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://koukia.ca/implementing-single-sign-on-part-1-solution-architecture-d9e12e7ea5e4?source=search_post---------41,"This is your last free member-only story this month. Sign up for Medium and get an extra one
When people talk about a Single Sign-on solution, basically it means to extract all the Authentication bits and logic into an external service or entity which is known as Identity Provider or IdP, in other words it means, in your web application will be configured to redirect the unauthenticated requests to this external service and that service will authenticate the user and return the authentication information based on some sign-on protocols like SAML-P, WS-Federation or OpenID Connect.This external service takes care of the authentication process. usually you introduce your application to the IdP service and after the authentication process was successful, the browser will be redirected back to the original uri or what ever uri you define (sometimes known as audience uri) and it sends the security token to the web application.I will demonstrate how to introduce your application in WAAD tenant, and do the configuration that is required for the single sign-on user experience.
Windows Azure AD works just like this. I am going to describe how to create a single sign-on experience using Visual Studio 2012 and the Windows Identity Foundation (WIF) classes in the .NET Framework 4.5 to configure an MVC 4 application to use a web sign-on protocol.
High Level Solution Architecture
these are basically the steps of a very high level scenario of Single Sign-on:
here is another context view of the whole process with the WAAD, Sign-in page and WIF in place for you to have a better understanding of the process:
right now I am thinking of having 3 more parts for deep diving into the implementation of this.In the next two part, I’ll show you how to work with Windows Azure Active Directory tenant and will continue with configuring a web application to use this WAAD tenant.
Vancouver based Software Engineer and Leader, Geek, Hiker …
48 
48 claps
48 
Written by
Software Engineer, Engineering Leader and Manager @ AWS. Living my dream life. http://koukia.ca/
Vancouver based Software Engineer and Leader, Geek, Hiker …. Living my dream life.
Written by
Software Engineer, Engineering Leader and Manager @ AWS. Living my dream life. http://koukia.ca/
Vancouver based Software Engineer and Leader, Geek, Hiker …. Living my dream life.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/extending-gke-with-externaldns-d02c09157793?source=search_post---------42,"There are currently no responses for this story.
Be the first to respond.
Update: 2020年7月25日 Resources with blog source code link
When deploying a web application, it is preferable to use a name like hello.mycompany.com rather than 34.98.86.241. This requires configuring DNS records when you deploy your web application, which be done with the Kubernetes add-on…
"
https://medium.com/theburningmonk-com/how-to-make-serverless-framework-boilerplates-customizable-c0a6ada65ad7?source=search_post---------43,"There are currently no responses for this story.
Be the first to respond.
The Serverless framework lets you reference external JSON, YML and JS files using the syntax ${file:(fileName):propertyName}. However, you can't customize these external config files with runtime arguments.
A reader asked me:
“I have this boilerplate that is repeated many times in my serverless.yml, how do I reuse the boilerplate but override only specific fields?”
It’s an interesting question and here are some potential solutions.
Disclaimer: the below examples use the HTTP event source for the sole purpose of illustrating the solutions with a trivial configuration object.
The simplest solution is to use YML anchors. You can define and apply the anchor and override specific fields. For example:
If the boilerplate is large, then you might wish to put the boilerplate into a separate file. Unfortunately, I haven’t found a way to make anchors work with external config files in the serverless.yml. I'm not able to import the content of an anchor from another file (as below). Please let me know in the comments if you know what I have done wrong here.
A bigger problem with anchors is that it’s difficult to replace nested fields. For example, given the following anchor:
Suppose I want to change location.city and insert a new field occupation.company. Look what happens if I apply the overrides before and after the anchor:
AFAIK, the only way to make this work is to declare multiple anchors:
It’s getting a bit messy though.
The Serverless framework lets you reference an external JS file too. But you can only reference properties and not invoke methods. This makes it difficult to make a customizable boilerplate config.
For example, if I want to template the HTTP event source configuration but allow the method to be customizable. Ideally, I would like to be able to do something like this:
Unfortunately, since I can’t invoke methods, I would need to wrap them into properties, e.g.
Then I would be able to reference them in the serverless.yml:
That’s a two-step process. Fortunately, with ES6 Proxy we can do better!
We can return a Proxy that traps any attempt to access a property on the exported object. The attempted property name (e.g. get) is used to construct the actual config object we will return.
And this is what happens when I reference ${file(xyz.js):get}:
With this simple technique, I can create a boilerplate config which you can customize a field even if it’s deeply nested. For example:
And import the config like this:
But wait! Can I insert/update a property at an arbitrary location, like in the defaultPerson example earlier?
Well, yes, you can do that by returning another Proxy from the first Proxy.
For example, the following config object lets you update either path or method, or insert a new field.
You can then reference it in the serverless.yml like this:
Which creates the endpoints as you’d expect:
This is what happens when I reference ${file(xyz.js}:method.post}:
But wait again! Can you customize the config with more than one variable? E.g. what if I want to customize both path and method?
You can have a Proxy that returns another Proxy that returns another Proxy and so on. So conceivably it's possible.
As suggested by Philipp Beau, you can also create a scheme to support key-value pairs in a comma-separated fashion. For example, ${file(xyz.js):key0_value0-key1_value1}
I’ll leave that as an exercise to you, the readers. Let your imaginations go wild and let me know what practical problems you are trying to solve. (I love a good hack, but solving actual problems are always more rewarding ;-))
Hi, my name is Yan Cui. I’m an AWS Serverless Hero and the author of Production-Ready Serverless. I have run production workload at scale in AWS for nearly 10 years and I have been an architect or principal engineer with a variety of industries ranging from banking, e-commerce, sports streaming to mobile gaming. I currently work as an independent consultant focused on AWS and serverless.
You can contact me via Email, Twitter and LinkedIn.
Check out my new course, Complete Guide to AWS Step Functions.
In this course, we’ll cover everything you need to know to use AWS Step Functions service effectively. Including basic concepts, HTTP and event triggers, activities, design patterns and best practices.
Get your copy here.
Come learn about operational BEST PRACTICES for AWS Lambda: CI/CD, testing & debugging functions locally, logging, monitoring, distributed tracing, canary deployments, config management, authentication & authorization, VPC, security, error handling, and more.
You can also get 40% off the face price with the code ytcui.
Get your copy here.
Originally published at https://theburningmonk.com on August 5, 2019.
the personal blog for Yan Cui
46 
1
46 claps
46 
1
the personal blog for Yan Cui
Written by
AWS Serverless Hero. Independent Consultant https://theburningmonk.com/hire-me. Author of https://productionreadyserverless.com. Speaker. Trainer. Blogger.
the personal blog for Yan Cui
"
https://pub.towardsai.net/can-we-live-without-google-c2f3ed0be151?source=search_post---------44,"This morning, Google went out of service, as it seems to be caused by a hacker’s attack, making millions of people unable to access their Google accounts, including Gmail and YouTube, after…
"
https://medium.com/google-cloud/using-collectd-ping-plugin-to-monitor-vm-vm-latency-with-google-stackdriver-806614508613?source=search_post---------45,"There are currently no responses for this story.
Be the first to respond.
Customers often want to instrument their application and monitor VM->VM and VM->external service latency as seen by simple ping probers between instances or containers. This article covers how to use an off the shelf Ping Plugin for collectdtogether with Google Cloud's logging and monitoring facilities to acquire and process latency statistics. Google Stackdriver uses collectd for monitoring and fluentd for logging so this article covers how to:
Ofcourse the preferable way to use Stackdriver collectd would be to emit statstics directly to Google Stackderiver as done by the supported set of plugins. At the time of writing, (7/21/18), the the ping collectd plugin is not supported....but thats ok, we can still add on collectd and configure it to emit custom metrics to GCP.
Note that certain cloud providers surface aggregated, sampled statistics such as VPC Flow Logs and the usecase for this is to view connectivity and latency “as seen” by the application using a ground truth ping statistics (or if you dont’ want to instrument the whole network with detailed flow logs)
The first step is to create the ping plugin shared object and enable your VM to run the ping plugin.
You can find the full config and DM templates here:
github.com
The following command will build a debian image and compile the ping plugin
If you are using technique , then you can use any standard ping.so and place that into the VM. If you want to try technique (2: Send pingstatistics to Cloud Logging), then you must compile the version of ping.c i've attached to this repo.
If you want to use
For both, acquire ping.so:
Note, ping plugin requires liboping0 so any host were you want to run this plugin, please also install
In this mode, the the ping plugin’s outputs are translated by collectd and emitted to Cloud Monitoring as custom metrics. This is the more standard way to emit metrics to GCP. The sample in Stackdriver is showng below for custom.googleapis.com/ping/ping_stdev metric for a given instance source and no filters on destination.
The sample configuration for this is included in the deployment manager script that is also included in this repo. Essentially, its just a matter of setting up a collectd filter.
As mentioned earlier, you must install libping0 on any host were you setup the ping plugin
This technique is just for demonstration as it utilizes a modification to ping plugin to emit its metrics as log lines and not statistics.
Basically, the output for the ping plugin is not sent to cloud monitoring but rather the output is written to a file ````/var/log/pinglog.log``` on the host. From there Cloud Logging’s fluentd will read that file and batch emit statistics to GCP.
For example, here are some json structured* cloud logging ping statistics
The following shows all the logs for a given VM
and since they are structured, its easy to construct a filter or log to metric:
Here are the modifications we made to the ping.c to write the log lines:
The attached ping.so is already compiled for these changes. The attached deployment manager template reads the same ping.so from a public GCS bucket here:
You ofcourse dont’ have to use the precompiled ping.so and can just compile it with the instructions in the setup step.
Anyway, lets run a sample Deployment Manager script which will spin up N instances with the collectd plugin and then proceed to setup the Plugin to connect to each node:
Once you deploy, you should see three instances
Give it a couple of minutes and then refresh cloud logging console for any instance you’ve created. You should see a new log type of pinglog.
You can also query the stackdriver metrics explorer too:
Ok, now we’ve got the metrics and for good measure, the logs too. Lets simulate an outage.
Go to the Compute Engine →Instances section of the cloud console and click ‘stop’ on instance-2.
Wait a couple of minutes and query the logs and stackdriver charts for droprate. What you should see is the droprate become non-zero in the first sample and in the next minute, droprate=1. The ping plugin prober frequency is every minute but aggregates the data for Stackdriver. What that means is you are seeing aggregated data (i.,e a non zero value for the first minute for droprate...and a decreasing value for that same metric during recovery)
The following snippet queries the cloud monitoring custom metric for ping using python monitoring client.
Remember to add in the instance_id for the VM you are seeking to display the metrics for.
Which gives a sample output like:
Google Cloud community articles and blogs
6 
6 claps
6 
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-believers/the-differences-between-artificial-intelligence-and-neural-networks-493f9aa4a602?source=search_post---------46,"There are currently no responses for this story.
Be the first to respond.
Artificial intelligence can automatically process documents, simulate biological responses, and the responses of nerves to various external stimuli are essentially processing events.
Artificial neural networks are inspired by studying the mysteries of the human brain, trying to use a large number of processing units (artificial neurons, processing…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/theburningmonk-com/serverless-observability-its-easier-than-you-think-48801abf2d66?source=search_post---------47,"There are currently no responses for this story.
Be the first to respond.
Observability is a measure of how well the internal state of a system can be inferred from its external outputs. It helps us understand what is happening in our application and troubleshoot problems when they arise. It’s an essential part of running production workloads and providing a reliable service that attracts and retains satisfied customers. It is even more important when it comes to serverless because we rely on managed services that we can’t simply “run on your machine”.
In my work as a consultant, many clients ask me about how to build observability into their serverless applications. If it’s better to build a custom solution using the native AWS services or to pay for a 3rd-party service such as Lumigo.
AWS offers a range of services that help you build observability into your serverless application, such as CloudWatch, CloudWatch Logs and X-Ray. These services cater for your basic needs cheaply, but they lack the premium developer experience that specialized vendors such as Lumigo can offer.
For example, when compared with X-Ray, Lumigo offers a significantly better developer experience, including:
While the native AWS services have plenty of limitations and don’t offer the best developer experience out-of-the-box, you can work around these and still create a solution that gives you a lot of observability into your serverless application. For example:
All of these are doable but they take a significant amount of engineering time and often require coordination and agreement between different parts of the organization. Teams have to follow the same conventions such as log message format and the approach towards correlation IDs (their format, and how to propagate them) for the solution to work.
If the build vs buy decision is purely based on capabilities and what will make your developers more productive, then I think the most sensible approach is to use a 3rd party service that ticks most of your boxes and supplement them with the AWS services. This is the approach I have used in all of my recent projects and it has allowed me to focus on solving the client’s business challenges and deliver the desired outcomes on time and on budget.
In my projects, I use Lumigo as the main entry point when I try to understand what’s going on in my application and to troubleshoot issues. I seldom write custom log messages anymore because Lumigo captures most of the information I need to be able to infer the internal state of my applications.
Take the Lumigo dashboard, for instance. It gives me a lot of insights into the activities in my system. I can see at a glance if there is a high number of Lambda invocation errors and if so, which functions are failing. I can see hot spots and identify functions that are invoked frequently, these are good candidates for optimization (e.g. right-sizing the memory setting to reduce cost).
I can see Lambda functions where cold starts are an issue. Lambda functions that have a high frequency of cold starts are also good candidates for Provisioned Concurrency.
And I can also see which of the services (3rd party APIs, AWS services, or internal APIs) my application depends on are performing poorly.
The dashboard alone saves me hours of engineering time to collect, analyze and visualize this (very useful) information. The information it gives me helps me quickly find and investigate problems that arise in production. Problems that used to take me hours to find can now be identified in minutes!
In fact, when a problem does arise, I would usually receive an alert through Slack or PagerDuty and I will find the problem waiting for me on the Issues page in Lumigo.
From here, I can navigate to the erroneous transactions and interrogate them. Because every outbound request from the Lambda function is recorded in Lumigo, I can see how long they took and what was said in those communications. Together with the invocation event and the environment variables that were used in the invocation, I can quickly infer the internal state of the function when it was invoked.
As you can see, that gives me a lot of observability and it takes only a few minutes to set up, and I didn’t have to make any changes to my code. Instead, all of my energy and time can be better spent on tackling the problems that actually matter to my clients and delivering value to their businesses. Which is very much aligned with the spirit of serverless — to move away from undifferentiated heavy-lifting and use the most precious resources we have (engineering time) to solve the most valuable problems.
Building custom solutions to solve the problem of serverless observability is fun and it can be challenging in the best possible way! But they are also not what differentiates your business from your competitors.
Observability is an essential part of running a production application, and the good news is that with platforms like Lumigo, observability for serverless applications is easier than you think.
To learn more about how to overcome common challenges with building production-grade serverless applications, join us on our next webinar on Friday, 19th November.
I will be speaking alongside Ryan Jones, the founder of ServerlessGuru, and share many of the lessons that we have learnt running serverless applications in production over the last few years.
Hope to see you there!
If you wanna try out Lumigo, it also has a generous free tier (no expiration) that lets you trace 150k Lambda invocations per month, for free. Sign up now.
Originally published at https://lumigo.io on November 11, 2021.
the personal blog for Yan Cui
2 
2 claps
2 
the personal blog for Yan Cui
Written by
AWS Serverless Hero. Independent Consultant https://theburningmonk.com/hire-me. Author of https://productionreadyserverless.com. Speaker. Trainer. Blogger.
the personal blog for Yan Cui
"
https://medium.com/@ethr/heres-a-quick-reminder-why-pc-storage-is-so-important-cca3a4f6062d?source=search_post---------48,"Sign in
Ethan Roberts
Nov 12, 2021·2 min read
Having some sort of additional storage helps so much more than you think. It does not matter if it is an external SSD, a second HDD, or even a flash drive. Additional storage is almost certainly going to make you feel more at ease.
Here’s how installing a second hard drive helped me be more productive and have better PC maintenance. I have two storage devices. I have a primary SSD…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@authorvinod/the-best-ways-to-backup-your-computer-b6dd2fa729a6?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vinod Sharma
Jun 11, 2021·2 min read
Nobody feels the need for data backup until their hard disk fails.
Did you ever lose your data? Do you know that it can cost hundreds of dollars to fix your failed hard drive?
I have lost my external hard disk that has plenty of family photos and videos. Only then did I realise what it means…
"
https://medium.com/@fortanix/part-3-keeping-the-keys-to-your-kingdom-google-and-fortanix-collaborate-to-deliver-byokms-d842048ebfc8?source=search_post---------50,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fortanix
Jun 26, 2020·6 min read
This is part 3 of a series of blogs on the integration between Google Cloud External Key Manager (EKM) and Fortanix Self-Defending Key Management Service™ (KMS). If you have not read Part 1 and Part 2, we encourage you to read those blogs that provide an introduction to this topic.
Google Cloud External Key Manager (EKM) service is now generally available and the integration of Fortanix Self-Defending Key Management Service™ (KMS) with Google Cloud EKM as an external key manager is also generally available. Since announcing the integration in November 2019, we have been working with many large enterprises to incorporate this new service and we have learned a lot during the process. In this blog, we explain some of the common requirements large enterprises have for implementing external key management and describe how Fortanix Self-Defending KMS meets those requirements. We also discuss some best practices for configuring your account in Fortanix when enabling the integration with Google Cloud EKM.
Cloud EKM enables enterprises to mitigate their security and compliance risk by retaining control of their encryption keys and securing them outside of the public cloud .To ensure the availability of keys, we recommend the following list of requirements when implementing external key management:
High availability is critical for GCP Cloud EKM because if the EKM key is not available to Big Query or Google Compute Engine (GCE), then the service would be unavailable to the user. These are often business critical services for an enterprise.
Fortanix Self-Defending KMS is architected to always work as a cluster of three or more nodes to provide high availability. The cluster allows for the service to be available and the keys to be usable as long as a majority of nodes are available in the cluster. In case of loss of a majority of nodes, Google Cloud EKM keys are still usable, but the ability to create new keys is lost.
The cluster of nodes can easily be spread across physically separate data centers, thus allowing for failure of entire data centers or failure of connectivity between data centers. The service still stays fully available if a majority of nodes are available in the cluster. In a multi-data-center deployment scenario, a global load balancer is used to route incoming traffic to the nearest data center.
While HA provides continuous access to the EKM key, it would still be disastrous if the EKM key is permanently lost or deleted, either accidentally by a human error, through a software error, or through a hardware failure or disaster at the facilities. Services like Big Query would lose all data in this scenario, so it is critical to have measures to recover from such a disaster.
Fortanix Self-Defending KMS provides several mechanisms to prevent disasters from happening and to recover from disaster when they might happen.
In case that the key does get deleted or lost even after having a quorum policy configured, there are two ways to recover the key.
There are three important aspects of performance — throughput, latency, and quality of service. For enterprises using Google Cloud EKM, they may have a certain peak throughput and maximum latency they expect from the external key manager. They also want these numbers to be predictable and consistent which imposes quality of service (QoS) requirements.
As we have seen above, the Google Cloud EKM key is very sensitive, and access to the key should be managed very strictly by a very small set of administrators in an organization. This is achieved using a very elaborate RBAC system in Fortanix Self-Defending KMS. Users and keys may be organized in separate groups, and access to keys for users is determined by their group membership and roles in those groups. By following the principles of least privileged access, it is possible to restrict access to a small number of users who can create and manage the EKM keys. As mentioned above, actions can further be restricted by configuring a quorum-based policy.
Auditability of operations is often a major requirement for enterprises. While this may seem simple, getting a consistent set of logs from a cluster deployed across multiple sites is challenging. Fortanix Self-Defending KMS provides a secure way to store and manage such cluster-wide logs, capturing various types of operations, such as administrative (user creation, group creation, etc.), authentication (all login and logout actions are logged), and cryptographic (key creation, editing, usage, etc.).
The logs collected in Fortanix Self-Defending KMS can be forwarded to a log collection software or services in real time with Splunk or any Syslog-based logging systems. We also have native support for forwarding logs to Google Cloud StackDriver. This enables users of Google Cloud EKM to stay in the Google Cloud ecosystem and use more native Google Cloud services.
Originally published at https://fortanix.com.
Fortanix™ has created the world’s first runtime encryption solution. Enterprises get provable, portable, and preventive security for their applications!
Fortanix™ has created the world’s first runtime encryption solution. Enterprises get provable, portable, and preventive security for their applications!
"
https://medium.com/adobetech/easily-coordinate-external-deployments-with-cloud-manager-and-adobe-i-o-events-a-step-by-step-bb2f651a18ae?source=search_post---------51,"There are currently no responses for this story.
Be the first to respond.
If you’re an Adobe Managed Services (AMS) customer, you likely depend on a series of complex applications that span both Adobe Experience Manager (AEM) and other systems to get your jobs done. In some cases, integrations between these services may be loosely coupled, with deployments being done on completely separate cycles. But in many cases, the applications are tightly coupled and need to be deployed in a coordinated fashion. This is made possible using Adobe Cloud Manager with Adobe…
"
https://servian.dev/external-ingress-control-for-cloud-run-afb3852c0d57?source=search_post---------52,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Betsy Varghese
Dec 23, 2020·5 min read
Cloud Run is a fully managed service provided by Google Cloud that allows you to deploy containerised applications. In deploying a container image to Cloud Run, you create what’s called a service. These services have an immutable URL associated with them which can be used to access your app. While you can lock them down using IAM, there’s always additional security benefits to using external ingress controls and whitelisting IPs from trusted sources. After all, you don’t want just anybody accessing your app.
How do we do it? Cloud Load Balancing and Cloud Armor! Network-based ingress controls can be implemented with the help of a load balancer that has security policies attached to it, ensuring that only trusted sources can access the load balancer. This can be done in addition to allowing only authenticated invocations of the service. For this tutorial, we’ll be whitelisting our own IP address.
We’ll be going through how we can set this up, step-by-step. Begin by following this Quickstart tutorial provided by Google Cloud to set up our Cloud Run service (pick any language you’re comfortable with). Once you’ve got your container up and running, register a domain name for your service (we will be using this later).
Cloud Load Balancing is a service that helps you manage traffic to your resources. We’re going to set up an HTTPS Load Balancer for our service.
Now you’ll have to configure the load balancer’s host rules and path matchers, backend, and frontend.
Backend
We’re going to create a serverless network endpoint for the backend. A network endpoint group (NEG) specifies a group of backend endpoints for a load balancer. To point to a Cloud Run, App Engine, or Cloud Functions service (all serverless), we use serverless NEGs. Depending on the service, these serverless NEGs can represent a single service or a group of services using URL masking. (Note that serverless NEGs can only be set up with HTTPS Load Balancing!)
Finally, click Done under New backend window and click Create.
Host Rules and Path Matchers
Leave the host rules and path matchers to their default values, meaning that all requests go to the service you choose in the backend.
Frontend
We’ve configured all aspects of the load balancer, and can now click Create. When it is done being created, click on the name of the load balancer and note the IP address associated with it in the Frontend configuration. Point the DNS A record of your domain name to this IP address. This is the URL we will use to access our service.
Congratulations! We’re nearly done. All we need to do now is to add IP whitelisting.
Cloud Armor is a Google offering that provides a web application firewall to control traffic to your applications. Find out what your IP address is, and head to the Cloud Armor page.
Finally, click Create policy. We’ve asked Cloud Armor to deny all traffic to the load balancer, with the exception of our IP address. You can also add a response for denied traffic.
Now’s a good time to find out if our setup works. Access your domain from your device using the required authentication. An easy way to do this is to click “Show information on service URLs” (available next to the service URL on the Cloud Run console) and copy the invocation provided. Replace the service URL with your registered domain name.
When I access this using my whitelisted device, I get the following output:
What about from other IPs? Try the same invocation from Cloud Shell, which is run on a temporary VM and has an ephemeral IP.
And there we have it! Only whitelisted IPs are able to access the Cloud Run service, providing a layer of security to our resources.
Happy building!
Currently, there is an alpha feature of Cloud Run that enables us to set the ingress parameter to “internal-and-cloud-load-balancing”. This ensures that only inbound requests from VPCs in the same project or Google Cloud Load Balancing are allowed to access the service URL. You can sign up to enable this for your project, or wait until it is GA.
101 
101 claps
101 
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hashicorp-engineering/calling-external-apis-in-terraform-cloud-sentinel-policies-d541be0756b6?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
HashiCorp recently added two important new features, a new HTTP import and Parameters, to the 0.13.0 release of its policy-as-code solution, Sentinel. In this blog post, we discuss some example Sentinel policies that use these new features in Terraform Cloud.
The HTTP import allows Sentinel policies to retrieve data from external API endpoints that return JSON documents.
Parameters were primarily added to enable policy authors to define API credentials securely outside of the policies themselves since these are often stored in VCS repositories. However, parameters can also be used for other purposes, making Sentinel policies more flexible.
At the same time, Terraform Cloud now allows parameters to be added to Policy Sets and allows them to be marked as sensitive. In other words, Terraform Cloud allows for the secure definition of the parameters that Sentinel policies can now accept.
Some customers want their Sentinel policies to call external APIs in order to check that change requests have been approved in their internal systems. Our first example below simulates that. Our second example below calls the Terraform Registry API to make sure that the most recent versions of Terraform modules are being used. Other possibilities include calling cloud APIs to fetch information about resources and services and calling Vault’s HTTP API to retrieve secrets.
You’ll find two policies that use the HTTP import in the hashicorp/terraform-guides repository here.
The first of these, check-external-http-api.sentinel, is quite simple and rather silly; it simply uses the HTTP import to call the public API, https://yesno.wtf/api, that randomly returns “yes” or “no” (but sometimes “maybe”). This API actually returns a JSON document that contains the answer, a flag called force, and a URL of an image. A typical response looks like this:
Here is the function in the policy that uses the HTTP import:
We have used bold-face for that part that uses the HTTP import and its data types. Since we only care about the answer, we only extract it from the response with answer = res.answer. We then use the Sentinel case statement that was added in Sentinel 0.12.0 to map the answer to a boolean that the function returns. We return true if the answer is yes or maybe and false if it is no. That’s about as simple as it gets!
While the check-external-http-api.sentinel policy does not use any policy parameters, the second policy, use-latest-module-versions.sentinel, does. This policy calls the Terraform Registry List Modules API of a Terraform Cloud (TFC) or Terraform Enterprise (TFE) server to determine the most recent version of each Terraform module in the Private Module Registry (PMR) of an organization on that server or in the public Terraform Registry. It then validates that all modules used in the current TFC or TFE run that come from the specified registry use the most recent versions. The versions of modules from other sources are not validated.
In case you have never used a module from a Private Module Registry, here is an example of Terraform code that does that:
Note that app.terraform.io is the multi-tenant Terraform Cloud environment run by HashiCorp. Cloud-Operationsis the organization, network is the module name, and azurerm is the main provider for the module.
Here are the parameters the policy uses:
These parameters are declared within the policy as follows:
The actual parameter declarations are in bold-face and are accompanied by comments. These are actually used by the Sentinel CLI as parameter descriptions that it shows to users who run the sentinel apply command without providing values for parameters missing default values. In this policy, public_registry and address have default values but organization and token do not.
Once a parameter has been declared in a policy, it can be used like a variable. This includes passing them into functions as is done in this function from the policy that actually uses the HTTP import to query the registry API:
Again, we’ve used bold-face above to highlight the use of parameters and the HTTP import.
We don’t analyze the rest of the policy in this blog post, but you can see all of it by following the link we gave above.
The repository that contains the two sample policies described above also contains Sentinel mock files and test cases so that the Sentinel CLI can be used to test them. Directions for doing this are in this README.md file. We recommend that you clone or fork the repository and follow those directions to test the policies with the Sentinel CLI. (Note that you do not need a Terraform Cloud account or Terraform Enterprise server to use the CLI.)
To use the policies in a Terraform Cloud or Terraform Enterprise organization, you need to add them to a policy set and then register that policy set with the organization. You will need access to an organization that has some modules in its Private Module Registry (PMR). You can follow these steps which are also given in the README.md mentioned in the last section.
Running the plan should trigger a check against your policy set. If your Terraform code uses the most recent versions of all modules in the PMR, then the policy will pass. If your code uses any older versions of modules in the PMR, the policy will fail.
In this blog post, we have illustrated how Sentinel’s new HTTP import and parameters can be used in Terraform Cloud Sentinel policies. We hope you found it interesting. We expect that many of our customers will use the HTTP import in their TFC and TFE Sentinel policies. Thanks for reading!
If you would like to learn how to use the Sentinel HTTP import in Vault Sentinel policies, please see this blog post.
A Community Blog by the Solutions Engineers of HashiCorp
12 
12 claps
12 
A Community Blog by the Solutions Engineers of HashiCorp and Invited Guests
Written by
Roger is a Sr. Solutions Engineer at HashiCorp with over 20 years of experience explaining complex technologies like cloud, containers, and APM to customers.
A Community Blog by the Solutions Engineers of HashiCorp and Invited Guests
"
https://medium.com/datamindedbe/unique-dashboards-for-external-customers-with-google-cloud-f5e1bcf947a?source=search_post---------54,"There are currently no responses for this story.
Be the first to respond.
How BigQuery and DataStudio can enable you to build your managers requested dashboards right now
Publiq, a client of ours wanted us to help them share their privacy-sensitive data insights with a number of cities in Belgium. The data describes how citizens participate in many events organized all over the country. It is stored in BigQuery and the cities each have an account with publiq. Each city should only see a dashboard based on “their data”, even though the entire dataset is driving Publiq’s publicly facing websites, such as uitinvlaanderen.be (for more info on this client project, see the note at the bottom).
This is a common scenario. While dashboards are widely used within organizations, they should also be possible to be shared with 3rd parties. Upstream supply chain partners may want to know the current workload of a factory to decide whether or not to ship more raw materials to the partner. Public organizations may want to share their performance metrics with the public and companies may want to share their KPIs with regulatory bodies.
Google’s GSuite offers a flexible way of sharing documents as every college student knows. What makes it cool though is the fact that this system also works for their data warehouse and reporting tools. Yes, with GCP it is as easy to share complex dashboards and metrics of a company as it is for a college student to share his lecture notes with her fellow students:
Lets look at how to get something like this set up with BigQuery and Data Studio in under an hour. For my example, I will use the Github public dataset, available on BigQuery. Below is a diagram that shows the target state
Let’s say I want to create a dashboard for specific repositories for an number of clients. In my case, I want to show torvalds/linux to my work email and apple/swift to my personal email.
To define the mapping, I have a table which simply maps email addresses to repository names, as I want to expose certain repositories statistics to certain individuals.
Below is a SQL query that feeds a view which resides in a separate dataset. Big Query only offers IAM rules on a dataset level, hence the separate dataset. This dataset is now shared with all of the clients, using either their own google credentials or credentials which we create for them using the GCP identity platform. The SESSION_USER() function in SQL is where the magic happens: It returns the current session user’s email address.
Next, we need to share the “public dataset” (containing the saved view) with the users that we want to be able to read from the view and the view itself needs to be authorized to read from the “private dataset” i.e. our customer data table.
Hopping over to the data studio, we create a new data source and select the view. In the source settings, it is important to select “Viewer’s credentials” so that the viewers credentials are used when accessing the report’s underlying data.
In the explorer, you can now build your dashboard as you please. I decided to build a small small dashboard that shows the different contributions of the top contributors of a repository (based on commit count). When I open the report in my two accounts (one part of our organization, one my personal account), I can see the two different repositories data and nothing else.
Publiq vzw, our client mentioned above, collaborates with cities, municipalities and regions in Flanders, Belgium to collect all leisure activities and promote them in online and offline media. Their loyalty program “UiTPAS” provides privacy-sensitive insights in citizens’ leisure participation. All of this data is stored in BigQuery. They needed a solution to share the data for a specific city, only with employees of that city, in a single Google Data Studio report, using their own federated login provider (auth0) as Identity Provider.
Starting in 2019, Pascal is a data engineer and certified GCP cloud architect at Data Minded, who is passionate about delivering business value and actionable insights through well architected data products. Pascal studied Information Systems in Cologne and Sydney. He’s also been working part-time as a Software Engineer since 2012 for a consulting firm, a successful drone startup and a space company.
Better data engineering
9 
9 claps
9 
Written by
Software Developer, Tech enthusiast, student, board sports and food lover
Better data engineering
Written by
Software Developer, Tech enthusiast, student, board sports and food lover
Better data engineering
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@firas-messaoudi/spring-boot-external-configuration-with-spring-cloud-config-and-spring-actuator-60603f2ff23f?source=search_post---------55,"Sign in
There are currently no responses for this story.
Be the first to respond.
Firas Messaoudi
Jul 6, 2020·4 min read
While developing a spring boot application , most developers have to define some fixed properties in their application.properties like the secret key for the token , the session timeout, the path of uploaded files and a lot of other properties.
The problem with this approch is that these properties are loaded only once when the server starts. So what’s gonna happen when we want to change some properties while the app is in prod. In this cas we have to shut down the server , change the properties and restart the server.
Even worse , what if we’re working multiple microservices , it would be a disaster if we shut down all microservices.
To avoid these complications we have to externalize our config files in a distance github repository and access these files using spring cloud config which represtents a distributed file server as described below:
As a first a step we have to create a git repository where we will store the configuartions of our application or microservices and name it for example“configuration-server-repo”.
In this repo we create a propertie file and name it with the exact name defined in the propertie : spring.application.name of the app , because that’s how our config server will link the application with it’s distant configuration.
For example we have an ecommerce application, in its application.properties file we add this line:
And the application.propeties file in the git repo will have the same name: ecommerce-application.properties
Now we move all the properties of the app (except the propertie spring.application.name )into the ecommerce-application.properties file in the git repo and commit the changes.
Our git repo will look like this:
Now go to spring initializr or from any IDE and create a new spring boot application with spring cloud config as starter
Open the project with your IDE and go to application.properties and put the following properties:
We have now to define the app as a config server by adding the annotation @EnableConfigServer
Start the server and navigate to http://localhost:9101/ecommerce-application/default. You will see your application.properties retrieved from the git repo:
As for now we have a distant repo linked to the server config, we must now tell the ecommerce application to get its configurations from the config server.
Update the pom.xml with the following dependency:
Go to the application.properties of the ecommerce application which only has the propretie “spring.application.name” and add the following propertie:
Start the ecommerce application and you should see this log in the console :
Which means that the ecommerce application is now fetching its configuration from the config server.
We have one last problem to solve: if you try to change the property upload.path and upload a new image from your application , you will notice that the path didn’t change. So do we still have to restart the config server ?
Not if we use spring Actuator which provides an endpoint to our application to refresh the configuration without restarting the app nor the the config server.
Go to pom.xml in the ecommerce application and the following starter:
And add the following propertie ( in the propertie file in the app or in the distant repo):
Finally we have to inform all the beans consuming any of the properties to get refreshed each time we make a change, by adding the following annotation:
Restart the ecommerce application , change your upload.path propertie from the git , commit the changes and trigger the resfresh event by making a post request:
http://localhost:8080/actuator/refresh.
Now go back and upload another image , and you’ll see the new path being considered by your application.
So now we have our ecommerce application fetching its config from a distant git repo through the config-server. And we can change our configurations with no need to restart the application nor the config server.
Enjoy reading and clap if you like this article.
Full stack (Spring boot — Angular ) developer and computer science engineering student
4 
4 claps
4 
Full stack (Spring boot — Angular ) developer and computer science engineering student
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/codebrace/working-on-on-prem-external-airflow-with-google-cloud-platform-gcp-5b2e77b0e3ba?source=search_post---------56,"There are currently no responses for this story.
Be the first to respond.
If you want to work with Airflow and just starting up with your installation then Google Cloud Composer is the best solution, As it creates all the required services and manages Kubernetes Cluster via GKE and everything connects like magic.
But if you already have an On-prem Airflow or Airflow working on some other Cloud Provider and want to connect with GCP, you will have to do a couple of things to get everything up and running.
You might want to test this setup locally in your local Airflow before deploying your DAG in your deployed instance.
There are 2 ways to install Airflow on your machines 1. Running Airflow locally on your machine  2. Running Airflow via Docker
If you have already decided not to work with Docker, good luck with that here are some docs you can refer .I love Docker and I will help you with this.
Step 1. Install Docker moreStep 2. Use Docker compose to get the instance up and running in a couple of mins.
Airflow should be available on Port:8080, by default here.
for more refer to the documentation.
Note — Here Conn Id is ‘google_cloud_default’, as this is default connection name for GCP, if we choose to have another, we will need to pass connection_id in operators.
please find the DAG here
Get going with BigData and Programming
2 
 brace yourself with newest tech and trends Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2 claps
2 
Written by
Big Data Engineer at Skyscanner , loves Competitive programming, Big Data.
Coding blog to help people get going with Competetive programming, Big Data and other technologies, visit http://medium.com/codebrace
Written by
Big Data Engineer at Skyscanner , loves Competitive programming, Big Data.
Coding blog to help people get going with Competetive programming, Big Data and other technologies, visit http://medium.com/codebrace
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/tech-learnings/sync-external-git-repository-to-cloud-manager-repository-342525fb13c0?source=search_post---------57,"There are currently no responses for this story.
Be the first to respond.
In the earlier tutorial, we have discussed the basic details on Cloud Manager and how to use CM API/Events to trigger the notification to Social Channel(Teams).
As discussed earlier, the Cloud Manager enables own Git repository to manage the deployment to different environments, for simple projects the Cloud Manager enabled git repository should be enough to manage the day to day development activities of the…
"
https://medium.com/@kavya_3110/external-hard-drives-with-cloud-storage-backup-options-1745b76513da?source=search_post---------58,"Sign in
There are currently no responses for this story.
Be the first to respond.
kavya sharma
Dec 15, 2016·3 min read
External hard drives are considered as the portable alternatives to hard disks and thus, sometimes called portable hard drives. External HHDs come in varying storage capacities and are connected to a computer either with the help of USB, eSATA, FireWire or wirelessly. On the other hand, cloud storage is known as a notable alternative to external hard drives. Today, there are numerous external hard drives which are capable of offering cloud storage as well as backup options:
This is a one-to-one cloud syncing portable external hard drive. Seagate and Amazon have joined hands in order to bring the best of local as well as cloud storage in a user-friendly device. The drive offers 1TB of storage. It is believed that cloud storage integration paves the way for automatic backups to Amazon drive. In addition, customers of Unites States will get 12 months unlimited cloud storage for the Seagate Duet HDD.
This belongs to a series of external hard drives that includes 200 GB of extra OneDrive cloud storage for few years. Available in four capacities i.e. 500GB, 1TB, 2TB and lastly, 4TB. Seagate Backup Plus satisfies the growing needs for data storage and backup with some amazing features such as OneDrive cloud storage, Lyve app as well as the new 4TB capacity. Hence, If you are in the search of lots of storage space on the go then this is the best option. available in the market.
A smart storage solution, which ranges from 500GB to 3TB and can also turn your PC into your personal cloud storage server, allowing you to upload and download your files to any from any location and device. The Connect II hard drive models are available in white, blue, black, red or satin gold and are available from $82.12 to $340.48. Lastly, this portable device takes precautionary measures to safeguard your data.
Offers 4TB, 6TB, 8TB, 12TB and 16TB storage facilities. 16TB model is available from $249.99 to $599.99. The device offers fast transfer speeds, huge capacity and integrated WD Red drives in order to provide the desktop storage solution. Furthermore, this hard drive has Dropbox cloud integration, which means users can take the backup of their Dropbox storage to the Duo hard disk.
READ All You Need To Know About Apple’s 9th September ‘Hey Siri’ Event
This is a hybrid NAS drive with models offering 2TB, 3TB, 4TB, 6TB and 8TB of hard drive storage. The drive is compatible with both the windows as well as Mac OS platforms and is available at $139.99. The best thing about this hard drive is that it offers remote file access across a wide range of devices with Mac, PC, Android and iOS apps. The drive also backs up videos as well as photos to cloud storage.
This external hard drive is compatible with windows 7, 8, 10 and Mac OS X. The biggest difference between MC Mirror and My Cloud is that it is a dual drive with a RAID 1 configuration. There are several models available such as 4TB, 6TB, 8TB and 16TB models. The drive provides remote access to files.
All these external hard drives include extra data storage and the facilities of remote file access and backup options.
Original article published at : http://techttalks.com/external-hard-drives-cloud-storage-backup-options/
Marketing Executive TechTTalks
Marketing Executive TechTTalks
"
https://medium.com/@oneneck/internal-external-roles-for-company-cloud-security-d21130dc5451?source=search_post---------59,"Sign in
There are currently no responses for this story.
Be the first to respond.
OneNeck
Nov 27, 2018·2 min read
Outsourcing is becoming an increasingly popular business strategy. By carving off business processes and giving them to outside vendors companies save money and resources. Outsourcing enterprise computing processes using cloud services, for example, allows you to hand off the cost and responsibility of maintaining on-premise hardware and software. However, just because you outsource your enterprise infrastructure does that mean your cloud service provider assumes total responsibility for your network? What about issues such as cloud security, which is a major concern for every IT manager and CIO? Can you hold your cloud service provider accountable for providing watertight data security?
As the use of cloud continues to grow, concern for data security grows with it. When data is an important business asset, management is hesitant to surrender control. However, it has been demonstrated over and over that cloud-based services tend to be more secure than on-premises systems. Most cloud services providers maintain rigorous security protocols for disaster recovery and protection from cyber-attack. Since providing secure and stable computing services is their primary business, cloud service companies use sophisticated tools to continuously monitor systems, identify vulnerabilities, and plug holes in cloud security. They also have service-level agreements (SLAs) to assure customers that security problems are remediated quickly.
So when you engage a cloud services provider you no longer have to worry about data security, right? Wrong!
Enterprise computing is more than just hosted enterprise hardware and services. Your cloud service provider is responsible for securing the foundation of your enterprise infrastructure; the computing systems, power, data storage, database, and networking. As the customer, you are still responsible for securing applications and related services.
Your cloud provider is generally responsible for cloud security at the network layer, including network segmentation, perimeter services, DDOS spoofing, and so forth. As the cloud customer, you are responsible for threat detection, security monitoring, and incident reporting. In other words, your provider offers cloud security for hosted switches and networks, but your responsibility is to secure the network applications and data traffic. Most SLAs are structured to make it clear that the customer is responsible for host layer data traffic, such as access management, patch management, security monitoring, and log analysis, i.e. any application security elements.
Assuming that your cloud service provider will include comprehensive cybersecurity as part of their contract is a mistake. There are areas where they have control over the infrastructure and therefore can take responsibility for data security, but there are other areas that have to be the enterprise customer’s responsibility. Developing a collaborative cloud security strategy is the best approach to address risk management and deal with security threats.
Let’s consider some of the most prevalent security threats and where they tend to compromise enterprise networks. According to the 2018 Verizon Data Breach Report security issues affect both enterprise network owners and cloud service providers.
To read the full list of providers and learning how to develop collaborative cloud security strategies, visit OneNeck’s blog for the original blog post.
Get the information you need to guide your IT strategy.
Get the information you need to guide your IT strategy.
"
https://medium.com/tomorrow-in-progress/the-future-for-rent-1813c4806c6c?source=search_post---------60,"There are currently no responses for this story.
Be the first to respond.
Recently Lakshmi had become an accidental owner. When her father passed away, he left her his house in Santa Cruz. But her life had no room for ownership, especially not homeownership. It meant maintenance, taxes, and limits. Her life was in San Francisco — or London, Mumbai, and sometimes Tokyo — where Lakshmi worked for CB, the largest living space company.
Living Spaces were homes-as-a-service. When you entered a new Living Space appliances were already personalized to your presets, dinner deliveries re-routed to your new location, and housekeeping subscriptions kept the place tidy. These living spaces had no cleaning supply closets or cabinets for kitchenware. Those were wasteful remnants of a time before the rental-service economy. Today you only owned if you could not afford rental services. As CB’s founder liked to say, “Our world is crippled by underutilization. With living spaces you can access everything without wasting anything. Invest in an experience, instead.” That inspired Lakshmi to join CB as a growth hacker.
She had read a case study about an old phone company that dominated the market and gained new users by buying out competitors’ contracts. It kept users loyal by constantly upgrading their phones. Minimize switching costs and always give the user something new - CB could do that. CB could buy out homes and sell the owners living spaces. People could even pay for lifetime membership through a 30-year, fixed-rate plan. Retirement with a CB Living Space meant a life of convenience and freedom. Lakshmi proceeded to launch CB’s biggest growth campaign.
Lakshmi’s success put her and her family in the fortuitous position of not having to own anything. They lived the CB dream. They spent holidays in an Alamo Square Victorian with a fireplace. On weekdays they stayed in a Noe Valley space with a large study and upgraded video walls. The children walked to school. They had more choice, more access, and less overhead in every part of their lives. Recently they upgraded to weekly refreshes on their clothing subscription. The premium-tier membership ensured access to the newest and trendiest living spaces.
Back in Santa Cruz she felt almost embarrassed by her father’s ownership footprint: a three bedroom home for just one person seemed so, well, wasteful. For a man who rarely cooked, the kitchen was full of appliances. In his closet hung an outdated suit, worn just once to Lakshmi’s wedding. Buy-to-dispose goods, made by mass production, populated each room. She felt claustrophobic and went outside for air.
Just then, an elderly man walked by. “Oh, you must be the daughter. Your father always talked about you.” He smiled kindly, “I’m Hades. I live next door. Every day your father came outside right about now to enjoy a cup of tea.” She smiled back, but it felt invasive that this neighbor knew so much about her father. Lakshmi excused herself and walked back inside.
On the stove she found her father’s teapot. As a child, he taught her about phase changes as steam rose out of the boiling kettle. Later, as a teenager, the two of them would stay up late sipping tea and talking about his genetics research or her app ideas. Just last year, over tea once again, she tried to convince him to move into a Living Space, but he refused, saying he enjoyed his possessions too much. That teapot brought back such vivid memories.
A soft buzz on her phone brought her back to the present. An email with a below-market offer for her father’s home had arrived. She knew she would accept it so she could get back to her life. Before leaving, however, she added tea bags to her recurring dinner delivery order. But she included a note that a teapot would not be necessary. She would provide that herself.
Contributors: Bansi Shah, Scott Paterson, Alastair Warren
This is part ten of Tomorrow in Progress from IDEO San Francisco. Tomorrow in Progress is a series that explores what the future of life in the Bay Area might be like in 10–15 years.
Editor’s note: The images used in this piece are worth a brief discussion of its own. Today, much of IKEA’s catalog imagery is computer generated. Tomorrow, as our possessions continue to be digitally represented so too might our acquisition of them become ephemeral. To visually support Bansi’s story, the images demonstrate this transformation. First, the top image is a collage. The background of the collage is by Benoit Dereau created in 2015 using a real-time gaming engine, Unreal Engine 4. Benoit’s image, which represents today’s capabilities, was then slightly modified and augmented to project the experience forward into the near future. The second image, representing the past, is from a well-known series of photographs from 1994.
Tomorrow in Progress is a series that explores what the…
80 
10
Thanks to Alastair Warren. 
80 claps
80 
10
Written by

Tomorrow in Progress is a series that explores what the future of life in the Bay Area might be like in 10–15yrs. It’s an outcome of a new foresights capability we’re practicing, like Design Thinking, that examines futures, design fiction, and inhabitation.
Written by

Tomorrow in Progress is a series that explores what the future of life in the Bay Area might be like in 10–15yrs. It’s an outcome of a new foresights capability we’re practicing, like Design Thinking, that examines futures, design fiction, and inhabitation.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@chaitali294/sd-wan-the-answer-to-all-networking-needs-6e77d0e7e067?source=search_post---------61,"Sign in
There are currently no responses for this story.
Be the first to respond.
Chaitali Sengupta
Feb 6, 2018·5 min read
It’s time you let WAN go and welcome SD-WAN. You must be wondering why? The reason is the increase of cloud and other external services in data storage. As much as the business data move to CTOs, CIOs, cloud and other managed services, the data securing platform providers will have to restructure their network designs.
Made up of virtualized components, software-defined network (SDN) is where SD-WAN is rooted. These virtual components help SDN win over local switches and routers, and improve agility and efficiency. SDN is used to profoundly rearrange networking and its provisioning, with predefined strategies for adjusting loads, portioning access controls and reacting to security dangers.
How beneficial is SD-WAN?
The demand for software-defined WANs (SD-WAN) has increased significantly in recent years. Market research firm IDC estimates that global sales of such solutions in 2020 will be around $ 6 billion. This is almost twice as much as in 2016.
SD-WANs are attractive to many companies because they allow them to use a combination of different network technologies in a wide area network (WAN): broadband Internet connections, 4G — and soon 5G mobile and traditional MPLS networks (Multiprotocol Label Switching).
As a result, less flexible and above all costly MPLS connections are at least partially removed. This pays off, especially when connecting branch offices to the corporate data center and cloud data center. In this case, expensive MPLS links will be replaced by Internet connections.
As the business sector is changing its shape and expanding globally, it is important to secure all that data with a tightly integrated security system. With more and more companies moving to cloud for all your business data storage and security needs, it’s high time you migrate your existing business to SD-WAN.
Believe that SD-WAN is the future of network security and connectivity.
Migration to an SD-WAN
Although, migration to a software-defined WAN is already challenging for small and medium-sized businesses with few field offices. The situation is even more difficult for large companies with branches in several regions of the world.
Coordinating and managing the services of multiple providers of broadband connections can be costly. Therefore, if a migration is in place, companies are increasingly using managed SD-WAN services.
The advantage: The user receives virtually a turnkey solution. This saves time and money that would otherwise be spent on implementing and managing an SD WAN. In a managed SD WAN service, the provider manages and monitors the network and infrastructure components. In addition to network service providers, Managed Services Providers (MSPs), systems integrators, and Value-Added Resellers (VARs) provide software-defined WANs.
New sources of revenue for service providers
At first glance, it seems that SD-WANs are disrupting the business model of providers of MPLS services. But that is not the case. After all, such providers can use software-defined WANs to integrate branches into a corporate WAN that do not have access to an MPLS infrastructure. A service provider can thus increase the range of his network by using publicly available Internet connections. This does not require a high investment in router infrastructure like MPLS.
As a result, large, established telecommunications companies are increasingly offering software-defined WANs. IDC said that was one of the reasons why the market for managed SD WANs reached $ 2.3 billion in 2020.
Flexible infrastructure
Service providers can offer long-distance services via SD-WAN infrastructure that meets individual customer requirements. At the start, they are the same as having software-defined networks (SDNs) or software-defined-storage architectures (SASs). These include greater flexibility and advanced automation features. This is because it is easier to reprogram the software of a virtualized network appliance than to deploy and manage hardware components at sites.
Service providers can use Orchestration software and a management solution for SD WANs to configure and manage the long-haul connections of client systems in field offices. No on-site IT professionals are required in the offices. This reduces the effort and therefore the costs. In addition, the SD-WAN management software can be combined with the billing systems used for billing customers.
Intelligent functions are distributed
These features for implementing, configuring, and managing an SD-WAN have several benefits for the service provider and its customers.
#1 Depending on the application, the “intelligence” of an SD WAN can be implemented where it is needed. This can be the appliances in a corporate office, but also the edge systems of a provider network and the SD-WAN components in a cloud data center.
#2 A software-defined WAN provides greater reliability and security when transporting traffic over Internet connections. For example, an SD-WAN automatically switches to alternate connections when a broadband Internet link is overloaded or down. In addition, features such as Forward Error Correction (FEC) and Packet Order Correction (POC) ensure that data packets are not lost and arrive in the correct order to the recipient. Add to this the lower cost of using public broadband Internet connections instead of MPLS infrastructure.
#3 SD-WAN allows extending its own WAN infrastructure using Internet connections and LTE. This allows a provider to offer new services, such as the connection to public clouds. In addition, it reaches more potential customers, even those who do not want or cannot use MPLS.
Managed SD WAN services are a good choice
Managed SD-WAN services are particularly suitable for the following users:
# Companies and organizations that want to integrate 100 or more sites into a corporate WAN
# Companies who try to stay away from the expense associated with “controlling” ten, twenty or even more broadband service providers. This is not only a challenge from an organizational point of view, but also from a technical point of view. Broadband services of different types and quality must be combined to form a homogeneous corporate WAN.
In conclusion, it can be said that providers of long-distance services are well-advised to deal with the issue of SD-WAN. As due to this approach, they can expand their range of services.
More importantly, this way SD WAN providers can meet their customers’ changing needs. In the future, these will increasingly use cloud services and web applications that are offered via Internet links. They would want to spend less money on WAN services. With a software-defined WAN, service providers can fulfill all these requirements.
Co-Founder at MWB (www.mywaterbottle.in) Creative Writer | Digital Marketing Expert
Co-Founder at MWB (www.mywaterbottle.in) Creative Writer | Digital Marketing Expert
"
https://medium.com/@vunvulear/cloud-lock-in-overview-and-pros-and-cons-of-cloud-lock-in-azure-87eba643a707?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radu Vunvulea
Jul 8, 2021·4 min read
I will start a series of posts about how we can avoid cloud lock-in and interoperability between managed cloud services and external systems using different solutions on the market.
Cloud lock-in Overview
There are multiple definitions on the market, but when we talk about cloud lock-in or vendor lock-in, we refer to direct and indirect costs of a platform to be moved from one cloud vendor to another.
The lock-in makes customers more dependent on a cloud vendor, and migration to another vendor becomes expensive. There are multiple dimensions of cloud lock-in that we need to be aware like:
We need to be aware that cloud lock-in comes with advantages, especially from the time-to-market and cloud economics point of view.
Cloud Managed Services like Azure App Services, Azure Service Bus, Azure Redis Cache help us build a platform in no time. The effort required to build the infrastructure and implemented the NFRs (e.g., availability, redundancy, backup and recovery) is drastically reduced, together with the SLA that are provided for each Cloud Managed Services. It is much easier for the support team to manage the services that are out of the shelve offered by cloud vendors like Azure or AWS.
Should I go ALL-IN?
Going all-in for a cloud vendor provides a high level of agility and the effort required to manage the infrastructure, stack components like cache or DB and middleware are very low. Using this approach there is more time to invest in security, scalability, DR and HA and cloud managed services competencies.
Pros and Cons
The most important cons of a cloud lock-in are:
The pros of cloud lock-in are:
The real challenges
Large organizations have a strategy to avoid cloud lock-in. This can be achieved at the program level, ensuring that different parts of the systems run in different cloud vendors or by building the systems to run seamlessly (more or less) on multiple cloud vendors.
The real challenge is finding the right balance between Cloud Managed Services (PaaS and SaaS) and the ones you managed by yourself.
In the last few years, adopting containerization solutions like Kubernetes combined with different solutions like Dapr enabled us to achieve a higher level of interoperability with less effort and a high degree of loosely couples between cloud services and the application.
The out of the shelf integration and abstraction layer that Dapr offers, give us the ability to switch between AWS SNS and Azure Services Bus without making changes at the application layer. Kubernetes enable us to run the same application inside AWS EKS or Azure AKS seamlessly.
There are many other ways to handle cloud lock-in and each of them will be discussed in a series of articles.
Is cloud lock-in so bad?
YES and NO
There are many aspects that need to be taken into account when you decide what level of cloud lock-in you want to have. You can find below a list of items that need to be taken into account when you take such a decision:
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/mindorks/send-device-to-device-push-notification-using-firebase-cloud-messaging-without-using-external-769476c79ffd?source=search_post---------63,"There are currently no responses for this story.
Be the first to respond.
Firebase Cloud Messaging (FCM) is a cross-platform messaging solution that lets you reliably deliver messages at no cost. It allows you to send push notification directly from Firebase console or with an app server or some trusted server where the logic runs. This article will tell you how to send push notification between devices like most of the chat apps without using any external server.
First, we need to create a new Android Studio project and add the relevant dependencies to it.
The first is a simple one — set up Firebase in your project. You can find a good tutorial here. In order to use FCM, you’ll need to add the following dependency to your app:
Now we need to create a Firebase service: MyFirebaseMessagingService. This service will used to receive and display the notifications. Services do not have any visual interface and are used for background running operations. To create a service, right click on the app folder and select New -> Service -> Service.
Type in your service name and click Finish button to create the service.
Go to the AndroidManifest.xml file and update your service declarations under the application tag. Also, add INTERNET and CLOUD TO DEVICE MESSAGING permissions so your app can interact with FCM server.
MyFirebaseMessagingService extends FirebaseMessagingService class in order to receive messages from the FCM server. This service handles the reception and display of notifications. OverrideOnMessageReceived() function within the service so that it will be called whenever a new notification message is received.
First you need to get your Server Key from Firebase console using the following steps:-
Implement the Sending Logic
Sending push notification requires just an HTTP post request to FCM server with the following request properties:
Method Type: POST
URL: https://fcm.googleapis.com/fcm/send
Headers:
Body:
Keeping in mind the above structure, you will first create a JsonObject of the notification body within your activity class. This object will contain the receiver’s topic, notification title, notification message, and other key/value pairs you wish to add.
We will be using volley library to make a network request to FCM server, then the server will use the request parameters to route the notification to the targeted device.
With that, you have completed building your app. You can start sending push notifications between devices without using any external server. Always ensure that you get the topic of the recipient correct else the notification will not be delivered. If you did everything right, you will have a similar result to this.
github.com
Thanks for reading! If you enjoyed this story, please click the 👏 button and share to help others find it! Feel free to leave a comment 💬 below.
Let’s connect on Twitter, LinkedIn
Our community publishes stories worth reading on Android…
389 
10
389 claps
389 
10
Written by
Software Engineer @ Grofers | Google-certified Android Developer | Open Source Contributor
Our community publishes stories worth reading on Android Development
Written by
Software Engineer @ Grofers | Google-certified Android Developer | Open Source Contributor
Our community publishes stories worth reading on Android Development
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@shraybansal/create-operation-for-external-rest-in-visual-builder-cloud-service-1084577d0963?source=search_post---------64,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shray Bansal
Sep 22, 2017·2 min read
Oracle Visual Builder Cloud Service allows users to expose external REST services as custom business objects through Business Object Provider(BOP) templates, which enable users to manage the VBCS interactions with external data using REST CRUD operations.
In a previous blog my colleague Shay Shmeltzer detailed how to configure BOP to read repository data from GitHub REST APIs in VBCS. This blog will serve as an extension to the earlier blog and highlight the process to configure create operation in VBCS in order to create new repositories in GitHub using the Create Repository REST API https://api.github.com/user/repos
For the purpose of the blog the GitHub BOP was been configured to read name and description of repositories and subsequently allows to users to create new repository by giving a name and a description on a Create page. This BOP uses reduced set of properties owing to the fact these are the minimal set of attributes required to create a new repository in GitHub. The EntityProvider snapshot highlights the configuration of Repository Entity with fields- name, which acts as a key, and description.
Now modify the Operations Provider to configure Read and Create operations for the Repository Entity as shown in the code
Notice that we have added the bop/js/api/operation/OperationInput api reference in the define block for the POST implementation.
The postUri variable corresponds to the GitHub Repository Create API
The user has to create a Resource in order to add the API to the whitelist resources associated with the BOP at server side.
Just as the OperationBuilder required an Operation of Type READ , so the OperationBuilder requires the Operation of type CREATE
The implementation of the REST call is done through Operation.perform(..) method, and the REST call leverages the BOPAuthenticator in order to use the built in authentication mechanism provided by VBCS.
After configuring the BOP, click on Save Change and follow the steps given in the video below to verify the BOP and to visualize the Create operation in VBCS by creating a Business Object.
With this we have successfully configured a sample Business Object Provider with a Read and a Create Operation for repositories in GitHub.
To know more about Oracle Visual Builder Cloud Service visit the link https://cloud.oracle.com/ApplicationBuilder
The views expressed in this post are my own and do not necessarily reflect the views of Oracle.
Duke MBA Candidate||Former Product Manager@Oracle|| Trying to make technology easier|| @BansalShray
6 
6 
6 
Duke MBA Candidate||Former Product Manager@Oracle|| Trying to make technology easier|| @BansalShray
"
https://medium.com/@shraybansal/reading-nested-json-data-from-external-rest-in-visual-builder-cloud-service-1feb9c6d520c?source=search_post---------65,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shray Bansal
Oct 10, 2017·3 min read
Oracle Visual Builder Cloud Service allows users to expose external REST services as custom business objects through Business Object Provider(BOP) templates by reading data from external REST services.
In a previous blog my colleague Shay Shmeltzer detailed how to configure BOP to read repository data from GitHub REST APIs in VBCS. This blog will serve as an extension to the earlier blog and highlight the process of reading nested JSON data from a REST service created in Apiary.
The data is of the form given below where “choices” attribute has an array of objects that indicate the choice given in the survey for a particular question and corresponding votes received. The REST API is accessible at https://private-7a7e9-testapi3209.apiary-mock.com/questions
For more information on designing and prototyping REST APIs with Apiary visit https://apiary.io
To create a Survey Results Business Object, configure a Business Object Provider(BOP) in the Extensions page of Application Settings. The EntityProvider snapshot highlights the configuration of Results Entity with fields- choice, which acts as a key, and votes.
Save the Changes. Navigate to RESTOperationProvider to configure the parser that will parse the results of the nested JSON as shown in the code
Lets look at the important facets of this code in greater detail. Notice that we have added the operation/js/api/OperationResult api reference in the define block to represent the data from the AJAX call
The baseUri variable corresponds to the Apiary REST API
Configure the AJAX Object you will leverage during the REST call for the resource.
The next step is to leverage the VBCS built-in authenticator to invoke the REST call using the AJAX object created in the previous step. With the help of this authenticator the user will be able to configure security required for the REST call such as Basic Authentication while configuring Business Object at runtime. Note that this API does not require any credentials.
The _parseDetails method allows you to parse the response of the AJAX call
Depending on the structure of the JSON response the user can modify the above block to parse the results. In the Apiary REST API we need to iterate over the array of objects for “choices” represented by
Save the changes and navigate to the Business objects page in the Data designer. Follow the steps in the video given in this blog to create a “Results” Business Object.
With this we have successfully configured a sample Business Object Provider to read nested JSON payloads from External REST Services
To know more about Oracle Visual Builder Cloud Service visit the link https://cloud.oracle.com/ApplicationBuilder
The views expressed in this post are my own and do not necessarily reflect the views of Oracle.
Duke MBA Candidate||Former Product Manager@Oracle|| Trying to make technology easier|| @BansalShray
See all (6)
6 
1
6 claps
6 
1
Duke MBA Candidate||Former Product Manager@Oracle|| Trying to make technology easier|| @BansalShray
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/oracle-saas-paas/how-to-embed-a-external-url-into-oracle-sales-cloud-using-page-integration-65579549622e?source=search_post---------66,"There are currently no responses for this story.
Be the first to respond.
How to embed a external URL into Oracle Sales Cloud using Page Integration
This post talks about embedding a external URL right into the sales cloud. This will open as within in the context of Sales Cloud rather navigating to a new tab. Which is pretty awesome as it feels the part of the CRM this way. Here are the steps:
"
https://medium.com/@josh.m.l.wood/external-application-config-with-spring-cloud-kubernetes-f963d74450d?source=search_post---------67,"Sign in
There are currently no responses for this story.
Be the first to respond.
Josh Wood
Aug 2, 2019·11 min read
blog.joshmlwood.com
A common pattern when deploying applications to a development, staging, and production environment is to build a jar or docker image one time, then supply different configuration values in the deployment for each stage. This configuration could be a Spring profile in separate yaml documents, additional properties files, environment variables, or some other configuration mechanism.
When deploying to Kubernetes, configuring a Spring application becomes a little more difficult. The option still exists to run our application with a profile, and just “enable” that profile specific application.yml. The downside here is all of our config was deployed in the docker image, so updating it requires a new deployment. We still have the option to configure environment variables in the Kubernetes deployment yaml and have Kubernetes map the provided value into the container created to run our application. A nicer option that integrates directly into the Spring bootstrap process is utilizing the Spring Cloud Kubernetes Config and a ConfigMap stored in the cluster. This allows us to define an environment specific application.yml in a Kubernetes ConfigMap and Spring will automatically find and merge the data into existing configuration properties. The added bonus to this approach is that changing the configuration only requires updating the ConfigMap and restarting the Spring context to read the new properties.
Note: this post will require access to a Kubernetes cluster or Minikube running locally and will expect you to have some operational knowledge of Kubernetes. If you do not have access to Kubernetes, you can install Minikube by following the official instructions from Kubernetes.io.
We need a project to start with so head to Spring Initializr to generate a new project. Once there, select Spring Web Starter as the only dependency, and ensure Java, version 1.8, and latest spring versions are set. For our purpose, we don’t need anything additional from the project creation, there will be other dependencies to add later. We are electing web support so that we have an API we can test rather than relying on only application logs for validation; it’s more fun to see a project working when calling an API rather than reading logs. If you do not wish to set up a project from scratch then you can clone the demo repo and navigate to the spring-cloud-kubernetes-config-demo directory to see the completed project.
Now that we have a basic project with web support, we need to add one more dependency to allow Spring to read ConfigMaps and Secrets from Kubernetes: spring-cloud-kubernetes-config with groupId: org.springframework.cloud. If you're using Maven, your pom should look similar to this (parts omitted for brevity):
If you’re not familiar with Spring Cloud, we have one more thing to add to the pom to get this to compile correctly, the dependency management section to allow spring cloud dependencies from a specific release train to be integrated into our project:
So what does it do for us? The spring-cloud-kubernetes-config dependency is one of the spring-cloud-starter-kubernetes family. It hooks in to the Spring bootstrap process to provide an additional properties source from a Kubernetes ConfigMap and Secret that share the same name as our spring.application.name configured in the application.yml. Additionally, it doesn't require a bean or any extra configuration in the project; an autoconfiguration is responsible for instantiating the configuration beans which makes it transparent to set up once the dependency has been added to a project. More info on the Spring Cloud Kubernetes project can be found on GitHub.
Awesome! We can call it a day now. We’ve done it, and it’s glorious!
Well, not quite. We still need to use the dependency and add some configuration to validate that it is working as expected. Then of course, we need to deploy to Kubernetes. So, next up…
Let’s add some stuff to our application.yml. First, make sure that the application name is set to spring-cloud-kubernetes-config-demo so that your project will match the demo and this tutorial. Now add a couple of application configuration keys to the yaml:
In this snippet, the config key is set to ""Default value"" under all circumstances, but the second key environmentVariable is defined to default to ""Default value"", or if ENVIRONMENT_CONFIG is defined on your host or the application host, then that value will be used instead.
We’ll use these configuration keys to demonstrate how Spring will map data from our Kubernetes cluster and container environment into the application at deploy / startup time. Now we need to actually use these somewhere so we can see how to configure them through Kubernetes.
The simplest way to verify these values will be to create a controller that also logs the values out during construction. This can be as simple or complex as you desire, but for my purposes, the example below will suffice.
At this point, the app should run and have a single endpoint available at http://localhost:8080/ which will return a map of response data containing the dynamic values mapped in at construction time.
Now we can create some of the Kubernetes objects we’ll need to deploy the application into a Kubernetes cluster.
We will take advantage of the default configurations used by the Spring Cloud Kubernetes Config dependency; the default is to look for a ConfigMap with the same name as our application if we can detect that the application is running within Kubernetes. So, we need to create a yaml to represent our ConfigMap. Since we will be using the default configuration for Spring to search for a ConfigMap of the same name as our Spring application name, make sure that the ConfigMap name here matches the name defined in your application.yml. This example will not work otherwise.
Technically this is all that’s needed but it doesn’t provide any configuration, much less anything that is useful to our Spring application. We can add a top level data key to the yaml where we can play any configuration's we'd like.
The data key in this example has a couple of important features; first it has a single nested key, in this case “application.yaml” and that key uses pipe hyphen (|-) to indicate all the values nested under it is a block and represents a single multi-line value. See (the Block Chomping Indicator yaml-multiline.info)[https://yaml-multiline.info/] for additional information. The important part is that it allows us to define all of the custom values of our application.yml in a single key in this ConfigMap. The other major point which is not obvious in this example is that since application.yaml is the only key in the data section, the name of it doesn't matter. We could in fact just have this:
The name in this case does not matter. However, if we wanted to use our ConfigMap for more than just storing an environment specific application yaml, such as an environment variable for a bash script used to start our application, or some other important configuration that’s required before Spring starts up, then we must name the key application.yaml. If we do not, then spring-cloud-kubernetes-config will be unable to find the relevant data to map in to Spring’s composite property source, thus we will not have the expected configuration values applied to our application at startup time.
While we’re at it, we can also play with creating another ConfigMap that stores a value which we can later use to map in to our application via an environment variable. We can use a Kubernetes Deployment to actually inject the value from our ConfigMap into our running container which will be shown below.
Since we have configs defined, we can move on to creating the deployment. As noted at the start of this post, this is assuming you have some Kubernetes knowledge already. So, we’re going to configure a very simple deployment that should allow access to the application without needing additional infrastructure such as Ingress, or anything more complicated than having network access to the IP of the node the application is deployed on. To achieve this, we’ll create a deployment that exposes the port of our application, and a service that configures Kubernetes to expose a Node Port and map that port on our node back to the application.
We’ve called our app here demo, and we're expecting Kubernetes to find the docker image for the application locally. This means we have to build the source from the same docker context that Kubernetes is using. If you are using Minikube, you can easily attach your current terminal to the docker context from Minikube by running the following command.
Now that our terminal should be configured to run docker commands in the Minikube environment, we can build the application locally and then attempt to deploy it. I’ve configured maven in the sample project to include the Spotify Dockerfile plugin so we can easily build our docker image with familiar tooling.
With this plugin, we can run ./mvnw clean compile package dockerfile:build and it will build our app into a docker image named and tagged jmlw/spring-cloud-kubernetes-config-demo:latest.
Before moving on, we need to define one more thing for our app so that our Spring Cloud Kubernetes Config dependency is able to do its job. By default current versions of Kubernetes enable RBAC (role based access control), so you’ll need to grant our deployment explicit access to the Kubernetes APIs that it will need to discover ConfigMaps and Secretes that it should be allowed to read.
With this rbac.yaml, we can grant permission to the service account ‘demo-service-account’ read access to pods and ConfigMaps. If you need or want, you can also add “secretes” to the list of resources in the role definition.
Once the docker image is built, we can use kubectl to apply the yamls we've defined to our Kubernetes cluster and watch as the application starts up and configures itself. To actually deploy, you can create the yaml files listed above and then run kubectl apply -f rbac.yaml environment-variable-config.yaml app-config.yaml deployment.yaml.
Otherwise, if you’re following the source from the demo-projects repository, then you can apply the same resulting yaml manifest by kubectl apply -f deployments/ which will deploy all yamls within the deployments directory.
Note: if you’d like to skip building the app locally or within your Kubernetes cluster, you can switch the ImagePullPolicy to ‘Always’ which will cause Kubernetes to pull ‘latest’ from Dockerhub
First, make sure the pod we deployed has started up and is healthy:
If the app is not running, debugging why the application is failing to start is outside of the scope of this post. However, the most like causes are 1) missing the env variable defined in the deployment which depends on a reference to a named ConfigMap, 2) the docker image is missing or is incompatible with your host, or 3) Java/Spring is failing to start which is likely a configuration issue.
Now that the app has started, check the logs from Kubernetes:
You should see some log statements printed out from the construction of our controller similar to this:
Now we can actually call the endpoint of our application. If you’re running in Minikube, you can just run the following which will call the root endpoint on the URL of our service name demo within Minikube.
On top of the basic option of mapping in a single ConfigMap’s application.yaml key or the only key of a ConfigMap, you can configure Spring to search for additional ConfigMaps from namespaces outside of the current namespace. There are other configuration options available that you can find on GitHub in the Spring Cloud Kubernetes repository. One interesting option that I have yet to try in a production environment is using the spring.cloud.kubernetes.reload.enabled value set to true. This allows Spring to hot-reload configuration properties dependent on the spring.cloud.kubernetes.reload.strategy, which could be refresh, restart_context, or shutdown.
A lingering question you might have is, why not just map in environment variables like ENVIRONMENT_CONFIG above? Honestly, it's just as easy in most cases unless your application.yml has special configuration for nearly every key. The biggest drawback of mapping these in via environment variables is that all of your configurations are defined three times; once in the application.yml, once in the deployment.yaml, and once in the configmap.yaml. That leaves three potential places for typos that could cause incorrect configuration or worse, application crashes. Otherwise, relying on a little spring magic, you can just use the ConfigMap and application.yaml key to provide the configuration, and the keys do not need to exist in the packaged appliation.yml either. In my mind, this is slightly higher cognitive overhead for the huge benefit of not duplicating, misspelling, or failing to update configuration values.
As always, a full working demo for this can be found in my demo-projects repository. Any questions or problems, feel free to open an issue and I’ll review as quickly as possible.
Happy coding and navigating the Kubernetes sea with a little Spring in your Boot!
Software engineer and home brewer
Software engineer and home brewer
"
https://medium.com/@conycarrasco/saving-your-data-in-a-cloud-storage-or-in-external-hard-drives-883c533d5fe4?source=search_post---------68,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cony Carrasco
Nov 10, 2014·3 min read
Saving your data in a cloud storage or in external hard drives?
As future teachers we are going to manage important information such as: test, books, school information, etc. Thus, is so important for us to find out how to maintain our data safe. Nowadays there are many options to get it done, but which of those is options is the best? Do we really care about maintaining copies of our vital information? In this essay, I will describe the different ways in which we can maintain copies for our information. I will focus on online backup and external hard drive backup, its advantages and disadvantages, prices, capacity, etc. The idea is to provide a broad view of how to back up our information, in order to take an informed decision. See more…
How many of you have lost vital information by a malicious virus or a malware, and just in that moment you realize how important it is to maintain a system which allows you to maintain your data safe. However, how many of you have deleted data by mistake? Or have pressed the wrong button? Or have spilled water into it? According to a study made by the computers company associations, in Canada, has found that “accidental deletions wreak 30 times more destruction on important data compared to viruses, becoming the leading source of corporate data loss” for that reason is so important to know how many backup method exists and how to use it? See more…
External Hard Drive back up
One of the advantages of using an external hard drive is that they are not difficult to use; you do not need much training on that. Currently they are extremely portable; therefore, easy to carry wherever you go. Another advantage is that they are very affordable you can buy one external hard drive for less than $100 dollars, with one terabit of capacity. You can save your data easily and faster. It takes just minutes to get your data transferred. You do not need subscriptions once you purchase a hard drive and you are in total control of your data. See more…
One of the disadvantages on backing up your data to an external hard drive is that, you can lose your external disk or even worse, someone can steal your external disc. You also need to be connected to your computer in order to get access to your data and at the very end an external hard drive is similar to a computer and exit the possibly that can fail at any moment. See more…
Online Back up
On the other hand, we have online backup. That is a very good option at the moment of safe keeping your data. However, not many people know how to use it, even though online backup services are generally easy to use, even if you have never backed up your data before. So for those who do not know how to use it I will describe the main advantages and disadvantages. A very good reason to use an online backup system it is because they are extremely affordable you can get till 2 GB accounts for free, and if you need more GB capacity you can get a subscription for less than $5 dollar per month. Another good reason is that you do not need to be connected to any device, if you want to get access to your data, you just need an Internet connection and you will be capable to get access to your files in any part of the world. However, there are some weaknesses that you need to be aware of before you get an account. The first one is that the initial backup could take at least one week, depending on your internet connection and your computer, but after a week the process would be almost instantaneous. The second disadvantage may be that it has ongoing costs. If you need more than two GB capacities you will need to pay a monthly fee or maybe once a year. The third disadvantage and the most important, I think, is that you need to trust that the company you are giving the information would treat your information with care, so that you can take a look to some web pages that give important information in order to take a decision. See more…
In conclusion, I think, that we need both of the methods above explained in order to maintain our information safe. We need an external hard drive, in case our computer might fail and we also need a cloud storage that would bring us the freedom of maintaining our information safe at any place that we need it. See more…
"
https://medium.com/@niravshah2705/google-cloud-registry-gcr-with-external-kubernetes-35ef620ecf92?source=search_post---------69,"Sign in
There are currently no responses for this story.
Be the first to respond.
NIRAV SHAH
Oct 15, 2020·2 min read
To connect to GCR from an environment other than GCP, you add an ImagePullSecrets field to the configuration for a Kubernetes service account. This is a type of Kubernetes secret that contains credential information.
Note:A GCP service account is different from a Kubernetes service account.
You can create the file with the following script. The script creates the necessary Google Cloud Platform (GCP) service account and gives it access to the registry.
Then create the secret and specify the file that you just created:
where the values must be as follows:
$SECRETNAME: An arbitrary string to serve as the name of the secretdocker-server: Must be set to “https://us.gcr.io” (or some variant, a subdomain may be required depending on your availability zone)docker-username: Must be set to _json_keydocker-email: Must be any well-formed email address (not used, but required)docker-password: The contents of the json key file that you created in the previous script
Note:The command above only creates the secret in the default namespace. You will need to specify -n and create a secret for each namespace that your pods are in, because pods can only reference the image pull secrets in their own namespace.
You can now add the secret to your Kubernetes configuration. You can add it to the default service account with the following command:
We are using this approach withing google cloud too. By default GKE has access of gcr on same project cluster. Howerver for other project we can use this process or we can use GKE cluster permission too.
Working as Cloud Architect & Software enthusiastic
Working as Cloud Architect & Software enthusiastic
"
https://medium.com/@michael-shan/agreed-the-external-control-nature-of-the-cloud-might-lead-to-various-side-effects-and-record-6450d220eae6?source=search_post---------70,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Shan
Aug 25, 2016·1 min read
nicolabk
Agreed. The external-control nature of the cloud might lead to various side effects, and record management is absolutely one of them. In a sense, cloud provides virtualized “on-premises” resources, such as infrastructure, platform, or software, so many traditional methods used to mange on-premises resources can continue to be employed in the cloud environment. Take database management as an example. The data modelling, data backup, and data encryption strategies can be applied to both one-premises or in-cloud database systems. On another hand, extra care needs to be taken for the data stored in the cloud because it is externally hosted and subject to potential malicious use by other parties. To mitigate the threats, one measure to consider is to encrypt the data, both its transmission and its storage. Anyway, heavy encryption may have performance impacts and thus organizations need to find a good balance between encryption and performance.
Another area to be considered is the establishment of corresponding laws and policies, which I briefly discussed in my blog. The cloud computing technology is fast developing, and as I mentioned, its technical standards are still being defined, not to mention the laws and policies needed to manage the technology. However, as the cloud computing technology becomes more and more mature, I believe corresponding laws and policies will be established, not only in individual countries, but also internationally across multiple countries, due to the global nature of cloud infrastructure.
"
https://medium.com/thermokline/kubernetes-v1-22-ends-cloud-provider-loadbalancer-lock-in-80ed7907695e?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
Users of Cloud Provider provisioned Kubernetes have been locked into using the Cloud providers LoadBalancers for external access to their applications. This changed in v1.22, a new feature called LoadBalancer class allows 3rd party solutions to selected as an alternative to the default Cloud LoadBalancer.
Kubernetes is integrated with the Cloud providers infrastructure using a “cloud controller”. Just as the control plane components are not visible in a Cloud provider provisioned cluster, the cloud controller is also not visible. The cloud controller integrates IAM, Storage, Networking and of course their Load Balancer as the default Load Balancer when a service is configured for a LoadBalancer. Most cloud providers publish their cloud controller as open source, that’s the only way to understand what its doing.
When LoadBalancer Class is present in the service spec, the Cloud Controller ignores the request for a Service LoadBalancer and the identified Service LoadBalancer controller responds and configured access to the service.
An example of this capability is provided by PureLB and EPIC from Acnodal. PureLB is installed on the Cloud provider provisioned cluster. EPIC, the platform that provides the LoadBalancer function by creating an Envoy proxy and orchestrating networking, routing, security and certificates is located outside the Cloud provider in a colocation facility.
Once created the public address of the LoadBalancer is advertised, requests are attracted to the EPIC proxy engine and forwarded to one or more kubernetes clusters where the services are associated with POD’s. The Acnodal platform uses Envoy providing significantly greater functionality when compared to generic Public Cloud LoadBalancers. To learn more about this solution, check out Acnodal at www.acnodal.io
LoadBalancer Class, as well as other new features such as IPv6 Dual Stack unlock exciting networking innovation for Kubernetes.
Going deep on Kubernetes networking
166 
3
166 claps
166 
3
Orchestrated & configured networking is fundamental to k8s operation, we talk about it.
Written by
Tech enthusiast, infrastructure specialist, leader & engineer
Orchestrated & configured networking is fundamental to k8s operation, we talk about it.
"
https://medium.com/@albert.brand/remote-to-a-vm-over-an-iap-tunnel-with-vscode-f9fb54676153?source=search_post---------72,"Sign in
There are currently no responses for this story.
Be the first to respond.
Albert Brand
Feb 14, 2020·4 min read
SSH-ing to a Google Cloud VM without an external IP address is possible using a tunnel. But what if you want to edit code remotely using VSCode over that tunnel? This solution is a little gem that I could not find on the interwebs.
With VSCode’s Remote-SSH extension you can develop and run code on any remote machine over SSH. This is a great option when you need to develop on larger, faster, or more specialised hardware than your local machine — for instance, building a deep learning network with GPU or TPU acceleration.
Google Cloud offers to create virtual machines to meet such needs. By default Compute instances are configured to get an external IP address assigned. But if you want to have a more secure VM, you can choose to assign none:
After creating your instance (mine is boringly named instance-1), you can SSH to it using Identity-Aware Proxy TCP forwarding. IAP handles the authentication part and tunnels data traffic from your local machine to the remote machine in an HTTPS stream.
By following the docs you create the tunnel and SSH to your new remote machine in one go with the following command:
(You can even leave out the --tunnel-through-iap as the gcloud CLI automatically detects there is no external IP assigned to your instance.)
So now we want to connect VSCode to this VM via the IAP tunnel, as editing code remotely using the terminal feels a bit… outdated. But how?
Under the hood, the gcloud compute ssh command starts a SSH command with the correct arguments to connect over IAP. The Remote-SSH extension can import such a command (parsing the command-line arguments) and save it in local SSH config storage.
To get the SSH command that gcloud compute ssh uses, you can run it in ‘dry run’ mode:
(This is my output on a Mac, your result will probably differ).
Let’s try to parse it in Remote-SSH. In VSCode, press Shift-Command-P and find & run Remote-SSH: Add a new host:
But alas, I get the following error:
It seems that the ProxyCommand argument is not picked up correctly. To check if the syntax is OK, I run the dry-run output in a shell and get a similar error:
Looking at the dry-run command, the ProxyCommand is supposed to contain the command that starts the IAP tunnel. However the argument is in the wrong format. By changing the ProxyCommand argument to the double quoted syntax ProxyCommand=""..."" it starts to work!
However, this command is still not properly parsed by the Remote-SSH extension. You need to strip the /usr/bin/ part from the command as well, and finally it will import correctly:
If you open the SSH config file after you saved the configuration it will show up as:
So, you can skip this import next time and directly add the config in this format. Well 🤷‍♂️.
Run Shift-Command-P and find & run Remote-SSH: Connect to server (or click on the green icon on the bottom left). Wait a while, choose a root folder and you are finally connected using IAP tunnelling:
Happy coding!
Senior consultant @ Xebia
See all (239)
148 
6
148 claps
148 
6
Senior consultant @ Xebia
About
Write
Help
Legal
Get the Medium app
"
https://blog.openbridge.com/export-tracking-data-from-salesforce-marketing-cloud-8a0a4c1f37dc?source=search_post---------73,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Thomas Spicer
Jul 19, 2017·10 min read
Salesforce Marketing Cloud is used for sales, customer service, digital marketing automation, loyalty, and CRM efforts. Data resident in Salesforce Marketing Cloud email marketing contains rich insights into digital marketing activities.
This data is part of a puzzle that allows you the ability to understand the customer journey. The better you can understand this journey the greater the opportunity to deepen customer engagement using data insights.
The Salesforce Marketing Cloud email data provides an insights backbone to the customer relationship management software. If you are a Salesforce Marketing Cloud Email customer and spending time with marketing automation then you really should be tapping into this data!
This article describes the steps to export tracking data from Salesforce Marketing Cloud: Email Studio, marketing software to an external data warehouse via SFTP.
You can also set up custom data extensions extracts in addition to the default tracking extracts. Why SFTP? There are limits to using the ExactTarget API for exporting bulk data from the ExactTarget marketing cloud. As a result, we suggest using the bulk export capability to support any external ExactTarget analytics efforts.
Here are the types of ExactTarget email extracts that are available:
A file location is the SFTP location where you want to deliver your tracking export files. To do this you need to go into Salesforce Marketing Cloud Email Studio:
Once in Email Studio, you go to Admin -> Data Management -> File Locations
Within File Locations enter the setup details of your External SFTP Site delivery location:
You will need to set up a dedicated File Location for each Tracking extract. For example, in the URL of the setup, you enter the name of the tracking extract being sent. In this case, we are exporting sends so you would use pipeline-01.openbridge.io/salesforce_sends/
If you were export opens then you would create a new File Location for opens you would use pipeline-01.openbridge.io/salesforce_opens/
You would do the same for the Name field. Use something description like Openbridge SFTP Server Sent to help you identify which extract this location is set up for.
The Username and Password would be provided to you by Openbridge so check with your account team for that.
Here are a couple of examples that show a location for sent and open tracking extracts. Notice the different locations for URL:
Depending on the number of exports, you will likely have a collection of file locations for each tracking extract you want to deliver:
This structure ensures that the data is properly organized and can be processed, routed, and loaded correctly into your warehouse. If are dumping a “mix-n-match” set of data to single location schemas and data types will be different resulting in data failing to load.
Once complete with each location, select Save.
Step out of Email Studio and go to journey builder ExactTarget -> Automation Studio
The first step is in Automation Studio is to define extract and file transfer “Activities”. Think of them as the tasks you need to have performed.
In Automation Studio select Activities
Then select Create Activity
You will be presented with a few different options. The first step will be to define a data extract from Salesforce. Select Data Extract
You will be presented with a screen where you define the properties of the Data Extract activity.
Select Next
You will want to name this specific to the type of data being extracted. You will also want to set up the filename pattern in a manner that is unique to the export and the data:
%%Year%%%%Month%%%%Day%%%%Hour%%%%Minute%%_send.zip
Select Next
You now get to define your extract
For Extract Type, we are using Tracking Extracts. Normally you will use a Rolling Range of 1 Day. This would reflect a daily push of all the prior days' transactions from Salesforce.
Make sure you set the character encoding to UTF-8 and the delimiter to the comma. The format should be set to CSV. Since we want the sent emails we need to select the Extract Sent option. To be safe, always select Quote Text. This will help ensure your comma delimiters are set for more complex use cases.
Select Next
Double-check your work. Look good? Great, you have just defined a Data Extract activity! Move onto the next step.
Select Finish
The next step is to create a File Transfer activity. This describes the mechanics of transferring extracts to the File Location you defined in Step 1.
Click Create Activity
You will be presented with a modal similar to the last step. This time you will select File Transfer
Select Next
In this step you need to define the naming pattern of for the delivery similar to the name used in the extract:
%%Year%%%%Month%%%%Day%%%%Hour%%%%Minute%%_send.zip
Also, this is where you select the File Location for your sends that you created in Step 1.
Select Next
Everything looks good, right?
Select Finish
Automation defines a process that executes one or more activities. This is the last step in configuring your exports.
Instead of clicking Activities, click Overview
Then click Automation
You will be presented with a modal. You will want to select Schedule
You will be presented with a workflow tool. This is where you configure the activities you created into a workflow Salesforce can execute
The two activities we are using are Data Extract and File Transfer
Drag one of each to the workflow window. The first should be the Data Extract and then the File Transfer. These will be empty templates. You need to choose the activity you want to run for each.
Select Choose
Select the Data Extract activity you defined for sends.
Select Done
Next, you link the File Transfer activity.
Select Choose
Select the File Transfer activity you defined for sends.
Select Done
Great, both are now configured!
The last step is to define a schedule. This describes when the Automation should be run. Since we want this to run daily we set the start date and tell it to repeat daily at 5 AM.
Once you complete the schedule, Success! Your extract will now be sent every day at 5 AM to be loaded into your warehouse.
Did you want to add more extracts? Repeat Steps 1 and 2 for each extract. Then add them to your automation.
That’s it! Your export data should be flowing. You can use the insights to create new customer experiences in a journey builder, refine social media strategies, or round out your cross-channel analytics efforts.
The Openbridge integrations provide a code-free, fully automated connector.
No need to hire a developer or build your own integrations. By using a pre-built, fully-managed Salesforce connector, you can increase efficiency, reduce labor requirements, and improve response time to customers.
D Want to discuss how Salesforce Marketing Cloud data for your organization? Need a platform and team of experts to kickstart your data and analytic efforts? We can help! Getting traction adopting new technologies, especially if it means your team is working in different and unfamiliar ways, can be a roadblock for success. This is especially true in a self-service only world. If you want to discuss a proof-of-concept, pilot, project or any other effort, the Openbridge platform and team of data experts are ready to help.
Reach out to us at hello@openbridge.com. Prefer to talk to someone? Set up a call with our team of data experts.
Visit us at www.openbridge.com to learn how we are helping other companies with their data efforts.
https://www.openbridge.com
13 
3
We cover ELT, ETL, data ingestion, analytics, data lakes, and warehouses Take a look.
13 claps
13 
3
Code-free, fully-automated ELT/ETL data ingestion fuels Azure, Athena, Redshift Spectrum data lakes or AWS Redshift and Google BigQuery cloud warehouses
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jmcogdell/when-was-the-last-time-you-backed-up-your-website-jeans-writing-on-wordpress-com-e9d8be2be28d?source=search_post---------74,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jean M Cogdell
Jun 21, 2017·2 min read
We all back up our computers, either to the “cloud” or to an external hard drive. But what about your blog?
What would happen if your blog content vanished? Years of hard work gone in the blink of an eye. How could that happen? WordPress has been around for eons! Ever heard of MySpace? Haven’t? There is a reason.
Cry, eat Bluebell Ice cream and drink Margaritas. Not in any particular order. Then I’d sit down and begin to vent. I mean, write.
Of course, the best thing would be to avoid such a catastrophe.
Click and read Suzanne’s terrific step-by-step instructions on how to backup your website. Don’t lose that amazing website you’ve slaved over for months.
Please head over and “like” my Facebook page at Facebook at jeanswriting . Or to connect with me, click the “write me” tab. Don’t forget you can follow me on StumbleUpon, on Twitter @jeancogdell , and Amazon.com.
Please stop by and say “hey!” I’ll leave a light on.
Originally published at jeanswriting.com on June 21, 2017.
Author, booklover, wife, mother and grandmother. Check out her website at www.jeanswriting.com
See all (142)
Author, booklover, wife, mother and grandmother. Check out her website at www.jeanswriting.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/origen/deploying-a-prestashop-ecommerce-the-cloud-friendly-way-part-4-d73f086c9035?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
This article is a continuation of Deploying a Prestashop ecommerce: the cloud friendly way — Part 3
This step is short to explain but it can be a huge headche to set-up. Also is totally optional and not required for your Prestashop to work. If it is yout first time you can skip it. 😉
To make this Prestashop installation even more cloud friendly we can use an external FileSystem for some of our files — images, media, etc-
We can use S3FS to mount an AWS S3 bucket as a FS. You can follow the instructions in here to install and configure S3FS. I would use this external mounted FS to store all the static files like images and other media.
When creating an S3 bucket for this purpose remember to:
See Deploying a Prestashop ecommerce: the cloud friendly way — Part 5
We are a small devshop of skilled and passionate engineers…
2
2
We are a small devshop of skilled and passionate engineers and designers. We love working on new challenges. Please do not hesitate to contact us.
Written by
Aerospace engineer working on software development.☺️ Building great products at @theorigenstudio. 🍕and 🚵‍♂️⛷ passionate
We are a small devshop of skilled and passionate engineers and designers. We love working on new challenges. Please do not hesitate to contact us.
"
https://medium.com/@jeffrey-scholz/thanks-for-writing-this-a-couple-more-points-i-would-add-95ac1c8ad5a0?source=search_post---------76,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeffrey Scholz
·Nov 19, 2020
Tommy Blanchard
Thanks for writing this! A couple more points I would add.
1. These tools can't replace intelligent domain knowledge yet, such as joining external data. I've gotten better scores than cloud autoML solutions that had logitudinal data just by rotating map a few times so xgboost could split regions easier. For niche fields, I'm sure more clever things apply. That said, annoying feature engineering like trying to rescale the data differently I'm glad to see automated away.
2. There are plenty of datasets in the real world that are simply not possible to extract a useful signal from (in fact, I've seen competitions where it's impossible to do marginally better than a baseline that effectively predicts the average). You need some expertise to recognize when these situations occur.
3. Building off of point 1, I don't think the tools could always detect data leaks. Random splits don't ensure you don't have a data leak if your data has natural clusters in it.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1 clap
1 
1
Ex-Senior Software Development Manager at Yahoo Inc. Currently building Donkeverse. Please connect on LinkedIn! https://www.linkedin.com/in/jeffreyscholz/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/adidoescode/internal-and-external-connectivity-in-kubernetes-space-a25cba822089?source=search_post---------77,"There are currently no responses for this story.
Be the first to respond.
As you are making your way through all the stages of your app’s development and you (inevitably) get to consider using Kubernetes, it is time to understand how your app components connect to each other and to the outside world when deployed to Kubernetes.
Knowledge you will get from this article also covers “services & networking” part of CKAD exam, which currently takes 13% of the certification exam curriculum.
Kubernetes services provide networking between different components within the cluster and with the outside world (open internet, other applications, networks, etc)
There are different kinds of services, and here we’ll cover some:
NodePort service maps (exposes) port on the Pod to a port on the Node. There are actually 3 ports involved in the process:
Selectors are the way to refer (link) service to a certain set of pods.
A set of pods gets selected based on the selector (in almost all cases, pods from the same deployment), service starts sending traffic to all of them in a random manner effectively acting as the load balancer.
If mentioned pods are distributed across the nodes, service will be created across the nodes to be able to link all the pods. In case of multi node service, service exposes same port on all nodes.
In the case of an application consisting of multiple tiers deployed to different sets of pods, a way to establish communication between different tiers inside the cluster is necessary.
For example, we have:
Each of above mentioned 18 pods have their own distinct IP addresses, but making communication that way would be:
ClusterIP service provides us with unified interfaces to access each group of pods — it provides a group of pods with internal name/IP.
ClusterIP is default type of service, so if service type is not specified, k8s assumes ClusterIP.
When this service gets created, other applications within the cluster can access the service through service IP or service name.
In short, a LoadBalancer type of service is provisioning external load balancer in cloud space — depending on provider support.
The deployed load balancer will act as NodePort, but will have more advanced load balancing features and will also act as if you got additional proxy in front of NodePort in order to get new IP and some standard web port mapping (30666 > 80). As you see, it’s features position it as the main way to expose service directly to the outside world.
The main downside of this approach is that any service you expose needs it’s own load balancer, which can, after a while, have a significant impact on complexity and price.
Let’s briefly review the possibilities:
Above creates external load balancer and provisions all the networking setups needed for it to load balance traffic to nodes.
Note from k8s docs: With the new functionality, the external traffic will not be equally load balanced across pods, but rather equally balanced at the node level (because GCE/AWS and other external LB implementations do not have the ability for specifying the weight per node, they balance equally across all target nodes, disregarding the number of pods on each node).
If you want to add AWS ELB as an external load balancer, you need to add the following annotations to load balancer service metadata:
When getting into space where we are managing more than one web server with multiple different sets of pods, above mentioned services turn out to be quite complex to manage in most of the real life cases.
Let’s review the example we had before — 2 APIs, redis and frontend, and imagine that APIs have more consumers than just frontend service so they need to be exposed to open internet.
Requirements are as following:
Setup needed using the above services:
ClusterIP is necessary, we know it has to be there — it is the only one handling internal networking, so it is as simple as it can be.
External traffic however is different story, we have to set up at least one service per component plus one or multiple supplementary services (load balancers and proxies) in order to achieve requirements.
Number of configs / definitions to be maintained skyrockets, entropy rises, infrastructure setup drowns in complexity…
Kubernetes cluster has ingress as a solution to the above complexity. Ingress is essentially a layer 7 load balancer.
Layer 7 load balancer is name for type of load balancer that covers layers 5,6 and 7 of networking, which are session, presentation and application
Ingress can provide load balancing, SSL termination, and name-based virtual hosting.
It covers HTTP, HTTPS.
For anything other then HTTP and HTTPS service will have to be published differently through special ingress setup or via a NodePort or LoadBalancer, but that is now a single place, one time configuration.
In order to set up ingress, we need two components:
There are few options you can choose from, among them nginx, GCE (google cloud) and Istio. Only two are officially supported by k8s for now — nginx and GCE.
We are going to go with nginx as the ingress controller solution. For this we, of course, need new deployment.
Deploy ConfigMap in order to control ingress parameters easier:
Now, with basic deployment in place and ConfigMap to make it easier for us to control parameters of the ingress, we need to set up the service to expose ingress to open internet (or some other smaller network).
For this we setup node port service with proxy/load balancer on top (bare-metal /on-prem example) or load balancer service (Cloud example).
In both mentioned cases, there is a need for Layer 4 and Layer 7 load balancer:
Layer 4 load balancer — Directing traffic from network layer based on IP addresses or TCP ports, also referred to as transport layer load balancer.
NodePort for ingress yaml, to illustrate the above:
This NodePort service gets deployed to each node containing ingress deployment, and then load balancer distributes traffic between nodes
What separates ingress controller from regular proxy or load balancer is additional underlying functionality that monitors cluster for ingress resources and adjusts nginx accordingly. In order for ingress controller to be able to do this, service account with right permissions is needed.
Above service account needs specific permissions on cluster and namespace in order for ingress to operate correctly, for particularities of permission setup on RBAC enabled cluster look at this document in nginx ingress official docs.
When we have all the permissions set up, we are ready to start working on our application ingress setup.
Ingress resources configuration lets you fine-tune incoming traffic (or fine-route).
Let’s first take a simple API example. Assuming that we have just one set of pods deployed and exposed through service named simple-api-service on port 8080, we can create simple-api-ingress.yaml.
When we kubectl create -f simple-api-ingress.yaml we setup an ingress that routes all incoming traffic to simple-api-service.
Rules are providing configuration to route incoming data based on certain conditions. For example, routing traffic to different services within the cluster based on a subdomain or a path.
Let us now get to the initial example:
Since everything is on the same domain, we can handle it all through one rule:
There is also a default backend that is used to serve default pages (like 404s), and it can be deployed separately. In this case, we will not need it since the frontend will cover 404s.
You can read more at https://kubernetes.io/docs/concepts/services-networking/ingress/
And, what if we changed the example to:
It is also possible, with the introduction of a new structure in the rule definition:
Note (out of scope): You can notice from the last illustration that there are multiple ingress pods, which implies that ingress can scale, and it can. Ingress can be scaled like any other deployment, you can also have it auto scale based on internal or external metrics (external, like number of requests handled is probably the best choice).
Note 2 (out of scope): Ingress can, in some cases, be deployed as DaemonSet, to assure scale and distribution across the nodes.
This was a first pass through the structure and usage of k8s services and networking capabilities that we need in order to structure communication inside and outside of the cluster.
I, as always, tried to provide to the point and battle-tested guide to reality… What is written above should give you enough knowledge to deploy ingress and setup a basic set of rules to route traffic to your app and give you context for further fine tuning of your setup.
Important piece of advice: Make sure to keep all the setups as a code in files, in your repo — infrastructure as a code is essential part of making your application reliable.
Where Software meets Sports
282 
282 claps
282 
Written by
http://rastko.tech/ — Programmer, Architect, Tech Lead, and Tech Enablement and SRE guy at adidas ecom.
Where Software meets Sports
Written by
http://rastko.tech/ — Programmer, Architect, Tech Lead, and Tech Enablement and SRE guy at adidas ecom.
Where Software meets Sports
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.searce.com/glusterfs-dynamic-provisioning-using-heketi-as-external-storage-with-gke-bd9af17434e5?source=search_post---------78,"Sign in
There are currently no responses for this story.
Be the first to respond.
Shruti Naik
Apr 23, 2019·6 min read
We recently migrated a FinTech conglomerate from an VM based on-premise environment to GCP, while modernizing their application and infrastructure and deploying on GKE. It was an exciting journey, and I thought I’d share one of the challenges we faced during migration and how we solved.
The problem is that of shared storage. Some of the services we deployed depended on NFS mounts before migration and we had to make sure that these continue to work with shared storage on GKE. We looked at GKE’s persistent volumes. Although documentation lists ReadWriteMany as one of the access methods, unfortunately a persistent volume can not be attached to more than one node in write mode
We needed a highly available shared storage platform, so we turned to GlusterFS and Heketi — RESTful based volume management framework for GlusterFS. Heketi provides a convenient way to unleash the power of dynamically provisioned GlusterFS volumes. It is kind of glue between Glusterfs and Kubernetes. Without this access, you would have to manually create GlusterFS volumes and map them to k8s persistent volume. The rest of this post explains how to configure this whole setup.
Add the following lines in the /etc/hosts file
Glusterfs packages are not included in the default CentOS. Run the following commands one after the another on all 3 servers.
Start and enable the GlusterFS Service on all the servers.
Allow the ports in the firewall so that servers can communicate and from glusterfs storage cluster (trusted pool).
Don’t Forget to change CentOS default Configuration:
Create a trusted storage pool which consists of server 1 and server 2 in and will create bricks on that and after that will create distributed volume.
Run the below command from server 1 console to form a trusted storage pool with server 2.
We can check the peer status using below command :
Repeat the same for server 3 and check the peer status for the same. Speaking of repeating — here is a Calvin ;)
Install Heketi on one of the GlusterFS nodes.
Create the heketi user and the directory structures for the configuration:
Heketi has several provisioners but here I will be using the ssh We need to set up password-less ssh login between the Gluster nodes so heketi can access them. Generate RSA key pair.
Change Permission for ssh key files in all 3 nodes for heketi access :
Create the Heketi config file in/etc/heketi/heketi.json
Create the following Heketi service file /etc/systemd/system/heketi.service
Start the service and check with journalctl:
Now enable the service by restarts:
Create topology/etc/heketi/topology.json config file:
where /dev/sdb is a 10GB raw block device attached to each gluster node. Then we load topology:
GlusterFS-client.yaml needs to be installed on all k8s nodes otherwise the mounting of the GlusterFS volumes will fail. Need to create DeamonSet likewise:
Create a kubernetes Secret for the admin user password in the following gluster-secret.yaml file:
Kuberentes has built-in plugin for GlusterFS. We need to create a new glusterfs storage class that will use our Heketi service. Create YAML file gluster-storageclass.yaml likewise:
Now, It’s time to create the resources :
To test it we create a PVC (Persistent Volume Claim) that should dynamically provision a 1GB volume for us in the Gluster storage. Create glusterfs-pvc.yaml likewise :
If we check now:
To use the volume we reference the PVC in the YAML file of any Pod/Deployment like this for example:
Hope you find this useful! Happy containerizing! :)
Thanks to Suganya G — The Linux Geek.😃
""Clearly Cloudy"" Tech Enthusiastic
289 
4
289 claps
289 
4
Searce is a niche’ cloud-native technology consulting company, specializing in modernizing infra, app, process & work leveraging Cloud, Data & AI. We empower our clients to accelerate towards the future of their business.
"
https://medium.com/@niko.kosonen/how-to-kubernetes-for-cheap-on-google-cloud-68203608f00a?source=search_post---------79,"Sign in
There are currently no responses for this story.
Be the first to respond.
Niko Kosonen
Feb 10, 2020·14 min read
[TL;DR: Run Kubernetes on two micro instances on GKE without external load balancers. Cluster setup from scratch. github.com/nkoson/gke-tutorial]
My excitement of running kubernetes on Google Cloud Platform was quickly curbed by the realization that, despite Google’s virtual machines starting at affordable price points, their network ingress is another story: Let’s say you want to set up a simple cluster for your own personal projects, or a small business. At the time of writing, a couple of micro nodes running in Iowa will set you back $7.77/mo, but the only (officially marketed, AFAIK) method of getting traffic in is by using a load balancer — which start at whopping $18.26 for the first 5 forwarding rules. That is a deal breaker for me, since there are plenty of other cloud providers with better offerings to smaller players.
That’s when I stumbled upon a great article about running a GKE cluster without load balancers. With this newly incited motivation, I set out to create my GKE cluster — with the requirement of it being as cheap as possible while enjoying a key benefit of the cloud: being free of manual maintenance.
I have composed this article as a step-by-step tutorial. Based on my own experience in setting up a cluster on a fresh GCP account, I try to cover every topic from configuring the infrastructure to serving HTTP(S) requests from inside the cluster. Please notice, that I did this mainly to educate myself on the subject, so critique and corrections are wholeheartedly welcome.
We’ll be using Terraform.io to manage our cloud infrastructure, so go ahead and register an account, if you haven’t already. You’ll obviously need access to a Google Cloud Platform account, as well.
Let’s get going by creating a new project on the GCP console:
Project selector (top bar) -> New Project -> Enter name -> Create
This will create a nice empty project for us, which differs from the default starter project in that the newly created blank doesn’t come with any predefined API’s or service accounts. We’ll start digging our rabbit hole by enabling the Compute Engine API, which we need to communicate with GCP using Terraform. We’ll also enable the Service Usage API so that Terraform can enable services for us as we go forward.
APIs & Services -> API Library -> Compute Engine API -> Enable
APIs & Services -> API Library -> Service Usage API -> Enable
Once the APIs have been initialized, we should find that GCP has generated a new service account for us. The aptly named Compute Engine default service account grants us remote access to the resources of our project. Next, we’ll need to create a key for Terraform to authenticate with GCP:
IAM & Admin -> Service accounts -> Compute Engine default service account -> Create key -> Create as JSON
The key that we just downloaded can be used in our terraform.io console as an environment variable, or directly from local disk when running Terraform CLI commands. The former requires newlines edited out of the JSON file and the contents added as GOOGLE_CLOUD_KEYFILE_JSON in our terraform.io workspace:
Workspaces -> (select a workspace) -> Variables -> Environment Variables
Make sure you set the value as “sensitive / write only”, if you decide to store the key in your terraform.io workspace. As stated above, it’s also possible to read the key from your local drive by adding the following in the Terraform provider resource:
In this tutorial, we’ll be using the latter of the two methods.
While we’re here, it’s worth noting that the Compute Engine default service account doesn’t have the permissions to create new roles and assign IAM policies in the project. This is something that we will need later as part of our terraforming process, so let’s get it over with:
IAM & admin -> edit Compute Engine default service account (pen icon)
Add another role -> select “Role Administrator” -> Save
Add another role -> select “Project IAM Admin” -> Save
We’re now ready to initialize Terraform and apply our configuration to the cloud.
This will set up your local Terraform workspace and download the Google provider plugin, which is used to configure GCP resources.
We can proceed to apply the configuration to our GCP project.
This will feed the configuration to the terraform.io cloud, check its syntax, check the state of our GCP project and, finally, ask for confirmation to apply our changes. Enter ‘yes’ and sit back. This is going to take a while.
Once the dust has settled, it’s time to check the damage. We set out to configure a minimal cloud infrastructure for running a kubernetes cluster, so let’s see how we’ve managed so far.
Compute Engine -> VM Instances
This page reveals that we now have two virtual machines running. These machines are part of node pools ingress-pool and web-pool. A node pool is a piece of configuration, which tells Google Container Engine (GKE) how and when to scale the machines in our cluster up or down. You can find the node pool definitions in cluster.tf and node_pool.tf
If you squint, you can see that the machines have internal IP addresses assigned to them. These addresses are part of our subnetwork range. There is a bunch of other address ranges defined in our cluster, which we’ll glimpse over right now:
Defined in google_compute_subnetwork, this is the address range of the subnetwork, in which our GKE cluster will run.
The master node of our kubernetes cluster will be running under this block, used by google_container_cluster.
Rest of our kubernetes nodes will be running under this range, defined as a secondary range as part of our subnet.
Also a secondary range in our subnet, the service range contains our kubernetes services, more of which a bit later.
Understanding the basic building blocks of our network, there are a couple more details that we need to grasp in order for this to make sense as a whole. The nodes in our cluster can communicate with each other on the subnet we just discussed, but what about incoming traffic? After all, we’ll need to not only accept incoming connections, but also download container images from the web. Enter Cloud NAT:
Networking -> Network Services -> Cloud NAT
Part of our router configuration, Cloud NAT grants our VM instances Internet connectivity without external IP addresses. This allows for a secure way of provisioning our kubernetes nodes, as we can download container images through NAT without exposing the machines to public Internet. In our definition, we set the router to allow automatically allocated addresses and to operate only on our subnetwork, which we set up earlier.
OK, our NAT gives us outbound connectivity, but we’ll need a inbound address for our cheap-o load balancer / ingress / certificate manager all-in-one contraption, traefik. We’ll talk about the application in a while, but let’s first make sure that our external static IP addresses are in check:
Networking -> VPC network -> External IP addresses
There should be two addresses on the list; an automatically generated one in use by our NAT, plus another, currently unused address which is named static-ingress. This is crucial for our cluster to accept connections without an external load balancer, since we can route traffic through to our ingress node using a static IP. We’ll be running an application, kubeip, in our cluster to take care of assigning the static address to our ingress node, which we’ll discuss in a short while.
This is a good opportunity to take a look at our firewall settings:
Networking -> VPC network -> Firewall rules
We have added a single custom rule, which lets inbound traffic through to our ingress node. Notice, how we specify a target for the rule to match only with instances that carry the ingress-pool tag. After all, we only need HTTP(S) traffic to land on our internal load balancer (traefik). The custom firewall rule is defined here.
Lest we forget, one more thing: We’ll be using the CLI tool gcloud to get our kubernetes credentials up and running in the next step. Of course, gcloud needs a configuration of its own, as well, so let’s get it over with:
Answer truthfully to the questions and you shall be rewarded with a good gcloud config.
Our cloud infrastructure setup is now done and we’re ready to run some applications in the cluster. In this tutorial, we’ll be using kubectl to manage our kubernetes cluster. To access the cluster on GCP, kubectl needs a valid config, which we can quickly fetch by running:
Disclaimer: I don’t recommend doing any of the things I’ve done in this section. Feel free to crank up the node pool machine types to something beefier (such as g1-small) in favor of keeping logging and metrics alive. At the time of writing this tutorial, I had to make some rather aggressive optimizations on the cluster to run everything on two micro instances. We did mention being cheap, didn’t we?
Realizing that it’s probably not a good idea to disable logging, we have disabled logging on GCP. Now that we’re up to speed, why don’t we go ahead and turn off kubernetes metrics as well:
That’s over 100MB of memory saved on our nodes at the expense of not knowing the total memory and CPU consumption anymore. Sounds like a fair deal to me! We’ll scale kube-dns service deployments down as well, since running multiple DNS services in our tiny cluster seems like an overkill:
kubernetes default-backend can go too. We’ll be using nginx for this purpose:
At this point I realized that the instance spun up from web-pool was stuck at “ContainerCreating” with all the kubernetes deployments I just disabled still running, so I just deleted the instance to give it a fresh start:
After a few minutes, GCP had spun up a new instance from the web-pool instance pool, this time without the metrics server, default backend and with only one DNS service.
The cluster we’re about to launch has three deployments: nginx for serving web content, kubeIP for keeping our ingress node responsive and traefik which serves a dual purpose; routing incoming connections to nginx, plus handling SSL. We’ll discuss each deployment next.
Incoming HTTP(S) traffic in our cluster is redirected to the nginx server, which we use as our web backend. Put simply in kubernetes terms, we’re going to deploy a container image within a namespace and send traffic to it through a service. We’ll do namespace first. Navigate to k8s/nginx-web/ and run:
Pretty straightforward so far. The namespace we just created is defined here. Next up is the deployment:
As you can see from the definition, we want our deployment to run under the namespace nginx-web. We need the container to run on a virtual machine that's spun up from the node pool web-pool, hence the nodeSelector parameter. We're doing this because we want to run everything except the load balancer on a preemptible VM to cut down costs while ensuring maximum uptime.
Moving on, the container section defines a Docker image we want to run from our private Google Container Registry (GCR) repository. Below that, we open the ports 80 and 443 for traffic and set up health check (liveness probe) for our container. The cluster will now periodically GET the container at the endpoint /health and force a restart if it doesn’t receive a 200 OK response within the given time. Readiness probe is basically the same, but will tell the cluster when the container is ready to start accepting connections after initialization.
We won’t dive too deep into Docker in this tutorial, but we have included a basic nginx:alpine container with placeholder web content. We’ll need to upload the container image to GCR for kubernetes to use it as per the deployment we just created. Navigate to docker/nginx-alpine and run:
This builds the image and tags it appropriately for use in our cluster. We need docker to authenticate with GCP, so let’s register gcloud as docker’s credential helper by running:
To push the image into our registry, run:
We can check that everything went fine with the deployment by running:
We now have an nginx container running in the right place, but we still need to route traffic to it within the cluster. This is done by creating a service :
Our service definition is minimal: We simply route incoming traffic to applications that match the selector nginx-web. In other words, traffic that gets sent to this service on ports 80 and 443 will get directed to pods running our web backend.
Working in a cloud environment, we cannot trust that our virtual machines stay up infinitely. In contrary, we actually embrace this by running our web server on a preemptible node. Preemptible nodes are cheaper to run, as long as we accept the fact that they go down for a period of time at least once a day. We could easily ensure higher availability in our cluster by simply scaling up the number of nodes, but for the sake of simplicity, we’ll stick to one of each type, defined by our node pools ingress-pool and web-pool
A node pool is a set of instructions on how many and what type of instances we should have running in our cluster at any given time. We’ll be running traefik on a node created from ingress-pool and the rest of our applications run on nodes created from web-pool.
Even though the nodes from ingress-pool are not preemptible, they might restart some time. Because our cheap-o cluster doesn’t use an external load balancer (which is expen$ive), we need to find another way to make sure that our ingress node always has the same IP for connectivity. We solve this issue by creating a static IP address and using kubeip to bind that address to our ingress node when necessary.
Let’s create the deployment for kubeip by navigating to k8s/kubeip and running:
We define kube-system as the target namespace for kubeip, since we want it to communicate directly with the kubernetes master and find out when a newly created node needs a static address. Using a nodeSelector, we force kubeip to deploy on a web-pool node, just like we did with nginx earlier.
Next in the config we define a bunch of environment variables, which we bind to values in a ConfigMap. We instruct our deployment to fetch GCP service account credentials from a kubernetes secret. Through the service account, kubeip can have the required access rights to make changes (assign IPs) in GCP.
We created a GCP service account for kubeip as part of our Terraform process. Now we just need to extract its credentials just like we did with our main service account in the beginning of this tutorial. For added variety, let’s use the command line this time. From the root of our project, run:
Now that we have saved the key, we’ll store it in the cluster as a kubernetes secret:
We have created a GCP service account for kubeip and configured kubeip to access it via the kubernetes secret. We will still need a kubernetes service account to access information about the nodes in the cluster. Let’s do that now:
We define a (kubernetes) ServiceAccount and below it the ClusterRole and ClusterRoleBinding resources, which define what our service account is allowed to do and where.
Next, we need to create the ConfigMap for the deployment of kubeip:
In the config, we set kubeip to run in web-pool and watch instances spun up from ingress-pool. When kubeip detects such an instance, it checks if there is an unassigned IP address with the label kubeip and value static-ingress in the reserve and gives that address to the instance. We have restrictedingress-pool to a single node, so we only need a single static IP address in our reserve.
External load balancers are very useful in keeping your web service responsive under high load. They are also prohibitively expensive for routing traffic to that single pod in your personal cluster, so we’re going to make do without one.
In our tutorial cluster, we dedicate a single node to hosting traefik, which we configure to route traffic to our web backend (nginx server). Traefik can also fetch SSL certificates from resolvers such as letsencrypt to protect our HTTPS traffic. We're not going to cover procuring a domain name and setting up DNS in this tutorial, but, for reference, I have left everything that's required for setting up a DNS challenge commented out in the code.
Let’s create a namespace and a service account for traefik. Navigate to k8s/traefik and run:
Next, we’ll create the deployment and take a look at what we’ve done so far:
Using a nodeSelector once again, we specify that we want traefik to run on a machine that belongs to ingress-pool, which means that in our cluster, traefik will sit on a different machine than kubeip and nginx. The thought behind this is that both of our machines are unlikely to go down simultaneously. When web-pool goes down and is restarted, no problem; traefik will find it in the cluster and resume routing connections normally. If our ingress-pool went down, the situation would be more severe, since we need our external IP bound to that machine. How else would our clients land on our web backend? Remember we don't have an external load balancer...
Luckily, we have kubeip which will detect the recently rebooted ingress-pool machine and assign our external IP back to it in no time. Crisis averted!
There’s a couple key things in our traefik deployment that sets it apart from our other deployments. First is hostNetwork which we need for traefik to listen on network interfaces of its host machine. Secondly, we define a toleration, because we have tainted the host node pool. Since our traefik deployment is the only one with this toleration, we can rest assured that no other application is deployed on ingress-pool.
Finally, we give traefik some arguments : entry points for HTTP, HTTPS and health check (ping in traefik lingo). We also enable the kubernetes provider, which lets us use custom resources. Let’s create them now:
Now we can add routes to traefik using our new custom resources:
The two routes now connect the “web” and “websecure” entrypoints (which we set up as arguments for traefik) to our nginx-web service. We should now be able to see HTML content served to us by nginx when we connect to our static IP address.
Please enjoy your cluster-on-a-budget responsively!
219 
219 claps
219 
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/introduction-to-external-dns-in-kubernetes-654aa4cf38e6?source=search_post---------80,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
How to automatically create DNS records in Kubernetes using External DNS
Are you running your workloads in GKE / EKS / AKS? Do you use Services of type LoadBalancer? If yes then this is the right article for you. Let us suppose you have a web application running in your Kubernetes Cluster. You might have a public-facing load balancer so that your application is accessible to the entire world. Imagine you have a domain name called kubernetesisgreat.com and you want to map that to your public-facing load balancer provisioned by Kubernetes. For this, you can either use a gcloud command or maybe do it manually. Now, what if you have hundreds of projects and thousands of DNS records to be created? What if the LoadBalancer Endpoint changes? How do you keep a track of hundreds of thousands of DNS records? Here come external-dns to the picture. ExternalDNS allows you to control DNS records dynamically via Kubernetes resources in a DNS provider-agnostic way. ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS supports multiple DNS providers. A few of them are
In the scope of this article, we will use Terraform to create the cluster. We can automate the process of creating the GKE cluster using Github actions. You can refer to my article on how to create a GKE cluster using GitHub Actions. You can also clone my GitHub repository to apply it from your cloud shell machine. Or the cluster can be simply created using the gcloud CLI.
You can follow the steps here to register a new Domain or Either Import your existing domain to the GCP and then create a Managed Public Zone. You can alternatively use freenom to get a free domain for yourself and create a Public zone in your GCP account.
cloud.google.com
Once the aforementioned steps are completed let us now deploy external-dns to our cluster.
Before you deploy you might have to change the name of the domain in the domain-filter section ( line 56 ). In my case, the name of my domain is bettercallpavan.tk ( You can also get a free domain using freenom.com ). Let us now deploy the external dns manifests to our cluster
Once you apply the manifests, you should find the following components up and healthy. Let us check the logs of the external-dns pod now.
You can now see that the pod external dns is able to communicate with our Google Cloud DNS.
Let us now deploy a sample web application that has an httpd Deployment and a service of type LoadBalancer.
Let us now check the logs of the external DNS pod. You can see that the records are already being created. Let us also verify the same from the Google Cloud Console.
The entries for my LoadBalancer are created in the GCP console.
Let me now try to access my application from the browser by navigating to http://prodwebapp.bettercallpavan.tk.
Hurrah, our application is now accessible from the Internet. You can thus use ExternalDNS to automatically create DNS records in your hosted zone.
Thanks for reading my article. Hope you have liked it. Here are some of my other articles that may interest you.
medium.com
pavan1999-kumar.medium.com
pavan1999-kumar.medium.com
pavan1999-kumar.medium.com
github.com
👋 Join FAUN today and receive similar stories each week in your inbox! ️ Get your weekly dose of the must-read tech stories, news, and tutorials.
Follow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
16 
2
GitHub: https://github.com/pavan-kumar-99
Medium: https://pavan1999-kumar.medium.com/
Linkedin: https://www.linkedin.com/in/pavankumar1999/
16 claps
16 
2
Written by
Cloud DevOps Engineer at Informatica || CKA | CSA | CRO | AWS | ISTIO | AZURE | GCP | DEVOPS Linkedin:https://www.linkedin.com/in/pavankumar1999/
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Cloud DevOps Engineer at Informatica || CKA | CSA | CRO | AWS | ISTIO | AZURE | GCP | DEVOPS Linkedin:https://www.linkedin.com/in/pavankumar1999/
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.netsil.com/the-4-golden-signals-of-api-health-and-performance-in-cloud-native-applications-a6e87526e74?source=search_post---------81,"Modern cloud applications are highly services-driven and leverage a lot of APIs including external APIs such as Twitter Auth API, Twilio API, Google Maps API, and various PaaS APIs. In a previous blog post, we had talked about the shift from monolithic architectures to microservices and the implications of that change from an operational perspective for Site Reliability Engineers (SREs) and DevOps engineers.
In this blog post, we focus on the golden signals of monitoring that are the foundation of service-level observability for large-scale production applications. These golden signals ultimately help measure end-user experience, service abandonment and impact on business. After discussing these signals, we describe how we have approached their measurement in a way that is fundamentally different from the existing approaches that primarily require code-embedded agents or instrumentation of code.
The four golden signals of monitoring are latency, traffic, errors, and saturation. These have been championed by the Google SRE team and the larger web-scale SRE community as the most fundamental metrics for tracking service health and performance.
Here is a brief description of these four golden signals:
By focusing their monitoring and alerting on these golden signals, SREs and DevOps can effectively support the Service Level Objectives (SLOs) for the services and APIs that make up their production applications.
The Netsil Application Operations Center (AOC) is an observability and analytics product used by SREs and DevOps who run API and services driven cloud applications.
While the programming languages and web frameworks used to build the services change frequently with time, service communication protocols such as HTTP remain relatively constant, acting as the glue between them. Leveraging this insight, Netsil captures service interactions in real-time as its source-of-truth, without instrumenting the application code. Netsil’s specialized stream-processing technology analyzes these interactions and automatically discovers the service topology map of the application, overlaying it with operational metrics (including the golden signals). A previous blog post covered our architecture in more detail.
Code-based application monitoring tools require upfront investment in the form of instrumentation of code before they can provide value. Further, they do not give visibility into calls made to external APIs as they cannot instrument them. The Netsil approach works seamlessly for both internal and external APIs, and the lack of upfront investment leads to fastest time-to-value.
Latency, Traffic and Errors Tracked Based On Service-Level KPIs
The service-level KPIs tracked by Netsil include the golden signals of latency, traffic and errors. Let’s take the example of HTTP REST APIs. Each signal is tracked at the granularity of individual REST endpoints, which are automatically discovered from service interactions. Here is how Netsil measures them:
Many application health issues are rooted in problems related to the underlying network or infrastructure. These include issues being experienced by the cloud provider. Out of the golden signals, Saturation is a signal which is related to the underlying infrastructure. Saturation is measured by looking at infrastructure metrics such as Disk I/O, Network I/O and System Memory.
Figure 3 shows these infrastructure metrics being tracked in the Netsil AOC.
The AOC also allows you to look at network flows including TCP retransmission rates, connection drops, round-trip-times and out-of-order packet counts. If your cloud provider is having issues (say a networking issue in the datacenter or VMs being overloaded), network packets will start getting dropped. By tracking the average TCP retransmissions per second and TCP out-of-order packets per second, you can quickly identify this issue. Figure 4 shows how network flows are presented in the AOC.
Bringing It All Together
In conclusion, the Netsil AOC is built from ground-up for powering observability and analytics in modern cloud applications that use APIs and services. The product brings together service-level KPIs and infrastructure metrics in a way that allows SREs and DevOps to efficiently measure the four golden signals of API health and performance. Further, the Netsil approach of using service interactions as the source-of-truth rather than code instrumentation allows seamless monitoring across internal & external APIs and fastest time-to-value.
We welcome your comments on this post and also encourage you to try the Netsil AOC at https://netsil.com/.
Universal Observability and Monitoring for Modern Cloud…
68 
68 claps
68 
Written by
Universal Observability and Monitoring for Modern Cloud Applications
Universal Observability and Monitoring for Modern Cloud Applications
Written by
Universal Observability and Monitoring for Modern Cloud Applications
Universal Observability and Monitoring for Modern Cloud Applications
"
https://medium.com/swlh/register-an-external-domain-with-aws-api-gateway-using-an-aws-certificate-414a1568d162?source=search_post---------82,"There are currently no responses for this story.
Be the first to respond.
When developing cloud applications in AWS, the generated endpoint names are not that meaningful or readable and usually for production environment it’s ideal to have a unique domain name to host the application.
In this blog I will be using NameCheap as the domain provider which will be used to expose an AWS API Gateway. As the name…
"
https://medium.com/thermokline/building-a-k8s-external-gateway-4d98028f10ca?source=search_post---------83,"There are currently no responses for this story.
Be the first to respond.
This design for an external k8s Gateway can provide a high level of security from outside access. It ensures that only the specific ip address and ports used by services are exposed to outside traffic and provides the option to add traffic rate limiting to services using additional annotations in the services.
In addition to your k8s cluster, the solution consists of three components.
The Linux Gateway. Linux has all of the necessary networking capabilities to create the k8s gateway. In this solution we recommend the use of Ubuntu 18.04LTS however you can use other distro’s if you wish. One of the benefits of using ubuntu 18.04 is that the default output queuing discipline (qdisc) is fq_codel (fair queuing with controlled delay). This queuing mechanism is ideally suited for our purposes as its fair queue functionality allocates on a flow basis and the controlled delay functionality avoids starvation without requiring significant configuration. Practically this means that this queuing scheme should provide some protection against a small number of bad actors, assuming the count doesn’t get too high or the bandwidth overrun. It can be configured on distros either by default as every interface must have a default qdisc (often fifo) or via linux-tc. In addition the nft_operator also configures the ingress qdisc on the public network interface. This special qdisc is applied on input path and can be used for filtering and input rate shaping, it’s used for the latter by the nft_operator. There are lots of possible configuration options for managing traffic, we will cover some of them in part 3. There are a number of good routing implementations for linux, we chose FRR, a well maintained fork of quagga that uses common industry configuration syntax. The routing configuration is pretty simple, it uses BGP towards the k8s system and if deployed in an enterprise network could use BGP towards the existing network, however the solution does not require the use of BGP on the public network side.
Note in the BGP configuration that this includes a bgp peer to an upstream router. Where there is an upstream peer, it would be customary to summarize the range allocated to metallb seen in the aggregate-address entry. When a host route is advertised by metallb in this range, FRR will advertise the aggregate route only limiting the host route to the gateway. (Note. The metallb webpage provides another alternative advertising summaries however that solution will install a less specific route in the gateway defeating the benefit of host routes)
Using BGP dynamic peers removes the need to manually add a peer entry for every node running the metallb speaker. As this is dynamic a password is suggested not for security, more for avoiding misconfiguration of bgp neighbors incorrectly populating the routing table.
While discussing the BGP configuration, it’s worth noting that the connection between the metallb speaker nodes and the gateway is eBGP, shown by the different AS numbers used. iBGP, where the same AS number is used, or eBGP could be used in the configuration, however the selection mechanism is different with eBGP allowing more extensive routing policy to be applied, it is not being used in this case.
Finally redistribute connected avoids the need to statically specify the local interface networks.
MetalLB is a k8s Load Balancer manager. It uses the service API to allocate IP addresses to services and configures IPtables to map connectivity into PODs. It has an address manager or IPAM for keeping track of addresses used by the service and runs goBGP in pods called speakers on all of the nodes that provide external access. In our example all nodes will be running the speaker, we will discuss alternatives and impacts later. Each service configured to use a load balancer is allocated a host route advertised by the speakers to bgp peers, in this case the Gateway. Therefore if no services are advertised there are no routes in the Gateway to the k8s system causing all packets to be dropped at the Gateway. This behaviour is very different to standard routing behaviour where the routers task is to forward all packets and then a firewall is put in place to limit what can be passed. Combining Metallb using BGP with a router is a good way to create secure access assuming you are careful on how the overall routing structure is configured.
Note that this document covers how this system operates with kubeproxy in the default mode, iptables mode, not IPVS. If your k8s cluster has a default configuration, it will be using iptables mode.
This k8s ansible operator increases the security of the linux gateway. When a service is created in k8s, the protocol and port number are defined, metallb dynamically adds a host route and the nft_operator watches for services to be created and applies filters based upon the service configuration. The operator uses net filter tables, not IPtables. In addition to performance benefits, one of the most important aspects of nftables used is the ability to make filter table changes atomic. Unlike iptables, nftables processes the complete configuration and then replaces it avoiding incomplete configuration errors that can be introduced in iptables. As previously mentioned, the nft_operator also configures linux traffic control (tc) adding an ingress qdisc to the public interface. Adding an annotation to the service definition, a traffic rate can be added for the service to control bandwidth for that service.
Note enable_ssh and enable_bgp are applied on the firewalled interface allowing ssh access and bgp peers to be established from the public network. If you’re troubleshooting you can add a rule to /etc/nftables.d/local-chain.nft to enable your specific addresses to pass, this file is created but not managed by the nft_operator.
Note: Just a snippet, take a look at iptables-save on a node.
In Understanding Packet Forwarding we describe in detail how traffic is forwarded to nodes, the use of externalTrafficPolicy and ingress controllers.
.
Going deep on Kubernetes networking
38 
1
38 claps
38 
1
Orchestrated & configured networking is fundamental to k8s operation, we talk about it.
Written by
Tech enthusiast, infrastructure specialist, leader & engineer
Orchestrated & configured networking is fundamental to k8s operation, we talk about it.
"
https://itnext.io/secrets-injection-from-external-vault-into-kubernetes-poc-83a52c8cf5cb?source=search_post---------84,"When you work in a multi cloud environment, you can't always use AWS secrets manager for storing all your secrets. Hashicorp Vault is an awesome solution for storing and managing your secrets. In this POC I will show how you can easily and directly inject secrets into your pods thanks to https://github.com/hashicorp/vault-k8s. A lot of tutorials demonstrate how to use vault when it is running IN the cluster, but here we'll use an external running vault.
Before diving into the code, the most important thing is to understand how the vault integration is done with Kubernetes, what components do and how the all play together.
The flow starts within a pod where you want to inject your secrets. This pod is running with a Service Account (ex: sa-awesome-app). This SA logs into theVault server and asks to assume a role (awesome-app). If the role is allowed to read the secret, it is returned to k8s and injected as a file in a volume mounted in the pod.
Initially, Vault receives the JWT from sa-awesome-app. To verify if this JWT is correct, Vault asks k8s to validate the token. Vault is configured to bind a k8s service account with a role. This role has a policy which grants it permission to read the secret path (secret/my-awesome-app/config).
For k8s to be able to validate the token, we need another Service Account (sa-vault-auth) of type ClusterRole to delegate the authentification.
Now that we have a better view of how vault is integrated into kubernetes, let's build our component. This POC is available on github: https://github.com/sylwit/vault_injection_k8s_poc.
If you don't want to wait any longer, just clone the repo, open the Makefile and run each target in order.
TL;DR : make k8s
Start our cluster, create a namespace ""demo"" to deploy our awesome app, set it as the default namespace.
Create Service Account sa-vault-auth with its secret and bind it to the ClusterRole system:auth-delegator. This SA will be used by vault to validate JWT token.
Install hashicorp vault via the official helm chart. We need to set injector.externalVaultAddr to the vault server and this URL must be accessible from your cluster. I'm fetching the IP in the makefile
TL;DR : make vault
Vault will be started with docker-compose and needs a .env file. (make .env)
If you want to access Vault UI, navigate to http://localhost:8200 and the token is : root. It is hard-coded in the docker-compose, otherwise you can read it from docker-compose logs vault
Once vault is running, we create a secret then enable and configure kubernetes authentication.
We now write the policy to allow read permission to this secret and bind it to the SA sa-awesome-app.
I wrap all this set up in a vault-init.sh script and execute it on the running vault container
TL;DR : make app
The application is a simple HTTP server which reads a file and serves its content. The repo is available here: https://github.com/sylwit/http_filereader
We also expose our awesome app through a service running on port 30100.
The full code is available in this file : https://github.com/sylwit/vault_injection_k8s_poc/blob/main/app.yaml
The magic happens in these annotations
Wait few minutes then navigate to http://`minikube ip`:30100?file=credentials.txt and voila.
k8s-vault integration can be done thanks to init and side containers.
Before the application starts, the init container will get the secret and inject it in the volume located as /vault/secrets as mentioned before.
The volume is mounted in the app container so immediately accessible.
A side container is also started, every 5 minutes, it checks if the secret has changed and updates the volume for us if needed.
I really like the fact that the side container avoids a new release when the secrets change. It looks a bit tedious at the beginning to configure all the resources but once it's done and scripted it's pretty easy and reusable.
ITNEXT is a platform for IT developers & software engineers…
53 
2
Thanks to Matt Leus. 
53 claps
53 
2
Written by
AWS Solutions Architect at National Bank of Canada — https://sylwit.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
AWS Solutions Architect at National Bank of Canada — https://sylwit.com
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/google-cloud/how-to-integrate-external-data-sources-with-bigquery-9e126d5751ea?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
Data Federation is a holistic approach adopted by organizations to harvest meaningful insights, by collecting information from disparate data sources to host a unified data control point. This enables organizations to implement a centralized repository for enabling analytics.
A federated data source, known as an external data source, is a data foundation that can be used to query directly from heterogeneous data locations, even though the data is not stored in BigQuery. With this approach, one can extract structured data from multiple sources in the desired data format, thereby avoiding data copy or transfer from the source systems. This blog explains the methods to query data from multiple sources, such as Cloud Storage, Cloud SQL, and Google Drive.
Federated query provides a uniform method to retrieve data from multiple non-contiguous data sources with a single query even if the underlying data sources are heterogeneous. A single query displays a view of combined data structure by accessing federated data sources.
Google Cloud community articles and blogs
27 
1
27 claps
27 
1
Written by
Passionate Leader, Technology Enthusiast, Innovator, and Mentor
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Passionate Leader, Technology Enthusiast, Innovator, and Mentor
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kasunkodagoda/design-patterns-for-cloud-federated-identity-pattern-a8e0b2a24dcb?source=search_post---------86,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kasun Kodagoda
Apr 10, 2017·8 min read
Federated Identity Pattern is a cloud design pattern that is used to delegate authentication responsibility to an external identity provider. It’s a useful pattern when your application needs to provide Single Sign-On (SSO) experience to the users, where applications needs to authenticate users with multiple partners, in Software as a Service (SaaS) applications where tenants of the SaaS application can authenticate using a suitable identity provider etc. Let’s look at what are the reasons behind using Federate Identity Pattern, what is the solution the pattern provides and what are the considerations and issues with the pattern you need to understand before using the pattern.
In any application, doesn’t matter if it is a web, mobile or a desktop application, if the application has restrictions to access its content, then the users of the application needs to be authenticated in order to access the application. The application needs to have a record of the user and his/her capabilities in order to authenticate. This record is called the Identity of the user. An Identity is a representation of the user and specifies what the user has access to in the application. Normally an application would store the identities of the users in a database to be used in authenticating the user.
Today, users deal with a large number of applications, be it an On-Premise Application or a Cloud Application. This is the case even in an enterprise scenario. Normally what happens is that the user has to register with the application and create a username and password to authenticate. If the user needs to access 10 applications the this has to be done 10 times and that is 10 passwords to remember. This leads to forcing bad password practices. What this leads to normally is that the users will reuse passwords, which is a bad practice. This allows the uses password to leak even if one application gets compromised and automatically leaving all the applications that users uses to be vulnerable.
Let’s take a scenario in a corporate environment, where employees have to use 5 applications that needs authentication to access each one of them. As mentioned above, if each individual apps needs to login, then its 5 usernames and passwords. Then imagine suddenly an employee is terminated and his access from these applications needs to be removed immediately. Then the operations team needs to individually remove the user from 5 different applications which is difficult to manage and error prone. If Ops team makes a mistake, then that user may still have access to some applications.
So, this scenario is a headache to the Operations teams to manage and a hassle to the users at the same time since they have to remember 5 different passwords to login. Overall a poor user experience all round. Not only that, this approach is complex when it comes to development and maintenance, where you have to develop 5 different authentication mechanisms for each application and maintain them. And it’s difficult to keep the systems safe and secure from outside threats since your focus and time is divided between multiple applications. Also, managing user credentials is no easy task and require a lot of responsibility and attention which is something organizations are not really fond of. And as mentioned above, User Administration is really difficult.
Federated Identity Pattern removes these problems and provide you with a robust way of authenticating the users by separating the authentication from application code and delegating the authentication to a Trusted Identity Provider (IdP). This decuples authentication from authorization, where Authentication is the responsibility of the Identity Provider and Authorization is the responsibility of the Application itself. These trusted identity providers can be Corporate Directories, On-Premise Federation Services, and other Identity Providers like Business Partners and Social Identity Providers like Facebook, Twitter, Google etc.
This allows the users not to enter their credentials for each and every application they need to login, by doing so hiding the user credentials from all the other applications except the original identity provider. This removed the overhead of managing the user credentials from the individual applications. Providing a Single Sign-On (SSO) experience that is much more user friendly and will improve overall User Experience.
This also allows to include more advanced identity management features like Two-Factor/Multi-Factor Authentication, Password Change Reminders, Stricter Password Policies to insure had to crack passwords which was difficult and expensive to implement in individual applications. And if we take the previous scenario where a user has access to 5 corporate applications and that user gets terminated, then the Ops team only needs to remove the user from one location and the user will automatically be removed from all the applications. Which is great from the perspective of the Operations team when it comes to Managing Users.
To explain how Federated Identity Pattern works, let’s take an example where a user needs to access an application that requires authentication.
The flow of authenticating is like this.
1. The user navigates to the secure application
2. Applications tell unauthenticated user to authenticate at the Identity Provider (IdP)
3. User Authenticates with the Identity Provider (E.g. Entering username & password)
4. Identity Provider providers an Access Token to the user (claims may be included)
5. User goes back to the application with the Access Token
Here, the users navigate to the application to access its content. But the application is secure and it detects that the user is not authenticated and tells the user to authenticate at the Identity Provider (IdP). The user is redirected to the Identity Providers login screen since the application knows about the Identity Provider and trusts the IdP. Then the user enters his/her credentials and after authenticating the Identity Provider issues an Access Token to the user. This access token can include Claims that can be used to authorize access to certain content in the application. The user then goes back to the application with the access token and then the application allows access to the content of the application according to the authorization of that user (authorization level can be decided using the claims that came with the Access Token)
In the above diagram the Identity Management is the Identity Provider, this is a common implementation where Identity Management Application acts at an Identity Provider and the Secure Token Service (that provides the Access Token). The Identity Management can also be integrated with External Identity Provides such as Azure Active Directory, ADFS etc. and provide User Administration and Management capabilities.
As mentioned above the Identity Provider can be included in to an Identity Management application that integrates with external identity providers. Take a look at the diagram below.
In this diagram, you can see a high-level overview of real world implementation of this pattern. Here the Identity Management acts as the Identity Provider for the mobile, web and desktop apps that has a Trust Relationship with the Identity Management system. It also integrates with external Identity Providers like Azure AD, ADFS, Social Providers and other Identity Provider Services. The Identity Management system has User Administration and Management capabilities where Operations Teams can quickly perform user administration tasks like removing users from the system, controlling access to certain applications/resources, managing password policies etc. This makes is so much easier for operations teams to handle these at a single place rather than looking at 5–10 different systems.
Most of the time the External Identity Providers that are connected to the Identity Management does not provide a lot of information about the user. Most of the time what External IdPs would provide is the email address of the user or some unique ID that is associated with the user. Then Identity Management System has the responsibility to associate this identifier with a specific user that tries to login to applications that has a trust relationship with the Identity Management system and creates the Access Token with the necessary details included as Claims and pass this token along to the user to login in to the applications.
It’s important to understand that Identity Management System is a Mission Critical Production System. It’s not a part of your application(s) but without it your application(s) will not work since no user can authenticate. This makes the Identity Management System a Single Point of Failure. Therefor to ensure your users can always login and use the application(s) you need to make sure that Identity Management System is always Available, Secure and Performs exceptionally well.
This is an issue if your Identity Provider (IdP) also acts as the Identity Management System (as seen in the above diagram) So, having a fully featured Identity Management System has its drawbacks as well. To eliminate these issues with availability, security and performance you can opt for Identity Management as a Service offering if it suits your requirements.
It’s very important to understand that The Identity Management System (and Identity Provider as well) does NOT Provide Authorization for the user, it Only Provides Authentication. It’s the applications responsibility to implement authorization mechanism using the details provided by the Identity Management or Identity Provider (via claims).
You can use the Federated Identity Pattern when you need to provide Single Sign-On (SSO) Experience to the user. Where a single external Identity Provider can be used to seamlessly login to a number of applications. This provides a better user experience.
You can Allow Access to External Users to your applications with this pattern. This may be a partner company your organization works with in a Business to Business (B2B) scenario or you can also open up your applications to customer that can use their social accounts to login in a Business to Consumer (B2C) scenario. The identities for this scenario lives with in the external identity provides, and these IdP has a trust relationship with the Identity Management System.
You can also provide Federated Identity for your SaaS Applications. This is a common scenario where Independent Software Vendors (ISVs) provide a SaaS offering to multiple clients. With this ISVs, can provide the flexibility to use their own identity provider. Each client can bring in their own existing Identity Provider and authenticate to your application. The Identity Management System in the SaaS application sits in the middle of this and provides the capabilities to configure identity providers, Manage users and user access etc.
This article gives you a high-level understanding of what Federated Identity Patter is and where, when and how to use it. It also provides information about what to look out for when implementing this pattern in your applications. For more information refer to the Cloud Design Patterns — Federated Identity Article in Microsoft Technical Documentation.
wpdevkvk.wordpress.com
Passionate about technology and computer science. Crazy for all things mobile and Technical Lead at @99XTechnology
See all (337)
50 
50 claps
50 
Passionate about technology and computer science. Crazy for all things mobile and Technical Lead at @99XTechnology
About
Write
Help
Legal
Get the Medium app
"
https://servian.dev/encryption-authentication-and-external-access-for-confluent-kafka-on-kubernetes-69c723a612fc?source=search_post---------87,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ryan Morris
May 14, 2019·5 min read
Confluent provide a helm chart that makes the installation of their Kafka platform on a Kubernetes cluster super easy. This guide is aimed at those who have used this Helm chart to create a Kafka installation, or have otherwise rolled their own Kubernetes installation using the Kafka docker images and wish to expose it outside the cluster with SSL encryption and authentication.
Out of the box, the helm chart doesn’t support SSL configurations for encryption and authentication, or exposing the platform for access from outside the Kubernetes cluster. To implement these requirements, there are a few modifications to the installation needed. In summary, they are:
This article will run you through a solution for creating a secured, externally accessible Kafka cluster on Kubernetes.
Kafka uses two-way SSL for its authentication which means you’ll need to generate keys and certificates for each broker and client of the Kafka cluster. Kafka, which is written mostly in Java uses the Java KeyStore(JKS) for it’s key/certificate management. You’ll need to create both a keystore (holds it’s own private keys/certificates) and a truststore (holds other client/brokers certificates) for each client and broker.
Generating these for Kafka is fairly well documented already and covered in Confluent’s docs here.
Once the broker’s keystore and truststore JKS stores have been generated, they can be provided to the brokers via Kubernetes Secrets.
We’re going to use Kubernetes Secrets to make these files accessible to the broker pods. Basically, the secrets configuration in Kubernetes will hold some base64 encoded strings representing your keystore, truststore and passwords, which you’ll mount as files to a volume in your StatefulSet definition ie. broker pods.
So first, get the base64 encoded value for your broker’s keystore and truststore:
Create a secrets.yaml file as below with the encoded files:
Apply the secrets.yaml to your cluster. Given that you’ll want to keep your key passwords even more secret, a good idea is to inject these at deploy time with your CI/CD process.
Mount a secrets volume to make them available to the brokers in the broker StatefulSet:
Now that the keystore and truststore are available to your broker pods, we can configure them to enable SSL encryption and authentication. Add/modify the following environment variables to the existing Kafka broker StatefulSet created by the Helm template to enable SSL:
The Kafka docker image seems to be hardcoded to look for keystore files under /etc/kafka/secrets, so no need to specify the mount path.
Modify the ADVERTISED_LISTENERS environment variable to specify SSL as the protocol for the listeners:
This creates two listener ports; one for inter-broker communication and the other for external. The difference between the two being the advertised listener address, which is the address that clients are directed to look for brokers. Now, kubectl apply the statefulset.yaml updates and check the container logs. At this point inter-broker communication should be happening successfully over SSL.
This creates a “secured-topic” topic that is only be writable by a client with the private key of the certificateCN=Dataflow,OU=1,O=1,L=1,ST=1,C=1 , signed by the root CA in the broker’s truststore. So, the user there should be changed to match the client certificate produced earlier.
The default Helm chart installation gives you three Kafka brokers, each only accessible from clients within the Kubernetes cluster. Allowing access from outside the cluster is tricky, particular with a multi-broker setup, but possible. The first step is to create an external service for every broker pod in your cluster.
Why can’t I just have one external load balancer, balancing the load to all the broker pods, doing its load balancing thing?
Each client requires the ability to connect to a specific broker, depending which is the leader of the partition being written to. This means that each broker, which is a pod in this setup, needs to be externally addressable by the client. One way of achieving this is to create a LoadBalancer service for each broker, and have that service’s selector route to it’s broker pod.
Repeat the below for each broker, to create a LoadBalancer with a public IP (you can do it like this on GKE):
Then modify the LoadBalancer services to point to a particular broker pod using the pod name label:
Now, we need a way to set each broker’s ADVERTISED_LISTENERS variable in the broker StatefulSet to be configured with it’s unique external LoadBalancer address, which will be different for each broker pod. A relatively simple way to do this is to have a DNS sub-domain for each broker that directs to its corresponding LoadBalancer IP address, so then you can just use the KAFKA_BROKER_ID to form each address, like below:
So with the default three broker install, your subdomain mapping would be something like:
With those changes applied, external access to each broker is now possible.
In case you’re using Google Cloud Dataflow, here’s some example configuration to get it working with Kafka, authenticated via SSL to the external address. Basically, configuring the keystore and truststore similarly to the broker configuration and making them accessible to your Dataflow job.
github.com
In summary, getting the Helm chart install of Confluent Kafka platform to support SSL, while not configured out of the box, is simple enough to set up. And if you want it to be accessible from outside your cluster, you’ll probably want to do it.
Data Engineer @weareservian
47 
2
47 
47 
2
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
"
https://blog.doit-intl.com/gcp-announces-no-more-free-external-ips-estimate-your-future-costs-11bd3a8193cc?source=search_post---------88,"What used to be a free service will cost a little more now, as Google Cloud decided to charge customers for compute instances that use external IPs as of January 1st, 2020. According to the announcement, the charging will be applied as an additional cost of $0.004 US per hour for an on-demand machine, while preemptable instances will be charged only $0.002 per hour for the use of external IPv4 address. Running a machine for an entire month will result in an extra of $2.92 or $1.46 per month, respectively. It is worth mentioning that unused static IP charges will remain unchanged at $0.01 per hour ($7.30 per month).
As Google Cloud’s largest world-wide partner, DoiT International has set up a short script that will assist cloud users to analyze their current environment to spot VMs running with external IPs. Running the script will allow users to estimate the impact of the upcoming changes to external IPs charging throughout their organization. “We strive to achieve cost efficiency for our customers, but we would also wish to share our knowledge with broader global cloud community,” says the company’s CTO.
The script lists all the instances that have external IPs across all the projects of a GCP organization, using the gcloud command-line tool. It uses a service account with the viewer role permissions for the organization level to make the API calls to Google Compute Engine API. You can create such a service account, or have your GCP organization admin create this service account for you, and provide you with the authentication JSON file. If you are authenticated with organization viewer permissions, you can run the script without authenticating with a service account first.
This execution will list all the projects under the organizations to which the caller is authorized to view and iterate through those projects searching for VMs that has an external IP assigned.
You can also execute this script for a single project by passing through the project name like below:
In the folder that the script is executed from, is a file named projectsIPs.csv, like the one below:
Running the script will reveal which VMs are currently running in your organization that may incur additional charges. The first step would be to optimize your projects’ VMs and remove external IPs from VMs that don’t require internet access. However, if these machines do require external accesses, you might want to consider egressing traffic through a NAT gateway (which has its own pricing considerations). There are of course other ways in which you can optimize your VMs to pay only for the external IPs you really need. To discuss that, feel free to contact us through here or leave your thoughts in the comments below.
Liked this story? Follow DoiT International on Medium to get more cloud cost-optimization stories.
Software & Operation Engineering. Written By Engineers.
207 
1
207 claps
207 
1
Written by

Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
Written by

Here, our SRE and SWE teams share the solutions to some of the most interesting challenges we encounter during our daily work at DoiT International. If you like what we do, check out our https://careers.doit-intl.com page and join the team!
"
https://medium.com/google-cloud/secured-nifi-cluster-with-terraform-on-the-google-cloud-platform-58c0ca6624d7?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
This story is a follow up of this previous story about deploying a single secured NiFi instance, configured with OIDC, using Terraform on the Google Cloud Platform. This time it’s about deploying a secured NiFi cluster.
In this story, we’ll use Terraform to quickly:
Note — I assume you have a domain that you own (you can get one with Google). It will be used to map a domain to the web interface exposed by the NiFi cluster. In this post, I use my own domain: pierrevillard.com and will map nifi.pierrevillard.com to my NiFi cluster.
Disclaimer — the below steps should not be used for a production deployment, it can definitely get you started but I’m just using the below to start a secured cluster (there is no configuration that one would expect for a production setup such as a clustered Zookeeper, disks for repositories, etc).
If you don’t want to read the story and want to get straight into the code, it’s right here!
What is Terraform?
Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom in-house solutions.
Configuration files describe to Terraform the components needed to run a single application or your entire datacenter. Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to determine what changed and create incremental execution plans which can be applied.
The infrastructure Terraform can manage includes low-level components such as compute instances, storage, and networking, as well as high-level components such as DNS entries, SaaS features, etc.
What is NiFi?
Apache NiFi is an easy to use, powerful, and reliable system to process and distribute data. Apache NiFi supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. In simpler words, Apache NiFi is a great tool to collect and move data around, process it, clean it and integrate it with other systems. As soon as you need to bring data in, you want to use Apache NiFi.
Why ZooKeeper?
Best is to refer to the documentation, but, in short… NiFi employs a Zero-Master Clustering paradigm. Each node in the cluster performs the same tasks on the data, but each operates on a different set of data. One of the nodes is automatically elected (via Apache ZooKeeper) as the Cluster Coordinator. All nodes in the cluster will then send heartbeat/status information to this node, and this node is responsible for disconnecting nodes that do not report any heartbeat status for some amount of time. Additionally, when a new node elects to join the cluster, the new node must first connect to the currently-elected Cluster Coordinator in order to obtain the most up-to-date flow.
First step is to create the OAuth Credentials (at this moment, this cannot be done using Terraform).
Once the credentials are created, you will get a client ID and a client secret that you will need in the Terraform variables.
By creating the credentials, your domain will be automatically added to the list of the “Authorized domains” in the OAuth consent screen configuration. It protects you and your users by ensuring that OAuth authentication is only coming from authorized domains.
In your GCP project, create a bucket in Google Cloud Storage. We are going to use the bucket to store the Apache NiFi & ZooKeeper binaries (instead of downloading directly from the Apache repositories at each deployment), and also as a way to retrieve the certificates that we’ll use for the HTTPS load balancer.
Note — you’ll need Apache ZooKeeper 3.5.5+.
You can download the binaries using the below links:
Here is what it looks like:
Note — you’ll need to use the NiFi Toolkit version 1.9.2
Once you have completed the above prerequisites, installing your NiFi cluster will only take few minutes. Open your Google Cloud Console in your GCP project and run:
If you execute the above commands, you’ll be prompted for the below informations. However, if you don’t want to be prompted, you can directly update the variables.tf file with your values to deploy everything.
Variables to update:
Here is what it looks like on my side (after updating the variables.tf file):
The first step is to deploy the NiFi Toolkit on a single VM to run the CA server that is used to generate certificates for the nodes and the load balancer. Once the CA server is deployed, a certificate is generated for the load balancer and pushed to the Google Cloud Storage bucket.
The script you started is waiting until the load balancer certificate files are available on GCS. Once the files are available, files are retrieved locally to execute the remaining parts of the Terraform template. It will deploy the ZooKeeper instance as well as the NiFi instances and the load balancer in front of the cluster. All the configuration on the NiFi instances is done for you. Once the script execution is completed, certificates files are removed (locally and on GCS).
The load balancer has been created and you can retrieve the public IP of the load balancer:
You can now update the DNS records of your domain to add a DNS record of type A redirecting nifi.pierrevillard.com to the load balancer IP.
I can now access the NiFi cluster using https://nifi.pierrevillard.com and authenticate on the cluster using the admin account email address I configured during the deployment.
Here is my 6-nodes secured NiFi cluster up and running:
I can now update the authorizations and add additional users/groups.
Note — you could use Google certificates instead of the ones generated with the CA server to remove the warnings about untrusted certificate authority.
To destroy all the resources you created, you just need to run:
terraform destroy -auto-approve
As usual, thanks for reading, feel free to ask questions or comment this post. More posts about NiFi can be found on https://www.pierrevillard.com/.
Google Cloud community articles and blogs
8 
1
8 claps
8 
1
Written by
Apache NiFi PMC member | Working @Google | Twitter & Github — @pvillard31 | Blog @ www.pierrevillard.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Apache NiFi PMC member | Working @Google | Twitter & Github — @pvillard31 | Blog @ www.pierrevillard.com
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@asicoderofficial/jupyter-notebook-google-cloud-44fbc34c9b03?source=search_post---------90,"Sign in
There are currently no responses for this story.
Be the first to respond.
Asier Serrano
Oct 3, 2020·1 min read
Fast and easy configuration, without external packages.
Once you are connected to your remote machine, follow these steps:
2. Create the Jupyter configuration file:
"
https://medium.com/@_stevenlevine/querying-external-data-with-bigquery-1e7d055df4bc?source=search_post---------91,"Sign in
There are currently no responses for this story.
Be the first to respond.
Steven Levine
Nov 19, 2019·7 min read
TLDR — In this post I will walk through how to use BigQuery’s new capability of querying Hive Partitioned Parquet files in GCS. It is a really cool feature..
I have a huge interest in Data Lakes, especially when it comes to the query engines that are capable of querying cloud object stores like Spark, Presto, Hive, Drill, among others. With that being said, when Google Cloud announced that BigQuery…
"
https://servian.dev/migrating-dialogflow-webhook-from-cloud-function-to-app-engine-6835a3dd52c1?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
Scott Shellien-Walker
Oct 23, 2018·5 min read
In Dialogflow we may want to integrate with other external systems in order to send information from the user and dynamically populate the response. This would be essential if a chatbot needs to book movie tickets, check flight times or stream conversation data to analytics platforms.
The easiest way to create a webhook endpoint, and the one suggested in the Dialogflow documentation, is a Cloud Function. Cloud Functions are serverless, autoscaling, very cheap and can run functions in several languages, so they are a good way to limit overhead. However there is no dedicated instance when using Cloud Functions. The cold start times can mean slow response times, and therefore a hindered experience for users of your Dialogflow chatbot.
If you are suffering from extended response times, or your function has to do a lot of work, such as calling multiple external services, another solution is to move away from serverless and onto a dedicated server on Google’s App Engine. Using App Engine will allow you to have a dedicated instance for your Dialogflow fulfillments, and offers more options on the size and speed of the server needed the handle larger functions.
The endpoint URL can be set in Dialogflow in the fulfillment section.
For each intent the webhook then can be enabled. The Dialogflow request will be sent to the webhook, and in turn the response will need to come back from the webhook.
Google Cloud Functions are a great way to implement the webhook as there are no servers to provision and they scale automatically. We set the http trigger for Dialogflow to hit…
Then add our source code to handle each Dialogflow event and send the response back. Notice here that the Firebase framework allows you to set the function inside index.js that is evoked by the trigger.
Cloud Functions are powered by Firebase so we need to import the Firebase module, and the Firebase function framework will then take care of initiating express and parsing our request and response jsons using body parser.
Our app in this case is the Actions on Google module designed to handle Dialogflow events via the Google Assistant.
Then for each intent we can specify some behaviour such as the response to give. The conv.ask() method here will send the SSML response once this intent is trigged. In the app.intent() function we could also call another external services to populate the wait time of the coffee, for example, or send our conversation data to a database such as BigQuery or Chatbase.
Cloud Functions however, suffer from the drawback of having roughly ~10s cold start times. The server is not waiting to receive your function, but is instead triggered when it is needed, which can lead to slow chatbot conversation response times, especially on the first intent.
The advantage of using Cloud Functions is that they are cheap and readily available but with the release of App Engine Standard Edition, which has a free tier, the same result can be achieved on a dedicated server without too much operational overhead.
To move to App Engine we can use the same index.js and package.json files, but we also need to define our App Engine build in an app.yaml file. For App Engine Standard Environment we just need to set the runtime (for Node.js only 8 is available on Standard).
In the index.js file we need to start express and body parser as we will no longer be using the Firebase framework that automatically handles this when using a Cloud Function.
Define the Google on Actions app in the same way to handle intent conversation requests.
Then set the body parser to handle the request and response jsons from the Actions on Google app and set the express app to listen for requests coming into the endpoint.
Initial the GCloud SDK usinggcloud init and select the appropriate Google project. Navigate to the directory you index.js, app.yaml, and package.jsonfile are stored and simply type gcloud app deploy to push the files up to the App Engine instance.
Set the webhook endpoint in Dialogflow to the new App Engine site for post requests to be handled by the new app.
Information on the improvement in performance to follow…
28 
3
28 claps
28 
3
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
"
https://medium.com/codex/stop-using-external-hard-drives-389f401f2f29?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
External hard drives are not the best way to store valuable data anymore. If you truly care about your memories, photos and videos, work-related info, book library, etc, I suggest forgetting that archaic piece of metal.
The main reason to skip hard drives is reliability. 10 years ago I owned a 0.5TB Transcend hard drive and always worried about keeping it safe. It needed to be clear from falling and magnets or whatever. I kept photos, videos, music and books on it. Eventually, I just put it on the shelf.
Every time I moved to a new apartment, which happened once every 2–3 years I had to bring it with me. Lately, I discovered that it doesn’t work anymore. My old data now is marinated bits on broken magnet tapes. I lost 0.5TB of data in 10 years.
Now let’s look at what cloud storage provide. AWS S3 or Google Cloud offers storage plans claiming 99.999999999% durability or 11 nines. To put it simply, for 10'000 objects (files) it may lose one every 10 million years. Now that’s durability I’m talking about. It means my most embarrassing moments captured in the photo will be kept for generations.
There are less durable options (according to Service Level agreements) for less price but they are still impressively durable. They achieve it with data redundancy. Data is replicated across several availability zones, datacenters. If one becomes unavailable or destroyed by asteroid data is still stored on the other ones.
It is like having 3 external hard drives in 3 different facilities, separated geographically, guarded, secured and always online. And with backup power sources. But more reliable because service is provided by industry giants pioneering in the cloud business for years.
Don’t even get me started on connecting hard drive to different computers and OS.
I started with Windows on the Asus laptop, then switched to Linux and finally landed with MacBook. If the drive worked fine with Windows, it was a pain to use with another system because of file system incompatibility. I either need to format it in NTFS or EFS4 and use it with one OS only.
I haven’t enough UNIX evil in my soul to make it work. I just wanted data to be available. Why do I need this pain in the ass?
When I travel I want as few cables and devices as possible. Do I need to book baggage for my hard drives and live in constant fear of getting my data corrupted? No, sir! They going to be recycled, except there is no company in my country that is going to recycle that trash. And creators of that junk don’t bother either.
Hard storage is the cheapest (although slowest) part of computational resources nowadays. However, big companies like Apple or Samsung needs money (who doesn’t) and squeeze any additional dollar of potential profit. For example, they charge additional X% of the initial device price for each 64GB of additional storage. They force us to worry about how to fit all media that we produce and consume. And we overpay for 1–2TB SSD. Same device. 1TB. Twice the price.
Companies once produced floppy drives and CDs now produce flash drives and external hard drives. If we look closely and compare the prices for hard storage vs cloud storage, you may be surprised.
Apple 16' MacBook difference between 512GB and 1TB is 400$. The difference between MacBook Air 256GB and 512GB is 250$. Apple thinks that 256GB costs 250$. Ok.
Microsoft Surface Go difference between 128GB and 256GB versions is 50$. Microsoft charges 50$ for 128GB — 150% less than Apple.
On Amazon, you can buy an external hard drive with 1TB capacity for about 100$. It is way cheaper than top laptops with built-in memory.
Using AWS S3 we can store 1TB of data for ~1$, ~4$, ~13$ or ~22$ per month depending on the frequency of access. Google Cloud Storage offers to store 1TB for ~4$, 7$, 10$ or 26$. Azure Storage has similar prices. Oracle offers 32GB for free forever! You make conclusions.
Data stored on the cloud will have high durability if set up correctly. It will be secured and accessible from any place on Earth with the internet. And free of ritual dancing around hard disks, system drivers, USB connectors and cables.
Cloud economy rules.
But you still need fast internet to access it, though.
Data has value when analyzed or consumed. Not when stored. Why limit yourself to carry the burden of fragile pieces of metal and plastic if you can throw data into the cloud?
Everything connected with Tech & Code. Follow to join our 650K+ monthly readers
26 
5
26 claps
26 
5
Everything connected with Tech & Code. Follow to join our 650K+ monthly readers
Written by
Software Engineer / Solution Architect - Інженер програмного забезпечення
Everything connected with Tech & Code. Follow to join our 650K+ monthly readers
"
https://medium.com/hashmapinc/why-you-should-use-snowflakes-external-tables-d209db04eff8?source=search_post---------94,"There are currently no responses for this story.
Be the first to respond.
Snowflake in its modern avatar is a data cloud that offers a collection of services built atop a highly elastic distributed processing engine. Among the multiple services it offers like data engineering, data warehousing, data sharing, data science, and data lakes, it also paves the way for a connected ecosystem of data applications that can be accessed from a single stop.
The applications written for the platform are typically powered by E-T-L or E-L-T pipelines running in the background to ensure that data is moved into Snowflake and then across it through various logical layers of “data preparedness” until it is ready to be served or consumed.
When designing data infrastructure with Snowflake it is important to evaluate all options. The purpose of this post is to share an option for designing a Raw data layer to power the higher-level models in data pipelines.
So far, all data that is intended to be processed within Snowflake has to be onboarded using the COPY INTO command or via PIPE (Snowpipe). This works out well if all the processing on that data was to be done within Snowflake alone.
An alternative approach or secondary solution to onboarding data focuses on the data lake. With this approach, data can be stored outside the boundaries of Snowflake in cloud object stores as flat files (in a variety of formats) thereby making the data available for any present or future tool/technology without explicit data extraction.
With Snowflake this approach is implemented via the EXTERNAL TABLE feature which gives a tabular window of access over the flat files that reside externally within the cloud.
These are Snowflake objects that overlay a table structure on top of files stored in an EXTERNAL STAGE. They provide a “read-only” level of access for data within these remote files straight from the object store.
These tables store metadata (name, path, version identifier, etc) in order to facilitate this type of access, which itself is made available through VIEWs and TABLEs in the INFORMATION_SCHEMA.
Snowflake provides DDL statements to create, alter, drop, and show external tables within a SCHEMA.
Fittingly, external tables can also be configured with “auto-refresh” so that any new files dropped at the connected EXTERNAL STAGE can get registered automatically. This does require enabling another type of Snowflake object (i.e. a NOTIFICATION INTEGRATION, which uses a cloud vendor’s messaging service to be notified of the file events in the object store).
Side Note: Without using this setup, file detection can still be triggered using the ALTER EXTERNAL TABLE…REFRESH statement which will need to be manually executed periodically.
Let's run through a scenario and take this feature for a quick spin.
Imagine there is a network of sensors that captures the device statuses periodically and pools all the information together in a CSV file to be dumped into object storage at every hour. A real-life frequency could be even lower considering the import of the activity being “sensed” through the devices (…and maybe not routed via an object store, but its just an example).
Assume that every file contains the device’s identification, its location, and present status/condition.
This file now can be queried directly from the object store and inside Snowflake via an external table just like any other table as:
The difference between an internal table and an external table would be in the number and type of columns that external tables make available straight out of the box, namely ~
- Value : A VARIANT type column that contains the entire record as json
- Metadata$Filename: Name of the file(s) present at the object store, optionally preceded by any partition directories defined.
It is possible to provide additional column definitions when creating the table, but they would be virtual and calculated using expressions based on the Value column. Virtual column definitions could also be provided to create partitions based using expressions based on Metadata$Filename.
Using this data it is now possible to build a model on top that stores the history (say, Type-2 SCD) of device statuses such that every change in the state of a device over the course of its entire lifetime on the network gets captured on a per-record basis.
Over time as files land in the object store, data will build-up; and to understand status changes at regular intervals for each sensor, CDC (Change Data Capture) should be performed on this accumulating data. To be specific, the main point of interest should focus on capturing the most recent file(s) containing possible updates to devices.
At this point, you can add a STREAM on top of the external table to identify the records from the latest file or set of files dropped in the bucket.
Please note that STREAMs on external tables are “insert-only” which means that they track new files as and when they get added. However, they will not keep track of any files that get removed.
First, by designing the file contents in such a way that the changes are captured in a meaningful manner recording all kinds of DML ops.
In this example scenario a possible design option could be to always include one row of data (status, location etc) from each sensor whether it’s state changes or not. In this case, a sensor going down or taken off grid (delete op) will always be *explicitly* called out in the row with a corresponding status value; as will any other change to its state (update op) or even a new sensor being added to the network (insert op). This provides ‘at-least once’ semantics with one row per sensor in each file.
Second, by writing the MERGE statements that use the stream in such a way that all types of DML operations get handled leveraging the knowledge of file semantics.
Assuming the file semantics from before, the MERGE statements will always have to factor in the sensor state to decide if its time to upsert a record for a sensor or not.
Also, if the file contains multiple statuses or if multiple files get accumulated before the history model is refreshed, the merge statement would also need to factor in the time of status change and apply that in order to build the right history. This necessitates the presence of a timestamp-based watermark column.
Side note: For those familiar with dbt, another approach as opposed to writing MERGE statements can be to use ‘snapshots’ to create models with history:
Overall, using external tables is as straightforward as using internal or local tables with just a few notes on how they are created/configured.
However, there are a few caveats, and pros/cons to be pondered over.
- This approach leverages the single copy of data already extracted from sources into the object storage. - The format of the data remains open for other tools/technologies to process in case the need arises in the future.- Raw data storage only needs to be managed at one level which also simplifies cost management for storage.
- External tables are less performant compared to local tables and therefore not a suitable choice for higher level tables; especially tables that directly feed BI reports or other service layers to consumers.- Additional supportive objects are needed for enabling notification integration and auto refresh.
- Since external tables are not as performant as local Snowflake tables, using them at any higher data zone might not be the best choice. However, using them to make the Raw zone for data that is batch processed would still be a good choice. - Use them if the auto-refresh latencies are acceptable and in line with your data processing/publishing SLAs.- When designing CDC over external tables, first design how the changes (especially deletes) will be captured from a source in the file extracts that get placed in the object store.
All things considered, using external tables can be a viable approach to building a data lake with Snowflake. It saves one hop in an ETL/ELT pipeline and best of all, that second copy of data is no longer needed within Snowflake which means one less cost profile to manage.
The window of opportunity to process data within Snowflake just got wider and more accessible.
Note: The dataset shown in the example was taken from data.world’s open dataset “ArrayOfThings”.
At Hashmap, we work with our clients to build better, together.
If you are considering moving data and analytics products and applications to the cloud or if you would like help and guidance and a few best practices in delivering higher value outcomes in your existing cloud program, then please contact us.
Hashmap, an NTT DATA Company, offers a range of enablement workshops and assessment services, cloud modernization and migration services, and consulting service packages as part of our Cloud service offerings. We would be glad to work through your specific requirements.
Feel free to share on other channels, and be sure and keep up with all new content from Hashmap here. To listen in on a casual conversation about all things data engineering and the cloud, check out Hashmap’s podcast Hashmap on Tap as well on Spotify, Apple, Google, and other popular streaming apps.
www.hashmapinc.com
medium.com
medium.com
medium.com
medium.com
Chinmayee Lakkad is a Regional Technical Expert and Cloud/Data Engineer at Hashmap, an NTT DATA Company, and provides Data & Cloud solutions and expertise across industries with a group of innovative technologists and domain experts accelerating high-value business outcomes for our customers.
Innovative technologists and domain experts helping…
17 
17 claps
17 
Written by

Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by

Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/catalog-external-assets-for-a-360-data-lineage-448c8f6bf2b2?source=search_post---------95,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yannick Saillet
Jan 19, 2021·15 min read
In a previous article I have shown how to automatically catalog a high number of data sets by using IBM Cloudpak for Data and in particular Watson Knowledge Catalog.
A good enterprise catalog is the key stone of data governance. It is the place where the existence of all data and governance assets can be documented as well as their relationships with each others. Capturing the relationships between the assets is essential in order to answer data lineage questions, determine the dependency graph of any given asset, or do an impact analysis.
For example, a well maintained catalog should help answer questions like:
Where do the data from this data set come from?
Where are the data from this data set used?
Which asset will be impacted if this asset is modified or deleted?
A business catalog can only answer these questions if all the assets being directly or indirectly related to the assets of interest are properly catalogued. Such assets can be of very different nature like:
This list can be long and the type of assets playing an important role in the data lineage may be very different depending on the technology used by your company.
To complicate the problem, real life scenarios seldom involve only homogeneous solutions from a single vendor. It is very frequent that companies use solutions from different vendors, running on different systems but all playing a role in the way how data assets are produced, transformed or consumed.
No software vendor can provide a business catalog which can support out of the box all kind of assets you may encounter in a real life scenario. Therefore it is critical that the catalog can be customized to support new asset types, load them and captures their relationships with other catalogued assets, no matter what the nature of these new assets is.
The goal of this article is to show how to do this with IBM Watson Knowledge Catalog.
IBM Watson Knowledge Catalog is the business catalog of IBM Cloudpak for Data and supports out of the box very different asset types such as:
The list is not exhaustive and keeps increasing for each new version of Cloudpak for Data.
But like any software, unless your processes only involve assets created with Cloudpak for Data, the chances are high that your data and governance flows involve external assets which are not supported out of the box by the catalog, but still should be catalogued in order to get a complete data lineage. Such external assets could be ETL flows, reports or data models from other vendors, physical devices in an IoT flow, or assets from other IBM products managed outside Cloudpak For Data.
Fortunately IBM Watson Knowledge Catalog provides an API to register new type of assets and load external assets via a REST API. We will see in the rest of this article how to use this API with a simple example.
The customization of Watson Knowledge Catalog could be demonstrated with any kind of asset, but for the purpose of this article, I will demonstrate the process by showing as an example how to add support for IBM Information Analyzer data rules running in a classical Information Server outside Cloudpak for Data. You should be able to transpose the procedure to a different type of asset.
Data rules is an asset type created and executed in IBM Information Server, which defines a non trivial data quality constraint that has to be verified by all rows of a data set where the rule is applied. For more details on data quality constraints, see my previous article “How to quantify Data Quality”.
Explaining the details of data rules is outside of the scope of this article, but for the purpose of this exercise, the only thing that you need to understand for the moment is that a data rule is an asset with the following characteristics:
Information Analyzer data rules have other properties — like the binding details or output definitions, etc… — but in order to keep the example simple, we will only consider the previously listed properties for the purpose of this exercise, because they are sufficient to capture the dependencies between the data rules and the other assets.
An IBM Information Server installation may have hundreds of data rules defined it it. Each of them may be related to different data assets. In the rest of this article, we will see the different steps necessary to:
In order to keep this example simple, we will assume that all data assets referenced by the data rules to load are already catalogued in Watson Knowledge Catalog. This could be done by running an automatic discovery of the data sources referenced by the data rules, as described in my previous article.
In order to simplify the commands that we will use along this article, let us first define in an UNIX shell an environment variable WKC_HOST containing the host name of the Cloudpak for Data system where we want to catalog the data rules.
(You need to replace cpd_cluster_host with the host name of IP address of the target system)
Next, let us obtain an authentication token by running the following command as described in the CP4D documentation.
(Replace username and password with the credential details of an authorized user)
The response of that command is a JSON document as follow:
Let us copy the value of the token property and put it in another environment variable, so that we don’t need to copy that token in each command:
Now that we have an authentication token, we are ready to use the WKC REST API to manipulate the asset types of Watson Knowledge Catalog as described in the documentation. You can use the REST API to explore the default asset types and retrieve the JSON representation of their definition to better understand how they are defined.
In our case, we will create a new custom asset type named data_rule, by using the REST API POST {service_URL}/v2/asset_types?catalog_id={catalog_id} as described in this section of the documentation.
Custom asset types are specific to a catalog. Watson Knowledge Catalog may contain multiple catalogs, so that you need to find out the ID of the catalog where you want to import the new assets. You can easily determine the id of the catalog by opening it in the UI and looking at the URL in your browser. The catalog ID is a long string just after the path element /catalogs/ in the URL, as highlighted in the following example:
Here again, let us copy that ID and put it into an environment variable:
The POST command to create a new asset type requires as a body a JSON document which defines the meta-data of the new asset type to create. At a minimum, the document should contain the name and description of the new asset type and a description of its fields. In our case, we will use the following properties:
Note that we don’t need to define a property for the data assets related to the data rules, as these will be defined by using asset relationships which WKC allowed us to define between any kind of asset.
Based on this initial minimum list of fields to catalog for a data rule, the payload of the REST API to create the new asset type can be defined as follow:
Save the definition of the new asset type in a file data_rule_asset_type.json and run the following command to create the new asset type in the target catalog.
If the command is successful, it should return a success code (200) with the JSON definition of the new created asset type.
Let us next try to create a sample asset of the new type by using the API POST {service_URL}/v2/assets?catalog_id={catalog_id} as described in the documentation.
Like for the creation of the new asset type, we need to prepare a payload in JSON format describing the new asset to create in the target catalog.
Create a file dummyDataRule.json with the following content:
This payload contains the minimum information required to create a new asset:
Let’s submit the creation of this new data rule in the catalog by running the following cURL command:
If the command is successful, it should return a JSON description of the new created asset containing its id and you should be able to see the new created dummy data rule in the Watson Knowledge Catalog UI when opening the target catalog.
Like for any other asset, the new created custom asset can have relationships to other assets. You can define those relationships by using the UI, but in our example we want to do that programmatically, since we want to automatically import a large number of assets.
The API for creating a relationship between assets is documented under this link. In short, the API is PUT /v2/assets/{asset_id}/relationships/{relationship_name}?catalog_id={catalog_id}with a JSON body specifying the asset and catalog ids of the target assets.
In order to test this feature, let’s find out the id of a data set in the same catalog and define a relationship of type “uses/used by” between our dummy data rule and that data set. An easy way to find out the id of a data set is to open the data set in the UI and copying its id from the URL in the browser.
The JSON payload that we have to pass to the PUT command to create the relationship is as follow:
Replace the values for the catalog_id and the asset_id with the ids for the data asset — or it can be any other asset type — that you want to link to the data rule and save the payload in a file addRelationship.json. Then create the relationship by using the following cURL command — you will need to replace the placeholder <datarule_id> with the id of the dummy rule created previously.
After the command has run you should be able to see the new relationship of type uses when opening the data rule in the UI. If you open the data set, you will see the reverse relation ship of type used by pointing to the data rule.
Now that we have successfully defined the new asset type and tested that data rules can be imported into the catalog by using REST APIs, let’s automate the process to extract and catalog all data rules from an external IBM Information Server installation.
The first problem is to extract the meta-data of the rules. Information Server provides several mechanisms to do that. You can either use a REST API, or write SQL queries against the meta-data database (XMETA). In this article we will use SQL.
Explaining the SQL API of IBM Information Server is out the scope of this article. For more details you can refer to the Information Server documentation. For the simplicity of the article, I have prepared a ready-to-use SQL query which allows to retrieve the name and description as well as the path of the related data sets for all data rules of a given Information Analyzer project:
This query will return a result as shown in the next illustration. If a rule is bound to more than one table, the result will contain one row for each bound table. Therefore in the example shown below, the project “test_rules” contains 2 data rules, one is named “child_test_BANK_CLIENTS” and is bound to a data set with the path /BANK1/BANK_CLIENTS. The other one is named “credit_card_age_BANK2” and is bound to two data sets with the paths /BANK2/BANK_ACCOUNTS and /BANK2/BANK_CUSTOMERS.
From there, we need to write some script that will:
In order to keep this article simple, I have implemented all these steps in Python in a Jupyter noteboook:
After running all the cells of the notebook — you will need to adjust a few variables with the details of your system, as described in the notebook — , you will see in Watson Knowledge Catalog all the external data rules represented as normal catalog assets.
When opening the data rules, you can see that the relation to their related data sets have been created — assuming that the data asset were already imported in the catalog when the notebook was executed.
The same relationship can also be seen from the data asset, which contains a reverse relationship of type “Is used by” pointing to the data rule.
The new created custom asset are shown like any other asset type natively supported by Watson Knowledge Catalog with the exception of the Asset preview tab which remains by default empty.
It is also possible to embed a custom asset preview UI in the catalog as long as it is a Web UI which can be opened in a Web browser.
In our example the Information Analyzer Web UI for looking at the details of a data rule can be opened with the following URL:
Watson Knowledge Catalog provides a way to register a URL to invoke when the UI for previewing a particular asset type needs to be displayed. This is done by slightly updating the JSON definition of the asset type that we created previously:
The additional attribute marked in bold specify that, whenever an asset of this type needs to be previewed, WKC should invoke a URL built from appending the base_client_url and the value of the asset property specified in the attribute url_path_from_asset_attributes.
In order to test this change, replace <iis_hostname> with the name of your Information Server host name from which the data rules have been imported, and save the modified JSON document in a file named data_rule_with_preview_asset_type.json. Then update the definition of the asset type by using the following command:
This command will update the existing definition of the data_rule asset type. After the update, if you reload the asset preview screen of the catalog, you should see a message — at least on Firefox — indicating that for security reason the page cannot be embedded but you can load it in a different browser tab. This is a security feature from the Information Server UI preventing the original UI to be embedded in another UI, but this will still provide a convenient way to open this UI in a different tab. By deploying a different web application, it would be possible to embed UI panels which do not have this security limitation.
We have seen in this article how important it is for a catalog to support custom asset types in order to capture the full lineage. We have also seen how this can be easily done in Watson Knowledge Catalog by using its REST API. As a concrete example, we have customized a catalog to support external Information Analyzer data rules as a new asset type, and run a script to import all external data rules to our catalog. Then we have seen, how to create programmatically relationships between the assets. Finally we have explored how the UI itself can be customized with a specialized asset viewer.
The same procedure can be applied to support any kind of asset type for which it is possible to extract metadata information in order to import them to the catalog. We have only explored a few REST APIs. Other API commands would also allow us to define special types of relationships between the assets to better capture the semantic.
Software Architect, Master Inventor @IBM — Architect for Data Profiling and Data Quality in Watson Knowledge Catalog on IBM Cloud Pak for Data, IBM Cloud.
24 
24 
24 
Your home for data science. A Medium publication sharing concepts, ideas and codes.
"
https://medium.com/@marekbartik/google-kubernetes-engine-with-external-dns-on-cloudflare-provider-24beb2a6b8fc?source=search_post---------96,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marek Bartík
Feb 26, 2018·1 min read
Having a bunch of kubernetes services and ingresses you will need some automation with creating DNS records. Let’s say you use CloudFlare as a DNS provider and your dev flow is to push in git repo, automagically deploy to GKE your app which is later exposed via kubernetes ingress externally. Your Ops takes the new ingress IP and creates a dns A record branch-name.yourdomain.tld
It is not hard to automate this with CloudFlare API and checking the existing services/ingresses with cron with somehow finding out what the DNS will look like.
Using external-dns, however, I have all of this out-of-the-box thanks to zalan.do running as a pod in GKE.
With ingress like this:
The external-dns pod checks the ingress (and/or services) for annotations and creates DNS records in upstream DNS provider.
Just insert your CloudFlare API key and email and create the respective service account (and ClusterRole and ClusterRoleBinding) and kubectl apply this!
Check the pod’s logs:
Yay, it works!
Source repo with all the kubernetes resources necessary: https://github.com/bartimar/gke-external-dns-cloudflare
Check the external-dns tutorials for other cloud/dns providers like AWS, Azure, DigitalOcean and Google DNS, Route53 and so on.
DevOps freelance consultant minimalist https://marekbartik.com
25 
25 claps
25 
DevOps freelance consultant minimalist https://marekbartik.com
"
https://medium.com/@wakeupcoders/how-to-use-external-libraries-in-lambda-function-df1cee4a7c3a?source=search_post---------97,"Sign in
There are currently no responses for this story.
Be the first to respond.
Wakeupcoders
Aug 23, 2020·5 min read
In this blog, we are going to add libraries to our lambda function so that we can use external libraries for our use because AWS has Boto3 library only, so if you want to use other libraries than you have to add those libraries to your function layer.
so let's start the process of how to add libraries. In the previous blog, we have already explained to you the lambda function and how to create the lambda function.
Step 1: go to your lambda function which we have created in the previous blog, you can see in the below image.
Step 2: Go to the code section and here we want to use the Flask library of python so we will import the flask library by writing some code, After writing the code save your code by clicking on the save button as shown in the below image.
Step 3: After saving the code click on the test button to check if your code is working or not.
Step 4: You can see in the below image there is an error because it is not able to find the library or module name flask, so we have to add the library to the layer of our function.
Step 5: now go above and click on layers to see if any layer is connected or not.
Step 6: As you can see in the below image there is no layer added so we have to add the layer of our libraries to it.
Step 7: Create on folder give it any name and open the command prompt in that folder and create one directory of as shown in the below image and install flask to that particular folder.
Note: you have to create a virtual environment for this task otherwise it will do the change in your original flask library in your system.
Step 8: After downloading the library you have to create a zip file of your folder we have created a package.zip file of that as you can see in the below image.
Step 9: Now go to the layer option you can see in the below image it is marked in red, click on that layer option to add library.
Step 10: you can see we have already added libraries in our layer but we have to add another library so click on create layer option as marked in the below image.
Step 11: give any name to your layer, upload your zip file and choose the compatible runtime, you can see in the below image how we have done.
Step 12: After adding all the details click on create option.
Step 13: After creating again go back to your lambda function and click on the layers option as shown in the below image.
Step 14: After that click on Add layer option, you can see marked below.
Step 15: Now in the below the image we have three options, the first option is AWS layers these are the layers provided by AWS itself, In second the option custom layers in this you can add the layers created by you, and the Third one is specified ARN, this ARN is provided when we create the layers so you can choose either the second option or the third one, we are choosing the third one and we added the ARN of our flask layer ARN.
Step 16: Now if you will go back then you can see in your layer option there is one layer added.
step 17: Again go back to your code and write the code to import the flask library and click on the save button to save it.
Step 18: Click on the test button To check if your code is working or not as shown in the below image.
Step 19: Congrats your code is working properly as shown in the below image.
So this is how we add external libraries to lambda functions, hope you liked this blog.
Keep learning, Happy Coding!!!!!!
We make your business smarter and broader through the power of the internet. Researcher | Web developer | Internet of things | AI | www.wakeupcoders.com
See all (13)
17 
17 claps
17 
We make your business smarter and broader through the power of the internet. Researcher | Web developer | Internet of things | AI | www.wakeupcoders.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iraklis/an-unlikely-xxe-in-hikvisions-remote-access-camera-cloud-d57faf99620f?source=search_post---------98,"Sign in
There are currently no responses for this story.
Be the first to respond.
Iraklis Mathiopoulos
Sep 29, 2016·5 min read
TLDR: While trying to get admin credentials on my locked down Elisa Live IP camera, I discovered a XML External Entity (XXE) vulnerability on one of the backend systems of HiKVision, the manufacturer of the camera which is the market leader on IP cameras and PVRs.
Insecurity and the IoT era go hand in hand. A quick search in google will reveal hundreds of white papers and articles that have been published over the last few years.
About two months ago I had a free weekend so I wanted to play around with a cloud camera. I searched in Amazon for a cheap one with no published security vulnerabilities. I found this one: Elisa Live 720p HD IP Camera.
I will try to keep this post short so I will focus on the discovery of the XXE. I will not detail all the steps that I took to investigate the camera itself, only the relevant parts that lead to the XXE discovery.
The camera itself is a white-labelled RC8221 manufactured by Hikvision, a $20bn company that is the market leader (~20%) in the surveillance products industry. Elisa, a Finnish company, provides a cloud app that you can access in order to view the live stream of your camera.
As the trend dictates, you cannot get access to the camera itself without going through their cloud platform. In other words, the camera’s stream is uploaded to their backend system and you access the stream through a web or mobile app.
I connected the camera through its Ethernet interface to my lab and started intercepting network traffic. If you ever want experiment with IoT devices I highly recommend that for your first round of investigation you do not have an active internet connection. Some devices are shipped with old/insecure firmware, the very first thing they will do is check if an update is available.
Wireshark revealed two interesting and unencrypted calls:
A POST request to www.hik-online.com, Looks like a base64 encoded password. Let’s note this down for later.
And a GET request that downloaded the updated firmware from an s3 bucket.The firmware can be downloaded here.
A quick nmap scan showed a few open ports, including a web server with a login form. After trying a few default username/password combinations common for hikvision cameras, I quickly realised that I wasn’t going to go anywhere. The controller that accepts the credentials is protected with HTTP Digest Access Authentication, a unique feature of this firmware.
Playing around with the firmware using binwalk and hiktools,I was able to extract some interesting information, but nothing regarding the Digest Authentication. Just for reference, the root password is hiklinux, these are the contents of the /etc/passwd file:
The SSH port was closed so I couldn’t take advantage of this. I have to note that on my very first portscan before updating the firmware ssh was open.
At this point I backtracked and tried explore other avenues.
Going back to the first POST request we can see that it includes a base64 encoded string. Unfortunately it decodes to garbage, meaning that it’s probably symmetrically encrypted. The key has to be somewhere in the firmware, it’s just a matter of finding it. I didn’t verify if this password is used to authenticate the camera to the server, or if this is a password that is generated by the camera upon boot and submitted to Hikvision.
I have to admit that I was a bit discouraged with my progress. Over two days spent with little to show. I quickly browsed Hikvision’s website and found this:
Send E-mail to HSRC@hikvision.com to report the security flaw you find and we will contact you as soon as possible. To protect information security for users and enterprise, please do not publish or spread the flaw. HSRC will reward the reporter according to the Hikvision Security Flaw Assessment.
Cool, a bug bounty program. Let’s play around with the POST request.
As this is an XML post request, the first thing I tried was to see if we can request a local or external file with the SYSTEM entity. The local file method didn’t work, so I fired up a VPS and waited for incoming connections. It worked:
Things were starting to get interesting. Since we can use the SYSTEM entity to load external files, let’s try to pass a malicious DTD that will call back to our VPS with the contents of a file, say /etc/hosts.
A great tool that automates this procedure is XXEinjector.
Success. We can read arbitrary files on the server. But what permissions does the Tomcat server run with? We were able to get the contents of /etc/shadow, so root.
At this point I stopped and started the process of disclosing the findings to Hikvision. If I was a malicious attacker, I would probably continue by fully enumerating the 10 servers (The vulnerability existed in all geographically distributed API servers of hik-online.com). If getting the Tomcat database connection strings didn’t lead anywhere, there probably would be more things that one could look for like scripts that connect to other server with hardcoded credentials.
Ultimately, It wouldn’t be that difficult to get access to more than 100k cloud based cameras and DVRs.
The vulnerable servers are part of the backend system of http://www.hik-online.com/ , a service that Hikvision offers to access your PVRs and Cameras via the web.
If you are accepting XML content, make sure you are not vulnerable to XXE. Also make sure you use HTTPS for EVERYTHING, I cannot think of any reason to not do so.
15 
1
15 
15 
1
"
https://faun.pub/multi-cloud-multi-region-kubernetes-federation-part-2-e8d403150d4f?source=search_post---------99,"There are currently no responses for this story.
Be the first to respond.
In the previous part-1, we had identified some concepts between Multi-Cluster vs Federation deployment and looked at a brief introduction of using Kubefed. We deploy kubefed in a single host cluster and federated 4 other clouds, 2 in vSphere, and one each in GKE and AKS. For testing, we had deployed a simple echo server application and also federated Kubernetes services in Ingresses in a common namespace called ‘kubefed-poc’. Each service was of type load balancer and each cluster/cloud allocated a unique IP for the service as requested.
Next, we need to connect these different services from each cluster together or more appropriately combine these services into a single homogenous service.
Our current setup looks like this -
Each of our echo LoadBalancer service has an IP allocated by its respective cloud. (For brevity, I have left out the Ingress ip’s and hostname but they also have the same use case of merging IP/host information from all the clusters).
Remember, these services are federated, which means that they all share the same configuration across all the clusters in which they are deployed and an application should provide a similar HTTP response regardless of which IP endpoint is reached.
So we need a way to combine all the IP s into a single endpoint potentially for the end-user to consume. Some straightforwards options include
We will first look at what kubefed can do for us as is before looking at potential gaps and mitigations to those gaps.
Kubefed offers some basic out of the box solutions(with some potential limitations too) to this problem with a 30+yr technology to provide for our needs — DNS.
Before we look at the kubefed solution, let’s do a quick detour to refresher about DNS load balancing.
We all know that DNS is in simplistic terms used to lookup an IP address for a given hostname. So if we do
We see that the IP address of google.com is 216.58.194.206. The other value of interest is the bolded “29”. This is the called the TTL of the DNS record for the resolvers to cache. So if we query this after 30 secs we will see
The IP address is different. This is not because the IP of google.com is changing; its because DNS allows multiple addresses ( A records ) for a single domain name or FQDN. A lot of DNS services send out a list of IP addresses in a round-robin order every time and clients typically use the first address to connect. This way the load is automatically sent to different servers in the backend, distributing the load. This is just one way of DNS load balancing also know as round-robin DNS load balancing. There are DNS servers with advanced routing policies which can do load balancing based on weight definitions e.g., you can say “I want my google cloud load balancer to handle most of the requests” in which case, that particular IP address will be returned more frequently to the client at the top of the list. Some DNS Services or DNS add ons( e.g Azure Traffic Manager) also offer geo-location-based routing policy so that one can configure different IPs to be served based on the location of the client requests. Each DNS service provides simple to complex routing rules based on the product.
For our use case, if we follow this approach, we could create a single public FQDN and add all IP addresses to the A records for that domain e.g.
But service and ingress LoadBalancer IPs are ephemeral and can potentially change so doing this manually can be tedious. And what if we have 30 such services across multiple tenants? Thankfully there is a very cool Kubernetes project which in conjunction with kubefed provides some automation to this.
External DNS defines itself as
ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers.
And it does just exactly that, not too complicated. It takes any Kubernetes Services defined as LoadBalancer or ExternalType or an Ingress hostname and publishes a hostname defined via annotations to an external DNS provider. It supports quite a lot of DNS providers and has a simple enough API to support a DNS provider of choice. A simple example of a service defined as
will register “echo.external-dns.demo” with the IP of the LoadBalancer service. Similarly for ingress.
the IP of the ingress controller will be added as echo.external-DNS.demo.
Check out the FAQ for external-DNS for more in-depth info and details about its capabilities.
The External-DNS design basically involves “Sources and Sinks”. The source is an abstraction over a list of endpoints be it from a Service or Ingress and sinks are provider implementations that support different DNS providers. An endpoint maps mostly a single DNS record i.e name, list of targets, type of DNS record, TTL, etc.
So, for federated services in kubefed where a service is replicated across all the clusters how do external-DNS determine the endpoints?
External-DNS also supports a source of type CRD. A CRD source is any CRD type which can return a list of endpoints from its spec e.g.(from the source)
This way, one can represent a list of endpoints in a generic way.
Kubefed offers a higher-level API (CrossClusterServiceDiscovery) which can query all the services/ingresses across all the clusters based on Service (or Ingress) definition and create a CRD with a spec similar to above. External-DNS will take care of the rest.
The flow as illustrated in the kubefed docs is as above. For every multi-cluster service to be registered with a DNS provider using external-DNS, a user must create a ServiceDNSRecord(IngressDNSRecord for ingresses). The kubefed controller will consume this CRD object and generate a DNSEndpoint object which external-DNS then reads and will create DNS records.
What's more, the kubefed controllers will also generate endpoints per zone and region so for each cluster our URL will look like
<servicename>.<namespace>.<federation-domain>.<zone>.<region>.public-domain-site
This way you have a DNS entry for each cluster’s LoadBalancer with a region/zone prefix. So how are region and zone values determined? This is gathered from two labels on all the nodes in all the clusters -
If you check one of the public cloud nodes e.g GKE, these labels are already present . (These have actually been deprecated in favor of topology.kubernetes.io/region and topology.kubernetes.io/zone but some projects and applications e.g. kubefed still use the old labels)
For our private clouds, depending on the infrastructure, this may or may not be present. If not, we need to do this manually.
Our final deployment architecture will now look like this
Now that we have been introduced to all the various ingredients in this experiment, let’s put it all together. We need to do the following
First, we label our private cloud nodes with the zone and region info since my setup doesn’t infer this during installation
For a public domain, we can just get one from a public registrar — I got one from Namecheap for testing.
Next, we need to pick off the supported DNS providers which work with external-DNS (or implement our own if we so choose). For the purpose of this experiment, we use Google Cloud DNS. Private networks might want to use CoreDNS etc.
Create a google cloud DNS zone for our domain
Then we tell our domain registrar to point to the name servers for our zone. For this, get info of our zone -
We need to point our domain registrar to use the highlighted name servers above. This might take some time to take effect. You can verify this by checking on one of the whois websites.
Next, create a service account on google cloud with DNS administrator permissions and download the keys as a JSON file. Refer to the docs for instructions. We will need this JSON file for external DNS to authenticate with google cloud DNS.
Now its time to deploy external-DNS. We do this on the host cluster. First, we create a secret with our google service account JSON file.
Next, we deploy external-DNS. There is documentation for deploying this for google cloud DNS. We have to modify the manifest file a bit to mount our service account credentials and also to add RBAC permissions for the “multiclusterdns.kubefed.io” API group. Our final manifest looks like this
Apply this manifest
Next, we create a Service Domain and a ServiceDNSRecord for our echo service
Once we create this, in a few seconds a DnsEndpoint resource will be created
As we can see it has generated various DNS record endpoints with associated IP addresses with each record having the zone and region info from the nodes and also has a single DNS record with all the 4 IPs
You can check if these are present in your DNS zone record set on Google Cloud DNS
Let’s lookup our DNS now and try
All our records are updated. Now let’s try the service.
If you try this a few times, you might notice the hostname changes in the output corresponding to the different cloud the request goes to. But this actually depends on the DNS provider. Google Cloud DNS e.g. does not support round-robin load balancing. So the order in which the IPs are returned for the DNS A record can be random.
We can even try an Ingress DNS Record
After a small bit of time allowing things to propagate -
you will notice the 4 IPs of our Ingress Controllers. Of course the site also works as expected
Common Pitfalls with DNS Load Balancing
While the kubefed integration with our services worked pretty seamlessly with external DNS there are some common pitfalls with this approach of using DNS LoadBalancing —
All the above issues are significant when considering application development with existing features provided by kubefed. But these building blocks can be used to complete our solution. For example, if the service names and regions and zone names are fixed, we know the format of our DNS name. Network administrators can then manually manage Load Balancers configuring endpoints using these names. A lot of these won't change over time so manual updates aren’t that bad a tradeoff. Of course, this depends on a case by case basis.
As it has been repeatedly observed, the kubefed API is low level and simple allowing a lot more high-level API's which can enhance its current integrations with external-DNS or other types of endpoints to achieve the needs. This is generally the theme I found when dealing with kubefed which is relatively a young project with a lot more room to grow.
We will look at other open items like stateful applications, developer and production adoption ideas, etc. for using tools like kubefed and ideas of what can be built on top or in parallel to these initiatives in the final part.
[ Edit: Part-3 is now posted here ]
References
Subscribe to FAUN topics and get your weekly curated email of the must-read tech stories, news, and tutorials 🗞️
Follow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
33 
33 claps
33 
Written by
https://www.linkedin.com/in/venkatnsrinivasan/
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
https://www.linkedin.com/in/venkatnsrinivasan/
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@zhimin-wen/nfs-dynamic-volume-provisioning-in-ibm-cloud-private-ca44bc514cdb?source=search_post---------100,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zhimin Wen
Jan 13, 2019·3 min read
Network File System (NFS), due to its simplicity and the well-established techniques, are commonly available in the Enterprise IT infrastructure.
Kubernetes support the NFS based Persistent Volume (PV). However, it doesn’t provide an in-tree provisioner that enable the dynamic volume provisioning through the storage class. This limits the use of NFS in Kubernetes as most of the applications running in kubernetes are expecting the Persistent Volume to be dynamically created through the Persistent Volume Claim (PVC).
Thanks to the flexibility and extensibility of Kubernetes, an external-provisioner, that is not prefixed with the name of “kubernetes.io” and not shipped alongside Kubernetes, is available for NFS. Check out the URL.
This paper document the steps to enable the NFS dynamic volume provisioning in IBM Cloud Private (ICP) 3.1.1, and some simple validation test.
Its assumed that an NFS server is already available where the IP/hostname and the exported path are known.
In the meantime, in all the nodes of the cluster, make sure the NFS client is installed. For an example, in Ubuntu run the following command
Be default ICP 3.1.1 enforce the image security allowing only those images that are defined in a white list to run in the cluster.
Create and apply the following object,
Run the helm command to install the helm chart with the following command
Supply the NFS server’s IP address/hostname, and the exported NFS path. We also enable the flag podSecurityPolicy.enabled which is required for ICP 3.1.1. (For production usage, you may want to define a name instead of a randomly generated release name)
Watch the pod is running, and the storage class is created.
Let's request a PVC. Create and apply the following K8s object,
Watch the PVC is bound,
Now let's create a test deployment using that PVC.
In the deployment, we define a volume using the PVC created in the previous step, then mount it into the container to the path of /data.
Wait for the pod is running, then
Exec into it, check out the /data file system
Create a file and validate the content,
Exit, and delete the current pod, a new pod should be created. Exec into it again, the file should be persisted with the last content.
This is the end.
Cloud explorer
14 
14 
14 
Cloud explorer
"
https://medium.com/@hanusiak-tomasz/using-iterm2-to-work-with-openshift-clusters-to-manage-cloud-pak-for-data-b2f3224fbca8?source=search_post---------101,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tomasz Hanusiak
Feb 6, 2020·3 min read
As a team working with hundreds of Cloud Pak for Data clusters running on OpenShift each month (both internal and external) we had to come up with an easy, yet reliable method to simplify and increase the speed of cluster management & troubleshooting.
While there are several tools that can help you do that, they usually require deploying applications/plugins onto the OpenShift cluster or modifying the underlying operating systems (for example changing the shell or adding extensions to the existing ones) — neither was really a good option to us.
We then found a great Terminal replacement for macOS — iTerm2, which allowed us to create a simple, yet powerful solution.
iTerm2 allowed us to do a couple of interesting things very easily:
Let me now share a couple of screenshots, and explain how you can configure iTerm2 a similar way.
Small subset of our Profiles, with corresponding tags — please note that the profiles are dynamic — each change to the configuration file will refresh the list.
Example of a Smart Selection Rule and corresponding action. Whenever the regex is matched, the actions will show up in the context menu.
Subset of actions available on selecting a line in iTerm.
Using the Smart Selectors, both Utils and Pod actions are available. No need to type the commands and copy and past the pod & namespace names.
You can find a simple template with some most common actions we use here: https://github.com/Hanneck/iterm.
Obviously there is a lot more you can do in iTerm — key bindings, adding custom actions to MacBook’s Touch Bar, and much, much more… Plus it’s open sourced, so you could potentially change/add any feature you’d like.
Please have a look at the list of all the features — https://iterm2.com/features.html & https://github.com/gnachman/iTerm2
9 
9 claps
9 
About
Write
Help
Legal
Get the Medium app
"
https://blog.powerupcloud.com/how-to-configure-replication-from-aws-mysql-aurora-to-external-replica-b458ca34ffe7?source=search_post---------102,"Written By: Bhuvaneshwaran Rathinasamy, DBA, Powerupcloud technologies.
In our previous post, we explained how to replicate your Mysql RDS to an external replica. Now we are going to explain how to replicate MySQL Aurora to an external replica which is running on EC2.
Amazon Aurora using single cluster storage for Master and all the replica servers and applying redo logs instead of binlog, hence gives us 5X faster performance. But here we are using binlogs for replication, so please never expect the same performance here.
Let’s start to configure replication from AWS MySQL Aurora to External replica.
Learn about Powerupcloud's tech stories in Cloud, Big Data…
56 
6
56 claps
56 
6
Written by
Powerupcloud Stackadmin
Learn about Powerupcloud's tech stories in Cloud, Big Data & Artificial Intelligence fields.
Written by
Powerupcloud Stackadmin
Learn about Powerupcloud's tech stories in Cloud, Big Data & Artificial Intelligence fields.
"
https://medium.com/@rajathithanrajasekar/google-cloud-public-gke-cluster-egress-traffic-via-static-ip-addresses-for-ip-whitelisting-1cb024228e7e?source=search_post---------103,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rajathithan Rajasekar
Jul 22, 2020·6 min read
In Public GKE cluster —Each node has an ephemeral external IP address and the nodes route all egress traffic through the internet gateway associated with their node cluster.The internet gateway connection, in turn, is defined by the compute Engine network associated with the node cluster. It is not practically possible to whitelist all of…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/shashforce-blog/salesforce-wave-analytics-external-data-api-618be4be63cd?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
One of the latest additions to the Salesforce Cloud is the Wave Analytics Cloud. Wave is a powerful new age analytics platform which provides rich visualizations for data coming from various sources like back office systems or even from salesforce, in various forms like semi-structured and unstructured data.
Salesforce Wave is the analytics for the rest of us
A business user can get great value out of the available datasets using various charts, groupings and aggregations, all through a point-and-click interface.
The datasets can be created by developers in multiple ways, either through the UI in the case of Salesforce Data, or through the powerful Wave Analytics API.
One way we are going to see is to create datasets from CSV (comma-separated) data using the special purpose “External Data” API resources. When you upload data from the UI, there is a limit on the size of data restricted to 512MB. Through the External Data API, the limit goes way beyond t0 40GB!!
The External Data API, available in API version 31 and later, enables you to upload external data files to Wave Analytics. We can upload .csv files, and you can optionally specify the structure of your data by defining metadata in JSON format.
The API exposes two sobjects to enable external data uploads:
These objects are only available if you have Wave Analytics enabled in your org. If you would like to try Wave Analytics in a developer edition organization, you can register for a new DE org with Wave enabled here.
The high-level steps for uploading external data by using the API are:
Use Workbench REST explorer (https://workbench.developerforce.com/login.php)
Using Workbench REST explorer, insert into InsightsExternalData object
Method: POSTResource: /services/data/v36.0/sobjects/InsightsExternalDataRequest body:
Note: The MetadataJson value should be a base64 string. Create a JSON file with the metadata, convert it to base64, and provide the string. If you simply want to try an online tool for json to base64 conversion, you can try this and get the base64 string and include in as a value of MetadataJson.
Using Workbench REST explorer, insert into InsightsExternalDataPart.
Method: POSTResource: /services/data/v36.0/sobjects/InsightsExternalDataPartRequest body:
Note 1: Each part should be less than 10MB in size. If your file is larger, split it into more than 1 part, 10MB each.
Note 2: The value for InsightsExternalDataId should be the Id of the previously created InsightsExternalData record, which is returned when the creation is successful in the previous step. The value for FileData should be the csv data file converted to base64.
Using Workbench REST explorer, update the “Action” field on InsightsExternalData
Method: PATCHResource: /services/data/v36.0/sobjects/InsightsExternalData/id_of_InsightsExternalData_recordRequest body:
Using Workbench REST explorer, get Status of InsightsExternalData record.
Method: GETResource: /services/data/v36.0/sobjects/InsightsExternalData/id_of_InsightsExternalData_record
Now that our data set is ready, we can slice and dice the data using the powerful visualizations provided by Wave!
This is just a teaser of what Wave is capable of with visualizations. To explore Wave, try out the Wave exploration trail on Trailhead, the free and fun gamified learning platform by Salesforce.
To find out more about the Wave External Data API, do check out the documentation: https://developer.salesforce.com/docs/atlas.en-us.bi_dev_guide_ext_data.meta/bi_dev_guide_ext_data/bi_ext_data_overview.htm
Salesforce developer blog by Shashank Srivatsavaya
8 
2
8 claps
8 
2
Written by
Director, APAC Developer Relations, Salesforce trailblazer.me/id/shashforce
Salesforce developer blog by Shashank Srivatsavaya
Written by
Director, APAC Developer Relations, Salesforce trailblazer.me/id/shashforce
Salesforce developer blog by Shashank Srivatsavaya
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rameez.s.shaikh/spring-cloud-hashicorp-vault-hello-world-example-f92dcee4cab1?source=search_post---------105,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rameez Shaikh
Jul 28, 2020·6 min read
Microservices architecture have multiple services which interact with each other and external resources like databases. They also need access to usernames and passwords to access these resources. Usually these credentials are stored in config properties. So each microservice will have its own copy of credentials. If any credentials change we will need to update the configurations in all microservices. We have previously discussed one solution to this problem is using Spring Cloud Config Native Server or Spring Cloud Config Git Server where common global properties which are repeated in all the microservices are usually stored. But still storing the secrets in configuration file is a security concern. Above approach as 2 drawbacks-
In this tutorial will be using Spring Cloud Config and Hashicorp Vault to manage secrets and protect sensitive data.
Hashicorp Vault is a platform to secure, store, and tightly control access to tokens, passwords, certificates, encryption keys for protecting sensitive data and other secrets in a dynamic infrastructure.Using vault we will be retrieving the credentials from the vault key/value store.
We will be implementing a simple Spring Boot Microservice which returns employee details from MySql Database. We will be creating the Spring Boot + MySQL Application using Spring Boot JDBC. We have already seen Spring Boot MYSQL JDBC basics in a previous tutorial. In We will be initially storing the MySql credentials in the configuration file.
Later we will be modifying this application to fetch the MySQL credentials from HashiCorp Vault.
The project will be as follows-
Add the spring-jdbc-starter dependency.
In the application.properties file specify the datasource properties
Create the schema-mysql.sql file and specify the initialization scripts-
Create the Employee Domain class
Create the DAO interface.
The DAO implementation class.
Create Service interface to specify employee operations to be performed.
The Service class implementation.
Create the Controller class to expose a GET API which fetches the employee records
Finally create the class with @SpringBootApplication annotation.
Start the application. And got to localhost:8080/employees
Go to Hashicorp download Page and download Hashicorp Vault.
This is a zip file. Unzip it contains vault.exe. Create a file name vaultconfig.hcl for configuring vault on startup.
Open a command prompt and run the following vault commands-
Vault is now started. Open another command prompt and run the following commands-
We can see here that the Vault is sealed. We need to unseal it.
Next enable a key value store with named secrets
Store the MySql username and password in the Hashicorp Vault
In the pom.xml add the Spring Cloud dependencies-
Create a file name bootstrap.yml and add the following configuration.
Finally in the application.properties change the MySql username and password properties to get values from Hashicorp Vault.
If we now start the Spring Boot Application, it will automatically fetch the MySql username and password by making an API call to Vault.
https://www.javainuse.com
23 
1
23 claps
23 
1
https://www.javainuse.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@safwene-benaich/developing-on-remote-vm-via-vscode-using-google-clouds-iap-6b6549f9270c?source=search_post---------106,"Sign in
There are currently no responses for this story.
Be the first to respond.
Safwene Ben Aich
Dec 2, 2020·4 min read
Using Google Cloud’s Identity-Aware Proxy allows you to connect to your VM without an external IP through HTTPS and TCP proxy instead of exposing your service to the internet or unprotected internal network. But how we can access VM and edit code remotely using external SSH tools like VSCode?
I assume that you have VSCode already installed, in order to enable the remote developing and editing using SSH, you need to install the remote-SSH extension, a great option if you want to bring your remote environment on your local machine.
I’v created a simple VM without an external IP, this will be our target machine on GCP:
The only solution to establish an SSH connection to our VM without an external IP is by using the --tunnel-through-iap with the gcloud command, click on “View glcoud cmmand” then press “Run in cloud shell”:
Now, let’s try to establish an SSH connection from your local machine, first of all you will need to install the Google Cloud SDK, verify your installation by entering:
Last thing before moving to the main subject, you have to authenticate:
Google SDK will ask you to select the account and allow access:
As we can see, you can access your GCP resources from your local machine, now we have the ingredients but not yet the recipe ;),
Let’s try to SSH to our VM, but with adding an additional option and the username:
This command will not prompt an SSH session, but will display the command running behind the “gcloud compute ssh”, so you need to have something similar to, assuming i’m using a Windows OS:
The idea behind is to create a new entry on your .ssh/config file based on the output, press Ctrl-Pthen selectsconfigure SSH Hosts… :
Save your config, press Ctrl-Pand select your host.
One last thing, there are some problems you may face while setting up the environment:
Now you have everything you need to do great work, no more excuses !
IT engineer with good taste
See all (8)
62 
1
62 claps
62 
1
IT engineer with good taste
About
Write
Help
Legal
Get the Medium app
"
https://faun.pub/setup-email-notification-using-external-smtp-server-from-ec2-basic-aws-networking-revisited-344246231858?source=search_post---------107,"There are currently no responses for this story.
Be the first to respond.
Last discussion with internal team that there is this usecase that we are in need to create email notification from the apps inside EC2 instance which are using external SMTP. As simple as this is, yet as I mostly work on development infra and apps, I am not familiar with setting-up SMTP especially from this AWS environment to external SMTP Server (commonly I do configure on-premise which is local setup and I could manage the entire stacks).
So with that in mind I try to create a simple lab which may reflect a simple production environment with following criteria:
Following is the concept of my Lab to actually deploy the environment
It is quite refreshing to start over the basic networking in AWS as I revisit all basic network settings.
Here is the Terraform that I prepared (its only a draft, it may not be 100% useable). I love this Infrastructure-as-Code approach, as I could bring up environment fast (well documented and I could do some copy-paste)
After the environment are prepared, the last thing that need to be done is to install the SWAKS and send the email. As the architecture, I need to actually SSH from my local laptop to the so-called-Bastion-Host and then I do another jump using SSH to the Instance that have SWAKS.
And after all installation is done I need to test to send email
Which later-on prompt the username/password interaction. One note that the Gmail may have a hard time to authenticate this too-simple-auth so I think for temporarily I need to turn on the Google allow less secure apps permission.
Then it is done, Love to explore more and if there are more proper way to explore this approach please do not hesitate to send some ideas or inputs.
Follow us on Twitter 🐦 and Facebook 👥 and join our Facebook Group 💬.
To join our community Slack 🗣️ and read our weekly Faun topics 🗞️, click here⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
57 
57 claps
57 
Written by
Cloud Customer Engineer — Infrastructure Modernization @GoogleCloud. Stories are my own opinion. https://linktr.ee/alevz
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Cloud Customer Engineer — Infrastructure Modernization @GoogleCloud. Stories are my own opinion. https://linktr.ee/alevz
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kirirom_instituteoftechnology/tricks-on-unlimited-cloud-storage-1d68e771eb8d?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kirirom Institute of Technology
Feb 8, 2019·2 min read
Internet service has been improved exponentially. The breakage of USB flash drives, the loss of external hard disks, insecurely store credential data on personal computers, which is highly vulnerable for hackers to get access to your private files, is now come to an end.
So far, none of the large tech companies offer unlimited cloud storage as services yet. But here is a hacking technique to use the cloud storage unlimitedly.
2. Go to Google drive site
3. Go to your Google Drive account
By default you will get only 15 GB of cloud storage capacity for free so how can we store Office document unlimitedly?
Google Drive provides a full set of collaborative office suite which is a set of working software tools for free like:
4. Proof of free: as you can see, the Google Document I have created shows capacity as “(-)”.
Don’t forget to share this if you find it helpful and follow us for more useful articles like this.
Transforming You To A Better Life
203 
203 claps
203 
Transforming You To A Better Life
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jinnerbichler/gdpr-compliance-with-spring-boot-applications-part-i-external-databases-495ac1926dab?source=search_post---------109,"Sign in
There are currently no responses for this story.
Be the first to respond.
Johannes Innerbichler
Jun 12, 2018·4 min read
The new European General Data Protection Regulation (GDPR) comes with novel challenges when dealing with personal data. GDPR Article 25 requires data protection by design by default. This means that all of the application’s privacy settings must be set to protect a user’s privacy when the application is provisioned to users. Technical measures need to be implemented in order to protect precious personal data of individuals and companies. Article 32 states several technical details on the requirements for processing data in GDPR compliant applications.
According to article 32, it is necessary to always have full control over personal data. Therefore, external third party database services (e.g. hosted database as a service solutions) must be treated with caution. Such services might automatically store backups or replicate the data on multiple servers, which makes it challenging to delete personal (and sensitive) data consistently upon request. Another risk comes with the fact that service providers might be able to physically access data stored in the database. Both issues can be addressed with encrypting the data in the actual application before it leaves the application and is being stored in an external database.
This is the first article in a series, which has the goal to show several technical solutions for some of the new challenges. During the series, I will mainly focus on Spring Boot based applications. This first article shows an example application, which utilises functionalities from the Java Persistance API (JPA) for encrypting data before it is sent to a database service. It seamlessly integrates into the Spring Boot technology and, therefore, comes with little additional overhead. The source code of this project is available on GitHub. Let’s get started and do things GDPR-style.
The project comes with a self-contained Docker setup. The following versions are used throughout the example:
Docker: 18.03.1-cDocker-compose: 1.21.1Spring Boot: 2.0.2.RELEASEMaven: 3.5Bouncy Castle: 1.59
Due to the fact that a self-contained setup is provided, the following call builds and executes the application connected to a PostgreSQL database:
This command also starts a simple database viewer, which can be used to explore the stored data. After the application was executed successfully, the content of the database can be displayed on http://localhost:8081/. It can be seen that the exemplary customer data (go to Tables > customer) is fully encrypted and can not be read.
In general, the application exploits the Attribute Converters introduced in JPA 2.1. It allows specifying methods to convert between the database and the Java representation of an attribute. An implementation of the interface has to implement the following methods:
In our case, the database column is the encrypted String representation of the attribute. The following code snippet shows the applied implementation of the interface:
Every attribute is encrypted before it is sent to the database and decrypted after it was fetched. The concrete String representation is specific to the class of the attribute and is obtained via the concrete implementation of:
Implemented converters (i.e. StringConverter, LocalDateConverter, LocalDateTimeConverter) and are attached to individual columns via the @Convert annotation, as shown in the entity definition below.
Encryption is done via the BouncyCastleAesCbcBytesEncryptor, which applies AES256 with Cipher Block Chaining to the respective string representation. PBKDF2 is used as the password-based key derivation function.
The application mainly stores two separate customers and queries them by mail afterwards (see code snippet below). It can be seen that the familiar workflow of Spring Boot repositories stays untouched, even though all data is being encrypted.
Applying encryption to attributes via converters has the nice effect that no unencrypted data leaves the application. This leads to a higher level of control when dealing with personal data. Additionally, this mechanism is independent of the underlying database technology (e.g. PostreSQL) and, therefore, provides more freedom for developers. Some database technologies have built-in encryption capabilities (e.g. PostresSQL). In such scenarios, the encryption key has to leave the application, which lowers the level of security. Stay tuned for part II of this series.
I studied Computer Engineering and became a freelancer in the area of cloud applications, cloud security and data analytics. www.johannesinnerbichler.com
19 
19 
19 
I studied Computer Engineering and became a freelancer in the area of cloud applications, cloud security and data analytics. www.johannesinnerbichler.com
"
https://medium.com/swlh/connecting-your-amazon-ecs-setup-to-an-external-vpc-356e0081afbe?source=search_post---------110,"There are currently no responses for this story.
Be the first to respond.
In my previous tutorials I have talked quite a bit about web app deployment and Elastic Container Service (ECS). Now let’s look at a special case where you might need to connect to your ECS setup from a different VPC than the one you set it up inside.
Note: The following tutorial can apply to both VPCs in the same AWS account or in separate AWS accounts.
"
https://medium.com/@mazzine/how-to-connect-your-spyder-ide-to-an-external-ipython-kernel-with-ssh-putty-tunnel-e1c679e44154?source=search_post---------111,"Sign in
There are currently no responses for this story.
Be the first to respond.
Raphael Mazzine
Mar 3, 2019·2 min read
This is a simple tutorial to connect your local Spyder IDE to a remote ipython kernel (for example, a server from Google Cloud).
First, you must install in both server and client sides the Spyder-Kernels package.
Now you must start your kernel on the server console:
You will generate on your current folder a JSON file named remotemachine.json, it has connection parameters from the server kernel. Notice we assigned the IP address 127.0.0.1 for the Kernel, the remotemachine.json has something like this:
We will need all those ports numbers (shell_port, iopub_port, stdin_port, control_port and hb_port) so take note of them.
Now, you must copy the JSON file to your local machine.
On the local machine, set up PuTTY to connect to your server.
If you don’t know how to configure it, here is a good video showing how to connect to Google Cloud using PuTYY: https://www.youtube.com/watch?v=PFR5os97Wsc&t=347s
After configuring the connection parameters, you now must configure the SSH tunnelling parameters (on Connection-SSH-Tunnels).
We will tunnel our local client ports to the server kernel port numbers (that you took note). Therefore, you must type the port numbers on Source Port and, in Destination we will redirect to the server kernel IP and port. Repeat the same procedure to all other ports.
Now press Open to start the tunneling.
On the client side, start the Spyder IDE, click on the Consoles and Connect to an existing kernel. Browse and select the remotemachine.json file and press OK.
It’s done! Now you are connected to an external kernel through a SSH PuTTY tunnel.
I would like to thank Haidar Almubarak as I used part of his tutorial
PhD researcher in Applied Machine Learning. Loves AI, Soft. Development and Dogs
See all (5)
63 
1
63 claps
63 
1
PhD researcher in Applied Machine Learning. Loves AI, Soft. Development and Dogs
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@johnstarich/home-cloud-ingress-tutorial-53accf79c29e?source=search_post---------112,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Starich
Mar 29, 2019·6 min read
Web services running on the same network can be difficult to set up correctly, especially if you want all of them to work on an external network.
In this tutorial, I will guide you through setting up a Docker Swarm to route traffic to the appropriate containers, automatically.
There are three parts to get your cloud ready for web services: a load balancer to spread connections to available servers, an ingress router to direct traffic to containers, and DNS to point local traffic to the right IP addresses. Here’s a quick layout:
"
https://faun.pub/how-to-kubernetes-with-dns-management-for-gitops-31239ea75d8d?source=search_post---------113,"There are currently no responses for this story.
Be the first to respond.
When it comes to GitOps efforts, amongst the many caveats and the varied snags to watch out for when configuring these, — is the DNS toil. I have been long procrastinating to get a running demo of this External-DNS https://github.com/kubernetes-incubator/external-dns for a little while, alas it is here now. And it’s so dang straight forward.
External-DNS undertakes all that management, — mapping FQDN to a service and an ingress. Albeit the Kubernetes Service DNS management will require a public…
"
https://medium.com/@brevityinmotion/external-ip-domain-reconnaissance-and-attack-surface-visualization-in-under-2-minutes-b2ab06105def?source=search_post---------114,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ryan Elkins
Jun 27, 2020·8 min read
As part 1 of this project #straylight accelerator series, I would like to introduce a repeatable process to visualize the external, global presence of a domain (and all corresponding subdomains). This article will detail the process to retrieve extensive raw DNS data, process it, and visualize it into consumable heat maps. The article will walk through the steps beginning from a selected target domain although it could be generated from any list of IP addresses.
Large companies could adopt this process for a continuous approach to monitoring their external attack surface. Often, it may cost thousands of dollars of external consulting and subscription services to obtain consumable, actionable information. This configuration can run fully in a cloud environment such as AWS and was built for under $5 (ensuring that the SageMaker instance is shutdown when not in use). Each domain query only costs pennies to run and process.
The Jupyter notebooks containing the full code can be accessed at https://github.com/brevityinmotion/straylight:
We will utilize the publicly available Forward DNS (https://opendata.rapid7.com/sonar.fdns_v2/) dataset provided by Rapid7 (https://www.rapid7.com/research/project-sonar/).
Let’s begin by highlighting the knowledge, data, examples, and learnings from the following sources that were incorporated into this walkthrough:
- A special thank you to Evan Perotti for the awesome walkthrough for querying the project Sonar FDNS and the query code for the Lambda APIs (http://securityriskadvisors.com/blog/creating-a-project-sonar-fdns-api-with-aws/).
- Thank you to Rapid7 for the availability of this valuable dataset (https://www.rapid7.com/research/project-sonar/) and the walkthrough for querying the dataset (https://blog.rapid7.com/2018/10/16/how-to-conduct-dns-reconnaissance-for-02-using-rapid7-open-data-and-aws/)
Example use case:
It is important to understand and manage the external attack surface of a company. This is often a difficult challenge for large organizations and a reconnaissance goal from a red team/bug bounty perspective. The value of reconnaissance methods increase by limiting the number of interactions with the true target. For this example, this information is generated completely passively by using freely available public datasets — i.e. let’s let someone else collect the information for us and then we can generate the insights that we need. We never touch or interact with the target domain.
For this demonstration, we are going to compare the corporate domains and datasets for the 5 largest cloud providers in the world (Amazon, Microsoft, Google, Alibaba, and IBM).
We are going to leverage cloud-based services to perform the analysis. Historically, it has been cost-prohibitive to purchase the amount of hardware, processing, and storage to search and process insights from terabytes and petabytes of data. By using cloud services, we can avoid the high capital costs and pay for only what we use.
The notebook code utilizes AWS Secrets Manager to prevent hard-coding them into the code. If you choose to follow the same route to avoid hard-coding, they should be stored with the following names to avoid source code adjustments. It is important to note that the default IAM roles for SageMaker include access to Secrets Manager secrets using the AmazonSageMaker-* format):
For the GoogleMaps API:
For the MaxMind API:
In order to utilize the Jupyter notebook, the Amazon Athena database and corresponding tables will need created by running the following queries within the Athena web console.
One caveat to displaying the embedded GoogleMaps inline with the notebook is to enable the widgetsnbextension and the gmaps extensions. Since SageMaker does not maintain persistence and the enabled extensions requires restarting Jupyter, it needs to be implemented via a Lifecycle configuration. This can be setup to run at startup of the notebook instance. The script to use can be copied from:
Once the Athena, SageMaker, and Secrets Manager environments are configured, the best option is to utilize and reference the Jupyter notebooks at https://github.com/brevityinmotion/straylight/tree/master/notebooks for the detailed and fully functional code. The remainder of this blog will highlight the key components of the process and the outcomes of the capability.
Using the Jupyter notebooks available in Github (https://github.com/brevityinmotion/straylight/tree/master/notebooks), we can begin retrieving and processing the data.
To begin, run the configuration.ipynb notebook (https://github.com/brevityinmotion/straylight/blob/master/notebooks/configuration.ipynb). It will:
Now, let’s begin using the tools-r7sonar.ipynb (https://github.com/brevityinmotion/straylight/blob/master/notebooks/tools-r7sonar.ipynb) notebook.
The code to prepare the query is:
The code to submit the query to the pre-defined Athena configuration is:
Once we receive the execution id information for the query, we can begin monitoring for the completion of the query:
The retrieval of the data takes several steps to make the process smoother. It first checks the Athena query and waits up to 5 minutes at 5 second loops until the query completes. Then, a pre-signed S3 url is generated to provide URL based authorization to the results. The return value of this function is the URL for downloading the query results in a .csv format.
Although it seems like a lot going on, this all typically processes in under one minute. Using *.microsoft.com as an example, within about 45 seconds we have the full set of results (18,695 DNS records) loaded into a Pandas dataframe for processing.
In the next step, let’s process these entries using the MaxMind GeoIP database. The configuration.ipynb notebook would have downloaded the GeoLite2-City database locally for faster processing. The next function will take any IP addresses within the specified dataframe column and add the latitude, longitude, country, and state attributes to each row.
The coolest part about this query is that it is written to feed any column of IP addresses into it to output the mappings. Even though we have taken extensive steps to get this point, you could reuse this function for geo-mapping any list of IP addresses.
The key point of this code snippet is that it doesn’t matter what the structure of this dataframe is as long as you specify the dataframe column holding the IP addresses. In this case, it is called ‘value’ but could be anything.
The following function is where the IP address to geolocation mapping magic happens.
Mapping the 18K+ IP addresses only takes around 1 minute to process. The function returns a dataframe looking like the following:
Now that we have all of the data and corresponding metadata into a dataframe, we can aggregate and build insights on the data. In preparation for mapping, the prepare_location function in the previous code sample can be called to aggregate the records into latitude and longitude groupings for plotting.
The resulting, normalized dataframe will look like:
With this data, we will pass the entire dataframe to a heatmap function:
The output produces the following interactive, embedded heatmap directly into the Jupyter Notebook. Although it is a little difficult to see all of the lighter green areas in this article, when run directly, the mapping can be zoomed for more granular analysis.
Microsoft.com heatmap — 18,695 records processed
Once this base configuration is setup, we can run additional queries against any domain that we want from start-to-finish in about 2 minutes. Here are the comparisons between the other top 5 cloud global cloud providers.
Amazon.com heatmap — 21,096 records processed
It is interesting that this distribution closely aligns with Microsoft’s although there is a large presence in South Africa for Amazon.com.
Google.com heatmap — 54,508 records processed
Google’s results returned a much heavier presence in California with limited to no coverage in the Seattle region.
Alibaba.com heatmap — 87,263 records processed
Alibaba, as expected, has a much greater presence in China.
IBM.com heatmap — 54,684 records processed
The mappings for IBM differ quite a bit from the others as it indicates a much higher concentration of presence in the eastern United States.
Although it is neat to have the heat maps, there is tremendous value in the raw data obtained through this process. An enterprise can utilize this for asset management, triage, visibility, attack surface reduction, and general awareness of its externally facing presence. It is also important to be aware that an adversary also has access to this information. This level of detail for tens of thousands of records can be collected, processed, and analyzed in under two minutes using this process.
As we progress through this accelerator series as part of project straylight, I am excited to share a few additional examples in the coming days and weeks using large datasets as well as much smaller use cases and tasks such as generation of metrics and integration of technologies.
Feel free to reach out with any questions on this. Enjoy!
Information Security/Cloud/Hacking/Technology
52 
52 claps
52 
Information Security/Cloud/Hacking/Technology
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@garg-ravish/ai-platform-training-using-external-database-platform-e581069645f8?source=search_post---------115,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ravish Garg
Sep 2, 2020·5 min read
AI Platform makes it easy for machine learning developers, data scientists, and data engineers to take their ML projects from ideation to production and deployment, quickly and cost-effectively. Organizations can execute their training jobs in serverless fashion by either leveraging built-in algorithms (beta) or can create their own training application to run on AI Platform Training.
This post demonstrates how to package your training application when it needs to connect to an external (On-Prem / Multi-Cloud) database to fetch the required source data-set.
How it helps your business
All organizations use one or more forms of database products to store their business data which can be required by their training jobs or need to access that business data in real-time due to the nature of their training jobs.
Google AI Platform Jobs gives an option to your organization to access this data-set directly from it’s AI Platform leveraging its custom container feature.
Enabling AI Platform Jobs connectivity to database platform
In this blog, we walk you through building a sample AI Platform Training Job which can connect to an Oracle Database hosted on your on-premise datacenter or on another cloud platform by highlighting the following key components:
Set a reserved range using gcloud beta compute addresses create.
Establish a peering connection between your VPC host project and Google’s service networking, using gcloud beta services vpc-peerings connect.
Here we leverage Google Cloud AI Platform custom container feature which allows you to run your application within a Docker image and thus, gives you flexibility to include all non-ML dependencies, libraries and binaries that are not otherwise supported on AI Platform Training.
Now, let’s consider you want to access an Oracle database for your data-set (one of the common relational database platform used by organizations) which is hosted in your datacenter and your training job is written in Python which can connect to Oracle database using cx_Oracle module which internally loads Oracle Client libraries to communicate over Oracle Net to an existing database and thus, require free Oracle Instant Client “Basic” or “Basic Light” package for your custom container.
Note: This post provides instructions for working with this app: Oracle Instant Client version 12.2.0.1. The instructions might not represent newer versions of the app. For more information, see the documentation: Oracle Instant Client.
3. Create your training job script which needs to fetch data-set from your on-premise Oracle database.
4. Prepare your dockerfile with Oracle-InstantClient and required Python packages.
5. Build the Docker image and push it to Google container registry.
6. Initiate AI Platform Job using your docker image and VPC network.
Google Cloud is based on following principles of an open cloud:
Thus, in-line with above principles Google AI Platform allows you to run your job considering your own ML framework or algorithms, libraries, dependencies which your business might have without limiting you to only baked in feature-sets.
Customer Engineer, Data Specialist @ Google Cloud. I assist customers transform & evolve their business via Google’s global network and software infrastructure.
3 
Some rights reserved

3 
3 
Customer Engineer, Data Specialist @ Google Cloud. I assist customers transform & evolve their business via Google’s global network and software infrastructure.
"
https://medium.com/@techexpertise/wso2-api-cloud-spring-boot-rest-api-for-active-directory-user-validation-b4718b43b933?source=search_post---------116,"Sign in
There are currently no responses for this story.
Be the first to respond.
Seralahthan
Oct 27, 2021·6 min read
WSO2 API Cloud provides a feature to configure a RESTful service to connect to the Active Directory and integrate with external users.
Refer to the link for further details.
cloud.docs.wso2.com
In this blog we are going to explore, how we can connect external users to WSO2 API Cloud using a Spring Boot RESTful API which integrates with an Active Directory user store and validates the user credentials.
The RESTful api should expose a POST resource accepting the following request payload with Basic authentication,
Depending on the provided user credentials, the RESTful api should respond a 200 OK with the following response payload,
A RESTful service connecting to Active Directory for user validation is implemented as a Spring Boot application, packaged and deployed as a docker image.
The source code for the Spring Boot REST API is available in Seralahthan/active-directory-user-validation-service
github.com
Requirement
In order to expose the RESTful API over HTTP with SSL and to communicate with the Active Directory user store over LDAP with SSL, I have created and configured a Java KeyStore (spring-security-ad.jks) in the project.
The Java KeyStore spring-security-ad.jks can be found in the src/main/resources/keystore/ directory of the project.
SSL Application Properties Configuration
This Java KeyStore is configured as both SSL KeyStore and TrustStore as shown below in the application.properties file found in the src/main/resources/ directory of the project.
SSL Passwords Encryption
As observed from above the keystore and truststore passwords in the properties file are encrypted using Java Simplified Encryption plugin (Jasypt) Maven plugin.
It is highly recommended to encrypt the passwords in the application.properties file and decrypt them at the run-time to hide passwords from configuration files.
Kindly refer to the section “Jasypt Encryption On Application Properties” for further details on encrypting properties and deploying artifacts with Jasypt encryption.
plain text passwords used for the SSL keystore/truststore are the following,
The passwords are randomly picked. If you are going forward with using the Java keystore shared in the project you can refer to the above credentials.
It is always recommended to create your own Java KeyStore to be used as the SSL KeyStore/TrustStore with your own secure passwords.
Instructions to create your own Java keystore can be found below.
Following command can be used to generate a new Java keystore to be used as the SSL keystore/truststore.
The parameters key_alias, keystore_name and keystore_password needs to be passed as per the requirement.
In order to connect with the Active Directory over Secure LDAP (LDAPS) we need to import the public certificate of the Active Directory to the SSL TrustStore
Since we are using a single Java KeyStore as both SSL KeyStore and TrustStore we can import the Active Directory public certificate to that Java KeyStore.
Extract the Active Directory Public Certificate using OpenSSL Command
Active Directory public certificate can be extracted by executing the following OpenSSL command against the Active Directory IP and Port,
The parameters active_directory_ip and active_directory_port need to be substituted with the actual values. The public certificate will be stored in the file adcert.pem
Import Active Directory Public Certificate to TrustStore
Import the Active Directory public certificate to the KeyStore using the following command,
In order to successfully connect and perform user search on the Active Directory user store the following Active Directory related parameters need to be configured in the application.properties file.
The ad_hostname, ad_port and ad_user_password need to be configured as per Active Directory configuration.
I have provided some sample configuration values for the ad.baseDN, ad.userSearchBase, ad.userDN, ad.userObjectClass and ad.userNameAttribute. These values need to be configured as per Active Directory configuration.
Note: In order to successfully authenticate external users with WSO2 API Cloud, WSO2 API Cloud expects the username to be passed as an email address.
So the ad.userNameAttribute needs to be configured as mailAlso, the Active Directory user store should support authenticating users by passing the users email address as the username.
The ad.userPassword is specified in plain text format with DEC(…) tag; it needs to be encrypted with Jasypt encryption.
Kindly refer to the section “Jasypt Encryption On Application Properties” for further details on encrypting properties and deploying artifacts with Jasypt encryption.
As a best practice, in order to hide the plain text passwords in the application.properties file we need to encrypt it with Java Simplified Encryption (Jasypt) installing it as a Maven plugin.
Jasypt (Java Simplified Encryption), provides encryption support for property sources in Spring Boot Applications. It will help you to add basic encryption features to your projects with very fewer efforts and without writing any code with the help of a few additions in your project here and there
Please refer to the link for further details on Jasypt encryption.
Configuring Jasypt Maven Plugin
There is an out of the box maven plugin available which can be used to integrate Jasypt encryption to the Spring Boot project.
In order to enable Jasypt encryption add the following maven plugin and dependency to the Spring Boot project’s pom.xml file. It is already added in the shared project.
Encrypting/Decrypting Passwords in Application Properties using Jasypt
Encryption
In order to encrypt plain text passwords in application.properties file, the plain text passwords need to be initially configured with DEC(…) tag as shown below.
Once all the passwords in the properties file are wrapped with DEC tag execute the following command to encrypt all the plain text passwords in the application.properties file.
In order for the Jasypt encryption and decryption to work properly, it is required to maintain a unique master key for Jasypt Encryptor. Any password can be used as jasypt.encryptor.password (master key).
The encrypted passwords will be displayed with an encrypted value inside the ENC(…) tag as shown below.
Decryption
In order to decrypt the passwords in the application.properties file and revert back to the plain text format, execute the following command,
Now that the passwords in the properties file is encrypted, it must be decrypted at the compile time (building the docker image) and runtime (running the docker image)
By default, the jasypt encryption command uses the PBEWITHHMACSHA512ANDAES_256 encryption algorithm for password encryption. So, the same algorithm needs to be configured to be used when decrypting the passwords at runtime.
Following property is added to the application.properties file to specify the Jasypt encryption algorithm.
Following command can be used to build the docker image passing the jasypt.encryptor.password (master key)
Following command can be used to run the docker image with the jasypt encryptor password passed as an environment variable.
Once the Spring Boot user validation REST api is hosted and exposed to be accessible publicly over the internet via Secure HTTP (HTTPS), it can be integrated with WSO2 API Cloud following the documentation.
The project shared exposes the REST api url as POST resource with localhost via, https://localhost:8080/validate
The sample curl request to invoke the REST api is the following,
[1] https://cloud.docs.wso2.com/en/latest/learn/cloud-administration/authenticate-external-users-for-api-invocations/[2] https://github.com/Seralahthan/active-directory-user-validation-service[3] http://www.jasypt.org/[4] https://mvnrepository.com/artifact/com.github.ulisesbocchio/jasypt-spring-boot-starter/3.0.3
Hope you enjoyed the blog and got something to take away.
Thank you for Reading!Cheers!!!
Senior Software Engineer @WSO2, B.Sc.(Hons).Computer Engineering
88 
88 
88 
Senior Software Engineer @WSO2, B.Sc.(Hons).Computer Engineering
"
https://medium.com/@rspraneethkumar/data-ingestion-and-bigquery-how-to-load-data-from-external-sources-on-to-bigquery-c3a39c03d8c6?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
SP Kumar Rachumallu
Feb 28, 2021·4 min read
Speed and Scalability are two biggest advantages of Google BigQuery, complimenting its serverless architecture. BigQuery is comparatively a cost effective data warehouse that allows querying data that can scale up to petabytes. Being serverless allows customers to concentrate on insights than to manage infrastructure. BigQuery does not just fit to one paradigm of data lifecycle. You can use BigQuery to ingest, process, store and even perform core analytics on data.
If you have an external data source, you can either load data into BigQuery or you can query the external source without loading it at all (Federated Query). The key difference is performance. Cloud native tables has high performance and low latency than the external tables.
There are several ways of loading data into BigQuery depending on various factors. You can choose to load data in batches or stream. Or you may even want to check the option to integrate your existing partners of google (One example is Informaticas intelligent cloud service offering).
Batch Load or Batch Ingestion involves loading large datasets at regular frequencies according to criteria set by developers. BigQuery can ingest both compressed and uncompressed files from cloud storage which is often deemed as a preference to land your external data before loading it to BigQuery. Loading data into BigQuery does not incur any charges, while storage will. Unless your business needs near real time data, you can use loading rather than streaming.
Although BigQuery supports loading data from multiple file formats, speed varies depending on the file type. Choosing the file format is important, as it affects not just the load speed but cost of storage too. AVRO(Compresses/Uncompressed) followed by Parquet/ORC are cheaper when compared to CSV & JSON. If you have a choice, you should choose to export your data in a format that is faster, efficient and cheap.
You can either load data from your computer or cloud storage (GCS) into BigQuery table. Imagine you already created a dataset and an empty table within BigQuery. To load external data you should navigate to BigQuery from Navigation menu.
Click “Create Table” and select the source of data. If you choose “Cloud Storage”, you should browse to locate the storage bucket where the load file is saved. And if you choose “Upload” you should browse and select the file or enter the path where it is saved.
You can either enter schema manually, or detect schema automatically. Alternatively you can check the radio button to auto detect schema.
Under the advanced options, you can choose either to overwrite the existing table, append it or to write if the existing table is empty. Once you click create table, the data loads from the file reference you use and creates a table within BigQuery.
Googles bq command line interface (CLI) is a python based tool. To use CLI within you GCP console, you must install and initialize the Cloud SDK. Once you are through this step, you can activate cloud shell from the home screen, which opens a cloud shell terminal. You can load, update, query, list and do many more operations on data from terminal using “BQ” command.
For example, the script below loads a JSON file named “Netflix_Titles” saved under cloud storage to a table “Netflix_Titles” under sample dataset within BigQuery.
Auto detect detects the schema automatically when reading the JSON file from cloud storage bucket. If you want to specify a schema, you can use a schema file or type schema inline. You can use options such as replace and noreplace to append, overwrite or write data to an empty table. (Look for BQ documentation).
You can load data into BigQuery from runtimes such as Java and Python. To use the BigQuery API, you must authenticate your identity using service account. You can either choose to authenticate with application default credentials or with a service account key file.
In the example below, a local CSV file is being read to pandas dataframe and then loads same data to a bigquery table using load_table_from_dataframe() function. As a prerequisite, you should install pandas, pyarrow along with BigQuery python client library.
Apart from the above, you can also load data from pipelines created using Dataflow or Dataproc into BigQuery.
cloud.google.com
cloud.google.com
Lead Programmer at Novartis Healthcare Pvt Ltd.
5 
5 
5 
Lead Programmer at Novartis Healthcare Pvt Ltd.
"
https://medium.com/hashmapinc/video-artisanal-data-pipelines-best-practices-for-snowflake-external-staging-in-aws-and-azure-96545de1fb00?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
Ingestion is the lifeblood of your Snowflake Cloud Data Warehouse. Make sure that you’re spending less and doing more with these practical tips for using external stages with Snowflake. During this talk I covered file considerations, continuous loading, egress (!), and workflow design.
Also, this was the first video produced from Hashmap’s new office in OKC.
Get the slides from this video here!
This video was recorded during the 4th OKC Snowflake Meetup on September 13, 2019.
Innovative technologists and domain experts helping…
2 
2 claps
2 
Written by

Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by

Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ajitoct24/dataflow-design-pattern-updating-in-memory-configuration-with-external-trigger-on-the-fly-in-8e9be85e508b?source=search_post---------119,"Sign in
There are currently no responses for this story.
Be the first to respond.
ajit deshmukh
Feb 26, 2019·3 min read
Why? We had a use case where we had some metadata which is stored in Bigquery, which will get updated eventually. But when it gets updated, dataflow should have the latest metadata in memory. How to do this?
So flow is Initialize pipeline->read metadata from Bigquery-> Process data using metadata as side input in ParDo On the change in Bigquery table, again read from Bigquery and apply it to the next incoming data
We do not want to restart the pipeline
Solutions:1. Using PCollectionView as sideInput to ParDoOne solution can be have a pubsub input(whenever a change in system just trigger it) -> On Receival of this message hit Bigquery to fetch metadata->Have this metadata as View -> Use this view as side input to ParDo
Sounds good. But there is a problem. How PCollectionView Works with different types of trigger1. fireWithDiscardingPane -> In this case when we fire with discarding pane, once row gets fired its available for next record in PCollectionView, can be used for ParDo as side input. But for next record, it gets discarded and hence we have an empty PCollectionView for rest of records.2. fireWithAcculateingPane -> In this case you will have value in PCollectionView every time, but your value will not get updated when you have new row.
So, thats why it will not work.
2. Using Stateful ParDoSo here is an interesting concept called Stateful ParDo is to help.
What is Stateful ParDo?
Dataflow pipelines are meant to be used for record by record processing. So there won’t be any communication between two records. What if we want to store a message from one record and it needs to be used by another subsequent record, we will need a storage to save object. Thats is exactly what you call stateful ParDo. It can be used in use cases like assigning row number to each record. So assign a number to a record, save it in store, for next record increamenet it, assign to the new record and again store it for next record.
So here is stateful ParDo we are going to use.
We have the main flow, in blue and metadata flow in green. 1. On Initial we don't have a pubsub trigger to update metadata, so instead, we will use create a row, to generate a trigger.
PCollection<String> metadataInitiate = p.apply(“Initialize UpdateMetadata”, Create.of(Arrays.asList(“UpdateMetadata”)).withCoder(StringUtf8Coder.of()));
2. Now if we have a UpdateMetadata command from pubsub, we will read it and flatten flow with initial command.
PCollection<String> metadataCommandPubsub = p.apply(“UpdateMetadata Pubsub Read”, PubsubIO.readStrings() .fromSubscription(StaticValueProvider.of(options.getUpdateMetadataSubscription())));PCollection<String> updateCommand = PCollectionList.of(metadataInitiate).and(metadataCommandPubsub) .apply(“Flatten Update Commands”, Flatten.<String>pCollections());
3. Now we will flatten it with main flow, and will pass it to Stateful pardo,stateful pardo looks like,
@ProcessElement public void processElement(ProcessContext c, @StateId(“mymetadataview”) ValueState<Map<String, String>> mymetadataviewstate) { if (“UpdateMetadata”.equals(inputMessage)) { updateMetadata(mymetadataviewstate); }else{ metadata=retrivemetadata(mymetadataviewstate); }
Here in ParDo if we receive a UpdateMetadata command as input, we will fetch metadata from Bigquery and save it in ViewState. Else for each record will get value from ViewState and use it for processing.
Care To Be taken :Note that how stateful ParDo works. Stateful pardo works on Key of the record. Just like CombineFn, for one key it will have one node, so choose your key wisely. The issue we faced was, we didn’t have any key, so we assigned dummy key. So we had a single key for each record, so we had single node processing for this ParDo. So when we got a high rate from Source, this pardo becomes a bottleneck.
How to tackle bottleneck?So to tackle this instead of single dummy key have multiple keys, the same number between 1 to 20. So when there is update command generate 20 keys in loop so metadata cache in each key node will get updated. And when a normal record is in, just assign random key between 1 to 20.
Conclusion: Stateful ParDo is an interesting feature which enables many use cases like numbering each row. We are also able to solve on the fly update of in-memory configuration. The pubsub trigger can be easily added in the CI/CD pipeline if changes in the table are driven by it.
Any views and questions of the same are welcomed.
3 
3 
3 
"
https://medium.com/@fede.agu/taking-values-from-a-external-terraform-state-bfbb08ce4dfd?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fede Agu
Sep 20, 2019·1 min read
If you are using Terraform for several parts of the infrastructure, maybe you have several different states and may be the posibility of need put some values from one state to another state.
How do you this? terraform_remote_state is the solution
Assuming the state is currently stored in a bucket in AWS or GCP you only must to set up the following data block:
hereinafter you be able to access to the outputs of the vpc tfstate by the expression: data.terraform_remote_state.<data-name>.outputs.<name-of-the-output>
References: https://www.terraform.io/docs/backends/types/gcs.htmlhttps://www.terraform.io/docs/backends/types/s3.htmlhttps://www.terraform.io/docs/providers/terraform/d/remote_state.html
51 
51 
51 
"
https://medium.com/@tollie/downloading-your-unlimited-amazon-drive-to-an-external-disk-mac-solution-3cf759fe9e26?source=search_post---------121,"Sign in
There are currently no responses for this story.
Be the first to respond.
JT Williams
Jul 19, 2017·2 min read
After Amazon shuttered the “unlimited” option for Amazon Drive, it no longer became the best value to me. However, moving data back to local storage can be a challenge when your computer runs on an SSD with less free space than you have stored in the cloud.
For Windows, presumably, changing the Amazon Drive sync folder in the registry should work to defeat the app’s rejection of an external drive. However, this post (which is mostly just a note to my future self, for reference) is for Mac users.
As users familiar with the terminal might expect, the process is fairly simple and involves use of the link command.
First: Quit the Amazon Drive app if it’s running.
Second, mount the external disk you wish to use for backup. Since the app doesn’t gracefully handle external drives, I used a USB powered one, so if I need to move the laptop, I can carefully do so without being tethered to an AC outlet.
Third, open the Terminal from Applications > Utilities, and issue these commands:
cd ~ You were probably there already, but if not, go to the folder containing your Amazon Drive
mv Amazon\ Drive /Volumes/EXTERNAL/ Replacing EXTERNAL with the volume name of your external drive. Tip: just type the first letter then press [tab]. You may have to wait for this to finish, depending on the size and number of files.
ln -s /Volumes/EXTERNAL/Amazon\ Drive/ Amazon\ Drive This is the tricky one. This is what creates the symbolic link to the folder on the external drive, and tricks the Amazon App into thinking that it’s still talking to the local disk.
Finally, type exit and close the Terminal window. To check that things are as they should be you can open Finder to the Home Folder (cmd+shift+H) and confirm that an Amazon Drive “folder” appears there, but opening it takes you to the external drive.
If everything looks go, re-run the Amazon Drive app and brace yourself for hours of downloading as it syncs to your external drive.
PS. Your mileage may vary. You do this at your own risk. Etc. Etc.
Thanks for reading. I wrote it mostly as a reminder to myself just for reference, but I do hope it proves helpful to anyone that stumbles upon this post looking a solution.
Post tenebras lux. Time Person of the Year 2006.
33 
1
33 
33 
1
Post tenebras lux. Time Person of the Year 2006.
"
https://medium.com/@venkat_95554/how-to-automate-aws-external-network-scans-using-amazon-inspector-ae690bd9590b?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
VirClop
Nov 13, 2018·4 min read
The following information is very vital for security practitioners :
That information has hard to get on AWS, you have to either analyze flow logs to see the
or spend a few thousand dollars on a vendor to do external scans and still do 1 and 2 above.
Until now.
No more analysis of flow logs, no more spending $$$ on external scans (scores of companies selling products in this space, including Rapid7, Tenable, AlertLogic). Last week, Amazon Inspector has announced a cool feature that does agent-less network assessments on EC2 instances. The feature does all three in one shot, tells you which instances are externally reachable over Internet, which ports are open to Internet and which security groups are responsible , so you can take remediation action.
Let’s see whether we can automate the feature so it can generate reports for us on external reachable instances.
It only takes a few lines of code to set it up.
Once the scan is done, the report can be downloaded as a PDF or HTML, I do not like either . For automation, I just want to get a JSON converted to a CSV. So use describe_findings to get the json , convert into a DataFrame and then CSV.
As you can see, the report has all the items we need. The two columns of significance are : title (which has the instance ID and ports) and recommendation (which has the security group)
Is all the above too hard to set up, even good news : We automated the above in our Serverless Slack bot, VirClop . We automatically run the network reachability scan every week and store the report automatically for you in your own S3 bucket. if we find new instance that got added to the report, we will automatically inform you in Slack.
Finally, the big elephant in the room . Can this scan report can be used for PCI external scan requirement (PCI DSS 11.2.1) ? Not yet, but very shortly, stay tuned on that :)
2 
2 
2 
"
https://medium.com/@subbu_lak/how-to-configure-hive-metastore-to-an-external-compose-for-mysql-database-in-ibm-analytics-engine-97ac0f3aa829?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
Subbulakshmi Prabhu
Feb 13, 2019·4 min read
IBM Analytics Engine(IAE) should be used as a compute-only engine. Best practices documentation for using IBM Analytics Engine recommends to configure the cluster to work with data in IBM Cloud Object Storage with Hive table metadata stored in a Compose for MySQL service, which resides outside of the cluster to keep the cluster as a stateless cluster. Here are the step-by-step instructions on how to externalize hive metastore to IBM Compose for MySQL for a newly created IAE cluster.
2. Once provisioned, Open the service and create a new credential:
3. Note down the URI from the credential created:
Login to your IBM Analytics Engine cluster Ambari: (Example: https://chs-xxx-xxx-mn001.us-south.ae.appdomain.cloud:9443/#/login`)
3. You will get the following message in Ambari when you try to change the parameters. Please IGNORE the message as the mysql connector is already configured in the Analytics Engine cluster:
4. Restart all the affected services in Ambari.
How to validate the hive metastore configuration?
Example:
Session1:
List the metadata tables( about 58 of them). DBS and TBLS are the metadata tables for hive databases and hive tables.
You can see the DB metadata pointing to your hiveDB:
Session: 2
SSH into your IBM Analytic Engine cluster and Create a simple Hive table:
GO BACK TO Session 1: Check the table in the MySQL hiveMetastore by selecting from TBLS table and you can see the metadata for the newly created hive table there:
Refer: https://console.bluemix.net/docs/services/AnalyticsEngine/external-hive-metastore.html
https://console.bluemix.net/docs/services/AnalyticsEngine/best-practices.html#best-practices
1 
1 clap
1 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/analytics-vidhya/provisioning-ec2-instances-with-external-ebs-volume-attached-using-aws-cli-ac85a9ce67ed?source=search_post---------124,"There are currently no responses for this story.
Be the first to respond.
In the Dynamic & Rapid World Today, Automation has created its own image, & to survive in this world, it is very important to know the automation part.
The very first step towards automation is using the CLI of most of the programs because it gives one a real scripting feel. One more benefit of using CLI instead of WebUI is it consumes less internet bandwidth & resources. In addition to that, it saves a lot of time, if commands are known to you.
Therefore, to learn CLI in this world is a must & this blog is written to teach you some of the basics of the AWS CLI Package, which is the official CLI for AWS.
At the end of this blog, you will be able to create a key-pair for AWS EC2 instances, create a security group that will be attached to EC2 Instances, Launch an EC2 Instance, create one extra EBS Volume, & attach that to the EC2 Instance created.
Let’s start building the setup using AWS CLI:
First of all, to create a key-pair, execute the command given below.
The above-shown command will create one KeyPair with the name mentioned in the command, & save the Key into the file that is mentioned in the command. This KeyPair will be used in the future to access the EC2 Instance.
Now, let’s create one security group which will be attached to the EC2 Instance which will be created in the next step.
To create a security group, execute the command given below.
The above-shown command will create one Security Group with the name & description mentioned in the command.
After the creation of KeyPair & Security Group, let us create EC2 Instance now.
To create the same, execute the command shown below.
The above-shown command will create one AWS EC2 Instance with the specified resources.
Now, let us create one extra EBS Volume for persistent Storage.
To accomplish the same, execute the command given below.
The above-shown command will create one EBS Volume of size 1 GiB in availability zone “ap-south-1a”.
As a final step for this blog, let us attach this volume to the EC2 Instance which we have already created above.
To accomplish the same, execute the command given below.
The above-shown command will attach the created EBS Volume to the AWS Instance.
After running the command, the output will be like this:
The above-shown command will attach the EBS Volume created & will attach that to the EC2 Instance!
I hope my article explains each and everything related to the topic with all the deep concepts and explanations. Thank you so much for investing your time in reading my blog & boosting your knowledge. If you like my work, then I request you to give an applaud to this blog!
Analytics Vidhya is a community of Analytics and Data…
14 
14 claps
14 
Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.com
Written by
Big Data Enthusiast, have a demonstrated history of delivering large and complex projects. Interested in working in the field of AI and Data Science.
Analytics Vidhya is a community of Analytics and Data Science professionals. We are building the next-gen data science ecosystem https://www.analyticsvidhya.com
"
https://medium.com/@nipunsampath/an-intro-to-external-data-representation-and-marshalling-8072e888df0a?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nipun Sampath
Jun 21, 2020·4 min read
Usually, a program represents its data and information as data structures when dealing with them at runtime. For example, a library management system might represent the data about a certain book using a “Book” object which in turn may consist of primitive data items like “title, ISBN” and complex objects like “author”.
However, this library management system cannot represent the information about the book using the said “Book” data structure when sending data to another library management system. For that, the “Book” data structure needs to be flattened (converted to a sequence of bytes) before transmission and rebuilt at the destination. This applies to almost every data structure as they are not compatible to be transmitted through mediums like networks.
This is where concepts like marshalling and external data representation come into play.
XDR is a standard data serialization format that can be used to transmit data among different computer architectures. Conversion of data from local representation to XDR is called encoding and conversion from XDR to a local representation is called decoding. XDR implementations are portable between different operating systems and independent of the transport layer.
XDR uses a base of four bytes and serialized in big-endian order. Smaller data types will also occupy four bytes after encoding regardless of their size. Variable-length data types such as strings and opaque are padded to a total divisible by four bytes. Floating point numbers are represented using the IEEE 754 format.
Marshalling is the process of transforming a collection of data items into a form that is suitable for transmission in a message. At the destination, the message is unmarshalled to produce the relevant data items. In other words, marshalling converts structured data items and primitive values into an external data representation, and unmarshalling generates primitive values from their external data representation and rebuild the data structures.
So now that we know about XDR and marshalling, let’s talk about the three different approaches for it.
Common Object Request Broker Architecture or CORBA for short is a standard defined by Object Management Group (OMG) to facilitate the communication among diverse platforms. Common Data Representation aka CDR is used in CORBA distributed objects during remote invocations to represent structured or primitive data types that are passed as arguments or results.
CDR can represent 15 primitive data types including short, long, float, double, char, etc. Each argument or result in a remote invocation is represented by a sequence of bytes in the invocation or result message.
Java provides automatic serialization to the primitives data types and objects that are marked as serializable by implementing either java.io.Serializable or java.io.Externalizable interfaces. These serializable objects then can be used as arguments and results in Java Remote Method Invocation(RMI).
What java object serialization essentially does is flattening an object or a set of connected objects into a stream of bits that can be transmitted over a network or stored in a disk for later use. Deserialization is about reconstructing the object or the set of objects using the serialized stream of bits. When an object is serialized, all of the objects that are reachable from that object are serialized as well unless specified as transient.
In addition to the states of the objects, some information about the classes of the serialized objects is also included in the serialized form. This information is helpful for the recipient to load the appropriate class upon deserialization of the object. The class information contains a version number for the class to keep track of the correct version of the class. This version number changes whenever a class is modified. So, it can be used to verify whether the recipient has the proper version of the class.
XML is a markup language that defines a set of rules for encoding data in a format that is both human and machine-readable. It was introduced by the World Wide Web Consortium(W3C) for general use on the web. XML uses a textual encoding to represent both its data and the structure.
XML follows a hierarchical structure which makes it easier for humans to understand the documents intuitively and for the machines to parse the data easily using data structures like trees. XML data items are tagged with ‘markup’ strings. The tags are used to describe the logical structure of the data and to associate attribute-value pairs with logical structures.
However, unlike other markup languages, XML is not limited to a fixed set of tags. It is extensible in the sense that the users can define and use their own tags for data representation. But if the XML document is intended to be used by more than one application, then the tag names must be agreed between them. This is done using the XML schema and namespaces.
Following snippet is a example SOAP message which is represented using XML. (source: https://www.w3schools.com/xml/xml_soap.asp)
That’s it for this article. See you in the next one! 😊
50 
50 
50 
"
https://medium.com/@komal1491/airflow-external-sensor-a7e999ecadfd?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
Komal Parekh
3 days ago·4 min read
In a data warehouse project , we implement dimensions and facts. There is standard way to implement a data warehouse. We bring data from various sources to dimension stage/fact stage and then move the data respectively to dimensions/facts. Also, in the process of ETL/ELT design we create lookup tables which are later used in the data pipelines. We also create Aggregate and Snapshot tables from DW Fact table.
Now, all these tables have defined process flow set. For example to populate a snapshot table we have below dependencies
In Order to populate fact snapshot, we need fact to be populated first and for fact to be populated we need fact stage and lookup table to be populated
Similarly to populate dimension, we would need dimension stage should be first populated.
This makes dependencies between the load order of tables as an important aspect while setting up the data warehouse.
Airflow provides feature called external sensor which checks on the state of the task instance which is in a different DAG and if the state is success then the dag with the external sensors simply goes ahead and executes the task(s) which come next.
This way we can create multiple DAGs to load different tables and then apply dependencies between dag.
This makes the code more modular and easy for maintenance.
External Task Sensor takes below as parameters -
I have created 2 DAG — First.py and Second.py
So Second.py DAG executes only after First.py is completed. So in Second.py I have defined a external task sensor which points to first.py
Here if you see in ExternalTaskSensor, I have pointed it to first.py dag. By default allowed state is ‘Success’. So when second.py runs, it first check for the execution of first.py and if first.py was executed successfully then it runs task defined in second.py.
This way using external sensor, we can create dependencies between dags.
Happy Coding!
Senior Cloud Data Engineer | Business Intelligence | Datawarehousing
4 
4 claps
4 
Senior Cloud Data Engineer | Business Intelligence | Datawarehousing
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/perfsys-blog/cloud-virtualization-to-protect-privacy-data-ae8140451fb1?source=search_post---------127,"There are currently no responses for this story.
Be the first to respond.
In spite of most companies outsourcing payroll and many companies using external email services to hold delicate information, security is one of the most often-cited complaints to cloud computing. Cloud users face security threats both from outside and inside the cloud. Many of the security issues involved in protecting clouds from outside threats are similar to those already facing large data centers.
In the cloud, however, this responsibility is distributed among potentially many parties, including the cloud user, the cloud vendor, and any third-party vendors that users rely on for security-sensitive software or configurations. The cloud user is responsible for application-level security. The cloud provider is responsible for physical security, and likely for enforcing external firewall policies.
Security for intermediate layers of the software stack is shared between the user and the operator; the lower the level of abstraction exposed to the user, the more responsibility goes with it. While cloud computing may make external-facing security easier, it does pose the new problem of internal-facing security. Cloud providers must guard against theft or denial-of-service attacks by users. Users need to be protected from in another.
The primary security mechanism in today’s clouds is virtualization. It is a powerful defence and protects against most attempts by users to attack one another or the underlying cloud infrastructure. However, not all resources are virtualized and not all virtualization environments are bug-free. Virtualization software has been known to contain bugs that allow virtualized code to “break loose” to some extent. Incorrect network virtualization may allow user code access to sensitive portions of the provider’s infrastructure, or to the resources of other users.
These challenges, though, are related to those involved in managing large, non-cloud data centers, where dissimilar applications need to be secured from one another. Any huge internet service will need to ensure that a particular security hole doesn’t compromise everything else.
Voice of Perfect Systems
1 
1 clap
1 
Voice of Perfect Systems
Written by
РМ at Perfsys
Voice of Perfect Systems
"
https://medium.com/@Knowlarity/how-cloud-based-applications-build-a-customer-centric-organization-fa6ffd73973?source=search_post---------128,"Sign in
There are currently no responses for this story.
Be the first to respond.
Magic of Telephony
Jan 13, 2016·3 min read
Businesses are struggling with the change to be ‘customer-centric’ and, in the process, they are subjected to external pressure.
Speaking of customer-centricity, can businesses (especially small and medium-sized ones) transform customer support from a necessary function to a value-generating service by adopting technologies like the cloud? The answer is: Yes. As a matter of fact, it’s not always easy for businesses to create a powerful first impression with customers whose experiences are defined by end-to-end journeys comprising of frequently delivered, multiple interactions with contact centers. The initial power shift between brands and customers happened during the economic downturn, allowing the latter to be more selective about the brands they wish to be associated with.
In today’s consumer-driven landscape, customers are well equipped with the emergence of social media and other unprecedented ways like Web reviews of products that shape expectations, creating more informed and demanding buyers or users across verticals. It may go against all digital sensibilities, but the undying truth is that online interactions with brands add no personal value to conversations and may result in low display of customer-centricity in the eyes of the everyday customer. In other cases, sometimes, due to exponential growth of the Internet, businesses tend to ignore those who have low or no access to the Web; as if their value of existence means nothing to them. This complex nature of digital platforms comes across as an open threat to businesses in emerging markets.
Instead of becoming another work order in the queue, contact centers should seek a strategy that drives innovation and builds rapport with customers by following up on prior requests. Businesses are struggling with this change to be “customer centric” and, in the process, they are subjected to external pressure, with the biggest setback being unable to smartly handle business calls and deliver quick support.
Interestingly, research findings imply that challenges in customer-centric transformation arise from these significant numbers — 35% of organizations experience missing key technology platforms to manage data while 39% of them fail to define a work culture that is customer-focused.
However, retaining customers has become an expensive challenge for businesses, considering the fact that 89% of those who experience dissatisfaction or low-key responses move over to their brand’s competition. Therefore, more companies are willing to invest in cloud technology to increase retention and make new acquisitions.
While the services industry is filled with the need to respond to urgent requests, leveraging cloud collaboration can work wonders in creating a culture that aligns with these needs. According to research firm Forrester, nearly 95% of business leaders believe in harnessing the power of unified communications over digital platforms to provide quality service as part of their first strategic priority.
Despite the rapid growth of online tools, research findings by Nuance show that 79% of customers prefer to contact service centers directly over a call. This means migration to the cloud for business communications is inevitable.
On attempts to estimate the potential value of cloud-based communications, analysts at research firm Gartner predict that 2016 will be a “defining year for the cloud, as cutting-edge technology will get more sophisticated in the years to come”. Well, cloud technology is becoming a game changer for small and medium-sized businesses in India by offering cost-effective access to technology, scalable infrastructure and capabilities like seamless integration with almost all popular customer relationship management tools. This robust technology with state-of-the-art infrastructure prides itself on a single goal to help small and medium-sized businesses project a good sense of customer-centricity by shining the spotlight on business calls — using advanced mechanisms like interactive voice response, voice mail, time-based call routing, real-time analytics, email reports, call logs and much more.
Businesses tend to overemphasize channels and associated strategies to enable a seamless customer experience. To get to the next level of customer satisfaction, businesses need to reap the following benefits of cloud-based communications: Answer every call with call steering options; manage round-the-clock customer requests; use statistical database to know more about customers; deliver instantaneous response or feedback to customers; and become personalized.
Having said that, a greater understanding of customer satisfaction stems from meaningful business strategies, operational behavior and adoption of game-changing technologies like cloud that comes with proven expertise in creating next-generation customer-centric enterprises.
By Knowlarity
1 
1 
1 
By Knowlarity
"
https://medium.com/@alex-broadbent/integrating-with-a-third-party-soap-api-from-serverless-cloud-functions-e5974ade0ca9?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alex Broadbent
May 14, 2021·5 min read
Integrating with external APIs should be a simple task for any software engineer, almost all companies will rely on external parties or integrations which means that writing an integration should be a simple task.
In most scenarios, the third party uses some REST API where the integration is a service that makes calls out to the API to store or retrieve data. In our use case, we were using a third-party API to store and retrieve pension details but using a rather outdated means of communication called a SOAP API.
From the first inspection of an example of both types of responses, we can see that the SOAP example has a lot more boilerplate:
There’s quite a lot of heavy parsing required to have to dig through this response to find the UserId and for HTML elements we usually have to parse the entire document to find the value we’re looking for, though shortcuts can be used to just pull out the value, to make a reliable client we need to parse the whole response. When compared to an equivalent REST API that uses JSON responses we can see the difference in parsing complexity:
In our Node.JS TypeScript environment, I was using the XMLDOM and XPath to parse and pull out values from the responses we received. XMLDOM is used to parse text into an XML Document and XPath can be applied to retrieve any value from it. For example, when retrieving the UserId in the previous example, we would parse the response as so:
Our XPath library supports nested namespaces and then selecting an element from within any defined namespace that returns, this means we can just look for any field in the response document called m:UserId to retrieve our text value.
We used the CQRS pattern to split out our read queries and write modifiers to the database, this means that when calling third-party APIs we queued up a modifier to put the result of the API call into the database. The way we read that data changes as we can store data as a stream of events that occur over time and then read them based on a purpose and collate all that data into a more distinctive view. Using a stream of events means that we can track each individual request sent to an API rather than just the current state of the user records through a conventional database model.
Because we used serverless functions in Google Cloud Platform, a requirement is that our function has to finish running within the 9-minute lifespan or it will be killed off. While we should expect our functions to finish long within that time, it’s still a good idea to split out the responsibility of each part of the pipeline where we can so we can run our API calls asynchronously and allow them to be replayable and eventually consistent.
By splitting our requests into commands within the CQRS ecosystem, we have ensured that we will have eventual consistency and be able to see a historical audit of all calls to the third party APIs for all users, which also enables us to replay certain events if they failed for any reason.
For our third-party pension provider integration, this meant having events that defined a user behaviour, such as CreateAccount, updateAccount or createTransferIn.
A pitfall of using a SOAP API is that the request body is required to be in alphabetical order, this tripped me up a few times when writing the integration as JSON structures are not ordered so you can pick a more logical ordering based on the object. This got confusing for things like an address where a more logical ordering would be to go from most to least specific:
Within the services, a model class was passed around so it’s only when reviewing the logs that this ordering was particularly a pain, but our server that we were sending data to would fail silently if a request wasn’t in alphabetical ordering so it was quite hard to debug.
Request bodies have to be declared in a string, which can lead to a very ugly function, where we’re sending a CreateUserRequest which contains some additional fields. Unfortunately for us, there is no nice way to go from a UserRequest object to a request body string in XML, so we end up with a function like:
The outcome of this is that you can try to write some more generic functions for templating strings, for example breaking this out to a generic createRequestEnvelope and createRequestHeader but ultimately it’s the same logic.
This may be more implementation-specific, but for the integration I was working on a successful and failed response both had 200 OK status codes where the response body contained an error message if applicable. This differed from the usual approach to HTTP APIs as it means parsing the whole response to know if the call was successful, rather than determining from the response status.
In our case, we used the XPath response to determine if the error was present, in our example above we could add extra code to determine if the userId value is falsy, such as:
This approach is far from the ideal scenario, but I found this quite a lot when writing out functions as the lack of library support and implementation of the third party meant making sacrifices in design in order to produce a working integration.
Overall, integrating with a SOAP API isn’t hugely different from any other third-party, the same flows will exist and the management of the data going in and out of the system is managed by similar services. The main difference is the connection of sending requests and parsing responses in the client. Hopefully, the caveats listed above will save some headaches and the use of event sourcing provides some insights into how events were handled internally within the integration.
Senior Software Engineer at tesco.com, previously at freetrade.io, tryflux.com and dexda.io
50 
50 
50 
Senior Software Engineer at tesco.com, previously at freetrade.io, tryflux.com and dexda.io
"
https://medium.com/@CloudQBnet/protecting-your-saas-data-460eef7aefaa?source=search_post---------130,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud Quarterback
Jan 10, 2017·6 min read
This article was originally published on the Cloud Quarterback blog. Click here to see the original and access bonus content with it.
It is remarkable how many executives, IT departments and users fail to take data protection seriously. It’s especially troubling when you consider that for most SaaS businesses, the data they create, use and manipulate is their business, yet they don’t protect it vigorously.
Unfortunately, however, even companies that make data protection a priority aren’t immune to data loss. Back in May of this year, a Salesforce instance came down for 12 unplanned hours because of a database failure. Three and half hours of customer data was completely lost.
Keep in mind that Salesforce is a massive company with strong IT processes, but even they were hobbled by unexpected events. If they weren’t already taking steps to preserve their data, the situation could have been a lot worse.
According to an IDG Research report, “95% of organizations believe their SaaS provider can/will easily restore their lost data.” They don’t personally concern themselves with data protection, even though 58% of them had experienced some type of data loss in the previous year.
As a SaaS business, your data (and the data of your customers) is your responsibility. In many cases, you don’t need in-house IT. Outsourcing your security needs can be smart, especially for a growing company. But it’s imperative that you ensure your security provider is protecting you from potential threats.
Data protection is critical for SaaS businesses. Download our free guide to data protection methods you should be using.
It’s estimated that nearly 30,000 websites are infected with malware every day. We know about the high profile cases, like Target, JP Morgan Chase, and Adobe, but they affect small businesses too.
As soon as your SaaS gains a little traction, hackers will put you in their sights. Data from Symantec’s 2016 Internet Security Threat Report tells us that small businesses are becoming common targets for hackers.
Source: smallbiztrends.com
In most cases, cyber-attacks are hunting for financial data, but they also capture general user data if they feel it can be used for financial gain. Just because you don’t have credit card numbers doesn’t mean you’re safe.
Furthermore, software services become juicier targets to hackers (outsiders with malicious intents) as they get larger. Even though most SaaS businesses implement security measures as they become data-heavy, hackers are generally interested in disrupting companies for the highest reward.
So as your company grows, the likelihood of an outside attack increases. You need a data protection plan to cover that target on your head.
Preserving your data is important, but a modern, efficient business uses tools to restore their data directly back into the application with perfect accuracy and zero manual effort.
You shouldn’t just recover from data loss. You should be able to put it back just the way it was at any time. Otherwise you’ll spend hours manually recreating labels, file structures, and inputting settings.
The interval you set for restore periods is up to you. It depends on the type of data you collect, its importance to your business, and what your customers expect.
In many cases, data destruction, breaches, or leaks can be attributed to someone inside your organization. Perhaps you have a malicious employee who sends information to a competitor. Perhaps a team member accesses privileged data for personal reasons. Or perhaps someone with your company made a simple, benign error that exposed or destroyed your data.
An Intel Security study found that 43% of all data loss can be attributed to insiders. “But those could be accidents,” you might argue. Unfortunately, half of those cases were found to be intentional. Bloomberg Law discovered that only 35% of companies admit to appropriately monitoring employee behavior. Only 30% admit to doing enough when it comes to monitoring their vendors. Whether it’s accidental or intentional, it’s still a security incident.
It doesn’t matter why it happened. It’s important to protect your data from the people who access it the most — your employees. Strong security measures and data processes will help you restore your information and provide warning signs when something is compromised so you can take action.
A data sync error can occur when you import data from one device or link a device with an application. You may inadvertently overwrite data that you didn’t expect to, or cause a corruption in the database.
While data sync errors are user-driven, their nature makes them easy to roll back, provided protection measures are put in place. You should always have (and give your users) options to restore major changes.
You’re probably taking advantage of the cloud. In fact, about half of all B2B and customer-facing apps are using cloud-based services for data storage, CRM solutions, and security.
If you’re using another SaaS service to manager your data (for instance, you might be hosting through Amazon Web Services), it can be tough for them to distinguish your actions from malicious attacks. If a request (like “delete these folders”) appears to be legitimate, your provider is obligated to carry out the task.
Here’s an example that happens every day. Many businesses use Google Apps to coordinate with their employees. Google charges by the number of employees who use the application. When an employee leaves the company, the business deletes the user so they aren’t charged an additional slot for the replacement. If the user account is deleted before the data (file, documents, emails, etc.) can be assigned to a new user, the data is gone forever. There was no malicious intent here, but the data is still inaccessible.
In some cases, your vendors have security protections to identify when a request doesn’t seem valid. But they definitely can’t determine when a mistake is made due to operator error. This is called user-driven loss. Even if your vendors want to protect you from your own errors, they fundamentally can’t.
Remember the infamous Target hack? 40 million credit cards were compromised, as was the personal information of 70 million customers. Malicious code was added to the point-of-sale terminals, sending the compromised data to Russia.
The hackers invaded Target by way of an HVAC company in Pennsylvania. The HVAC company had access to Target’s vendor network. The hackers used the HVAC business’ credentials to get themselves into the Target network and make their way to the payment system.
In this case, a few mistakes were made. First, why did the HVAC company need network access? Second, why wasn’t the payment system (with sensitive financial data) separated from the rest of the network? Third, why didn’t the HVAC company have protections in place? They may have felt safe because their data wasn’t valuable (who cares how often a refrigeration gasket gets changed?), but their security flaws were just one step in the chain to a bigger fish.
Security incidents can come from all over. They can come from access points you specifically built for other purposes (like the HVAC company’s access credentials). There are likely flaws in your network that your engineers haven’t considered.
Compliance is important
If your industry (or the industry of your customers) is subject to any regulatory compliances, you’ll need to protect your data with the right certifications.
For instance, there are pages of regulations in regards to maintaining and transporting electronic medical records. If you aren’t following the law, you (or your customers) may be accountable for each instance of mishandled data.
At any given time, industry auditors or your customers may want documentation or proof that you’re in compliance. Make sure you have something to offer.
If you know you need security but you’re unsure where to start, download our free guide on security measures you should be taking.
You need a structured security strategy — something that identifies threats and proposes solutions. Your strategy should analyze likely security flaws and identify weak points in your network (like the HVAC company I mentioned before and their unnecessary access).
You could employ an in-house security team to implement a bevy of security features, but this would be costly. You could task your team members to prioritize your security concerns, but this takes them away from growing your business.
As a growing SaaS, it make sense to outsource your security needs to another company. Cloud Quarterback’s cyber security-as-a-service is what you need.
Virtual Information Security Team
1 
1 
1 
Virtual Information Security Team
"
https://medium.com/@jordangruenspan/the-cloud-cf0a8fa4e896?source=search_post---------131,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jordan Gruenspan
Oct 6, 2017·4 min read
When you are out on the weekend and you take a photo of what you did, the photo that you save is saved to your external memory of your phone. Then a few hours later you decide to post the photo you took to Facebook and caption “the best weekend ever!” The steps you took to share your photo to social media is putting your software on “The Cloud.” The analogy seen in movies when someone uses an application to post something online, a cloud follows them around wherever they go. Jess Fee states, “The cloud is a network of servers, and each server has a different function. Some servers use computing power to run applications or ‘deliver a service.’” SO next time when you post something to social media, think of what’s happening behind the scenes or what you’re actually doing when you post that image.
The Benefits of the Cloud
1. Better Flexibility Given to the Employers
Using cloud-based software is excellent for a company that is growing. With the flexibility to have as much bandwidth as possible, or if the company needs to scale down then the flexibility is still there.
2. Recover from Any Global Disaster
Without a physical thing and using cloud-based software nothing can get damaged. Salesforce UK states, “Businesses of all sizes should be investing in robust disaster recovery, but for smaller businesses that lack the required cash and expertise, this is often more an ideal than the reality.” With the ability to save all your work if a disaster occurs, this is an ideal software to work with.
3. Work from Anyplace Anywhere
With the software, if you have a connection to the internet then you are able to work on something on the cloud. You are never restricted on where you need to work. An advantage is businesses can offer flexible working hours to their employees. Rather then working regular work week hours, the employees can decide when they want to work and also where.
4. Reduce the Size of the Workspace
With the ability to create online workspaces, this can reduce the size of the companies’ offices. Cameron Cole of Skyhigh states, “The reduction of the numbers of servers, the software cost, and the number of staff can significantly reduce IT costs without impacting an organization’s IT capabilities”. Cloud computing can save lots of money too by not having physical servers, without limiting the companies overall work capabilities.
5. Less Impact on the Environmental
With less work hours and an overall company footprint, the company can be more green. All the resources can be shared online and using shared software can make the company work more efficiently and use less overall space/ energy entirely.
Types of Cloud Software
1. Microsoft Azure
This software provides IT and developers the ability to “build, deploy, and manage applications through our global network of datacenters.” This allows a database in order to create, host and manage all the software on an online service.
Advantages:
- More regions use this software compared to other cloud providers
- More capabilities
- Strong work Intelligence
2. 3LeafSystems
This allows people the ability to have next generation server solutions.
Advantage:
- Provides companies with a terabyte of DRAM at a very low cost
3. Google Cloud Platform
This software redefines the industry by providing online storage to companies. This is used as a storage storage in order to share information across an entire company.
Advantages:
- Optimal Availability for Live Data
- Storage and Archiving Redefined
- Seamless Data Lifecycle
Cloud software enables companies to work on projects behind the scenes. Also some software is used as a platform in order to store/share multiple files online in order for the company to access for use. It provides an easier way to work as it gives the employers a better flexibility on when they need to work. It also helps the environment by taking up less space by using online systems. Also having the ability to do the work online can benefit the company too, as there is more access to all the information online rather then just within the office.
See all (16)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@simplyitcloud/service-relationships-the-way-to-co-create-and-collaborate-with-your-customers-in-a-digital-era-87e07c5a6220?source=search_post---------132,"Sign in
There are currently no responses for this story.
Be the first to respond.
SimplyIT.cloud
Mar 22, 2019·3 min read
Nowadays many good internal and external service providers recognize that it is necessary not only to provide services but also to cooperate with their customers and vendors to create better value. Basically, it is about the co-creation of value between two or more units or organizations. This cooperation between service providers and their customers is called service relationship.
Service relationship is about the activities between a service provider and a service consumer to ensure continual co-creation of value.
It is necessary to recognize that also a service provider needs to act as a consumer because it is necessary to obtain and configure service components from internal and external teams enabling the service provider to create the service. So, service relationship needs to be managed on both sides of the service provider. With the consumers of the service as well with the vendors of the service components.
This topic is also briefly described within the ITIL 4 best practice. The definition of the service relationship is defined as: “A cooperation between a service provider and service consumer. Service relationships include service provision, service consumption and service relationship management.” ITIL 4 also defines service relationship model which shows interactions between organizations.
In the reality, the concept of service relationships is much more complicated than the picture illustrates. For example, a facility organization can rent office space to an IT organization as well as to the financial organization supported by the IT organization. IT organization can also provide IT services to the facility organization. Interactions between organizations are more likely a network or “ecosystem” than the simple sequence of interactions between organizations.
In the era of digital transformation, all service providers need to become digital enablers for their consumers. The responsibility of the consumer is to obtain and configure service components from internal and external service providers to be able to create, offer and provide their services. Most consumers believe that it is necessary just to obtain and configure service components from their service providers and they will automatically get the value from these service components. But co-creation of value is the continual process of coordination between the consumer and their service providers, not just a one-time activity to agree on the service contract. It is necessary to manage the service relationship during the service provisioning and consumption all the time.
From the consumer viewpoint, the necessary basis for successful management of the service relationships is to define and manage the following three topics:
This service relationship approach forces to extend the scope of traditional service management practices and processes like relationship management, supplier management, service catalogue management, service level management, continual improvement and many others. We discussed this new approach to multivendor service management in this whitepaper.
The shift from processes and governance to value requires a change of mindset and personal attitude that is not as easy and takes time. To support this shift even more, it is also necessary to use efficient (software) tools built and focused more on the service (value) aspect than on the process aspect like conventional Service Management / ITSM tools do.
Author: Miroslav Hlohovský
Consultant and Trainer who helps companies to deliver better value to their customers. Head of Supervisory Board of itSMF Slovakia.
Simplyit.cloud is a cloud-based service helping you to manage your service catalogue, vendors and govern related tasks.
Simplyit.cloud is a cloud-based service helping you to manage your service catalogue, vendors and govern related tasks.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@techforum/how-to-enable-google-cdn-for-custom-origin-websites-google-cdn-for-external-websites-56e3fe66cca9?source=search_post---------133,"Sign in
There are currently no responses for this story.
Be the first to respond.
Albin Issac
Jun 8, 2020·5 min read
Cloud CDN by Google is a low-latency content delivery solution for small to enterprise businesses. Cloud CDN (Content Delivery Network) uses Google’s globally distributed edge points of presence to cache external HTTP(S) load balanced content close to your users, when a user goes to your website, they retrieve your…
"
https://medium.com/@romainnorberg/how-to-deploy-a-simple-backup-app-on-clever-cloud-paas-4461eaf5d4ad?source=search_post---------134,"Sign in
There are currently no responses for this story.
Be the first to respond.
Romain Norberg
Mar 30, 2017·4 min read
For one of my project hosted on @Fortrabbit, i needed an external application that can backup a Mysql database and other thinks. (And stop to do this from my local environment 🙈)
root - bash/ |— backup.sh - buckets/ (ignored on project) - clevercloud/ | — buckets.json | — cron.json .gitignore
To configure your application to use buckets, you must tell her witch bucket to mount and where.
You can find a pre-filled json object to copy in the dashboard of your FSBucket add-on, in the “Dashboard configuration” tab. (documentation)
You can adapt the script with what you need (backup an entire folder, maintenance script, …).
Line 5: To use injected environment variable, we have to add this line at the beginning of your script: `bash source /home/bas/applicationrc`
Line 7: Clever Cloud do not currently support the clustering of cron tasks, you must manage it yourself if your application requires more than one instance. We run script only if instance type is `Production` and if we are on the first deployed instance.
Line 14: Simple `mysqldump` command. `${APP_HOME}` env. variable help us to locate our root project folder.
Line 17: Compress tar.gz with a password is quite complicated, so we use ZIP command with -P.
Line 19: remove .sql file
We want to run backup every day at midnight.
⚠️ All the servers are configured to use Coordinated Universal Time (UTC), please keep it in mind when configuring cron tasks to run at a specific hour.
Unlike the usual solutions on dedicated servers, the crontab is versioned with the project. 👍
According to the documentation, the folder must not exist in your repository (or it needs to be empty). Otherwise, the mount of your bucket will be ignored.
I will not go into too much detail in this step, the documentation is very well done. I choose to use automatic deployment with GitHub.
I invite you to read the excellent article of David Santiago published early 2017, steps are very well explained: How to deploy a Symfony app on Clever-Cloud 🚀
When you push or re-deploy app, you have access to a deployment log (see below).
You can also use Clever CLI tools to get it in your favorite terminal.
Clever Cloud offers solutions that are very easy and fast to set up, this article is not about a big project but it can give you ideas.
Why not deploy this application on FortRabbit rather than on Clever Cloud ?The two platforms have a different vision and offer a slightly different pricing. For this project, the best solution is Clever Cloud:- On Clever Cloud it will cost ~$6.44 per month, the Bucket is free up to 100Mo and management of crons is included.- On FortRabbit, you must subscribe to the “Plus” offer at $12 per month to take advantage of the crons job and backup solutions.
Do not hesitate to leave comments, or ping me on twitter @romainnorberg
Thanks you for reading.
Php developer, play with @symfony and @phalconphp frameworks. More on linkedIn: https://goo.gl/P5oMlN
Php developer, play with @symfony and @phalconphp frameworks. More on linkedIn: https://goo.gl/P5oMlN
"
https://medium.com/@rspraneethkumar/federated-and-external-queries-the-what-and-how-2af22512e913?source=search_post---------135,"Sign in
There are currently no responses for this story.
Be the first to respond.
SP Kumar Rachumallu
Mar 2, 2021·3 min read
BigQuery has the capability that allow users to query data from external data sources without the need of loading data at all. Currently BQ supports GC storage systems such as Cloud Storage, Bigtable, Google Drive and Cloud SQL. To use the federated queries, you should colocate your BQ dataset and the external data source. If you are using Cloud Bigtable, you might want to check the supported locations before making a decision.
If you are running a federated query on Cloud Storage or Google Drive or Cloud Bigtable, the process is pretty much the same with minor to no changes.
Navigate to BigQuery console, and click create table under the desired project :: dataset to run the federated query like you do with a BQ table. Choose the desired source of your choice. Please note, you cannot create an external table for Cloud Bigtable from cloud console due to current limitations.
You can either create a permanent table or a temporary one in order to run a query.
If you are querying data in Google drive, you need to authenticate drive before creating table definition files.
Running a federated query on Cloud Bigtable is almost same as G Drive or GCS, except you need to create a table definition file manually as mkdef does not support Bigtable currently.
To run an external query on Cloud SQL, we need to create a Cloud SQL instance and then add the data using BigQuery Console.
You can create Cloud Instance from Navigation Menu -> SQL -> Select the desired DB (MySQL/PostgreSQL) -> Enter the details as required. Under Create instance page, you can customize machine type, storage type etc.
Once the instance is created, navigate to BigQuery and click Add data. Fill in the details of cloud instance you have created in the above step:
You can copy the cloud instance id from cloud instance page:
As soon as you click create, you would see the external connection under your project.
You are all set to query the MySQL database available under Cloud SQL from BigQuery. A query run from console looks like this:
You can also create the Cloud SQL instance and add data into BQ using CLI commands gcloud and bq mk.
Before running a federated query on any external data source, one should have required IAM permissions, enable necessary APIs, read about limitations, location considerations and pricing.
References:
cloud.google.com
Lead Programmer at Novartis Healthcare Pvt Ltd.
Lead Programmer at Novartis Healthcare Pvt Ltd.
"
https://medium.com/@jpda/sharepoint-online-irm-external-users-366d6967950e?source=search_post---------136,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Patrick Dandison
May 31, 2014·3 min read
Since I can’t seem to find anything online regarding external users + IRM secured lists, I decided I should put it up here. In short,
External users using Microsoft Accounts can’t use IRM-secured documents that use an external client (e.g., Foxit).
There are some nuances, however. Some scenarios work, some don’t. I did all of this testing from a fresh, non-domain joined Windows 8.1.1 VM.
This appears to work. I shared an IRM lib with a Microsoft account and got to work. I could open and view the documents (Excel, Word & PowerPoint) in the Office Web Apps and the IRM restrictions persisted.
For managed PDFs, it’s not nearly as straightforward. Managed PDFs require one of two readers, Foxit or NitroPDF. I only tried Foxit, because Nitro wanted money. First, managed PDFs don’t open in the Word Web App (like they used to, hopefully that will come back one day), they require a client.
I tried to open the PDF from SharePoint, which prompted a download & open. Upon opening, Foxit told me I needed the AD RMS connector, which is a free download. Downloaded & installed that, tried again, then I needed the Microsoft Online Sign-In Assistant (MOSSIA) — another download/install. Did that.
The next time I opened Foxit, I was prompted by MOSSIA to sign in. Since the site was shared with my Microsoft account, I tried that. No dice — it just kept on kicking out my credentials. I tried app passwords, different Microsoft accounts, nothing.
I thought, perhaps it’s just broken, let me try the organizational account that belongs to the tenant which owns the SharePoint Online instance. This at least allowed me to login successfully, only to have Foxit kick me back out saying I didn’t have permission.
I killed Foxit and tried again — but now, my login information seemed to have persisted (granted, it’s what MOSSIA is supposed to do), so I was never prompted to login again. Fine, except that I couldn’t test any other accounts. Uninstalling MOSSIA didn’t help, so I’m guessing I need to whack some registry entries or some straggler files that are persisting my login information.
I didn’t test this. It’s on my list, but I haven’t tried yet.
Also haven’t tried this yet. I’ll be really curious, but since the client I’m designing this for isn’t going to have external users with org accounts, it fell off the priority stack today.
Since my specific parameters are IRM, PDF & External Microsoft account users, I’m left in a bind — there’s not a good story here. My parameters also are that the documents are read-only, so I found that if I convert the PDF to a Word doc and upload to the IRM-protected library, I can see it through the Word web app. That may not work for you, but it’s something to consider. It’s possible you could convert to some sort of an image as well, depending on your situation.
I build for the cloud. Making neat things for customers @Microsoft, raising kids and breaking things at home.
I build for the cloud. Making neat things for customers @Microsoft, raising kids and breaking things at home.
"
https://medium.com/adaltas/importing-data-to-databricks-external-tables-et-delta-lake-4318b3b8d151?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
During a Machine Learning project we need to keep track of the training data we are using. This is important for audit purposes and for assessing the performance of the models, developed at a later time. Depending on the properties of a dataset, notably its size or its expected evolution with time, we need to choose an appropriate format to import it to a data analytics platform. If we have a small and rather simple table that will not change, we can usually import it as-is. But if the dataset is big and it is…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@raduvunvulea/using-an-external-email-provider-with-episerver-dxc-environment-inside-azure-b9cc2e0686a2?source=search_post---------138,"Sign in
There are currently no responses for this story.
Be the first to respond.
Radu Vunvulea
Aug 29, 2018·5 min read
In this post, we talk about different options that we have we need to integrate an email service with an eCommerce application developed using Episerver and hosted inside DXC.
As a side note, DXC it is SaaS on top in Microsoft Azure, where clients can host their web applications developed on top of Episerver. More about the DXC environment will be covered in another post.
What we what to achieve?
We want to be able to integrate an external email service provider to be able to send emails to our clients for marketing purposes. Our web application it is already hosted inside DXC, and even if we could write custom code that can run inside DXC to communicate with the email service provider, there are some limitations that we need to be aware.
The authentification and authorization mechanism offered by the email service provider it is based on IP whitelisting. Only the IPs from that whitelist are allowed to make calls to the service and send emails.
Limitations
At this moment in time, Microsoft Azure is offering the possibility to assign static IP to your resources. Even so, because of DXC environment offers a high-availability SLA, the public endpoints for consumers and clients are based on CNAMEs and not on IPs. Additional to this resources might be shared between different deployments and customers.
It means that there is no way to get a static IP for our DXC environment that can be added in the whitelist of our email service provider.
Beside this, we need to take into account that there is no other authentification mechanism beside IP whitelisting used by the 3rd party and a custom URL provided for each client.
There are multiple solutions available. Let’s take a look on some options that we have. The last one is the one that I prefer, and I think that it is closer to production ready.
Option 1: Whitelist IP list of Azure Region
The public documentation is offering us the IP ranges used in each Azure Region. Additional to this we know the IP ranges for each Azure Services. The list of IPs is updated each time when something change and we can subscribe to this type of notifications.
Inside DXC our applications are running on top of Azure Web Apps, allowing us to provide to our email service providers only the range of IPs used by Azure Web Apps inside that Azure Region.
Even if the solution is simple, there are two risks that we need to take into account and mitigate. The first one is related to defining a process that ensures as that we provide the new range of IPs at the moment when Microsoft is updating the range of IPs. The second risk is related to who can use the service. Because the range of IPs that are provided to whitelist covers all the Azure Web Apps inside that Azure Region means that any web application hosted as a Web App inside that region can send emails if the email service URL it is known.
Option 2: Expose a REST API
The second option involves creating an external REST API, that can be called by our web application and forward the calls to the email service. There is already planned to use an Azure VM for some other functionality, outside DXC. It means that we can use this VM to host out REST API inside the IIS and assign a Static IP to the machine. The API would forward the calls to the email service.
The downside of this solution would be primarily from the security part; we need to design an authentication and authentification system for our REST API. Besides this we need to handle cases when the Azure VM it is not available, we don’t want to have clients that did not receive their emails.
Option 3: Windows Service and Service Bus Queue
This option is based on option 2 and involved moving the forward capability from the REST API to a Windows Service. We are in the context where the Azure VM already has other types of Windows Services deployed, and the most simple thing that we can do it is to add another one that can forward the request to the email service.
To be able to avoid losing messages when the Window Service is not available or when the output is too high, we can add a queue used to communicate between our application hosted inside the DXC and our Windows Service.
Option 4: Azure Function and Service Bus Queue
The downside of previous solutions is that we are keeping the logic inside a traditional on-premises solution. For better result and to simplify things more, we can add our logic inside Azure Functions. For this case, Azure Functions are perfect, because it is offering us a serverless environment where we can run our logic that forward calls from our system to email service.
In theory, the IP shall not change too often as long as we don’t delete our function or change the tier. For Static IP on Azure Functions, we need an App Service Environment, where we can have a clear list of Static IP
Option 5: Azure Functions, Service Bus Queue, and Azure Blob Storage
The previous option is almost perfect, except in one case. When the email that we want to send is bigger than the maximum size of a message in the queue (256K). Even if there is support for sessions on messages, where we can have multiple messages that are consumed together by the same consumer as a message, we need to mitigate the case when the email is bigger than the message queue.
A possible solution is to write the email content (body) directly to Azure Blob Storage and add only the URL to blob storage inside the message. The access can be controlled using Storage Accounts Keys or Shared Access Signature. Depending on how often an email is bigger than the maximum message queue, you can decide what would be the default behavior — email body in the message or inside Azure Blob Storage.
You can add extra logic that calculates the email body size and decides if the message body shall be added in Blob Storage or inside the message.
Conclusion
As you can see, there are multiple solutions to this problem Taking into consideration time constraints, environments, existing solution and many more you can decide which one suits you best.
The cleanest one is Option no. 5, but you might prefer Option no. 3 if you need to deliver a fast solution and you don’t have skills related to Azure Functions.
Technology Enthusiast
Technology Enthusiast
"
https://medium.com/@kaleido_io/kaleido-and-chainlink-partner-to-provide-oracle-services-in-the-blockchain-business-cloud-ee5d9604dcb1?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kaleido
Nov 30, 2018·3 min read
Enterprise permissioned blockchains can be complex because they require external data from beyond the private chain. This is often needed for smart contracts, where the execution of transactions are dependent upon off-chain data sources.
Oracles enable blockchains to connect to off-chain activities and data feeds for smart contract transaction validation. Existing oracles, however, are centralized services which can create a single point of failure, and require dApp (decentralized applications) developers to put explicit trust in the implementer and operator of the centralized service.
Kaleido and Chainlink partnered to provide a decentralized oracle service as part of its Blockchain Business Cloud to enable enterprises to connect their smart contracts to real world data, events and payments.
Steve Cerveny, Kaleido Founder and CEO, says,
“The organizations setting up permissioned chains on Kaleido love Ethereum because of the richness of the ecosystem. Chainlink is a great way to get data on-chain via an oracle, and will allow our customers to create richer, more robust dApps.”
Chainlink’s decentralized oracle network bridges on-chain activities to external data sources using a network of decentralized Chainlink nodes to provide reliable tamper-proof inputs and outputs for complex smart contracts for any enterprise blockchain solution. Some of these sources include legacy ERP systems, payment systems, data feeds, web APIs and more. This extends the reach and impact of blockchains and smart contracts by making decentralized oracles a secure cornerstone of the ecosystem.
For instance, let’s say a group of international companies are conducting trades on a blockchain and want make payment settlements in fiat currencies. Every cross-currency payment needs to be executed with the real-time exchange rate data. Chainlink can be used to query the corresponding monetary authorities on-demand, so that the smart contract can execute the payment settlement using the proper exchange rates.
Dan Kochis, Head of Business Development at Chainlink states,
“With the Chainlink integration, enterprises on the Kaledio Blockchain Business Cloud can now easily and securely connect their smart contracts to real world off-chain data feeds. This Kaledio-Chainlink integration simplifies the process for building connected smart contacts and we’re excited to see all the different business use cases that are created in the Blockchain Business Cloud.”
Kaleido is the first blockchain company to provide a full set of tools and capabilities with all of the essential building blocks to create and customize complete enterprise solutions. With this partnership, Chainlink’s service extends the capabilities of the Kaleido Marketplace and enables customers to incorporate and deploy complex smart contracts that can now easily tap into outside data feeds.
Chainlink is already live and available in the Kaleido Marketplace and can be accessed at: https://marketplace.kaleido.io/
Check out the Chainlink demo on the Kaleido platform: https://youtu.be/CMvsUgpwgHY
About Kaleido
Kaleido, a ConsenSys Enterprise Business, is dedicated to making blockchain radically simple for organizations to adopt so that our societies will fully benefit from decentralized models and technologies. Kaleido is collaborating with Amazon Web Services to offer its Blockchain Business Cloud, an all-in-one SaaS platform built for Enterprise Blockchain. For more information, visit www.Kaleido.io.
About Chainlink
Chainlink is a decentralized oracle network that enables smart contracts to securely access off-chain data feeds, web APIs, and traditional bank payments. Chainlink is consistently selected as one of the top blockchain technologies by leading independent research firms such as Gartner. It is well known for providing highly secure and reliable oracles to both large enterprises (SWIFT) and leading smart contract development teams.”
Start your own Chainlink by visiting their website. To learn more about Chainlink’s latest developments, join them on Telegram or Twitter.
The only truly full-stack platform providing complete enterprise blockchain solutions for today’s modern business networks.Try for free: https://www.kaleido.io
The only truly full-stack platform providing complete enterprise blockchain solutions for today’s modern business networks.Try for free: https://www.kaleido.io
"
https://medium.com/@ThuruTweets/circuit-breaker-pattern-for-cloud-based-micro-service-architecture-8422a84c80f6?source=search_post---------140,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thurupathan Vijayakumar
Nov 14, 2015·4 min read
Modern applications communicate with many external services; these external services could be from third party providers or from the same provider or they are components of the same application. Micro service architecture is a great example for disconnected, individually managed and scalable software components that work together. The communication takes place using simple HTTP endpoints.
Example: Think that you’re developing a modern shopping cart. Product catalog could be one micro service, ordering component would be another one and the user comment and feedback system would be another one. All three services together provide the full shopping cart experience.
Each service is built to be consumed by each other, they might have sophisticated API Management interfaces or just a simple self-documented REST endpoints or undocumented REST endpoints.
Another example is Facebook, it has messenger feature implemented by a totally different team from who manage the feeds page and the Edge Ranking stuff. Each team push updates and individually manage and operate. The entire Facebook experience comes from the whole collection of micro services.
So the communication among these components is essential. Circuit Breaker (CB) manages the communication by acting as a proxy. If a service is down, then no point trying it and wasting the time. If a service is being recovered, then better not to congest it with flooded requests; time to heal should be given to the service.
Circuit Breaker and Retry Logics
It is important to understand when to use Circuit Breaker and when to retry. In case of transient failures, application should retry. Transient failures are temporary failures; a common example would be TimeOutException. It is obvious that we can retry for one more time.
But think an API Management gateway blocks your call for some reason (IP restriction, Request Limit) or any 500 error then you should stop the retry and inform the caller about the issue. And let the service heal. This is where Circuit Breaker helps.
How Circuit Breaker Works?
Circuit Breaker has 3 states.
Look at the below diagram and follow the context for the explanation.
By default, Circuit Breaker is in the Closed state. The first request comes in and it will be routed to the externa service. Circuit Breaker keeps a counter for the non-transient failures occur in the external service in a given time period. Say that the given time period is 15 seconds and the failure threshold is 10, if the service fails 10 times within 15 seconds for n number of requests then Circuit Breaker goes to Open state. If there’re no or less than 10 failures during 15 seconds, the failure counter will be reset and Circuit Breaker remains in Closed state.
In the Open state, Circuit Breaker does not forward any requests to the external service, regardless of how many requests it receives. It replies to those requests with the last known exception. It remains in the Open state for a specified time period. After the Open state has elapsed Circuit Breaker enters the Partial Open state / Semi Open State.
In the Partial Open state, some of the requests are being forwarded to the external service while others are being rejected as Circuit Breaker is in Open State. In the allowed number of requests Circuit Breaker monitors the success of those calls, and if a specified number of calls are continuously successful then Circuit Breaker resets it counters and goes to the Closed state.
The mechanism of which calls should reach the service during the Partial Open state is up to the implementation. You can simply write an algorithm to reject one call after the other or you can use your own business domain. Example calls from members of Admin role can pass through and others fail.
Partial Open State and preventing Senseless blocking.
This is a bit tricky state because, this state does not have a timeout period. So Circuit Breaker will remain in the Partial Open state until the right number of requests come to satisfy the condition. This might not be preferable in some cases.
Example, consider the service is down at 10:00:00AM and Circuit Breaker goes to Open state. After 3 minutes (at 10:03:00AM) it goes to Partial Open state. From 10:03:00AM to 10:23:00AM only few requests came and some of them will be rejected by the Circuit Breaker, and still Circuit Breaker is waiting for more calls though the service is perfectly back to normal by 10:08:00AM. I named this kind of prevention from the Circuit Breaker as Senseless Blocking.
There are few remedies you can implement to prevent senseless blocking. Simply we can put a timeout period for the Partial Open state or we can do a heartbeat check from the circuit breaker to the external service using a background thread. But be mindful that senseless blocking is an issue in Circuit Breaker pattern.
When not to use Circuit Breaker?
When you do not make frequent calls to the external service, it is better to do it without going through a Circuit Breaker, because when your calls are not frequent there’s a high probability that you might face senseless blocking.
Implementation
I have provided a reusable pattern template for the Circuit Breaker.
Code is available in GitHub : https://github.com/thuru/CloudPatterns/tree/master/CloudPatterns/CircuitBreakerPattern
Originally published at thuru.net on November 14, 2015.
Chief Technology Officer @aventude | Author | Speaker | Azure MVP | http://www.thuru.net
Chief Technology Officer @aventude | Author | Speaker | Azure MVP | http://www.thuru.net
"
https://medium.com/@dockerturtle/2019-networking-secrets-in-google-cloud-8284217c3a66?source=search_post---------141,"Sign in
There are currently no responses for this story.
Be the first to respond.
Docker Turtle
Jun 20, 2019·1 min read
Containers on Cloud
Containers on Cloud
"
https://medium.com/@sabbarmehdi/how-to-setup-spring-cloud-config-server-microservices-df44c1acc33?source=search_post---------142,"Sign in
There are currently no responses for this story.
Be the first to respond.
SABBAR El Mehdi
Jul 15, 2021·2 min read
The Config server API is a central place to manage external properties for applications across all environments. In this article, I will include two ways.
Step 1:
Go to https://start.spring.io/ write a name for project(it’s preferable to be something significant, in my case it’s ApiConfigServer). Add Config Server to dependencies, check that the…
"
https://medium.com/@sannnaxo/how-does-cloud-store-your-information-acad4dc384c8?source=search_post---------143,"Sign in
There are currently no responses for this story.
Be the first to respond.
Afsana Jubaida
Oct 7, 2017·3 min read
The cloud stores all applications and data in an external source, removed from the local hard drive. This greatly reduces the storage space required and allows access to files from multiple devices. Many of us are familiar with web based email services such as Hotmail, Gmail and Yahoo which allows us to access our inbox from any device. In addition, Google Drive and iCloud use cloud computing to store our pictures and documents. In this article I will break down the mysteries behind the cloud so that we can understand how and where exactly our valuable information is being kept.
The cloud is composed of a front end and a back end, which are connected by a network (such as the internet). The cloud resides in the back end, while the front end is what the user is able to see. The front end is the computer and the application used to access the cloud. In the case of email, the web browser (like Internet Explorer) and the computer is the front end. Authentication, such as entering username and password is often used to secure information. This will then send a request to the back end (web servers and the data storage devices) which will give you the page you are requesting.
A central server oversees this process and uses protocols (specific rules) and middleware (a software that allows networked computers to exchange information) to make sure everything runs smoothly. To make sure no data is lost due to device malfunction, cloud systems practice “redundancy” and backup their stored data. This means that the cloud takes up twice as much storage space for the same information.
The cloud can run every program a normal computer can. Below we will look at some pros and cons of cloud.
Convenience: applications and files can be accessed from anywhere and any device.
Low cost: using a cloud keeps the cost of hardware and storage space down. A low cost computer with enough middleware to connect to the cloud and an input device is all that’s necessary. In addition, software doesn’t need to be bought and installed individually for each device that the file will be accessed on.
Space: smaller companies have the benefit of having more physical space available to do other things rather using it to store hardware.
Less system problems: there are less IT related issues in a cloud system since they all have the same machines and operating systems. In addition, lost devices can easily recover files.
Power of multiple computers: all the networks in a cloud system could be used to process information which would otherwise be impossible using a single computer. This is useful for scientists and researchers that compute complex equations that would take years to solve using a single system.
Security: if information is more accessible to users then it is also more accessible to hackers.
Privacy: some may be hesitant to hand over private information to an external source.
Offline access: if there is an issue with the network, such as power failure, the files will not be accessible. In addition, places with slower internet connections will experience longer wait times retrieving and accessing applications.
Cloud computing systems are found on almost every device these days. It allows us to run programs from our phone which would otherwise not be possible. Large devices are not necessary anymore since everything is stored in an external hard drive. As security measures improve and the internet becomes more accessible, I strongly believe the cloud is the future.
Information for this blog was retrieved from HowStuffWorks and Lifewire.
Blog for Digital Skills and Innovation for the Global Economy (EID100) at Ryerson University
Blog for Digital Skills and Innovation for the Global Economy (EID100) at Ryerson University
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lewiswalker701/how-cloud-based-applications-can-help-businesses-grow-67083c096d37?source=search_post---------144,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lewis Walker
Apr 13, 2017·2 min read
Cloud based computing refers to the practice of using external IT infrastructure to host a company’s applications and services. Historically, companies have hosted software and applications on servers and networks directly owned and maintained by the business. Cloud based computing provides business with a new model for managing their IT infrastructure and deploying applications and services.
Cloud computing relies on the use of virtual computers hosted by an infrastructure service provider. Cloud based computing is different from remote hosting. In the remote hosting model, companies rent physical servers which are then operated and maintained by a third party. Cloud computing allows companies to make use of virtual machines hosted on the Internet by service providers such as Amazon, Microsoft or, Google.
In the cloud based model a secure, private environment is established on a virtual server within the Cloud Service provider’s environment. Because, one cloud based server, hosts multiple virtual servers, the cost of operating and maintaining a company’s applications in the Cloud is significantly lower. The operating expenses are based on the fractional use of a server, rather than, owning the server and its associated software out right.
In addition to a lower cost of ownership Cloud computing offers other advantages:
The ability to scale up or add additional virtual servers on demand
Built In fault tolerance and redundant storage
Continuous updates and maintenance
Enhanced Security
All of these advantages help businesses to reduce operating costs and improve productivity. Partnering with an application and solution partner can ease a company’s transition to the cloud. Working with an experienced partner facilitates the transition and shortens the time to value.
When looking for a partner to assist with you transition to the Cloud research the potential partner’s background. Evaluate a company’s portfolio and speak to satisfied and dissatisfied customers. Make sure the company is experienced and backed by a team of qualified professionals who understand your business requirements and come up with solutions that can help you in improving your sales and bottom line.
Author’s Bio: The author is an avid blogger. This article is about the benefits of cloud based applications for businesses.
"
https://medium.com/@anushkabiradar819/how-cloud-computing-is-bringing-a-reform-in-the-digital-world-1db497b96840?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anushka Biradar
Jul 13, 2018·3 min read
Today cloud computing is a worldwide term and transforming the digital world gradually. Gone are those days where you had to use external storing devices to stock your valuable data. With the advent of cloud computing, storage and accessibility have never been so easy. And due to its popularity, many skill-based learning institutions are giving emphasis on cloud computing training and have made the topic their prime focus. So how exactly it is shaping the world? Well, let’s say that in the next few years, cloud computing will be the only storage medium that organizations will embrace.
When the term ‘Cloud Computing’ came to origin people had very limited knowledge about it let alone know about its advantages. But now, many organizations have noticed how efficient it can be and have been making the best use of it. Organizations have profited from the fact that they don’t have to make any investment in new infrastructure, offer cloud computing courses to new personnel, and license new software. Simultaneously Cloud has made real-time accessibility convenient without the need of subscription-based or pay-per-use services.
As the IT industry has been growing rapidly in the recent years, clouds computing training and the use of cloud have been leveraged by the IT sector in every aspect. Mainly because in the world the information-heavy IT industry, a cloud has accelerated business processes and reduced operational cost.
One of key factor why the IT industry has been utilizing cloud is because software upgrades have always been a laborious job, especially when changes had to be made on a corporate level. But due to the cloud computing, the service provider handles all the issues and hassles of system updates. In this way, IT industries are able to focus on core competencies without being distracted by ongoing updates. And that’s one reason why IT industries are more inclined to educate their employers by giving cloud computing training.
Scalability in the cloud is another reason why organizations, especially IT industries are keen to add to their service. Before cloud came into the picture, companies were struggling to keep up with the time upgrading the servers and the storage needs. But the emergence of a cloud has brought a drastic change by expanding storage needs and managing the server upgrades. Since the storage need is fluctuating, Cloud platforms are considerably scalable, and cloud computing training is important to educate the staffs.
In terms of cost, cloud computing allows a company to lower their operational cost and improve resource efficiency while cutting on equipment and IT staffing cost. Since most of the system issues and updates are handled by the service providers, companies can lower the cost of expenditure of infrastructure to half.
IIHT, Indian Institute of Hardware technology, from its inception has grown to become one of India’s biggest IT-based training institutions. IIHT also boast of being the first IT institute in India to offer cloud learning in its institution. Over 25 years of experience in the IT education segment, IIHT has made trained and have placed many youths and envisions doing more. Today its presence is all across India and overseas and aims to make the name known in every part of the world.
"
https://medium.com/@rahu1/aws-kms-using-keys-with-external-key-material-9ffe4befc587?source=search_post---------146,"Sign in
Rahul Sharma
Aug 19, 2021·3 min read
When we create keys within AWS KMS, AWS creates and manages the key material for that key. We can also create keys using our own key material that has been created outside of AWS. In this blog post, we will learn how to import our own key material into AWS KMS.
We will start by creating our key from AWS KMS by setting the key material origin to External. Then, we will download the key import wrapper from the AWS KMS service, generate the key in our local machine, and wrap it with the import wrapper. Finally, we will upload our key material that’s been wrapped with the import wrapper to finish key creation.
We can create a key configuration for an external key as follows:
Note: Keep the ImportParameters.zip file for the steps we will follow in the next section. Once we click Next, we will be taken to the screen for uploading our key material. First, we will generate our key material and return it to AWS KMS in the Rotating Keys in the KMS section.
Generating our key material using OpenSSL
We can generate our key material using OpenSSL as follows:
This will generate a file called PlaintextKeyMaterial.bin.
This will generate a file called EncryptedKeyMaterial.bin.
We can upload our key material from the AWS KMS console as follows:
Sr. Lead DevOps Lead @SourceFuse, AWS APN Ambassador, AWS Community Builder
Sr. Lead DevOps Lead @SourceFuse, AWS APN Ambassador, AWS Community Builder
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-technews/samsung-external-ssd-t1-is-the-smallest-fastest-and-most-stylish-ssd-ever-b99d795beee?source=search_post---------147,"There are currently no responses for this story.
Be the first to respond.
Samsung had the better year for their SSD selling statistics. They had gone through experiencing success for their SSD title and again for the Samsung External SSD T1 which gets the benchmark for the 450MB/s data transfer rate even in the smallest size parameter!
Samsung posted on their website, “We’ve taken the power and performance of an internal SSD, added cutting edge security and put it in a compact, stylish case.” Yes, they have put AES 256Bit encryption to provide military range security for the users. There are some extraordinary features that Samsung claims.
Have you ever imagined that you can put your External SSD into your Wallet? Yes, Samsung T1 SSD is the slimmest Drive that can be put into the wallet size carriers too! Are you worried about the performance of this small-sized HDD on your hand? Giving a 2.8X2X0.4 Inch dimension can provide you the best compatibility of storage function as well. Never judge by the size, it can burn your entire drive into its storage capacity which is up to 1TB!
Providing the same internal SSD-like performance, they have managed to put 450MB/s read and writing capacity for the serious users. This is 3–4 times faster than the native HDD drives, that are even faster than any other USB 3.0 pen drives too.
Just because you may drop the SSD from your hand, Samsung has ensured the highest durability with the Shock and Vibration reduction implication. It features the Dynamic Thermal Guard which can protect the data on high temperatures as well.
Many people love the External Drives for the Security matters. Well, Samsung External SSD T1 features the AES 256-bit encryption algorithm which is called the military class protection for the serious users. The SSD doesn’t even ask for the installation hassle, you just need the installation for the first time’s sake. Well, this is the Best external SSD coming with 250GB, 500GB and 1TB Storage with a variant price tag up to $379!
A technology media that aims at latest technews, tech…
Written by

A technology media that aims at latest technews, tech events, gadgets, smartphones, tools, innovations, startups and many more…
Written by

A technology media that aims at latest technews, tech events, gadgets, smartphones, tools, innovations, startups and many more…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lsnelgrove_ru/the-cloud-9c3a7111ea48?source=search_post---------148,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lauren Snelgrove
Sep 27, 2016·4 min read
(when carrying an external hard-drive around became irrelevant)
Before my Digital Skills class at Ryerson, I didn’t know much about “The Cloud” — even though I use Dropbox every day at work. I often imagine that I’m uploading my files to it and then losing them forever in this large, cloudy mist of files and data.
The truth is, nobody actually knows where the information is really going. However if you’d like to learn more about this mysterious technology, read on for 3 facts about Cloud Computing.
And it’s made up of 4 main pieces of equipment that makes it run:
Server Farms — picture a farm, except with a ton of equipment rather than animals. These farms are located all over the world (California, Texas and Chicago for example) and consist of a bunch of computers, routers, power supplies, and more related electronics. Want to learn more about server farms? Check out the blog “10 Facts You Didn’t Know About Server Farms” where I got some of my information!
Power Source — these farms need to have some sort of power source to run all their equipment. The above mentioned blog shares that Google’s server farms use around 260 million watts of power, which is enough to power 200,000 average homes!
Cooling — these huge farms all over the world are running all this equipment 24/7. Think about sitting on your laptop for even 1 hour, how hot does it get? That’s why cooling systems play such an important role in running server farms effectively. Facebook built a large-ass server farm in Lulea in Sweden, using the cold climate there to naturally cool its tens of thousands of servers without using electricity. Read more about it in this BBC News story.
Redundancy — AKA a backup server (or multiple). Humans = not redundant. We can’t create perfect copies of ourselves. However, you can make a perfect server copy, and these are used in case anything breaks. We don’t want all the people losing all their files if anything breaks!
You know I’m not talking about cumulonimbus and stratus clouds, if you want that information you’re reading the wrong website.
Amazon Web Services: Provides services like the “Elastic Compute Cloud” (computing capacity for developers) and the “Simple Storage Service” (easy-to-use, secure, highly scalable cloud storage… according to Amazon).
Dropbox: This is the Cloud Computing company that I’ve used. It’s simple and easy, for the everyday Cloud user. Upload any of your files onto it, and access them from anywhere. You only have to pay if you need a certain amount of storage (AKA use it often/for an external hard-drive for your computer). Otherwise, it’s free!
Rackspace: “Serving more than half of the Fortune 100”. This company offers a Cloud platform for building websites, Cloud storage service for your files, and Cloud Servers that provide access to virtualized server instances.
These are three companies among many that offer Cloud services. If you want to read about more, check out this blog.
“The Cloud” (Cloud Computing) is physical — it’s really there. It seems like we just upload our files into thin air (which we almost do), but those files are backed up by thousands and thousands of servers all around the world.
This amazing technology saves you and I both time and money. An external hard-drive can cost $100 (or more or less), depending on the storage space. If you do buy one, you need to carry it around with you to access what’s on it. You also need to not lose it, which for me is pretty tough.
With Cloud Computing, the only thing you need to remember is another password. You can share files from The Cloud easily by sending a link to another person, which is also amazing. So simple — so cheap — so efficient.
Thanks for reading. :) ❤
Ryerson student / Marketing Coordinator
Ryerson student / Marketing Coordinator
"
https://medium.com/@k0a8t1o6/i-am-also-using-backblaze-online-backup-service-too-for-my-internal-ssd-and-2-external-hdd-96f33b4fe91b?source=search_post---------149,"Sign in
There are currently no responses for this story.
Be the first to respond.
Takashi Kato
May 31, 2016·1 min read
I am now using Backblaze after switching from using CrashPlan. It backs up an unlimited number of files for around $5 per month through a small application installed on your Mac or PC. It also offers encryption of your files and if you use your own encryption key even Backblaze themse…
Adam Karnacz
I am also using Backblaze online backup service too for my internal SSD and 2 external HDD. Even so, the cost is still $5 which is really amazing.
Keeping photo on cloud service is not safe enough unless joining the company pool doing data back up as their first priority business. You do not want to see for example Google is shutting down Photos service just like Google Reader again.
Get multiple copies of your data asap. But remember to manage them well otherwise they would become a tiny piece of data of the bunch.
Thank Adam for his sharing :)
The Pokémon guy who takes photography, writing entry and seeking for unseen view.
The Pokémon guy who takes photography, writing entry and seeking for unseen view.
"
https://medium.com/@NordicBackup/external-hard-drive-repair-and-recovery-best-practices-681b99d8fb27?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nordic Backup
May 2, 2016·4 min read
While external hard drive loss is a common occurrence, many device owners are unprepared for what to do in the event that their hard drive crashes or becomes damaged.
Any physical storage device is susceptible to physical damage or technical error, and external hard drives are no exception. Hard drive recovery poses another challenge: restoring all of the lost data depends on the extent of disk damage. Recovering data from a severely damaged disk requires highly advanced technical skills, and still there is no guarantee that all of your data can be returned. If you’re in need of external hard drive repair and data recovery, these best practices can help. But remember, the best form of data loss prevention is preparation. Before you experience another data loss disaster, plan for the worst and invest in a cloud backup solution that will keep your data intact and available no matter what threat approaches.
As aforementioned, cloud backup is an ideal way of recovering your data in the event your physical storage device becomes inoperable. However, if you have a damaged external hard drive and no online backup available, you may be able to recover some data by repairing the hard drive. The following are tips on external hard drive repair.
An external hard drive comes with an enclosure that carries the disk. This enclosure consists of connectors and a USB cable. As the name implies, “connectors” join the hard drive to the enclosure. The connectors are delicate and easily susceptible to breakage especially if subjected to consistent dropping of the external hard drive. When an external drive stops functioning, you need to unscrew the enclosure and check the connectors for any damage. It is easy to identify connector damage, as the breakage is visible. The best way to identify if the connector has a problem is by testing the disk on a different enclosure.
In some cases, the hard drive’s circuit board might be damaged and in need of replacement. If you notice some burns on the circuit board, try replacing it with a new or spare board and test if it works. Only attempt this if you have technical knowledge and feel comfortable taking your device apart. Without this knowledge, you can make the problem even worse. When in doubt, always go to a professional.
There are varieties of software packages that provide solutions for external hard drive repair. These programs have testing and diagnostic tools that are vital for hard disk repair. They will test your hard drive and give recommendations if you need to make a replacement. Recuva is one popularly used option.
In some instances, your hard drive might get a virus or malware infection that common antivirus programs cannot repair. This situation will require you to make a clean format of the hard disk. A clean format involves erasing everything that is on the hard drive. Formatting the hard drive restores it to its original state. At the very least, you’ll be able to save your hard drive and will avoid infecting other devices you connect it to. You might panic about losing your data, however, the availability of powerful recovery software can help you recover some of your data before reformatting. You will however need to be wary of how you handle the data that’s been recovered so that you don’t reinfect your clean formatted hard drive, or a new device. Instead, anytime viruses are involved, it’s always best practice to recover lost data from your backup rather than from the infected device. Recovering data from your backup will allow you to recover a version of your data that existed before it was touched by a virus.
You should note that external hard drive repair requires technological skills. If you do not have expertise in hard disk repair, you will need to contract the services of a professional. They can often get back more of your data than you could on your own, and sometimes can retrieve more than a data recovery software could. Moreover, they will give you recommendations on the best way to backup your data.
If you’ve lost data to a damaged hard drive, your next step after repairing the device and retrieving all possible data from it, should be investing in an advanced data loss prevention tool, like cloud backup.
Hard drives are prone to physical damage, technological failure, theft, and other threats that can jeopardize and even delete your most important data.
Cloud backup is the antidote to data loss. It’s installed on your computer(s)/laptop(s) in a few simple clicks and allows you to create copies of your entire system, along with your external hard drives. This way, when your hard drive fails, is lost, or stolen, you can recover data directly from the cloud and reinstate it on a new device, or the old (repaired) one. Some other benefits of cloud backup include:
Relying on free data recovery programs from the internet is not an ideal option for maintaining and managing your data on any physical storage device. They’re often unable to retrieve all of the data that’s been lost, which can leave you missing some very important files. The best option is always to backup your data online using a reliable cloud backup service, like Nordic Backup. With cloud backup, you rest assured that all of the data on your computer and external hard drives is secure and recoverable, no matter what.
Leading the industry in secure online backup. The most comprehensive backup and recovery solution for business and personal users. http://hubs.ly/H01kyJQ0
Leading the industry in secure online backup. The most comprehensive backup and recovery solution for business and personal users. http://hubs.ly/H01kyJQ0
"
https://medium.com/propel/collaboration-best-practices-for-cloud-plm-b40d0b144620?source=search_post---------151,"There are currently no responses for this story.
Be the first to respond.
Recently one of our customers, who works with thousands of suppliers, asked me how Propel handles external collaboration. And they wanted a more detailed answer than my typical marketing response of “auto-magically” :) One of their PLM systems that they had been using for years didn’t handle suppliers well at all, and they’re thinking that Propel could help with supplier collaboration.
Sadly this is pretty common. PLM vendors have been talking about collaboration for over 30 years, but truth be told, it’s just been limited to a company’s own mechanical engineers. Sharing product data and changes with suppliers, partners, channels and customers has always been a pain, if not impossible. And it’s not just on-premise systems that make this hard, but even older cloud PLM systems.
But thanks to modern cloud platforms like Salesforce (which Propel is built on), it’s finally possible to get everyone working together. First of all, the Salesforce platform provides Propel some world-class network security, including two-factor authentication, IP login ranges, session timeouts, etc. But how exactly do you segregate different data and objects from different users? It’s so easy that even a marketing guy like me can set it up!
Both Salesforce and Propel have detailed documentation on how to do this, but here are 5 ways that Propel helps manage user security across internal and external users (apologies to those admins who may say I’m butchering some points!).
There are tons and tons of benefits from easier supplier, partner and customer collaboration. Faster on-boarding, lower overhead, more responsive partners, greater flexibility, scalability — you name it.
Want to find out more? Check out the Propel website.
Propel’s latest thoughts on product success, innovation…
Propel’s latest thoughts on product success, innovation, and cloud technology. Visit propelPLM.com for more information.
Written by
Driving Product Success
Propel’s latest thoughts on product success, innovation, and cloud technology. Visit propelPLM.com for more information.
"
https://medium.com/@kalosbonasia/anteprima-su-external-data-for-jira-fields-c90cc2b40498?source=search_post---------152,"Sign in
There are currently no responses for this story.
Be the first to respond.
Calogero Kalos Bonasia
Oct 16, 2021·2 min read
Di recente, per un mio cliente, ho dovuto affrontare e risolvere una esigenza relativa al collegamento di Jira Cloud a fonti di dati esterne. Ho provato numerosi plugin, anche di fornitori che per plugin “versione Jira Server” sono blasonati e fanno da punto di riferimento per tutti gli altri.
Alla fine nessuno di questi soddisfaceva in pieno le mie esigenze, tranne External Data. Come potete vedere dalla tabella seguente
supporta i campi nativi di Jira e, soprattutto quello che interessava me, la possibilità di correlare tra di loro i campi sui moduli di Jira Service Desk.
Il plugin si trova sull’Atlassian Marketplace a questo indirizzo: https://marketplace.atlassian.com/apps/1219994/external-data-for-Jira-fields ed è importante notare che il fornitore aderisce al Marketplace Bug Bounty Program e garantisce l’autovalutazione della sicurezza.
Questo plugin permette di recuperare dati da fonti esterne via REST API, oppure accedendo a MySQL, PostgreSQL, Microsoft Azure (SQL Server di Microsoft) o Salesforce. Inoltre permette di usare un progetto Jira con le sue issue come fossero un database ed i suoi record. Interessante no ?
Code Fortynine è una piccola società di software con sede a Karlsruhe, in Germania, ma nonostante questo ho riscontrato un eccellente supporto al cliente (e tanta pazienza per rispondere alle mie numerose domande, bisogna riconoscerlo).
Questo è uno degli aspetti che tengo sempre in considerazione quando provo un plugin: faccio quattro o cinque domande “ovvie” per tastare la velocità di risposta del supporto ai clienti e la qualità delle risposte stesse.
Ecco tutti gli altri plugin che mettono a disposizione degli utenti:
OMNIA MEA MECUM PORTO [https://www.linkedin.com/in/calogerobonasia/]
See all (41)
 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
OMNIA MEA MECUM PORTO [https://www.linkedin.com/in/calogerobonasia/]
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cloudwithchris/33-external-config-and-claim-check-pattern-easier-management-and-externalising-payloads-1e3770d84ead?source=search_post---------153,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cloud With Chris
·May 1, 2021
This post was originally published on Fri, Apr 2, 2021 at cloudwithchris.com.
How often do you think about the configuration of your applications across environments/regions/deployment boundaries? What if that configuration was stored somewhere externally but centralised, to make management easier? That’s the idea behind the external config pattern! How about another scenario — What about those times where you’ve wanted to use a messaging service, but your payload is too big? Thought about externalising that payload too? Well, that’s the Claim-check pattern! Join Peter and Chris as they talk about both of these patterns in this episode of Cloud with Chris!
Exploring Cloud concepts with Chris Reddington (Welsh Tech Geek, Cloud Advocate, Musical Theatre Enthusiast and Improving Improviser!)
"
https://medium.com/@dpaunin/kubernetes-production-to-be-or-not-to-be-3f79516016a6?source=search_post---------154,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dmitriy Paunin
Sep 22, 2017·9 min read
Hundreds of containers. Millions of external requests. Myriads of internal transactions. Monitoring and incident notifications. Simple scalability. 99.9% uptime! Deployment and rollback.
Kubernetes as a magic pill! To be, or not to be: that is the question!
"
https://medium.com/riskified-technology/engineering-for-failure-f73bc8bc2e87?source=search_post---------155,"There are currently no responses for this story.
Be the first to respond.
Not so long ago, our systems were simple: we had one machine, with one process, probably no more than one external datastore, and the entire request lifecycle was processed and handled within this simple world.
Our users were also accustomed to a certain SLA standard — a 2-second page load time could have been acceptable a few years ago, but waiting more than a second for an Instagram post is unthinkable nowadays.
(Warning: buzzwords ahead)
When systems get more complex, with strict latency requirements and a distributed infrastructure, an uninvited guest crawls up our systems — request failure.
With each additional request to an external service within the lifecycle of a user request, we’re adding another chance for failure. With every additional datastore, we’re open to an increased risk of failure. With every feature we add, we risk increasing our latency long-tail, resulting in a degraded user experience in some portion of the requests.
In this article, I’ll cover some of the basic ways we at Riskified handle failures in order to provide maximal uptime and optimal service to our customers.
Every external service, no matter how good and reliable, will fail at some point. We at Riskified learned this the hard way when we experienced short failures with a managed, highly available service that almost resulted in data loss. That incident taught us the hard lesson that request failures should be handled gracefully.
In Google’s superbly written Site Reliability Engineering Book, they describe The Global Chubby Planned Outage, in which a service was so reliable, that its customers were using it without taking into account the possibility of failure, and even using it without a real essential need, just because it was so reliable.
As a result, Chubby, Google’s distributed locking system, was set a Service Level Objective (SLO) for service uptime, and for each quarter this SLO is met, the team responsible for the service intentionally takes it down. Their goal is to educate users that the service is not fail-safe and that they need to account for external service failures in their products.
So how should engineers handle request failures? Let’s cover some comment patterns:
Retrying a failed request can, in many cases, solve the problem. This is the obvious solution, assuming network failures are sporadic and unpredictable. Just set a reasonable timeout for each request you send out to an external resource, and the number of retries you want, and you’re done! Your system is now more reliable.
Something to consider, however, is that additional retries can cause additional load on the system you’re calling, and make an already failing system fail harder.
Implementing and configuring short-circuiting mechanisms might be a thing to consider. You can read more about it in this interesting Shopify engineering blog post.
One of the best ways to avoid failure while calling an external service is to avoid calling this service at all.
Let’s say we’re implementing an online store — we have a user service and an order service, and the order service needs the current user’s email address in order to send them an invoice for their last purchase.
The fact that we need the email address, doesn’t mean we have to query the user service while the user is logged in and waiting for order confirmation. It just means that an email address should be available.
In cases of fairly static data, we can easily pre-fetch all (or some) user details from the user service in a background process. This way, the email is already available during order processing, and we don’t need to call the external service. In the event the service fails to fetch user details, that failure remains outside of the main processing flow and is “hidden” from the user.
In his talk, Jimmy Bogard explains it better than I do (the link starts from his explanation about prefetching, although the whole talk is great!)
In some cases, we should just embrace failure, and continue processing without the data we were trying to get. You’re probably wondering — if we don’t need the data, why are we querying it at all?
The best example we have for this in Riskified is a Redis-based distributed locking mechanism that we use to block concurrent transactions in some cases. Since we’re a low-latency oriented service, we didn’t want a latency surge in lock acquiring to cause us to exceed the SLA requirements of our customers. We set a very strict timeout on lock acquiring so that when the timeout is reached, we continue unlocked — i.e we prefer race conditions over the increase in latency for our customers. In other words, the locking feature is a “nice to have” feature in our process.
In some cases, you may be able to use previous results or sub-optimal estimations to handle a request while other services are unavailable.
Let’s say we’re implementing a navigation system, and one of the features we want is traffic jam predictions.
We’d probably have a JammingService (not to be confused with the Bob Marley song), that we’d call with our route to estimate the probability of traffic jams. When this service is failing, we might choose a sub-optimal course of action, while still serving the request:
In both examples, the solution is obviously not optimal, but probably be better than failing a request. The general idea here is to make a simple estimation of the result we’re trying to get from the external resource.
If the business of the product allows it, it’s possible to delay the processing of the request until the problem with the external resource is solved.
As an example, let’s take the JammingService from the previous solution — when it fails we can decide to queue all requests in some internal queue, return a response to the user that the request cannot be processed at the moment, but a response will be available as soon as possible via push notification to the user’s phone, or via webhook for example.
This is possible mostly in asynchronous services, where we can separate between the request and the response. (If you can design the service to be asynchronous to begin with, that’s even better!)
On some mission-critical features, a more complex solution is needed. In some cases, the external service is so critical to our services, that we’d have to fail a request if the external service fails.
One of the solutions we devised for such critical external resources, is to use “simplified” in-process versions of them. In other words, we’re re-implementing a simplified version of the external service as a fallback within our service, so that in the event the external service fails, we still have some data to work with, and can successfully process the request.
As an example, let’s go back to our navigation system. It might be such an important feature of our system, that we want each request to have a fairly good traffic jam estimation, even if our JammingService is down.
Our JammingService probably uses various complex machine learning algorithms and external data sources. In our simplified fallback version of it, we might choose, for example, to implement it using a simple greedy best-first algorithm, with simple optimizations.
In this case, even if there’s a failure of the JammingService, some fairly good traffic jam estimation is available within our navigation system.
This isn’t optimal since now we need to maintain two versions of the same feature, but when the feature is critical enough, and may be unstable enough — it could be worth it.
At school, I was quite a bad student, so failing is not new to me. This taught me that as an engineer, anything I lay my hands on might fail, and simply catching the exception is not enough — we need to do something when we catch it, we still need to provide some level of service.
I encourage you to dedicate a big part of your time to failure handling, and to make it a habit to announce your systems are production-ready only when you handle your failures in a safe and business-oriented way.
As always, you’re welcome to find me at my Twitter handle: @cherkaskyb
Engineering, Data Science, Architecture, Scaling and more
112 
112 claps
112 
Software Engineering, Research, Data, Architecture, Scaling and more, written by our very own engineers and data scientists.
Written by
Software engineer, clean coder, scuba diver, and a big fan of a good laugh. @cherkaskyb on Twitter
Software Engineering, Research, Data, Architecture, Scaling and more, written by our very own engineers and data scientists.
"
https://medium.com/@gtsopour/kubernetes-ingress-aws-eks-cluster-with-aws-load-balancer-controller-cf49126f8221?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
George Tsopouridis
Apr 12, 2021·5 min read
Kubernetes Ingress is an API object that manages external access to the Services in a Kubernetes Cluster. Ingress exposes HTTP and HTTPS routes from outside the Cluster to Services within the Cluster. Traffic routing is controlled by rules defined on the Ingress resource.
Here is a simple example where an Ingress sends all its traffic to one Service:
An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a Load Balancer, though it may also configure your edge router or additional frontends to help handle the traffic.
Kubernetes Service is an abstract way to expose an application running on a set of Pods as a network service. For some parts of the application (for example Frontends, public API interfaces) you may need to expose a Service onto an external IP address, that’s outside of the Kubernetes Cluster.
Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.
Type values and their behaviours are:
ClusterIP, NodePort, LoadBalancer and ExternalName are all ways to get external traffic into the Cluster. Ingress is also used to expose a Service but it is not a Service type itself. It acts as the entry point for the Cluster. It lets you consolidate the routing rules into a single resource as it can expose multiple Services under the same IP address.
ClusterIP is the preferred option for internal Service access and uses an internal IP address to access the Service. Some examples of where ClusterIP might be the best option include Service debugging during development and testing, internal traffic and dashboards.
NodePort is a virtual machine (VM) used to expose a Service on a Static Port number. It’s primarily used for exposing Services in a non-production environment and it is not recommended for production.
LoadBalancer uses an external Load Balancer to expose Services to the Internet. You can use LoadBalancer in a production environment but Ingress is often preferred.
Ingress enables you to consolidate the traffic-routing rules into a single resource and runs as part of the Kubernetes Cluster. Some reasons Kubernetes Ingress is the preferred option for exposing a service in a production environment include the following:
In order for the Ingress resource to work, the Cluster must have an Ingress Controller running. If Kubernetes Ingress is the API object that provides routing rules to manage external access to the Services, Ingress Controller is the actual implementation of the Ingress API. The Ingress Controller is responsible for reading the Ingress Resource information and processing that data accordingly. The following is a sample Ingress Resource:
Looking deeper, the Ingress Controller is an application that runs in a Kubernetes Cluster and configures an HTTP Load Balancer according to the Ingress Resources. The Load Balancer can be a software Load Balancer running in the Cluster or a hardware or Cloud Load Balancer running externally. Different Load Balancers require different Ingress Controller implementations.
Ingress controllers in AWS use ALB to expose the Ingress Controller to outside traffic. They have added benefits such as advanced routing rules (e.g. path-based routing /service2) and consolidating Services to a single entry point for lower cost and centralized configuration.
The AWS Load Balancer Controller manages AWS Load Balancers for a Kubernetes Cluster. Please note that this controller was formerly named as AWS ALB Ingress Controller. The controller provisions:
The AWS Load Balancer Controller is a popular way to expose Kubernetes Services using Kubernetes ingress rules to create an ALB. The following diagram is from the original ALB Ingress Controller announcement to show benefits such as ingress path-based routing and the ability to route directly to pods in Kubernetes instead of relying on internal service IPs and kube-proxy.
In the AWS ALB Ingress Controller, prior to version 2.0, each Ingress object created in Kubernetes would get its own ALB. Customers wanted a way to lower their cost and duplicate configuration by sharing the same ALB for multiple Services and Namespaces and this feature is available with the AWS Load Balancer Controller https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html
By sharing an ALB, you can still use annotations for advanced routing but share a single load balancer for a team, or any combination of apps by specifying the alb.ingress.kubernetes.io/group.name annotation. All Services within the same group.name will use the same Load Balancer.
The Kubernetes Ingress API lets you expose your applications deployed in a Kubernetes Cluster to the Internet with routing rules into a single source. To implement Ingress, you need to configure an Ingress Controller in your Cluster as this is responsible for processing Ingress Resource information and allowing traffic based on the Ingress rules. In case of an AWS EKS Cluster, you can share an ALB with multiple Kubernetes Ingress rules by installing the latest AWS Load Balancer Controller and configuring the alb.ingress.kubernetes.io/group.name annotation.
Senior Software Architect / Engineer at Sunrise UPC, Zurich, CH - Github https://github.com/gtsopour
110 
110 
110 
Senior Software Architect / Engineer at Sunrise UPC, Zurich, CH - Github https://github.com/gtsopour
"
https://medium.com/@zhimin-wen/running-wild-container-image-on-icp-3-1-1-security-and-enforcement-19bf9e26a3d8?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zhimin Wen
Dec 9, 2018·5 min read
With the enterprise-grade Kubernetes’ release, IBM Cloud Private 3.1.1, running a docker image from any external resources has been tightly controlled.
By default, when a new Kubernetes object is deployed, ICP will validate the container images to make sure only those in the predefined whitelist are able to run. If not the container will be rejected to be scheduled. The error message is reflected in the output of kubectl/helm command line tool. For an example,
You can define the whitelist in the ICP console or define it by using the kubectl command line with a sample YAML file as below,
You use wildcard * to include the image repo in the whitelist as shown in the above list. By default, after the installation, only the ICP private registry images, IBM images from Bluemix registry and some of the docker hub images that are used by ICP are predefined in the whitelist.
Other than ClusterImagePolicy you can also define ImagePolicy which applies to the namespace scope. Its noticed once you define an ImagePolicy at the namespace level, all the whitelists of ClusterImagePolicy are ignored. You have to define all your own image whitelist in the namespace level ImagePolicy. (The error message shown above is actually when I defined an empty ImagePolicy at the default namespace)
There are more parameters for the ClusterImagePolicy/ImagePolicy, such as policy and Vulnerability Advisor settings. Refer to here for more details.
Before move on, let’s do a simple test to examine a container image running error.
Create a new namespace as exp with kubectl create ns exp followed by creating a simple deployment as below,
Apply it. Success. Everything seems fine. However,
CreateContainerConfigError ?! Let's find out more by running a describe,
So the pod is not able to run as root. This is related to the Pod Security Policy (PSP).
In the latest ICP 3.1.1, default Pod Security Policy is turned on as “restricted”.
If we list the existing PSPs, we have
When the ICP’s PSP is set as restricted, the PSP of ibm-restricted-psp is applied. List this specific PSP, we got
Notice that RUNASUSER is set as MustRunAsNonRoot. That's the reason why the busybox pod failed to run.
But how exactly, the PSP is applied to the Pod?
In the latest ICP, a ClusteRole ibm-restricted-clusterroleis defined. It uses the PSP ibm-restricted-psp The detail YAML of it is listed as below (the annotation field is deleted to save the space)
Next, ICP has a ClusterRoleBinding named as ibm-restricted-psp-users It binds the service accounts to the clusterrole ibm-restricted-clusterrole
Now it’s clear why the busybox container cannot be run. When the deployment is deployed to the namespace exp, the default service account is used, in which the cluster role of ibm-restricted-clusterrolis bound, and the associated PSP of ibm-restricted-pspblocks the root user execution in the container.
Understanding the reason, the fix is easy.
Apply the following yaml file for ClusterRoleBinding.
Delete the old pods, then you will see the pod is running,
2. Instead of patching the default account, create a new service account and bind the clusterrole of ibm-anyuid-clusterroleto it. In the deployment, use the new service account to run the container.
This approach is more specific to the Deployment. It will not lose the security control on the other pods in the same namespace.
3. Fundamentally fix the root user usage in the Dockerfile.
This is the most secure approach. But sometimes you may not have control over the container images.
As ICP has turned on the tighter control on the container images, if there is a business need to run some external containers, careful planning is required. You will require to
Cloud explorer
121 
121 
121 
Cloud explorer
"
https://towardsdatascience.com/sql-complexity-and-8-different-ways-to-join-tables-22ed7ae0060c?source=search_post---------158,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mengyong Lee
Oct 20, 2019·4 min read
Recently while working on Salesforce Marketing Cloud platform, a colleague and I came across an SQL query written by an external vendor in a form that we were rather unfamiliar with. It goes like this:
This query performed very badly on Salesforce Marketing Cloud with very long run time and frequent timeouts, giving us much frustration. Unlike Google Bigquery which we are familiar with being a fully managed petabyte-scale data platform, Salesforce Marketing Cloud has a much lower computational power and cannot process the load of a complex query.
Notice that in the inefficient query, two of its sub-queries referenced the main table in the where table.
This prompted an intellectual curiosity within the team; `How many ways can we write an SQL query and what is the time complexity of each query?`.
We decided to replay an experiment based on a 1988 article by Fabian Pascal to compare the performance of different ways of writing joins in an SQL query. This will help in building our intuition when troubleshooting , editing someone’s query, and understanding the time complexity of SQL, and allow us to write better SQL.
Presenting ‘8 ways to join tables in SQL’ with added in time complexity from Google Bigquery, hope you enjoy this piece of work!
TLDR: The most efficient join is also the simplest join, ‘Relational Algebra’. If you wish to find out more on all the methods of joins, read further.
Method 1: Relational Algebra
Relational algebra is the most common way of writing a query and also the most natural way to do so. The code is clean, easy to troubleshoot, and unsurprisingly, it is also the most efficient way to join two tables.
Method 2: Uncorrelated Subquery
The uncorrelated subquery method executes the filter function by first creating a subquery list of account_number, followed by an IN function to filter account number in the subquery. Although efficiency is not as good as Relational Algebra, it is one of the more commonly used join method due to the ease of writing.
Method 3: Correlated Subquery
In the Correlated Subquery, the EXISTS function is used to search in an unfiltered subquery `SELECT *`. The filter operation in the subquery requires a `where mp.account_number = t.account_number`. This is one of the join function used in the inefficient query. The performance of this join is agonizing.
Method 4: Scalar Subquery in the WHERE clause
By using a subquery as a filter on the WHERE function, the query is able to filter f_online = 1. Quite a cool way of thinking but unfortunately it doesn’t perform well.
Method 5: Scalar Subquery in the SELECT clause
Another really interesting way of writing a query, this method uses a subquery in the SELECT function to extract the account_number from another table, but as the two tables have a many to many relation, we have to add in a filter to remove the nulls.
Method 6: Aggregate function to check existence
Similar to Scalar Subquery, this method uses a subquery in the WHERE function. The difference is that this method uses a subquery COUNT(*) with a filter of >1.​
Method 7: Correlated Subquery (double negative)
Similar to Correlated Subquery, but uses a double negative. This is also one of the join used in the inefficient query . But interestingly, it did not perform as badly as I expected. This could be simply due to the data structure where there are more exceptions than inclusions.
Method 8: Uncorrelated Subquery (double negative)
Similar to Uncorrelated subquery but uses a double negative.
In conclusion, the way we write our SQL query does have a big impact on the efficiency of the query. The most efficient query runs for 13.8 seconds, compared to the least efficient query runtime of 52.6 seconds. We re-wrote the inefficient query by the external vendor on Salesforce Marketing Cloud and we were able to reduce the run time from 45 mins coupled with frequent timeouts, to 4 mins. After that, we went for a nice cold beer and celebrated our victory.
Bonus
Queries don’t start from `SELECT` but `FROM`. Now we know why LIMIT doesn’t reduce compute cost. Source
Reference*1988 article by Fabian Pascal “SQL Redundancy and DBMS Performance” in the journal Database Programming & Design — http://www.dbdebunk.com/2013/02/language-redundancy-and-dbms.html*Day 4: The Twelve Days of SQL: The way you write your query matters — https://iggyfernandez.wordpress.com/2011/12/04/day-4-the-twelve-days-of-sql-there-way-you-write-your-query-matters/
Thanks Daniel for the inspiration behind this article and Shawn for the the bonus material on SQL sequence. I am blessed with cool colleagues like you guys.
And thanks George for sharing Fabian Pascal’s 1988 experiment ;)
Cheers,
Mengyong
Marketing Science, Master Degree in AI
67 
1
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
67 claps
67 
1
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-native-the-gathering/istio-ingress-to-expose-your-k8s-services-via-individual-dns-2ec9c2717b81?source=search_post---------159,"There are currently no responses for this story.
Be the first to respond.
Istio ingress provides external access to your mesh. You can follow official documentation to find you $INGRESS_HOST:$INGRESS_PORT combination. It works perfect, but what if the service client knows nothing about the mesh implementation? All he needs is to use a unique hostname to reach a specific service.
For example, you have myapp service running in myns namespace. The goal is to expose this service outside the K8s cluster with its unique DNS name from your domain. Let’s assume that you are the owner of mycompany.com domain. In this case, the exposed myapp service URL would be:
https://myapp-myns.istio-gw.mycompany.com
Let’s follow the steps on how we can achieve this in AWS cloud by configuring:
You can follow this guide to issue certificates or ask your security team to provide you ones.
We’re looking for a ‘*’ wildcard certificate in your domain to match all the service endpoints
In this demo, we’ll use *.istio-gw.mycompany.com certificate and the guide above:
We’ll continue with TLS but you can also use mTLS instead
Create a Gateway object that expects connections to *.istio-gateway.aeg.cloud hosts
Create the AWS Load Balancer and configure the listener 443 port. Use K8s minions as target hosts and 31390 port (default Istio ingress TLS port)
Navigate to Route 53 page in AWS
NOTE: mycompany.com hosted zone must be added to AWS
Choose mycompany.com hosted zone and create a Record Set
Configure the Record Set with wildcard name and add ELB/NLB DNS name from step 4. to Alias Target
In this step, we’ll configure the mapping between the external resolvable DNS name
This VirtualService will listen for https://myapp-myns.istio-gw.mycompany.com:443 requests and convert them to https://myapp.myns.svc.cluster.local:8443 internal cluster calls
Use REST client test tool for testing
A gathering place for all things cloud native…
36 
36 claps
36 
Written by
Software Engineer
A gathering place for all things cloud native, microservices, api development
Written by
Software Engineer
A gathering place for all things cloud native, microservices, api development
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ibm-watson/chatting-with-watson-to-hook-any-tweets-webhook-tutorial-bf0fac67d604?source=search_post---------160,"Sign in
There are currently no responses for this story.
Be the first to respond.
Erika Agostinelli
Aug 29, 2019·6 min read
In this tutorial you will learn how to make external API callouts from Watson Assistant. We will use Webhooks, Cloud Function, and Twitter API, to retrieve the last tweets of any account.
How can you personalise your assistant and improve it by adding new functionality? How easy it is to make external API calls from Watson Assistant? With Webhooks is really easy.
In this tutorial, we will use the new webhooks feature from Watson Assistant, and cloud functions (FaaS) to achieve this goal and you will be able to ask your virtual assistant to retrieve the last tweets from any Twitter account.
Code for this tutorial is available on Github
Let’s focus on the Cloud Function, first. Go to your IBM Cloud account click on “create resource” (top right) and search for “functions” and select the result. Then, click on the “Actions” tab in the left banner, and create e new action (Create → Create Action). Note: On the top right, be sure to pick a CF-based namespace and not a new IAM-based namespace, since currently (last update oct-2019) you can make a call to an action that is managed by Cloud Foundry, but not to an action that uses token-based Identity and Access Management (IAM) authentication. Moreover, you should select the same region where your Watson Assistant instance sits.
Be sure to work on a CF-Based namespace and the Cloud Function and Watson Assistant instance need to be in the same region.
Pick a name for your function (e.g. “last_tweets”) and runtime (in this case I have picked Python 3) then click on “Create”. You can find the code to copy & paste here but make sure to add your Twitter Dev Account credentials in the appropriate fields (client_key , client_secret) and hit Save:
Now, click on “Parameters” (left banner) and add a variable called “account”. This parameter will receive information from your Assistant when the user explicitly asks for tweets of a specific account. Here, I’m leaving a default value (@taylorswift13) so that I can test my function.
Let’s check if everything is working fine: go back to the “Code” tab (on the left) and invoke your function. You should be able to get the last tweets from Miss Swift (Default Value for now).
Everything working? If yes, next step:
Since you have completed your cloud function, click on “Endpoints” and take notes of:
Now, you are ready to move to Watson Assistant (WA). Download the skill in json format and upload it into your Watson Assistant instance (how to upload a skill). Again, be sure that your instance is in the same region as the cloud function.
A Webhook sends a POST request callout to an external application that performs a programmatic function. When used in a dialog skill, a webhook is triggered when the assistant processes a node that has a webhook enabled.
How to use a webhook:
Click on “Option” tab -> “Webhooks”:
Go to the “Dialog” tab: click on “Twitter — Last tweets” node and see its children nodes. If the user specifies a twitter account then the first child node will be triggered (“Account Specified”) and in that node, Webhook functionality has been indeed activated. How to do it? It’s really simple: you need to click on the “customise” button, scroll down to the Webhooks section, and switch the toggle to On, and then click Apply (how to add a webhook in a node).
In “Account Specified” node (where Webhook option is active) you can see several new fields.
Once you have created your cloud function, and correctly set up the webhook in the Options tab, the Watson Assistant Skill in the git repo already contains all the intent, entity and dialogue structure that allows you to play directly in the Try it out panel (top right). Try typing: “tweets from @blackmirror” (note the @ symbol) you should see displayed text and some images (if tweeted by the account).
Now you should be able to create your own cloud function, use webhooks in Watson Assistant and ask your virtual agent to show the latest tweets of your favourite artist, event or TV show etc!
Code for this tutorial is available on Github
Data Scientist — IBM Data Science & AI Elite Team
26 
2
26 
26 
2
Watson Assistant is a conversational AI platform that’s easy to use. It’s designed for non-technical builders who want to help answer questions and help their customer get stuff done.
"
https://towardsdatascience.com/aws-lambda-integration-with-snowflake-426debc9ec3a?source=search_post---------161,"Sign in
There are currently no responses for this story.
Be the first to respond.
Amit Singh Rathore
Aug 1, 2021·3 min read
Recently I and Supreeth Chandrashekar collaborated on a task, where we wanted to run multiple inferences on some of data stored in Snowflake. We leveraged external function to trigger lambda which in turn got the inference from AWS Sagemaker endpoints(serving different models). In this blog, we will discuss how to invoke lambda from Snowflake.
Just create an empty role with no permission. We will add the cross-account trust later.
Create a lambda function, which will respond to the API gateway. Use the following code. The important thing to note here is event structure and response structure.
Snowflake send the event in the form of list of list in JSON format wrapped in key called data. And it expects the response object to contain an statusCode key and a data key with JSON value.
Create an API gateway with a POST method having lambda proxy integration. Use the Lambda ARN we created in the previous step. In “Method Request” add IAM Authorization.
After replacing the relevant fields in the following JSON add the same resource policy for the API.
Finally, in the lambda console, you should observe API triggered Lambda function.
We create an API integration in Snowflake. This integration will create a user and allow that user to assume the role we created in AWS setup step 1. Also, it records the execution endpoint of the API we created in the API gateway.
We create an external function by using the integration we created in the previous step. Use the following code for integration and external function creation.
Once Integration is created. Describe the integration. You will result similar to the below one.
Note down the API_AWS_IAM_USER_ARN & API_AWS_EXTERNAL_ID from the output of the describe integration command. Replace it in the following policy and then add this as trust policy in the “Assume role” created at the start.
To test the end-to-end functionality run the function in snowflake and you should see the response from lambda.
Adding functionality to execute python code from within the snowflake provides greater integration between snowflake and cloud system. We can plug in these functions and trigger the external process from within snowflake.
Note: there is a limit of 6MB payload on lambda, so the trigger from Snowflake should not be too big.
Happy Integrating!!
Cloud | ML | Big Data
See all (71)
132 
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
132 claps
132 
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://blog.openpolicyagent.org/open-policy-agent-v0-19-release-921d49179440?source=search_post---------162,"Last week we released OPA v0.19, containing 63 commits from 12 contributors (of which, 9 were external.) This release includes many important fixes and enhancements, as well as a new Rego parser written in Go that speeds up parsing time by ~100x in most cases. You can find more details on the GitHub releases page.
Since many in-person events have gone virtual due to the COVID-19 crisis, there have been several virtual events, webinars and podcasts featuring OPA over the last few weeks. Here’s a quick roundup:
Since KubeCon 2019 in Barcelona, we have asked users to post Q&A style queries on Stack Overflow instead of slack.openpolicyagent.org. The reason is that most answers posted on Slack are not discoverable! If you have Q&A style questions (e.g., “How to test `not deny`?”), try posting on Stack Overflow and tagging with open-policy-agent.
The largest change in v0.19 is the new Rego parser, which is written from scratch in Go. Previously, OPA relied on a generated parser that was defined using PEG (Parsing Expression Grammar [wikipedia]). Over the years, as the grammar has grown, and larger inputs have been thrown at it, the generated parser became a bottleneck (e.g., it could take about 10x longer to parse an input than compile and evaluate the query.
Inside OPA we were able to workaround the performance problems with caching, using Go’s “encoding/json” package and manually converting to AST (“Abstract Syntax Tree”) values when possible, etc. However, new users embedding OPA as a library would (understandably) make mistakes and wonder why performance was poor. The majority of the performance problems in the generated parser were due to a significant amount of heap allocations required to parse any input.
In addition to performance, we also struggled with usability around parser error messages. If the parser was not able to match an input, you would be presented with an error like “policy.rego:19: no match found”. No match? Tell me more!
Rather than attempt to continue incrementally improving the existing parser, we decided to rewrite it from scratch in Go. The result is a new parser that allocates significantly less memory (which improves performance by approximately 100x in most cases) and has better error messages. One important requirement for the new parser was backwards compatibility — the new parser could not break existing policies OR programs that embed OPA as a library (e.g., the parser APIs and the AST types also had to remain the same). To ensure we did not break existing (valid) policies, we checked for differences in the output of the old and new parser for hundreds of thousands of Rego snippets (which deserves another blog post in the future.) Lastly, we also applied the wonderful go-fuzz project to the parser to help catch crashes and other bugs.
> Since we no longer have a declarative representation of the language grammar in Go, please refer to the ENBF grammar in the OPA documentation as the authoritative source.
The chart below shows the difference in performance between the old (v0.18 and earlier) and new (v0.19 and later) parser (log scale):
Overall, we are happy with the process. In the future we plan to continue optimizing performance in the parser and looking for ways to improve error messaging and usability.
In addition to the new parser, v0.19 includes dozens of bugfixes and feature enhancements. @olivierlemasle contributed code to generate OPA man pages from the OPA CLI definitions. The `man` pages are automatically available if you brew install opa:
@jpeach submitted a number of patches that improve testing and support for the http.send built-in function. For example, policies can now explicitly set TLS server names as well as certificates and keys when invoking the built-in function (previously they could only come from the environment or local files). This is useful if you want to specify those values in data or as local variables inside the policy itself.
Lastly, the release also includes a pointer to the new rego-mode Emacs package developed by @psibi. The package provides syntax highlighting, formatting and more. In the future, the package could be extended to support many of the same features as the OPA extension for VS Code.
At KubeCon 2019 in San Diego we announced support for compiling OPA policies into WebAssembly (Wasm). Wasm enables OPA policies to execute in new environments like CDNs, service proxies and more without requiring an out-of-process RPC call to query OPA.
This week we are excited to release further support for Wasm in OPA with the new golang-opa-wasm project! This project wraps the wasmerio/go-ext-wasm runtime library to provide convenient APIs for policy execution and more. The golang-opa-wasm SDK is still work-in-progress but feedback and contributions are welcome.
The Open Policy Agent project blog.
62 
62 claps
62 
Written by
Software engineer and builder. Co-creator of the Open Policy Agent (OPA) project. VP of Open Source at Styra. https://www.styra.com/
The Open Policy Agent project blog.
Written by
Software engineer and builder. Co-creator of the Open Policy Agent (OPA) project. VP of Open Source at Styra. https://www.styra.com/
The Open Policy Agent project blog.
"
https://medium.com/@rishavverma/whitelist-ips-on-gcp-kubernetes-ingress-4f7c6f4eb0f?source=search_post---------163,"Sign in
There are currently no responses for this story.
Be the first to respond.
#rishavnotes
Sep 28, 2018·1 min read
What ? — Allow traffic from specified external public IP address(es) into a Kubernetes cluster on Google Cloud Platform (GCP).
Why? — Security !
How ? — GCP’s firewall rules cannot be applied on the Global Load Balancer it attaches with an Ingress that is created on GKE. If you want to restrict access to only specific IP addresses (for example : users connecting via VPN, in this case the VPN gateway’s IP address) then there is no out of the box solution on GCP, especially GKE.
Nginx and Http header “x-forwarded-for” to the rescue
If you are using GKE, chances are that you have a Microservices architecture and you are using an API Gateway, chances are that Nginx is the API Gateway. All that needs to be done is to configure nginx to only allow requests that have the following IPs
user.ext.static.ip → Public IP of the client
app.global.static.ip → Global static IP assigned to Ingress
The http header extraction and filtering logic is written in Lua. Here’s what the nginx.conf looks like
And the validate ip lua script
The lua script requires the cjson library
23 
1
23 claps
23 
1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hashmapinc/snowflake-azure-storage-integration-b93f8264ea28?source=search_post---------164,"There are currently no responses for this story.
Be the first to respond.
We have all been guilty of generating SAS tokens and using them to access cloud resources. I am especially guilty of this for my personal projects when I just want things to “work”. However, these sorts of things would never fly in a production environment. Can you imagine a team of 20 developers? They each want access to a different resource within a blob container. If one of them wanted to create an External Stage, each developer would have to generate a SAS token, and then you have problems. Who generated what? What if someone saves the SAS token locally? What if, god forbid, it gets uploaded to a public-facing Github repo?
Here at Hashmap, we work with clients every day who not only need better practices to handle their development but also on how to control their security. When we see an anti-pattern develop, we try to correct and educate on better practices. Data Engineering is about learning together.
So how does the Storage Integration fix our issue? In a nutshell, a Storage Integration is a configurable object that lives inside Snowflake. Once configured, you can use it to create an External Stage without having to input the SAS token every time. Instead, you just call the Storage Integration and you have all of your secrets locked away behind the Storage Integration. Here is how you would create a storage integration inside of Snowflake:
Here you can see I am setting what type of storage I am using by the type parameters, storage_provider here is Azure. We must also provide the tenant id where the service is going to check the Azure IAM role assigned to the Snowflake instance. Finally, we provision which parts of the blob store we would like the integration to access. We can also block certain areas from being accessed as well. This allows for greater control over the areas that an external stage can be created.
Once this is run, Snowflake will create a service principal in your Azure account that you can give reader access. After this is done, all you have to do is grant usage to other roles in Snowflake.
NB: This must be done from the Account Admin level as this is the only role allowed to create and give permissions to other roles within Snowflake using Storage Integrations.
Now we can finally create the external stage; here is the code to create the external stage using the storage integration:
That’s it, folks! The advantages of creating an external stage with storage integration in Snowflake are hard to understate. Developers are no longer responsible for generating a SAS token for each external stage they create.
At Hashmap, we work with our clients to build better, together.
If you’d like additional assistance in this area, Hashmap offers a range of enablement workshops and consulting service packages as part of our consulting service offerings, and would be glad to work through your specifics in this area.
How does Snowflake compare to other data platforms? Our technical experts have implemented over 250 cloud/data projects in the last 3 years and conducted unbiased, detailed analyses across 34 business and technical dimensions, ranking each cloud data platform.
www.hashmapinc.com
www.hashmapinc.com
medium.com
medium.com
Kieran Healey is a Cloud and Data Engineer with Hashmap providing Data, Cloud, IoT, and AI/ML solutions and consulting expertise across industries with a group of innovative technologists and domain experts accelerating high-value business outcomes for our customers.
Innovative technologists and domain experts helping…
12 
1
12 claps
12 
1
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Data Engineer Love blogging about new technologies and sharing simple tutorials to explain the tech.
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
"
https://medium.com/@rspraneethkumar/automating-the-data-movement-from-gcs-using-data-transfer-service-e156b851a9bb?source=search_post---------165,"Sign in
There are currently no responses for this story.
Be the first to respond.
SP Kumar Rachumallu
Mar 4, 2021·4 min read
BigQuery Data Transfer Service is an offering by GCP that automates the movement of data from different external sources into BigQuery under given conditions on periodic basis.
Currently DTS supports few Google SaaS apps such as Campaign Manager, Google Ads etc., Amazon S3 (Cloud storage), and couple of data warehouses such as Teradata and Amazon Redshift. You can also automate data load from Salesforce CRM and Adobe Analytics using third party transfers for DTS available under Google Cloud Marketplace.
Like any other GCP service, DTS can be initiated from Cloud Console, CLI, or an API. On configuring the data transfer, data will be automatically loaded into BigQuery as per the schedule.
To use this service and create transfers you should be grated bigquery.admin role. Under APIs & Services console, ensure you enable BigQuery API as well as BigQuery Data Transfer Service.
In this example, Let us assume we have a cloud storage bucket with URI gs://bucket-dts-bq/ which has several csv files and there’s a destination table created in BigQuery as dtstransfer.salesdata.
Navigate to BigQuery page, and click Data Transfers under the left pane. At this stage I assume you have already enabled APIs and Services mentioned above.
On create transfer page, you would need to make selections required for your data transfer.
Imagine there are problems with source data, or there is an outage at the source. You can initiate data backfills. This process is called Refreshing and you can initiate it from your data transfer console. Please note this option is enabled only for runtime parameterized transfer configurations.
Parameterization is process of setting up runtime parameters in order to load data by run_time. Using runtime parameters, you can specify how you want to partition the destination table. You can easily retrieve files by matching a particular run_time.
For example, salesdata$20210403 (Table_name$Run_Time) is the file loaded into partitioned BQ table from GCS at specific run time.
The above assumptions apply here as well.
Lead Programmer at Novartis Healthcare Pvt Ltd.
7 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
7 claps
7 
Lead Programmer at Novartis Healthcare Pvt Ltd.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cranecloud/how-i-am-using-axios-api-e4d16ab430db?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
Axios is an external library whose basis is Javascript Promises that is used to make HTTP calls. Previously we all used the “in-house” Javascript Fetch API but Axios has more to offer than the Javascript Fetch API; at least according to my experience.
Axios Interceptors are simply methods that will get called before the main method is called and the main methods are of 2 types the requests and responses. So we have request interceptors and response interceptors. What is commonly used are response interceptors that are usually used to handle data returned from an API call.
Previously I had a codebase and this is how part of my code looked like:
The code above is for an API call to fetch a user’s books. Notice that the API URL is manually added, and so is the token that is required for the call. With time, as the application grew bigger, I realized more API call functions were literally the same as the one above.
Before we talk about interceptors, I wanted to have a file where most of this could get handled, so I created a file I named “axios.js” in the root directory of my project. This is how that file looks like:
You realize that solves the problem of having to always add them in any further new API functions.
You can add interceptors to this and for my case, I added a response interceptor which I created for my 401 and 403 errors but the example below is more generic.
With the above, that is all I have for my Axios file. So back to the API call function we began with. I no longer import the Axios library into the file but make use of my Axios instance by importing from the “axios.js” I created.
The function we began with thus changes to the code below:
With this final code, you can see that less code is required as some key things are handled in the Axios file I created and I do not need to keep replicating them here or in further new API call functions.
Managed Cloud Service
109 
109 claps
109 
Written by
Greatest — Coder Ever, Father to a Princess.
Automated application deployment, scaling and management.
Written by
Greatest — Coder Ever, Father to a Princess.
Automated application deployment, scaling and management.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/effective-debugging-with-net-2ed70167de58?source=search_post---------167,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Most of the projects I work with today have external dependencies of some sort. This could be a database, a cloud provider, or just another API that I need to talk to. It’s rare that I don’t have to communicate with something over a network in order to develop my code. Being able to effectively develop and debug against…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ibm-cloud/assessing-security-risk-of-your-containers-with-vulnerability-advisor-f6e45fff82ef?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
Vulnerability Advisor (VA) checks the security status of container images on your private registry and external registry, as well as the containers running on your cluster.
Vulnerability assessment report includes the list of packages which are affected by known vulnerabilities. By checking this report, you can take action to remediate issues by updating, reconfiguring, and disabling discovered vulnerable packages. When a user has hundreds of images and containers, the remediation task can be a big task. Some vulnerabilities may be quite severe while others are relatively less impactful. Priority must be given to remediate vulnerabilities with higher risk.
In this post, I will introduce the risk analysis capability with an example report for one of my images deployed on the IBM Cloud Private cluster.
VA user interface displays a clear, quantitative risk assessment of container images, and running containers that are based on well-established industry standards for you. The risk analysis view helps you understand the severity of each vulnerability and the security risk of each container, which helps prioritize the remediation actions.
You can view a table list of the packages that were affected by discovered vulnerabilities, details of the CVSS Base Rating, and details of the CVSS Temporal Rating. The table list is ordered with respect to the risk level of each vulnerability, starting with the more critical items. View the quantitative risk assessment from the Risk Analysis tab:
The risk rating is the overall risk level of your container. Risk rating is determined by the following criteria:
CVSS Base Rating is the computation of the maximum CVSS Base Rating among all packages. The base rating includes the information of a vulnerability that are constant over time and across your environments. The CVSS Base Rating for a package is derived by the maximum CVSS Base Rating among all CVEs, which affects the package vulnerability.
For example, from the Risk Analysis diagram mentioned previously in this article, the CVE-2019–13638 is the maximum CVSS Base Rating among all the listed CVEs. The CVSS Base Rating is composed of the following metrics (specified in the CVSS specification):
CVSS Temporal Rating is the computation of the CVE, which gives the maximum CVSS Base Rating. The temporal base rating shares information of a vulnerability that might change over time, but not across your environments.
The CVSS Temporal Rating is composed of the following metrics:
The temporal rating score is available from the XForce Exchange API, which is provided from XForce Threat Intelligence Research in IBM. VA leverages IBM XForce Exchange to access detailed information on each CVE. XForce Exchange provides the actual CVSS scores and the details for each CVE ID.
Then, the CVSS Base Score and temporal score can be recomputed using the equations defined in the CVSS specification. VA updates the ratings and scores in the Risk Analytics table in VA report.
Base Score is derived by the equations from the CVE specification. View the Base Score equation examples when the scope metric is unchanged or changed, and how the temporal base score is calculated:
Equation when the scope metric is unchanged:
Equation when the scope metric is changed:
Temporal Score can be derived by the following equation:
Risk analytics capability in Vulnerability Advisor helps you understand the severity of each vulnerability and the security risk of each container, which helps prioritize the remediation actions. For more information, check Vulnerability Advisor guide.
Understand how to bring elastic runtimes to the Enterprise…
5 
Thanks to Mikela Dockery. 
5 claps
5 
Written by
Cloud Security Researcher, Senior Technical Staff Member, IBM Research
Understand how to bring elastic runtimes to the Enterprise with effective security and data protection at scale.
Written by
Cloud Security Researcher, Senior Technical Staff Member, IBM Research
Understand how to bring elastic runtimes to the Enterprise with effective security and data protection at scale.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@plazagonzalo/snowflake-snowpro-core-certification-stages-24be111e65ea?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gonzalo Fernandez Plaza
Sep 15, 2021·5 min read
Stages in Snowflake specify where data files are stored (staged) so that the data in the files can be loaded into a table. It is the location of where the files are before moving them to SnowFlake tables. There are two types of stages, internal and external…
"
https://medium.com/@er.ujjwalsaxena/point-clouds-and-mergers-b8e9c738f582?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ujjwal Saxena
Apr 4, 2018·5 min read
A point cloud is a set of data points in space. The sensors that generate point cloud, measure a large number of points on the external surfaces of objects around them. Point clouds are used for many purposes, like creating 3D CAD models, formetrology and quality inspection, and for a multitude of visualization, animation, rendering and mass customization applications.
“The sensors that generate point cloud” — what is this supposed to mean? Are there multiple sensors that generate a Point Cloud ?
Yes !! LIDARs are not the only ones. IR sensors, RADARS, SONARs, RGB-D Sensors, along with many others generate a Point cloud.
But how to visualize a point cloud ?
To get this, is easy. A point in xyz coordinate system is represented by 3 values and is the easiest entity to work with. You don’t have to worry about its shape, rotation, length etc. Only position and color are things that matter for computation. Imagine a set of such xyz values bunched together for each point.
Using individual, unrelated points is a key to point clouds usefulness, because points are objects that are easiest to handle large amount of. Also for complete newbies here, I’ll mention that each time an EM wave(be it from a RADAR or LIDAR) strikes an obstacles and reflects back it generates a single point in space relative the point of sensor source. However it’s worth mentioning that the point cloud shown above may not the result of a single scan. This is because all EM waves travel straight and anything lying in the shadow of an obstacle goes undetected.
So do I mean a car with a LIDAR mounted on it, in the middle of dense forest(I mean really dense) on a sharp blind turn cannot detect a truck coming from the other side ? Well you’re indeed a clever reader to ask me that. I never believed the forests to be safe anyways. Just the same as hills.
Is there a way to perceive an object from all sides ? Actually for a LIDAR mounted on a Car, there is no way. Otherwise, there is. And that is to simply get a point cloud generated from various perspectives. I mean if I scan a building from front and side, I have two point clouds now. However to merge them is a little tricky and an area of research too. These techniques are known as Registration Techniques for Aligning 3D Point Clouds
There are various ways for alignment, I’ll try to share the thought behind some.
5. There are also various online platforms that allow the alignment of point clouds like CloudCompare, pointclouds etc.
This can be really helpful for real world simulator designing but as I said earlier this is in no way directly helpful for a car with a mounted LIDAR or RADAR and it will still be turning on that blind forest turn with the same faith as before. Atleast for now.
good reads :
For all other articles by me, please visit: https://erujjwalsaxena.wordpress.com/
A learner in autonomous vehicle development, deep neural networks and computer perception at Infosys.
See all (31)
36 
36 claps
36 
A learner in autonomous vehicle development, deep neural networks and computer perception at Infosys.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@avraham.paz/research-labs-at-healthcare-providers-1e65da66fe80?source=search_post---------171,"Sign in
There are currently no responses for this story.
Be the first to respond.
Avi Paz
Aug 25, 2020·2 min read
With the tremendous development and growth of AI and Machine Learning opportunities, the desire for access to healthcare data is also greatly increasing.
In turn, healthcare providers have their own incentives in making their data accessible for external organizations. Most prominent considerations for that may be: operational-optimizations, cost reduction and data-monetization.
However, the process of making health data accessible to consumers might be challenging as well, as healthcare providers are keen to prevent any data exposure to non-authorized entities.
Solution:
Extracting and connecting health data from multiple sources is a challenging step. In many cases you need to overcome it before you can use the data for analytics, machine learning, and actionable intelligence. Some providers handle it with the FHIR standard and API server. You can read about Azure for Health offering here.
Once the datasets are ready, healthcare providers are thinking of secured ways of giving consumers access to the available datasets. The data must be contained, with no way to export it.
The following hybrid architecture had been implemented with several of our customers.
This hub-spoke architecture allows healthcare providers to provision Azure labs on-demand, secure their platform and to prevent data leakage outside their IT infrastructures.
Any ‘Lab’ (spoke) is in fact an isolated environment, hosting Compute and Storage resources on private vNet. All outside traffic is blocked with NSG. Copy and paste might be blocked as well with a specific policy.
That being said, some flexibility inside the lab is still possible through a Nexus server that enables the query of approved repositories. Another mechanism that might be considered is azure logic app and file-share sync to allow lab enrichments with external datasets.
Thanks,Avi.Paz@microsoft.comSenior Cloud Solution Architect — Data & AI
Cloud Solutions Architect at Microsoft | Data & AI Software Architect | Big Data Engineer | Full-stack Software Developer | Entrepreneur
2 
2 
2 
Cloud Solutions Architect at Microsoft | Data & AI Software Architect | Big Data Engineer | Full-stack Software Developer | Entrepreneur
"
https://medium.com/@prashanth9962/event-scheduler-using-gke-python-and-sqs-37d41e9b3235?source=search_post---------172,"Sign in
There are currently no responses for this story.
Be the first to respond.
prashanth k
Nov 21, 2018·1 min read
At KiSSFLOW, we need to run a lot of scheduled events such as sending webhooks to external systems at the specificed time, generating reports for customers. Our initial design started with Cron, but we ended building our own event scheduler. We are using microservices architecture and Google Kubernetes Engine for container orchestration.
The scheduler is built using python/tornado server, which queries the data store for available events in the specified time range (Eg: Events to be triggered in 10 min from 10.AM) and publishes messages to Amazon SQS Delayed Queue so that messages will be visible to task workers at the specified time. Scheduler service provides a API through which other services can create/update/delete events and manually trigger events.
In GKE, We have specified cron job that invokes Scheduler every 10 mins to push events to AWS SQS.
We can use distributed task workers which listens for events in SQS , run corresponding jobs.
Software Engineer @KiSSFLOW , Traveller, amateur photographer
44 
44 
44 
Software Engineer @KiSSFLOW , Traveller, amateur photographer
"
https://medium.com/google-cloud/enforce-tls-policy-on-gke-cluster-with-istio-or-anthos-ab46ee1fe038?source=search_post---------173,"There are currently no responses for this story.
Be the first to respond.
Photo by Jon Moore on Unsplash
Google Cloud provides an easy way to enforce TLS policies such as allowed version(s) of SSL and TLS protocols and cipher suites allowed to be used with these versions. It can be configured at HTTP(S) External Load Balancer. If your workload runs on GKE clusters that expose services in a way that does not use Google Load Balancers or the workloads are exposed internally (e.g. using HTTP(S) Internal Load Balancer) then you need another way to enforce TLS policies. The following tutorial shows how to configure it using Anthos Service Mesh (ASM) service. It is also possible to implement the tutorial using Istio OSS. However, ASM allows to apply TLS policies both to ingress and pod-to-pod traffic while Istio supports TLS policies for ingress traffic only.
The tutorial runs on GCP and requires a project with a valid billing account. The commands in this tutorial imply that there is an environment variable PROJECT_ID that stores the project id of that project.
NOTE: If you run this tutorial you will be billed for using GCP resources. Enabling some of the APIs may incur additional cost.
Use the following command to enable the required APIs:
If you use Istio OSS you only need compute.googleapis.com and container.googleapis.com APIs.
GKE clusters with ASM need at least e2-standard-4 machine types. The tutorial was not tested on the clusters composed of the smaller machines. Create a GKE cluster:
NOTE: No need to provision the cluster with --workload-pool parameter if you mean to use Istio OSS.
Enforcement of pod-to-pod TLS policy is supported starting with ASM 1.9. The following commands install the latest revision of the ASM 1.9:
Need to capture the ASM revision for later:
The tutorial uses an Online Boutique shop application that can be found on github. The following steps deploy the application into the dedicated namespace, configure ASM ingress gateway with proper certificates and then apply the TLS policies for ingress and pod-to-pod traffic.
Configure a Kubernetes namespace to run the demo application in it.
Enforce mTLS for all workloads deployed into the demo namespace:
The Online Boutique shop is a microservice application running on GKE and used for different demonstration purposes. This tutorial uses the latest version of the microservice container images.
To test the work of TLS you will need to have a valid domain name for the demo application. There are several ways to configure it. You can use Cloud DNS or another DNS service like one that your domain registrar provides. You can use a local /etc/hostname just to do tests. Either way, achieve the ASM gateway IP address:
And associate it with a hostname of your choice. For the following steps will use the hostname demo.acme.clouddemo.acme.cloud. Please, replace it with your selected hostname.
To enforce TLS policy for ingress traffic you have to configure ASM ingress gateway. To do this the ingress gateway has to be configured for the demo application hostname (see secure gateways for more details).
Edit a virtual service:
And replace all the '*'in the list of hosts: with the 'demo.acme.cloud'.
Edit the frontend gateway:
And replace all data under servers: with the following:
NOTE: Keep the indentation in-check to preserve YAML format of the manifest.
The tls section of the gateway manifest defines TLS policy as a range of TLS protocol versions and a set of cipher suites allowed for these versions. Note that according to the TLS 1.3 protocol specifications the cipher suites of the TLS 1.3 cannot be changed. So the set of cipher suites is applied to protocol versions ≤ TLS 1.2.
All steps until now can run with Istio OSS. The TLS policy enforcement for pod-to-pod communication is possible with ASM when mTLS is enabled. In contrast to TLS policy for the ingress traffic which can be enabled and configured per host basis, the TLS policy for pod-to-pod communication is enabled cluster-wise. For this, edit the ASM controller:
And add the following environment variables to the single container spec of the deployment:
ASM by default limits TLS protocol versions to 1.2 and 1.3 only. So, there is no need to limit the range of the TLS protocol versions.
It is possible to constraint which TLS protocol versions and which cipher suites will be used for ingress and pod-to-pod communication within GKE. The values can be configured using Istio OSS or ASM gateway for ingress and ASM controller for pod-to-pod communication.
Google Cloud community articles and blogs
3 
3 claps
3 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevRel Engineer in Google Cloud, specializing in Observability and Reliability. I try to whisper to horses in free time.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.cloudboost.io/javascript-went-up-the-beanstalk-3ba2688c2aee?source=search_post---------174,"Over here, at Artifact Uprising, we rely on a lot of external services to be able to deliver our awesome products to folks and make them happy. Internally, us in the engineering team, have been really good at monitoring our services and infra to the point that we can tell with a significant ‘heads up’ that something is about to happen and quickly move towards addressing it. Now, the same can’t be said about our external services. Those are beyond our control and require our eyes over emails, Twitter status messages, and others. Now, we do want to have some specs on and be able to see right away the notices from those APIs out there. So off to think of something simple and fast! Especially since we are about to hit our best time of the year.
The tool for this ad-hoc (read: hack) monitor was divided into three parts: some sort of quick code, the deployment, and the notification point. The notification center was easy. It was a simple choice to use Slack for this as it does a fantastic job at delivering goofy gifs, memes, and detect curse words in near real time. Nothing better to get something right away in front of eyes!
Our deployer was a place to try something new so I went with AWS Elastic Beanstalk. After some buzz from a dude named Andy E. and a bit of wait time to see what the interwebs had to say about it, Beanstalk was it. For the app language, I felt this was a great way for me to start working a bit more with NodeJS. Yep, a little bit late to the JS party but if you follow me, you can tell that I have other things that occupy my time.
This “nano” server listens to payloads from StatusPage. Most decent services use StatusPage and provide a subscription to a webhook listener. Effectively, this is what we built: a subscriber to StatusPage. Sadly though, NewRelic doesn’t provide such hook subscriber. I say sadly because recently they seem to be having some issues and I have to rely on email to read about them. Come on NewRelic, you are pretty awesome, let us subscribe!
This pathetically simple simple server’s function can be represented like this:
The codeAs stated, this hook listener is written in NodeJS. Make a folder for your project and save the files directly there. Copy paste this code and save it as server.js:
Create and save a package.json:
Now zip those two files into my_awesome_server.zip. Congrats!, you have a BeanStalk-ready package. We will move it in a sec!
The deployOur way to deploy it via BeanStalk was a little different due to some infrastructure items that we have which are beyond the purpose of this text. However, the quickest way to do it is like this via the aws cli:
You will now be prompted for information. You should have AWS creds set somewhere so I will not assume how you will fill up the information. However, select Node.JS as the project.
Once the init process completes, modify or create the following directory and file .elasticbeanstalk/config.yml with this content:
Finally, type the following command:
You are done. AWS will do its’ thing and give you back a URL that listens on port 80 and an AWS domain like: http://my-awesome-server.aws.amazon.com:80. This is the URL you will use when subscribing to webhooks from services. For more details, go here.
As soon as a service posts an issue to StatusPage, you will get it in Slack:
And this is another example of how we do things at Artifact Uprising Engineering!
The Realtime JavaScript Backend.
5 
1
5 claps
5 
1
Written by
DevOps for Artifact Uprising in downtown Denver, CO. https://about.me/jesusxuxogarcia.
The Realtime JavaScript Backend.
Written by
DevOps for Artifact Uprising in downtown Denver, CO. https://about.me/jesusxuxogarcia.
The Realtime JavaScript Backend.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@lisapark9/the-most-affordable-high-performance-diy-8tb-nas-774bcebd4d7f?source=search_post---------175,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lisa Park
Aug 1, 2017·3 min read
A ODROID forum user @linuxest reported the most affordable 8TB external storage could work with XU4 UAS driver in Kernel 4.9.I also wanted to make my own NAS because my Google Cloud storage was almost full.I ordered the same storage(STEB8000100) from Amazon. It is US$180 only. 1TB costs $22 approximately.
https://www.amazon.com/Seagate-Expansion-Desktop-External-STEB8000100/dp/B01HAPGEIE
It has arrived here this morning and I have run a few samba performance test with kernel 4.9 on my xu4q. I formatted the storage with EXT4 file system before testing because its stock file system is NTFS.
Downloading an 8GB file to my laptop from the XU4 NAS. It shows 110MB/sec of transfer speed stably.
Uploading an 8GB file to the XU4 NAS from my Windows laptop. It shows 90~100MB/sec of transfer speed.
Helios LanTest (3GB transfer option) also shows good performance too.
How to Assemble !
XU4Q and the official case
Velcro tape (3M Scotch 40mm x 25mm 4-pairs)
Attach Velcro tape (loop side) to the XU4Q case bottom
Attach Velcro tape (hook side) to the Seagate HDD case
Place the XU4Q case on the HDD case
Connect cables. DC plug for HDD, DC plug for XU4Q, Ethernet and USB 3.0 cables.
ODROID-XU4, NAS
Originally published at com.odroid.com.
5 
5 claps
5 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@deandrako/eagle-eye-networks-launches-camera-cyber-lockdown-d145df03c4ed?source=search_post---------176,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dean Drako
Feb 21, 2017·3 min read
Enhanced Eagle Eye Cloud Security Camera VMS now includes firewall to block cameras from external communication even if they have infections or trojans.
Austin, TX — February 21, 2017. Eagle Eye Networks, Inc. today announced it has added Camera Cyber Lockdown to all of its products. Eagle Eye Camera Cyber Lockdown blocks cameras from communicating with the Internet, stops them from being attacked and compromised, and will not allow any trojans which may have been implanted in the cameras to communicate with the Internet. This effectively makes all cameras secure from cyber attacks regardless of the quality of their software or the regular application of software updates. In addition, Eagle Eye Camera Cyber Lockdown can detect if an unwarranted communication attempt is made and inform appropriately.
Dean Drako CEO of Eagle Eye stated “Our goal at Eagle Eye Networks has always been to make the most cyber secure systems for video surveillance in the world. We work on security every day. We have worked hard to make our cloud services cyber secure and are happy that we can extend that se-curity all the way to the camera endpoints.”
Over the past few months, the number and size of attacks utilizing IOT devices like DVRs and cameras have received global attention. Security camera DVRs often come configured with telnet and web interfaces enabled, allowing users to configure the devices and view their security footage over the Internet. These security weaknesses make them vulnerable to attacks. Checking security cameras and DVRs to determine if they have been compromised is not a simple task.
“As an integrator we offer a variety of services and solutions to our customers to cover a wide range of applications from small business to enterprise organization,” said Rich Mellott, Director of Product Management at Stanley Security. “At the local level we do a lot of small commercial and industrial installations that are very price competitive and often times it requires us to leverage cameras that are not cyber hardened like our enterprise offerings. In those applications, the Eagle Eye technology allows us to provide a much more robust network solution with the camera cyber hardening to lockdown the security solution in an otherwise open and higher risk environment.”
The Eagle Eye Camera Cyber Lockdown separates the cameras from the Internet onto an isolated, protected network so they can’t be compromised or used maliciously. The Eagle Eye Camera Cyber Lockdown is implemented in the Bridge or Cloud Managed Video Recorder located on the premise. The Eagle Eye Cloud-Premise Flex Storage feature allow video to be stored in the cloud or on premise, but still be managed in a consistent fashion. Regardless of where the video is stored the Eagle Eye video is encrypted both in transit and at rest and the Camera Cyber Lockdown will block the attacks from and to the cameras.
To learn more about how Camera Cyber Lockdown will ensure your cameras are secure, click here to download our whitepaper.
About Eagle Eye Networks
Eagle Eye Networks, Inc. delivers the first on-demand cloud based security and operations video management system (VMS) providing both cloud and on-premise recording. Eagle Eye Networks also provides a cloud video API for integrations and application development. The Eagle Eye Platform offers secure, encrypted recording, camera management, mobile viewing and alerts, and first responder real-time video access — all 100% cloud managed. The Eagle Eye Cloud Security Camera Video Management System supports a broad array of IP and analog cameras while using Intelligent Bandwidth Management™, making it easy to deploy at single and multiple sites. The API platform uses the Eagle Eye Big Data Video Framework™, with time based data structures used for indexing, search, retrieval and analysis of the live and archived video. Eagle Eye Networks sells through authorized reseller and installation partners. The headquarters is at 4611 Bee Caves Rd, suite 200, Austin, Texas, 78746. For more information, please visit www.EagleEyeNetworks.com or call +1–512–473–0500.
This post was originally seen on https://www.eagleeyenetworks.com/
Parallel Entrepreneur. CEO of @EagleEyeCloud & @ICManage. Founder of @barracuda.
3 
3 
3 
Parallel Entrepreneur. CEO of @EagleEyeCloud & @ICManage. Founder of @barracuda.
"
https://aws.plainenglish.io/custom-metering-script-for-your-kubernetes-pods-925b4d2bad7?source=search_post---------177,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
How do you meter or get the duration for which a pod ran without using Prometheus or any other external tools? Use the in-built command “kubectl get events”.
I wrote this script due to a custom requirement that came my way and it works pretty well without much effort. There is not much to write about in this case, the script will work out of the box for any pod…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@wombat060152/ive-used-terabyte-image-for-windows-for-years-688ae526f49b?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Lee Andrew
Feb 20, 2020·1 min read
Kirk Evans
I’ve used Terabyte Image for Windows for years. It’s never failed & is now very easy to set up & use. The manual is also a good learning resource.
I run two W10 laptops. Both backup to external HDD every night. The software takes care of the full/differential schedule. When I’ve needed to restore, it’s been quick & easy.
I can therefore strongly recommend this product to anyone.
In relation to file backup: we all have access to free cloud storage, or very cheap if you need more. That too is frequent & automatic. There’s no reason for anyone to lose data now, barring an apocalypse (in which case it won’t matter).
Australian male born 1952, Adelaide. Anti-religious, socialist. Walk, think, inquire, learn; share ideas, music & pleasure.
1 
1 
1 
Australian male born 1952, Adelaide. Anti-religious, socialist. Walk, think, inquire, learn; share ideas, music & pleasure.
"
https://medium.com/@totalcloudio/list-of-essential-kubernetes-tools-3c83123a0e59?source=search_post---------179,NA
https://medium.com/@ringpeter/ingesting-fixed-width-text-files-with-snowflake-80fa1e3bbb30?source=search_post---------180,"Sign in
There are currently no responses for this story.
Be the first to respond.
Péter Ring
Aug 1, 2019·3 min read
Snowflake data warehouse offers many options for importing data into their platform. They support several external locations (local, AWS S3, Azure Blob Storage and GCS buckets), many file formats (CSV, JSON, PARQUET, XML, AVRO, ORC) and even different compression methods.
For our current use case, we are integrating several legacy systems data dumps into our cloud data warehouse. We get several extracts many times a day in our Azure blob storage. The files compressed in GZ and have a CSV format.
The recommended way to import data is to use a COPY INTO command which specify several things about the source file. You can encapsulate this information in a FILE_FORMAT definition, this help you to re-use your already declared metadata.
While exploring the files, we found out that besides the delimiters they also were in a fixed width format. A flat (or fixed width) file is a plain text file where each field value is the same width and padded with spaces. It is much easier to read than CSV files but takes up more space than CSV.  It is strange to have both file structure in the same file, because you only need one to import them correctly.
We started with the following FILE_FORMAT:
TRIM_SPACE is going to remove the unwanted trailing spaces from the fields and EMPTY_FIELD_AS_NULL will convert the empty fields to null. This is important because we wanted to cast the columns to their correct type while ingesting into the tables.
When we loaded several files, we faced the following error for one of our loads:
After checking the delimiter number for every row in our source file, it seems that they are not always equal, some of the rows have more delimiters. The legacy system sometimes gives an extra delimiter when there is no data present for the field. Because of this the width of the rows are still correct, but the delimiter structure is not.
Turns out Snowflake COPY parses the first line of the file using the defined field delimiter and uses that information for every row. When it finds a different character, it throws an error. This makes sense, because we expect to have the same number of delimiters for every row. It is also possible to get the following error, this means that the first line had more delimiter than the next.
Solution
Because of the structure violation we must change our approach. Changing the legacy system export is out of question. We can preprocess and validate the files, but it would add another complexity layer to our system. We can use the fixed width structure of the files, but Snowflake does not support it yet.
So, we had to create a workaround. We decided to load the files as they are into the tables as a single column and parse the information out later. To do so, we created a new FILE_FORMAT in the system:
We can use any field delimiter, we decided to use ÿ because it not common in our line of business. We are validating our data ingestion further in the pipeline.
Copying the data into our staging environment:
Because we know the width of the data we can use SUBSTR function to parse out the correct fields. We can also use TRIM, TRY_CAST to further
Transforming the data from staging to the business layer:
Conclusion
This workaround let us ingest data as it is and propagate the parsing logic into the SQL layer. The cost of it is processing power: The ingestion need ~20% more resources (substring, casting etc…) While Snowflake data warehouse is great, this additional ingestion option would improve further the system.
I am a data enthusiast with 8+ years experience as a Data Engineer / Database and Software Developer.
50 
50 
50 
I am a data enthusiast with 8+ years experience as a Data Engineer / Database and Software Developer.
"
https://medium.com/@MahindraComviva/what-are-managed-vas-services-18cb9f356129?source=search_post---------181,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mahindra Comviva
Aug 19, 2015·3 min read
Managed Value Added Services or VAS essentially refers to the business practice of partnering with an external vendor with a view to delegating a part of the IT operations to them to achieve greater efficiency in operations and cost by delegating certain tasks to a third party to perform instead of assigning those to internal personnel.
As per a CISCO report, the managed VAS services industry grew to an estimated $217 billion globally, a sharp 15 per cent CAGR rise from $110 billion in 2009. The upward trend is likely to gain further momentum as organizations increase spending on Managed VAS services to avail greater operational efficiencies.
Managed value added services are finding increasing favor among operators as they realize the business benefit of concentrating on their core business. It has been noticed that leading operators are following the wider global trend toward outsourcing in order to concentrate on their core business expertise in order to provide customers with a superior experience with the objective of expanding their customer base to include more customers. Choosing to partner with external vendors for managed VAS services has become a matter of strategic priority as operators find it much more profitable to focus their limited expertise and resources on building their core competencies with a view to gaining competitive edge over other players in the market.
One of the biggest advantages to the organizations from availing Managed VAS is that it frees its internal resources for working on development and growth of business critical areas. Another big benefit accruing to businesses is the fact it is able to access and leverage the expertise, technology and resources available with a specialist in the specific domain to be able to derive maximum capacity utilization, something it might not be able to achieve due to limited expertise and resources in that area.
The benefits of Managed VAS services for business in terms of financial gain can broadly be summed up as below:
With an estimated 20 per cent savings in OPEX and CAPEX, there is a very sound business case for choosing Managed VAS in functions that are not core to the business.
Partnering with a vendor for Managed VAS services does not in any way imply that the organization has to give over control and ownership of the equipment, application or service to the vendor. The organization retains full control and ownership over the asset and only delegates the day to day management of the asset to a 3rdparty vendor for business reasons. In fact one of the main reasons why Managed Value Added Services is slowly gaining ground over strategic outsourcing it that it enables organizations to gain access to highly specialized expertise without losing control over company owned IT asset or infrastructure. All of these make a sound case for the growth of Managed VAS services in times to come.
Originally published at blog.mahindracomviva.com on August 4, 2015.
Global leader in Mobile Finance, Mobile Money, Mobile Banking Solutions
Global leader in Mobile Finance, Mobile Money, Mobile Banking Solutions
"
https://medium.com/cloud-techies/useful-commands-solutions-49f1c1b4e033?source=search_post---------182,"Sign in
There are currently no responses for this story.
Be the first to respond.
Arun Kumar
Feb 5, 2021·3 min read
If you need a quick way to simulate / fake external server response, then shell2http is an easy way to simulate the response.
Example:
Reference:
"
https://medium.com/trivago-tech/circuit-breaker-with-aws-step-functions-b3a91518c990?source=search_post---------183,"There are currently no responses for this story.
Be the first to respond.
At trivago, we have several workflows which interact with external services. The health and availability of external services can have an impact on keeping our workflows alive and responsive. Think of an API call made to an external service which is down. Our workflows have to be prepared to expect these errors and adapt to it.
We have to be prepared to handle three specific anomalies in the behaviour of the external service.
When building distributed systems, we have to anticipate services going down. We also have to consider the general availability and responsiveness of these external services. A common and useful pattern to handle this is the Circuit Breaker. The idea is to put a component between your workflow and the external service call. When we detect the external service to have gone down, the component opens the circuit and all subsequent calls are paused. For every call to the service, a check is done against the component to see if the circuit is open or closed.
Think of the gatekeeper component as very similar to the electrical circuit breakers in buildings. However, in buildings, when the circuit breaker goes down, it needs an external intervention to reset things. But for software circuit breakers, we can have the breaker itself detect if the underlying calls are working again. We can do this by configuring a reset behaviour by an attempt to make the call the external service after a suitable interval.
We use AWS lambdas heavily as components in our pipelines. Natively lambdas are meant to be stateless, which makes building complex lambda to lambda workflows difficult. But with step functions, you can model and orchestrate workflows with lambda functions. Step functions enable you to define your workflow as a state machine.
In most cases, a single state in the state machine invokes a lambda function. But you can also incorporate branching logic, error handling, wait-states or even invoke multiple states in parallel. The state machine can be configured as a JSON document and deployed on AWS.
A feature we use a lot in step function is a combination of a lambda function state and error handling. With error handling, we specify which state can get executed next for a specific type of Error.
Think of a simple call to an external service which can throw several exceptions, and different control flows for each of them. As we progress further, we can refine the state machine below to add more capabilities.
When we are capable of distinguishing between errors, we gain the capability of handling them separately. Some of them can be retried directly. But some of them need action before a retry, like opening the circuit to prevent further calls.
If you have exhausted your quota of calls for a time period, the service can throw a throttling error. The best action here is to wait for a predefined interval and try again. You can do this using a wait state and looping back to the original state once the wait interval is complete.
You can directly specify the time to wait or it can infer this from the incoming message from the previous state.
If the service throws an unexpected error, it’s best to retry a few more time before giving up.
If no action needs to be taken other than a retry, this can be configured directly on a state retry block.
Especially in the case of a throttling error, you are absolutely sure no further calls will succeed for a while. Before the request enters the retry phase, the circuit has to be opened to avoid further calls. The HandleThrottledException state can open the circuit. We use an external state store to persist the state of the system. Before reaching the ServiceEntrypoint state which talks to the external service, we check the state store to see if the circuit is closed or open. If it’s open, the request is moved into a waiting queue. We use DynamoDB as the state store. AWS Elasticache or even an external Redis cluster can be used for this.
When the original request which keeps on retrying until it succeeds, it resets the flag in DynamoDB state store, thereby closing the circuit and allowing all the further calls to the service.
For calls which resulted in errors from the external service, even after multiple retries, it’s a good practice to move them into a Dead Letter Queue. We think it’s better than throwing away those requests as this allows us to have a look at those requests and try them again in the future. This also makes sure, if we end up having many errors due to a bug in the pipeline. We use SQS for maintaining the Dead Letter Queue, and periodically, this is inspected and evaluated and even retried if some conditions are met.
We feel it’s really helpful to be in total control of how your system interacts with external services. Especially, in the case of throttling errors, not handling it properly can lead to your pipelines throwing repeated errors. We continually improve our pipelines to make it more resilient and stable.
Originally published by Nikhil Kuriakose at tech.trivago.com on April 9, 2019.
With employees from all over the world, trivago is an…
With employees from all over the world, trivago is an international IT company operating on a very large scale to provide the best hotel search possible. This blog is the ideal place to talk about our ideas, our prototypes and our tech stack which make our vision become reality.
Written by
A diverse team of talents that make a blazing fast hotel search powered by cutting-edge tech and entrepreneurial innovation. Join us: trv.to/medium
With employees from all over the world, trivago is an international IT company operating on a very large scale to provide the best hotel search possible. This blog is the ideal place to talk about our ideas, our prototypes and our tech stack which make our vision become reality.
"
https://medium.com/excraftexchange/excrafts-unique-internal-architecture-4bbf245bbd56?source=search_post---------184,"There are currently no responses for this story.
Be the first to respond.
Our last article External Risk Management in ExCraft’s Architecture discussed how ExCraft analyzes and categorizes threats coming from outside of our exchange. The write-up previous to that in a section titled “What does ExCraft do differently” glossed over the internal makeup that makes ExCraft state-of-the-art. Before continuing, please review these posts on Medium if you have not read them already.
Beyond hacking, CEXs have suffered scalability issues, performance outages, and long delays in implementing new trading assets; this is because most exchanges were built hastily as a single service, meaning that there is less protection between different layers. The monolithic design of traditional CEXs result in poor performance that is difficult to scale and lacks security between operational strata. Minute exploits in a server’s code could permit an attacker entry to compromise an exchange built using this outline, which has happened repeatedly. Relying on a firewall around your exchange should not be sufficient enough to ensure user confidence. ExCraft believes each exchange service must be isolated, hardened, and fine-tuned.
Most existing exchanges are coupled to physical resources relying on specific technologies in colocation facilities and inherently lack scalability, as well as the ability to pivot into better solutions. Throughout this discourse, we will delve into four main areas of interest internally that not only allow ExCraft to manage risk better than major exchanges, but also make us the most scalable, hybrid exchange available today.
Ensuring protection of our users’ data and information is of the highest priority. ExCraft separates our internal network from the core functions of our exchange by using Google’s Compute Engine and Cloud Platform; this serves two purposes: It allows us to make use of all the additional tools that come with Google’s Cloud services and limits access to ExCraft operational staff departments. Despite many people’s opinion on Google, their services meet the highest levels of regulatory compliance across all Cloud technologies. Like ExCraft, they use independent auditing services to meet standards requirements in security and privacy including ISO 27001, ISO 27017, ISO 27018, SOC 2, and SOC 3.
ISO27001 — a security standard that outlines and provides the requirements for an information security management system (ISMS). It specifies a set of best practices and details a list of security controls concerning the management of information risks.
ISO27017 — gives guidelines for information security controls applicable to the provision and use of cloud services by providing:
Additional implementation guidance for relevant controls specified ISO27002
Additional controls with implementation guidance that specifically relate to cloud services
This standard provides controls and implementation guidance for both cloud service providers (like Google) and our cloud service customers.
ISO27018 — relates to the protection of personally identifiable information (PII), and as such, deals with one of the most critical components of the cloud — privacy. This standard is primarily focused on security controls for public-cloud service providers acting as PII processors. ISO 27018 works in two ways:
Builds off of existing ISO 27002 controls with specific items for cloud privacy
Provides completely new security controls for personal data
SOC 2 — evaluates an organization’s information systems relevant to security, availability, processing integrity, and confidentiality or privacy.
SOC 3 — a report based on the existing SysTrust and WebTrust principles.
In addition to having separated exchange and internal operation functions, ExCraft has added even more security through CloudFlare’s services; this gives us a robust firewall, equipped with packet inspection, intrusion detection, and other preventative systems. Even with separated resources and traditional site security measures in place, ExCraft still uses a designated monitoring and response team to ensure the exchange continues to run smoothly.
ExCraft is what we call a “Cloud-native” exchange. Within the ExCraft Cloud, we have separated core exchange functions into secure microservice containers that loosely couple Google’s Cloud. Docker containers with custom coded-solutions in several different computer languages (Go, Python, C++, and C- -) are connected using an Istio Service Mesh by Google Remote Procedural Call (gRPC) and orchestrated through Kubernetes. Each microservice scales with high availability, clustering to achieve low latency, and high throughput all at the lowest possible management costs
By deploying these containers as a Google Virtual Private Cloud (VPC), ExCraft attains greater scalability than existing CEXs. Google’s Compute Engine allows load balancing for resource distribution across several regions as well as smart autoscaling, which can divert spikes in traffic. Multiload balancing and smart autoscaling also tie into the preventative measures we have available against DoS attacks; this means ExCraft can handle over 10 million requests per second without any preparation (prewarming).
On top of scalability and throughput, Google’s VPC makes sure that data is stored securely with privacy safeguards, and that communication between our services, as well as users, are assured privately over the internet. All transmitted information not controlled by the Google platform requires encryption, authentication and is checked for integrity at one or more layers to guarantee that data sent reaches its destination unaltered.
Finally, Google’s Cloud services make it easier to set a Disaster Recovery plan based on recovery time objectives (RTO) and recovery point objectives (RPO), thus significantly reducing even more costs in comparison to traditional recovery plans. Non-traditional Disaster Recovery plans coupled with our DevOps team working around the clock on patches, upgrades, and new functionality (see our Roadmap) means that ExCraft has prepared for the worst case scenario.
Operations are the most crucial aspect of an exchange, which we have the most control over. If you remember the table with all of the lost funds due to exchange hacks and exploits mentioned in An Introduction to the Security Behind ExCraft, you would have noticed how many have succumbed to not having enough security measures built amongst their employees. For example, look at the number of susceptible cold wallet hacks; this is a critical operational problem. ExCraft keeps our multi-sig cold wallet with the minimum liquidity necessary (<5%) for trading while promoting highest safety. All measures have been taken to prevent these and any other funds from security breaches.
The ExCraft team has received mandatory training on security practices and handling personally identifiable information (PII) for out KYC processes. We take pride that our staff is aware of the most common type of types of phishing attempts such as fake Telegram groups, Twitter giveaways, and QR code malware. We have also taken measures to acquire as many domains similar to our official ExCraft.com website to prevent potential phishing attacks. ExCraft is very selective about the levels of access given to our team members. Mandatory 2FA, password complexity, rotation, lockouts, and audit reviews on all critical services are implemented across the board internally.
To ensure compliance, which we alluded to in our external risk management article, ExCraft will routinely go through scheduled and unscheduled audits with vulnerability testing and penetration testing. Key metrics and requirements will be automated as they are with most major exchanges. With these, we hope to achieve the highest levels of transparency with our community.
In promoting our financial security, customer funds are maintained separately from operational funds. Operational reserves are kept in full, meaning no borrowing. Rest well knowing we do not margin trade with our users’ funds.
We hope that this has been an educational and worthwhile read for our community. The ExCraft team wants to build trust with our growing community and highlight what makes us stand out from other major exchanges. Continue to follow us on our social media channels for more content like this and other updates, which will be coming out shortly!
ExCraft Exchange (Website)：https://www.excraft.com
ExCraft Telegram (English)：https://t.me/ExCraftExchangeENG
ExCraft Telegram (Chinese)：https://t.me/ExCraftExchangeCN
ExCraft Telegram (Korean)：https://t.me/ExCraftExchangeKR
Twitter|Facebook|Steemit|Reddit|Mastodon|Naver
ExCraft is a cloud-native cryptocurrency exchange based in…
ExCraft is a cloud-native cryptocurrency exchange based in Hong Kong that implements microservices to achieve a highly secure and high-performance architecture.
Written by
ExCraft is a cloud-native cryptocurrency exchange based in Hong Kong that implements microservices to achieve a highly secure and high-performance architecture.
ExCraft is a cloud-native cryptocurrency exchange based in Hong Kong that implements microservices to achieve a highly secure and high-performance architecture.
"
https://medium.com/@ApacheAPISIX/does-etcd-3-support-http-access-perfectly-b64996bf7429?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apache APISIX
Sep 18, 2021·4 min read
After etcd was upgraded to version 3.x, the protocol of its external API was switched from normal HTTP1 to gRPC. etcd proxied HTTP1 requests through gRPC-gateway to access the new gRPC API in the form of gRPC for those special groups that cannot use gRPC. (Since HTTP1 is too awkward to pronounce, the following is simplified to HTTP, which corresponds to gRPC. Please don’t get hung up on the fact that gRPC is also an HTTP request.)
When Apache APISIX started using etcd, we used the etcd v2 API, and since Apache APISIX version 2.0, we have upgraded our dependency on etcd to 3.x. Since there is no gRPC library in the Lua ecosystem, etcd’s HTTP compatibility has helped us a lot, so we don’t have to go through a lot of effort to patch This was a big help, so we didn’t have to go to a lot of trouble to fill in the gaps.
It has been 8 months since the release of Apache APISIX version 2.0 last October. In the course of practice, we have also discovered some issues with etcd’s HTTP API that interoperates with the gRPC API. In fact, having a gRPC-gateway does not mean that HTTP access is perfectly supported, there are some nuances here.
Just a few days ago, etcd released version v3.5.0. This release solves a problem that has been bothering us for a long time.
Unlike HTTP, gRPC limits the size of data that can be read in one request by default. This limit is called “MaxCallRecvMsgSize” and defaults to 4MiB. When Apache APISIX fully synchronizes etcd data, this limit can be triggered if configured enough and the error “grpc: received message larger than max”.
Miraculously, if you use etcdctl to access it, there is no problem at all. This is because this limit can be set dynamically when establishing a connection with the gRPC server. etcdctl sets this limit to a large integer, which is equivalent to removing this limit.
Since many users have encountered the same problem, we have discussed countermeasures.
One idea was to use incremental synchronization to simulate full synchronization, which has two drawbacks.
Another idea is to modify etcd. If you can remove the restrictions in etcdctl, why not treat gRPC-gateway the same way? The same change can be made to gRPC-gateway.
We’ve adopted the second option, and have given etcd a PR: PR #13077.
The latest release of v3.5.0 includes this change that we contributed. If you encounter “grpc: received message larger than max”, you may want to try this version. This change has also been back-ported to the 3.4 branch by the etcd developers, and the next release of the 3.4 branch will carry this change as well.
This incident also shows that gRPC-gateway is not foolproof. Even with it, there is no guarantee that HTTP access will have the same experience as gRPC access.
After Apache APISIX added support for etcd mTLS, some users reported that they have been unable to complete the checksum, while accessing with etcdctl was successful. After talking to the user, I decided to take his certificate and reproduce it.
During the replication process, I noticed this error in the etcd log:
The “bad certificate” error message looks at first glance like it is because we sent the wrong client certificate to etcd. But if you look closely, you will see that this error is reported inside the gRPC server.
The gRPC-gateway acts as a proxy inside etcd, turning outside HTTP requests into gRPC requests that the gRPC server can handle.
The general architecture is as follows:
Why does etcdctl connect directly to the gRPC server, but not with a gRPC-gateway in between?
It turns out that when etcd enables client-side certificate validation, a client-side certificate is required to connect to the gRPC server using the gRPC-gateway. Guess where this certificate comes from?
etcd uses the configured server-side certificate directly as the client-side certificate here.
A certificate that provides both authentication on the server side and identity on the client side doesn’t seem to be a problem. Unless server auth expansion is enabled on the certificate, but client auth is not enabled. Execute the following command on the faulty certificate:
You will see output like this:
Note the “TLS Web Server Authentication” here, if we change it to “TLS Web Server Authentication, TLS Web Client Authentication” or without this extension, there will be no problem.
There is also an issue about this problem on etcd’s repository: Issue #9785.
Although we have listed a few minor issues above, etcd’s support for HTTP access is still a very useful feature.
Thanks to the users of Apache APISIX, we have a large user base to find these details of etcd. As a large user of etcd, we will continue to communicate with the etcd developers for many years to come.
Apache APISIX is a Cloud-Native Microservices API Gateway
Apache APISIX is a Cloud-Native Microservices API Gateway
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@swu/how-should-we-back-up-our-important-files-6ac43c2e60af?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sunny Wu
Dec 30, 2015·2 min read
Just like a lot of people, I used to use an external hard drive to back up all my personal files, business files, photos, and songs. About 1 years ago, the external hard drive I was using failed and I couldn’t access my files. When I turned it on it made a terrible sound like scratching a needle on a disc. I took it to a local data recovery specialist and found out it would cost thousands of dollars to recover my data with no guarantee.
I stopped using hard drives to back up after that and started moving all of my data to cloud storage. As an IT professional, I can tell you that a cloud drive is way more reliable than the hard drive sitting on your desk or the one spinning in your laptop. Data centers maintain constant temperature and humidity (to reduce the defect rate of hard drives) and are redundant both geographically and within the same data center. This means in most cases if the specific hard drive storing your data crashes or breaks, you won’t lose their data. In fact, your access to the data won’t be interrupted at all and you wouldn’t even know something catastrophic happened.
Now I’m paying for cloud storage space. It is a good decision because, in addition to all of the reliability benefits, I can access my files anywhere and easily share with friends and colleagues.
But wait a minute.
Once I put my data in the cloud there are risks that others will be able to see my data. There have been countless stories in the news about data breaches happening, such as a large number of female celebrities had photos hacked and then leaked from their cloud accounts. How can I protect my privacy when I put all my data into the cloud? What happens if my account is hacked? What if I lose my password?
We need a product to help user’s protect their privacy and data in the cloud from prying eyes, and it should be an extremely easy-to-use product for everyone in our daily digital life. That product is OneSafe.
After you connect OneSafe with your favorite cloud storage accounts, and upload any files through OneSafe, your files will be encrypted with an encryption key known only to yourself. Your files will look like complete gibberish in the cloud to prying eyes, and no one will be able to decrypt and access your data. Not OneSafe, not your cloud storage providers, and not hackers.
Now with advanced cloud storage services like Dropbox, Google Drive or OneDrive, plus the extra layer of protection from OneSafe, backup is not something you should worry about anymore.
Serial Entrepreneur
Serial Entrepreneur
"
https://medium.com/@thomas_spicer/thanks-for-sharing-insights-f3b8701f54e8?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thomas Spicer
Jan 12, 2018·1 min read
Chris Merrick
Thanks for sharing insights. It can be really hard to build these types of system right. A lot of trial and error to be sure, plus the underlying models, architectures and technology is changing rapidly.
As you point out as more and more companies rely on external tools and systems data is becoming more difficult to reach. We touched on the gap that data pipelines need to solve here:
https://blog.openbridge.com/the-new-openbridge-data-pipeline-marketplace-say-goodbye-to-data-that-is-messy-complicated-scary-c0c2fecf243b
Looking forward to more posts on the topic.
https://www.openbridge.com
https://www.openbridge.com
"
https://medium.com/@nynexiii/week-5-everyones-data-d2cf4069c048?source=search_post---------188,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nicole
Sep 7, 2016·2 min read
http://www.scientificamerican.com/article/the-internet-has-become-the-external-hard-drive-for-our-memories/
It is quite amazing to witness the progression of data storage. From the very first punch card, to the latest USB sticks and hard drives that are able to hold terabytes of data, this development has been driven by the increasing demand from users to have more data, consume more data, and generate even more data.
The Cloud stores and transmits all our information through cables linking countries from all over the world. It has developed greatly, from the copper cables of the past, to the current fibre optics submarine cables that are now able to transmit a huge amount of data at high speeds across the world.
With the advent of social media sites and social communication applications like WhatsApp, there is an increasing demand and a need to record and share every facet of our lives on the Internet. This results in an incredible amount of data production, which of course leads to an increasing demand for data storage. Us as humans cannot possibly store all our information and memories in our brains alone, so we developed a reliance on the cloud to store all our information for us.
Let us take Facebook as an example. Many users upload photos, post short status updates about how they are feeling at the moment, others write long journals about topics that they feel strongly about, and use Facebook as an avenue to let these feelings out, while many post photos of their daily happenings, special occasions, holidays. Having all these stored on the cloud, we can one day come back to them and reminiscence, not having to worry too much about having misplaced it, or the ink wearing off, or the photos fading.
However, all these data that we have left behind has turned us into data too. The digital footprint that we unknowingly leave behind, all the information about ourselves that we only meant to share with a select few, we are actually voluntarily handing over to the public and third parties. Nothing on the internet is completely “private” after all. Our information can be tracked by the websites we visit, which companies can collect. This information is used by companies to improve their marketing strategies in order to more effectively target us, the consumers.
In addition, in the linked article, Wegner and Ward (2013) discussed how we are becoming so over-reliant on how we can use the internet. Be it storing information or retrieving information, the majority of internet users would think of turning to the internet at the first instance of facing an issue.
It is quite amazing, yet at the same time frightening how the internet has become such an important and essential part of our lives. Should we embrace it, or should we be cautious about it?
1
1
"
https://medium.com/@NordicBackup/data-security-best-practices-for-smbs-keeping-data-safe-from-loss-54f40fe1f624?source=search_post---------189,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nordic Backup
Apr 9, 2016·5 min read
With the wide array of internal and external threats to data within a business environment, keeping records safe poses more challenges today than ever before. Luckily, for all of the new technological threats that are created, there are a number of data loss prevention tools and solutions that businesses can rely on to keep their data protected and safe from permanent loss.
If your small business is concerned about keeping your data safe from loss, follow these data backup and data security best practices.
Before we get into the best practices your business should follow to keep its data protected and retrievable, it’s important to understand the need for them.
More and more businesses are transitioning away from physical storage and into digital storage spaces, and with that movement comes an increased risk for errors and losses. Businesses can come face to face with data loss due to natural disasters, technological failures, physical device damage, malicious intent, human error and more every day.
While many businesses recognize the reality of these threats, not all of them are prepared to handle them when they arise. “Cyberattacks” and “Data Fraud or Theft” are listed as the two highest areas of concern for businesses in the U.S., according to The Global Risks Report 2016, ranking above terrorist attacks, fiscal crises, asset bubble and a number of other risks.
There are however, ways to keep your data safe from loss and recoverable to you should your data become inaccessible or deleted.
By far, the best way to keep data safe in any business is through preventative measures. With the right security and backup measures in place, you can keep your data safe from digital threats and have the data recovery tools you need on hand, should your data ever get deleted.
To prevent data breaches and leaks, make sure your employees are informed and up to date on how to handle data securely. They should be aware of which data in your organization is classified, as well as which members within, and outside of, your organization are considered ‘authorized’ to view data. There may be different levels of access control you need to consider, as is the case with medical offices handling PHI.
Be sure to outline these considerations so that everyone in your office is on the same page about whom can receive information, and what information they are privy to. Also be sure to distinguish how data should be shared, destroyed, archived, and saved for later use.
Without clear instructions for this, it’s possible for an employee to improperly dispose of a document, leaking private information. Additionally, they could accidentally delete a file that was intended to be saved.
Have a plan for how you manage data on all fronts, and you’ll be much less likely to have to deal with the repercussions of mishandling.
Viruses are a common cause of data loss. Cryptoviruses are notorious for encrypting a user’s data, and only giving them the unlock key after a high dollar ransom amount has been paid. And small businesses are frequent targets, as they have more capital than an individual and have more holes in their network security than larger companies. To avoid them:
Taking steps to prevent the occurrence of risks like employee error and viruses is a good step toward eliminating data loss, however you also need a plan for when threats inevitably make their way past your security measures.
Backing up your data is the only way to recover it in the event of a data loss disaster — whether that disaster is by human error, physical device failure, theft, viruses, or some other threat.
Many businesses make the mistake of backing up their data to physical storage devices alone — especially when these devices are all housed within the same location, keeping them open to the same threats. Physical storage devices are vulnerable and should never be your business’s only backup plan. It’s important to back data up to at least 3 locations: your primary storage, secondary storage device, and most importantly a secure, off-site cloud backup.
Cloud backup is the most reliable form of data backup, keeping data protected and available at your fingertips, regardless of the loss scenario. It can be used to retrieve and restore healthy data after a malicious attack, restore correct versions of files after an employee error, or even reinstate all data to a repaired or new computer if the primary device crashes.
There are a variety of other features that make cloud backup the preferred storage method by businesses. Look for a cloud backup provider, like Nordic Backup, who offers:
Ultimately, cloud backup is the data security best practice that will keep your data safe from data loss arising from nearly any situation. If you want a guaranteed way to retrieve and restore your data, no matter what, cloud backup is the answer.
Secure your business data free, for 90 days with cloud backup, provided by Nordic Backup, and experience total peace of mind.
Originally published at pages.nordic-backup.com.
Leading the industry in secure online backup. The most comprehensive backup and recovery solution for business and personal users. http://hubs.ly/H01kyJQ0
Leading the industry in secure online backup. The most comprehensive backup and recovery solution for business and personal users. http://hubs.ly/H01kyJQ0
"
https://medium.com/@mainone54/enterprise-data-and-voice-solutions-in-nigeria-1b79870d75b7?source=search_post---------190,"Sign in
There are currently no responses for this story.
Be the first to respond.
main one
Jun 27, 2016·1 min read
The MainOne Voice Service is a high end enterprise solution that enables you make external calls via your existing PBX (Private Branch Exchange) infrastructure. Your current phone lines and extensions already connected to your PBX will have the ability to make direct outgoing calls thereby eliminating the need for further investment.
Owning our own IP-NGN network means we can send traffic via the most direct route so you can be sure of the highest connection quality for your end-customers. This integrates your voice and existing internet services, providing a converged network solution for your business and guarantees that your voice communication and internet quality are not compromised and you enjoy reliable and continuous world class service. This single point of accountability ensures that your business efficiency is optimized and provides cost savings.
Contact us :
Nigeria (VI) — +234–1–342 2000, +234–1–448 9500
Nigeria (Ikeja) — +234–1–342 2000
Nigeria (Abuja) — +234–1–342–2000
Nigeria (Port Harcourt)- +234–1–342 2000
Ghana — +233–302 744 030
Mauritius — +230–212–9800
Portugal — +351–926–371–059
Website — http://www.mainone.net/
Enterprise Data and Voice Solutions in Nigeria, Best Internet Service Provider in West Africa, Cloud Services in Nigeria
"
https://medium.com/@kellyleereeves/thank-you-8b317dfa20fb?source=search_post---------191,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kelly Reeves
·Jul 27, 2021
Lanu Pitan
Thank you. You can also download all of your Medium content on a regular basis so you don't lose it in the unfortunate event your account is closed.
I also recommend taking the additional step of backing up to an external hard drive, or even better, to a cloud service. I keep all of my posts in Google Docs. I learned after over 20 years in tech to have a backup of your backup. I've also worked with a number of storage companies. ;)

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Entrepreneur, writer, coach, and animal lover. I write about entrepreneurship, personal growth, and the occasional life lessons from dogs. IG @thesouloprenuer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@LucaDellAnna/an-addition-some-external-behavior-bias-or-trait-might-not-be-directly-beneficial-but-might-be-a-96b02155303e?source=search_post---------192,"Sign in
There are currently no responses for this story.
Be the first to respond.
Luca Dellanna
Aug 22, 2017·1 min read
Another principle:
Nassim Nicholas Taleb
An addition:Some external behavior, bias or trait might not be directly beneficial, but might be a necessary byproduct of a trait whose other byproducts are beneficial to survival.For example, I suspect it’s the case of some autoimmune diseases.
As another example, seeing imaginary faces in clouds isn’t beneficial, but a man who’s able to see faces in clouds is also able to spot a predator in the forest (can’t reduce false positives without increasing false negatives).
Author of some books on emergent human behavior. Read more at luca-dellanna.com. Twitter: @DellAnnaLuca
See all (153)
1 
1 clap
1 
Author of some books on emergent human behavior. Read more at luca-dellanna.com. Twitter: @DellAnnaLuca
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@michaelthompson_15510/i-am-stretching-through-the-vacuum-external-color-has-dissipated-36882f4424f4?source=search_post---------193,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Thompson
Mar 9, 2019·1 min read
Sea of Dirac
I am stretching through the vacuum; external color has dissipated. Astronomy is an automatic Bible, the hoping clouds staring through the grinning inch. You appear despite the chance. They hesitate. You cared for your childhood. Has the atom served to extend circulation? One grieves behind the delirium defense, the small dream, a son’s parade, embryos that don’t speak, closed to the nearby estate, dropping away, unclasping the cube, the concrete captive whose Hell closely flew contributing to the expedition.
An unpublished writer from Sacramento, California. He writes short stories, flash fiction, and fragments.
An unpublished writer from Sacramento, California. He writes short stories, flash fiction, and fragments.
"
https://medium.com/data-science-at-microsoft/how-we-used-ml-and-heuristic-data-labeling-to-help-customers-with-their-cloud-migration-d3af7ff020fc?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
As a data scientist on the Microsoft Cloud Data Sciences team, I work closely with internal stakeholders to find ways to leverage Machine Learning (ML) techniques to detect and uncover patterns from data that help our business make better decisions on behalf of our customers. Because our work is often judged by the satisfaction of our stakeholders, it’s important that we listen carefully to their requirements and feedback so that we can incorporate it into our work. In this way, we make stakeholders our main focus and closely involve them in decision-making as we develop our models.
Part of my day-to-day work involves listening to stakeholders describe their problems and challenges, which I then translate into requirements. I then frame the requirements as ML problems and decide the tasks I should be working on with respect to their potential impact and feasibility. Inherent in this approach is balancing what’s most important to do right away and what can be put in the backlog.
In this article, I pick a recent project to demonstrate how I’ve taken a real business problem, worked to understand its impact and how to measure it, and finally converted it into an ML problem while mitigating various challenges that come up along the way.
Multiple industry sources suggest that cloud migration is going to be a significant factor on the horizon. According to Gartner, by 2024, “more than 45 percent of IT spending on system infrastructure, infrastructure software, application software, and business process outsourcing is expected to shift from traditional solutions to the cloud.” According to Ed Anderson, Distinguished VP Analyst at Gartner, “[t]he proportion of IT spending that is being allocated to cloud will accelerate even further in the aftermath of the COVID-19 crisis, as companies look to improve operational efficiencies.” Gartner estimates that spending on cloud system infrastructure services is expected to grow from $44 billion in 2019 to $81 billion by 2022.
To help customers make this digital transformation, Microsoft has created a flexible, cost-effective, and high-confidence program to simplify and accelerate the cloud migration journey. In this context, migration is when a customer moves an existing workload — such as an application, service, or capability — from being done on premises or from another cloud solution to Azure. Here are some examples that demonstrate how we distinguish migrated versus cloud-native workloads:
A customer can choose to migrate using our Azure Migrate tools, third-party tools, or manually by using DevOps pipelines and ARM templates. While it’s straightforward to understand the number of virtual machines migrated using our own first-party tools, it is considerably more challenging to understand the number of virtual machines created as a result of migration (instead of being originated in the cloud for a new workload). Given that most VMs are not explicitly associated with migration, it’s not always clear which ones have been deployed as result of migration activity.
We believe it’s very important to help customers accomplish their cloud migration goals. As a result, it’s necessary for us to understand the migration execution funnel and the customers present at each stage of the migration process. To help, we developed an ML model to improve visibility, actionability, and efficiency in the customer cloud migration process. This model allows us to:
The goal of the model is to detect whether a specific VM in Azure has been created due to migration or is cloud native. After some deep analysis on the existing data, however, I realized that the model is not actually the biggest challenge here, but instead, getting the data labeled so it can be used in the model.
Supervised ML involves training the machine using data that is well labeled. In this approach, the dataset is used as the basis for predicting the classification of other unlabeled data through the use of ML algorithms. In contrast, unsupervised learning is where we have only input data (x) and no corresponding output variables. The goal of unsupervised learning is to model the underlying structure or distribution of the data to help us learn more about it. This is called unsupervised learning because, unlike supervised learning, there are no correct answers. Algorithms are left to their own devices to discover and present the interesting structure in the data.
In our case, because we expected to have the right answer for each example (migrated VM versus cloud-native VM), we knew that we wanted to go with supervised learning. We found, however, that there was not much in the way of existing labels for migration in the data — only two to three percent of migrated VMs were labeled in the telemetry. As a result, it was necessary to invest significant effort in figuring out heuristics to derive labels.
To start, our domain experts shared a number of assumptions that they believed we could make. For example, anytime someone creates a VM using an older version of an operating system, that choice is probably driven by the system requirements of an existing application, so the VM is likely related to migration. That’s not always the case, of course, but these assumptions were good enough for us to create our initial labeled data.
Another way we collected labeled data involved aligning the incentive of the stakeholders to the data labeling task. If stakeholders know how the model can help them unblock an existing customer, they are eager to help with the labeling effort.
We also relied on the knowledge of our domain experts about the approximate ratio of migrated VMs versus cloud native VMs for our labeling work, building on their insights to create the same distribution in our initial labeled dataset. Figure 2 demonstrates the overall process of collecting labeled data and how we improved the model as a result.
The first step of this process is to apply the heuristic rules to the existing dataset. Next, we train an ML model and send the output to account managers and different stakeholders for validation. Then, we fix the labeled data, add new heuristic rules, and retrain the model. Although the process illustrated above looks simple and straightforward, we spent more than 50 percent of our time just labeling the data.
We knew that with the heuristic labeling approach we could cover only small portions of actual migrated VMs, but it was still a good start for the project. One key recommendation here is to communicate these sorts of challenges to stakeholders at the beginning of the project and align expectations on model accuracy. Given that there was no other way to measure migration progress, our stakeholders agreed to start with this model and work with us to improve it over time.
After we created an initial dataset for training, we developed an ML model that combines a set of features typical of VMs. The model considers the likelihood a VM was migrated from being on premises versus being cloud native.
In building the model, we tried various approaches such as decision tree, logistic regression, random forest, and gradient boosting trees, among others. We realized the best performance on our test dataset from the LightGBM model, and Figure 3 shows its results. The area under the curve for the ROC curve is 0.9 and the area under the curve for the precision-recall curve is 0.91. (A perfect model would have an area of 1.)
With this approach, stakeholders can choose a point on the precision-recall curve (at right in Figure 3) taking into account the tradeoff they must decide on between getting high precision versus getting high recall in ways that best fit their requirements.
This article summarizes a journey from business need through to developing an ML model and challenges encountered along the way. The key takeaway is that the first version of a model doesn’t need to be perfect — it’s OK to make valid assumptions that enable starting with some sort of initial model. In other words, don’t try to build the perfect model from the beginning. It’s OK to take risks and make assumptions, start small, and then scale it up. Iterate quickly and incrementally — fail fast and learn faster.
For this project, I told the business that we would make many assumptions that might have an impact on model accuracy. Stakeholders agreed because they didn’t have tools that would yield better insights into migration. (And although I started with a lot of assumptions, after a couple of months, I was able to collect more than one million confirmed labels.) It’s important to note, however, that you can make valid assumptions only if you align on the right expectations with stakeholders from the outset. Additionally, start with field validation as soon as possible — it’s always better to have the pipes already in place for when you need them later.
I also recommend keeping momentum going and incorporating feedback from real customers all the time. Don’t stop doing this after the model is already in production — feedback is the key to improving models and making them better over time.
The author would like to thank Assaf Berenson who also contributed to this work.
Lessons learned in the practice of data science at…
18 
1
18 claps
18 
1
Written by

Lessons learned in the practice of data science at Microsoft.
Written by

Lessons learned in the practice of data science at Microsoft.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/automate-jira-cloud-workflow-with-golang-2de29828aad6?source=search_post---------195,"Sign in
Percy Bolmér
Aug 11, 2021·5 min read
JIRA is a very popular framework for tracking issues and project management. It allows project leads or scrum masters to set up projects and creates issues that are then assigned to developers. It’s a great framework for…
"
https://medium.com/@techgigdotcom/top-3-no-code-and-low-code-tool-builders-for-developers-932ac3b0aaea?source=search_post---------196,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechGig
Aug 24, 2021·3 min read
We developed a list of the most feature-rich internal tool builders you can use now based on these factors that toolmakers prefer to select a low code or no code tool builder.
There are two ways to construct internal tools: with code and without code, depending on your tech skills and capabilities. The use of an internal tool builder is required in the second option.
Although coding offers a lot more flexibility, using internal tools allows you to create the software you need faster, with less effort, and without blowing your budget.
What is an internal tool? An internal tool is usually internal-facing software that you create to automate or simplify some of your company’s internal activities. Internal tools are designed to automate micro-processes, increase team productivity, improve customer assistance and ticket management, and so on.
A CRM, a database GUI, an admin dashboard, and other internal tools are examples. Custom internal tools outperform ready-made general out-of-the-box solutions since they are always designed with your business operations in mind.
Preferences for an internal tool builder
We developed a list of the most feature-rich internal tool builders you can use now based on these factors:
1. UI Bakery There is no free plan available. The cost of a paid subscription per end-user begins at $7 per month. Based on the criteria, the following are the key features:
2. Retool There is a free plan available. The cost of a paid subscription per end-user begins at $10 per month. Based on the criteria, the following are the key features:
3. JetAdmin There is a free plan available. Paid subscriptions begin at $24 per month per user. Based on the criteria, the following are the key features:
ConclusionEach of the internal tool builders listed has its own set of advantages and disadvantages. They differ in terms of customization options, price points, the level of data protection they can provide, third-party integration capabilities, and customer support, among other things.
You should see and attempt all of them inside before engaging with a specific internal tool / internal database app builder. The majority of the internal tool builders we’ve listed offer a free trial (UI Bakery, DronaHQ) or a forever-free plan (UI Bakery, DronaHQ) (Retool, Internal, Jet Admin). In general, such plans limit the number of features you can try.
India's Largest Tech Community | 4 Million+ Developers | Guinness World Record Winner | Limca Book of Records
See all (43)
33 
33 claps
33 
India's Largest Tech Community | 4 Million+ Developers | Guinness World Record Winner | Limca Book of Records
About
Write
Help
Legal
Get the Medium app
"
