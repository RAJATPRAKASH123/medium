story_url,bodyText
https://medium.com/globallogic-cloud-and-devops-blogs/clouds-compared-aws-vs-azure-vs-gcp-c59519b9d5e4?source=search_post---------0,"There are currently no responses for this story.
Be the first to respond.
Public cloud has, in the last few years, broken through the mainstream enterprise consciousness. Even if these enterprises are not betting it all on cloud, cloud adoption is, in one form or the other, an integral part of most enterprises‚Äô infrastructure strategy and roadmap. For enterprises who are about to embark on this journey, the questions being asked are, ‚ÄúWhich cloud platform should I adopt?‚Äù, ‚ÄúWhich cloud platform provides cost effective services that are a fit for me?‚Äù, and ‚ÄúHow do I go about my cloud adoption journey?‚Äù This blog attempts to answer the first two questions.
To that goal, I have compared the core infrastructure services across the most popular cloud providers, which are Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP). In addition to the core infrastructure services, each cloud provider brings their unique proprietary offerings in the NoSQL, Big Data, Analytics, ML and other such areas.
This blog calls out each cloud‚Äôs unique aspects that may influence an enterprise‚Äôs choice of cloud based on their specific business and technical requirements.
AWS is the oldest public cloud provider with the widest range of products, compute and data storage options and managed services. AWS Marketplace is also the largest marketplace for third party applications and appliances. They also rapidly iterate to continuously add a substantial set of product features. Highly driven by customer feedback, their new services provide close integration with their core services like IAM, KMS etc. They have a strong focus on security and architecture best practices. Their enterprise frameworks such as the Well-Architected Framework and Cloud Adoption Framework have been developed from their experience with large enterprise customers. Besides their mainstream services, they are also known to release unconventional services as SnowMobile (data transfer appliance in a truck), RoboMaker (robotics framework) and Ground Station-as-a-Service (for managed satellite data download). This keeps customer interest piqued and has potential to open up entire industries. Their 51% market share is a proof of that. That said, they aren‚Äôt the cheapest cloud on the market. They also don‚Äôt seem to be worried about providing deeper container offerings. Their EKS (managed Kubernetes) service was relatively late to the market. Instead, they seem to be placing bets on MicroVMs (Firecracker) and managed functions.
They have lately seem to realign their focus towards hybrid cloud, and have announced offerings such as Outposts, (in partnership with VMware) that will enable customers to use well known AWS services and APIs on infra running in their private data centers.
AWS is a great choice for startups and enterprises alike. From web and analytical workloads, to large scale data center migrations, AWS provides a breath of services that customers can leverage. To help get customers of all shapes and sizes started on the platform, AWS has released supposedly niche services such as RoboMaker on one end, while going backwards to build services such as LightSail (virtual private server) to help even the smallest single server workloads be onboarded without much overhead.
When it comes to compute, AWS provides the widest range of VM types. AWS also currently has the highest compute and storage options available in the market. Their wide range of VM types (136 VM types over 26 VM families) enable customers to run everything from small web workloads to the biggest HPC and SAP workloads.
For machine learning and AI workloads, AWS also provides the highest configurations of GPU enabled VM types. For workloads that require single tenancy for compliance and regulatory reasons, AWS also now provides Bare-Metal-as-a-Service. For virtualized workloads that might need it, AWS provides features such as placement groups to ensure that the workloads run on a designated underlying hardware.
AWS hopes that the various t-shirt VM sizes will match your workload requirements. It therefore doesn‚Äôt support creation of custom VM sizes (vCPU, RAM). Unlike other cloud providers (CSPs), it only provides a specific set of VM families which come with GPUs. It does not allow attaching GPUs to any or all VM types in its portfolio.
Block storage comes with a variety of options, such as dynamic resizing, different disk types (magnetic and SSD). Unlike other CSPs, AWS does not restrict IOPS by volume size. You can provision IOPS at extra charge to even small disks.
On the managed relational database front, AWS supports managed databases for MySQL, PostgreSQL, MariaDB, Oracle (both SE and EE) and MS SQL (Web and Enterprise editions) under their RDS offering. In addition, they have their own MySQL and PostgreSQL compatible database offering that boasts Oracle like performance at fraction of the cost. They are investing in this heavily and have also announced Multi-Master and Serverless versions.
For NoSQL databases, AWS has had their DynamoDB product available for over half a decade. This evolved from their SimpleDB offering. AWS is a proponent of and provides a range of purpose-built NoSQL databases. These include, DynamoDB (key value and document), Neptune (Graph), and Elasticache (key value caching).
AWS has improved its networking services portfolio over the last decade. It started with VPC and related network features such as security groups, network ACLs and Internet Gateways. At the time, users still had to configure their own NAT servers, bastion hosts etc. AWS has listened to customer feedback and gradually added these as managed network services to its portfolio. AWS now provides a managed NAT gateway, VPN Gateway, Transit Gateway, Direct Connect Gateway etc. They recently also announced a managed Client VPN service. This removes the need for customers to deploy OpenVPN servers to manage access to cloud VMs.
For network security, AWS has launched managed services for DDoS protection (AWS Shield) and Web Application Firewall (WAF), along with AWS Inspector, AWS Config and CloudTrail for inventory and policy management and auditing. GuardDuty provides threat detection.
For data security, AWS provides encryption at rest for most of its storage services. AWS also has KMS and CloudHSM services for key management. Macie provides an AI driven data loss prevention (DLP) service.
On the queueing, messaging and notification front, AWS provides managed AMQP compatible queue service (Amazon MQ) in addition to its SQS offering. For Pub/Sub, AWS has offered Kinesis and has recently added a managed Kafka offering. SNS provides a multi-channel integrated notification services that allows customer notifications over SMS, mobile, SMS and email notifications. Internally, it also connects with its other services to enable event driven loosely coupled architectures.
AWS serves US Government workloads in separate GovCloud regions in the US. Customers who need to provide services to customers in China can rely on AWS‚Äôs China region, which is provided via partnership with third party providers.
All in all AWS provides a breadth and depth of services and features that are suitable for a substantial number of enterprises.
Microsoft had lagged behind AWS in the public cloud game, but it focused first on SaaS and PaaS offerings as its strengths lie in both enterprise and consumer software. Microsoft initially focused on PaaS services for Azure. These were focused on their existing base of Microsoft developers. Over time, Microsoft expanded focus to both Linux and IaaS services. This also reflected in their re-branding Azure from Windows Azure to Microsoft Azure, and Microsoft loves Linux campaigns. Over time, Microsoft has also made Azure more startup friendly and built out API support for its various services. However, despite the breadth of its services, Microsoft lags substantially behind AWS in enterprise adoption. Large enterprises that already have existing Microsoft relationships remain a large part of the user base, though Azure is seeing robust growth in year-on-year revenue.
Azure is a mature cloud platform with a breadth of features which may be a preferred platform for customers that are already using Microsoft products in some way. While Azure supports a number of open source product based services, the Microsoft portfolio on cloud is what sets it apart for customers.
Azure has over 151 VM types over 26 VM families that support everything from small web workloads to the HPC, Oracle and SAP workloads. Azure has both Windows and multiple flavors of Linux (RHEL, CentOS, SUSE, Ubuntu). Azure has a separate family of instances for ML/AI workloads.
If you need to run high-end workloads that require up to 128 vCPU and 3.5 TB memory, Azure is a good bet. If you have existing licenses for windows OS, MS-SQL and bring them to cloud (BYOL) via Microsoft License Mobility Program, Azure is the cloud to choose. License costs form a substantial part of infrastructure expenses, and will be a major consideration for customers who run large deployments of MS-SQL etc.
Azure was also the first cloud player to recognize the trend towards hybrid cloud, and had one of the first hybrid cloud and Cloud-in-your-Datacenter offering (Azure Stack). Customers who wanted the interface of Azure but wanted to run services in their own data centers could use Azure Stack. Other cloud players are only catching up with Azure in this space. Azure also provided support for hybrid storage appliances like StorSimple, which was unique in the public cloud space.
If you have a data center with predominantly Microsoft workloads and need to do a large scale data center migration to cloud, while taking advantage of familiar tools, Azure provides tooling and services, such as Azure Site Recovery.
When it comes to SQL and NoSQL databases, Azure has a fairly well rounded set of services. It provides managed MS SQL Server and SQL Datawarehouse. Azure also provides managed databases for MySQL, PostgreSQL, and MariaDB. Azure Table is a managed key value store, whereas CosmosDB provides multi-model, globally distributed NoSQL database with multiple consistency models. It provides an API compatible with MongoDB, Cassandra, Gremlin (Graph) and Azure Table Storage. If you need to run multiple managed data models, including document, graph, key-value, table, and column-family data models in a single cloud, Cosmos may be the way to go. Azure cache for Redis rounds off the offerings with a managed cache.
In addition to PAYG billing model with credit card and invoicing modes, customers with existing enterprise accounts may pre-purchase Azure subscriptions as part of their annual renewals. This is useful for customers who want to be able to budget the annual cloud spend upfront. This prevents the uncertainty and additional mid-year budget approvals that are usually associated with PAYG models. When doing this, enterprises should size their projected workloads with some accuracy so that no pre-paid credits are wasted at the end of the year.
License mobility to cloud for Microsoft products is also relatively easy for customers with multiple Microsoft products running on-prem.
Google Cloud Platform (GCP), while late to the game and having the smallest market share of the public cloud providers compared here (current market share at about 4%), is showing a robust percentage growth. It boasts of several features that put it ahead of its competitors in certain areas. GCP is also riding the wave not only new customers who are already part of the ecosystem, but also early cloud adopters who are looking to expand their landscape to Google as part of a multi-cloud strategy. Google also started with PaaS services but has been steadily expanding its product portfolio.
Along with innovative features, Google boasts the lowest list price on infrastructure compared to all the other cloud providers. Of course, total expenditure for any enterprise depends on services used and cost governance measures in place.
From a compute perspective, Google has the smallest number of t-shirt VM sizes (28 instance types over 4 categories). However, it has one feature which makes these numbers slightly irrelevant. Google allows users to create their own custom sizes (CPU, memory) so that customers can match their cloud workloads sizing to their on-prem sizing. They also bill customers based on the total CPU and memory used, rather than individual VMs. This reduces wastage of unused capacity.
Another unique feature is that GCP allows almost all instance types to attach GPUs. This can turn any standard or custom instance into a ML ready VM. Google was also a leader in per-second billing, which forced other CSPs to follow suit. Compared to the pervious norm of per hour billing, per-second billing greatly reduces any capacity wastage. This results in up to a 40% savings overall, compared to relying on standard VM t-shirt sizes and per hour billing.
VM startup times in GCP are phenomenally fast, and leave other CSPs in the dust. This makes scaling out especially responsive. GCP also allows dynamic resizing of disks, so that you don‚Äôt have to do sysops acrobatics when your disks fill up. IOPS are assigned based on disk sizes and cannot be provisioned separately. This might be problematic for customers who want high IOPS on a small data set, and result on wasted dollars for unwanted storage.
Google has also tied up with or purchased third party cloud migration tools. These tools, such as CloudEndure, Velostrata and CloudPhysics, help customers assess, plan and live-migrate their VMs to GCP essentially for free. On other cloud providers, some of these tools cost several hundred dollars per VM. Google is clearly making migration to GCP as easy as possible.
Networking is where GCP shines. They have a global low latency network. Even from customer perspective, a VPC network spans all their regions. Other CSPs limit VPC networks to a region. This makes it easy for GCP customers to build applications that serve customers globally, without building complex cross region infrastructure design and data replication mechanisms.
Object storage also supports a multi-regional mode, where data is replicated across regions automatically. For customers considering a migration from AWS, or considering a multi-cloud strategy, GCP supports importing object storage from AWS.
For relational databases, GCP provides support for managed MySQL and PostgreSQL databases. For customers who want a globally distributed database that still supports immediate consistency and ACID properties, GCP has built Spanner. Spanner uses consensus algorithms and atomic clocks to synchronize transactions between nodes. This offering is unique to GCP and makes Spanner very attractive to large enterprise customers who have these requirements from their relational data store. In fact, another open source database, CockroachDB, is based on the Spanner paper that was published by Google.
From a NoSQL perspective, GCP has a product called BigTable. BigTable is a petabyte scale, managed wide-column NoSQL database that is used by Google in its own products such as Gmail.
From a billing perspective, Google provides automatic discounts such as sustained use discounts which reduce the on-demand price if a VM runs more than a certain number of hours in a month. If you want the most cost effective cloud provider in the market today, GCP is a good choice.
While it may not have the sheer depth of features of some of the other CSPs, it has some unique products in its portfolio, and is an attractive option being a price leader in the market.
As detailed above, each cloud has features and advantages that appeal to specific customer needs. While all the cloud providers will continue to provide certain common services (such as managed MySQL database), each CSP will continue to build out unique, differentiated services (e.g. Aurora, Cosmos, Spanner) that are purpose built to solve very specific customer needs. CSPs hope that this will increase customer stickiness and create a lock in.
From a customer perspective, these services will also become a driver to adopt a multi-cloud strategy. As an example, a customer might likely want to use GCP for one app that needs Spanner‚Äôs features, while they use AWS for their AI services, and Azure for specific Windows workloads.
Even for future looking services like computer vision and speech recognition, customer needs might drive them to mix and match services across cloud platforms, to meet their application‚Äôs requirements. Customer will likely use one cloud as their primary platform, while using services from others for specific applications.
This blog is part of our ongoing cloud series. To find out how GlobalLogic can help in your cloud adoption journey, please reach out to us at cloud@globallogic.com.
This blog contains articles, nuggets and pearls of wisdom‚Ä¶
1.3K 
10
1.3K¬†claps
1.3K 
10
This blog contains articles, nuggets and pearls of wisdom from GlobalLogic Cloud and DevOps Practice
Written by
Cloud and DevOps Lead ‚Äî APAC at GlobalLogic
This blog contains articles, nuggets and pearls of wisdom from GlobalLogic Cloud and DevOps Practice
"
https://servian.dev/azure-az-900-exam-preparation-guide-how-to-pass-in-3-days-dabf5534507a?source=search_post---------1,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
Top highlight
Perth Ngarmtrakulchol
Aug 28, 2019¬∑10 min read
AZ-900 is the exam that test on foundational level knowledge of cloud concepts and Azure services.
Those who have experiences in other major cloud providers might find the most of AZ-900 materials to be very similar to they have learned.
I found the content of this exam to be very beneficial for anyone who would like to break in cloud computing on Azure.
This exam guide will give you all the information you need to pass the exam in 3 days or less.
Most of the exam information is on Microsoft website. However, before going into the exam, I was researching how long is this exam. The Microsoft website does not show how many questions and how long the exam is.
So I gathered these numbers based on my exam:
What I like about this exam is: Your result sheet is printed right after the exam, so you can see which area you are doing well and which area you need to work on.
This is the same study plan I used to pass AZ-900 exam:
One day means 6‚Äì8 hours of work. So it might take around a week to study 2‚Äì3 hours per day after work.
I studied from 3 resources and spent around $15 for the preparation (excluding $99 for AZ-900 exam itself).
The 3 resources are:
[Free] Microsoft Learn platform (11 modules)
Microsoft provides a free learning platform for everything you need to pass any Azure certificate.
Unlike other learning platforms such as Google Cloud‚Äôs Coursera or Udemy, Microsoft Learn platform is 90% text with few short videos here and there. However, the material is great quality and the learning platform is easy to use.
I found that by having learning material in text, I can take note much faster by copy-paste the text directly.
This course took me 9‚Äì10 hours including taking notes and research on random technical words e.g. N-tier architecture.
[$15] Udemy course ‚ÄúMicrosoft Azure Beginners Guide‚Äù
This Udemy video course contains all you need to know to pass AZ-900 exam. The content is more hands-on than Microsoft Learn since you will see the instructor showing you inside Azure Portal, whereas Microsoft Learn only teach you in text.
The beginners in Azure will learn a lot more from seeing real Azure portal than reading.
For me, I had some experience playing around Azure portal before. So I skipped to the last section: AZ-900 Preparation. This section provides 70 questions x 2 practice tests.
I found the practice questions to be very similar to Whizlabs, the last resource I used for exam preparation. I have heard from my colleague who passed AZ-900 only using this course.
My recommendation is you could study from videos in this course or from Microsoft Learn. The content should be very similar.
At my company Servian, we provide Udemy Business subscription for staffs. So I can access this course for free.
[$15] Whizlabs AZ-900 Exam Practices Tests
Whizlabs is the popular website containing a lot of IT exam practice tests. Note that Microsoft also offers Official Practice Test but at the significantly higher cost and a similar number of practice questions. So I went for Whizlabs this time.
At the time of writing, Whizlabs has 3 practice tests with 55 questions each. The questions are surprisingly similar to the real exam, so you can get a feeling of what kind of questions you would get examined on.
This section is based on the list from Microsoft website which is quite accurate to the exam.
There are 4 main parts. For each part, I am giving summarised information here to speed up your learning or you can also use them as revision material.
Part 1) Cloud Concept
This part contains general knowledge about cloud computing e.g. what are the benefits of cloud, the differences types of cloud service offering, and the differences in cloud deployment models.
Technological benefits of cloud:
Business benefits of cloud:
Types of cloud service offering:
Differences in cloud deployment model:
Part 2) Core Azure Services
This part contains the introduction to different services offering in Azure for each service category:
Part 3) Security, Privacy, Compliance, and Trust in Azure
This part contains the security Azure provided for their services as well as Azure‚Äôs commitment in privacy and regulatory compliance.
Azure services for security in different areas:
Part 4) Azure Pricing and Support
This part contains the information about different types of Azure subscription, cost factors of Azure services, tools for cost calculation, and the support plans in Azure.
Azure subscription types:
Cost Factors of Azure services:
Tools for cost calculation for Azure
Service Level Agreement (SLA)
Public Preview vs Private Preview features
What you will learn from the recommended courses listed above as well as the summary in this article will make you pass the exam with flying colors.
For the next step, the knowledge in AZ-900 exam is crucial for any Azure role-based certification in the future. Whether you would like to become Azure Administrator or Azure Data Engineer or any other roles, AZ-900 will be the great starting point for your career.
Wish everyone all the best for your exam!
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help our customers sustain competitive advantage. If you need any help from us in these areas, feel free to ask!
Data Consultant at Servian | Monash Data Science Alumni | Front-end Developer based in Melbourne, Australia | LinkedIn: http://bit.ly/lkdn-perth
See all (779)
1.2K 
27
1.2K¬†claps
1.2K 
27
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-i-won-the-azure-machine-learning-award-418ff35c6e4d?source=search_post---------2,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Every year, Microsoft hosts the Imagine Cup. Young developers often call it the ‚ÄúOlympics of Technology‚Äù and consider it one of the top competitions related to software design. As a result, it attracts lots of young participants from around the world, who collaborate to solve some of the world‚Äôs toughest challenges.
In 2016, they hosted the Hello Cloud Machine Learning Award, where winners were selected from all entries based on quality, creativity and the effectiveness of their use of Azure Machine Learning Studio.
I jumped into the competition to learn more about machine learning and ended up as one of the winners of the challenge. What got my interested in the first place was the focus of the competition: to build creative and inventive systems using machine learning.
When a user stops using a good or service (in this case, a game), we call that ‚Äúchurn.‚Äù Based on past player history, or data for similar players over time, we can create a machine learning model to predict when a player is most likely to quit.
In the first part of the competition we had to build, train, score, and evaluate a model to do just that in the Azure ML Studio. We then had take a basic game provided by them, connect it to the Azure ML service, and publish it to the web.
One of the key factors that makes a player abandon a game is its difficulty level. If it's too easy the game gets boring, and if it's too hard it demotivates the user to keep playing.
I decided to use the rock-paper-scissors dynamics in the game. To earn new superpowers in the game, the players (math students) would have to solve some math equations (these were used like attacks in a fighting game).
Based on the data for each player, we could adjust the difficulty of the math equations to keep them motivated to play the game. We could also identify what kind of equations were causing kids the more trouble (subtraction? multiplication?). This is an amazing opportunity to help teachers and everyone involved in the learning process.
One thing I know for sure in competitions is this: focus on being different instead of just focusing on being better. We don't know the number of competitors for sure, but based on what we heard there in Seattle, the contest had almost 1,000 entries. That's a lot of games for the judges to evaluate. You have to do everything you can to stand out from a crowd that size.
I bet that when you read the word ‚Äúdifferent‚Äù your first thought was ‚ÄúGreat, now I have to come up with something big and strange out of nowhere.‚Äù Don‚Äôt worry ‚Äî that isn‚Äôt the case. Because here‚Äôs another thing I know for sure: to be different, you can just focus on being yourself. I know it sounds cheesy, but let's elaborate on that.
‚ÄúBe yourself; everyone else is already taken.‚Äù ‚Äï Oscar Wilde
You are you, right? No one else in the world have had the experience you had, did everything you did or felt exactly everything you felt. That's it, you just have to use this to be different (and original). Now let's go back to what I did in the contest.
I bought my first Wacom tablet in time for the competition, and honestly I was just looking for excuses to use it. I like to get adventurous in other areas, and I know that this is something that differentiates me. So I decided to work and change the assets of the game.
In the competition we first had to follow a tutorial. Only after that could we begin to create our own version of the game. This is a great way to design the workflow of our projects (and side projects): always find a way to make the start phase easy.
I first heard this advice in the book Think Like a Programmer. It‚Äôs true for programming, but it‚Äôs also true for a bunch of other aspects of our lives.
Once you have divided the problem up into pieces, for example, go ahead and complete any pieces you already know how to code. Having a working partial solution may speak ideas about the rest of the problem. Also, as you may have noticed, a common theme in problem solving is making useful progress to build confidence that you will ultimately complete the task. By starting with what you know, you build confidence and momentum toward the goal. ‚Äï V. Anton Spraul, Think Like a Programmer
Let's be honest: programming is hard. During the competition I had some moments of frustration, but things like seeing my first predictive model running and seeing the pieces of the game start to work together got me going. Make sure you can start to see progress in a project, right from the beginning.
This was the key factor for winning the competition, because without it, my entry wouldn't even have been submitted. Time is a limited source. Everybody knows this, but it‚Äôs something that we always have to keep reminding ourselves of ‚Äî especially us programmers.
If you take a look at my Game Design Document above, you can see that my first idea for the game had a lot of features. For example, we had levels for the players, items they would be able to collect, healing effects, and so on. As the deadline approached, I realized that I didn't have time to execute all of those ideas. So I had to think: what is the one thing that I should have in the game to make sure that it would accomplish my goal? The answer was math equations and the rock-paper-scissors dynamics, and that was what I implemented.
It's not easy to give instructions about how to adapt, because every situation is a different. But you should know that you will have to make choices along the way. Your main focus should be to finish the thing on time, so you can indeed compete in the contest.
Well, as you may know by now, my project was one of the two winners of the challenge (yay!). I won a trip to the Imagine Cup World Finals and had mentoring sessions with members of the Microsoft Data Platform product.
With this project I finally found my main career goal: to design Machine Learning systems that let humans do things they care about.
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
459 
6

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
459¬†claps
459 
6
Written by
Award-winning Data Scientist üë©üèæ‚Äçüíª Loves to write and explain things in different ways‚ú® - http://deborahmesquita.com/
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Award-winning Data Scientist üë©üèæ‚Äçüíª Loves to write and explain things in different ways‚ú® - http://deborahmesquita.com/
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/deep-ai/study-guide-for-microsoft-azure-data-scientist-associate-certification-dp-100-c2e4611cb071?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
Update 22/4/2020:
Updated 17/5/2019:
Microsoft recently released a certification named DP-100: Designing and Implementing a Data Science Solution on Azure for the title of Microsoft Certified: Azure Data Scientist Associate. This certification is for a data scientist to check his knowledge and solutioning skills on Azure. This certification checks for both the width and depth of data science knowledge. Despite being the first one in the series this certification is a tough one to crack, buts that's where I can help.
Read about the certification on Artificial Intelligence here: Study Guide for Microsoft Azure AI-100: Designing and Implementing an Azure AI Solution (Beta)
Total time 220 minutes, with actually 180 minutes for 60 questions.
Below pointers are based totally on my certification experience.
Below are the type of questions:
Only 30‚Äì40 % questions were specific to Machine learning on Azure
Here is a comprehensive list of study material covering DP-100 scope & questions, you can thank me later.
If you need further help or have a question then write in the comments below or find me on LinkedIn. Also, do let me know about any changes in questions as the examination is still in beta, so questions or pattern might change. Thanks.
If you have any comment or question, then do write them below.
To see a similar post, follow me on Medium & LinkedIn.
If you enjoyed then Clap it! Share it!! Thanks!!!
Towards AI: Read, Learn, Apply¬†!!
825 
11
825¬†claps
825 
11
Written by
MCT | MCSE: Azure | MCSA: Machine Learning | Blockchain| R, Architect/Consultant/Trainer. I love working with cutting-edge technologies.
Towards AI: Read, Learn, Apply¬†!!
Written by
MCT | MCSE: Azure | MCSA: Machine Learning | Blockchain| R, Architect/Consultant/Trainer. I love working with cutting-edge technologies.
Towards AI: Read, Learn, Apply¬†!!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/azure-functions-choosing-between-queues-and-event-hubs-dac4157eee1c?source=search_post---------4,"There are currently no responses for this story.
Be the first to respond.
Top highlight
I have this conversation about twice a week. Someone has decided they want to take advantage of the benefits of serverless Azure Functions for an upcoming project, but when starting to lay out the architecture a question pops up:
‚ÄúShould we be using Azure Event Hubs, Queues, or Event Grid?‚Äù
It‚Äôs honestly a great question ‚Äî and it‚Äôs a question with consequences. Each of these messaging technologies comes with its own set of behaviors that can impact your solution. In previous blogs, I‚Äôve spent some time explaining Event Grid, Event Hubs ordering guarantees, how to have ordering guarantees in queues / topics, and how to keep Event Hub stream processes resilient. In this blog, I specifically want to tackle one other important aspect: throughput for variable workloads. Here was the scenario I was faced with this last week:
We have a solution where we drop in some thousands of records to get processed. Some records only take a few milliseconds to process, and others may take a few minutes. Right now we are pushing the messages to functions via Event Hubs and noticed some significant delays in getting all messages processed.
The problem here is subtle, but simple if you understand the behavior of each trigger. If you just have a pile of tasks that need to be completed, you likely want a queue. It‚Äôs not a perfect rule, but more often than not queues may have the behavior you are looking for. In fact, this blog will show that choosing the wrong messaging pipeline can result in an order of magnitude difference in processing time. Let‚Äôs take a look at a few reasons why.
If you look at the problem posed by the customer above, one key element let me know queues could be best here.
‚Äú‚Ä¶Some records only take a few milliseconds to process, and others may take a few minutes.‚Äù
If I have a queue of images that need to be resized, a 100kb image will resize a lot quicker than a 100mb panoramic image. The big question is: ‚ÄúHow does my processing pipeline respond to a long task?‚Äù It‚Äôs a bit like driving on a freeway. An incident on the road is much more impactful for a one lane road than a five lane road ‚Äî and the same is true for your serverless pipelines.
Azure Event Hubs cares about two things, and it does a great job with those two things: ordering and throughput. It can receive and send a huge heap of messages very quickly and will preserve order for those messages. Our team has posted previously on how Azure Event Hubs allowed us to process 100k messages a second in Azure Functions. The fact that I can consume a batch of up to 200 messages in every single function execution is a big reason for that. But those two core concerns of ordering and throughput can also get in each others way.
> If you‚Äôre not already familiar with how Event Hubs preserves order, it may be worth reading this blog on ordering first and coming back.
Imagine I have a series of events that all land in the same Event Hub partition. Let‚Äôs say my Azure Function pulls in a batch of 50 events from that partition into a function. Messages 1‚Äì9 process in a few milliseconds, but message 10 is taking a few minutes to process. What happens to events 11‚Äì50? They are stuck. Event Hubs cares about ordering, so message 10 needs to complete before message 11 starts. Imagine a scenario where the events you are receiving are stock market events ‚Äî you very much do care that a ‚Äúbuy‚Äù happened before a ‚Äúsell‚Äù event, so you‚Äôd be glad it‚Äôs holding the line. But what if your scenario doesn‚Äôt care about ordering? What if events 11‚Äì50, or even 51‚Äì1,000 could be running while we wait for message 10 to complete? In this case Event Hubs behavior is going to slow down your performance dramatically. Event Hubs will not process the next batch in a partition until the current batch, and every event in that batch, has completed. So one bad event can hold up the entire partition.
What about queues? Standard queues don‚Äôt care as much about ordering¬π (session-enabled queues do). Queues care more about making sure that every message gets processed and gets processed completely. Head into any government building and you will usually witness distributed queue processing in action. You have a single line of people that need something done, and a few desks of employees to help (hopefully more than one üòÑ). While some employees may have requests that take a number of minutes, the other employees will keep grabbing the next person in line as soon as they are available. One long request isn‚Äôt going to stop all work from continuing. Queue triggers will be the same. Whichever instance of your app is available to take on more work will grab the next task in line. No task depends on the completion of another task in that queue¬≤.
As if deciding between Event Hubs, Event Grid, and queues wasn‚Äôt hard enough, there‚Äôs a sub-decision on storage queues vs service bus queues. I‚Äôm not going to go very deep into this here. There‚Äôs a detailed doc that will lay out the big differences. In short, Service Bus is an enterprise-grade message broker with some powerful features like sessions, managed dead-letter queues, and custom policies. Storage queues are super simple, super lightweight queues as part of your storage account. I often stick with storage queues because my Azure Function already has a storage account, but if you need anything more transactional and enterprise grade (or topics), Service Bus is absolutely what you want.
To illustrate the difference in behavior for variable workloads I ran a few tests. Here‚Äôs the experiment.
I put 1,000 identical messages in an Azure Event Hub, Azure Service Bus queue, and an Azure Storage queue. 90% of these messages would only take one second to process, but 10% (sprinkled throughout) would take ten seconds. I had three mostly identical JavaScript functions that I would start processing on those messages. The question is: which one would process fastest?
Shouldn‚Äôt be much of a surprise given the explanation above, but Event Hubs took roughly 8x longer than queues to process all 1,000 messages. So what happened? It actually processed about 90% of the messages by the time the queue function apps finished, but that last 10% had a very long tail. Turns out one instance got unlucky with its assigned partitions and had about 40 of those ten-second tasks. It was stuck waiting for long tasks to complete before moving on to the next set of events, which likely contained another ten-second task. The forced sequential processing for the final 10% was significant.
Storage queues and Service Bus queues were extremely close in terms of overall time to process for this experiment (within a few seconds). There is one subtle difference I want to call out here though. Behind-the-scenes in serverless there are instances, or worker nodes (you could even call them‚Ä¶ servers), processing your work. While we handle the scale out for you, Azure Functions gives you the ability to process multiple messages on a single node at one time. This is actually super helpful in a number of cases as you can have much better connection pooling, cache sharing, and resource utilization. For both service bus queues and storage queues, the Functions runtime will pull down a batch of messages and process them in parallel on each running instance of your app. The default concurrency for both kinds of queue triggers is 16 messages. In my case my functions scaled out to many instances during the short test, so my total concurrency was higher than 16 messages, but each instance was processing sets of 16.
Why this matters is storage and service bus queues handle the batch slightly different. The big distinction point on: ‚ÄúHow many messages have to be processed before the next message, or batch, will be retrieved.‚Äù
For Azure Storage queues there is a ‚Äúnew batch threshold‚Äù that has to be crossed before the next batch is retrieved. By default, this is half of the batch size, or 8. What that means is the host will grab 16 messages and start processing them all concurrently. Once there are <= 8 messages in that batch left, the host will go grab the next set of 16 messages and start processing (until the to-be-completed count gets to<= 8 messages again). In my case, since only 10% of messages were slow, this threshold could generally be trusted to keep instance concurrency high. But you can still see the little bursts of batch thresholds in Application Insights analytics. The sharp jumps correlate to the processing batch size and when new batches are retrieved.
Azure Service Bus queues rely on the MessageHandler provided by the Service Bus SDK to pull new messages. The message handler has an option for max concurrency (by default set to 16 for functions), and can automatically fetch more messages as needed. In my experiment you can see a much smoother rate of messages being concurrently processed. If 1 slow message was holding up one of the 16 concurrent executions, the handler could still keep cycling through messages on the other 15.
So as expected, queues of both flavors performed much better when your problem set is ‚ÄúI have a bunch of variable tasks that need to be executed in any order.‚Äù
Why wasn‚Äôt Event Grid included in this? There is a big difference in Event Grid‚Äôs throughput behavior than these other options. Event Grid is push based (via HTTPS) and these are pull. If I get 1000 Event Grid events, my functions get poked by 1000 concurrent HTTPS requests. They‚Äôll have to scale and queue and handle them as quickly as possible. I would expect since ordering isn‚Äôt guaranteed for Event Grid it would perform closer to queues. That said, for this customer specifically they had some throughput considerations of downstream systems, so they wanted the flexibility to pull these messages at a more consistent click which Event Grid cannot provide (they‚Äôll poke the function whether it wants more or not).
The only other note I‚Äôll make in regards to Event Grid is it is meant for processing events, not messages. For details on that distinction check out this doc.
Hopefully, this experiment helps clarify that messaging options are not one-size-fits-all. Each comes with its own set of priorities and strengths, and it‚Äôs important to understand each when creating serverless architectures.
#BlackLivesMatter
1K 
7
how hackers start their afternoons. the real shit is on hackernoon.com.¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1K¬†claps
1K 
7
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/aws-vs-azure-vs-firebase-vs-heroku-vs-netlify-how-to-choose-the-best-platform-for-web-projects-482d017de254?source=search_post---------5,"Sign in
You have 1 free member-only story left this month. Sign up for Medium and get an extra one
Ali Kamalizade
Aug 2, 2019¬∑6 min read
Web and mobile development has come a long way in recent years. Modern web applications are often built based on powerful JavaScript features like Angular, React, and Vue.js. While you could host those web applications anywhere, you may need more than only hosting. Different big cloud companies like Google, Amazon, and Microsoft are offering practically anything you can ask for, while upcoming competitors like Netlify want to provide an impressive UX for building modern websites.
In this article, I want to focus on web-based projects. We will look at the following platforms:
Keep in mind that this is just a short look into this topic. There are many factors to consider depending on your project size and needs, such as:
Let‚Äôs get started and have a look at what these platforms can do for us.
Amazon Web Services has been in the cloud computing market for quite some time. No matter what you might need: AWS probably offers it. AWS offers many products with cool names like:
Netflix, Unilever, and Samsung use AWS. You can get one free year, which should be plenty of time to get a grasp. However, since AWS has grown a lot over the years, the usage is not always intuitive.
AWS fulfils ISO 27001 and SOC2
Microsoft Azure is one of the top cloud computing platforms. Similar to AWS, Azure has everything you will need. Microsoft offers special programs for startups which provide free limited access to Azure services. Azure offers services like:
Leading companies like Adobe, BMW, and HP rely on Azure. Microsoft has a very good standing in B2B, which is why many large corporations prefer Microsoft Azure. Similar to Amazon, the Azure UI is rather complex, which can be challenging for new users.
Firebase is a development platform owned by Google. Technically, Firebase is using the Google Cloud Platform. PayPal, Twitter, and Target are customers of Google Cloud. Thanks to the experience of Google, Firebase offers many useful services like:
Many developers love Firebase because of its strong tooling and its powerful Google infrastructure. However, especially due to GDPR and increasing strict data protection policies, some companies want to avoid using Google infrastructure.
Being a Google product has its upsides and downsides which applies to Firebase as well. Even though Firebase seems to perform quite well, we all know what happens to Google products which don‚Äôt live up to expectations.
Heroku is a Platform as a Service owned by Salesforce, an American cloud-based software company which is mostly known because of their CRM solutions. The free version of Heroku is fine for experimenting, but the server will sleep after some time of inactivity. There are lots of free and paid add-ons which provide additional functionality like:
While Heroku used to be rather unique when it started, it has slowed down when it comes to innovation and competition has caught up.
Heroku supports most popular languages like Java, Python, and JavaScript. Also, Heroku provides a CLI which you can use to deploy with one command. Citrix, Toyota, and Unsplash are known to use Heroku.
Fun fact: Heroku‚Äôs physical infrastructure and managed within Amazon‚Äôs secure data centers and utilize the AWS technology.
Netlify is a rather new contender. The free version is already quite generous and there is no sleeping, unlike with Heroku‚Äôs free version. The UX and the features Netlify provides make working with it seamless and intuitive. Some of the powerful add-ons Netlify provides are:
The downside is that cloud providers like Microsoft and Amazon offer way more functionality beyond web projects. Besides, you also cannot use other programming languages like Java or C# since Netlify promotes the usage of JAMstack. However, you can use Functions as an alternative to server-side languages like Java or C#.
Companies like WeWork, Verizon, and Nike are users of Netlify. Also, some popular open-source projects like Vue.js and Kubernetes have decided to use Netlify.
I have previously written an article about how to deploy an Angular app on Netlify:
itnext.io
As you can see, there are lots of options to choose from.
As I said, there are lots of important questions you need to ask yourself when choosing a platform for web projects. Some aspects might be more important than others depending on your situation: if you‚Äôre a startup you‚Äôre more looking for a easy and cheap solution while bigger projects and companies often require more sophisticated features and security standards.
Which platform are you using? Let me know in the comments about your experiences.
Senior Software Engineer @LeanIX. Co-founder of Sedeo. Passion for software engineering and startups. Looking forward to build great things. ÊúâÈõ£„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇüöÄ
1K 
6
1K¬†claps
1K 
6
Advice for programmers. Here‚Äôs why you should subscribe: https://bit.ly/bp-subscribe
"
https://medium.com/hackernoon/making-sense-of-azure-durable-functions-645ecb3c1d58?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Stateful Workflows on top of Stateless Serverless Cloud Functions ‚Äî this is the essence of the Azure Durable Functions library. That‚Äôs a lot of fancy words in one sentence, and they might be hard for the majority of readers to understand.
Please join me on the journey where I‚Äôll try to explain how those buzzwords fit together. I will do this in 3 steps:
Traditionally, server-side applications were built in a style which is now referred to as Monolith. If multiple people and teams were developing parts of the same application, they mostly contributed to the same code base. If the code base were structured well, it would have some distinct modules or components, and a single team would typically own each module:
Usually, the modules would be packaged together at build time and then deployed as a single unit, so a lot of communication between modules would stay inside the OS process.
Although the modules could stay loosely coupled over time, the coupling almost always occurred on the level of the data store because all teams would use a single centralized database.
This model works great for small- to medium-size applications, but it turns out that teams start getting in each other‚Äôs way as the application grows since synchronization of contributions takes more and more effort.
As a complex but viable alternative, the industry came up with a revised service-oriented approach commonly called Microservices. The teams split the big application into ‚Äúvertical slices‚Äù structured around the distinct business capabilities:
Each team then owns a whole vertical ‚Äî from public communication contracts, or even UIs, down to the data storage. Explicitly shared databases are strongly discouraged. Services talk to each other via documented and versioned public contracts.
If the borders for the split were selected well ‚Äî and that‚Äôs the most tricky part ‚Äî the contracts stay stable over time, and thin enough to avoid too much chattiness. This gives each team enough autonomy to innovate at their best pace and to make independent technical decisions.
One of the drawbacks of microservices is the change in deployment model. The services are now deployed to separate servers connected via a network:
Networks are fundamentally unreliable: they work just fine most of the time, but when they fail, they fail in all kinds of unpredictable and least desirable manners. There are books written on the topic of distributed systems architecture. TL;DR: it‚Äôs hard.
A lot of the new adopters of microservices tend to ignore such complications. REST over HTTP(S) is the dominant style of connecting microservices. Like any other synchronous communication protocol, it makes the system brittle.
Consider what happens when one service becomes temporary unhealthy: maybe its database goes offline, or it‚Äôs struggling to keep up with the request load, or a new version of the service is being deployed. All the requests to the problematic service start failing ‚Äî or worse ‚Äî become very slow. The dependent service waits for the response, and thus blocks all incoming requests of its own. The error propagates upstream very quickly causing cascading failures all over the place:
The application is down. Everybody screams and starts the blame war.
While cascading failures of HTTP communication can be mitigated with patterns like a circuit breaker and graceful degradation, a better solution is to switch to the asynchronous style of communication as the default. Some kind of persistent queueing service is used as an intermediary.
The style of application architecture which is based on sending events between services is known as Event-Driven. When a service does something useful, it publishes an event ‚Äî a record about the fact which happened to its business domain. Another service listens to the published events and executes its own duty in response to those facts:
The service that produces events might not know about the consumers. New event subscribers can be introduced over time. This works better in theory than in practice, but the services tend to get coupled less.
More importantly, if one service is down, other services don‚Äôt catch fire immediately. The upstream services keep publishing the events, which build up in the queue but can be stored safely for hours or days. The downstream services might not be doing anything useful for this particular flow, but it can stay healthy otherwise.
However, another potential issue comes hand-in-hand with loose coupling: low cohesion. As Martin Fowler notices in his essay What do you mean by ‚ÄúEvent-Driven‚Äù:
It‚Äôs very easy to make nicely decoupled systems with event notification, without realizing that you‚Äôre losing sight of the larger-scale flow.
Given many components that publish and subscribe to a large number of event types, it‚Äôs easy to stop seeing the forest for the trees. Combinations of events usually constitute gradual workflows executed in time. A workflow is more than the sum of its parts, and understanding of the high-level flow is paramount to controlling the system behavior.
Hold this thought for a minute; we‚Äôll get back to it later. Now it‚Äôs time to talk cloud.
The birth of public cloud changed the way we architect applications. It made many things much more straightforward: provisioning of new resources in minutes instead of months, scaling elastically based on demand, and resiliency and disaster recovery at the global scale.
It made other things more complicated. Here is the picture of the global Azure network:
There are good reasons to deploy applications to more than one geographical location: among others, to reduce network latency by staying close to the customer, and to achieve resilience through geographical redundancy. Public Cloud is the ultimate distributed system. As you remember, distributed systems are hard.
There‚Äôs more to that. Each cloud provider has dozens and dozens of managed services, which is the curse and the blessing. Specialized services are great to provide off-the-shelf solutions to common complex problems. On the flip side, each service has distinct properties regarding consistency, resiliency and fault tolerance.
In my opinion, at this point developers have to embrace the public cloud and apply the distributed system design on top of it. If you agree, there is an excellent way to approach it.
The slightly provocative term serverless is used to describe cloud services that do not require provisioning of VMs, instances, workers, or any other fixed capacity to run custom applications on top of them. Resources are allocated dynamically and transparently, and the cost is based on their actual consumption, rather than on pre-purchased capacity.
Serverless is more about operational and economical properties of the system than about the technology per se. Servers do exist, but they are someone else‚Äôs concern. You don‚Äôt manage the uptime of serverless applications: the cloud provider does.
On top of that, you pay for what you use, similar to the consumption of other commodity resources like electricity. Instead of buying a generator to power up your house, you just purchase energy from the power company. You lose some control (e.g., no way to select the voltage), but this is fine in most cases. The great benefit is no need to buy and maintain the hardware.
Serverless compute does the same: it supplies standard services on a pay-per-use basis.
If we talk more specifically about Function-as-a-Service offerings like Azure Functions, they provide a standard model to run small pieces of code in the cloud. You zip up the code or binaries and send it to Azure; Microsoft takes care of all the hardware and software required to run it. The infrastructure automatically scales up or down based on demand, and you pay per request, CPU time and memory that the application consumed. No usage ‚Äî no bill.
However, there‚Äôs always a ‚Äúbut‚Äù. FaaS services come with an opinionated development model that applications have to follow:
Frankly speaking, the majority of existing applications don‚Äôt really fit into this model. If you are lucky to work on a new application (or a new module of it), you are in better shape.
A lot of the serverless applications may be designed to look somewhat similar to this example from the Serverless360 blog:
There are 9 managed Azure services working together in this app. Most of them have a unique purpose, but the services are all glued together with Azure Functions. An image is uploaded to Blob Storage, an Azure Function calls Vision API to recognize the license plate and send the result to Event Grid, another Azure Function puts that event to Cosmos DB, and so on.
This style of cloud applications is sometimes referred to as Serviceful to emphasize the heavy usage of managed services ‚Äúglued‚Äù together by serverless functions.
Creating a comparable application without any managed services would be a much harder task, even more so, if the application has to run at scale. Moreover, there‚Äôs no way to keep the pay-as-you-go pricing model in the self-service world.
The application pictured above is still pretty straightforward. The processes in enterprise applications are often much more sophisticated.
Remember the quote from Martin Fowler about losing sight of the large-scale flow. That was true for microservices, but it‚Äôs even more true for the ‚Äúnanoservices‚Äù of cloud functions.
I want to dive deeper and give you several examples of related problems.
For the rest of the article, I‚Äôll define an imaginary business application for booking trips to software conferences. In order to go to a conference, I need to buy tickets to the conference itself, purchase the flights, and book a room at a hotel.
In this scenario, it makes sense to create three Azure Functions, each one responsible for one step of the booking process. As we prefer message passing, each Function emits an event which the next function can listen for:
This approach works, however, problems do exist.
As we need to execute the whole booking process in sequence, the Azure Functions are wired one after another by configuring the output of one function to match with the event source of the downstream function.
In the picture above, the functions‚Äô sequence is hard-defined. If we were to swap the order of booking the flights and reserving the hotel, that would require a code change ‚Äî at least of the input/output wiring definitions, but probably also the functions‚Äô parameter types.
In this case, are the functions really decoupled?
What happens if the Book Flight function becomes unhealthy, perhaps due to the outage of the third-party flight-booking service? Well, that‚Äôs why we use asynchronous messaging: after the function execution fails, the message returns to the queue and is picked up again by another execution.
However, such retries happen almost immediately for most event sources. This might not be what we want: an exponential back-off policy could be a smarter idea. At this point, the retry logic becomes stateful: the next attempt should ‚Äúknow‚Äù the history of previous attempts to make a decision about retry timing.
There are more advanced error-handling patterns too. If executions failures are not intermittent, we may decide to cancel the whole process and run compensating actions against the already completed steps.
An example of this is a fallback action: if the flight is not possible (e.g., no routes for this origin-destination combination), the flow could choose to book a train instead:
This scenario is not trivial to implement with stateless functions. We could wait until a message goes to the dead-letter queue and then route it from there, but this is brittle and not expressive enough.
Sometimes the business process doesn‚Äôt have to be sequential. In our reservation scenario, there might be no difference whether we book a flight before a hotel or vice versa. It could be desirable to run those actions in parallel.
Parallel execution of actions is easy with the pub-sub capabilities of an event bus: both functions should subscribe to the same event and act on it independently.
The problem comes when we need to reconcile the outcomes of parallel actions, e.g., calculate the final price for expense reporting purposes:
There is no way to implement the Report Expenses block as a single Azure Function: functions can‚Äôt be triggered by two events, let alone correlate two related events.
The solution would probably include two functions, one per event, and the shared storage between them to pass information about the first completed booking to the one who completes last. All this wiring has to be implemented in custom code. The complexity grows if more than two functions need to run in parallel.
Also, don‚Äôt forget the edge cases. What if one of the function fails? How do you make sure there is no race condition when writing and reading to/from the shared storage?
All these examples give us a hint that we need an additional tool to organize low-level single-purpose independent functions into high-level workflows.
Such a tool can be called an Orchestrator because its sole mission is to delegate work to stateless actions while maintaining the big picture and history of the flow.
Azure Durable Functions aims to provide such a tool.
Azure Functions is the serverless compute service from Microsoft. Functions are event-driven: each function defines a trigger ‚Äî the exact definition of the event source, for instance, the name of a storage queue.
Azure Functions can be programmed in several languages. A basic Function with a Storage Queue trigger implemented in C# would look like this:
The FunctionName attribute exposes the C# static method as an Azure Function named MyFirstFunction. The QueueTrigger attribute defines the name of the storage queue to listen to. The function body logs the information about the incoming message.
Durable Functions is a library that brings workflow orchestration abstractions to Azure Functions. It introduces a number of idioms and tools to define stateful, potentially long-running operations, and manages a lot of mechanics of reliable communication and state management behind the scenes.
The library records the history of all actions in Azure Storage services, enabling durability and resilience to failures.
Durable Functions is open source, Microsoft accepts external contributions, and the community is quite active.
Currently, you can write Durable Functions in 3 programming languages: C#, F#, and Javascript (Node.js). All my examples are going to be in C#. For Javascript, check this quickstart and these samples. For F# see the samples, the F#-specific library and my article A Fairy Tale of F# and Durable Functions.
Workflow building functionality is achieved by the introduction of two additional types of triggers: Activity Functions and Orchestrator Functions.
Activity Functions are simple stateless single-purpose building blocks that do just one task and have no awareness of the bigger workflow. A new trigger type, ActivityTrigger, was introduced to expose functions as workflow steps, as I explain below.
Here is a simple Activity Function implemented in C#:
It has a common FunctionName attribute to expose the C# static method as an Azure Function named BookConference. The name is important because it is used to invoke the activity from orchestrators.
The ActivityTrigger attribute defines the trigger type and points to the input parameter conference which the activity expects to get for each invocation.
The function can return a result of any serializable type; my sample function returns a simple property bag called ConfTicket.
Activity Functions can do pretty much anything: call other services, load and save data from/to databases, and use any .NET libraries.
The Orchestrator Function is a unique concept introduced by Durable Functions. Its sole purpose is to manage the flow of execution and data among several activity functions.
Its most basic form chains multiple independent activities into a single sequential workflow.
Let‚Äôs start with an example which books a conference ticket, a flight itinerary, and a hotel room one-by-one:
The implementation of this workflow is defined by another C# Azure Function, this time with OrchestrationTrigger:
Again, attributes are used to describe the function for the Azure runtime.
The only input parameter has type DurableOrchestrationContext. This context is the tool that enables the orchestration operations.
In particular, the CallActivityAsync method is used three times to invoke three activities one after the other. The method body looks very typical for any C# code working with a Task-based API. However, the behavior is entirely different. Let's have a look at the implementation details.
Let‚Äôs walk through the lifecycle of one execution of the sequential workflow above.
When the orchestrator starts running, the first CallActivityAsync invocation is made to book the conference ticket. What actually happens here is that a queue message is sent from the orchestrator to the activity function.
The corresponding activity function gets triggered by the queue message. It does its job (books the ticket) and returns the result. The activity function serializes the result and sends it as a queue message back to the orchestrator:
When the message arrives, the orchestrator gets triggered again and can proceed to the second activity. The cycle repeats ‚Äî a message gets sent to Book Flight activity, it gets triggered, does its job, and sends a message back to the orchestrator. The same message flow happens for the third call.
As discussed earlier, message passing is intended to decouple the sender and receiver in time. For every message in the scenario above, no immediate response is expected.
On the C# level, when the await operator is executed, the code doesn't block the execution of the whole orchestrator. Instead, it just quits: the orchestrator stops being active and its current step completes.
Whenever a return message arrives from an activity, the orchestrator code restarts. It always starts with the first line. Yes, this means that the same line is executed multiple times: up to the number of messages to the orchestrator.
However, the orchestrator stores the history of its past executions in Azure Storage, so the effect of the second pass of the first line is different: instead of sending a message to the activity it already knows the result of that activity, so await returns this result back and assigns it to the conference variable.
Because of these ‚Äúreplays‚Äù, the orchestrator‚Äôs implementation has to be deterministic: don‚Äôt use DateTime.Now, random numbers or multi-thread operations; more details here.
Azure Functions are stateless, while workflows require a state to keep track of their progress. Every time a new action towards the workflow‚Äôs execution happens, the framework automatically records an event in table storage.
Whenever an orchestrator restarts the execution because a new message arrives from its activity, it loads the complete history of this particular execution from storage. Durable Context uses this history to make decisions whether to call the activity or return the previously stored result.
The pattern of storing the complete history of state changes as an append-only event store is known as Event Sourcing. Event store provides several benefits:
Here is an illustration of the notable events that get recorded during our sequential workflow:
Azure Functions on the serverless consumption-based plan are billed per execution + per duration of execution.
The stop-replay behavior of durable orchestrators causes the single workflow ‚Äúinstance‚Äù to execute the same orchestrator function multiple times. This also means paying for several short executions.
However, the total bill usually ends up being much lower compared to the potential cost of blocking synchronous calls to activities. The price of 5 executions of 100 ms each is significantly lower than the cost of 1 execution of 30 seconds.
By the way, the first million executions per month are at no charge, so many scenarios incur no cost at all from Azure Functions service.
Another cost component to keep in mind is Azure Storage. Queues and Tables that are used behind the scenes are charged to the end customer. In my experience, this charge remains close to zero for low- to medium-load applications.
Beware of unintentional eternal loops or indefinite recursive fan-outs in your orchestrators. Those can get expensive if you leave them out of control.
What happens when an error occurs somewhere in the middle of the workflow? For instance, a third-party flight booking service might not be able to process the request:
This situation is expected by Durable Functions. Instead of silently failing, the activity function sends a message containing the information about the error back to the orchestrator.
The orchestrator deserializes the error details and, at the time of replay, throws a .NET exception from the corresponding call. The developer is free to put a try .. catch block around the call and handle the exception:
The code above falls back to a ‚Äúbackup plan‚Äù of booking another itinerary. Another typical pattern would be to run a compensating activity to cancel the effects of any previous actions (un-book the conference in our case) and leave the system in a clean state.
Quite often, the error might be transient, so it might make sense to retry the failed operation after a pause. It‚Äôs a such a common scenario that Durable Functions provides a dedicated API:
The above code instructs the library to
The significant point is that, once again, the orchestrator does not block while awaiting retries. After a failed call, a message is scheduled for the moment in the future to re-run the orchestrator and retry the call.
Business processes may consist of numerous steps. To keep the code of orchestrators manageable, Durable Functions allows nested orchestrators. A ‚Äúparent‚Äù orchestrator can call out to child orchestrators via the context.CallSubOrchestratorAsync method:
The code above books two conferences, one after the other.
What if we want to run multiple activities in parallel?
For instance, in the example above, we could wish to book two conferences, but the booking order might not matter. Still, when both bookings are completed, we want to combine the results to produce an expense report for the finance department:
In this scenario, the BookTrip orchestrator accepts an input parameter with the name of the conference and returns the expense information. ReportExpenses needs to receive both expenses combined.
This goal can be easily achieved by scheduling two tasks (i.e., sending two messages) without awaiting them separately. We use the familiar Task.WhenAll method to await both and combine the results:
Remember that awaiting the WhenAll method doesn't synchronously block the orchestrator. It quits the first time and then restarts two times on reply messages received from activities. The first restart quits again, and only the second restart makes it past the await.
Task.WhenAll returns an array of results (one result per each input task), which is then passed to the reporting activity.
Another example of parallelization could be a workflow sending e-mails to hundreds of recipients. Such fan-out wouldn‚Äôt be hard with normal queue-triggered functions: simply send hundreds of messages. However, combining the results, if required for the next step of the workflow, is quite challenging.
It‚Äôs straightforward with a durable orchestrator:
Making hundreds of roundtrips to activities and back could cause numerous replays of the orchestrator. As an optimization, if multiple activity functions complete around the same time, the orchestrator may internally process several messages as a batch and restart the orchestrator function only once per batch.
There are many more patterns enabled by Durable Functions. Here is a quick list to give you some perspective:
Further explanation and code samples are in the docs.
I firmly believe that serverless applications utilizing a broad range of managed cloud services are highly beneficial to many companies, due to both rapid development process and the properly aligned billing model.
Serverless tech is still young; more high-level architectural patterns need to emerge to enable expressive and composable implementations of large business systems.
Azure Durable Functions suggests some of the possible answers. It combines the clarity and readability of sequential RPC-style code with the power and resilience of event-driven architecture.
The documentation for Durable Functions is excellent, with plenty of examples and how-to guides. Learn it, try it for your real-life scenarios, and let me know your opinion ‚Äî I‚Äôm excited about the serverless future!
Many thanks to Katy Shimizu, Chris Gillum, Eric Fleming, KJ Jones, William Liebenberg, Andrea Tosato for reviewing the draft of this article and their valuable contributions and suggestions. The community around Azure Functions and Durable Functions is superb!
Originally published at mikhail.io.
#BlackLivesMatter
1K 
6
how hackers start their afternoons. the real shit is on hackernoon.com.¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1K¬†claps
1K 
6
Written by
Cloud, Azure Functions, Serverless, F#, Functional Programming. Microsoft Azure MVP.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Cloud, Azure Functions, Serverless, F#, Functional Programming. Microsoft Azure MVP.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kamran-bilgrami/ethical-hacking-lessons-building-free-active-directory-lab-in-azure-6c67a7eddd7f?source=search_post---------7,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kamran Bilgrami
Jan 6, 2020¬∑21 min read
The majority of IT experts concur that Active Directory is the dominant approach for managing the Windows domain networks. This is why adversaries get attracted to discover and exploit vulnerabilities within the Active Directory echo system. In order to defend against those types of attacks, there is a need for practice grounds‚Ä¶
"
https://medium.com/@vivekraja98/building-end-to-end-covid-19-forecast-model-using-azure-ml-16da338864b3?source=search_post---------8,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vivek Raja
Jun 9, 2020¬∑7 min read
A few months before, we would have never thought how the entire would change due to the pandemic caused by a virus so-called COVID19 a.k.a Coronavirus. It has changed how the entire world works right now and we must salute to the people working in the frontline to prevent, cure us of this deadly virus.
As a Data & Cloud Enthusiast, I wanted to contribute my purpose in these times of dire situation and most importantly to empower people with AI/ML knowledge to contribute. I hope this tutorial article would serve one of its purposes. I have tried my best to simplify the process that you can build your own model at the end of this article. All you need to know is the basics of Python Programming to get started!
The website I had created is https://newcovid.herokuapp.com to visualize and get real-time information about COVID-19 status worldwide, India (State and District wise) and most importantly, the forecast of COVID-19 cases in India for next 7 days.
On you mark‚Ä¶.
First thing first, we need to get an Azure Account to get started. If you have one, that‚Äôs well and good. If not, don‚Äôt worry, Microsoft is giving a free trial to access Azure Cloud. Register and sign up here: https://azure.microsoft.com/free
We need to create a workspace for our project. Workspaces are Azure resources, and as such, they are defined within a resource group in an Azure subscription, along with other related Azure resources that are required to support the workspace.
In your Azure portal homepage, click ‚ÄúCreate a Resource‚Äù. Since we are going to do many ML tasks, now select ‚ÄúAI + Machine Learning‚Äù under Azure Marketplace column. Select the first option ‚ÄúMachine learning‚Äù. Now create a workspace by filling up the following details. Enter the details as shown the image. You are free to use any name for the Resource Group, Workspace. Select the appropriate subscription (Free trial if you are using one). Click ‚ÄúReview and Create once it is done‚Äù.
Since we need a compute instance to run the entire ML tasks, let‚Äôs create one. Go to ml.azure.com. On the left side, click ‚ÄúCompute‚Äù. Under Computer Instances, click New. Give a name for your Compute Instance and select Virtual Machine type as ‚ÄúSTANDARD_DS3_V2‚Äù. Wait a minute or two for your VM to spin up.
Click ‚ÄúJupyter‚Äù and that will take to Jupyter Notebooks.
Create a new folder by clicking ‚ÄúNew‚Äù on the right side. Navigate to the folder and then create new ‚ÄúPython 3.6 Azure ML‚Äù. Open the .ipynb notebook file you had just created.
We need to connect our load our workspace and set up a new experiment, say ‚Äúcovid_exp‚Äù. This can be done as shown below
We are going to create a script file which will be the input to the ML Pipelines. ( Don‚Äôt worry, we will come to back to Pipelines in the later section of the article)
The following lines of codes under this step must be executed in single notebook cell.
The script is going to perform the following actions under the run context of our ‚Äúcovid_exp‚Äù experiment.
First, we will create a new python script file called prediction.py and import all necessary libraries. You can run these lines of code in our .ipynb Python Notebook we are working on.
We need to define the run context for the experiment, followed by importing the COVID-19 Indian data from https://api.covid19india.org/data.json
Load the Time series data in Pandas data frame. We will be working on the daily number of cases of confirmed, recovered and deaths. Split the data as Train and test dataset with 0.75 ratios.
We can infer that this is a typical Time Series Forecast Problem BUT it is not. To model the spread(infection) and control(recovered) of infectious diseases, a mathematical model such as SIR (Susceptible-Infected- Recovered) Model. This section of the article is the playground of Data Scientists, you can come up with the models for this data.
To know more about SIR Model and its implementation, refer: https://www.kaggle.com/lisphilar/covid-19-data-with-sir-model
Write the forecast data to a local file called covid_pred.txt
The forecast data is available in covid_pred.txt must be uploaded to Azure Storage as a blob. This blob can we be accessed by Anonymous request to use the forecast data. The following steps need to be done to create a storage account and upload the blob.
4. Once the container is created, navigate inside the container. The Container is going to hold the uploaded forecast data as a blob. Click ‚ÄúUpload‚Äù and select an empty text file. Give a name for the blob .
5. Click ‚Äú‚Ä¶‚Äù of the created blob and select ‚ÄúGenerate SAS Token‚Äù. Leave the settings as follows. Copy the Blob URL (you need it later).
6. Coming back to our pipelinescript.ipynb. As our last step of our Python script file prediction.py is to upload the content of covid_pred.txt to the blob we had just created using SAS token. Complete the run of the experiment by run.complete()
A Pipeline object contains an ordered sequence of one or more PipelineStep objects. The pipeline is going to execute the script prediction.py
Allocate the compute cluster for the pipeline to be executed. You can use the same compute for this.
Create a Conda Environment for the pipeline by including the necessary libraries for execution. Name the environment, say ‚Äúcovid_env‚Äù
Now we need to define the pipeline Object which consists of the Pipeline step which is going to execute prediction.py on compute cluster.
Now the pipeline is ready and we need to execute the pipeline and publish it as ‚Äúcovid-pipeline‚Äù.
We can schedule the pipeline to be run at the time specified by us. For our problem statement, the SIR model will predict the new forecast for next 7 days every day. So the pipeline needs to be scheduled to run once a day using the pipeline id generated in above code.
Now execute all the cells in pipelinescript.ipynb. You can view the execution details in the output cell. Once the execution is over, we can notice two results
The blob forecast data may vary according to your model.
Now we need to make the blob to be accessible by applications. This can be done by an HTTP request. To make sure the data access safe, we need to modify CORS policy such that our application can access the blob.
Go to Storage Account in your Azure Portal ‚Üí Click the account ‚Üí Under Settings click CORS ‚Üí Update the URL and necessary information as below.
As my website wants to access the blob, I had allowed the ‚ÄòGET‚Äô method.
The blob now can be accessed with the SAS Blob URL we had generated in Step 4.
The plot of trends of daily confirmed cases + forecast and Daily recovered + forecast can be seen in the below two graphs hosted in the website
newcovid.herokuapp.com
Voila! Congratulations on creating an end to end ML model for COVID-19 forecast system using Azure ML. If you loved the article/tutorial, give claps, share to other devs as well.
Feel free to reach out to me regarding queries, suggestions, critics, appreciation, feedback through Email: vivekraja98@gmail.com, LinkedIn.
Microsoft Certified Azure Data Scientist, AI Engineer, Data Engineer Associate | Tech Speaker, mentor & Researcher| 15x Hackathon Winner|
See all (64)
5.3K 
6
5.3K¬†claps
5.3K 
6
Microsoft Certified Azure Data Scientist, AI Engineer, Data Engineer Associate | Tech Speaker, mentor & Researcher| 15x Hackathon Winner|
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@to_pe/deploying-create-react-app-on-microsoft-azure-c0f6686a4321?source=search_post---------9,"Sign in
There are currently no responses for this story.
Be the first to respond.
Toni Petrina
Jan 2, 2017¬∑3 min read
Creating React apps has never been easier with the advent of tools like create-react-app or next but deploying them is both easy and hard at the same time.
Since I usually deploy apps on Microsoft Azure, it was natural to deploy a shiny new frontend on an Azure Website. The CLI tool can build your React app that can be served as a simple static site since it consists only of a single html file, a single js file and a bunch of static images and style sheets.
Our first step is ensuring we have create-react-app installed on our system. To do that, run the following command:
To create a new project, simply invoke the tool with a project name:
This will create a new folder named AzureTest that will contain all the files used for developing and building your website. Let‚Äôs build the project using the following command:
After it is finished, a new folder named build is created which contains everything you need to run the website in production.
Let‚Äôs create a new website that will host our React app. Go into Azure Portal and create a new Web App. Let‚Äôs assume it is named AzureReactTest1 and before continuing download the publish profile which contains FTP credentials.
Once connected via the FTP client, copy the entire content of the build folder created earlier into the /site/wwwroot/ folder on your Azure Website.
Refresh the site in a browser and it should display the default page.
Once routing is added into the mix, problems appear. If routing is done on the client side, e.g. if you are using create-react-app and react-routerto build a single page app, the url site.com/section won‚Äôt work out of the box.
This means if we try to open the link above manually in a browser, we would get a 404 error page because the Azure is trying to find an index.html (or some other index.* named) file inside a section folder. To fix this, Azure needs to know how to pass the intended route to the one and only index.html placed at the site‚Äôs root. Create a new file in the /site/wwwroot folder named web.config with the following content:
This will tell Azure to rewrite all urls as if they are pointing to our root file and our SPA application can handle the links correctly.
Deploying React apps is as simple as deploying static sites ‚Äî provided one is aware of the routing problems. In the next post we‚Äôll setup a CI integration on our Azure website.
Apathy on the rise, no one cares! Will code for food!
See all (197)
1.1K 
26
No rights reserved
¬†by the author.
1.1K¬†claps
1.1K 
26
Apathy on the rise, no one cares! Will code for food!
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@matthewleak/deploying-a-static-website-to-azure-storage-using-azure-devops-fa0bed457d07?source=search_post---------10,"Sign in
There are currently no responses for this story.
Be the first to respond.
Matthew Leak
Mar 17, 2019¬∑5 min read
I had a recent requirement to host a static website for a Government project in MS Azure. There was also an additional requirement that all testing, building and deployment should be automated using CI/CD in Azure DevOps. I‚Äôm fairly new to the world of Azure after having used AWS for almost everything over the past 5+ years and so I wanted to use this as an outlet to document the process.
I decided to use Azure Storage over Azure App Service (S3 and Elastic Beanstalk being the respective AWS equivalents) as there were only a few static assets to host, such as an index.html file, some css and a couple of images. The infrastructure that comes out of the box with any PaaS (Azure App Service / Beanstalk) seemed pretty unnecessary here as I like to reduce my cost footprint wherever possible. (even when cost isn‚Äôt necessarily an issue ‚Ä¶)
As with any new project in Azure we want to create a Resource Group. This will allow us to logically group resources for this project -
The next step is to create a storage account where our static assets will be stored and used for our static website -
While in the Storage Account, navigate to ‚ÄúAccess Keys‚Äù and take a note of ‚ÄúKey #1‚Äù as we‚Äôll need this later.
Here we begin to build our pipelines. Navigate to https://devops.azure.com and if required click ‚ÄúCreate Project‚Äù. Give it a name, set your preferred visibility option and then click ‚ÄúCreate‚Äù.
With our DevOps project created, it‚Äôs time to start building the Build pipeline which will produce a .zip artefact that will be used by the Release pipeline that will be created in the next section.
For this article, we‚Äôll use the visual designer to build but I‚Äôll provide the YAML config at the end of this article.
With the build pipeline complete and a zip archive of our build directory being produced, we need to release it to Azure Storage. To accomplish the upload, we‚Äôll use the Azure CLI in the release pipeline as that‚Äôs what I‚Äôve had the best results with.
You‚Äôll notice that in the CLI script above, our destination is set to $web which is the storage blob that Azure automatically creates when you enable the static website feature in a storage account. Any object inside of this blob will be publicly accessible via the static website URL.
We would normally take advantage of pipeline variables and reference those in our CLI scripts instead of adding sensitive keys to the tasks directly. I felt they were out of scope of this article and are a self-explanatory topic if you‚Äôve ever used any other CI/CD tool.
A full reference to the Azure DevOps YAML schema can be found here along with a catalog of tasks that also provide YAML snippets.
Build Pipeline:
If this article has been useful to you, be sure to leave lots of claps! (you can leave up to 50!)
UK-based Technology Consultant working in London and Manchester.
1.8K 
13
1.8K¬†
1.8K 
13
UK-based Technology Consultant working in London and Manchester.
"
https://medium.com/young-coder/could-microsoft-azure-actually-win-the-cloud-18c78b8780fe?source=search_post---------11,"There are currently no responses for this story.
Be the first to respond.
Being the first mover in a new field is a calculated risk. If you get it right (think Apple‚Äôs iPhone), you can own the market for years. But if you misjudge the technical challenges or your customers‚Äô needs, you‚Äôll end up with an expensive flop (think Apple Newton).
"
https://medium.com/@kyleake/exam-az-900-microsoft-azure-fundamentals-most-complete-preparation-guide-ever-76614d31a59c?source=search_post---------12,"Sign in
There are currently no responses for this story.
Be the first to respond.
Korkrid Akepanidtaworn (Kyle)
Aug 1, 2019¬∑60 min read
‚ÄúThis exam is designed for candidates looking to demonstrate foundational level knowledge of cloud services and how those services are provided with Microsoft Azure. The exam is intended for candidates with non-technical backgrounds, such as those involved in selling or purchasing cloud based solutions and services or‚Ä¶
"
https://medium.com/swlh/10-great-courses-for-aws-google-cloud-and-azure-ec89bef8a078?source=search_post---------13,"There are currently no responses for this story.
Be the first to respond.
As 2019 comes to an end, it‚Äôs that time of year when we look to setting new goals, and focusing on what we want to learn next year.
As engineers many of those goals revolve around keeping up with every new technology and framework. In particular, technologies such as AWS, Azure and Google Cloud all‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/statuscode/getting-key-vault-secrets-in-azure-functions-37620fd20a0b?source=search_post---------14,"There are currently no responses for this story.
Be the first to respond.
NOTE: Updated on 11/28 to reflect new key vault and function capabilities
One of the common questions around building Azure Functions is how to deal with secrets that a function needs. We know we shouldn‚Äôt be putting secrets in the code. Another sometimes satisfactory alternative is place secrets as an app setting. However, since these app settings are retrievable via clear-text through the API/Portal, and would need to be replicated to each function app that needs the secret, some prefer to leverage Azure Key Vault.
Here are the steps to build an Azure Function which can retrieve secrets at runtime. The below uses C# compiled class libraries, but the same underlying pieces can be used with any Azure Function language or binding.
I followed the instructions here to create a key vault in my Azure Subscription. After the key vault was created I ran this command to add the secrets to the vault.
Now authorized identities can retrieve this secret as needed in a central, secure location. One way to access is create my own custom application identity, or ‚Äúservice principal‚Äù, which I could use to access the secret. The rub with that approach though is in order to retrieve a valid token for a service principal I need to provide an application secret ‚Äî which itself is‚Ä¶ a secret. With the announcement of Azure managed service identities, I can now give my resource (in this case, a function) an identity and give it permissions just as it were another user.
After creating the Azure Function, I enabled a managed service identity for the function app. This will now add an identity in my Azure active directory I can give permissions to any resource I may need.
After enabling the managed service identity, I went into my key vault and added an access policy so my Azure Function app had permissions to read secrets.
The last step is to fetch the secret when I run the app.
Starting in the fall of 2018 I don‚Äôt have to make any code changes in order to access secrets in key vault.
Here‚Äôs a sample Azure Event Hub triggered function that makes a simple log message. In this case this app has two secrets: the Event Hubs connection string, and the private variable superSecret.
When running locally I can use local.settings.json to have local stored values to develop and test off of. This can connect to a development Event Hub and use some development secret.
However, once I publish, I just have to modify the Application Settings to resolve the right values as secrets.
In the Azure Portal (or via Visual Studio / VS Code / CLI / whatever tool you want to use to manage app settings), I need to associate some app settings to correspond to the secrets in key vault. To do that I will create two new application settings for SuperSecret and EventHubConnectionString that mirror the local.settings.json ‚Äî but instead of having the secrets in the app settings, I will point to where the secrets are in my key vault. It should be noted that the key vault secret ID is itself not a secret, it‚Äôs just an address of where the secret can be found.
I browsed to my key vault and found the secrets I needed and copied their address or Secret Uri (it looks something like https://{name}.vault.azure.net/secrets/{secret}/{id}) and then added special app settings that will fetch the secret from that source when the app instantiates on an instance. This means the secret only gets fetched once per instance, and not every execution (so I don‚Äôt have to worry about key vault throttles).
The app setting key is EventHubConnectionString and the value is something like @Microsoft.KeyVault(SecretUri={theSecretUri}).
When done configuring, here‚Äôs what my app settings looked like to get my secrets:
If I wanted to, I could make any of these app settings into key vault references, including things like the AzureWebJobsStorage account.
Once I click save, I‚Äôm done. The function instance will restart, authenticate to key vault using the managed service identity I configured, resolve the key vault secrets and store them in the appropriate environment variables, and my code can now trigger on the Event Hub and log the message as written.
Keeping developers informed.
531 
23
531¬†claps
531 
23
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Keeping developers informed.
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Keeping developers informed.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ngconf/angular-on-azure-part-i-d842e8f76462?source=search_post---------15,"There are currently no responses for this story.
Be the first to respond.
Using Azure Pipeline
Developers test software in a local environment using servers such as LAMP, WAMP, node, IIS, and Apache. Local deployment is good in terms of fast development and debugging, but we can‚Äôt ship our physical machines to the client in order to access to the application üòÇ. We have to deploy an application to a web server/cloud in order to make it accessible to the end user on their preferred platform (mobile, desktop, etc).
A variety of cloud providers exist in the current market, the most popularbeing Microsoft Azure, Google Cloud Platform, AWS. These providers offer an unbeatable combination of ease, speed, and automation, so if you have not deployed using such a platform, then this is the article for you! This article focuses on deploying an Angular app to Azure.
The action of bringing resources into effective action
In web development, deployment is concerned with making our static/dynamic resources available over the wire so the end user can access them in a desired device, using a browser.
Web hosting is a service that allows organizations and individuals to post a website or web page onto the Internet.
The deployment process is incomplete without hosting.
Deployment exposes your web application using a philosophy that has beenfollowed for year. The diagram below outlines typical deployment steps that could be applied to any type of software.
Azure is a cloud platform service which provides cloud services, including those for compute, analytics, storage, serverless, AI and ML, IoT, containers, DevOps, mobile, and networking. It is widely considered as both a PaaS and IaaS product. This article covers the development, deployment, and DevOps aspects of the platform.
If you are new to Azure, a free subscription is available for those wishing to try the platform without any commitment.
The Azure App Service is part of the PaaS section of the platform. It easily builds and deploys a highly available web app to the cloud. Multiple features are available right out of the box, as illustrated below.
The important steps are numbered in the above screenshot. If a resource group is not created, then do so in step 3. Also, if you do not have a service plan, create one at this time. Make sure you select ‚ÄòASP .NET 4.7‚Äô in the ‚ÄòRuntime Stack‚Äô option in step 5. For more information, follow the guide for the detailed creation of Azure Service Plan and Azure App Service.
Once you‚Äôre done with the fill in details, click on ‚ÄúReview and create‚Äù button and then on the next screen press ‚ÄúCreate‚Äù button. To see the newly created resource you can click on ‚ÄúAll Resources‚Äù option in the sidebar.
The following url can be loaded to check if the recently deployed application is available in the cloud, https://<app-name>.azurewebsites.net/
In my case I used app name as ‚Äúangular-deployment‚Äù so URL would become https://angular-deployment.azurewebsites.net/
But, before moving forward, we should minimize the final bundle size of theapplication. No worries; that process is discussed in a later section.
Angular CLI tooling is incredible; simply executing ng serve compiles angular code to Javascript and generates bundle files. For a simple hello-world app, however, the total file size is far short of desirable.
Angular currently offers two compilation modes
In short, JIT ships the Angular compiler over the wire and component templates are compiled inside the browser. AOT mode precompiles all templates and makes the resulting JS available for further optimization before shipping the bundled application over the wire. Smaller application sizes and quicker response makes for better UX!
For those new to the Angular CLI, AOT mode is enabled with the command
ng build --prod
This command compiles all templates, then applies tree-shaking, optimization, minification, and bundling to create a final, highly-optimized package. All distribution files are automatically placed in the dist folder of your project, which can be directly hosted to a cloud provider.
In this article, Azure DevOps (formerly known as VSTS) is used to deploy an application to the cloud.
If you have not created an organization, then do so before clicking the ‚ÄòCreateProject‚Äô button, as shown in the above diagram. This displays the ‚ÄòCreate NewProject‚Äô dialog.
In ‚ÄúCreate new project‚Äù screen, fill project name, description and select visibility (I selected private). Also, version control was set to ‚ÄòGit‚Äô, and ‚ÄòWork item process‚Äô defaulted to ‚ÄòAgile.‚Äô Then, click the ‚ÄòCreate‚Äô button.
The dashboard page is displayed after a project is created. Several actions maybe performed from the dashboard sidebar.
The most important feature in the above list for purposes of this article is theAzure Pipelines setup.
Select the ‚ÄòPipelines‚Äô option from the left sidebar, which displays the ‚ÄòNew Pipeline‚Äô button in the middle of the screen. The following dialog is displayed after clicking the ‚ÄòNew Pipeline‚Äô button.
Pipelines are created with yaml files. A new yaml file may be created with avisual tool or by using the ‚ÄòUse the classic editor‚Äô link at the bottom of the dialog.
The next step is selecting a repository resource, which can be a new repository(above) or using an existing repo as shown below. I‚Äôm using my existing Github repo, so I selected ‚ÄòGithub‚Äô at this stage. To select a Github repo, click on the ‚Äò‚Ä¶‚Äô Button to browse repositories.
Select the desired repository for deployment. In this example, I selected the‚Äòweather-cast‚Äô repo. Then, click the ‚ÄòSelect‚Äô button.
At this point, you are very close to creating a brand new pipeline! By default, the ‚Äòmaster‚Äô branch is selected. Click on the ‚ÄòContinue‚Äô button.
Now, you‚Äôve made it to the final page of pipeline creation! Next, we create a ‚ÄòJob‚Äô, or the steps involved in the actual deployment. For now, just select ‚ÄòEmpty Job‚Äô to create a blank Job with no content. Don‚Äôt worry, we will add steps for it in the next section.
After the pipeline is created, you will see a screen where an Agent pool isassigned to run a job when any tasks are to be deployed. For this tutorial, we are going to setup deployment tasks under the ‚ÄòAgent Job 1.‚Äô Simply click on the ‚Äò+‚Äôbutton in the dialog.
Cool! We‚Äôve finally made it to the stage where we can add tasks for thedeployment job! Refer to the following screen shot.
After clicking the ‚Äò+‚Äô icon beside ‚ÄòAgent Job 1,‚Äô you can search by ‚Äònode‚Äô in the list (item 1 in the screen shot) then select ‚ÄòNode Tool Installer.‚Äô When that dialog displays (item 2), click the ‚ÄòAdd‚Äô button (item 3 in the above screenshot).
This displays the first task in the ‚ÄòAgent job 1‚Äô list. Next, fill in the details for this task. Enter display and version spec, as shown above. This configures NodeJS on our VM.
As before, search for ‚Äònpm‚Äô in the task list and then click the ‚ÄòAdd‚Äô button. Fill in the details as shown above to install the Angular CLI as the next step in the task list.
Continue the same process as above to create a task that installs all npmdependencies.
Again add npm ask and fill in the details shown above. This time select command as in ‚Äúcustom‚Äù, and ‚Äúcommand and arguments‚Äù would be run build. Basically, it calls ng build --prod command written as scripts in . This task helps to create a production ready angular package.
This is the task that creates the production-ready Angular package.Continue as before using the details shown in the above screenshot. ‚ÄòCommand‚Äô is ‚Äòcustom‚Äô and the ‚ÄòCommand and arguments‚Äô input is ‚Äòng build ‚Äî prod‚Äô. This causes the ng build --prod command to be written in the ‚Äòscripts‚Äô section of the package.json file.
Next, search for ‚ÄòAzure App Service Deploy‚Äô and add it to the task list. Fill in the details as shown below. This task hosts and deploys code to the server.
After you have finished entering details, click on the ‚ÄòSave and queue‚Äô button.This saves and subsequently runs the pipeline. You will see a message with ahyperlink containing a build number. Or, you may navigate to ‚ÄòPipelines > Builds‚Äô to see the following screen.
After the job is finished, we can check it as shown below.
This article outlined the steps to deploy an Angular application to Azure directly from Github or another repository. An Azure pipeline is a powerful feature to setup and visualize a deployment job with minimal effort.
If you like this article press üëè clap button 50 times or as many times you want. Feel free to ask a question if you have any. Thanks a lot for reading!
Soon, I will release part II of this article which covers CI and CD with AzurePipeline. Stay tuned!
For more Angular goodness, be sure to check out the latest episode of The Angular Show podcast.
Come hear top community speakers, experts, leaders, and the Angular team present for 2 stacked days on everything you need to make the most of Angular in your enterprise applications.Topics will be focused on the following four areas: ‚Ä¢ Monorepos ‚Ä¢ Micro frontends ‚Ä¢ Performance & Scalability ‚Ä¢ Maintainability & QualityLearn more here >> https://enterprise.ng-conf.org/
The World‚Äôs Best Angular Conference
2.1K 
6
Get up-to-date info, news, special offers and more from ng-conf! Join us for ng-conf 2022 March 16-18. ¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2.1K¬†claps
2.1K 
6
Written by
üë®‚Äçüíª Virtusa | MVP | GDE | Tech Savvy
The World‚Äôs Best Angular Conference
Written by
üë®‚Äçüíª Virtusa | MVP | GDE | Tech Savvy
The World‚Äôs Best Angular Conference
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@brentrobinson5/containerised-ci-cd-pipelines-with-azure-devops-74064c679f20?source=search_post---------16,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brent Robinson
May 23, 2019¬∑15 min read
It starts simple ‚Äî one application, one technology, one build server ‚Äî and CI/CD is working flawlessly. Time and technology move on, complexity grows, and your build server, while stable, is loaded with numerous versions of different tools. One day, a new tool is required, which cannot co-exist with others, and before long, build servers begin to exist for specific projects which match their precise needs. Perhaps you‚Äôre not as fortunate, and new projects are held back, forced to use legacy tools because the effort to upgrade all other projects is too much.
"
https://koukia.ca/keep-calm-and-use-azure-application-insights-ff38e04a43fd?source=search_post---------17,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
If you have been following me here, I started writing about Azure Application insights 4 or 5 years ago on my blog when it first came out and I was really impressed by its power even then, but now after several years, I think it has made a lot more improvements so it‚Äôs time for another closer look and some of the new features it has.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://serifandsemaphore.io/azure-cloud-functions-vs-aws-lambda-caf8a90605dd?source=search_post---------18,"Microsoft announced their answer to AWS Lambda at their Build 2016 conference. Brace yourself for the next cloud hosting change; because, it now seems like everyone is going ‚Äúserverless.‚Äù Amazon Web Service‚Äôs Lambda, Google‚Äôs Cloud Functions, IBM‚Äôs OpenWhisk, and now Azure‚Äôs Functions.
There‚Äôs too much to compare in one article, so I‚Äôll write about the differences in a series of articles. I‚Äôve had the opportunity to play with some. Google‚Äôs Cloud Functions are in private alpha, so I can‚Äôt share much about them, but Microsoft‚Äôs are in public preview.
I‚Äôve been using Lambda a lot lately and it‚Äôs been a game changer.
While Azure‚Äôs cloud function usage is billed the same way, the service is radically different than Lambda under the hood.
Before you make any major decisions, keep in mind that these services are evolving.
This will change over time for every cloud function service.
Azure Functions supports Node.js, C#, F#, Python and PHP. They also list Java, bash, and batch but it‚Äôs not clear how to use these (again, it‚Äôs in preview).
AWS Lambda supports Node.js, Python, and Java for now.
I think PHP support is a very strategic choice given its popularity, but I wish one of these services would better support Go.
Why Go? Because it runs fast and its code is easy to read. It is a language of brevity and therefore fits well for the cloud and (micro)services. Add Rust and Swift to the list too.
It‚Äôs important to note that you can execute binaries in both services through one of these languages. This is the method in which Go works in Lambda. Both Apex and also Sparta are serverless frameworks that let you use Go in Lambda as well.
It would be great for a cloud function service to support Go natively. Swift for mobile developers who often need simple back-end services would also be a smart move.
No amount of fancy devops tricks or tooling will save you from having to use the web dashboard at least every now and then.
Azure‚Äôs Function usage charts are still being developed, but they will be there and at least on par with Lambda‚Äôs.
Azure‚Äôs new dashboard is beautiful.
Once you understand the organizational structure of Azure Function it becomes a little easier. The same can be said for AWS Lambda too.
One important difference is the editor. Both are on par when it comes to the function‚Äôs settings screen. However, Azure has the more robust Visual Studio Online which can also be used.
Visual Studio Online is found under the Function‚Äôs App Service‚Äôs tools section. It‚Äôs worth noting that there are some other cool tools here too, including a console.
Most of your development time should be outside of dashboards anyway. I don‚Äôt consider them a huge factor in my decision making process but they can offer convenience.
Let‚Äôs dive into the architecture of Azure Functions compared to Lambda.
An ‚ÄúApp Service‚Äù is a container, or environment, for a set of Azure Functions. This is much different than Lambda. In fact, the two services couldn‚Äôt be more different.
Lambdas are organizationally independent, where Azure Functions are grouped logically into an ‚Äúapplication.‚Äù
This App Service can either by ‚Äúdynamic‚Äù or ‚Äúclassic.‚Äù The dynamic option is where you pay only for the time and memory your functions use. This is the biggest similarity between Lambda and Azure Functions.
It‚Äôs important to note that the memory you allocate is per app service (with potentially many functions), not per function like Lambda.
I think of the ‚Äúclassic‚Äù App Service more like Amazon ECS where you have EC2 instances running to handle the tasks or functions. The pricing model for this is then like EC2.
Azure is actually more like a blend between ECS Tasks and Lambda. For example, you can set environment variables on App Services which are then available for your Azure Functions. AWS Lambda cannot persist environment variables, but ECS Tasks can (correction/update: Lambda now can as pointed out by Jeremy Axmacher, thank you).
The entire container architecture is different. Lambdas provision a brand new one and deploys your code (from a zip file) on a cold request. Subsequent requests can be subject to container re-use and be handled much faster. However, you need to understand there is no persistence and with Node.js Lambdas you need to watch your variable scope because the container can be re-used.
However, Azure Functions are less subject to the cold/warm request effects. Azure still provisions resources as needed, but your files aren‚Äôt ‚Äúfrozen‚Äù somewhere in a zip file. They run on top of Azure‚Äôs WebJobs.
Azure Functions are also supposed to be accessible via FTP by connecting to the App Service‚Äôs FTP. Though I couldn‚Äôt get it to work so far during my review (no matter how many times I reset my credentials).
Azure Functions of course is a Windows system (32bit for dynamic services) while Lambda is a Linux system. Though I do hope that changes since Ubuntu can now run on Windows. In fact, that would put Azure in a good position with cloud hosting.
Speaking of working with the files, you can deploy your Azure Functions in a variety of ways:
You have many more inherent options than AWS Lambda, but I didn‚Äôt explore any sort of CLI deployment. Lambda has some great 3rd party tools for deploying Lambdas and both cloud services have SDKs to make just about anything possible.
I do have to hand it to Microsoft here though. You can hook up GitHub, even with your favorite CI tool, and easily deploy your cloud functions to Azure. Release managers rejoice.
Though it‚Äôs important to note that Lambdas can be versioned whereas Azure Functions can not (at least not yet). So if you wanted to rollback, you‚Äôll need to rely on Git or some other version control system.
One challenge with Lambda is development workflow. People have created tools to test your Lambda locally so you can write code, run, and then even deploy if all looks good.
There are also several ‚Äúserverless frameworks‚Äù that help with this process too.
Regardless, Lambda is closed source. However, Azure Function‚Äôs runtime is open-source. I would expect some clever CI integrations in the future.
I also have to concede that Azure Functions make things a bit easier in this department too. Every function automatically maps to an HTTP endpoint if enabled. Whereas with Lambda, you must configure API Gateway separately.
API Gateway is fine, but complex and time consuming. Again, some serverless frameworks ease this pain point by automatically setting up an API for Lambdas.
Microsoft gets points for UX because you have a lot less to configure. I‚Äôm not sure you can run Azure Functions from multiple triggers though. It doesn‚Äôt appear that way, but maybe by editing the function.json directly you can.
I would say that AWS Lambda has a slight edge in flexibility here, but both services are constantly changing.
The Azure app and function names are reflected in the URLs to trigger the cloud functions. This helps avoid confusion, but might also be limiting.
Apparently all HTTP methods are supported (GET, POST, etc.) through the endpoint assigned to each cloud function. With API Gateway, you need to configure each method manually.
One thing I don‚Äôt see is path variables. It seems that everything must be passed as a querystring or in the request body.
To be fair, it‚Äôs called an ‚ÄúHTTP trigger‚Äù so when thinking about it in those terms, it makes sense. If you need more robust settings, then you‚Äôre talking about ‚ÄúAPI configuration and management.‚Äù
Azure Functions also have super easy authentication. You can protect the cloud function endpoints with 3rd party authentication; this includes Facebook, Twitter, Google, and of course Microsoft‚Äôs auth.
I‚Äôve never setup Facebook auth for an HTTP endpoint so quickly and easily before in my life.
You can configure CORS and you can also manage your own domain name for the endpoints. Though I couldn‚Äôt see if there was a way to limit the HTTP request methods. I imagine with Azure‚Äôs API management service more will be possible, but then you‚Äôre going through as much work as API Gateway.
I don‚Äôt think there‚Äôs a clear ‚Äúwinner‚Äù here. Both Microsoft and Amazon have powerful features for triggering cloud functions with HTTP requests. Though I definitely think Amazon could learn a thing or two about UX from Microsoft.
I was thoroughly impressed by Azure and I think it‚Äôs in part due to its new dashboard. The new dashboard and the process of setting up cloud functions in a logical group called an ‚Äúapp‚Äù makes complete sense.
To illustrate how obvious this pain point is, look no further than Serverless (formerly JAWS) and Apex. These are two serverless frameworks that address this specific organizational concern. They help a developer logically group functions.
Another pain point with Lambda is the maximum execution time limit. It wasn‚Äôt always clear and many people never thought their functions would run that long (at least in my experience).
The time limit used to be 1 minute for Lambda, but increased to 5 to help with ‚ÄúETL‚Äù operations according to Amazon. I‚Äôm happy for the time limit increase, but even 5 minutes may not be sufficient for all operations.
Iron.io pointed out that their workers had no time limits in a comparison with Lambda. Iron workers are another option, though I stayed the course with AWS Lambda. I even found a way to re-purpose your Lambda code as ECS Tasks to cope with the 5 minute limit.
While that solved my own problems with the time limit, I understand that it‚Äôs not a solution for everyone. It also requires EC2 instances which are billed in a different fashion. So it‚Äôs not a perfect solution if ‚Äúpay as you go‚Äù is a top priority.
Azure‚Äôs Functions really are the perfect blend of Lambdas and ECS Tasks. You can write and execute the same exact code regardless of the environment and billing model. There are apparently no time limits either(so long as your function is not idle).
On the other hand, I imagine there could be some surprises in billing too. Lambda‚Äôs 5 minute execution time limit does serve to protect you from expense. Will Microsoft keep billing you for some accidental loop in a cloud function? Assuming the same amount of resources were used, will the cost work out to be about the same regardless of billing models? It‚Äôs perhaps too early to say since it‚Äôs still in preview and there is no pricing information available yet.
There is some chatter about this concern though if you follow the GitHub issue.
As I explored Azure Functions and it‚Äôs underlying architecture (as best I could), I realized a few other things of note.
First, since there is persistence and no execution time limits, working with large files and ETL is easier. Just watch out for how cost effective this ends up being or not being.
With Lambda + API Gateway, the content-type of the response is set via API Gateway. With Azure Functions, the content-type is set by the code. This can be very useful, but could also lead to some bloated functions.
Remember, your goal is not to build a complete application within a cloud function. It‚Äôs not really possible to do with Lambda‚Äôs limitations, but it is with Azure. So be careful not to fall into any bad practices.
Watch out for Azure Function‚Äôs limitation ‚Äî only 10 concurrent executions per function.
Again, Azure Functions run on a server with a persistent filesystem and webroot. It‚Äôs just not accessible to the outside world. However, it is accessible to you while logged in.
All of your code and assets can be seen via a URL like this: https://yourAppName.scm.azurewebsites.net/dev/api/files/wwwroot/HttpTriggerNodeJS1/index.js
This maps this location on disk: /home/site/wwwroot/HttpTriggerNodeJS1
Is there a cost to request these files this way? Can you somehow open this to the public? I don‚Äôt know. It definitely might raise an eye-brow or two though.
You have more insight, access to and control over the environment running your cloud function than you do with AWS Lambda‚Ä¶For better or worse.
Speaking of insight, when listing the directory of D:\home\site\wwwroot where your files are, it indicates that there is over 5 terabytes of disk space available!
This is much more than Lambda‚Äôs default ephemeral disk space of 500MB. There isn‚Äôt as much info about Azure‚Äôs Cloud Functions limitations just yet, but it is important to note that the instance limit is 10 while using ‚Äúdynamic‚Äù service apps (up from 4). The concurrent execution limit per instance is a bit unknown. Lambda‚Äôs limit is 100 concurrent executions total (which is a soft limit).
Is everyone‚Äôs code on the same storage volume? Or does each Azure account gets its own, very large, storage volume? I‚Äôm not familiar enough with what‚Äôs going on to know, but exploring is fun.
I would encourage everyone to explore and try each of these services. Understand why and when they they can help you. The end result can be a game changer for you too.
The cloud has powerful magic, but not all clouds are the same. Get to know the environment your code lives in as well as the capabilities of the services available to you. Read the fine print. Understand the limitations and tradeoffs.
Only with this information can you make an informed decision. Remember, your choice will be the path less taken here.
My choice? The jury is still out. I use a variety of services because I believe there is strength and opportunity in diversity.
I primarily use Lambda and ECS because it‚Äôs well established. I love AWS and will continue to use it happily, but I definitely think these companies can learn a thing or two from each other.
There‚Äôs a lot of opportunity in this fast changing landscape. There aren‚Äôt many books. Few best practices. Barely any frameworks. Less than ideal monitoring and debugging. There will be trial and error. You are subject to the possibility of having to re-build code and move your operations. If it gives you any solace, you will be in good company. Some major companies and smart developers are on board with cloud services and the (micro)service movement. This is the new frontier on the web. Buy the ticket, take the ride.
The best-laid plans of unicorns and men‚Ä¶
357 
8
357¬†claps
357 
8
Written by
Product Person
The best-laid plans of unicorns and men‚Ä¶
Written by
Product Person
The best-laid plans of unicorns and men‚Ä¶
"
https://medium.com/@dancurrotto/creating-a-devops-pipeline-to-deploy-docker-containers-using-azure-kubernetes-service-and-89a18365aedb?source=search_post---------19,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dan Currotto
Jan 7, 2019¬∑8 min read
Of course you know Bill Gates is no longer the richest man in the world. It is now his nemesis over at Amazon. Bill Gates is not running Microsoft but has a full time job giving his money away faster than anyone can spend it.
Meanwhile the Cloud War blazes on‚Ä¶
And I was sure the book seller had it in the bag until I began looking at Microsoft‚Äôs offering for Kubernetes.
And again, they‚Äôve greatly simplified something that is very complex in a way that‚Ä¶ well, only Microsoft can.
Kubernetes is a container-orchestration system that automates deployment, scaling and management of containerized applications.
It was originally designed and used at Google and is now maintained by the Cloud Native Computing Foundation.
It is taking the country by storm.
Kubernetes orchestrates the deployment of containers onto clusters. And it does it in a way that is pretty magnificent.
Some of the tasks it performs:
¬∑ Horizontal scaling
¬∑ Service discovery and load balancing
¬∑ Automated rollouts and rollbacks
¬∑ Secret and configuration management
But it contains a lot of complexity and is not easy to do.
After a spending a lot of time working with Kubernetes on Aws, I decided to also take a look at what Microsoft offered.
I was not disappointed.
This 3 part article will explain and show how to get a .Net Core application up and running on Docker Containers in load balanced clusters using the Azure Kubernetes Service (AKS) and Infrastructure as Code.
It will be done inside a Continuous Deployment pipeline!!
And if you‚Äôre signed up for an Azure Free Tier, run your builds as public on Azure DevOps (formerly VSTS) and put your code on free Git repo, you can do this one and many like it for free!
The source code for this article is a public repo in my GitHub account.
You can just grab the repo and following along. If you like, fork it and use it.
Just realize that in this section with the resource group templates you‚Äôll have to use the templates that you will create in this series of articles.
This is what we‚Äôll be doing in 3 parts:
1. Create a Kubernetes cluster using the Azure Kubernetes Service, create an ARM template (Azure Resource Manager) from the service deployment and save it as Infrastructure as Code.
2. Create an automated build that will build the Docker container and push it out to Docker Hub.
3. Create an automated release that will create or update the clusters and apply the deployment to the cluster, which gets the containers with the .Net Core workload and runs them.
Part 1: Create a Kubernetes cluster using the Azure Kubernetes Service, create an ARM template (Azure Resource Manager) from the service deployment and save it as Infrastructure as Code.
The first thing we‚Äôll do is create a Kubernetes cluster using the Azure Kubernetes Service. We‚Äôre creating this cluster only to create an ARM template from it. Then we‚Äôll use the ARM template later in the DevOps pipeline to create and update the cluster.
Infrastructure as code (IaC) is the process of managing and provisioning infrastructure using resources definition files. This allows all the infrastructure, the VMs, networks, load balancers, etc. to be stored in files inside source control.
Create a Kubernetes cluster using the Azure Kubernetes Service
After creating your free tier in Azure or if you already have one, go to the Azure Portal.
The first thing we want to do is create a Resource Group. A Resource Group is an Azure resource that allows you to group other Azure resources inside of it. You can also delete the resource group to blow away all the resources inside of it at once.
Click on Create a resource.
Type in ‚Äòresource group‚Äô and hit enter.
Click on resource group when it comes up.
Click on the Create button. This begins the creation process.
Choose a unique name for your resource group and a Region. Here I am putting an ‚Äìrg on the end of my resource group name to specify it is a resource group. This is to make it easier to read and identify.
Review and create.
Click on ‚ÄòResource Groups‚Äô in the left menu. You‚Äôll see the resource group you just created. Click on the resource group to open it.
It will be empty.
Now let‚Äôs create our Kubernetes service. Go back to the Azure home page. Click on ‚ÄòCreate a resource‚Äô.
Type in ‚ÄòKubernetes‚Äô and hit enter.
Choose Kubernetes Service when it comes up.
Click the Create button. This begins the creation process.
Choose the resource group you just created and a cluster name. I like to choose these names in a way they will make sense when I am looking at them together later on. Also put in a short DNS name prefix. It can be anything.
Change the Node Count from 3 to 2. They are going to be replicas anyway.
A word of warning here: With a node count of 2, this is going to create 2 VMs. I will show you how to delete them at the end, but don‚Äôt go off and forget about them and just let them run. You‚Äôll burn through all your free tier minutes for the month or worse, if you have a card set up, it will start charging your card. Not to scare you away; it‚Äôs just something to be aware of.
Click Review + create.
After the validation passes, click on Create.
You‚Äôll see a moving line to indicate your cluster is being created.
Click on Resource Groups in the Menu. You‚Äôll see the resource group you created earlier but also there will be another one that is named similarly but has an MC_ prefix. There is where Azure will put the resources for the new cluster.
Click on it and you‚Äôll begin to see the new cluster resources as they are created. It looks like a lot but there is little or no charge for most of them. The VMs incur the most charge on your free tier minutes.
After the progress indicator stops, click on the icon and you see the deployment is complete. Your cluster has been created.
You‚Äôve got 2 resource groups with cluster resources in them. This includes everything that the Kubernetes service creates.
At this point you have all the resources for a cluster. You could break out a command line and do everything you need to do to get Docker containers running on your clusters.
But that‚Äôs not good enough for us. We want Continuous Integration/Continuous Delivery. We want the whole thing running in a pipeline.
So let‚Äôs get our infrastructure as code.
Open the primary resource group.
Click on Deployments in the side menu and click on the only deployment available.
Highlight the template and click on View template.
Here is our infrastructure as code. Click on Download.
Download this to your machine.
Unzip it. These are the ARM (Azure Resource Management) template files. We will use this in the Deployment section of your deployment pipeline to create and maintain our Infrastructure as Code.
Once you have created these template, you‚Äôll want to go back and delete the resource groups containing your clusters so you won‚Äôt incur charges. We will be creating them in Part 3 in the Release Pipeline using IaC (Infrastructure as Code)!
Go back to the Azure Portal. Click on Resource Groups.
Locate the 2 Resource Groups that were created that contain the cluster and related infrastructure and delete both of them.
I am a DevOps and Cloud Architect/Developer. I love learning new technologies and helping companies get the most out of them.
290 
4
290¬†claps
290 
4
I am a DevOps and Cloud Architect/Developer. I love learning new technologies and helping companies get the most out of them.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/200-practice-questions-for-azure-ai-900-fundamentals-exam-e981d28ce91d?source=search_post---------20,"There are currently no responses for this story.
Be the first to respond.
The exam AI-900 is a fundamental AI exam from Azure. According to the study guide here, Candidates for this exam should have a foundational knowledge of machine learning (ML) and artificial intelligence (AI) concepts and related Microsoft Azure services. This exam is an opportunity‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iizotov/azure-functions-and-event-hubs-optimising-for-throughput-549c7acd2b75?source=search_post---------21,"Sign in
There are currently no responses for this story.
Be the first to respond.
Igor Izotov
Jan 14, 2019¬∑9 min read
TL;DR. I take a fairly standard serverless event processing scenario ‚Äî an Azure Function triggered by messages in an Event Hub ‚Äî and talk about how how to optimise this system for throughput through: a) overall architecture b) EH partitioning, andc) tweaking the Event Hub trigger host.json settings: maxBatchSize, prefetchCount and batchCheckpointFrequency. I doubt anyone would be interested in reading a yet another theoretical opus, so for my reader‚Äôs benefit I ran a series of experiments which involved testing multiple combinations of Event Hub partition counts and host.json settings, and observing the impact on latency and throughput. Feel free to jump straight through to the approach and results published to Power BI.
In other words, I‚Äôm trying to help those individuals who are at present staring detachedly in the middle distance trying to understand:
Why is my Azure Function suddenly struggling to catch-up / lagging a few hours behind. I thought the stupid cloud would scale out my stuff automagically‚Ä¶
Near real-time event processing is powerful and also complex since you‚Äôre no longer dealing with a traditional once-a-day batch job (that can run late, be rerun if needed, etc.) but with a distributed event-driven system, triggered by individual events (or micro-/mini-batches).
In Azure, a textbook serverless streaming scenario would be an Azure Function App triggered by messages in an Event Hub. It‚Äôs a common pattern, especially when your data requires a transformation to make it palatable for the rest of your pipeline.
Imagine you‚Äôre dealing with a stream of gzipped xml messages (happens more often than you think!)
Azure Functions‚Äôs native Event Hub trigger will take care of firing your code in response to events in the stream. While the trigger works great out of the box, when hitting scale (>10K msgs/sec) you will be required to tweak a couple of settings in host.json and function.json. By the way, everything discussed here applies across all languages supported by the Azure Functions Runtime, v1 and v2 (the latter is recommended due to improved performance). Please mind the syntactic differences in host.json between the two versions though.
Azure Function‚Äôs Event Hubs trigger is not exclusive to Azure Functions. The trigger is built on the Event Processor Host that you can use when building your own Event Hub consumers running in VMs, Containers, Webjobs, etc.
When measuring latency, taking the average is almost never a good idea, it hides outliers; consider using a combination of percentiles and a maximum value (a good read if you want a deeper dive). Since we may not always have full control over event publishers and their network connection, decide whether you are going to measure end-to-end latency or pipeline latency (or both).
When measuring end-to-end latency you will require every message to contain a timestamp you can trust, also known as Application Time. When measuring pipeline latency, Event Hub‚Äôs enqueuedTime attribute reliably captures the time of arrival of each message. Please keep in mind that since the pipeline latency is based on messages‚Äô time of arrival, it‚Äôs usually not a bad idea to implement a policy for late arrival and out-of-order events.
When measuring throughput, decide if you‚Äôre counting messages or bytes, both approaches are valid (which one do you think would be a better fit if your payload size fluctuates significantly?). Also, it‚Äôs recommended to measure sustained throughput, or a running average over a sufficiently long period of time.
Some throughput metrics can be used out-of-the box, for instance Event Hub‚Äôs Incoming Messages and Outgoing Messages. One way of measuring pipeline‚Äôs latency is via custom metrics in Azure Application Insights. All that‚Äôs needed is a couple of extra lines of code to calculate the difference between EventHub‚Äôs enqueuedTime and your code‚Äôs invocation timestamp and push this value out to AppInsights.
Here‚Äôs a half-decent sample SLA statement for a streaming scenario: 99% of all events arriving in the Event Hub must be processed within the hot path within 1 second. The maximum processing delay should never exceed 30 seconds. The pipeline should be capable of processing up to 15K messages per second 24/7, provided the message payload does not exceed 1KiB
Let‚Äôs empirically measure how the throughput and latency is impacted by the number of EH partitions, batch and prefetch sizes. Since it will require quite a few iterations, let‚Äôs automate the process via Terraform.
For every iteration let‚Äôs create a 20 TU Standard Event Hub and an EH-triggered Azure Function. Let‚Äôs also provision a load generator to flood the EH with messages saturating the ingress. Our goal is to measure the resulting Throughput (T) and Latency (L) for various combinations of:- P: the number number of Partitions in the Event Hub- B: The maximum Batch size (maxBatchSize setting in host.json) and- R: The pRefetch Limit (prefetchCount setting in host.json)
Each iteration should be independent with a single EH per EH Namespace, one consumer group per EH. 20 Throughput Units should allow us to achieve a theoretical 20MB per second on the ingress (event publisher) side and 40MB per second on the consumer (Azure Function) side. In addition to the throughput allowance, the Throughput Unit also limits the number of ingress/egress events, each TU granting you ~1K ingress and ~4K egress events per second. The relationship between an ingress event and an actual message can be subtle and probably warrants a separate post. In short, if you‚Äôre sending events in batches one ingress event does not equal one message.
The number of Event Hub partitions does not define its theoretical throughput. There is no specific throughput limit on an Event Hub partition (‚Ä¶anymore)
Azure Function is running in a Consumption Plan on Windows performing a null operation on every message (it could‚Äôve been anything) and reporting the processing latency per batch of messages. You can report per individual message within each batch but you should assess how much more telemetry will be generated and adjust sampling accordingly.
If interested, look through the code and explore the results in Power BI or read on. To produce the Power BI report, I‚Äôm joining a couple of Event Hub metrics stored in Azure Log Analytics with the custom metrics in Application Insights via a cross-resource query in Kusto which in turn can be easily consumed in Power BI.
I‚Äôd call this ‚Äòexpected behaviour‚Äô: the pipeline‚Äôs latency (top chart) is in the hundreds of seconds in the beginning, as the Azure Function is gradually scaling itself out (bottom chart) until eventually it reaches 32 instances, which is precicely one consumer per partition per consumer group.
Remember, since we‚Äôre dealing with an Event Hub, it‚Äôs parallel consumers, not competing consumers
Once the Function is running at full throttle it eventually catches up with the backlog of messages in the Event Hub and the latency drops to around 220ms towards the end of the 15-minute run. The 5-min 95P and 99P latency values read 3.5 and 3.88 seconds respectively, if I had it running for longer these values would‚Äôve dropped further.
The middle chart shows the average message batch that my Azure Function was being triggered with. Interestingly, it stays well below the maxBatchSize=64 which indicates some spare capacity in the processing pipeline.
What if we increase maxBatchSize to 512?
The throughput is very similar to the run above but the pipeline catches up much quicker (understandably, look at the batch sizes in the beginning!). The 5-minute P95 and P99 latency measurements are both 180ms which is pretty low if you ask me.
Okay, what if we keep reduce the number of Event Hub partitions to 4?
Ouch. The pipeline does not seem to ever catch up: look at the growing latency and the difference between IncomingMessages (that is, how many messages were enqueued in the Event Hub) and OutgoingMessages (how many messages are processed by the Azure Function).
Why? Not enough parallelism. There‚Äôs only 4 partitions, hence 4 consumers for the same 20 Throughput Units. The two options to consider here are:
There‚Äôs 57 more runs published to Power BI here for you to have a play.
There‚Äôs (roughly) four areas to look at to improve your pipeline‚Äôs throughput and reduce latency: Event Publishers, Event Hub, Event Hub trigger settings and Azure Function code.
Some additional reading here and here
Thanks for reading and until next time!
Enterprise Solutions Architect @ AWS. Opinions shared are my own.
425 
6
425¬†
425 
6
Enterprise Solutions Architect @ AWS. Opinions shared are my own.
"
https://medium.com/@jeffhollan/in-order-event-processing-with-azure-functions-bb661eb55428?source=search_post---------22,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Feb 9, 2018¬∑6 min read
‚ö°Ô∏è UPDATE: Azure Service Bus now supports sessions, so you can do in order queue processing with service bus queues and topics in addition to Event Hubs listed below. Service Bus Sessions provide the added benefit of reprocessing failures individually instead of in batches. While Event Hubs can guarantee order as show below, if a partition lock is lost the in-order batch could resume in another instance causing duplicates. Consider using Service Bus Sessions if this is an issue. Both provide at-least-once delivery guarantees.‚ö°Ô∏è
I met with a customer this week who was interested in using serverless but didn‚Äôt know if it would work for their business requirements. Here was the scenario:
Thousands of events are being sent from hundreds of different users at any given time. Each of these events needed to be processed and analyzed, and the processing must be done in order for each user.
They wanted the rapid development and abstracted infrastructure serverless brings, but their question to me was ‚ÄúIs there a way to guarantee processing events in a specific order when serverless can scale and be running on multiple parallel instances?‚Äù Below are details on how to do exactly that.
The first question to be answered is how to get the events to the Azure Function to begin with. HTTP or Event Grid wouldn‚Äôt work, as there is no way to guarantee that message 1 will arrive and be processed by your function before message 2 arrives and is processed. In order to maintain processing order, order needs to be persisted somewhere that my Azure Function can pull from. Queues work, but if using Azure remember that Azure storage queues do not guarantee ordering so you‚Äôd need to stick with Service Bus Queues or Topics. For this specific scenario though, we‚Äôll focus on Event Hubs. For details on how to do in order processing with Service Bus queue/topics, see my other blog.
Azure Events Hubs can handle billions of events, and has guarantees around consistency and ordering per partition. For the scenario above, the partition was clear: each users events would need to go to the same partition so consistency for all that users events would be guaranteed. This would be the first step in enabling in-order processing.
Now that the events are persisted in order with Event Hubs, I still have the challenge of processing them in order. This may seem extra complex with serverless technology, as a serverless function can scale across many parallel instances which could all be processing events concurrently. However there are some behaviors that allow us to enforce order while processing.
First, Azure Function‚Äôs event hubs trigger uses event hubs processor hosts to pull messages. The processor host will automatically create leases on available partitions based on how many other hosts are provisioned. There is another important constraint that works in our benefit in this case: a single partition will only have a lease on one processor host at a time. That means multiple function instances can‚Äôt pull messages from the same partition. This also means your function will only ever scale to n+1 instances, where n is the number of partitions for the event hub. It‚Äôs n+1 because we keep one running and ready to immediately start pulling messages from a partition if an instance goes away for any reason (you aren‚Äôt charged for this overprovisioning though ‚Äî because serverless is awesome).
So as long as I know the events that need to be kept in order are in the same partition, I can also guarantee that a single instance of Azure Functions will receive those messages at one time. But are those messages preserved throughout execution? Let‚Äôs run some tests.
For my first test I have a simple Azure Function that will trigger on a message from Event Hubs and add it to an ordered list in a Redis cache. I will publish 1000 messages ‚Äî in order ‚Äî for 100 users in parallel (with each user ID being a partition key). I have 20 partitions in my event hubs, but that‚Äôs ok. Multiple users events may land in the same partition, but I still know all events in a single partition have order preserved, so it all works out.
After my functions trigger and process all messages, I can then compare the order in Redis cache to the order of the messages published.
Here‚Äôs what the first Azure Function looks like ‚Äî I‚Äôm just going to pull in a single message and immediately push it to the end of the Redis list.
After sending 1000 in-order messages for 100 users in parallel, here‚Äôs what one of the user lists looked like in Redis:
Notice anything? While the order is pretty close, it‚Äôs not perfect. You can see a few examples above where one message got processed before or after its order. So what‚Äôs happening here? The answer is that even though a single instance of a function app gets all messages in a partition, it may process them concurrently. Specifically, when the Azure Function‚Äôs event hubs trigger grabs messages, it pulls them off in a batch (as many as Event Hubs will give it, usually around 200 messages but can be more). So even though the messages are initially pulled in order, they are then processed concurrently causing some items to complete before others.
To verify this, I configured the ‚ÄúmaxBatchSize‚Äù for my function to 1. Now my function will only ever pull a batch of 1 message. After running my second test with this setting, I did in fact get perfect order, but it came at a cost. The function took a long time to burn down all of the messages, because it was having to go fetch each message individually from event hubs. Fetching each of the 100,000 total messages individually took a while to process. Luckily, there is a better way.
I can take advantage of the throughput benefits from pulling batches from event hubs, and preserve order. The trick is simple: instead of passing a single event hub message into a function execution, I‚Äôll pass in the entire ordered batch. I can then process each message in order, without the costly call to event hubs for each message. Here‚Äôs what the in-order batched function looks like:
Now when running the same test (sending 1000 messages in-order for 100 users in parallel), the results were just what I wanted. My function was able to process the 100,000 total messages in a few seconds, and when checking the results in Redis cache? Perfect order, for each of the 100 user‚Äôs lists. Event Hubs is preserving each user event order in partitions, functions pulls in ordered batches from available partitions (scaling out as much as possible), and maintains that order while processing the batch.
‚ö† It should be noted that in the case of failure or event hub lease loss, it‚Äôs possible that messages are reprocessed from the same batch again. In general with distributed computing you should work under the assumption of ‚Äúat-least-once‚Äù delivery. If, for example, a failure was hit on message 223 and the batch retried, I could get: ‚Äú220,221,222,223,220,221,222,223,224.‚Äù While ordering is preserved, the batch behavior is important to note. One recommended alternative if this behavior matters is to use Service Bus Sessions with Azure Functions where failures can be handled individually rather than in batches. More strategies on reliable message handling can be found here. More info on how to handle resiliency can be found here ‚Äî in general it‚Äôs best to assume at-least-once and duplication may occur.
Hopefully this example has been helpful. I have the entire working solution (including the console app that sends the 1000 events for 100 users) in GitHub here.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
See all (133)
511 
11
511¬†claps
511 
11
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/clean-architecture-with-partitioned-repository-pattern-using-azure-cosmos-db-62241854cbc5?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
"
https://medium.com/bb-tutorials-and-thoughts/200-practice-questions-for-azure-data-dp-900-fundamentals-exam-ea2446ee3a0?source=search_post---------24,"There are currently no responses for this story.
Be the first to respond.
The exam DP-900 is an Azure Data Fundamentals exam. According to the exam guide here, Candidates for this exam should have a foundational knowledge of core data concepts and how they are implemented using Microsoft Azure data services. Candidates should be familiar with the concepts of relational and‚Ä¶
"
https://medium.com/@hakant/api-versioning-with-swagger-azure-api-manager-and-asp-net-core-an-introduction-a403d498eb5e?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hakan Tuncer
Aug 5, 2018¬∑5 min read
API versioning has no ‚Äúright way‚Äù and I am fairly convinced that API evolution is usually a better option for both API authors and clients of those APIs. Evolving APIs smoothly without breaking any existing clients and making it easy for them to consume new features seems like the best of both worlds. GraphQL, for example, chooses API evolution over versioning.
At frequent and incremental release cycles it‚Äôs usually easy to evolve an API without introducing any breaking changes, until it isn‚Äôt. At one point or another evolving an existing endpoint can have costly underlying implications on the codebase. I‚Äôve seen many businesses struggling to find a stable vision for their product which can lead to large shifts in business direction which means large changes in APIs ‚Äî and remember business always wins. If some kind of versioning eventually becomes a necessary evil, how and when should one version ?
The answer is a range of possibilities. One that goes from the most conservative approach to more liberal ones.
Microsoft, for example, doesn‚Äôt discard API evolution but dictates that all APIs support explicit versioning:
All APIs compliant with the Microsoft REST API Guidelines MUST support explicit versioning. It‚Äôs critical that clients can count on services to be stable over time, and it‚Äôs critical that services can add features and make changes.
As an organization, if you‚Äôre following Microsoft REST API Guidelines, your APIs should support versioning already from the get go.
Next question is when to increment the version number. Microsoft says:
Services MUST increment their version number in response to any breaking API change.
This part sounds very trivial at first. But what is a breaking change ?
Teams MAY define backwards compatibility as their business needs require.
Clear examples of breaking changes:
* Removing or renaming APIs or API parameters
* Changes in behavior for an existing API
* Changes in Error Codes and Fault Contracts
* Anything that would violate the Principle of Least Astonishment
So backwards compatibility as a concept contains some clear objective areas as well as some subjective areas that are highly dependent on the nature of a business. It turns out that Azure follows a more conservative approach following Hyrum‚Äôs Law and even defines the addition of a new JSON field in a response to be not backwards compatible. Office365 on the other hand perfectly allows it.
So one interesting key takeaway here is that even if you didn‚Äôt change a single thing in your API interface (URL, parameters, payload etc.) but radically changed the behaviour of an existing endpoint, you have introduced a breaking change. So it‚Äôs not always about mechanical changes but also about what that change means in your business domain.
One of the best ways to version a .NET API is to go to aspnet-api-versioning repo and use a package that‚Äôs suitable to your Web API version. These packages make it very easy and elegant to introduce versioning semantics to an API.
For my experiments I‚Äôve been using the sample project based on ASP.NET Core.
Azure API Management is an API Gateway Service in its essence but it can do a lot of things when properly configured. For organizations that are creating and managing large number of APIs, Azure API Manager can serve as a central point of discovery and governance.
Microsoft recently added versioning capabilities to API Manager and is supporting a few main versioning schemes such as url path, http header and url querystring. So now developers can add explicit versioning to their version-less APIs or publish their versioned APIs through API Manager for other benefits.
Such a service can be invaluable for an organization but for developers and operations it‚Äôs yet another moving part to keep an eye on and maintain. All of a sudden, deploying your APIs isn‚Äôt sufficient anymore and now you also have to properly publish and configure them through Azure API Manager at each deployment cycle. In the long run, integration with the API Manager can only succeed if all these steps can be automated as part of a continuous delivery pipeline.
One nice thing about API Manager is that it can automatically import APIs that have implemented the OpenAPI Specification. That makes a big part of the automation a piece of cake. One bad thing about API Manager is that it‚Äôs a fairly young service in Azure with some rough edges, insufficient documentation and inconsistencies around automation. 80% of what you need is low hanging fruit, remaining 20% makes you cry ‚Äî and you need 100% to be successful with it.
The OpenAPI Specification, formerly known as the Swagger Specification, comes with a set of tools & libraries that help API developers to document their APIs.
Here is a complete definition, emphasis mine:
The OpenAPI Specification (OAS) defines a standard, programming language-agnostic interface description for REST APIs, which allows both humans and computers to discover and understand the capabilities of a service without requiring access to source code, additional documentation, or inspection of network traffic.
Yes I know OpenAPI Specification doesn‚Äôt support Hypermedia and thus it can‚Äôt be REST but let‚Äôs accept the fact that most of us are building ‚ÄúRPC over HTTP APIs‚Äù and move on to the benefits.
Swashbuckle.AspNetCore repo is the home of Swagger tools for documenting API‚Äôs built on ASP.NET Core. When you annotate your action methods using the constructs from this library you get a human readable interactive documentation as well as a machine readable JSON representation of you API surface. If you‚Äôre running multiple versions of your API side by side Swagger got you covered as well. You can represent each version of your API docs separately.
Here‚Äôs how a human readable Swagger API doc looks like, notice the dropdown that let‚Äôs the reader switch to different versions.
And here is the same API spec (v3) in computer readable format:
After this introduction, in the next follow up post I‚Äôll talk about a frictionless developer experience for versioning .NET APIs and automatically publishing those versions to Azure API Manager. I‚Äôll share a PowerShell script that can run as a VSTS release step, automatically detect all available versions of an API and publish them to Azure API Manager.
Thanks for reading and until then, take care!
Update: Second part is published. You can find it here.
Originally published at www.hakantuncer.com on August 5, 2018.
Full Stack Software Developer & Architect focusing on .NET, Javascript, Angular, Node.js and Azure | Amsterdammer
See all (135)
353 
353¬†claps
353 
Full Stack Software Developer & Architect focusing on .NET, Javascript, Angular, Node.js and Azure | Amsterdammer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/medialesson/best-way-to-host-a-single-page-application-spa-in-microsoft-azure-3e70cbd075c3?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
I frequently need to deploy static websites (e.g. our company website https://www.media-lesson.com) or Single Page Applications and I‚Äôm always looking for ways to improve cost and time-to-deploy.
Microsoft just recently announced public preview of static website hosting for Azure Storage adding yet another option to host a single page application (SPA) on Azure.
Just before Microsoft added back support for Proxies in the Azure Functions Runtime 2.0.11776-alpha on May 14th providing a way to host a static website in Azure Storage and forwarding traffic through a Azure Function proxy route.
Both new options add to the traditional way of hosting of (static) website in an Azure App Service. There are even more options of hosting websites in Azure e.g. using Containers, Docker, Kubernetes, Virtual Machines etc. but I will keep the focus on easy and cost efficient Deployments of single page applications.
And since we‚Äôre evaluating hosting options it‚Äôs also worth comparing manual and automatic deployment using Visual Studio Team Services (VSTS) for the mentioned services.
So let‚Äôs answer these questions to help decide which service to use when hosting a Single Page Application:
So lets start this experiment by creating a simple SPA based on Angular using the Angular CLI as a test app to deployment and test on the 3 different services: ng new testapp --routing Please note that I‚Äôm directly adding the routing feature to app using the --routing parameter. I find this an important aspect to test when looking at hosting options as routing can be a configuration challenge in some environments like App Service because we need to configure the web server to allow route handling by our client app instead of server side.
To fully test routing we also need to have some sample routes. Therefore lets add a home and a about component to our app using the CLI: ng generate component home and ng generate component about. Next we need to have two routes in our app-routing.module.ts to allow navigation between the two components:
Finally some navigation buttons to allow the user to navigate between the two components in our app.component.html:
Let‚Äôs build our app locally in preparation for manual deployment using ng build --prod which gives us this little folder of build artifacts:
Of course we also want to dry automatic deployment as part of a continuous deployment process. Therefore let‚Äôs push our app to a VSTS repository and setup a build definition with these 3 tasks:
npm install
npm build
Publish build artifact from path dist to app
Azure App Service is a PaaS (Platform as a Service) offering and the classic way of hosting web content on Azure. Using app service we need to take care of configuration and scaling of our service by hand. To test this let‚Äôs create a new app service in the Azure Portal. I choose the S1 tier for this test as it‚Äôs the entry level tier for production apps.
Because of it‚Äôs nature in being a PaaS offering, developers to take care of the configuration of the underlying web server (in case of Windows it‚Äôs IIS). This means to enable routing inside our Angular app we have to provide a web.config file with instructions for the web server how to handle routing or host the angular app inside a ASP.Net Core 2.1 app with the SpaServices middleware enabled. Here a simple sample of a web.config with SPA routing:
For manual deployment let‚Äôs just use FTP to upload the content of our dist folder and the web.config to the wwwroot folder of the app service. The FTP credentials can be downloaded in the Azure portal in the overview tab of the app service by clicking on ‚ÄúGet publish profile‚Äù.
Setting up a release definition in VSTS to deploy the app to app service is pretty straight forward as we just need an empty release definition with one task:
To configure an FTP service endpoint let‚Äôs click on ‚ÄúManage‚Äù and add a new generic endpoint with the same credentials used for manual deployment.
In this variant we‚Äôre going to use Azure Storage as an inexpensive store for our static files and an Azure Function App with proxies to serve the SPA to the user. This allows us to scale automatically (when using the consumption plan), combine our SPA with other functions (e.g. API calls) under one domain and manage our app in a serverless fashion.
So we need to create a Function App and a Storage Account (automatically created when creating a Function App). Next we need to create a blob container named ‚Äúweb‚Äù in the storage account where we will deploy our files to later.
The routing magic now happens in the way we configure proxies to forward request from our function app to the storage account. Therefore we will need 2 proxies:
The first one to forward requests to the base url to our index.html
and a second one to forward requests for other static assets like javascript files or stylesheets to their location in the storage account.
To manually deploy click on ‚ÄúContainers‚Äù in the storage account in the Azure portal and select the ‚Äúweb‚Äù container that we just created. Now let‚Äôs upload the files from the local build.
To test this we need to create a second release definition in VSTS with the empty process template and add the AzureBlob File Copy task:
This is the newest option that just recently went into public preview. The idea is to use storage to host the SPA because a real web server is not needed to serve just static files qualifying this variant for the buzzword ‚Äúserverless‚Äù. So let‚Äôs create a new storage account (general purpose v2) and then enable the static website feature:
This is all configuration we need. Storage scales automatically and routing also works out of the box. The primary endpoint is our SPA‚Äôs url. Nice!
To manually deploy click on ‚ÄúContainers‚Äù in the storage account in the Azure portal and select the ‚Äú$web‚Äù container (which is created automatically when enabling static website). Now let‚Äôs upload the files from the local build:
And that‚Äôs it!
To test this we need to create a third release definition in VSTS with the empty process template and add the AzureBlob File Copy task:
Make sure to select version 2.* (preview) otherwise the deploy wail fail because the ‚Äú$‚Äù character in the container name was not allowed previously.
To get an idea about the performance let‚Äôs run some artillery.io load tests on our 3 deployments. Here are my results when firing 1,000, 10,000 and 100,000 requests:
There‚Äôs a drastic performance benefit using Azure Storage and avoiding the web server component while Functions and App Service run at a comparable speed. This makes sense as both share the same underlying infrastructure. I‚Äôm a bit surprised that the Function app is running even slower that the App Service.
I find this even stranger when comparing the total runtimes to complete the 100,000 requests test. This took around 46 seconds to complete to Azure Storage and 14 minutes and 27 seconds for App Service and even 17 minutes and 2 seconds for the Function app. I would have expected the Function app to gain speed over time as I was expecting it to scale horizontally automatically. Which doesn‚Äôt seem to work in this scenario.
So we have a clear winner in this discipline: Storage is really fast!
Getting the cost right is tricky as all have different billing models. Here‚Äôs my example calculation for monthly costs for 100.000 requests/day in the West Europe region (which I‚Äôm not 100% sure it‚Äôs complete and accurate!):
1x S1 Instance in West Europe running Windows (1 core, 1.75 GB RAM, 50 GB Storage) = 61.56 ‚Ç¨/month
Our SPA consists of 5 files * 100,000 requests/day * 31 = 15,500,000 requests per month total. The total size of our app is roughly 0,33 MB which accounts for 0,98 TB of outbound traffic per month total. The minimum execution time is 100ms (which would be enough for our proxy purpose) for we can handle 10 requests/execution second.
1x Function App with 1.550.000 executions with a execution time of 1s each per month (each less than 128 MB of memory) for handling the requests going through the proxies = 0.17 ‚Ç¨/month
1x Storage General Purpose V2 Block Blob Storage account with LRS redundancy and hot access tier. Capacity 1 GB (we actually need just 300kb but that‚Äôs the smallest size available), 100 Write Operations, 100 List operations, 15,500,000 read operations and 0,98 TB data retrieval = 5.64 ‚Ç¨/month
Total: 5.81 ‚Ç¨/month.
For the storage we just take the same calculation than above:
1x Storage General Purpose V2 Block Blob Storage account with LRS redundancy and hot access tier. Capacity 1 GB (we actually need just 300kb but that‚Äôs the smallest size available), 100 Write Operations, 100 List operations, 15,500,000 read operations and 0,98 TB data retrieval = 5.64 ‚Ç¨/month
Hosting an SPA in Storage should be a no-brainer for dev, test and staging situations as it‚Äôs fast to setup and in most cases even free for those scenarios. I didn‚Äôt find any disadvantages yet so will dive a bit deeper and see if we can also use it in production.
Please feel free to provide feedback.
We help our customers design, architect, develop and‚Ä¶
548 
18
548¬†claps
548 
18
We help our customers design, architect, develop and operate modern, intelligent, beautiful and usable apps on any platform powered by the Cloud, IoT and AI.
Written by
CEO @ medialesson. Microsoft Regional Director & MVP Windows Development. Father of identical twins. Passionate about great User Interfaces, NYC & Steaks
We help our customers design, architect, develop and operate modern, intelligent, beautiful and usable apps on any platform powered by the Cloud, IoT and AI.
"
https://towardsdatascience.com/how-to-decide-between-amazon-sagemaker-and-microsoft-azure-machine-learning-studio-157a08af839a?source=search_post---------27,"Sign in
Steve Dille
May 16, 2019¬∑13 min read
I recently published a walk-thru of Microsoft Azure Machine Learning Studio (Studio) https://towardsdatascience.com/how-microsoft-azure-machine-learning-studio-clarifies-data-science-8e8d3e6ed64e and was favorably‚Ä¶
"
https://koukia.ca/build-your-first-web-api-with-f-giraffe-and-host-it-on-azure-cloud-1d9dc07dc248?source=search_post---------28,"Today I am going to talk to you about programming Web APIs using F# programming language.
F# has been around for a while now, but what I would like to talk about is F# in .Net Core because it has a lot of cool features specially for Web Programming and building Web APIs and with playing around with it building some Web API, so far I personally‚Ä¶
"
https://medium.com/caf%C3%A9-con-leche/azure-7c827dbe2084?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
Azure, it means bright blue
a beautiful azure sky, not a cloud in sight
Azure means the same but she sounds so different
she has more feel than blue
more life
more harmony
more character, the way she dances right off the tongue.
"
https://medium.com/cloudskills/build-your-first-ci-cd-pipeline-using-azure-devops-b9dc930cefe1?source=search_post---------30,"There are currently no responses for this story.
Be the first to respond.
Continuous integration and continuous delivery (CI/CD) are considered by most to be the backbone of DevOps. Things start to get really interesting when you combine these practices with programmable infrastructure and a suite of services that allow you to automate the entire lifecycle of an application.
The goal with this guide is to give you a practical example of what that all looks like when you‚Äôre building, testing, and deploying applications with Azure DevOps Services. I‚Äôll walk you through the end-to-end process of building a fully automated build and release pipeline for a Node and Express application. We‚Äôll use Azure DevOps Services to create the CI/CD pipeline and Azure App Service for deploying to development/staging and production.
To follow along, you‚Äôll need a GitHub account and Azure Subscription. The demo application is open source, so the Azure DevOps pipeline infrastructure we build will be covered under the free tier.
The first step is to navigate to dev.azure.com and sign in to Azure DevOps. If you‚Äôve never done this before, you‚Äôll need to create a new organization.
You need to have at least one organization, which is used to store your projects, structure your repositories, set up your teams, and manage access to data. The guidance from Microsoft is to keep things simple and start with a single organization. For more advanced scenarios, take a look at plan your organization structure in Microsoft‚Äôs documentation.
After clicking on continue, you may end up with an organization name that was generated at random. You can change this as shown in Figure 2.
Simply navigate to Organization Settings > Overview and update the name.
I wanted to demonstrate an application that was somewhat realistic but not overly complex for this walkthrough. The Node and Express app is a simple website for a fictitious company. This app uses Express and Handlebars to serve up a few common pages you‚Äôd see on any company website. Also included are some unit tests that ensure those routes are working and serving up the right content.
You can head over to my GitHub account to fork this repository.
Next, we can move on to deploying the infrastructure to support both development and production deployment slots using Azure App Service.
We‚Äôre going to use an Azure Web App for Linux resource to power our Node and Express application. We‚Äôll set things up so our CI/CD pipeline can build and deploy the app into a development/staging slot. Then we‚Äôll set up up a manual approval into the production slot.
We‚Äôll use an Azure Resource Manager (ARM) template to build the App Service infrastructure.
Navigate to the node-express-azure repository you forked in the previous step. You‚Äôll see a ‚ÄúDeploy to Azure‚Äù button about halfway down the screen.
Clicking the ‚ÄúDeploy to Azure‚Äù button will redirect you to the Azure portal as shown in Figure 5.
Notice that you‚Äôll need to set a globally unique hostname for your web application, along with a name for the new app service plan. I‚Äôd recommend deploying these resources into a new resource group. That way when you‚Äôre done with this walkthrough, you can clean up the Azure resources easily by deleting the resource group.
Click ‚ÄúPurchase‚Äù to launch the template to agree that you‚Äôll have to pay for the App Service resources that this template deploys on your behalf.
After you launch the template you should see a successful deployment message, and you should have a new resource group similar to the one shown in Figure 6. Notice that there is an App Service Plan, a web app that represents the production deployment slot, and a slot for development called ‚Äúdev‚Äù.
Quick side note about the ARM template: the Deploy to Azure button references the azuredeploy.json ARM template in my GitHub repository. If you want to update the template, update the version in your own repo, and don‚Äôt forget to change the target of the button in the source of your README.md file.
We‚Äôre ready to move on and set up a build pipeline in Azure DevOps. Head back to dev.azure.com and create a new project inside your organization. Use the settings shown in Figure 7.
After clicking on the ‚ÄúCreate project‚Äù button, you‚Äôll see a summary page for the project. Navigate to Pipelines and click on Builds as shown in Figure 8.
Next, click the button to create a new build pipeline. You‚Äôll be prompted to choose a repository. Select GitHub. You‚Äôll see a screen like the one in Figure 9 where you‚Äôll need to authorize the Azure DevOps service to connect to your GitHub account on your behalf. Click Authorize.
After your connection to GitHub has been authorized select the node-express-azure repo that you forked in the first step. You should end up seeing a ‚ÄúNew pipeline‚Äù screen like the one shown in Figure 10.
The new pipeline wizard should recognize that we already have an azure-pipelines.yml in the repository. This file contains all of the settings that the build service should use to build and test our application, as well as generate the output artifacts that will be used to deploy the app later in our release pipeline.
After you click ‚ÄúRun‚Äù to kick off your first build, you should see a screen like the one shown in Figure 11.
Notice that a lot went on with the build. The service used an Ubuntu 16.04 build agent to grab the code from GitHub, installed our development dependencies, and then ran our unit tests to validate the application. Finally, the code was bundled into an output artifact and published so we can use it as an input artifact for our upcoming release pipeline.
Click on the release button at the top of this screen to create a new release pipeline.
When you get into the release pipeline screen, you‚Äôll need to select a template. For this scenario, we are going to choose ‚ÄúApp Service deployment with slot‚Äù.
Click on the apply button to create the new deployment stage within the release pipeline. On the next screen, you‚Äôll be able to configure this stage. Change the name to ‚Äúdevelopment‚Äù as shown in Figure 13.
While on this screen, click on the link that says ‚Äú2 tasks‚Äù inside your development stage. This will take you to a screen where you can configure the deployment task. Make sure you fill out all the fields as shown in Figure 14.
Next, highlight and remove the second deployment task for swapping the slots.
Finally, click Save.
Head back over to the ‚ÄúPipeline‚Äù tab at the top left of the screen. Inspect the deployment triggers for the artifacts as shown in Figure 17.
Notice that continuous deployment is enabled by default. Going forward, each new build will trigger a deployment to our development slot in Azure App Service.
First, let‚Äôs trigger a manual release.
Click the ‚ÄúRelease‚Äù button on the top right of the release pipeline screen and create a new release. Use the settings as shown in Figure 18.
Click on the ‚ÄúCreate‚Äù button to deploy the application to the development deployment slot. You should see a successful status in the properties of the release.
Navigate to the public URL of the ‚Äúdev‚Äù deployment slot in your web browser. The hostname will have ‚Äú-dev‚Äù appended to it. For example, my web app is named ‚Äúnode-express-demo‚Äù and the ‚Äúdev‚Äù deployment slot URL is https://node-express-demo-dev.azurewebsites.net.
You should see the sample web application when you visit the ‚Äúdev‚Äù slot URL. The production slot will show the default Azure App Service splash page since it is virtually untouched at this point. Let‚Äôs change that in the next step.
Head back over to the Azure DevOps portal and go to Pipelines > Releases. Click on the ‚ÄúEdit‚Äù button to modify the pipeline. Highlight the Development stage and click the dropdown to clone the stage.
Rename the stage to ‚ÄúProduction‚Äù.
Next, click the pre-deployment conditions button for the Production stage. Enable pre-deployment approvals and add yourself as an approver.
We‚Äôre doing this because we don‚Äôt want automated deployments going straight into production. We‚Äôre not building a continuous deployment pipeline for production. We‚Äôre building continuous delivery pipeline.
Continuous delivery is a process that ensures our application is production ready. When we are doing a scheduled deployment we can do so with confidence since we know the application has been through a pipeline of tests before-hand.
Next, click on the ‚Äútask‚Äù link on the Production stage. We need to modify this task so that it does not deploy our code into the development slot.
Simply uncheck ‚Äúslot‚Äù and this will infer that the production slot of the web app should be used during the deployment. Click save when complete.
Navigate to your GitHub account and into the views folder of the demo application. Edit the index.handlebars file to update the app to version 2.0.0.
Committing the change in this repo should automatically trigger a build, perform our tests, and publish a deployment package. We can confirm this by reviewing the build status.
After the build, you should see a new release. The development stage should be green indicating that the deployment succeeded. The production stage should be blue and show that it‚Äôs pending approval.
Click approve to kick-off the production deployment.
Go back to your pipeline view and you should see the deployment to production succeeded.
Finally, head over to the web app URL for the production slot to confirm the correct version is running.
You should see version 2.0.0 on the homepage.
Have you ever seen those build pass/fail badges when browsing projects on GitHub? They‚Äôre really cool because you can tell at a glance if the code is still working or if it‚Äôs old and busted.
Let‚Äôs set up a badge for this project.
Go back to your Builds section and click the status badge button.
Copy the markdown code for the status badge.
Now, go back to GitHub and modify the README.md file in your node-express-azure repo. Paste the markdown you copied from the status badge page.
Commit the change and view the README. You should see a build passing status icon.
If you‚Äôre still reading after all this time, respect! You now know who to build a CI/CD pipeline on Azure.
You can simply delete all the resources to clean things up. Delete the resource group you created for this project, delete the Node demo project in the Azure DevOps portal, and delete the GitHub repo that you forked from my account (unless you want to keep a copy).
Isn‚Äôt this awesome stuff? There‚Äôs so much more. For now, check out these resources to dive deeper.
Originally published at cloudskills.io.
The Official Publication of CloudSkills.io
441 
3
441¬†claps
441 
3
Written by
Twenty-year tech industry veteran, author, educator, and entrepreneur. Founder of https://cloudskills.io
The Official Publication of CloudSkills.io
Written by
Twenty-year tech industry veteran, author, educator, and entrepreneur. Founder of https://cloudskills.io
The Official Publication of CloudSkills.io
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/increasingly-strong-and-serious-competition-from-switzerland-for-amazon-aws-microsoft-azure-co-7924ef0e0662?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
Initiated by Netkom IT Services GmbH, a Swiss based IT and cloud provider with a proven track record of 17 years of successful operations in various countries and one of the cloud pioneers in Europe, n‚Äôcloud.swiss AG provides high quality services and products in the field of cloud computing. Founded in 2001 by Andr√© Matter, Netkom IT Services GmbH developed early through various consulting mandates and IT projects as a specialist for IT strategy and its implementation in the operational IT infrastructures and organization of the respective customers. The latter were well-known large companies from Switzerland and abroad such as Citibank or ABB Turbo Systems. In the following years, further core competences in the fields of transformation and digitization have been established.
Swiss made alternative to Amazon AWS, Microsoft Azure, Google Cloud and other major cloud providers
Since small and medium-sized enterprises are generally unable to afford one‚Äôs own datacentre infrastructures, Netkom built its own cloud platform for this segment already in 2009 under the name n‚Äôcloud. Today, the n‚Äôcloud.swiss platform is considered as a serious ‚ÄúSwiss made‚Äù alternative to the major cloud providers like Amazon AWS, Microsoft Azure and Google Cloud Platform. With the expertise and experience as a specialist for IT strategy and its implementation in the operational IT infrastructures and organization of the respective customers, n‚Äôcloud.swiss AG is expanding on the international level and aiming to provide their products and services to a worldwide market range. Nowadays, cloud computing is the promise of having a modern and state-of-the-art IT infrastructure without the need for substantial capital investments and personnel increases. The cloud market revenue is expected to double in the next three years reaching up to 162 billion USD. Here is where the huge potential of n‚Äôcloud.swiss AG of establishing as a Swiss/European alternative to the leading cloud providers becomes clear.
n‚Äôcloud.swiss heralds a new era in cloud computing
n‚Äôcloud.swiss stands for a new era in cloud computing. In contrast to many cloud providers, this innovative cloud platform is capable of addressing and serving all cloud deployment requirements such as private, public, hybrid and community cloud. In addition, all cloud service models from Infrastructure- over Platform- to Software-as-a-Service models are included within this same product. The advantage for customers range from agility, flexibility and adaptability to a maximum amount of freedom to build their cloud according to their needs.
In addition, all cloud service models from Infrastructure as a Service (IaaS), over Platform as a Service (PaaS) to Software as a Service (SaaS) are part of the platform. All this is surrounded by an innovative internal n‚Äôcloud.swiss application catalogue. Within the latter, n‚Äôcloud.swiss offers more than 142 applications from 30 different IT categories ‚Äúfree and ready to go‚Äù as well as the opportunity to upload also other development applications and tools easily. Along personal support and competitive pricing models, API connectivity for easy and fast transfers of existing developments from or to other major cloud platforms award n‚Äôcloud.swiss a unique selling point and a competitive advantage.
n‚Äôcloud.swiss features demo
The establishment of n‚Äôcloud.swiss across the globe with the launch of an ICO
The number of ICO launches nowadays has reached a level making difficult for investors to stay on top of things. More than ever, ICOs of established companies are in demand given the fact that the number of scams is high as well as the number of ICOs leading to projects that fail. The ICO of n‚Äôcloud.swiss is truly the ICO of a proven business case. The particularity here clearly is the fact that investing in this ICO signifies investing in a real company distinguished by a successful record of accomplishment and existing customers. However, the question arises why such a company plans to launch an ICO. The target for n‚Äôcloud.swiss is to achieve a substantial market share in the global cloud market for IaaS, PaaS, SaaS services in the next 5 years. Consequently, as a decisive step towards its establishment as the undisputed alternative to the major cloud providers, n‚Äôcloud.swiss ICO has been launched to successfully expand and rollout the existing business operations into 60 additional countries around the world. The n‚Äôcloud.swiss ICO is divided into two main stages. While the Pre-ICO will be held from April 14th to April 29th, 2018, the main ICO event will take place from April 30th to July 8th, 2018. During these two events of ICO, investors can buy tokens. As the first and only company in the world, n‚Äôcloud.swiss AG is offering to investors the unique service of managing their tokens. The idea behind is to open up the doors to unexperienced ICO investors and allow them to benefit from the company‚Äôs success.
US Cloud Act or how foreign laws do not matter anymore
While the U.S. Congress recently approved the CLOUD Act for data stored overseas and strictly speaking opened a back door for the FBI, CIA, and NSA to intercept virtually anyone without any court order, the need for an alternative to the major cloud providers becomes probably greater than ever. A market that is dominated by four American (Amazon AWS, Microsoft Azure, Google Cloud Platform and IBM Softlayer) and one Chinese (Alibabcloud), definitely presents numerous possibilities for n‚Äôcloud.swiss AG. ‚ÄúSwiss made‚Äù high quality standards in terms of security and reliability in setup and operation of fail-safe cluster-systems can present for the company a valuable competitive advantage. On the other hand, investors in this ICO will be able to join a company with a huge potential, contribute to its international expansion and establishment and benefit from a success story in the making.
#BlackLivesMatter
404 
1
404¬†claps
404 
1
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/hackernoon/azure-container-instances-vs-aws-fargate-3216607f63f4?source=search_post---------32,"There are currently no responses for this story.
Be the first to respond.
Last week, during their re:Invent 2017 extravaganza, AWS announced their new container service Fargate. Fargate is AWS‚Äôs clusterless/serverless way of running containers. Fargate‚Äôs announcement was joined by the announcement for AWS managed Kubernetes clusters, dubbed AWS EKS and available only through sign up somewhere in 2018.
In a rare occurrence, it seems like AWS is playing catch-up with Microsoft‚Äôs Azure. Azure has had Azure Container Instances (ACI) in preview for the last months and has already rolled out managed Kubernetes (and DC/OS) as part of the Azure Container Services‚Ä¶ehm‚Ä¶service.
Fargate is brand new and currently only available in the us-east-1 region, but it‚Äôs still fun to compare it to Azure Container Instances as they seem to both target the same audience and provide the same value. But looks can be deceiving, as it turns out they are quite different beast! Shock, horror!
Let‚Äôs first see how easy it is to get from zero ‚Äî where zero means you have an AWS and/or Azure account with appropriate rights ‚Äî to a running container exposing a simple web page to the internet on both services.
ACI has no real visual setup wizard or GUI. Luckily the Azure CLI is pretty nice and getting a container running is really dead simple: two bash one-liners.
After this, a quick az container show informs me my service is available on address 52.191.116.216:3000. And yes, it also actually worked:
Let‚Äôs start with a minor niggle: according to AWS, Fargate ‚Äúallows you to run containers without having to manage servers or clusters‚Äù. However, the first step in running a Fargate container is‚Ä¶creating a cluster! Ok, it is very quick and you don‚Äôt have to manage it, but still.
In general though, setting up a first running container on Fargate is a lot more involved than ACI. Using the AWS GUI, you go through the following steps, where each step can have a ton of options, drop downs and input fields.
What is immediately clear is that Fargate is completed embedded in the current Amazon Elastic Container (ECS) service. You literally need to select it as a ‚Äúlaunch type‚Äù, in contrast to the EC2 version.
This means Fargate comes with all the configuration and setup flexibility/bagage attached. Not an issue an sich but to me at least AWS ECS was always cumbersome to set up and configure.
After finishing this setup procedure, the dashboard informs us the container is running. But where can we reach it? This had me stumped for a while: there was nothing in the UI and where using the applicable describe call in the AWS CLI there was nothing in the resulting JSON. Turns out out you have to navigate to the ENI (Elastic Network Interface) of the task in the service. This takes you to the standard EC2 console which lists the public IPv4 address!
An initial try didn‚Äôt result in a response. This was solved by adding port 3000 to the Security Group attached to the ENI.
Note: this complete setup can be done using the AWS CLI, see AWS‚Äôs own write up here.
Now the containers are up and running, what can we do with them?
ACI has no GUI really, only a read only screen showing the running containers, called ‚Äúcontainer groups‚Äù for some reason. This means you‚Äôll be interacting with ACI using the Azure CLI, which is mercifully short on what you can actually do with an ACI container:
No stopping, no suspending, no scaling, no metrics, no updating, no volumes/disk mounting, no nothing. You can tail the logs and that is about it. This is immutable infrastructure brought to its logical conclusion! The fact that there is no scaling, as in starting multiple load-balanced versions of the same container is a big miss though, even in this MVP form of the service.
As such, ACI seems like an island in the normally pretty well integrated Azure landscape and I bet Microsoft is working on integrating their load balancing solutions and things like volume mounting.
update 9‚Äì12‚Äì2017: In the comments the ACI PM Sean jumped in and mentioned volume mounting DOES exist: https://docs.microsoft.com/en-us/azure/container-instances/container-instances-mounting-azure-files-volume These CLI parameters are however not listed in the CLI version I‚Äôm using.
As noticed earlier, the setup is a lot more complex than ACI. But after jumping through all the hoops you do have a pretty flexible and well supported container platform with basically all ECS functions:
As you probably have noticed by now, Fargate is not really a separate AWS service, although AWS does kind of present it as such: Fargate is a different container runtime for ECS. In your day-to-day usage, you use ECS not Fargate.
BIG CAVEAT: This is an anecdotal ‚Äúperformance test‚Äù at best. It is actually mostly a network throughput test‚Ä¶Still, I was curious if there where any obvious and huge differences in simple performance between both services.
My test service is a Node.JS Hapi application serving a simple page. Running an Apache Bench benchmark with 1000 requests and a concurrency of 5 over 10 runs gave the following results:
The results are really, really close. What was noticeable though was that Fargate/ECS seems to have less peaky behaviour, e.g. the 99 percentile was generally lower.
Both services use a fairly similar pricing model where the main cost is a function of:
duration * containers * memory * CPU
ACI adds a ‚Äúcreate request‚Äù cost of $0.0025 per created container. This sort of muddles the pay-per-use model based on resources, but probably not a big issue for most. Fargate containers are charged at a 1 minute minimum.
Here‚Äôs an example: you create 5 container instances with a 1 core, 2 GB configuration once daily during a month (30 days). The duration of each instance is 10 minutes (600 seconds). Fargate does not offer the combination of 1 vCPU / 1GB. The minimum is 2GB for a 1 vCPU container.
The cost in $ / month works out as follows:Azure Container Instances: $3.75AWS Fargate: $1.90
You can see the calculation for the above example in the Google Sheet I whipped for this post. Feel free to use it to do a price comparison for your own use case.
docs.google.com
Note: This calculation looks only at the pricing for the given services and treats Azure cores the same as AWS vCPU‚Äôs. Also, AWS will prompt you to add other, paid-for, services like a load balancers to ‚Äúcomplete the package‚Äù.
Diving into this comparison, I expected both services to be extremely similar. They are not. In their current incarnations both offerings have vastly different use cases.
Azure Container Instances almost felt as a service like Requestbin or JsFiddle. A quick and simple sandbox for running containers with almost Heroku-like behaviour: one bash command and your container is online. And I actually liked that a lot: I‚Äôve never, ever launched a container to the internet in such a quick way. Probably even quicker than a local Docker instance with a service such as Ngrok. I can see myself using this during prototyping, when working remotely with clients or when I need to show a quick demo at a meetup or conference.
As AWS describes it, Fargate is a technology within ECS and later EKS. This means it sits in the DC/OS and Kubernetes space and comes with that feature set. As mentioned earlier, you don‚Äôt use Fargate day-to-day. It is there in the background taking care of managing your ECS cluster.
Azure Container Instances summary
AWS Fargate summary
Tim is a product advocate for https://vamp.io, the smart & stress free application releasing for modern cloud platforms.
#BlackLivesMatter
325 
9
325¬†claps
325 
9
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Code and Product. Writing about solopreneurship, Javascript and containers. Founder at checklyhq.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/microsoftazure/deploying-create-react-app-as-a-static-site-on-azure-dd1330b215a5?source=search_post---------33,"There are currently no responses for this story.
Be the first to respond.
Building apps is fun! Deploying them can be slightly frustrating; but it doesn‚Äôt have to be. In this guide, we‚Äôll take a detailed look at all of the different ways to deploy React to Azure as a static application.
Static means that we aren‚Äôt deploying any server code; just the front end files. Azure will do all of the web work for us.
While this guide assume the use of create-react-app, this is of course not necessary to run a React app on Azure. If you don‚Äôt have an Azure account and would like one, you can get a fully functional trial by going to https://azure.microsoft.com/en-us/free/
First, you need to create a new website on Azure. You can do that by searching for ‚ÄúWeb App‚Äù in the search field and then selecting ‚ÄúWeb App‚Äù.
When asked for Windows / Linux (preview), leave the default at Windows.
You will need to configure Azure to handle your routes. Otherwise the application will break when you add in client-side routing.
To find your site‚Äôs FTP path, username and password, download the site‚Äôs publish profile.
That file (which is just XML despite having a ‚Äú.publishSettings‚Äù extension) contains all of the necessary FTP information.
Just use your favorite FTP client (mine is Filezilla) and publish.
You can also have Azure pull directly from Github. This can be accomplished via the CLI or the portal. The CLI is well covered in the documentation, so we‚Äôll use the portal here.
Select ‚ÄúDeployment Options‚Äù on your site.
Select ‚ÄúChoose Source‚Äù. You will see a bunch of different deployment options. Azure supports both local Git and Github. Local git is just you pushing code to a Github repo on Azure. Github is Azure pulling from your Github repo.
If you select Local Git, you will get a new setting on the Overview page which shows you the git clone url and your local git username. That username is coming from the ‚ÄúDeployment Credentials‚Äù tab where you can change it or the password at any time.
Remember that we don‚Äôt want to push the entire project to production ‚Äî with the source code, node_modules and the whole bit. We only want the ‚Äúbuild‚Äù folder. If you aren‚Äôt currently using another git, this is easy. Just move into the ‚Äúbuild‚Äù folder and do‚Ä¶
You can see from your terminal output that the deployment succeeded. Note that I trimmed out some items to make it smaller so yours will look more verbose.
It‚Äôs likely that your project is already under source control, in which case you may already have it up on Github.
Go to ‚ÄúDeployment Options‚Äù and select Github. Connect to your Github account and then select the repo that you want to deploy.
Azure then pulls in all of the code from your repo. By default, create-react-app adds a .gitignore entry for the ‚Äúbuild‚Äù folder. You are going to need to commit your build for this to work, so go ahead and remove that entry.
Now tell Azure where your ‚Äúindex.html‚Äù file lives by adding a new ‚ÄúDefault Documents‚Äù entry.
Usually people do not want to commit their ‚Äúbuild‚Äù folder, hence its automatic inclusion in .gitignore. What we really want to be able to do is have Azure pull our code from Github, run npm build and then deploy just the build folder. We can do that with Visual Studio Team Services, which is completely free to use for 5 users or less. Perfect for setting up CI with create-react-app.
Start a new ‚ÄúBuild And Release‚Äù project.
Make sure you select the correct ‚ÄúVersion control‚Äù for your project. If you are using Github, then ‚ÄúGit‚Äù is the correct answer.
Now click on the ‚ÄúBuild And Release‚Äù tab.
Select ‚ÄúNew Definition‚Äù
Search for ‚ÄúEmpty‚Äù template and select the Empty Template.
Select Process, and then set the Default Agent Queue to ‚ÄúHosted‚Äù.
Now select the Get Sources tab. Select ‚ÄúGithub‚Äù and authorize VSTS to your account.
It takes some time to populate the drop downs if you belong to several organizations and have several repos.
Now add a task to run ‚Äúnpm build‚Äù on the code that gets pulled from the Github repo.
To do that, search for ‚Äúnpm‚Äù and select the npm task. Add it to the process. By default the npm task just runs npm install, which is precisely what we need to do, so there is no more work to be done here.
Add a second npm task, but this time set it to custom and ‚Äúrun build‚Äù.
Search for the ‚ÄúApp Service Deploy‚Äù task.
Select the correct Azure subscription and click ‚ÄúAuthorize‚Äù.
Now select the correct App Service Name. You may have to click the little refresh arrow next to the drop down to get the list to populate.
Make sure that you point the ‚ÄúPackage or folder‚Äù to
Now click ‚ÄúSave & Queue‚Äù. This will queue your job up for build and deployment.
I mentioned this earlier in the article, but please refer to this article for how to create and deploy a web.config that will do routing correctly. The route will work from inside your app, but there will be no deep linking because Azure will try to route https://yourapp.com/customers/add to ‚Äúindex.html‚Äù from the path ‚Äúcustomers/add‚Äù instead of just loading the root ‚Äúindex.html‚Äù file and letting React handle the rest.
Happy deployment!
Any language.
367 
8
367¬†claps
367 
8
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nikovrdoljak/deploy-your-flask-app-on-azure-in-3-easy-steps-b2fe388a589e?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
Niko Vrdoljak
Feb 6, 2019¬∑5 min read
In this article, I will show you how to deploy and publish your Flask web app on Azure. We will use an Azure App Service on Linux, which provides a highly scalable web hosting service using the Linux operating system.
I will assume that you are familiar with Flask framework, but in any case, I will create minimal Flask application for demonstration purposes and emphasize some aspects of app environment needed for successful publishing on Azure (or any other hosting service).
First in ‚Äúhello.py‚Äù file create ‚ÄúHello world‚Äù app:
Next, create and activate virtual environment, set startup file, initialize local git and start the app from your terminal:
Navigate to http://127.0.0.1:5000/ to check that your app is running.
We will use git later to push our app to Azure. Also, create a .gitignore file to specify files and folders you don‚Äôt want Git to check in:
(if you don‚Äôt have an Azure subscription, create a free account before you begin)
We will configure service via Azure portal. There is also an alternative to do this via Azure Cloud Shell, but we will skip it this time. So, log in to Azure portal. We will configure several elements to complete this step:
A resource group is a logical container into which Azure resources like web apps, databases, and storage accounts are deployed and managed.
On Azure portal left navigation bar click ‚ÄúResource groups‚Äù and then ‚ÄúAdd‚Äù. In displayed form, select your Azure subscription, location for your resources and type the name for your group.
Click ‚ÄúCreate and Review‚Äù button and wait for notification:
Now we can create web app or App Service. On portal left navigation bar click ‚ÄúApp Services‚Äù and then ‚ÄúAdd‚Äù. Select ‚ÄúWeb App‚Äù and click ‚ÄúCreate‚Äù:
In Create Web App form, enter app name, select your subscription and existing resource group, Linux as OS, Code as publishing mode and Python 3.x as runtime stack:
Also, you have to create an App Service Plan for your app. An App Service Plan defines a set of compute resources for a web app to run. These compute resources are analogous to the server farm in conventional web hosting. One or more apps can be configured to run on the same computing resources (or in the same App Service plan). So click the App Service plan button and then Create new and fill the form:
Select appropriate pricing tier, but for demo purposes, Basic tier is the most suitable option.
Click ‚ÄúCreate app‚Äù and wait for notification message:
Go to app service resource and examine your app info:
Notice URL for your app. If you click on it, it will open default page for your app which at this moment looks like:
Remember that URL of your app is:
http://<app_name>.azurewebsites.net
Our goal is to deploy our app from local git to Azure App Service that we created. To enable that, we first have to configure some deployment settings. Click ‚ÄúDeployment center‚Äù, and select ‚ÄúLocal Git‚Äù:
On the next step, select ‚ÄúKudu‚Äù as the build server:
Click ‚ÄúFinish‚Äù, wait for notification and you will get Git Clone Uri like this:
https://flask-hello.scm.azurewebsites.net:443/flask-hello.git
Also, click ‚ÄúDeployment Credentials‚Äù to see your app credentials:
Here you can see your app username and password, which you can change or create another user credentials for your deployment purposes.
Now we are ready for our deployment. Go to your local terminal and add Azure remote to your local Git repository. Replace <deploymentLocalGitUrl-from-create-step> with the URL of the Git remote that you get in the previous step:
git remote add azure-hello https://flask-hello.scm.azurewebsites.net:443/flask-hello.git
Now commit any changes to local git:
git commit -a -m ‚Äúfirst commit‚Äù
And push to the ‚Äúazure-hello‚Äù remote to deploy your app with the following command:
git push azure-hello master
When prompted for credentials by Git Credential Manager, make sure that you enter the credentials you created in Configure a deployment user, not the credentials you use to sign in to the Azure portal.
This command may take a few minutes to run. While running, it displays something like:
You can see similar log information in the Deployment Center:
If you refresh your app, you will see that nothing changed yet. The reason is App Service uses Gunicorn WSGI HTTP Server to run an app, which looks for a file named application.py or app.py. Since our main module is in hello.py file, we have to customize startup command. Go to ‚ÄúApplication settings‚Äù and enter the following line in ‚ÄúStartup File‚Äù field:
Click ‚ÄúSave‚Äù, go to ‚ÄúOverview‚Äù, and click ‚ÄúRestart‚Äù to start the app with the new configuration. Now refresh your app. If everything is OK, out sample app should run in App Service on Linux:
That‚Äôs it. If you want to learn more on how you can customize the behavior of App Service with Python visit:
docs.microsoft.com
Also, if you want to get more information about your web site, it‚Äôs configuration, or access the diagnostic console from Bash or SSH, go to associated SCM service site:
https://<app_name>.scm.azurewebsites.net/
Father, husband, basketball fan, developer, project manager...
484 
7
484¬†
484 
7
Father, husband, basketball fan, developer, project manager...
"
https://medium.com/@sibeeshvenu/create-your-own-cryptocurrency-in-private-consortium-network-ethereum-azure-blockchain-2e4385018777?source=search_post---------35,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sibeesh Venu
Jan 15, 2019¬∑10 min read
The words Blockchain and Cryptocurrency have been successful in disrupting the market for such a long time now, the effects they have made in the industry is massive. It looks like it has its own future, so thought to write something about it. In this article, we will create our own cryptocurrency‚Ä¶
"
https://medium.com/@fiqriismail/how-to-secure-your-reactjs-frontend-with-azure-ad-b2c-8fd165f602e8?source=search_post---------36,"Sign in
Fiqri Ismail
Jan 31, 2019¬∑8 min read
A couple of months back I was introduced into the world of ReactJS. A client requirement to build a web frontend. All my developer days were spent on developing backend systems using Microsoft ASP.NET Web API and C#. And you can‚Äôt say no to your clients, right? if you have to do it you have to do it. So step into a new realm of building frontend applications.
My client already had a WEB API and a web frontend. This requirement was to create another separate sub-module using ReactJS to interact with that Web API. As usual security concerns put into the table and yep they already have Azure AD B2C setup for user management. The challenge was to use ReactJS to interact with Azure AD B2C for authentication and authorization.
Trust me, there were few tutorials on how to connect Azure AD and Azure AD B2C with frontend technologies. But all were bits and pieces here and there. I couldn‚Äôt find a full step by step tutorial that guides you through. So thought why not write one.
I have made a few assumptions here, that you already:
VS Code has this nice little feature called terminal window. This will open a terminal inside the code editor. It‚Äôs a very handy feature. Click on Terminal > New Terminalin the menu bar.
This will execute the ReactJS project. And it should look like this.
All good to go, let‚Äôs prepare our Azure AD B2C environment now.
In this section, we will be preparing our Azure AD B2C environment for authentication and authorization.
Using this URI, you will allow the permission to your application to access certain features in your directory. As an example, this could be reading user profile information.
I have specially marked this because if you didn‚Äôt give an identifier in this location, you won‚Äôt see any scopes under ‚ÄúPublished scopes‚Äù. I am not sure its a bug or not but without it, you won‚Äôt get default scopes here neither can create new.
This is all you need at Azure AD B2C end. Let‚Äôs do a checklist.
Excellent, now the setup is done. Let‚Äôs go back to our react application and do some coding.
Now, go back to your ReactJS application. In the terminal type the following command to install the library. Remember we were using VSCode terminal window.
react-azure-adb2c is a library that will help you to get the functionality or Azure AD B2C to your ReactJS application. By clicking here you will get brief documentation of how to use it in your ReactJS application.
Now you have successfully installed the library. In your ReactJS application click on the index.js file, at the top of the file add the following line of code.
Add this line of code after the import to initialize.
Now you need to replace the items marked in ‚Äú<>‚Äù from the values at your Azure AD B2C Application.
Now go back to the Azure portal and grab the following information.
To grab the value for the tenant, go back to your Azure AD B2C directory. Under overview, copy the value in ‚ÄúDomain name‚Äù field.
Now, to grab the applicationId, click on the Applications label, and copy the id from the newly created application, in this case, ‚ÄúReactJS AADB2C‚Äù and replace the value at applicatoinId field.
Now click on the User flows (polices) label and copy the name of the policy and replace the value at signInPolicy field.
Now the scopes array field. This array will give the necessary permissions to your application. These permissions will allow your ReactJS application to access functionality at Azure AD B2C.
To grab this information:
Visit this link to get a full detailed documentation on scopes.
Excellent, we are almost done. Now, your initialize code should look like this.
One more thing to add. Let's replace the default ReactDOM.render() code with this.
After all these changes, your index.js file should look like this.
Almost there. Let's do a test run. In your terminal window type and execute the following command.
You should see this screen.
Now use your login details for the Azure portal or you can create a new account by clicking on ‚ÄúSign up now‚Äù. Remember? we have created a user flow for both sign-in and sign-up. Cool isn‚Äôt it.
After creating a new account or using an existing account, you can log in to the application. But, you might not see the default ReactJS page. This might happen due to insufficient application permissions.
To fix this,
Lets‚Äô go back to our ReactJS application and refresh or rerun it.
Congratulations !!! You are done.
Let‚Äôs grab some information from Azure AD B2C and display it under the react logo.
Go back to the terminal and install the following package.
This package will allow you to decode the JWT token from Azure AD B2C and grab information inside it.
Now you need to visit back to Azure portal and let Azure AD B2C send you this information. To do this,
Go back to your ReactJS application and click src directory. Add a new file. Name it as Auth.js. Copy and paste the following code inside the file.
Now open the App.js and replace with this code.
We are all done. Lets rerun our ReactJS application.
You will be prompted with the Microsoft login screen, after a successfull login you should see this screen.
And grab the code from here.
github.com
Have a nice day.
Architect| Microsoft MVP | Community Leader | Speaker | Blogger | Founder of PeachIT (www.peachit.digital)
586 
20
586¬†claps
586 
20
Architect| Microsoft MVP | Community Leader | Speaker | Blogger | Founder of PeachIT (www.peachit.digital)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-you-can-do-continuous-delivery-with-vue-docker-and-azure-2f1e31fff832?source=search_post---------37,"There are currently no responses for this story.
Be the first to respond.
A few weeks ago at ng-conf, I announced the launch of vscodecandothat.com ‚Äî a project I worked on with Sarah Drasner to centralize all of my favorite VS Code tips into a collection of short, silent video clips. It‚Äôs like a site full of GIFs, except without the 600 megabyte payload and crashed browser tab.
Sarah designed and built the site using Vue. I put together the video clips with excessive pug references.
Sarah and I both work on the Azure team, so it was a good chance for us to use our own tools here at Microsoft to work with a real application. In this article, I‚Äôm going to break down how we do continuous delivery with vscodecandothat.com, and how you can do it yourself using the same tools we use.
Before we talk about the setup, I want to define exactly what I mean by ‚Äúcontinuous delivery.‚Äù
The term Continuous Delivery refers to making releases easy, fast, and streamlined. We can argue about the exact definition of the term, but just remember that I am a front-end developer so my eyes may glaze over. I may snore. But go on. I swear I‚Äôm listening.
For our purposes, ‚ÄúContinuous Delivery‚Äù means that the process of building and deploying the site is completely automated. Here‚Äôs how that looks in real life:
Got all that? Basically, we‚Äôre going to automate everything that you would normally do as a developer so that checking code into Github is all you have to worry about. And lord knows that‚Äôs hard enough as it is.
OK, let‚Äôs begin at the beginning. The first thing we need to do is look at the application to see how it runs. And how it runs is ‚ÄúIn a Docker, y‚Äôall.‚Äù
vscodecandothat.com is entirely front-end driven. It‚Äôs all HTML, JavaScript, and CSS in your browser. That being the case, all we want to do is serve up the index.html page from the dist folder. We use an nginx web server.
When you are just serving up static assets, the Dockerfile is very simple‚Ä¶
Sarah created an nginx configuration file that we just copy in when the container gets built. Because you do not want to be in the business of configuring nginx (OMG you don‚Äôt), Sarah has posted her config file to a gist.
I use the Docker extension for VS Code so that I can see and manage all of my images and containers. I‚Äôm not afraid of the terminal, but my brain can only remember so many flags.
Now we need a registry to push the container to. We‚Äôre going to configure Azure Container Services (ACR) for that.
You can create an ACR repository from the web portal, but to prove I‚Äôm not afraid of the terminal, we‚Äôll do it with the Azure CLI.
First, we need a group for resources. I called mine ‚ÄúvsCodeCanDoThat‚Äù.
Now create the ACR repository. I called mine ‚Äúhollandcr‚Äù.
Now we can push our image to that by tagging it with the path to the Azure Container Registry.
In the video you can watch me login to the Azure Container Registry from the terminal. This is important because your push will fail if you are not logged in.
OK ‚Äî now we need a site to host our container. For that we use Azure App Service.
First create a Linux service plan. For that, you need your app name and your resource group.
So
Becomes
Now create the web app and point it at the container that was pushed to the AKS registry. This takes 4 parameters.
And that‚Äôs it. You‚Äôll get back a URL, and you should be able to open it and see your site running.
Now what we want to do is automate everything that we just did. We never ever want to have to go through any of these steps again.
The first thing we will do is to set up our site for ‚ÄúContinuous Deployment‚Äù from our container registry.
If you are using the App Service extension for VS Code, all of your Azure sites will show up right in the editor. You can just right-click and say ‚ÄúOpen in Portal.‚Äù
Select the ‚ÄúDocker Container‚Äù menu option‚Ä¶
On this page you will see the container you configured from the terminal. There is an option at the bottom to turn on ‚ÄúContinuous Deployment.‚Äù
When you toggle this on and click ‚Äúsave,‚Äù a webhook will get created in your Azure Container Registry for this specific container. Now, anytime the image with tag ‚Äúlatest‚Äù is updated, the webhook will fire and notify App Service which automatically pulls in your image.
So we‚Äôve automated some of this already. Once we push the image, it will be deployed. There is nothing we have to do besides push it. But we don‚Äôt want to push it. We want someone else to that.
And who will do it? The robots, that‚Äôs who. Or whom? OR WHOMST. Fortunately I‚Äôm not in high school English anymore. I failed it once and that was enough.
This is the point at which I tell you that we are going to use Visual Studio Team Services (VSTS). Then you say, ‚ÄúVisual Studio? I‚Äôm not using .NET‚Äù. And I say, ‚ÄúI know, it‚Äôs confusing.‚Äù
We need a system specifically designed to automate builds and deployment. This is exactly what VSTS is/does. Also, it‚Äôs free for 5 users or less (in a project space) and ‚Äúfree‚Äù is the only word in my love language. The only word besides ‚Äúbeer.‚Äù
Create a VSTS account if you don‚Äôt have one. Once you do, you land on the dashboard screen.
From here, you want to create a new team project.
Give your project a name and a description that nobody will find helpful. Leave the version control at Git.
The next screen gives you a Git URL to check your code into. But we already have Github, so just ignore that and select the ‚Äúor build code from an external repository‚Äù option.
Authorize VSTS to Github and select the repo‚Ä¶
The next screen is offering to help you start with a template. In this case we are going to roll from an empty process. Because we are hard core like that.
Now we are going to start adding steps for VSTS to perform to do the build and deployment. The pull from source control is already happening, so the first thing we need to do is to run npm install on our code. To do that, add a task to ‚Äúphase 1‚Äù. There is only 1 phase in our build / deployment.
Search for ‚Äúnpm‚Äù and add the npm task.
By default, you get the npm install task, which is exactly what we want. You don‚Äôt need to add any options to this task.
Next, we‚Äôll be running the npm run build command, which will build a production instance of our Vue app with all of its Webpacking magic. For that, add another npm task. This time, change the name to ‚Äúnpm run build.‚Äù Set the ‚Äúcommand‚Äù to ‚Äúcustom‚Äù and the ‚Äúcommand and arguments‚Äù to ‚Äúrun build.‚Äù
Great! We‚Äôve got the build, now we‚Äôre ready to Dockerize it. Add a new task and find the ‚ÄúDocker‚Äù one.
This is a big screen, so here‚Äôs the image and then we‚Äôll walkthrough the highlights.
Lastly, we want to push the image. Add another Docker task. This time, set the ‚ÄúAction‚Äù to ‚ÄúPush an image‚Äù. Set the ‚ÄúImage Name‚Äù to $(Build.Repository.Name) ‚Äî just like before.
DO NOT SELECT THE ‚ÄúPUSH IMAGES‚Äù ACTION. If you do, your build will fail and you will blame god and all humanity before you figure out that you selected the wrong action. Don‚Äôt ask me how I know that.
And that‚Äôs it for defining the Build definition. You can now click ‚Äúsave and queue‚Äù at the top. Make sure that you select a ‚ÄúHosted Linux Preview‚Äù agent. The Docker tasks needs the Linux agent.
Now sit back and wait for a build to kick off. If you‚Äôve done everything right, you have now setup a completely automated build and deployment system for a Vue app that utilizes Docker and Azure. That‚Äôs the most buzzwords I‚Äôve ever squeezed into one sentence.
This seems like a lot to setup, but once you have it just like you want it, all you have to do is check in code to your Github repo and all of this manual deployment üí© happens automatically. Your customers will love you. Your developers will love you. Heck ‚Äî even YOU might love you.
I hope you find this helpful. I‚Äôm off to update my r√©sum√© with all of these buzzwords.
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
633 
4
633¬†claps
633 
4
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/@coderonfleek/hosting-a-laravel-application-on-azure-web-app-b55e12514c46?source=search_post---------38,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fikayo Adepoju
Jun 11, 2017¬∑6 min read
This post was born out of the pain I went through while trying to host a Laravel application on Azure for a client who insisted we host all his web apps there, after reading loads of blog posts and stack overflow suggestions i was able to get it running smoothly so i decided to share this knowing it will help someone out there.
I would like to give credit to Peter Katelaan on this blog post which really helped in finding some good answers to some disturbing points in my deployment.
Now lets get into it starting with some prerequisites. Before you proceed you should have and be comfortable with the following:
You good? Great! Lets get it on.
This can be done easily by running laravel new <app-name> from your terminal
Run git init within your project root. Then create a remote git repository with your favorite provider (I will be using Bitbucket) and add it as a remote branch to your project by running git remote add <endpoint name> <your git url> .
Now commit some code and push to your remote master branch (or any appropriate branch you have setup for the purpose of the deployment)
Got to the Azure Portal and navigate to New >> Web + Mobile >> Web App
Enter an appropriate name for your web app (Azure will verify in real-time if the name you have chosen is available) and select the Pricing tier (i am using the free tier for this practise)
When, you‚Äôre done filling all the appropriate fields, click ‚ÄúOK‚Äù at the bottom of the panel, Azure will begin setting up your web app and you will be notified immediately the process is complete
Click on your web app and go to the SETTINGS section and click on Application Settings, here you verify that the PHP version on your web app meets the minimum required by the version of Laravel you are running.
If not, change the PHP version and click the Save button to persist your changes.
Then, you need to install composer which is the dependency manager for laravel. To do this, go to the DEVELOPMENT TOOLS section and click on Extensions. From there you click the Add button at the top left-hand corner.
This opens up a blade section from which you can then select Composer, agree to the Terms and Conditions, then click OK to install it.
There is a range of differences between how Azure servers work compared to your typical Apache on Linux servers. One of which is that Azure serves out your web app from its wwwroot unlike public_html on Apache servers. However, Laravel is not a fan of this style so in order to get Laravel to work we need to tweak some settings.
Now lets setup the Deployment option. Go to the DEPLOYMENT section and click on Deployment Options. Select your Remote Git Provider (Note, you can also deploy directly from your local git repo), so i select Bitbucket and authorize it by entering my username and password.
Then, you select the repo and the branch you have setup as your deployment branch (I am selecting the master branch) and click Ok.
(Almost) Immediately, you will see your first commit being deployed.
As .htaccess is to Apache so is web.config to IIS which is the server Azure is using. Thus the .htaccess file that comes with your laravel app (in the public directory) will not work. So go into your public folder, create a web.config file and paste in the content below:
Commit and push this change to your remote branch and Azure automatically deploys it.
One important thing to note (and boy did this skip my mind) is that the .gitignore file that comes with your Laravel app ignores your .env file by default (for very good reasons one of which is to have different environment settings for your local and production environments).
Thus you need to add one in your production environment. Now don`t make make the mistake of removing it from one of the ignored files in .gitignore . Azure provides an app management console known as the Kudu Console which provides you loads of awesome tools to manage your web app. Simply navigate to it by going to https://<app-name>.scm.azurewebsites.net , for me that will be https://laravel-azure-app.scm.azurewebsites.net
From the top menu, go to Debug Console >> CMD , there you will see an FTP-like interface for your folder structure and below it is command-line interface (yeah, i know right) which is just awesome.
From the folder structure interface, click on the site folder. Then from the command line, simply create the .env file and copy the contents from .env.example by running the following command:
copy .env.example .env
This then creates your .env file, but wait, we ain‚Äôt done yet. Just a lil more edit to go.
Open your .env file by clicking the edit icon beside it
Then from the local version of your .env file, copy your APP_KEY and paste it in your production version. Also, change the APP_URL to that of your azure web app.
Phewww! Now we are done. We can now unveil our app. Simply go to the address of your app (for me that will be: http://laravel-azure-app.azurewebsites.net) and you will see your app Live and Robust. Wonderful!!!
@LinkedIn Author | Technical Writer | Software Developer
See all (71)
743 
24
743¬†claps
743 
24
@LinkedIn Author | Technical Writer | Software Developer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/microsoft-azure-sentinel-not-your-daddys-splunk-3775bda28f39?source=search_post---------39,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 5, 2019¬∑7 min read
OK, I must admit; this title is misleading. I am not going to do a side by side comparison of Splunk and Azure Sentinel. Although that seems to be the thing that people on social media are talking about these days: how does Azure Sentinel compare to other SIEM solutions such as Splunk, etc.
Instead, I‚Äôll be focusing on what role Azure Sentinel plays in securing your enterprise. And while Azure Sentinel does provide the advanced SIEM capabilities and dashboarding that many companies need, I really want you to understand the broader picture as Azure Sentinel, as a cloud security solution, is set to disrupt the SOC.
And with Microsoft owning and operating a big part of the technology you use every day in your workplace, along with making security a strategic investment and bet, I argue that they are becoming the biggest security company in the world.
Biggest security company in the world
Microsoft is investing heavily in security in recent years. Not only have they upped their game in finding and fixing product defects, they for instance also have a big organizational unit around threat intelligence (Microsoft Threat Intelligence Center). They are investing tens if not hundreds of millions in developing security products and solutions for their platforms.
And while one could argue that the early days of their AV solution were not watertight, they certainly turned around that ‚Äúship‚Äù, and Microsoft should not be underestimated if they are taking security seriously. If you look at their evolved EDR solution today, Windows Defender is not only achieving high scores, it also detects bad actors in ways and speed other vendors do not and cannot.
Because Microsoft‚Äôs owns both one of the two biggest cloud platforms in the world, as well as sell the most used cloud endpoint (Windows), they are poised to become the biggest security player in the world. On top of this, it can leverage its immense computing power to use machines learning and artificial intelligence to really make a difference in how security is approached.
You see this coming to life when you connect Windows Defender to their Azure cloud; you start to receive threat intelligence feeds, and new malware is detected and remediated through machine learning in under 14 minutes. This is why Defender ATP is growing very strong in adoption at enterprises in recent months.
Traditional SIEM‚Äôs and the cloud: a sour-sweet combination
As mentioned, Microsoft has an EDR solution called Windows Defender. But it has many more offerings. For instance, they also have specific solutions for protection your valuable data such as Cloud App Security and Office 365 ATP. They can protect your identity with Azure AD, and Azure ATP. Microsoft also has Azure Security Center to protect the assets that run on Microsoft Azure, and there are many more security solutions in their portfolio.
One thing that seemed to be lacking was a central orchestrator. A coordinator for all your security efforts. Something that ties this all together.
In the past years, enterprises would hook up the alerts that Microsoft security solutions were generating and forward them back to their on-premise SIEM solution as part of their cloud security strategy. But they are struggling to keep pace with the increasing volume and variety of data they process. Unhappy users complained about the inability of their SIEMs to scale and the volume of alerts they must investigate.
Enterprises struggling with the cost of data analysis and log storage often turn to open source tools like Elasticsearch, Logstash, and Kibana (ELK) or Hadoop to build their own on-premise data lakes. However, to gain useful insight from the data they collect, they realiz the expense of building and administering these ‚Äúfree‚Äù tools is just as great as the cost of commercial tools.
Sentinel, orchestrating your security efforts
This is where Azure Sentinel comes in; a central place to analyze your security data, across all parts of your environment. Cloud security solutions like Azure Sentinel are set to disrupt the SOC, Forrester concludes:
‚ÄúThis week, as thousands of security pros gather in San Francisco for RSA, tech titans Microsoft and Google (Alphabet) launch cyber security tools that promise to disrupt the traditional way of taking in and analyzing security telemetry. Chronicle Backstory (an Alphabet company) and Microsoft Sentinel are cloud-based security analytics tools that are addressing the challenges faced by SOC teams such as:
Chronicle and Microsoft are making these challenges cloud native with virtually unlimited compute, scale, and storage. These vendors have a unique advantage over legacy on-premise tools since they also own their cloud infrastructures and aren‚Äôt dependent on buying cloud at list price from would-be competitors.‚Äù
Connecting any and all clouds
One could lead to think that this will be an all-Microsoft centered approach. But nothing is truer. While Microsoft has not confirmed this publicly, they are indeed working with other cloud vendors to get their security data programmatically.
If you take a look at the Data Connections section of the Azure Sentinel preview, you already see a placeholder section for connecting the AWS CloudTrail data soon. I‚Äôll write more about this in an upcoming blog.
The Intelligent Security Graph is at the center of this all
I‚Äôve written about what Microsoft‚Äôs Intelligent Security Graph is before:
‚ÄúMicrosoft describes ISG as a way to ‚Äòbuild solutions that correlate alerts, get context for investigation, and automate security operations in a unified manner.‚Äù
But with the release of Azure Sentinel, it really amplifies that strategy and makes it come to life. The intelligent security graph is a core piece of Sentinel‚Äôs backend to grab the relevant information from other Microsoft services such as Azure ATP, Defender ATP, Azure Security Center, etcetera.
But not only for Microsoft services. Exactly a year ago at RSA 2018, many vendors such as Palo Alto Networks, F5, Symantec, Fortinet and Check Point integrated their solutions into the intelligent security graph. Azure Sentinel leverages those technical integrations to get events from the network.
But not only network vendors integrate with Microsoft‚Äôs intelligent security graph. Well-known names such as Anomali, Sailpoint, Ziften and many others have joined the party recently.
Using the dashboards technology already available in Azure, Sentinel is able to provide you with a single pane of glass on the security of your environment. And because of the graph, it provides detailed out of the box drill-down dashboards for those network vendors, as part of your investigation.
Azure Firewall is the perfect example
But it doesn‚Äôt stop at getting even data from the network. Microsoft just announced new capabilities in its own Azure Firewall, most notably a feature called Threat intelligence-based filtering.
‚ÄúAzure firewall can now be configured to alert and deny traffic to and from known malicious IP addresses and domains in near real-time. The IP addresses and domains are sourced from the Microsoft Threat Intelligence feed powered by The Microsoft Intelligent Security Graph.‚Äù
Threat intelligence-based filtering is default-enabled in alert mode for all Azure Firewall deployments, providing logging of all matching indicators. Customers can adjust behavior to alert and deny.
Democratizing AI: meet Azure Sentinel FUSION
Azure Sentinel features something Microsoft calls FUSION. As Microsoft is looking to democratize Artificial Intelligence, they are making it easy to use machine learning as part of your triage.
Instead of sifting through a sea of alerts, and correlate alerts from different products manually, ML technologies will help you quickly get value from large amounts of security data you are ingesting and connect the dots for you.
For example, you can quickly see a compromised account that was used to deploy ransomware in a cloud application. This helps reduce noise drastically.
Conclusion
I agree totally with Joseph Blankenship:
‚ÄúFor security pros that have been around awhile, don‚Äôt let your cynicism block the potential advantages your organization could experience by making use of Azure Sentinel. Take off the tinfoil hat and realize that Microsoft is a security company now. What Google and Microsoft have introduced will make the entire industry better, and that‚Äôs something to applaud.
The future of cybersecurity, just like the IT resources it protects, is in the cloud. The Tech Titans are staking out a claim and changing the way security solutions are purchased, delivered, and consumed‚Ä¶ and it couldn‚Äôt come at a better time for the industry.‚Äù
Over the course of the next couple of weeks I‚Äôll share my real-world experiences on Azure Sentinel with you in a multi-part blog series at http://www.maartengoet.org.
Part one will be about ‚Äòdesign considerations for Azure Sentinel‚Äô. Stay tuned!
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
178 
1
178¬†claps
178 
1
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/reliable-event-processing-in-azure-functions-37054dc2d0fc?source=search_post---------40,"There are currently no responses for this story.
Be the first to respond.
Event processing is one of the most common scenarios in serverless and Azure Functions. A few weeks ago I wrote about how you can process events in order with functions, and for this blog I wanted to outline how you can create a reliable message processor so you avoid losing any messages along the way. I‚Äôll be honest ‚Äî this blog could have easily broken into two or three parts, but I‚Äôve decided to keep it all here in a single post. It‚Äôs lengthy, but goes from basics all the way to advanced patterns like circuit breaker and exception filters. While these samples are in C#, all patterns work across any language (unless explicitly stated otherwise).
Imagine a system sending events at a constant rate ‚Äî lets say 100 events per second. Consuming these events from Azure Functions is easy enough to setup, and within minutes you could have multiple parallel instances processing these 100 events every second. However, what if the event publisher sends a corrupt event? Or your instance has a hiccup and crashes mid-execution? Or a downstream system goes offline? How do you handle these while preserving the overall integrity and throughput of your application?
With queues, reliable messaging comes a bit more naturally. In Azure Functions when you trigger on a queue message, the function can create a ‚Äúlock‚Äù on the queue message, attempt to process, and if failing ‚Äúrelease‚Äù the lock so another instance can pick it up and retry. This back-and-forth continues until either success is reached, or after a number of attempts (4 by default) the message is added to a poison queue. While a single queue message may be in this retry cycle, it doesn‚Äôt prevent other parallel executions from continuing to dequeue the remaining messages ‚Äî so the overall throughput remains largely unaffected by one bad message. However, storage queues don‚Äôt guarantee ordering, and aren‚Äôt optimized for the same high throughput of services like Event Hubs.
With event streams like Azure Event Hubs, there is no lock concept. To allow for high throughput, multiple consumer groups, and replayability, services like Event Hubs read more like a tape drive when consuming events. There is a single ‚Äúoffset‚Äù pointer in the stream per partition, and you can read forwards or backwards. While reading the event stream, if you hit a failure and decide to keep the pointer in the same spot, it prevents further processing on that partition until the pointer progresses. In other words, if 100 events per second are still coming in, and Azure Functions stops moving the pointer to new events while trying to deal with a single bad one, those events will start to pile up. Before long you have a massive backlog of events that are constantly growing.
Given this offset and consumer behavior, functions will continue to progress the pointer on the stream regardless if the execution succeeded or failed. This means your system and functions need to be aware and structured to deal with those behaviors.
The behavior for the Azure Functions Event Hubs trigger is as follows:
There‚Äôs a few important things to note in this. The first is that if you have unhandled exceptions you may lose messages ‚Äî because even an execution that results in an exception will progress the pointer. The second is that, as is the norm with distributed systems, Functions guarantees at-least-once delivery. Your code and dependent systems may need to account for the fact that the same message could be received twice. I have examples showing both of these behaviors, and how to code around it, below:
For these tests I did the following ‚Äî I publish 100,000 messages to be processed in order (per partition key). I‚Äôll log each message to a Redis cache as it‚Äôs processed to validate and visualize order and reliability. For the first test, I wrote it so every 100th message throws an exception without any exception handling.
When I push 100,000 messages at this sample, here‚Äôs what I see in Redis:
You‚Äôll notice I missed a whole chunk of messages between 100‚Äì112. So what happened here? At some point one of my function instances got a batch of messages for this partition key. That specific batch ended with 112, but at message 100 my exception was thrown. This halted that execution, but the function host continued to progress and read the next batch. Technically these messages are still persisted in Event Hubs, but I‚Äôd need to manually go and re-fetch 100‚Äì112 to reprocess.
The simplest resolution to this is adding a simple ‚Äútry/catch‚Äù block in my code. Now if an exception is thrown, I can catch it inside the same execution and handle it before the pointer progresses. When I added a catch in the code sample above and re-run the test, I see all 100,000 messages in order.
Best Practice: All Event Hubs functions need to have a catch block
In this sample I used the catch to attempt an additional insert into Redis, but you can imagine other viable options like sending a notification, or outputting the event to a ‚Äúpoison‚Äù queue or event hub for later processing.
Some exceptions that arise may be transient in nature. That is, some ‚Äúhiccup‚Äù or other issue may just go away if an operation is attempted again a few moments later. In the previous section I did a single retry in the catch block ‚Äî but I only retry 1 time, and if that retry failed or threw its own exception I‚Äôd be out of luck and still lose events 100‚Äì112. There are a number of tools out there to help define more robust retry-policies, and these still allow you to preserve processing order.
For my tests, I used a C# fault-handling library called Polly. This allowed me to define both simple and advanced retry policies like ‚Äútry to insert this message 3 times (potentially with a delay between retries). If the eventual outcome of all retries was a failure, add a message to a queue so I can continue processing the stream and handle the corrupt or un-processed message later.‚Äù
And the resulting Redis:
When working with more advanced exception catching and retry policies, it is worth noting that for precompiled C# class libraries, there is a preview feature to write ‚ÄúException Filters‚Äù in your function. This enables you to write a method that will execute whenever an unhandled exception is thrown during a function execution. Details and samples can be found in this post.
We‚Äôve addressed the kind of exceptions that may occur if your code hits an exception, but what about the case where the function instance has a hiccup or failure in the middle of an execution?
As stated earlier ‚Äî if a function doesn‚Äôt complete execution the offset pointer is never progressed, so the same messages will process again when a new instance begins to pull messages. To simulate this, during the 100,000 message processing I manually stopped, started, and restarted my function app. Here are some of the results (left). You‚Äôll notice while I processed everything and everything is in order, some messages were processed more than once (after 700 I reprocess 601+). That overall is a good thing as it gives me at-least-once guarantees, but does mean my code may require some level of idempotency.
The above patterns and behaviors are helpful to retry and make a best-effort at processing any event. While a few failures here and there may be acceptable, what if a significant number of failures are happening and I want to stop triggering on new events until the system reaches a healthy state? This is often achieved with a ‚Äúcircuit breaker‚Äù pattern‚Äî where you can break the circuit of the event process and resume at a later time.
Polly (the library I used for retries) has support for some circuit-breaker functionality. However these patterns don‚Äôt translate as well when working across distributed ephemeral functions where the circuit spans multiple stateless instances. There are some interesting discussions on how this could be solved in Polly, but in the meantime I implemented it manually. There are two pieces that are needed for a circuit breaker in an event process:
For my purpose I used my Redis cache for #1, and Azure Logic Apps for #2. There are multiple other services that could fill both of these, but I found these two worked well.
Because I may have multiple instances processing events at a single time, I needed to have shared external state to monitor the health of the circuit. The rule I wanted was ‚ÄúIf there are more than 100 eventual failures within 30 seconds across all instances, break the circuit and stop triggering on new messages.‚Äù
Without going too deep into specifics (all of these samples are in GitHub) I used the TTL and sorted set features in Redis to have a rolling window of the number of failures within the last 30 seconds. Whenever I add a new failure, I check the rolling window to see if the threshold has been crossed (more than 100 in last 30 seconds), and if so, I emit an event to Azure Event Grid. The relevant Redis code is here if interested. This allows me to detect and send an event and break the circuit.
I used Azure Logic Apps to manage the circuit state as the connectors and stateful orchestration were a natural fit. After detecting I needed to break the circuit, I trigger a workflow (Event Grid trigger). The first step is to stop the Azure Function (with the Azure Resource connector), and send a notification email that includes some response options. I can then investigate the health of the circuit, and when things appear to be healthy I can respond to ‚ÄúStart‚Äù the circuit. This resumes the workflow which will then start the function, and messages will begin to be processed from the last Event Hub checkpoint.
About 15 minutes ago I sent 100,000 messages and set each 100th message to fail. About ~5,000 messages in I hit the failure threshold, so an event was emitted to Event Grid. My Azure Logic App instantly fired, stopped the function, and sent me an email (above). If I now look at the current state of things in Redis I see a lot of partitions that are partially processed like this:
After clicking the email to restart the circuit, running the same Redis query I can see the function picked up and continued on from the last Event Hub checkpoint. No messages were lost, everything was processed in order, and I was able to break the circuit for as long as I needed with my logic app managing the state.
Hopefully this blog has been helpful in outlining some of the patterns and best practices available for reliably processing message streams with Azure Functions. With this understanding you should be able to take advantage of the dynamic scale and consumption pricing of functions without having to compromise on reliability.
I‚Äôve included a link to the GitHub repo that has pointers to each of the branches for the different pivots of this sample: https://github.com/jeffhollan/functions-csharp-eventhub-ordered-processing. Feel free to reach out to me on Twitter @jeffhollan for any questions.
#BlackLivesMatter
569 
7
how hackers start their afternoons. the real shit is on hackernoon.com.¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
569¬†claps
569 
7
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/net-core-mensageria-exemplos-utilizando-rabbitmq-e-azure-service-bus-66a81d02a731?source=search_post---------41,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
May 28, 2018¬∑3 min read
Message Brokers s√£o solu√ß√µes voltadas √† troca de mensagens entre diferentes aplica√ß√µes. O uso de deste tipo de tecnologia est√° comumente associado √† integra√ß√£o entre projetos e emprega um pattern de mensageria conhecido como publish-subscribe, o qual prev√™ uma aplica√ß√£o (publisher) da qual se originam as mensagens, geralmente uma fila (queue) controlada pelo Message Broker e um ou mais consumidores (subscribers) desta queue.
A imagem a seguir traz uma representa√ß√£o esquem√°tica deste padr√£o:
Dentre as vantagens oferecidas pela ado√ß√£o de mecanismos de mensageria √© poss√≠vel destacar:
Este post traz 2 exemplos empregando solu√ß√µes de mensageria em aplica√ß√µes baseadas no .NET Core 2.0 e no ASP.NET Core 2.0:
Open source e multiplataforma (contando ainda com suporte a containers Docker), o RabbitMQ √© sem sombra de d√∫vidas uma das alternativas mais populares em se tratando de Message Brokers. Aplica√ß√µes constru√≠das em tecnologias como .NET, Node, Java, Ruby e Python podem se beneficiar dos recursos oferecidos por esta solu√ß√£o em projetos que dependam de mecanismos de mensageria.
No reposit√≥rio a seguir temos exemplos de uso do RabbitMQ com o .NET Core 2.0 e o ASP.NET Core 2.0:
https://github.com/renatogroffe/RabbitMQ-DotnetCore2-Selenium
Aplica√ß√µes .NET podem interagir com um Broker baseado no RabbitMQ empregando para isto o package RabbitMQ.Client (este √∫ltimo j√° compat√≠vel com o .NET Standard):
O RabbitMQ foi tamb√©m tema de um hangout do Canal .NET neste ano de 2018. A grava√ß√£o se encontra dispon√≠vel no YouTube e pode ser assistida gratuitamente (inclusive os projetos aqui mencionados foram descritos em detalhes neste v√≠deo):
Alternativa ao RabbitMQ, o Service Bus √© um servi√ßo de mensageria que integra o Microsoft Azure. A seguir est√° uma nova vers√£o do mesmo exemplo apresentado na se√ß√£o anterior j√° adaptado para uso desta solu√ß√£o (inclui o uso do Application Insights, servi√ßo de monitoramento que integra a plataforma de cloud computing da Microsoft):
https://github.com/renatogroffe/ASPNETCore2_API_AppInsights-ServiceBus-SQL
Em aplica√ß√µes .NET o package Microsoft.Azure.ServiceBus (tamb√©m compat√≠vel com o .NET Standard) permitir√° a integra√ß√£o com o Azure Service Bus:
Conte√∫dos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0/7.1/7.2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
371 
3
371¬†claps
371 
3
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/aws-vs-azure-vs-google-detailed-cloud-comparison-b075a35fc8b8?source=search_post---------42,NA
https://medium.com/free-code-camp/how-to-build-a-serverless-report-server-with-azure-functions-and-sendgrid-3c063a51f963?source=search_post---------43,"There are currently no responses for this story.
Be the first to respond.
It‚Äôs 2018 and I just wrote a title that contains the words ‚ÄúServerless server‚Äù. Life has no meaning.
Despite that utterly contradictory headline, in this article we‚Äôre going to explore a pretty nifty way to exploit SendGrid‚Äôs template functionality using Timer Triggers in Azure Functions to send out scheduled tabular reports. We are doing this because that‚Äôs what everyone wants in their inbox. A report. With numbers in it. And preferably some acronyms.
First, let‚Äôs straw-man this project with a contrived application that looks sufficiently boring enough to warrant a report. I have just the thing. A site where we can adjust inventory levels. The word ‚Äúinventory‚Äù is just begging for a report.
This application allows you to adjust the inventory quantity (last column). Let‚Äôs say that an executive somewhere has requested that we email them a report every night that contains a list of every SKU altered in the last 24 hours. Because of course, they would ask for that. In fact, I could swear I‚Äôve built this report in real life in a past job. Or there‚Äôs a glitch in the matrix. Either way, we‚Äôre doing this.
Here is what we‚Äôre going to be building‚Ä¶
Normally the way you would build this is with some sort of report server. Something like SQL Server Reporting Services or Business Objects or whatever other report servers are out there. Honestly, I don‚Äôt want to know. But if you don‚Äôt have a report server, this gets kind of tedious.
Let‚Äôs go over what you have to do to make this happen‚Ä¶
This is the kind of thing that nobody wants to do. But I think this project can be a lot of fun, and we can use some interesting technology to pull it off. Starting with Serverless.
Serverless is a really good use case for one-off requests like this. In this case, we can use Azure Functions to create a Timer Trigger function.
To do that, I‚Äôm going to use the Azure Functions extension for VS Code. I‚Äôm going to use it for everything in fact. Why? Because I don‚Äôt know you, but I do know it‚Äôs highly likely that you are using VS Code. VS Code is great because it‚Äôs like a movie that all developer‚Äôs can universally agree is completely awesome. Sort of the opposite of ‚ÄúChildren of Men‚Äù. That movie was terrible and you know it.
Make sure you install the Azure Functions extension.
marketplace.visualstudio.com
Now create a new Function App from within VS Code.
Then create a new Timer Trigger function. Timer Trigger functions are scheduled using standard Cron Expressions. You have likely not ever seen before because I had not seen one until a few months ago. And I‚Äôve been in this industry for a LONG time. I am old, father William.
Cron expressions look kind of scary cause they have asterisks in them. In the case below, I‚Äôm saying that when minutes is 0 and seconds is 0 and hours is evenly divisible by 24, fire the function. This would be midnight.
Now we can run this locally (F5). We‚Äôll see in the embedded terminal the schedule on which our Function will be called; the next 5 occurrences.
It feels good, man.
OK, now we need to get some data. I‚Äôm not going to drag you into the specifics of me querying SQL Server from this function because that‚Äôs not what this article is about, but here‚Äôs the code anyway.
I‚Äôm connecting to the database, doing a simple query and‚Ä¶.wait a minute‚Ä¶did not I say I wasn‚Äôt going to get into specifics? You had me there for a minute, but I‚Äôm onto your game!
So this pulls in data and we get it in a JavaScript object that we can pass as JSON. If we were to JSON.stringify this, we will see the data set that we need to send in the report.
OK! We‚Äôve got data, now we just need to make it pretty and email it to someone we don‚Äôt like. How are we going to do that? With SendGrid!
SendGrid is a nifty service with a really nice dashboard. You will like it. Or you won‚Äôt. Either way, you have to use it to get through this blog post.
You can create a free account if you don‚Äôt already have one. That‚Äôs plenty for what we‚Äôre doing here today.
Once you create a report, SendGrid is going to drop you into your ‚Äúdashboard‚Äù. From this dashboard, you need to create a new API Application and get the key.
Make sure you copy your API key when it gives it to you. You can‚Äôt ever get back to it and you‚Äôll have to do this all over again. Let‚Äôs face it: it was kinda boring the first time around.
Copy that key into your Azure Functions project. Put it in the local.settings.json file so you can access it as a Node.js environment variable later.
Now we are going to create a template in SendGrid. That‚Äôs what we will use to design our report. SendGrid has something called ‚ÄúTransactional Templates‚Äù. I have no idea why they are called that, but we are going to be needing one.
Once you create a new one, you have to create a new ‚Äúversion‚Äù. I had a hilariously hard time figuring this out. But then again, my brain is tad on the smallish side of little.
Choose to design your template with the Code Editor. You don‚Äôt need no freakin‚Äô Designer Editor!
SendGrid support handlebars, which is a template syntax that‚Äôs so easy, even I can do it. In the Code Editor, you can paste the JSON data into the ‚ÄúTest Data‚Äù tab‚Ä¶
Now iterate over the data using its key name from the JSON‚Ä¶
It‚Äôs BEAUTIFUL! I‚Äôm crying. Ship it.
ALRIGHT. Fine. We‚Äôll make it a little nicer on the old eyeballs. Here is a style that I shamelessly ripped off of the gorgeous Bulma CSS framework.
It‚Äôs ok at this point for you to be audibly impressed.
Now you might have noticed that the Subject of the email is missing. How do we fill that in? Well, after another embarrassing period of failure followed by introspection, I figured out that it‚Äôs behind the ‚ÄúSettings‚Äù icon on the left. You just have to pass a value in your JSON for ‚ÄúSubject‚Äù.
Now we need to get the template ID and add it to our Azure Functions project. Save this template and select the ID from the main template screen.
Drop it in the trusty local.settings.json file right underneath your SendGrid API key.
Now we are ready to pass our data from our Azure Function to SendGrid and send out this incredible work of business art.
Azure Functions provides a binding for SendGrid. If you create a function through the Azure Portal, it will create this binding for you when you select the ‚ÄúSendGrid‚Äù template. If you are doing it locally like I am, you have to add it yourself.
First you need to open the function.json file for the CreateReport function and add in the SendGrid binding.
The SendGrid binding comes as an extension for Azure Functions. Run the following command in the terminal to install it.
When you run this command, VS Code will ask you to restore some dependencies. You can click restore. Nothing bad will happen‚Ä¶OR WILL IT?!
One other thing you need to do is tweak your extensions.csproj file to reference the latest SendGrid library. This is required to use dynamic templates.
When you add that, VS Code will prompt you to restore again and yes, you definitely need to do it this time. VS Code needs to build these binaries and the restore does that.
OK! Now we‚Äôre ready to send an email via our SendGrid template. Here is the code to do it. It‚Äôs depressingly simple. I know after all this you were hoping for enough code to choke a cat (what? you‚Äôve never heard that metaphor before?), but this is all it takes.
The items of note are me passing in a Subject as part of the JSON. As well as the fact that you can override to/from addresses specified in the function.json file here.
Now you can run your function and wait 24 hours to test it!
No but seriously ‚Äî how do you manually test a Timer Trigger without constantly modifying the damn Cron Job?
I‚Äôll show you how I do it and then you can figure out a better way.
I create an Http Trigger in the same project and call it ‚ÄúRunCreateReport‚Äù. In that function, I just import and call the timer function.
The only drawback to this is that you have to repeat your SendGrid binding settings from function.json in the ‚ÄúCreateReport‚Äù over in the ‚ÄúRunCreateReport‚Äù function.json. But other than that, this works just fine. Now you can run this thing, fire up a browser and hit the URL which will call the timer function immediately. You can test without having to touch that icky old Cron expression.
Now go check your email and bask in the glory of the report. Note that you don‚Äôt have to own an email address to send from SendGrid. You can literally send from any address. Seriously. Go ahead and try. JUST THINK OF WHAT YOU CAN DO WITH THIS POWER.
Here‚Äôs what my inbox looks like. Heads up, it does go to junk. Probably because I don‚Äôt own the sender email address.
WHAT? There‚Äôs a ‚ÄúBusiness Resilience Conference‚Äù? OMG so much business. I bet those people get a LOT of reports.
You can get this project from Github.
github.com
Here are a few other Azure Functions resources to keep you busy.
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
301 
3
301¬†claps
301 
3
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/@maarten.goet/securing-kubernetes-on-microsoft-azure-are-your-container-doors-wide-open-bb6e879cec5d?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Jan 15, 2019¬∑8 min read
Matt Asay from InfoWorld wrote the following a couple of weeks ago: ‚ÄúThe operating system no longer really matters. And for developers and the cloud, that means that Linux no longer really matters. We now live in a Kubernetes world.‚Äù
Kubernetes is quickly becoming the new standard for deploying and managing software in the cloud. Microsoft last year launched their managed Kubernetes offering in Azure called Azure Kubernetes Service (AKS) and IBM‚Äôs main driver behind the $34 billion acquisition of Red Hat in 2018 was its Kubernetes-based OpenShift offering.
Few people have extensive experience with Kubernetes. And if they did have training or have gotten their hands dirty with deployments, how many only focus on general engineering and administration and overlook the security aspect?
A quick search on Shodan, the leading search engine for finding vulnerable internet-connected systems, reveals that in early 2018 over 15K+ Kubernetes dashboards were directly facing the internet, and few of them had security measures in place.
Have you secured your Kubernetes environment, or are your container doors wide open?
Securing Linux to secure Kubernetes
Jose Alvarez summarizes it nicely: you need to make sure you tackle the basics first ‚Äî secure Linux to secure Kubernetes. There are a couple of things to look at:
By no means is this a complete list, but a justa few things to get you thinking on this topic. Do you have other things you secure on Linux as part of hardening your Kubernetes environment? Please share them in the comments below.
Whilst it‚Äôs possible to run Microsoft Windows Server as the OS for Kubernetes worker nodes, more often than not, the control plane and worker nodes will run a variant of the Linux operating system. There might be many factors that govern the choice of Linux distribution to use (commercials, in-house skills, OS maturity), but if it‚Äôs possible, use a minimal distribution that has been designed just for the purpose of running containers.
Examples include CoreOS Container Linux, Ubuntu Core, and the Atomic Host variants. These operating systems have been stripped down to the bare minimum to facilitate running containers at scale, and therefore, have a significantly reduced attack surface.
PRO TIP: Aqua Security is also available directly from your Kubernetes cluster overview page in Azure!
Kubernetes is a framework, not a platform
So why is Kubernetes not secure by default? Jussi Nummelin summarizes it nicely: from a security standpoint, Kubernetes is more like a framework intended to be utilized in building more higher-level solutions. What that means in practice is that plain Kubernetes does not lock down everything properly by default, instead it is expected to be configured by the operators setting it up.
Not sure how Kubernetes works? Microsoft together with the Cloud Native Computing Foundation (CNCF) released a cartoon called ‚ÄúPhippy Goes To The Zoo‚Äù telling about the basics. But also Daniel Sanche has written a great overview of the core concepts in Kubernetes you could look at.
Tesla found out, the hard way
I wrote about the rise of the cryptocurrency miners in a previous blog. Well, add Tesla to the legion of organizations that have been infected.
In a report published early 2018 researchers at security firm RedLock said hackers accessed one of Tesla‚Äôs Amazon cloud accounts and used it to run currency-mining software. The initial point of entry for the Tesla cloud breach, the report said, was an unsecured administrative console for Kubernetes.
‚ÄúThe hackers had infiltrated Tesla‚Äôs Kubernetes console which was not password protected,‚Äù RedLock researchers wrote. ‚ÄúWithin one Kubernetes pod, access credentials were exposed to Tesla‚Äôs AWS environment which contained an Amazon S3 bucket that had sensitive data such as telemetry.‚Äù
ArsTechnica has a great in-depth article on the breach which you can read here.
Netscylla was intrigued on how easy this could happen to any customer cloud estate. So, they decided to do some OSINT (Open Source reconnaissance) and their own research. Want to understand what ports are involved and how to find vulnerabilities? Have a look at their writeup.
Kubernetes on Microsoft Azure
One of the questions I have been hearing a lot from customers, is about how RedHat‚Äôs OpenShift (which is based on Kubernetes) compares to the managed Kubernetes services provided by most of the major public cloud providers.
Google introduced the world to Kubernetes mid-2014, and their Google Cloud Platform has a managed service called ‚ÄúGoogle Kubernetes Engine‚Äù (GKE). Amazon has a service called ‚ÄúAmazon Elastic Container Service for Kubernetes‚Äù (EKS) and Microsoft has a managed Kubernetes based offering called ‚ÄúAzure Kubernetes Service‚Äù (AKS).
If you want more detail about choosing OpenShift or not, Chris Saunders, who works at RedHat, wrote a lengthy article on what OpenShift provides additionally to a plain-vanilla Kubernetes environment. For now, I‚Äôll focus on Microsoft‚Äôs Azure Kubernetes Service.
Amplifying that Microsoft is serious about containers, Kubernetes and open source in general is the hire of Brendan Burns. Brendan helped create the Kubernetes open source container orchestration technology and joined Microsoft to work on the Azure in 2016.
Want to get your own Kubernetes cluster running on Microsoft Azure? Microsoft has great documentation on this, and how to get your first application running on that cluster.
Securing your Kubernetes environment
Here are some general recommendations on improving security:
Etienne Dilocker has written a lengthy article on comparing authentication methods for Kubernetes.
Microsoft provides an own network policy module to implement Kubernetes network policies with the Azure CNI plugin for acs-engine and AKS called Azure NPM. My good friend Daniel Neumann wrote a blog on how to use this, and you can find the relevant Microsoft documentation here.
John Kindervag‚Äôs Zero Trust Network Architecture is a great concept to embrace, and optimizes the security architectures for future flexibility. It works great in a data-centric world with shifting threats and perimeters and provides you with new network designs that integrate connectivity, transport, and security around potentially toxic data. John calls this ‚Äúdesigning from the inside out.‚Äù
PRO TIP: Docker is often viewed as a simple and fast replacement for virtualization. While this is true to certain extent it should be noted that containers, compared to virtual machines, do not provide the same level of isolation from either the host or other containers running within the same host. Please always remember that the host‚Äôs kernel is shared among all the running containers.
The Cloud Native Computing Foundation (CNCF) published a couple of helpful pointers: 9 Kubernetes Security Best Practices Everyone Must Follow and also Nikita Mazur shared some great insights into the things you should be doing to secure your Kubernetes environment.
Beware: the Kubernetes network stack!
The most contentious point for building a Kubernetes cluster is the network stack. There are a ton of providers available and if you have never deployed K8s before, it can be daunting!
Kubernetes handles networking using a different approach to the normal ‚ÄòDocker way‚Äô of doing things. Docker uses port-forwarding from the host to the underlying container using virtual bridge network. This means that coordinating ports across multiple devices is a difficult thing to do at scale and exposes users to cluster-level issues outside of their control.
Kubernetes on the other hand imposes the following requirements on its network implementation:
Rory Chatterton does a great job in explaining, and providing you with an approach to use.
DevSecOps is the new DevOps
While DevOps approach to software delivery has positively revolutionized the IT industry, the security requirements have largely been left unattended. This is what DevSecOps aims to fix.
DevSecOps is one of the most important DevOps trends. It is an approach to IT operations security, allowing to utilize the principles and best practices of DevOps to ensure better, faster and more secure software delivery. This essentially means, that all the security requirements are codified and weaved into the automated unit tests from the start, instead of having to deal with them before the product release.
‚ÄúDevSecOps is the trend to automate all security checks by codifying them in the unit tests and using them from the very beginning of the software development, not at the end of the cycle.‚Äù
Arseny Chernov has written a lengthy article on securing DevOps and what approach to take. A recommended reading if you want to go from DevOps to DevSecOps.
Summary
Kubernetes is a very powerful platform that makes application deployment much easier compared to a traditional on-premises server or a VM. That‚Äôs probably one of the key success factors that made Kubernetes adoption boom over the last couple of years. The counterpart is that this way of doing things is relatively new, which unfortunately sometimes leads to poor implementation of good practices in terms of security.
The need to secure your Kubernetes environment is real, as exploits are now being found in the wild. Learning about Kubernetes and understanding how the framework works is essential to understand what to secure and I would certainly recommend Nigel Poulton‚Äôs course for this.
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
272 
272¬†
272 
Microsoft MVP and Microsoft Regional Director.
"
https://medium.com/@mauridb/calling-azure-rest-api-via-curl-eb10a06127?source=search_post---------45,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
May 4, 2018¬∑4 min read
In these days I needed to call Azure REST API directly, without having the possibility to use some nice wrapper like AZ CLI or .NET SDK or Python SDK or whatever, since the REST API I needed to call was not included in any of the mentioned tools.
Make sure you check out the latest updates at the bottom of the article!
I decided to use curl since it is one of the easiest way to issue HTTP requests. But it turned out to be a little more complex that I what I could have expected at the beginning, especially while dealing with the authentication phase. The entire process is pretty simple as you‚Äôll see, documentation is just a bit scattered all around...so it may be difficult to quickly understand the path you must follow to get everything working nicely.
Azure API security, and thus authentication (which is based on OAuth2) is a pretty broad topic as you can see from the long documentation available here:
docs.microsoft.com
I read throughout all the documentation, hyperlinks included and at the end I was still confused. There are so many options and each one have quite a number of prerequisites that requires even more reading. So, for my future reference and for all those who just need a straightforward way to solve the problem, here‚Äôs the list of all steps required.
In order to access resources a Service Principal needs to be created in your Tenant. It is really convenient to do it via AZ CLI:
for much more details and options see the documentation:
docs.microsoft.com
What is happening here is that you‚Äôre registering your application in order to be able to be recognized by Azure (more precisely: from the AD tenant that is taking care of your subscription). Exactly like when you register your application to access Twitter or Facebook in order to be able to read and write posts/tweets/user data and so on.
As said before authentication used the OAuth2 protocol, and this means that we have to obtain a token in order to authenticate all subsequent request. We need to use the client_credential flow:
all the three required information:
can be obtained from the previous step. You already have the PASSWORD since you used it to create the Service Principal. The TENANT_ID and the APP_ID will be returned by the az ad sp create-for-rbac command you executed before. Otherwise you can execute the following az command to find it the tenant id:
And the following to get the APP_ID:
The result of the curl call will be an Authorization Token that looks like the following:
The obtained token that needs to be used in the Authorization HTTP header as the Bearer Token to make sure your HTTP call will be authorized:
And that‚Äôs it. Is really easy at the end. And once you have the token it is also easy to use it in your preferred REST client tool, be it Postman or Insomnia.
If you want learn more on how to use the OAuth2 authentication protocol to access Azure, just go here:
docs.microsoft.com
If you need a token just to run some test and you don‚Äôt want to go through Service Principal creation I just discovered that now you can just do
and you‚Äôre good to go, you‚Äôll get your access token with a maximum validity of 1 hour, which is more than enough to do tests. Using curl is really easy now:
In the latest version of AZ (2.0.67, at moment of writing) there is new command rest that allows to call any azure REST API with just one command:
the {subscriptionId} will be automatically replaced with your active subscription Id. Great! Can‚Äôt be easier that this now :)
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
407 
14
407¬†
407 
14
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://medium.com/@gercheq/how-500-000-microsoft-azure-sponsorship-might-kill-your-startup-42912f9b22a1?source=search_post---------46,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gercek Karakus
Aug 6, 2018¬∑6 min read
At Raklet, we‚Äôre building automated messaging and payments solutions for communities. We got accepted into Microsoft Ventures Accelerator in April 2016 and we got offered $500,000 Azure sponsorship for the 3 years. Yay!
We were really happy to receive the credit as server costs were one of the major costs at that time. We started migrating our infrastructure from a monolithic architecture to a more scalable modular one. We‚Äôve setup local, dev, test, prod resource groups; scaled up our app services and databases. Everything was running much faster, finally!
Then, fast-forward 24 months, I realized that our monthly server costs ramped up above $20,000/month. Unbelievable.
At that point, we only had 12 months left on our sponsorship and we‚Äôd run out of credits even before the expiration deadline. So it was time for us to focus on infrastructure optimization instead of growth.
Billing for Microsoft Azure Sponsorship is not on the portal so you have to visit an external website for the usage.
Visiting the https://www.microsoftazuresponsorships.com website, you can only generate an excel sheet similar to the one below:
ResourceIDs and names in this sheet do not match to the ones on Azure portal. So we decided to reach out to the Professional Direct Services. After spending 2 months back and forth with them, they simply gave up and we were left with this problem. There‚Äôs no way for Microsoft Azure Sponsorship usage to be tied to actual resources on Azure Portal.
I hope this email finds you well. Please note that my team, the Startups Business Desk, has been closely following the congruent support request #118041818024304. As stated in that support request, it is a technical limitation of Azure Sponsorship reporting that we are unable to provide usage by resource group. We understand that this is an important feature and heavily weighs on your continued use of the platform and apologize that we cannot provide a workaround at this time. Please note that engineering is working to implement reporting by usage group in the future. However, we do not have an ETA. ‚Äî Microsoft Azure Team
We were left with the excel sheet to figure out what was going on with our billing. So, we built pivot tables to figure out what was going on with our account.
We downloaded the excel file and became a pivot table master. With some tricks, we were able to identify break down of each service such as name, type and resource.
Sounds like the ultimate solution but again, ID or names in this sheet do not map to your resources on Azure.
We were still not able to identify which resources were being used for different resource groups (local, dev, test, prod) out of our 250+ resources. So we had to drill down into each resource to find our way to optimization.
We were able to drop down to $7.5k/month in May and $4k/month in July. Below you can see the breakdown for July 2018.
Let me summarize what we have done for each service type and how much it helped:
With a little bit educated guess work, we can categorize resources to different groups (again in a sheet) and see that our daily costs are around ~$120/day as of August 1st, 2018.
We‚Äôre lucky to identify this issue with Azure billing with sufficient time in our hands so that we can prepare accordingly.
We‚Äôve always postponed optimizing our infrastructure in order to focus on growth which is pretty much understandable startups searching for product-market fit. However, years pass by with the blink of an eye and credits expire much faster than anticipated.
Our goal is to drop our costs to $1,000 per month and we‚Äôll see how far we can go‚Ä¶
Thanks for reading this far and please spread the word üôè
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more product & design stories featured by the Journal team.
Product Architect @Raklet
524 
8
524¬†
524 
8
Product Architect @Raklet
"
https://faun.pub/deploy-to-azure-kubernetes-service-aks-using-azure-devops-and-helm-77f3fa804ee7?source=search_post---------47,"Lately I was seeing lots of innovation and excitement on deployment automation using DevOps and Kubernetes. This have nothing to do with software development (I mean writing code). This is the business of the DevOps guys. Especially for someone like me who spent the last five years focusing on software development, and so much far away from anything Ops.
I was curious, but really curious so that I decided I want to learn more about these stuff. In this blog I want to share what I have learned.
I started with Azure DevOps, created the CI/CD pipelines for ASP.NET Core, Angular, Xamarin and Dockerized apps. I used Azure and App Center to deploy to.
Then I started thinking about the Database ! I found we can add it to the CI/CD pipelines. During the Build, we create the .dacpac file. Then we publish it during the release. I also used ARM templates (IaC) to automate the deployment of the infrastructure.
After that, I moved to Kubernetes, created a Docker container and deployed it to k8s. At first, I used kubectl to deploy manually. Then I automated the process of deployment through Azure DevOps.
This blog will serve as a continuation to these workshops. Should we start ? I hope you got a clear idea about k8s and you are ready to learn Helm Charts. So what is Helm ?
Helm is the package manager for Kubernetes.
It is npm for javascript, nuget for .NET and maven for Java. Helm makes it easier to deploy apps to Kubernetes. It can deploy multiple apps with multiple yaml files as one single package. It provides a way to solve dependencies between different resources. The package is called Chart. Charts could be shared and reused by different teams. If you want to deploy Cassandra, you‚Äôll have a Chart for that. If you want to install SonarQube or Jenkins, then that becomes easy as running one single command. This means you don‚Äôt need to figure out how to install the web app, the database, mount a disk volume, connect web app to database, run a script that waits until the database starts...
The community creates the Charts and they are verified before they get published as official/stable. They are published here: github.com/helm/charts/tree/master/stable
The traditional way to deploy an application to k8s is through yaml files. These files contains k8s objects like Deployment, Service, ConfigMap, Secret‚Ä¶ They describe the required configuration like which docker containers to use, the networking, the persisted volume, secret keys‚Ä¶ The ones we use are here: github.com/HoussemDellai/ProductsStoreOnKubernetes
Once we have the required yaml files, we can deploy them to Kubernetes using the following command:
kubectl apply -f deployment.yaml
Helm needs to be installed into both the machine (client) and Kubernetes cluster (server). For the client and depending on the OS, it could be installed by running one of the following commands:
For the server side (Helm is called Tiller in this land), it could be installed by:
Check the documentation here for more details on the install. docs.helm.sh/using_helm/#installing-helm
To generate a sample Chart, we can use helm create command. This will create sample files for deployment, ingress and service. All they have values defined in values yaml file. The Chart file defines the version of the template. helmignore is the equivalent of gitignore, used to not include specific files into the package. Then, lint command to validate the template.
With Helm, we will reuse the same k8s yaml files, but we‚Äôll make them configurable and reusable. We‚Äôll extract all the values that might change from one deployment to another. And we‚Äôll put them inside a specific file that contains all these values defined as key-value pairs. This file is values.yaml.
Kubernetes configuration files (deployment, service, pvc, configmap, secret, pod‚Ä¶) can use these values to set its properties. The advantage is now more clear: we have one single file to configure k8s objects.
Note: Another advantage of using Helm is to resolve dependencies between objects.
Now these values should be substituted using Helm. For this reason, we need to install Helm on both the client machine and the cluster (server). Helm on the server is called Tiller. Here‚Äôs the official guide to install Helm (helm.sh/).
Note: Helm on the server called Tiller will be removed by the v 3.0 of Helm.
Now we can deploy the the k8s objects using Helm cli instead of kubectl cli. The first step is to create the package:
This will create a tgz compressed file that contains all the files with a specific version 0.1.0.
Then, to deploy the generated package, we run:
This will deploy the v 0.1.0 of the the package. To update the app, we make the changes to the files, generate a new package (v 0.1.1), and run:
At this point you can check if the app was successfully deployed by running either viewing the kubernetes dashboard,
$ kubectl get {deployments/services/pvc/configmap} or using $ helm
We want to automate the deployment. The goal is that in each time we push new commit to the app‚Äôs source code, a new package will be created during the CI pipeline. And that package will be deployed during the CD pipeline.
The CI/CD pipelines are at the end a sort of sequence of command lines. This means that the task that will create the Helm package should run the command $ helm package productsstore.
But before that, we need to make sure Helm is installed on the client side. For that, we‚Äôll use the task ‚ÄúPackage and deploy Helm charts‚Äù with the command:
Now, we are ready to create the Helm package. We‚Äôll use again the same previous task to generate the package, as following:
Note: The first two tasks, will build and push a Docker container into Docker Hub. And the last task will copy and publish the Helm package to an artifact called drop.
We are done with the CI/Build pipeline. We can run the pipeline using the Queue button and check from the console view that Helm was installed and the package was generated sucessfully.
Let‚Äôs move to the CD/Release pipeline. In this stage, we want to get the package published during the Build and deploy it to Kubernetes. Let‚Äôs start creating the pipeline by choosing the template ‚ÄúDeploy an application to a Kubernetes cluster by using its Helm chart.‚Äù
After configuring the connection to AKS cluster, we need to go through 3 steps. First, we need to install helm in both client and server (Tiller). That‚Äôs the role of the first task which will install 2.11.0. Then, we need to run helm init, to configure helm on client side. Finally, we run helm upgrade with the specified package.
Note: Despite we already installed Helm on client side during the Build pipeline, we need to install it again during the Release. That is because sometimes it is not the same machine that runs both Build and release.
Now we are ready to deploy our app by running the Release pipeline.
Join our community Slack and read our weekly Faun topics ‚¨á
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
237 
1
237¬†claps
237 
1
Written by
Premier Field Engineer at Microsoft, ex MVP, Cloud & DevOps enthusiast. I share what I learn in my professional experience through articles and videos.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium‚Äôs largest DevOps publication.
Written by
Premier Field Engineer at Microsoft, ex MVP, Cloud & DevOps enthusiast. I share what I learn in my professional experience through articles and videos.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium‚Äôs largest DevOps publication.
"
https://medium.com/data-hackers/aws-vs-google-cloud-vs-azure-o-que-cada-um-tem-de-melhor-52107174f7b7?source=search_post---------48,"There are currently no responses for this story.
Be the first to respond.
Pessoal de tecnologia j√° sabe: depois do ‚Äúminha linguagem √© mais cabulosa‚Äù, tem discuss√£o mais comum no buteco da TI que ‚Äúqual Cloud √© melhor?‚Äù
Inspirado por uma discuss√£o no Slack do Data Hackers, resolvi escrever esse texto breve sobre as MINHAS opini√µes sobre o que cada cloud tem de vantagem sobre a outra. Minha avalia√ß√£o, obviamente, √© enviesada. Se voc√™ tem outra opini√£o, comenta a√≠ pra gente trocar aqueles 5 minutos saud√°veis de porrada, sem perder a amizade HAHA!
A cultura da Amazon sempre foi de fazer o poss√≠vel pelos consumidores. Jeff Bezos n√£o tem s√≥ a cara de louco, ele realmente √© louco pelo mercado.
Muita das vezes, a AWS lan√ßa um produto sem ele estar nem mesmo pronto! Isso √© horr√≠vel? Sim! Por√©m abre uma vantagem absurda: a de ouvir os primeiros clientes que aplicam para o teste da ferramenta‚Äî que s√£o os que querem aquele produto badly!
Mas uma coisa que eu vejo que √© essencial, e muita gente que t√° fora do ramo de cloud as vezes se esquece, √© a for√ßa da Rede de Parceiros. Um grande fornecedor, n√£o consegue vender, nem suportar as vendas, sem uma forte rede de suporte. E a AWS valoriza, capacita e apoia muito seus parceiros.
Pensa s√≥, um Arquiteto de Solu√ß√µes da AWS (SA), por mais foda que seja, nunca vai alcan√ßar a expertise de um bom consultor com anos de experi√™ncia numa sub-area da cloud (containeres, data, ML‚Ä¶). Com 2 anos, √© bem prov√°vel que o SA saia do pa√≠s. Fora que ele fica longe do dia a dia dos projetos nos clientes, que √© onde o pau quebra de verdade. Essa √© indiscutivelmente uma vantagem competitiva da AWS. A Oracle dominou (e ainda domina) o mercado de bancos de dados on-premises justamente com essa estrat√©gia. Andy Jassy, que n√£o √© bobo, copia o que deu certo.
Todos os grandes produtos da Google Cloud s√£o fruto de anos de pesquisa dentro da pr√≥pria empresa. Podemos listar alguns que tem lideran√ßa quase que inquestion√°vel:
GCP preza pela inova√ß√£o dos seus servi√ßos. Tais produtos s√£o o estado da arte nas √°reas de dom√≠nio, e por isso dominam o mercado com folga. GCP, por sua vez, tem uma rede de parceiros fraca e, francamente, n√£o est√° nem a√≠ pra eles. Se tivesse uma boa rede e um customer services decente, estaria despontando nessa briga, sem sombra de d√∫vidas.
Desde que Satya Nadella assumiu como CEO da Microsoft, a estrat√©gia da empresa mudou muuuuito. Voc√™ deve ter percebido isso ap√≥s a compra do Github, pela bagatela de U$ 7.5 bilh√µes. Mas o que talvez voc√™ n√£o saiba, √© que grande parte dos novos servi√ßos da Azure s√£o sobre plataformas open-source amplamente adotadas. Posso listar algumas:
O suporte da Azure e o apoio √† rede de parceiros √© bom, n√£o √© t√£o forte quanto da AWS, mas beeem melhor que o da Google. Com essa pol√≠tica, a Microsoft diminui a fric√ß√£o de ado√ß√£o de cloud.
Isso sem contar que Cloud H√≠brida j√° existe na Azure desde sua funda√ß√£o, principalmente para virtualiza√ß√£o e bancos de dados.
Digital Ocean: se destaca pela facilidade de uso. √â muito f√°cil come√ßar um projeto na Digital Ocean. O problema vem quando se exige escala. √â comum ver empresas migrando para uma das 3 anteriores, visando diminuir os custos e ter acesso a servi√ßos mais complexos que simplesmente VMs, Redes e Storage.
Heroku: √© a mais dev-friendly de todas. Claro que √© mais um PaaS do que um IaaS, mas vale citar dado o tempo de mercado e grande penetra√ß√£o no mundo dev. Lembro ainda que a Heroku foi a primeira a disponibilizar Kafka-as-Service de todos provedores.
IBM/Red Hat: nem iria citar a IBM aqui, j√° que seu projeto de Cloud, o Bluemix, vinha sofrendo v√°rias baixas no mercado. Mas um shift bem importante nessa batalha foi a compra da Red Hat, que trouxe todo o know-how da plataforma OpenShift, o melhor provedor de servi√ßos de Containeres na minha opini√£o de noob.
Resumindo, as principais caracter√≠sticas que levam a ado√ß√£o de cada cloud, na minha opini√£o s√£o:
Claro que existem diversos casos e nem sempre isso √© verdade. A melhor cloud √© aquela que resolve seu problema, mais r√°pido e no menor custo. E essa decis√£o tem muito mais vari√°veis que essas que apontei nesse texto.
[Edit 1] ‚Äî Eu n√£o entrei em detalhes de custo. Por dois motivos: qualquer um consegue avaliar isso no primeiro contato com qualquer cloud. N√£o faz sentido expor isso, j√° que escrevo a partir da minha razo√°vel experi√™ncia com clouds. Segundo: custo √© relativo e particular de cada produto e √† produtividade do time que faz o sustain/evolution da plataforma. Quanto mais um produto me entrega, melhor o meu servi√ßo/produto. Minha organiza√ß√£o ganha sobre isso em diversos aspectos. Logo, avaliar somente custo, n√£o √© diferencial‚Ä¶ √© tratar tecnologia como commodities.
E a√≠? Qual sua opini√£o sobre essa guerra de clouds? Voc√™ √© fanboy de alguma?Comenta a√≠ enquanto eu vou buscar minha luva de boxe üòÅ Valeu e at√© a pr√≥xima!
Blog oficial da comunidade Data Hackers
285 
2
285¬†claps
285 
2
Written by
CTO | Lead Data Engineer | Co-Founder of Data Hackers and Dadosfera. Loves science, code and cats ^*^
Blog oficial da comunidade Data Hackers
Written by
CTO | Lead Data Engineer | Co-Founder of Data Hackers and Dadosfera. Loves science, code and cats ^*^
Blog oficial da comunidade Data Hackers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/deploying-terraform-infrastructure-using-azure-devops-pipelines-step-by-step-d58b68fc666d?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 13, 2020¬∑13 min read
Azure DevOps is a hosted service to deploy CI/CD pipelines and today we are going to create a pipeline to deploy a Terraform configuration using an Azure DevOps pipeline.
In this story, we will take a look at a step by step procedure to have our Azure DevOps Pipelines ready in few minutes.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/using-aad-pod-identity-in-your-azure-kubernetes-clusters-what-to-watch-out-for-73d5d73960f?source=search_post---------50,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Azure has been improving its products to support contextual authentication. This is powered by a feature called Managed Identities, which, in simple terms allows you to assign an identity to a Azure Compute (i.e. AKS, Virtual Machine, App Service, etc). Applications inside that compute can then request access tokens to access other Azure Services, such as a Sql Azure database or KeyVault instance.
Without going into too much detail, this is based on the Azure Instance Metadata API. A HTTP end-point which returns a new access token when called inside an Azure compute with this feature enabled. The URL for calling it looks like this:
In this case specifically, instead of having a connection string in your application with a ‚Äúfixed‚Äù user name and password for your database, you would instead use an access token generated at run-time. An example from the official documentation on setting this for an App Service and Sql Azure can be found here.
The idea is quite interesting and provides a few security benefits for your applications, such as:
Last year, Microsoft started an open source project to bring this concept to Kubernetes clusters, allowing you to bind an Azure Managed Identity to a running Pod, its name is aad-pod-identity.
To make the security concerns raised here clearer, we will consider a multi-workload cluster, shared among at least two different teams. Each one of them being responsible for their own pipelines, namespaces (within the cluster) and resource groups (within Azure), all properly configured and with least privileges in mind.
The green box represents the cluster control plane, whilst the blue box represents Azure control plane. The red dotted rectangle represents what Team A should have access to, whilst the blue is the same for the Team B.
The idea behind the approach above is to implement security in depth. Ensuring that if any point gets compromised, it stays contained within its vicinity and doesn‚Äôt take over the entire cluster or Azure resources. There are a lot of things that need to be put in place in order to attain that, which goes beyond this post. The goal here is to highlight where AAD Pod Identity may make it harder to attain that.
Before going any further, it‚Äôs important to first understand how it all fits together.
Once deployed, aad-pod-identity will add a few new things into your cluster:
Node Managed Identity is a daemonset, that hijacks (yes, this is pretty much the same as a man-in-the-middle attack) all calls to Azure‚Äôs Instance Metadata API from each node, processing them by calling MIC instead.
Managed Identity Controller is a pod that invokes Azure‚Äôs Instance Metadata API, caching locally tokens and the mapping between identities and pods.
A new Customer Resource type that represents an Azure Identity inside Kubernetes.
A new Customer Resource type that links Azure Identities to Pods inside the cluster, using labels.
A sample application would look like this:
There are a few things worth noting;
Here are the scenarios that could lead to lateral movement and privilege escalation by abusing the default settings of this project.
Some of these considerations may or may not be relevant to you. This will largely depend on your security posture, and your use case. For example, if you only have a single application within your cluster, a single Azure Identity shared across all applications, or if in general ‚Äúleast privilege‚Äù is not high in your priority list. :)
By re-using the same value for the aadpodidbinding label in the customer-pod, a rogue pod would be linked to the customer identity. Now if it requests for an access token from the Instance Metadata API, it will be granted one that allows it to do any actions that Customer Identity is allowed in Azure:
In this scenario, a compromised account with read access to the default namespace and create pod access anywhere else within the cluster, would be able to acquire access tokens on behalf of the Customer Manager Identity.
What makes this more likely to happen, is the fact that both AzureIdentity and its binding are in a shared namespace. Which can mean that multiple teams/applications will have access to it, even if just as read-only.
Similarly to the scenario above, a new AzureIdentityBinding can be created, linking to the existing customer AzureIdentity.
At this point in time, the AzureIdentityBinding object must be created in the default namespace. Once namespaces are supported, it could also be deployed in other namespaces, if the annotation forceNamespaced is not set to true.
In this case, the compromised account trying to move laterally would need pod read and AzureIdentity/Binding write accesses to the default namespace and also create pod access anywhere else within the cluster.
Another version of the scenario above. This time creating both an AzureIdentity and AzureIndentityBinding, however, pointing to the Managed Identity in the prod-customer-rg resource group.
Access permissions and threat vectors would be pretty similar to the previous scenario.
The NMI pod ‚Äúintercepts‚Äù all the requests to the Instance Metadata API, and handles it internally. However, none of this traffic is sent using a secure channel (TLS). The access tokens in always transported in plain-text.
Privileged pods with NET_ADMIN capabilities can eavesdrop network traffic on anything scheduled on the same node, which could benefit from plain-text sensitive information being sent around. It is actually worse on this case, that the end-point address is well-known.
Note that if the cluster is configured to have Managed Identity enabled, the NMI pod will talk with the Azure Instance Metadata. If it is not, it will talk with Azure AD instead, in which case, part of that communication would be under TLS (from nmi to Azure AD) and part of it won‚Äôt (from requesting pod to nmi).
This is a bit more complicated than the previous scenarios, although it only requires the compromised account to have create pod access. There must be nothing blocking the creation of privileged pods ‚Äî which is the default behaviour.
Note that it does not matter what namespace the rogue pod is deployed. A privileged pod goes beyond that level of isolation, and will have access to all the traffic within the node it was scheduled in.
Currently AKS stores in plain-text the SPN (Service Principal Name) credential used for the cluster to talk with the Azure API. This file is physically available within each and every node. This is a known issue and I blogged about it a while ago.
The MIC pod mounts that file into itself, therefore, any user (or service account) with execute access on MIC has access to it with a single line:
Note that the existing template provided on the aad-pod-identity places all its components in the default namespace.
The threat vector here is any user, or service account, which has pod execute access in the default namespace would be able to privilege escalate to at least have access to all Azure resources that the cluster has. For example, it could manipulate the node VMs, load balancer, Network Security Groups (NSGs), etc.
This can also be a problem on AKS-Engine, if it was setup with useManagedIdentity = false.
The overall concern here is ‚Äúhow easy‚Äù lateral movement between the cluster Control Plane and the Azure Control Plane can be achieved, for a non-admin user, after all, none of the scenarios above requires clusterAdmin rights.
The recommendations below covers the scenarios mentioned above for the implementation of AAD Pod Identity. But it also focuses on improving the isolation within the cluster. Here it goes:
Almost two decades ago it was quite common to find references to SD3+C in Microsoft‚Äôs security literature. That concept was a shorthand for: Secure by Design, Secure by Default, Secure in Deployment, and Communications. Basic concepts which meant a more holistic view to security in software development.
That concept was as valid then, as it is now, especially when security is high on your priority list. However, it is clear that Secure by Default is not ‚Äútop of mind‚Äù in quite a few open source projects (including Kubernetes), which then requires a lot more know-how from end-users to implement them securely.
ITNEXT is a platform for IT developers & software engineers‚Ä¶
205 
1
205¬†claps
205 
1
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/how-to-pass-variables-in-azure-pipelines-yaml-tasks-5c81c5d31763?source=search_post---------51,"There are currently no responses for this story.
Be the first to respond.
This is a quick reference on passing variables between multiple tasks in Azure Pipelines, a popular CI/CD platform. They have recently enabled support for multi-stage pipelines defined in YAML documents, allowing the creation of both build and release (CI and CD) pipelines, in a single azure-pipelines.yaml file. This is very powerful, as it lets developers define their pipelines to continuously build and deploy apps, using a declarative syntax, and storing the YAML document in the same repo as their code, versioned.
One recurrent question is: how do you pass variables around tasks? While passing variables from a step to another within the same job is relatively easy, sharing state and variables with tasks in other jobs or even stages isn‚Äôt immediate.
The examples below are about using multi-stage pipelines within YAML documents. I‚Äôll focus on pipelines running on Linux, and all examples show bash scripts. The same concepts would apply to developers working with PowerShell or Batch scripts, although the syntax of the commands will be slightly different. The work below is based on the official documentation, adding some examples and explaining how to pass variables between stages.
This is the easiest one. In a script task, you need to print a special value to STDOUT that will be captured by Azure Pipelines to set the variable.
For example, to pass the variable FOO between scripts:
Full pipeline example:
You can also use the $(FOO) syntax inside task definitions. For example, these steps copy files to a folder whose name is defined as variable:
Wondering why the vso label? That's a legacy identifier from when Azure Pipelines used to be part of Visual Studio Online, before being rebranded Visual Studio Team Services, and finally Azure DevOps!
Passing variables between jobs in the same stage is a bit more complex, as it requires working with output variables.
Similarly to the example above, to pass the FOO variable:
A full example:
At this time, it‚Äôs not possible to pass variables between different stages. There is, however, a workaround that involves writing the variable to disk and then passing it as a file, leveraging pipeline artifacts.
To pass the variable FOO from a job to another one in a different stage:
Example:
Here‚Äôs the pipeline running. Note in the second stage how line #14 shows some value in both bash scripts. However, take a look at the script being executed on line #11: in the first case, the variable was expanded inside Azure Pipelines (so the script became echo ""some value""), while in the second one bash is reading an environmental variable (the script remains echo ""$FOO"").
If you want to pass more than one variable, you can create multiple files within the $(Pipeline.Workspace)/variables (e.g. for a variable named MYVAR, write it inside $(Pipeline.Workspace)/variables/MYVAR), then read all the variables in the second stage.
Originally published at https://withblue.ink on August 5, 2019.
Any language.
326 
8
326¬†claps
326 
8
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Cooker of great risotto. Sometimes tech nerd. Driving dev tools, @code & open source @Microsoft @Azure ‚òÅÔ∏è Opinions are mine üáÆüáπüá®üá¶üá∫üá∏
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://koukia.ca/entity-framework-core-2-0-vs-dapper-net-performance-benchmark-querying-sql-azure-tables-7696e8e3ed28?source=search_post---------52,"While the .Net Core 2.0 is still being baked (Preview 3 is out now!), I thought I‚Äôd give the Entity Framework Core 2.0 a try and do a benchmark and compare it to Dapper.Net.
So without further rant, lets get to it:
"
https://medium.com/microsoftazure/azure-event-grid-the-whole-story-4b7b4ec4ad23?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
Azure Event Grid is a cloud service that provides infrastructure for event-driven computing. Event Grid focuses on events or messages that declare, ‚Äúsomething happened.‚Äù It is not designed for commands that imply ‚Äúsomething will happen.‚Äù The service allows you to send messages, route them to endpoints, and consume them with custom code. It enables near real-time delivery (typically less than one second) at scale (thousands of events per second).
Most Azure services automatically send messages through Event Grid and many can directly consume messages ‚Äúout of the box.‚Äù Event Grid also supports posting to secure web API endpoints to deliver messages and uses the WebHook standard for delivering messages. Therefore, any language or platform that supports posting to a web endpoint and consuming an HTTP payload is capable of working with Event Grid.
Azure Event Grid supports a ‚Äúpush model.‚Äù Your application will never have to ask or ‚Äúpoll‚Äù for events. Instead, events are delivered immediately to an endpoint that you specify. This means your code can respond to events as they happen. In many cases it can also lead to cost savings because it removes the overhead of polling on a regular basis and instead triggers code only when it is needed to consume an event.
The infrastructure is fully managed through a serverless model: it automatically scales to meet your demands and only bills when you are actively using the service. To illustrate the billing model, consider that five million events are published during the month to two active subscriptions. One of the subscriptions has a filter for ‚Äúadvanced match‚Äù of messages coming in, resulting in one million messages passing the filter. One service handler goes down for a period, missing one million messages (so the delivery attempt fails). Event Grid is built to redeliver all of those messages after the service comes back up. The cost comes out to $10.14 USD for the month.
You can explore pricing further by using the üí∞Azure Pricing Calculator. For the cost, Event Grid provides guaranteed delivery in a 24-hour window (with option to save undelivered messages) and 99.99% availability.
Now that we‚Äôve introduced what Event Grid is, let‚Äôs dig into some examples of how you would use it in practice. After an application updates a status, it may raise a message ‚Äústatus was changed.‚Äù This then allows you to consume the message to integrate with other systems, kick off workflows and synchronize data.
The following diagram represents an example of a serverless application that uses Event Grid, taken from a session in Microsoft Ignite | The Tour. We will use this example to explore Event Grid concepts and terminology.
In this example application, the command line interface (CLI) tool is responsible for uploading images and updating a table of SKUs. A SKU is a stock-keeping unit and represents a unit of inventory. The CLI raises several events related to the lifecycle of a SKU:
In response to the events, three workflows are kicked off:
The important thing to note is that all these processes are built independently of the main application. After it sends the appropriate messages, the rest of the application can be built independently with multiple workflows kicked off by the same messages. This enables not only application scale, but the ability to scale development teams as the product grows.
üîóAccess the GitHub repository for the Tailwind Traders example
There are several concepts that are useful to understand when working with Event Grid.
Native Azure services have predefined topics and already publish messages. These messages are available for you to subscribe to from the appropriate source. For custom topics, you simply create your Event Grid topic and then begin publishing and setting up subscriptions. The topic has an endpoint to publish messages and a set of keys for secure access. There are many other features and available options for configuring Event Grid to meet your specific needs.
The following section provides video to illustrate the various features available with Azure Event Grid.
Note: all the following videos have no audio.
In this video, an Azure Function is created with an Event Grid trigger (it is called by Event Grid for a subscription). The subscription is added to Azure Storage, so that whenever a file is uploaded to blob storage, the function is triggered.
As another example of how Azure services are built ready to consume Event Grid events, this video shows how to consume events using Azure Storage Queues.
This video shows how to create a custom Event Grid topic.
The Event Grid SDK is available for many popular languages and platforms. The library makes it easy to authenticate and publish messages. In this .NET Core example, a payload with information about a SKU event is wrapped in a message and published.
It is not necessary to use the SDK. Any language or platform that supports setting an authentication header and issuing an HTTP POST is capable of publishing to Event Grid. This is the .NET Core code to publish without using the SDK.
üîó Access the GitHub repository for the ‚Äú//build Event Grid‚Äù example
The code for the handler has two responsibilities. The first is to honor a validation handshake. To avoid spamming endpoints with messages, Event Grid requires a handler to ‚Äúopt-in‚Äù to a subscription. It will post a special validation request with a unique token that must be echoed back. If this doesn‚Äôt happen, no further messages will be sent to the handler. The second responsibility is to simply parse messages as they come in.
The following code is part of an ASP.NET Core MVC app. The controller exposes an endpoint to receive the messages. It will echo back the validation token for subscriptions and write the contents of the payload to the application logs for incoming messages.
These examples in .NET Core can easily be implemented in Go, Node.js, Ruby, Python, or any other language.
Sometimes it may not be possible to modify the handler to echo a validation request back. When requesting validation, Event Grid sends an optional validation URL that can be used instead. It expires after several minutes. The URL can be accessed using the GET method, so it is suitable for either automatically validating in code (for example, by a third-party process that examines the application logs and issues the request) or manually by pasting it into the browser. This video demonstrates manual validation.
Event Grid guarantees delivery within a 24-hour window. If a handler goes down for any reason, Event Grid will continue to retry until the delivery window expires. This video shows how to configure the delivery time frame and retry count and verify that guaranteed delivery works by bringing a handler down, issuing a message, then bringing it back up to confirm delivery.
If a handler is not able to recover in a 24-hour window, Event Grid will stop trying to deliver those messages. It is possible to configure Event Grid to store ‚Äúdead letters‚Äù (messages that could not be delivered) in storage. This way you can parse the missed messages and event replay them once the handler is back up and running. This video shows how to configure and confirm dead letter delivery.
Event Grid uses a proprietary schema to wrap messages. The Event Grid team worked with the Cloud Native Computing Foundation (CNCF) to create an open standard for cloud messaging called CloudEvents. The following video shows how to configure Event Grid to use the CloudEvents schema instead of the proprietary one.
The final video shows an alternate way to consume events via Azure Logic Apps to kick of integrations and workflows.
Tip: the example uses the Event Grid schema to parse information from the header. The Data portion (payload) can contain custom properties depending on the event, so it‚Äôs not available to parse automatically. Logic Apps provides a Parse JSON connector that allows you to specify the schema of the payload and parse its information in later steps.
In this article you learned about Azure Event Grid, one component of the Azure serverless platform that provides the infrastructure for event-based applications. You calculated how Event Grid bills per operation, explored terminology and concepts behind Event Grid, and studied an example serverless application that uses Event Grid. The walk through demonstrated how to use various features from publishing and subscribing to configuring delivery retries and capturing undelivered messages in storage.
A major benefit of Event Grid is the ability to manage all of your events in one place. It was also built to reliably handle massive scale. You get the benefits of publishing and subscribing to messages without the overhead of setting up the necessary infrastructure yourself.
üëçüèª You can learn more about Azure Event Grid and get started by visiting the comprehensive Event Grid documentation.
üîóAccess the GitHub repository for the Tailwind Traders example
üîó Access the GitHub repository for the ‚Äú//build Event Grid‚Äù example
Are you using Event Grid in your own solutions? Do you have questions or feedback after reading this article? Please share your stories, insights, suggestions and feedback in the comments below!
Any language.
240 
2
Thanks to sigje.¬†
240¬†claps
240 
2
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/t-t-software-solution/%E0%B9%81%E0%B8%8A%E0%B8%A3%E0%B9%8C-mock-up-%E0%B9%83%E0%B8%AB%E0%B9%89%E0%B8%81%E0%B8%B1%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B8%A1%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B9%88%E0%B8%B2%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B8%88%E0%B9%88%E0%B8%B2%E0%B8%A2%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-pencil-azure-2142ee7d85f8?source=search_post---------54,"There are currently no responses for this story.
Be the first to respond.
‡∏ú‡∏°‡πÑ‡∏î‡πâ‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏´‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ ‡∏™‡∏£‡∏∏‡∏õ Requirements ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏≤‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ô‡∏≠‡πÑ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ú‡∏° ‡∏ó‡∏±‡πâ‡∏á Designer ‡πÅ‡∏•‡∏∞ Developer
‡πÄ‡∏•‡∏¢‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ß‡πà‡∏≤ ‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏ã‡∏∂‡πà‡∏á‡∏ú‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏Ñ‡∏£‡πà‡∏≤‡∏ß‡πÜ‡πÑ‡∏î‡πâ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ
‡∏à‡∏≤‡∏Å‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô ‡∏ú‡∏°‡∏à‡∏∂‡∏á‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÉ‡∏ä‡πâ Tools ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ ‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ
‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ú‡∏°‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠ ‡∏à‡∏∞ ‡πÉ‡∏ä‡πâ Pencil ‡πÄ‡∏û‡∏∑‡πà‡∏≠ ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡πÅ‡∏•‡∏∞ Export ‡πÄ‡∏õ‡πá‡∏ô HTML ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥ files ‡πÑ‡∏õ‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Azure ‡∏Ñ‡∏£‡∏±‡∏ö
‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Pencil Tool ‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
Shapes
Clip-art
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Pencil Design ‡∏ó‡∏µ‡πà‡∏ú‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á
‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Export HTML File ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ô‡∏µ‡πâ
‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ HTML Files ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mock-up ‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á ‡∏•‡∏á‡πÉ‡∏ô Azure ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ
‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏Å
‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ Deploy HTML files ‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏π‡πà Azure
‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏≠ 2 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô ‡∏Ñ‡∏∑‡∏≠ 1. FTP, 2. Local Azure Git
Deploy ‡∏ú‡πà‡∏≤‡∏ô FTP
Deploy ‡∏ú‡πà‡∏≤‡∏ô Local Azure Git
‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏°‡∏≠‡∏á‡∏´‡∏≤ Tool ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ó‡∏µ‡∏°‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡πÜ‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ô‡∏≤‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô
https://www.tt-ss.net/
150 
1

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
150¬†claps
150 
1
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/gochain/announcing-gochains-private-deployment-offering-on-microsoft-azure-dcfd28d6aa85?source=search_post---------55,"Sign in
There are currently no responses for this story.
Be the first to respond.
GoChainGo
Oct 4, 2018¬∑3 min read
RENO, Nev. (October 4, 2018) ‚Äî GoChain is excited to announce its private blockchain deployment offering is now available on Microsoft Azure. Enterprise clients and developers can easily deploy their own private GoChain network with a few clicks. As an Enterprise grade product offering, it can be used to build decentralized applications and smart contracts in a fast, secure, and reliable private environment. GoChain is now among the only public blockchain protocols with a private offering listed on the Azure Marketplace:
Official Microsoft Blog PostAzure Marketplace ListingDeploying GoChain on Azure
A private implementation of GoChain provides a number of advantages in five key areas compared to existing solutions:
Speed, Performance, and Reliability ‚Äî all of the performance optimizations built into GoChain have been tested and proven in a private, controlled environment.
100% Ethereum Compatible ‚Äî GoChain is frontwards and backwards compatible with Ethereum making it seamless and easy to transition from Ethereum to GoChain or vice versa with no code changes.
New Consensus Model ‚Äî moving away from Proof-of-Work (PoW) increases the transaction speed, security, and scalability of the network via our Proof of Reputation consensus mechanism.
Upgradeable Smart Contracts ‚Äî as every developer knows, the flexibility to update code to fix bugs or add functionality is critical. In blockchain, where a lot of value is on the line and hacking runs rampant, this is even more true. GoChain is developing upgradable smart contract functionality, where trust is retained between parties as well as moving critical loss prevention features, such as pausing a smart contract, to the protocol level.
Service and Support ‚Äî GoChain takes pride in providing hands-on support to ensure our users have successful implementations. They have a client success and support team consisting of professionals and industry experts from IBM, Microsoft, and Oracle.
The GoChain team comes from Fortune 500 companies and Enterprise-level corporations. This has given them an in-depth understanding of what Enterprise clients need in a changing business ecosystem. On the technical side, clients want performance, reliability, and increased functionality. Our client success and support team allow our offering to go way beyond the deployment solution on its own. GoChain is proud to be the first and only blockchain company to provide a public and private blockchain solution designed with enterprise in mind.
‚ÄúWe are really looking forward to working with Azure clients and providing them with an Enterprise level of support not currently available in Blockchain today. This will also help drive mass Enterprise adoption across the GoChain ecosystem.‚Äù
‚Äî Jason Dekker, GoChain CEO
For press inquiries, contact GoChain‚Äôs Marketing Director, Adam Norris at anorris@gochain.io. Our press kit is available for download here.
About Microsoft AzureMicrosoft Azure is a cloud computing service created by Microsoft for building, testing, deploying and managing applications and services through a global network of Microsoft-managed data centers. It provides Software as a Service (SaaS), Platform as a Service (PaaS) and Infrastructure as a Service (IaaS) and supports many different programming languages, tools and frameworks, including both Microsoft-specific and third-party software and systems. Learn more at azure.microsoft.com
About GoChainGoChain, a public cryptocurrency, is a scalable, high-performance, low-cost, decentralized blockchain network protocol that supports smart contracts and distributed applications. It is the only network protocol that offers Enterprise level support, high transaction speeds, and upgradeable smart contracts. Learn more about GoChain at gochain.io.
More info at https://gochain.ioJoin our Telegram GroupPress kit here
Follow us on Social!TwitterMediumRedditCrunchbaseInstagramFacebookBitcoin Talk
The Medium Behind the Chain ‚Äî https://gochain.io/
2.4K 
1
2.4K¬†
2.4K 
1
100% Ethereum Compatible, 100x Faster - https://gochain.io
"
https://medium.com/ontologynetwork/you-can-now-develop-ontology-smart-contracts-on-google-cloud-aws-and-azure-4c7425e0cfb7?source=search_post---------56,"There are currently no responses for this story.
Be the first to respond.
Today the Ontology Development Platform (ont_dev_platform) was released on Google Cloud Platform Marketplace, making Ontology one of the first public blockchains to have a development platform on the leading cloud provider marketplaces: Google Cloud, Amazon Web Services, and Microsoft Azure. Using the Ontology Development Platform on one of these cloud providers allows you to play around with and develop smart contracts without having to go through the fuss of configuring and setting up an environment locally.
Ontology has also joined the Google Cloud Technology Partner program, which gives Ontology the opportunity to collaborate with Google in marketing activities. With this new relationship and the development platform releases, Ontology hopes to grow the tech community and make developing dApps more accessible for all.
Are you a developer or want to start out? Please check out the:
Are you a developer? Make sure you have joined our tech community on Discord. Also, take a look at the Developer Center on our website, there you can find developer tools, documentation, and more.
Ontology website / Ontology GitHub / ONTO website / OWallet (GitHub)
Telegram (English) / Discord
Twitter / Reddit / Facebook / LinkedIn
A high performance, open-source blockchain specializing in digital identity and data.
973 
973¬†claps
973 
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
"
https://towardsdatascience.com/how-i-passed-the-microsoft-azure-fundamentals-certification-in-5-days-75b8e261d5d1?source=search_post---------58,"Sign in
There are currently no responses for this story.
Be the first to respond.
Arunn Thevapalan
Dec 27, 2020¬∑9 min read
For the past 5 days, I‚Äôve been preparing for an exam called Microsoft Azure Fundamentals AZ900. I sat for it today, and it turns out I passed. Yes, today. I‚Äôm writing this‚Ä¶
"
https://medium.com/@jeffhollan/serverless-doorbell-azure-functions-and-ring-com-f24b44e01645?source=search_post---------59,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Dec 20, 2017¬∑3 min read
My parents sent me an early Christmas present this year ‚Äî a Ring video doorbell. The doorbell notifies my phone whenever someone rings the doorbell (or if it detects motion) and can capture video and audio of my doorstep.
I‚Äôve seen these doorbells in more than a few sales this year, and as a result they seem to be growing in popularity. Like any good IoT maker, I wanted to see how I could extend and connect it with my other things. In the space of an afternoon was able to build a simple serverless app to trigger whenever someone rings my doorbell, and push that data instantly wherever my hacker-heart desires (including face/audio detection). All of this functionality will add a grand total of about $0.40 to my Azure subscription each month. In the end, I used Azure Functions to create the trigger for ring.com events, and push the data to Azure Event Grid to notify any listeners of events.
I firmly believe any good IoT device should have an equally good API. Unfortunately, Ring doorbells have no public API. Luckily, it only took a quick GitHub search to find this project from @davglass who had reverse-engineered the API and published a super easy npm package to access it. The project lives here if interested.
Since there is no way to trigger directly to an event like a doorbell press, I need to poll the ring.com API for events. Azure Functions timer triggers are a great fit for these kind of custom polling triggers. I wrote this simple cron expression so my function will check for rings every 15 seconds: */15 * * * * * . I poll for new events (either a ‚Äòring‚Äô or a ‚Äòmotion‚Äô event). If the doorbell has sent an event, the function logic executes. If not, the function completes.
See polling logic in GitHub
Once I detect a motion or ring event has occurred, I want to take some action. Maybe I want to create a log in a Cosmos DB database, or make my WiFi connected lightbulb flash. I don‚Äôt know when I‚Äôm authoring the trigger function what interested entities may exist. This type of event routing and ‚Äúpublish/subscribe‚Äù was a nice fit for Event Grid. By forwarding my ring.com events to Event Grid via Azure Functions, I can now add and remove reactive components as needed. A logic app here, a function there ‚Äî anything that exposes a secure URL. Even my raspberry pi inside my local network can participate by leveraging an Event Hub subscriber to these events.
I followed this tutorial to create an Event Grid and grabbed the endpoint URL and key.
Adding the integration to Event Grid to my function is as easy as adding an outgoing HTTPS request. I POST the event data in an ‚Äúevent envelope‚Äù to the event grid endpoint, and the event is instantly routed (seriously, like no latency). The top-level event envelope allows me to set the right context so my subscribers can listen only to the events or sources they care about ‚Äî like ‚Äúfilter to only ‚Äòring‚Äô events from the front door‚Äù:
Event Grid allows for decoupling of the event source and the event actions. This triggering function I wrote doesn‚Äôt need any knowledge of what may be listening, and listeners can be added or removed independently.
Here‚Äôs the javascript code I added to my function to push data to Event Grid:
Now that the events are pushed to the Event Grid, I can start building the subscribers. Some of the two easiest tools for this are Azure Logic Apps (connecting my doorbell to over 200 connectors) and Azure Functions. In part 2 of this post, I will go into details of creating serverless components to listen to events and react with elements like adding documents to Cosmos DB. If time permits this week, I‚Äôm hoping to add some nice Cognitive Service integration to bring my doorbell to the next level of awesomeness üòé.
Continue to part 2 ‚Äî Integrating with Cosmos DB
https://github.com/jeffhollan/functions-node-ring-doorbell
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
213 
213¬†claps
213 
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://blog.bernd-ruecker.com/orchestrating-azure-functions-using-bpmn-and-camunda-a-case-study-ff71264cfad6?source=search_post---------60,"Serverless functions are all the hype at the moment whether it be AWS Lambda or Azure Functions. I am happy that I am allowed to share a case study from NexxBiz to show you how Azure functions and BPMN work together. For a customer in the insurance field they set up an architecture around the Microsoft Cloud (Azure). They have build a very interesting tool-chain to master this environment.
NexxBiz run multiple customers (tenants) on their platform. They need to implement dedicated business processes or customize standard ones for every tenant. Business functionality is available as dedicated serverless functions. As they are using the Microsoft .NET stack, Azure Functions are the way to go.
For most use cases just calling one simple function is not enough, you have to call multiple functions in the right order to implement a proper end-to-end use case or business process. This is referred to as orchestration. And it is pretty typical as functions intend to cut overall business logic into rather small and stateless pieces. I want to spare any discussion on orchestration vs. choreography in this blog post but can recommend Events, Flows and Long-Running Services: A Modern Approach to Workflow Automation if you are interested).
Azure does not offer complex orchestration capabilities, you can only do very simple flows within Azure Logic Apps (which is basically Microsoft Flow). Logic apps follow a low code approach which is typically not suitable for complex flows. That‚Äôs why they chose Camunda.
To give you a quick first impression, this is a simple but real-life orchestration flow from their system checking if a prospect is already known in the CRM, showing some runtime statistics as heatmap on top of the BPMN process model (using Camunda Cockpit).
There are multiple ways to run the Camunda Workflow Engine (see Architecture options to run a workflow engine). NexxBiz decided to minimize contact with Java and therefore,
See Use Camunda without touching Java and get an easy-to-use REST-based orchestration and workflow engine for more details on how to run Camunda in context of .NET.
In order to call the Azure Function they used the Azure Service Bus. Every Azure Function can be naturally triggered by a message on this bus.
To connect Camunda to the Azure Service Bus they decided to build small connectors consisting of three Azure Functions on their own:
The cool thing is, these functions are auto-generated and automatically provisioned during the deployment. No need to do anything manually.
Any configuration they need is done via configuration of the external tasks or input/output mappings in the BPMN process.
An interesting detail I want to highlight is that they wrote Java Unit tests for their BPMN processes. But wait, didn‚Äôt they want to avoid contact with Java? Indeed. But our consultant onsite talked them into writing the unit tests in Java anyway ‚Äî as we have so much support for doing this (Camunda test support, assertions, scenario tests and visualizations of test runs). And this was a great idea as they told me:
Thank god Niall (Note: The guy with the hawk) talked us into sticking with Java Unit Tests, they are so powerful and easy to write. And as we can program in C# it is not hard to do that in Java.
To get this architecture off the ground they automated the build and deployment pipeline. And as they wanted to have much more control over these pipelines than they could get by Visual Studio Team Services out-of-the-box, they ‚Äúdrunk their own champagne‚Äù and used Camunda to control this. So their tool stack looks like this:
Another interesting aspect was that they use Cawemo to capture and discuss business processes and their requirements in the first stage. It saves them lots of paper for specification. They told me that they just conduct a one day workshop instead which is sufficient to get a solid basis for a first working increment.
Whenever a BPMN model is mature enough it is exported into the developers workspace and placed into normal version control. Now the Camunda Modeler is used to add properties like expressions or input/output mappings for external tasks. A unit test for the process is also written.
When everything is executable the workflow can be deployed onto Azure as described above with all glue code being generated automatically.
I asked the NexxBiz guys what they like about their approach and architecture. This is what they told me:
Thanks to NexxBiz for walking me through their architecture and allowing me to share this information. Please not that this blog posts reflects their architectural decisions and not expresses a recommendation from me or Camunda.
As always, I love getting your feedback. Comment below or send me an email.
Stay up to date with my newsletter.
My personal blog.
228 
1
Thanks to Darya Niknamian.¬†
228¬†claps
228 
1
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
"
https://medium.com/@jeffhollan/serverless-doorbell-ring-com-and-azure-functions-part-3-7e865f28a1f?source=search_post---------61,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Dec 22, 2017¬∑5 min read
This is part 3 in a series of how I have been using Azure Serverless to extend and connect the functionality of my ring.com video doorbell. Part 1 is here.
Ever since I got my ring.com video doorbell installed, I‚Äôve been connecting and extending it with serverless components. Previously I hadn‚Äôt added much new functionality that I couldn‚Äôt already get from the ring app. Today is when that all changed. Below is how I used Azure Logic Apps, Functions, and Cognitive Services to detect the faces of anyone who comes to my doorstep, grab the audio, and create an AI-powered log inside of a Cosmos DB database.
I knew I wanted to have this type of facial recognition the moment I had access to the ring.com API. However it wasn‚Äôt as easy as just making a call to ‚Äúgrab video feed‚Äù for it to work. Here‚Äôs how the reverse-engineered ring.com API works:
So the challenge I was faced with was how can I detect and analyze the video as quickly as possible? If I did something like ‚Äúwhen a ring event occurs, fetch the recording‚Äù in an Azure Function, I would get a 404, as the recording doesn‚Äôt exist yet. At the same time I never know exactly how long it will be until the event ‚Äúcompletes‚Äù and the recording is published. I need some way to orchestrate and maintain state for the process ‚Äî which isn‚Äôt always easy in the traditionally stateless world of serverless. Enter Azure Logic Apps.
Azure Logic Apps provided the perfect orchestration capabilities I needed to solve this rather simply. Not to mentioned, Logic Apps comes loaded with 200+ connectors to different APIs reducing the amount of code I needed to write. The only code I actually had to write for this entire process was these 23 lines of Azure Functions code to grab the video recording link. #serverlessFTW
Here‚Äôs what‚Äôs happening:
Now I know Logic Apps better than the average developer, but even then this entire logic app took about 45 minutes to build. 23 lines of code, 3 out-of-the-box connectors, and 1 workflow later, I get rich insights into the videos captured by my IoT doorbell. How much do I have to pay for this science fiction fantasy come to life? Assuming I get 3 visitors a day, it‚Äôd come to about ~$0.18 a month. All told my entire serverless project to this point is less than $2 a month.
Now that my videos are automatically being uploaded and analyzed into my Cosmos DB account and Video Indexer profile, what can I do more? Well first off, when the initial videos started coming in, all of the faces were ‚Äúunknown.‚Äù Unfortunately no celebrities have been visiting, so the cognitive service doesn‚Äôt recognize any of the faces. No worries though ‚Äî I can ‚Äútrain‚Äù my profile and teach it about the faces it is seeing. After opening a recording in the Video Indexer portal, I can replace the label ‚ÄúUnknown Person‚Äù with their name. Next time they show up in a video, Video Indexer recognizes them and will correctly assign a name. This same type of learning can be applied to the audio transcripts as well, so I can start to teach my profile about the words it may expect to hear on my doorstep to get more accurate results. While the results aren‚Äôt perfect ‚Äî sometimes the quality of video or angle isn‚Äôt quite good enough to pick up a face ‚Äî it works often enough that I‚Äôm very happy with the results.
You may be asking: ‚ÄúDoes anyone really need an in-depth historic analysis of the patterns of faces that appear on their doorstep every month?‚Äù Maybe not. But for less than a cup of coffee a month, why not create my own doorstep version of HAL 9000 so my home may one day turn against me?
If interested in trying to build something similar yourself ‚Äî I‚Äôve got all the code here in my GitHub account. Enjoy! https://github.com/jeffhollan/functions-node-ring-doorbell
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
See all (133)
197 
2
197¬†claps
197 
2
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/asp-net-core-azure-kubernetes-orquestra%C3%A7%C3%A3o-de-containers-na-nuvem-parte-1-67f8d3568ba9?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 23, 2018¬∑5 min read
In√∫meras s√£o as vantagens obtidas ao empregar containers Docker em projetos de software. Isolamento de aplica√ß√µes, uma utiliza√ß√£o mais racional de recursos computacionais, velocidade no deployment e uma menor depend√™ncia em rela√ß√£o aos ambientes operacionais est√£o entre os fatores que contribuem para o crescimento no uso desta tecnologia.
Por mais que as vantagens trazidas pelo Docker sejam ineg√°veis, existem tamb√©m dificuldades decorrentes de sua ado√ß√£o:
As respostas a tais quest√µes est√£o em solu√ß√µes voltadas ao gerenciamento e uso orquestrado de containers Docker. Dentre as alternativas mais populares neste segmento temos o Kubernetes, o Docker Swarm e o DC/OS da Mesosphere. O pr√≥prio Microsoft Azure n√£o ficou alheio a este tipo de demanda, contando com um servi√ßo conhecido como Azure Container Service e que suporta estas 3 tecnologias.
Mais recentemente (Outubro/2017) a Microsoft disponibilizou uma nova op√ß√£o no Azure voltada ao uso de Kubernetes: trata-se do Azure Kubernetes Service (AKS), solu√ß√£o que visa simplificar o gerenciamento e opera√ß√£o de ambientes baseados em Kubernetes.
Este artigo √© a primeira parte de um tutorial no qual abordarei a utiliza√ß√£o combinada do ASP.NET Core, do Microsoft Azure e do Kubernetes (atrav√©s do servi√ßo AKS) para a implementa√ß√£o de projetos que dependam da orquestra√ß√£o de containers. Nesse post est√£o os principais conceitos envolvendo o Kubernetes, bem como descrita a aplica√ß√£o ASP.NET Core que servir√° de base para os testes com o AKS.
Tamb√©m conhecido como K8s ou kube, o Kubernetes √© um projeto open source escrito na linguagem Go e desenvolvido originalmente pela Google. Mantido atualmente Cloud Native Computing Foundation, o Kubernetes conta com recursos de gerenciamento que viabilizam a orquestra√ß√£o, auto recupera√ß√£o, rein√≠cio, replica√ß√£o e escalonamento de containers Docker.
Assim como acontece com outras tecnologias populares na atualidade, o Kubernetes possui tamb√©m uma ferramenta de linha de comando: o kubectl, utilit√°rio que permite a execu√ß√£o de instru√ß√µes voltadas ao gerenciamento de clusters e containers.
As diferentes estruturas controladas via Kubernetes ser√£o criadas a partir de arquivos no formato YAML e por meio da execu√ß√£o de comandos via kubectl. Um ambiente para testes pode ser disponibilizado atrav√©s da instala√ß√£o do Minikube (software que possibilita a cria√ß√£o de um cluster local para uso do Kubernetes).
Dentro da arquitetura adotada pelo Kubernetes merecem destaque os seguintes elementos:
Um cluster no Kubernetes est√° dividido em:
J√° um Pod √© uma estrutura que agrupa um ou mais containers. A implanta√ß√£o deste elemento (Pod) acontece em algum dos Nodes dispon√≠veis, com os containers que comp√µem o mesmo compartilhando o mesmo endere√ßo de IP, nome de host e outros recursos.
A figura a seguir traz a representa√ß√£o esquem√°tica de um Pod:
Um objeto Deployment √© uma abstra√ß√£o de um Pod, contando ainda com recursos adicionais:
Na pr√≥xima imagem temos a representa√ß√£o de um objeto Deployment:
Um objeto do tipo Service √© uma estrutura que funciona como um Load Balancer, cuidando assim do acesso aos diferentes Pods de uma aplica√ß√£o. Trata-se de um elemento mais est√°vel, at√© porque Pods s√£o criados ou removidos continuamente ao se escalar uma aplica√ß√£o.
Dentro da arquitetura do Kubernetes o elemento conhecido como Replication Controller determinar√° quantas c√≥pias id√™nticas de um Pod ser√£o executadas e em quais locais (Nodes) do cluster. J√° o Kubelet √© o servi√ßo que garante a inicializa√ß√£o e execu√ß√£o dos containers nos diferentes Nodes.
A imagem a seguir traz mais uma representa√ß√£o do Kubernetes, com Pods distribu√≠dos entre os diferentes Nodes de um cluster:
Para os testes envolvendo o uso do Kubernetes a partir do Microsoft Azure ser√° criada uma API REST baseada no ASP.NET Core 2.0. Esta aplica√ß√£o produzir√° como retorno a quantidade de acessos √† API, al√©m de exibir o nome do host/m√°quina e do sistema operacional utilizado pelo container Docker.
Na pr√≥xima listagem est√° a implementa√ß√£o da classe Contador, a qual armazenar√° a contagem de acessos √† API REST de testes:
J√° o tipo ContadorController receber√° solicita√ß√µes HTTP atrav√©s da Action Get, controlando a utiliza√ß√£o sincronizada de uma inst√¢ncia de Contador (via instru√ß√£o lock) e devolvendo o n√∫mero de acessos √† API, o nome da m√°quina/host e a vers√£o do sistema operacional em uso:
A seguir temos um exemplo de execu√ß√£o desta API por meio do Visual Studio 2017:
As demais etapas envolvendo a publica√ß√£o da aplica√ß√£o no Kubernetes ser√£o detalhadas na segunda parte desta s√©rie. Os fontes deste projeto de testes j√° est√£o dispon√≠veis no GitHub:
https://github.com/renatogroffe/ASPNETCore2_Kubernetes
Azure Container Service (AKS)
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
Kubernetes - Site Oficial
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
196 
196¬†claps
196 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mauridb/powerbi-and-azure-databricks-193e3dc567a?source=search_post---------63,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
Feb 8, 2018¬∑6 min read
Please note that from middle of February 2018 connection to Azure Databricks is also possible via Spark connector as described here, which is now the recommended approach. Continue reading this article if you‚Äôre interested in setting up a connection via ODBC.
I‚Äôve recently changed my role and as a result I‚Äôm now again 100% focused on my first love, data and databases.
Among all the cool stuff released in the last two years in the data space, for which I haven‚Äôt had enough time to play with, there is Databricks, the Spark distribution, created by the creator of Spark themselves. Spark has also reached version 2.x and I really wanted to give it a test run, to play with the new features by myself.
Since Databricks is available on Azure, I just created new cluster and to get confident with Azure Databricks I firstly did the ‚ÄúGetting started ‚Äî A Gentle Introduction to Apache Spark on Databricks ‚Äù tutorial. It‚Äôs very introductory and allows you to get confident with terminology, concepts and usage of Notebooks. If you already used tools like Jupyter or Apache Zeppelin you‚Äôre already familiar with the Notebook idea and you can go through the tutorial real quick.
After the first tutorial, the second one is the ‚ÄúApache Spark on Databricks for Data Scientists‚Äù.
Once I finished the tutorial I immediately thought that would have been great to connect to the available data using PowerBI. Doing data exploration and visualization on the notebook is great but I wanted to do some interactive data exploration. In addition I also wanted to create a nice dashboard that can be easily shared with non-tech users, and for these things PowerBI (or other tools like Tableau) is just a killer application.
So, how do you connect PowerBI to Spark on Azure Databricks? There is a complete documentation here:
docs.azuredatabricks.net
And following it is really not difficult, but it may be a bit tricky, if you‚Äôre not familiar with the PowerBI ecosystem (Service, Gateway and Desktop). If you just want to connect your PowerBI desktop client to Spark on Azure Databricks, just keep reading.
First of all make sure you have the latest PowerBI version installed on your machine:
powerbi.microsoft.com
and then download the Apache Spark JDBC/ODBC driver:
databricks.com
Make sure you select the Windows (ODBC) version and then proceed to install it. Since I‚Äôm using PowerBI x64, I just installed the 64-bit one.
Once installation is done, go and open the ODBC Data Source Administrator:
and create a new DSN. Choose a ‚ÄúUser DSN‚Äù if it‚Äôs only for you, or ‚ÄúSystem DSN‚Äù if other people on the same machine will be using it.
To create a new DSN click on ‚ÄúAdd‚Äù and the select the ‚ÄúSimba Spark ODBC Driver‚Äù. After that you‚Äôll see a window like the following:
Since we‚Äôre connecting to a Spark 2.0 cluster, as you can verify in the ‚ÄúConfiguration‚Äù page of you Databrick Spark cluster,
the ODBC ‚ÄúSpark Server Type‚Äù option needs to be set to the following:
The ‚ÄúHosts(s)‚Äù and ‚ÄúPort‚Äù option must be set to the values you can see in the JDBC url (that is available in the ‚ÄúConfiguration‚Äù page of the Spark cluster too):
Azure Databricks allows authentication via unique users tokens. The ODBC Authentication mechanism needs to be set to ‚ÄúUser Name ad Password‚Äù and the user name must be set to ‚Äútoken‚Äù. Literally, just without quotes.
The password is the token that can be generated on the Databricks portal by clicking on the user icon on the top right and selecting ‚ÄúUser Settings‚Äù:
This will bring up the ‚ÄúAccess Token‚Äù page. Just click on Generate New Token, specify the lifetime of the token and then copy the generated token somewhere. The token is something like:
This is actually the password you have to specify in the ODBC Driver:
Authentication is now one. It‚Äôs now time to configure the Thrift Transport. It must be set to HTTP and then ‚ÄúHTTP Options‚Äù and ‚ÄúSSL Options‚Äù needs to be configured accordingly.
In the ‚ÄúHTTP Options‚Äù specify the HTTP Path you can see in the JDBC Url (or, again, in the Cluster Portal, Configuration Tab, HTTP Path section)
In the ‚ÄúSSL Options‚Äù just make sure you check ‚ÄúEnable SSL‚Äù.
That‚Äôs it! Now click on the ‚ÄúTest‚Äù button to the test connection and you should be able to see a window like this (Just make sure your cluster is running and you don‚Äôt have any firewall blocking connections)
So far, so good, ODBC DSN is created. It‚Äôs now time to open PowerBI Desktop and get data from the Spark cluster by using the newly created DSN:
Make sure you select the DSN you created before
use the token as a password again when asked and make sure the user name is set to ‚Äútoken‚Äù (again, without quotes)
and you‚Äôll be good to go!
As you may notice I‚Äôve also added the ‚Äúfarmer_markets‚Äù as a non-temporary table, so that I can do exactly what the tutorial tells you to do via Spark, but using PowerBI this time.
And now you can easily explore and visualize data, PowerBI way:
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
128 
3
128¬†
128 
3
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://medium.com/awesome-azure/azure-difference-between-azure-load-balancer-and-application-gateway-9a6019c23840?source=search_post---------64,"There are currently no responses for this story.
Be the first to respond.
Comparison: Azure Load Balancer vs Application Gateway in Azure.
Azure Load Balancer works with traffic at Layer 4.Application Gateway works with Layer 7 traffic, and specifically with HTTP/S (including WebSockets).
"
https://medium.com/@evonneyifangtsai/b2b%E5%B9%B3%E5%8F%B0%E7%AD%96%E7%95%A5-%E5%BE%AE%E8%BB%9F%E6%80%8E%E9%BA%BC%E6%8E%A8%E5%BB%A3%E4%BB%96%E7%9A%84azure-iot%E5%B9%B3%E5%8F%B0-9a3e34d9744f?source=search_post---------65,"Sign in
There are currently no responses for this story.
Be the first to respond.
Evonne Tsai
Sep 17, 2017¬∑8 min read
ÂèÉÂä†Microsoft IoT Expo‰πãÂæåÔºåÈ©öË®ùÊñºMicrosoft AzureÂπ≥Âè∞ÁöÑÂÆåÊï¥ÊÄßÔºå‰πüÂæûÂÆÉÂ¶Ç‰ΩïÁ∂ìÁáüËàáË°åÈä∑ÈÄôÊ®£‰∏ÄÂÄãB2BÂπ≥Âè∞Ë¶∫ÂæóÁç≤ÁõäËâØÂ§ö„ÄÇ
ÈóúÊñºAzure IoTÁöÑ‰ªãÁ¥πÔºå‰ª•ÂèäÂè∞ÁÅ£Âª†ÂïÜÁöÑÁ≠ñÁï•ÔºåË´ãÂèÉËÄÉÔºöÂ§ßÁ•ûÈÉΩÂÅöÂÆå‰∫ÜÔºåÂè∞ÁÅ£Âª†ÂïÜÈÇÑÊÄéÈ∫ºÊêûÔºüÂæÆËªüIoT ExpoÊúâÊÑüÔºå‰ª•‰∏ãÂ∞àÊ≥®ÊñºÂàÜÊûêÂæÆËªüÈõ≤Á´ØÂπ≥Âè∞ÁöÑÂïÜÊ•≠Ê®°ÂºèËàáÊé®Âª£Á≠ñÁï•„ÄÇ
Ë¨õÂà∞Èõ≤Á´ØÂπ≥Âè∞ÔºåÂ§ßÂÆ∂Á¨¨‰∏ÄÂÄãÈÇÑÊòØÊúÉÊÉ≥Âà∞AmazonÔºåÂÖ∂ÂØ¶ÁõÆÂâçÈõ≤Á´ØÂª†ÂïÜÂõõÂº∑‰∏≠ÔºåÈô§‰∫ÜAmazon‰πãÂ§ñÔºåMicrosoftÂ∑≤Á∂ìÊìÅÊúâÁ¨¨‰∫åÂ§ßÂ∏ÇÂç†ÁéáÔºåÊé•ËëóÊâçÊòØGoogle‰ª•ÂèäIBM„ÄÇÂú®ÈÄô‰∫õÁ´∂Áà≠ËÄÖÁï∂‰∏≠ÔºåÂæÆËªüÊòØÊÄéÈ∫ºÂÆö‰ΩçËá™Â∑±ÁöÑÔºü
Ê†πÊìöÂæÆËªü„ÄåAzure‰πãÁà∂„ÄçË©πÊ£Æ‚ÄßÊ°ëÂæ∑Âú®Êï∏‰ΩçÊôÇ‰ª£ÁöÑÂ∞àË®™‰∏≠ÊèêÂà∞ÔºåÂæÆËªüÁöÑÂÑ™Âã¢Âú®ÊñºÊèê‰æõÊï¥ÂêàÊúçÂãôÔºö
„ÄåÊàëÂÄëÁöÑÁ´∂Áà≠Â∞çÊâãÔºåÊúâ‰∫õÂè™ÂÅöIaaSÔºåÊúâ‰∫õÂè™ÂÅöSaaSÔºå‰ΩÜÊòØÂæÆËªüÂÅöÁöÑÊòØÂü∫Á§éÂª∫Ë®≠„ÄÇÊàëÂÄëÊúâPaaS„ÄÅÊúâSaaSÔºåÊúâAzure‰πüÊúâOffice 365ÔºåÊâÄÊúâÁöÑÊúçÂãôÈÉΩÊê≠Âª∫Âú®‰∏ÄËµ∑„ÄÇÁâπÂà•ÊòØÂú®‰ºÅÊ•≠Áí∞Â¢ÉË£°ÔºåÂæàÂ§öÂÖ¨Âè∏ÊúÉÁî®OfficeÔºåËÄå‰ªñÂÄë‰πüÈúÄË¶ÅÁî®Âà∞Èõ≤Á´ØÔºåÂæÆËªüÂâáÊòØÂîØ‰∏ÄÂèØ‰ª•Êèê‰æõÊï¥ÂêàÊÄßÊúçÂãôÁöÑ„ÄÇ„Äç
ÂèØ‰ª•ÁúãÂæóÂá∫‰æÜÔºåÂæÆËªüÂ∏åÊúõËÉΩÁôºÊèÆÂÖ∂Êó¢ÊúâËàá‰ºÅÊ•≠ÁöÑÈóú‰øÇËàáÂÑ™Âã¢Ôºå‰ΩøÁî®ITÈÉ®ÈñÄÁÜüÊÇâÁöÑÊû∂ÊßãËàáË™ûË®ÄÔºå‰∏¶Âæû‰ºÅÊ•≠‰ΩøÁî®ËÄÖÁÜüÊÇâÁöÑOfficeÂàáÂÖ•ÔºåÊèê‰æõÊï¥ÂêàÊÄßÁöÑÊúçÂãô„ÄÇÈÄôÁöÑÁ¢∫ÊòØÂæÆËªüÁõ∏Â∞çÊñºAmazonËàáGoogleÁöÑÂÑ™Âã¢„ÄÇ
ÊàëË©¢ÂïèÂæÆËªüÊî§‰Ωç‰∏äÁöÑÊ•≠ÂãôÔºåÂæÆËªüÂíåÁ´∂Áà≠ËÄÖÊúâ‰ªÄÈ∫ºÂ∑ÆÂà•Ôºå‰ªñÂÄë‰πüÊúâÈ°û‰ººÁöÑË™™Ê≥ïÔºö„ÄåÂæÆËªüÂ∞àÊ≥®ÊñºB2BÔºåÂíå‰ºÅÊ•≠Èóú‰øÇÂ•ΩÔºå‰πüÁêÜËß£‰ºÅÊ•≠ÁöÑÈúÄÊ±ÇÔºåÁ´∂Áà≠ËÄÖÂâáÂú®Êñ∞ÂâµÂÖ¨Âè∏ÊàñÂÄã‰∫∫ËºÉÂ∏∏Ë¢´‰ΩøÁî®„Äç„ÄÇÊâÄ‰ª•ÈÄôÊ®£ÁöÑÂÆö‰ΩçÊòØÊúâÂÇ≥ÈÅîÂà∞Á¨¨‰∏ÄÁ∑öÁöÑÈä∑ÂîÆ‰∫∫Âì°ÁöÑÔºå‰∏îÈä∑ÂîÆ‰∫∫Âì°‰πüÂæàÊ∏ÖÊ•öÔºå‰∏¶‰∏çÊòØË¶ÅËàáÂÖ∂‰ªñÁ´∂Áà≠ËÄÖÊØîÂäüËÉΩ„ÄÅÁà≠Ê©üÊàøÊï∏ÈáèÔºåËÄåÊòØÂæûÂÆö‰Ωç‰∏äÂ∑ÆÁï∞ÂåñÔºåÁî®‰∏çÂêåÁöÑË™ûË®ÄËàáÂÖ∂ÈÅ∏ÂÆöÁöÑÁî®Êà∂ÂÅöÊ∫ùÈÄö„ÄÇ
ÊàëÂú®IoT Expo‰∏äÁúãÂà∞ÁöÑdemoÊòØPaaS(Platform as a ServiceÔºåÂπ≥Âè∞Âç≥ÊúçÂãôÔºå‰æãÂ¶ÇÂàÜÊûêÂπ≥Âè∞)Âä†‰∏äSaaS(Software as a ServiceÔºåËªüÈ´îÂç≥ÊúçÂãôÔºå‰æãÂ¶ÇÂêÑÁ®ÆËÉΩÁï´Âá∫ÊºÇ‰∫ÆÂ†±Ë°®ÁöÑÊáâÁî®Á®ãÂºè)ÔºåÁï∂ÁÑ∂ÈÄô‰∏ÄÂàáÊòØÊû∂Âú®AzureÁöÑinfrastructure‰∏ä(IaaSÔºåÊû∂ÊßãÂç≥ÊúçÂãô)„ÄÇ
ËàáÁ°¨È´îÂª†ÂïÜÁöÑÂêà‰ΩúÊ®°ÂºèÂÖ∂ÂØ¶ÊúâÈªûÂÉè„Äå‰ª£Â∑•„ÄçÔºåÂæÆËªüÊèê‰æõÂπ≥Âè∞ËàáËªüÈ´îÔºåËÆìÁ°¨È´îÂª†ÂïÜÂ∞áË≥áË®ä‰∏üÂà∞Âπ≥Âè∞‰∏äÈÅãÁÆóÔºå‰∏¶Â∞á‰ªãÈù¢ÂÆ¢Ë£ΩÂåñÊàêË©≤Á°¨È´îÊèê‰æõÁöÑÁç®ÁâπÊúçÂãô„ÄÇ
ÈÄôÊ®£ÁöÑÂπ≥Âè∞ÊúâÂπæÂÄãÁâπÊÄßÔºö
‰æãÂ¶ÇË¶ÅÊé®Êô∫ÊÖßÈõ∂ÂîÆËß£Ê±∫ÊñπÊ°àÔºåÈô§‰∫ÜÂπ≥Âè∞‰ª•ÂèäÊáâÁî®Á®ãÂºè‰πãÂ§ñÔºåË¶ÅÊúâËÉΩÊèê‰æõÈõ∂ÂîÆÁõ∏ÈóúÊÑüÊ∏¨Âô®ÁöÑÁ°¨È´îÂª†ÂïÜÂä†ÂÖ•Ôºå‰πüË¶ÅÊúâÁÜüÊÇâÈõ∂ÂîÆÂÆ¢Êà∂ÁöÑÁ≥ªÁµ±Êï¥ÂêàÂïÜËàáÂÆâË£ùÂïÜÂä†ÂÖ•„ÄÇ
‰ΩÜÊòØ‰∏ÄÈñãÂßãÂπ≥Âè∞‰ªÄÈ∫ºÈÉΩÈÇÑÊ≤íÊúâÊôÇÔºåÂÖ©ÈÇäÊòØ‰∏çÊúÉÊúâÂª†ÂïÜÈ°òÊÑèÂä†ÂÖ•ÁöÑÔºåËÄåÊ≤íÊúâÂª†ÂïÜÁöÑÊÉÖÊ≥Å‰∏ãÔºåÊòØ‰∏çÊúÉÊúâÊàêÂäüÊ°à‰æãÂê∏ÂºïÁµÇÁ´ØÂÆ¢Êà∂ÁöÑÔºåËÄåÊ≤íÊúâÂ∞àÊ°àËàáÂÆ¢Êà∂ÔºåÂª†ÂïÜÊú™ÂøÖÈ°òÊÑèÊäïË≥áÁîüÁî¢ÔºåÈÄôÂΩ¢Êàê‰∫ÜÈõûÁîüËõãËõãÁîüÈõûÁöÑÂïèÈ°å„ÄÇ
Âõ†Ê≠§Âπ≥Âè∞‰∏ÄÈñãÂßãÂøÖÈ†àÂÖàÊèê‰æõÈùûÂ∏∏Â§öÁöÑË™òÂõ†ÔºåÂê∏ÂºïÂ§öÈÇäÂä†ÂÖ•„ÄÇÂ∞çÂæÆËªü‰æÜË™™Ôºå‰πüË®±Â∞±ÊòØÈùûÂ∏∏ÂÆåÂÇôÁöÑÂàÜÊûêÊñπÊ≥ïËàáÊáâÁî®Á®ãÂºè„ÄÅÂÖ∂ÈüøÂôπÂôπÁöÑÂêçËÅ≤ÔºåÈÇÑÊúâËàá‰ºÅÊ•≠ÁöÑÈóú‰øÇÔºåÈÄô‰πüÂíåÂæÆËªü‰∏ÄÈñãÂßãÂÆö‰ΩçÂú®„ÄåB2B„ÄçÂ∏ÇÂ†¥ÂèØ‰ª•‰∫íÁõ∏ÂëºÊáâ„ÄÇ
Êó¢ÁÑ∂ÊèêÂà∞Âπ≥Âè∞ÁîüÊÖãÁ≥ªÁµ±ÁöÑÊê≠Âª∫ÊúâÈõûÁîüËõãËõãÁîüÈõûÁöÑÂïèÈ°åÔºå‰ΩÜÁ∏ΩÊòØË¶ÅÂæû‰∏ÄÊñπÈñãÂßãÔºåÈÇ£È∫ºÊáâË©≤Ë¶ÅÂæûÂì™‰∏ÄÂÄãstakeholderÈñãÂßãÁ∂ìÁáüÂë¢Ôºü
ÈóúÈçµÂú®Êñº‰ªÄÈ∫ºÊ®£ÁöÑËßíËâ≤Âä†ÂÖ•ÔºåËÉΩÂâµÈÄ†ÈÄôÂÄãÂπ≥Âè∞ÁöÑÁ∂≤Áµ°ÊïàÊáâÔºåÂê∏ÂºïÊõ¥Â§ö‰∫∫Âä†ÂÖ•Ôºü‰∏îÂ∞ç‰ªÄÈ∫ºËßíËâ≤ËÄåË®ÄÔºåÈÄôÂÄãÂπ≥Âè∞ÊúâÊúÄÂ§ßÁöÑÈªèËëóÂ∫¶ÔºåÁî®‰∫ÜÂ∞±‰∏çÂ§™ÊúÉÊèõÊàêÂÖ∂‰ªñÂπ≥Âè∞Ôºü
ÔºàË®ª:Á∂≤Áµ°ÊïàÊáâÔºöË∂äÂ§ö‰∫∫‰ΩøÁî®ÔºåÊïàÁõäË∂äÈ´òÔºå‰æãÂ¶ÇÁ¨¨‰∏ÄÂÄã‰ΩøÁî®ÈõªË©±ÁöÑ‰∫∫ÂÖ∂ÂØ¶Ê≤í‰ªÄÈ∫ºÂÉπÂÄºÔºåË¶ÅÂ§ßÂÆ∂ÈÉΩ‰ΩøÁî®ÈõªË©±Ôºå‰∫∫ÂÄëÊâçËÉΩÁî®ÈõªË©±‰∫íÁõ∏Ê∫ùÈÄöÔºåÈõªË©±ÊâçÊúâÂÉπÂÄº„ÄÇÁ§æ‰∫§Âπ≥Âè∞‰πüÊòØÈ°û‰ººÁöÑÊ¶ÇÂøµ„ÄÇÔºâ
Á≥ªÁµ±Êï¥ÂêàÂïÜÂú®Êñ∞Áî¢ÂìÅÂâõÈñãÂßãÊé®Âª£ÊôÇÔºå‰∏çÊúÉÊòØÈÄôÊ®£ÁöÑËßíËâ≤Ôºå‰∏ÄËà¨ÊòØËßÄÊúõËªüÁ°¨È´îËß£Ê±∫ÊñπÊ°àËàáÂÆ¢Êà∂ÈúÄÊ±ÇÔºåÂ∞ãÊâæÈÅ©ÂêàÁöÑcomponent‰æÜËá™Â∑±ÂÖúÔºåÂì™ÈÇäÊúâÈå¢ÔºàÁµÇÁ´ØÂÆ¢Êà∂ÈúÄÊ±ÇÔºâÔºåÂ∞±ÂæÄÂì™ÈÇäÊèê‰æõÊúçÂãôÔºåÂ∞§ÂÖ∂Âú®ÂêåÊ®£ÂÖ∑ÊúâÊäÄË°ìËÉΩÂäõÁöÑITÈù¢ÂâçÔºåSIÁöÑËßíËâ≤ËºÉË¢´Ê∑°ÂåñÔºå‰ΩÜÊúâ‰∫õÁî¢Ê•≠SIÂèçËÄåËÉΩÂ∏∂‰æÜÁµÇÁ´ØÂÆ¢Êà∂ÔºåË¶ÅÂíåÂÖ∑ÊúâÂÆ¢Êà∂ÂêçÂñÆËàáÊäÄË°ìËÉΩÂäõÁöÑSIÂêà‰Ωú„ÄÇ‰ª•‰∏ãË®éË´ñÁ°¨È´îÂª†ÂïÜËàáÁµÇÁ´ØÂÆ¢Êà∂„ÄÇ
Â∞çÁ°¨È´îÊèê‰æõËÄÖËÄåË®ÄÔºåË∂äÂ§öÁ°¨È´îÂª†ÂïÜÂä†ÂÖ•ÂÖ∂ÂØ¶ÊòØÁ´∂Áà≠ËÄÖË∂äÂ§öÔºåÂÖ∂Áç®ÁâπÊÄßÊ∏õÂ∞ë‰∫ÜÔºåËÄå‰∏îÁî±ÊñºÂêÑÈõ≤Á´ØÂπ≥Âè∞Êú¨‰æÜÂùáÊúÉÈ°ßÂèä„ÄåÁõ∏ÂÆπÊÄß„ÄçÔºåÁ°¨È´îÊèê‰æõËÄÖËΩâÁßªÁ≥ªÁµ±ÁöÑÊàêÊú¨ÂÖ∂ÂØ¶‰∏çÈ´ò„ÄÇ
Âõ†Ê≠§ÊàëË™çÁÇ∫‰∏ÄÈñãÂßãÂê∏ÂºïË®±Â§öÁ°¨È´îÂª†ÂïÜÂä†ÂÖ•ÊïàÁõä‰∏çÈ´òÔºåÊáâË©≤ÊâæÂ∞ãËÉΩÊèê‰æõ„ÄåË∂≥Â§†Â§öÊ®£„ÄçÁ°¨È´îÔºàÂåÖÂê´ÊÑüÊ∏¨Âô®ËàáÈ°ØÁ§∫Âô®ÔºâÁöÑÂª†ÂïÜÔºåÊàñÊòØÂÖ∑ÊúâÊ•≠ÁïåÈ†òÂ∞éÂú∞‰ΩçÁöÑÁ°¨È´îÂª†ÂïÜÁµêÁõüÂ∞±Â§†‰∫Ü„ÄÇ
Â∞çÁµÇÁ´ØÂÆ¢Êà∂ËÄåË®ÄÔºå„ÄåË®±Â§öÁöÑÁ°¨È´îÂª†ÂïÜ„Äç‰πü‰∏çÊòØÈáçÈªûÔºåÂè™Ë¶ÅÂæÆËªüËÉΩÁ¢∫‰øùÊó¢ÊúâÁöÑÊÑüÊ∏¨Âô®Áõ∏ÂÆπÔºåÊàñÊòØÊúâÂª†ÂïÜËÉΩÊèê‰æõÂÆåÊï¥Ëß£Ê±∫ÊñπÊ°àÂ∞±Â•Ω„ÄÇ
ÊàëÊÉ≥ÈÄô‰πüÊòØÁÇ∫‰ªÄÈ∫ºÂæÆËªü‰∏ÄÈñãÂßãÂè™Ë∑üÂπæÂÆ∂ÂúãÂÖßÂ∑•Ê•≠ÈõªËÖ¶Â§ßÂª†Âêà‰ΩúÁöÑÂéüÂõ†„ÄÇÈÄô‰∫õÂ∑•Ê•≠ÈõªËÖ¶Â§ßÂª†ÁîöËá≥ÂéüÊú¨Â∞±Â∑≤Á∂ìÊúâÈóú‰øÇËâØÂ•ΩÁöÑÁ≥ªÁµ±Êï¥ÂêàÂïÜËàáÂÆâË£ùÂïÜÂ§•‰º¥ÔºåÂèØ‰ª•Ëß£Ê±∫Â∞ãÊâæÁ≥ªÁµ±Êï¥ÂêàÂïÜËàáÂÆâË£ùÂïÜÁöÑÂïèÈ°å„ÄÇ
Âú®ÂæÆËªü‰ª•ÂâçÁöÑÊôÇ‰ª£ÔºåOfficeÊòØÂÖ∑ÊúâÈÄôÊ®£ÁöÑÁ∂≤Áµ°ÊïàÊáâÁöÑÔºçÔºçË∂äÂ§öÁµÇÁ´ØÂÆ¢Êà∂‰ΩøÁî®OfficeÔºåÂ§ßÂÆ∂Ë∂äÈõ¢‰∏çÈñãOfficeÔºåÂõ†ÁÇ∫OfficeËÆäÊàê‰∫Ü‰∏ÄÁ®Æ„ÄåÊ∫ùÈÄöÊñπÂºè„Äç„ÄÇÔºàÈÄô‰πüÊòØË®±Â§ö‰∫∫‰∏çÈ°òÊÑèÊèõÊàêMacÈõªËÖ¶ÁöÑÂéüÂõ†ÔºåÂõ†ÁÇ∫Â§ßÂÆ∂ÈÉΩÁî®OfficeÔºåËΩâÊ™îÂ§™È∫ªÁÖ©ËÄå‰∏îÊ†ºÂºèÂèØËÉΩË∑ëÊéâÔºå‰∏çÁî®officeÊ™îÊ°àÈõ£‰ª•ÊµÅÈÄö„ÄÇÔºâ
‰ΩÜÊòØÔºåÈõ≤Á´ØÂπ≥Âè∞ËàáÂàÜÊûêÊï∏Êìö‰ºº‰πé‰∏çÂÖ∑Êúâ„ÄåÊ∫ùÈÄöÊñπÂºè„ÄçÁöÑÁâπÊÄßÔºåÂõ†ÁÇ∫ÂÆÉÈ†ÇÂ§öÊòØÂÄãÂàÜÊûêÁöÑÂæåÂè∞ËÄåÂ∑≤Ôºå‰∏çÊúÉÂõ†ÁÇ∫Ë∂äÂ§ö‰∫∫‰ΩøÁî®ÂæÆËªüÂπ≥Âè∞ÔºåÂ∞éËá¥ÁµÇÁ´ØÂÆ¢Êà∂ÁöÑ‰ΩøÁî®ÊïàÁéáÔºèÊïàÁõäËÆäÈ´òÔºàÈ†ÇÂ§öAIËæ®Ë≠òÊØîËºÉÊ∫ñÔºå‰ΩÜAIÊúÄÂº∑Â§ßÁöÑGoogleÈÇÑÂú®Á¨¨‰∏âÂêçÂë¢ÔºÅÁúãËµ∑‰æÜAIËæ®Ë≠òÊ∫ñÂ∫¶ÁõÆÂâçÈÇÑÊ≤íÊàêÁÇ∫ÂÅöBIÁöÑÁóõÈªûÔºåÂèØËÉΩÁõÆÂâçÈÇÑÊòØ„ÄåÊúâÁ∏ΩÊØîÊ≤íÊúâÂ•Ω„ÄçÁöÑÈöéÊÆµÂêßÔºüÔºâ„ÄÇ
‰ΩÜÈÄôÂÄãÂπ≥Âè∞ÈÇÑÊòØÂâµÈÄ†‰∫Ü„ÄåÈªèËëóÂ∫¶„Äç„ÄÇ
Â∞çÁµÇÁ´ØÂÆ¢Êà∂ËÄåË®ÄÔºåÂ∞§ÂÖ∂ÊòØITÈÉ®ÈñÄÔºåÂè™Ë¶ÅÈÅ∏ÂÆö‰∫ÜÈõ≤Á´ØÂπ≥Âè∞ÔºåÂπæ‰πé‰∏çÂ§™ÂèØËÉΩÊúÉÊèõÔºåÂõ†ÁÇ∫ÂæûË≥áÊñôÔºåÂà∞‰º∫ÊúçÂô®ÔºåÂÖ®ÈÉΩÂª∫ÊßãÂú®Èõ≤Á´ØÂπ≥Âè∞‰∏äÔºåË¶ÅÊê¨ÁßªÊòØÊÄéÈ∫ºÊ®£ÁöÑÂ§ßÂ∑•Á®ãÂëÄÔºÅ
ËÄå‰∏îÈõ≤Á´ØÂπ≥Âè∞Êû∂ÊßãÈÇèËºØËàáÊáâÁî®Á®ãÂºèÁöÑ‰ΩøÁî®ÔºåÈÉΩÊòØÂ≠∏ÁøíÁöÑÊàêÊú¨ÔºåÊ≤í‰∫ã‰∏çÊúÉÁµ¶Ëá™Â∑±ÊâæÈ∫ªÁÖ©ÊèõÁ≥ªÁµ±ÁöÑ„ÄÇÊâÄ‰ª•ÂæÆËªüÁî®ÈùûÂ∏∏‰æøÂÆúÁöÑÊñπÂºèÂÖàÊé®Âª£ÔºåÂõ†ÁÇ∫ÁµÇÁ´ØÂÆ¢Êà∂‰∏ÄÊó¶‰ΩøÁî®ÔºåÂ∞±Á∑äÁ∑äÈªèÂú®‰∏äÈù¢‰∫Ü„ÄÇÁµÇÁ´ØÂÆ¢Êà∂ÔºåËÄå‰∏îÊòØITÈÉ®ÈñÄÔºåÊòØÂæÆËªüÊúÄÈÅ©ÂêàÈñãÂßãÊé®Âª£ÊúüËß£Ê±∫ÊñπÊ°àÁöÑËßíËâ≤„ÄÇ
ÈÄô‰∫õÈÉΩ‰ª∞Ë≥¥ÂæÆËªüÁöÑÂ∏ÇÂ†¥ÊïôËÇ≤‰ª•ÂèäÁîüÊÖãÂúàÊúçÂãôÊèê‰æõËÄÖÔºåÈÄôÂπæÂÄã‰πüÊòØÂæÆËªüÂº∑Ë™øÁöÑË≥£Èªû„ÄÇ
ÔºàÂúñÁâá‰æÜÊ∫êÔºöÂæÆËªüAzure IoTÁ∂≤Á´ôÔºâ
ÂæÆËªüÁöÑAzureÊû∂ÊßãÂØ¶Âú®Â§™ÈæêÂ§ßÔºå‰∏îÁî±ÊñºÂÖ∂Âπ≥Âè∞ÁöÑÊú¨Ë≥™ÔºåÈáùÂ∞ç‰∏çÂêåstakeholderÂùáÊúâ‰∏ÄÂ•óË≥£ÈªûËàáËß£Ê±∫ÊñπÊ°àÔºå‰ΩÜÊòØÂÖ®ÈÉ®ÁöÑË™ûË®Ä‰∏ÄËµ∑Ê∑∑ÂêàÂú®Á∂≤Á´ô‰∏äÔºåÂØ¶Âú®ËÆì‰∫∫Èõ£‰ª•ÁêÜËß£ÔºåÂÄãÂà•ËßíËâ≤Èõ£‰ª•ÊâæÂà∞ÊâÄÈúÄË≥áË®ä„ÄÇ
ÂÖâÊòØÈ°û‰ººapp storeÁöÑÊáâÁî®Á®ãÂºè‰∏≠ÂøÉÔºåÂ∞±ÊúâAzure MarketplaceË∑üAppSource Âú®Âêå‰∏ÄÂÄãÈ†ÅÈù¢ÔºåË£°Èù¢ÂùáÊúâÊï∏ÂçÉÂÄã‰ª•‰∏äÁöÑÊáâÁî®Á®ãÂºèÔºåÈõñÁÑ∂ÂÖ∂ÊúâÊèêÂà∞ÂâçËÄÖÊòØITËß£Ê±∫ÊñπÊ°àÔºåÂæåËÄÖÊòØSaaSÂïÜÂãôËß£Ê±∫ÊñπÊ°àÔºå‰ΩÜÊàëÈÇÑÊòØÈùûÂ∏∏Ëø∑ÊÉëÈòøÔºÅ(ÂèØËÉΩÊàë‰∏çÊòØÂÆÉÁöÑtarget audienceÊàë‰∏çÁî®ÊáÇÂêß......)
ÊõæÁ∂ì‰ª•ÁÇ∫ÂæÆËªüÁöÑÊôÇ‰ª£Â∑≤Á∂ìÈÅéÂéª‰∫ÜÔºå‰ΩÜÂÖ∂ÊàêÂäüÂú∞ËΩâÂûãËá≥Èõ≤Á´ØÊ•≠ÂãôÔºåÂä†‰∏äÂñÑÁî®ÂÖ∂Èï∑‰πÖ‰ª•‰æÜÁ¥ØÁ©çÁöÑ‰ºÅÊ•≠ÂÆ¢Êà∂Èóú‰øÇËàáË™ûË®ÄÔºåËÆìÂÆÉÁî®AzureÊÄ•Ëµ∑Áõ¥ËøΩÔºåË∫ãË∫´Èõ≤Á´ØÂ§ßÂª†ÂõõÂ§ßÂ∑®È†≠ÔºåÂùê‰∫åÊúõ‰∏ÄÔºà‰ΩÜÂ∞ç‰∏ÄÊòØÈÅ†ÁõÆÔºåÂìàÔºÅÔºâÔºåÈÄôÊòØ‰ª§ÊàëÁõ∏Áï∂‰Ω©ÊúçÁöÑ„ÄÇÊàëÊúüÂæÖÈÄôÊ®£ÁöÑÂπ≥Âè∞ËÉΩËàáÂúãÂÖßÁ°¨È´îÂª†ÂïÜÂêà‰ΩúÁôºÊèÆÁ∂úÊïàÔºåÂâµÈÄ†ÂÉπÂÄºÔºåÁîöËá≥Â∞áÂè∞ÁÅ£ÁöÑËß£Ê±∫ÊñπÊ°àÊé®Âª£Âà∞‰∏ñÁïåÔºÅ
P.S. ÊàëÂπæ‰πéÊòØÂæûÈ†≠ÈñãÂßãË™çË≠òAzureÈÄôÂÄãÊúçÂãôÔºåÂÖ∂Êû∂ÊßãÂèàÁÑ°ÊØîÂ∑®Â§ßÔºå‰ΩÜÂæûÂàÜÊûê‰∏≠Â≠∏Âà∞ÂæàÂ§öÊù±Ë•øÔºå‰πüÂ∏åÊúõÂ§ßÂÆ∂Áµ¶Êàë‰∏Ä‰∫õÈºìÂãµÊàñÂõûÈ•ãÔºÅ
Ë≤°Èáë‰∫∫ÔºåÁßëÊäÄ‰∫∫ÔºåË°åÈä∑‰∫∫ÔºåÂïÜÊ•≠ÊÄùÁ∂≠Â≠∏Èô¢Áî¢ÂìÅË™≤Á®ã‰∏ªÁêÜ‰∫∫ÔºõÂçÅÈ§òÂπ¥Áî¢ÂìÅË°åÈä∑ËàáÁÆ°ÁêÜÁ∂ìÈ©óÔºåÂïÜÊ•≠ÊÄùÁ∂≠Â≠∏Èô¢Á§æÂúòÔºöhttps://lihi1.cc/jC9rkÔºõ ÂÆòÁ∂≤Ôºöhttps://bizthinkers.com/hi ÔºõMail: evonnetsai417@gmail.com
310 
1
310¬†
310 
1
Ë≤°Èáë‰∫∫ÔºåÁßëÊäÄ‰∫∫ÔºåË°åÈä∑‰∫∫ÔºåÂïÜÊ•≠ÊÄùÁ∂≠Â≠∏Èô¢Áî¢ÂìÅË™≤Á®ã‰∏ªÁêÜ‰∫∫ÔºõÂçÅÈ§òÂπ¥Áî¢ÂìÅË°åÈä∑ËàáÁÆ°ÁêÜÁ∂ìÈ©óÔºåÂïÜÊ•≠ÊÄùÁ∂≠Â≠∏Èô¢Á§æÂúòÔºöhttps://lihi1.cc/jC9rkÔºõ ÂÆòÁ∂≤Ôºöhttps://bizthinkers.com/hi ÔºõMail: evonnetsai417@gmail.com
"
https://medium.com/net-core/deploy-an-asp-net-core-app-with-ef-core-and-sql-server-to-azure-e11df41a4804?source=search_post---------66,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to deploy an ASP.NET Core web application with EF Core and SQL Server to App Service on Azure. Besides, I will demonstrate how to create an AzureSQL Database and use this database from the application.
The sections of the post will be as follows:
The application that we will deploy manages a database of TV Shows and its main page looks like below:
The app is an ASP.NET Core MVC web application and it uses EF Core to perform CRUD operations on a SQL Server database.
You can download the source code from this Github repository.
If you don‚Äôt have an Azure account, go to Microsoft Azure site and click Start Free button. Sign up with your Microsoft account and get $200 (for free) to explore any Azure service for 30 days.
In this section, we will create our SQL database on Azure.
Azure SQL Database is a general-purpose relational database, provided as a managed service. With it, you can create a highly available and high-performance data storage layer for the applications and solutions in Azure. It‚Äôs based on the latest stable version of the Microsoft SQL Server database engine.
SQL Database is a fully managed service that has built-in high availability, backups, and other common maintenance operations. Microsoft handles all patching and updating of the SQL and operating system code. You don‚Äôt have to manage the underlying infrastructure.
You can learn more about Azure SQL database here.
Now, we will create our SQL database on Azure.
Go to Azure Portal and select Create a Resource -> SQL Database as shown below:
Select your Subscription and click Create new in Resource Group field:
In the Database Details part, click Create New in the Server field:
After filling the necessary fields, click Ok. Please note your login and password as we will need this to connect to this database in the next section.
In the next screen, enter your Database name and click Configure Database to choose the plan that suits your needs:
Next, go the Networking tab and update the tab as shown in the following image:
Here we add the current client IP address because we will need this to connect to this database from our local machine.
We allow the Azure services to connect to this server as we will need this when we publish our web application to Azure. As mentioned in the highlighted section above, this gives access to all Azure services.
If you want to learn more about setting Azure SQL server/database firewall rules, please refer to this page.
Click Review + Create and then click Create. When the deployment is complete, click Go to resource button. Your resource will resemble the following:
If you click the All resources on the left pane, you can see the resources that you created:
In this section, we will connect to the database that we created on Azure from our local machine using Visual Studio.
In Visual Studio select View -> SQL Server Object Explorer and click Add SQL Server icon. Then fill the necessary fields with your database information as shown in the following image:
and click Connect.
After establishing the connection, you will see this database in the SQL Server Object Explorer. Right-click on the server name and select Properties and get the connection string from there:
Go to appsettings.json file in the project and add this connection string as follows:
Please note that you should update your password here as it is masked with stars.
In our scenario, we will use our local database for development and Azure SQL database for production. So, we need to add the following part to Startup.cs to provide the related connection string for each environment. Update the ConfigureServices method as follows:
The Database.Migrate() call helps you when it's run in Azure, because it automatically creates the databases that your .NET Core app needs, based on its migration configuration.
Now, we will test the application on our local computer.
Before that, we need to update the ASPNETCORE_ENVIRONMENT variable to use the Azure SQL database
Right-click on the project in Visual Studio and select Properties. Then change the variable as follows:
and then Save.
Now we can run our application.
In this test configuration, our application server is on localhost and database server is on Azure.
Run the application on the Visual Studio and click TVShowsApp on the web page and then Create New on the next page.
After creating our first record, it is shown below on the main page:
We can check the record created using SQL Server Object Explorer as well:
Now that we checked that we can use the Azure SQL database we created, we can revert the ASPNETCORE_ENVIRONMENTto Development.
In this section, we will deploy our application to App Service on Azure.
Azure App Service enables you to build and host web apps, mobile back ends, and RESTful APIs in the programming language of your choice without managing infrastructure. It offers auto-scaling and high availability, supports both Windows and Linux, and enables automated deployments from GitHub, Azure DevOps, or any Git repo.
We can deploy our application to App Service via automated deployment or manual deployment.
Azure supports automated deployment directly from several sources. The following options are available:
Below are options that you can use to manually push your code to Azure:
We will use the manual deployment with Visual Studio option.
In Visual Studio, right-click on the project and select Publish‚Ä¶ Then click Create Profile in the next screen:
In the next dialog, enter your App service Name and select the Resource Group that you created when creating the Azure SQL database on Azure portal. Then, click New in the Hosting plan field and choose Free for Size and fill the Name and Location fields.
Then click OK and Create respectively.
Now, our publishing profile is created as seen in the following image:
If we click the Publish button, our application will be deployed to Azure.
Before that, we need to add ASPNETCORE_ENVIRONMENT variable to our app service in Azure. We can check and update the App Service that we created on Azure portal.
Go to Azure Portal and click the App Services and then click the app service that you created:
In the app service, click Configuration and click New application setting. In the next dialog, enter ASPNETCORE_ENVIRONMENT for Name and Production for Value. Then, a new application setting will be formed as seen below:
Click Save.
Now, we can publish our application to Azure. Go to Visual Studio and click the Publish button that we saw in the publishing profile above.
After the deployment operation is completed, the web page is launched automatically and the application‚Äôs URL is as follows:
I created two more records and the main page of the application running on Azure looks like below now:
We can see the newly added records from the database as well:
When you‚Äôre working in your own subscription, it‚Äôs a good idea at the end of a project to identify whether you still need the resources you created. Resources left running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.
Now we will delete the resource group to stop billing for all the resources used within the group.
Go to Azure portal and click Resource groups and then select the resource group that you created. You will get a screen like below:
As you see in the above image, we have four resources in the group. We created the SQL server and the database from the Azure portal. App service and the app service plan were created automatically from the Visual Studio during the creation of the publishing profile.
You can click the Delete Resource group link to delete all of the resources.
That‚Äôs the end of the post. I hope you found this post helpful and easy to follow. If you have any questions and/or comments, please share them in the responses section below.
And if you liked this post, please clap your hands üëèüëèüëè
Bye!
https://docs.microsoft.com/en-us/azure/app-service/app-service-web-tutorial-dotnetcore-sqldb
https://docs.microsoft.com/en-us/aspnet/core/tutorials/publish-to-azure-webapp-using-vs?view=aspnetcore-3.1
https://docs.microsoft.com/en-us/learn/modules/host-a-web-app-with-azure-app-service/5-deploying-code-to-app-service
https://docs.microsoft.com/en-us/azure/sql-database/sql-database-technical-overview
Posts related to¬†.NET Core (ASP.NET Core, MVC, Web API‚Ä¶)
260 
5
260¬†claps
260 
5
Posts related to¬†.NET Core (ASP.NET Core, MVC, Web API‚Ä¶)
Written by
A software developer who loves learning new things and sharing these..
Posts related to¬†.NET Core (ASP.NET Core, MVC, Web API‚Ä¶)
"
https://medium.com/microsoftazure/setting-up-a-macbook-pro-mac-os-x-high-sierra-for-java-and-azure-cloud-development-ca5d60ed79ba?source=search_post---------67,"There are currently no responses for this story.
Be the first to respond.
A bit of background here before we start, because to me using Mac is a very recent event.
If you like this article, please clap for it! Click on the little hands icon to the left or bottom of this page.
I had been a Linux user for over 15 years. Not a specialist/hacker kind of thing ‚Äî just a user of Linux on the desktop (Ubuntu mostly; but also used Slackware and Fedora in the early days).
Why am I using Mac now? Well‚Ä¶ Let's go from the beginning.
In January of 2018 I left Oracle after about 5,5 years working in the Product Management organization doing Developer Relations for Java and Cloud ‚Äî you can see me here at Oracle Code conference keynote in October 2017.
At Oracle, by the time I joined the company in California in 2015(though I had been working for them since 2012 in Brazil), I was given only one laptop option: a Lenovo. Either I'd have Windows, or Linux. I stood with the Tux. Only in late 2017, Oracle started giving MacBooks for the engineering group by default ‚Äî which is pretty cool.
Left Oracle. Where was I heading to? Canada!
Because I recently joined Microsoft's Cloud Developer Advocacy group, to help the product engineering teams building and enhancing products and services to the taste and standards of the Java audience.
When I was transferred from Oracle Brazil to Oracle USA, I was given what is called the Intracompany Transferee Visa, aka L-1B type. This visa has a restriction: you are bond to that company until you get either a permantent resident card (green card), or you change status to an H1 type visa. Given my short stay in the US, neither processes completed in time.
So, in my conversations with the Microsoft Immigration team, we found that the best solution would be to move to Canada, where I'd be close to Redmond ‚Äî and in the same timezone.
Just one minor detail: Canada also issues a Work Permit bond to the company you are going to work for, if that is the case during the visa application. My wife on the other hand received a Work Permit that allows her to work pretty much for any company and almost any area, but that thanks to the relationship with me. Visa approval arrived in 2 weeks. Passport stamps plus 2 weeks. Pretty fast in this case, but if you want you can apply without a ""sponsor"". It will likely take more time to be approved ‚Äî if it gets approved (although in most cases, it does).
Done already with the background story‚Ä¶ let's get to the business!
At Microsoft I was given two options: either I‚Äôd get a Mac, or a Windows laptop. And given the similarities (in the terminal) between Mac and Linux, I chose the former. And as I said before, I do know how to exit vim.
First of all, you will need a decent terminal. No developer should rely solely on mouse and windows, so a powerfull command-line interface is a must!
This is one article I found that helped me set up my iTerm2. Walkthrough it.
medium.com
Besides the terminal, by following the article above you also get Homebrew, the missing package manager for Mac.
Cask is a complementary tool for Homebrew that extends to other capabilities such as installing common desktop applications distributed as DMG files somewhere on the Internet, but that are not available on Homebrew directly.
The next step is to install some basic tools for Java development.
Although Java 9 is available, many of my projects are quite still on Java 8. Some of them need changes in Maven POM files, and I just want to get them up and running. So that is why I am sticking to java8 below, but feel free to just say java, and cask will pick the latest (9).
It is also a good idea to always check the package version on Cask and compare with the latest on the website of the tool. Use info for that.
Now‚Ä¶ if you really want, you can also install Eclipse through Cask‚Ä¶And I will leave it to you :-)
These should give you enough to get started, but in the real world, you will need more. A lot more!
Quite frankly, if you know any other important tool for Javascript development, please let me know.
The reason I installed Wireshark twice, one with cask, was to get the GUI.
Now let's get into more sys admin, DevOps, runtime tools‚Ä¶
The Docker package available on Homebrew is just the CLI. If you install with Cask, you get the fully featured Docker for Mac. I strongly recommend you get this one. Lastly, Terraform migrated from Cask to Homebrew/Core, so that's why.
Docker also has an Edge version that includes Kubernetes out of the box. If you prefer that, don't install minikube (because you won't need it), and instead install the package docker-edge. Personally, I prefer Docker (stable version) and Minikube.
The Azure CLI is a key element for development with Azure services. Almost all Azure services are accessed through this single CLI, so keep it installed!
This one is tricky. In the past I used to have MySQL installed and that's it. Most of the time I'd start MySQL daemon, create a database and grant a new user entire permission, and then use that in my application.
These days, I think that for develompent purposes, all developers should be using databases inside Docker containers. But only for development/testing. I am still not so sure about RDBMS on Docker in production.
Now, what you really need to have are the Client CLIs for these databases. So let's install them!
Sadly, the new Oracle Database CLI ‚Äî sqlcl ‚Äî does not provide an easy install method. You will have to go to its download page and take it from there. Good luck.
The part about running databases locally for develoment purposes on Mac will come in a future post.
Other databases ‚Äî none, because I'd rather stick with good old fashion SQL :-)
One thing I noticed initially was that the battery of this Mac wasn't lasting long enough. I was barely getting 2 hours straight.
So I dug the internet for tweaks. Here are they:
So that's it! If you know any other tool that you believe is extremely important for a Java developer to have on its Mac, please comment below!
Cheers!
Any language.
157 
8
157¬†claps
157 
8
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Brazilian, Product and Program Manager for Java at Microsoft. Promoting great developer technologies to the world. Previously at Oracle.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://towardsdatascience.com/code-free-data-science-with-microsoft-azure-machine-learning-studio-65f245b81ee0?source=search_post---------68,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gilbert Tanner
Jun 17, 2019¬∑11 min read
In the last weeks, months and even years a lot of tools arose that promise to make the field of data science more accessible. This isn‚Äôt an easy task considering the complexity of most parts of the data science and machine learning pipeline. None the less many libraries and tools including Keras, FastAI, and Weka made it significantly easier to create a data science project‚Ä¶
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/data-science-reporter/a-simple-hands-on-tutorial-of-azure-machine-learning-studio-b6f05595dd73?source=search_post---------69,"There are currently no responses for this story.
Be the first to respond.
I‚Äôve recently stumbled upon a Microsoft Azure tool called Microsoft Azure Machine Learning Studio, which is a graphical, web interface to‚Ä¶
"
https://medium.com/asos-techblog/azure-service-bus-functions-urban-airship-at-work-together-for-an-e-commerce-website-94cc7e44e561?source=search_post---------70,"There are currently no responses for this story.
Be the first to respond.
A modern e-commerce website based on the microservices architecture is probably powered by a variety of different services and APIs: Promotions, Customers, Product Catalog, Search, Checkout, Recommendations, Order Processing and potentially many more pieces. These services can communicate between each other by direct API calls, if they need an immediate feedback, or they can use a messaging-based system implementing the Publisher-Subscriber pattern, where they basically send a message and then zero, one or multiple subscribers can be notified and react to the message in different ways. In the Microsoft Azure cloud, the messaging systems are Azure Service Bus, Event Hub and Event Grid (see here for a comparison), and listeners can be WebJobs, Console apps, Azure Functions or other types of applications.
The diagram below offers a simplified and high-level overview of how some of these pieces can work together to implement a feature that allows customers to receive push notifications when an out-of-stock product comes back in stock:
This article focuses on showing how to set up and implement the bits around sending messages to Service Bus and the Azure Function that is triggered by those messages, which then sends a push notification through Urban Airship.
I won‚Äôt cover this step by step, because this article on MSDN does a great job already. Suffice to say, I created a new Azure Service Bus resource, created a Topic named ‚Äòproductstockchanged‚Äô and a subscription for it named ‚ÄòBackInStockNotifications‚Äô. (Since I wanted to test how things work with multiple subscriptions, I also created a second, named ‚ÄòUpdateCatalog‚Äô. This is an extra step and not needed for the scenario being implemented here). Here‚Äôs what the final state looks like:
Reminder: Topics are for when you might need multiple listeners to receive and process an event independently, while Queues are for when there will be only one processor. As shown by the diagram at the beginning, our ‚Äòstock changed‚Äô messages would be processed by at least a couple of services (one that sends out push notifications and one that updates the ‚Äòin stock‚Äô information on the Product Catalog database), so a Topic was required.
In reality, it would probably be more accurate to say that a Queue is for commands that someone sends to a recipient, and the recipient is the owner of the message. A Topic instead implements the actual Pub-Sub pattern, where the publisher is the owner that broadcasts a message without knowing who are the listeners that will pick it up.
In the real implementation it would be the Stock microservice that sends messages to the Service Bus Topic once a product goes out of stock or is back in stock. For this demo however, I created a simple Console app that sends messages according to comments entered on Terminal.
Microsoft.Azure.ServiceBus is the NuGet package that must be added as a new dependency:
The code is quite simple: a TopicClient object is instantiated with the Service Bus connection string and the Topic name in input, and its SendAsync method is used to send a new Message object, which is created with the json produced by serialising a ProductStockChanged object. ProductStockChanged contains the ID of product affected by the change, and a boolean indicating whether the product is now in stock or out of stock. An additional attribute that I could have added is the product title, but I decided to keep things simple.
New messages are sent by sending commands like ‚Äòinstock 123‚Äô or ‚Äòoutofstock 123‚Äô (where ‚Äò123‚Äô is the ProductID) from the command line. Here‚Äôs the complete code:
If you now run this, send a few sample commands, and then reload the Topic page on the Azure Portal, you‚Äôll see that the ‚ÄòMessage Count‚Äô for the subscriptions created previously was increased because there are now messages there waiting to be picked up and processed.
There are plenty of good services to send push notifications for iOS, Android and Web, but I chose Urban Airship (UA) because it‚Äôs well documented, has a nice UI for creating messages (I used the UI during some initial testing, before switching to the API) and a good client-side SDKs and RESTful API. However, other services work just as well (Microsoft itself offers Notification Hubs, and I have also successfully used OneSignal previously), and the overall solution would stay exactly the same.
For this demo, I chose to send web push notifications rather than notifications to an Android or iOS app, because I integrated push notifications on many native mobile apps in the past but never tried it on web and I felt it was easier to set up for a demo project build from scratch. I was right in thinking this ‚Äî while it takes a few steps for web, it takes longer to create a dummy native app and deal with push certificates. However, the beauty is that since we‚Äôre using UA we‚Äôre abstracted away from the platform-specific implementation ‚Äî we just ask UA to send a push notification and it does it for all registered/configured platforms. This means that if we wanted to add support for iOS or Android later, there wouldn‚Äôt be code changes.
So, I created a free account, and set up a web-based project: you choose a title, default action url (this is where the user would be redirected to if they click the notification) and icon url, and you‚Äôre ready to go: Urban Airship will now let you download a zip file, which contains a JS file and a snipped to add to your front-end website. More on this later‚Ä¶
As we are going to interact with UA via its API, I accessed Settings / APIs & Integrations and grabbed the AppKey and App Master Secret, which are required to authenticate the calls as you‚Äôll soon see in the Function implementation.
It‚Äôs time to create the ‚Äòcore‚Äô of the solution, which is the Azure Function that actually sends the push notification when it gets triggered by new messages for the productstockchanged Topic on the BackInStockNotifications subscription. I created a new Azure Functions resource and created a new function from the ‚ÄòService Bus Topic trigger‚Äô template, as shown below:
Should you need to modify the trigger‚Äôs settings after the function has been created, visit the Integrate page:
When a new message is posted to Service Bus, the Azure Function is instantiated and its Run method is invoked. Run has a string input parameter, which is the json of the Message created previously, that contains the ProductId and IsInStock attributes. The json body is deserialised into a ProductStockChanged object, and, if its IsInStock property is true, the UA API is used to send a push notification to all clients that have expressed an interest in knowing when that particular product is back in stock.
How do we target only those customers/clients and not everyone though? A quick solution is using ‚Äòtags‚Äô: a client registers for a tag named notify-instock-{product id here}, and the push request has a parameter indicating that we‚Äôre only targeting clients with that tag associated to them. A client could of course also unregister for that tag later on, and therefore stop receiving notifications for some products.
The actual UA API call is done by doing a POST request through a HttpClient object, which has an Authorization header with the UA credentials, and takes in input a json body with the text of the notification and the audience‚Äôs tag. (Targeting can be much more advanced, for example, you can combine multiple tags using AND or OR; can target only specific device types, and can schedule a notification for a specific time. Read more here.)
The following code should be self-explanatory:
The Urban Airship credentials needed to call the UA API should never be hardcoded in the script, but be in app settings and read through the environment variables, or in Azure Key Vault.
Note the #load statement in the first line, which loads an external file with the ProductStockChanged class definition. This was created from the ‚ÄòView files‚Äô tab displayed below:
Please note: I initially created and tested the Function locally with Visual Studio, but then I manually copied/pasted the code into the online editor because the ‚ÄòPublish to Azure‚Äô wizard was not available on my installation for some reasons. I‚Äôm using macOS, but I guess you‚Äôd have a better experience with Visual Studio on Windows though.
Refer to this page about Azure Functions Core Tools to find out more about the local development experience. Developing the Function locally and then deploying to Azure means that you‚Äôll have a Function based on a precompiled library rather than on a C# script, and also means you‚Äôll be able to more easily reference other libraries/namespaces as you would do normally. In order to use this option however you can‚Äôt create the Function from the portal.
Refer to this page to see how a new Function can be created through the Azure CLI rather than the portal.
Once the Function is complete, go back to your console app developed previously, and send a ‚Äòinstock 123‚Äô command. The log window on the Azure Function should display some lines confirming the Function was invoked by the Service Bus Topic trigger and some custom logs. If you then switch to the Urban Airship‚Äôs Activity log, you should see a new push notification sent for the notify-instock-123 tag.
The above log is the only way to verify that the UA call is successful so far, because the client web page hasn‚Äôt been implemented yet.
This article by Zhongming Chen, published on the ASOS Tech Blog, has some very good notes about Azure Functions, so take a look if you‚Äôre planning to use them.
The last step involves creating the client app, which is a website or, rather, a single web page for this demo. In a real-world implementation there would be product listing pages and product details pages getting data from something like a Product Catalog API, and, as part of the product attributes returned by the API, there would be something like an ‚ÄòisInStock‚Äô attribute. When that‚Äôs false, the customer might click a button to register their interest for the product, and the notify-instock-{productId} described before would be associated to them.
For the purpose of this article however, the simplest, and ugliest page ‚Äî with a static list of three product names and a ‚ÄòNotify‚Äô checkbox on their right-hand side will do just fine. The UA client-side SDK, as well as the initial configuration, is fully explained on this page, but here‚Äôs the gist:
Here‚Äôs the full HTML/js code:
When the page loads for the first time (or even subsequently, if you just cancel the prompt for the notifications) this is what you should get:
Click ‚ÄòAllow‚Äô, and then tick the ‚ÄòNotify‚Äô checkbox for one or more products:
Finally, use the Console app to send a ‚Äòinstock 123‚Äô command. If everything is set up correctly, you should get a notification like this on Chrome (even if you‚Äôre on another page of course):
On a Mac, the notification will also be shown on the system-level Notifications panel on the right-hand side of your desktop:
Please note: don‚Äôt despair if you don‚Äôt immediately get a notification. It sometimes takes a few minutes to show up, after you see the log on the Urban Airship console.
Urban Airship and the other third-party providers abstract a lot of details. If you want to better understand how the Web Push Protocol works behind the scenes, start from this page.
By using third-parties, it spares you the need to create all the infrastructure to record the IDs of the devices that register to get the notifications (ie: a DB with a table of device IDs, their tags etc.) and all the work to send the notifications to all targeted devices. After you make a single call to Urban Airship targeting a tag, it might mean 1M calls that UA does on your behalf, one per device, if there are 1M users/devices registered for that tag. You can easily see how not having to worry about that is good.
When you create a new Azure Service Bus Topic subscription through the Azure Portal, you won‚Äôt have the option to set up a ‚Äòfilter‚Äô, which means that all new messages for that Topic will be copied into the subscription. However, you might only want to process a sub-set of messages, not all. In the scenario described in this article, only messages with ‚ÄòIsInStock = true‚Äô need to be processed, assuming that customers only want to be notified when something they liked is back in stock‚Ä¶and they don‚Äôt need to be notified about something going out of stock (that might be a reasonable option as well in reality, but not for the purpose of this demo).
The way it works so far is that there‚Äôs an if statement when the Function starts and the json data is decoded into a ProductStockChanged message. While this works, it‚Äôs not optimal, because it means the function is going to be executed plenty of times for messages it doesn‚Äôt need to process ‚Äî this means wasted compute time, which in turns means additional instances being necessary and additional cost. A better solution is to create a subscription with a rule/filter, so that only messages matching certain conditions are copied into it for further processing. It‚Äôs not possible to create the filter directly from the Portal unfortunately, but luckily, it‚Äôs simple enough to do it programmatically (in a method that you could execute from a management console app or something similar). Here‚Äôs the method that modifies the existing subscription by adding a SqlFilter for ‚ÄòIsInStock = true‚Äô:
The SqlFilter can refer to attributes defined in the message‚Äôs UserProperties array, not on attributes defined directly in the json (because the message‚Äôs body could be plain text, json, xml or anything else)‚Ä¶so now line #67 of the Console app‚Äôs code displayed in Section 2 of the article should be clearer.
After executing this code, use the Console app to send some ‚Äòoutofstock {id}‚Äô commands, and you‚Äôll see that they‚Äôll end up in the UpdateCatalog subscription (the second subscription I had created at the beginning, with no filter), but not in the BackInStockNotifications subscription. A much better result. (I still left the check in the Function itself as well though, just in case the filter doesn‚Äôt work or is removed by mistake.)
An even better solution that doesn‚Äôt require executing custom managed code as part of your deployment/configuration pipeline is using the Azure CLI as described here.
This was meant to be a POC, but in a real implementation you will very likely want to consider the following aspects when sending out push notifications:
That‚Äôs all folks. Albeit quick and simple to set up, the proposed implementation is quite powerful and flexible, as it‚Äôs based on loosely coupled components (the checkout and stock microservices know nothing about the Function that handles the stock-related messages to send push notifications) and it offers great scalability (the Azure Function will be scaled up automatically according to how many messages it has to process). Last but definitely not least, the use of Service Bus offers a better overall reliability of our system, because if the delivery of the notification fails for any reason (eg: Urban Airship is down), the Function fails, the message goes back into the queue and it will be processed again later. Hooray Azure!
Who am I / what do I do? I proudly work as a Solutions Architect in the Mobile Team @ ASOS.com (iOS app | Android app), and we‚Äôre always looking for strong, friendly and talented developers that want to have an impact on how customers shop online. ASOS is the biggest online-only retailer in the UK and, let‚Äôs be real, the best tech and fashion company in the world. Some of the technologies we use are Swift for iOS, Kotlin for Android, React and Node on the web front-end, .NET and Azure on the back-end. If that sounds interesting to you, and you happen to live in beautiful London (or are willing to move here ‚Äî after all, it‚Äôs the best city in Europe, except for some in Italy!), do get in touch!
A collective effort from ASOS's Tech Team, driven and‚Ä¶
224 
1
Thanks to Giovanni Puntil.¬†
224¬†claps
224 
1
Written by
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
A collective effort from ASOS's Tech Team, driven and directed by our writers. Learn about our engineering, our culture, and anything else that's on our mind.
Written by
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
A collective effort from ASOS's Tech Team, driven and directed by our writers. Learn about our engineering, our culture, and anything else that's on our mind.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-to-deploy-a-blazor-application-on-azure-cf6f3b1f03a0?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
In this article, we will learn how to deploy an ASP.NET Core hosted Blazor application on Azure. We will use Visual Studio 2017 to publish the app. We will create a SQL database server on Azure to handle DB operations.
Please refer to my previous article Cascading DropDownList in Blazor Using EF Core to create the application that we will be deploying in this tutorial.
We will create a resource group on Azure portal to contain all our resources on Azure.
Login to Azure portal and click on Resource groups on the left menu and then click Add. It will open a ‚ÄúResource group‚Äù window as shown in the image below:
In this window we need to fill the following details:
We will create the SQL database and a database server on the Azure portal to handle our DB operations.
Click on SQL databases on the left menu of your Azure portal and then click Add. It will open a ‚ÄúSQL Database‚Äù window as shown in the image below:
Here you need to fill in the following details:
Before creating the database, we need to create a database server for the SQL database. Click on the ‚ÄúServer configure required settings‚Äù and then click Create a new server. It will open a ‚ÄúNew server‚Äù window as shown in the image below:
Here we need to furnish the following details:
Check the ‚ÄúAllow Azure services to access server‚Äù check box and click on Select to create your DB Server.
Note: The word ‚Äúadmin‚Äù is restricted for the administrator user name of the database server. Use any other username than ‚Äúadmin‚Äù.
Once the DB server gets created, you will be redirected back to the ‚ÄúSQL Database‚Äù window. You need to click on the ‚ÄúCreate‚Äù button to create your database.
Here is the whole process explained in a gif.
The database DDLDemodb do not contain the tables that we are using in our application. We will connect to Azure database using SQL Server Management Studio (SSMS) to create our DB objects.
Open SSMS in your machine and put the server name as ddldbserver.database.windows.net. Provide the admin user id and password that you have configured in the previous section. Then click on ‚ÄúConnect‚Äù.
You will get a pop up window for configuring the firewall rule to access the Azure DB. Login with your Azure account credentials and add your machine‚Äôs IP address under Firewall rule. Click on OK to connect to the Azure database server. Refer to the image below:
Once the connection is successful, you can see the DDLDemodb database on the server. Refer to my previous article Cascading DropDownList in Blazor Using EF Core. Run the SQL commands to create and insert sample data in the Country and Cities tables that we are using in our application.
After creating the database objects, we need to replace the connection string of local database in our application with the connection string of the Azure database.
Open Azure portal and click on SQL databases on the left menu. It will open a window displaying the list of all the databases that you created on the Azure portal. Click on DDLDemodb database and select Connection strings from the menu. Select the ADO.NET tab and copy the connection string. Refer to the image below:
You need to put the admin user id and password for the database server that you have configured earlier in this connection string.
Open the BlazorDDL application using Visual Studio, navigate to BlazorDDL.Shared/Models/myTestDBContext.cs and replace the local connection sting with this new connection string.
Launch your application from Visual Studio to verify if the new connection string is configured correctly and you are able to access the Azure database.
If the application is not working and you are unable to connect to the database, then check if your connection string is correct or not. Once the application is working as expected in your local machine then move to the next section to publish it on Azure.
To publish the Blazor app on Azure, Right-click on the Server project of your solution and click publish. In this case, it will be BlazorDDL.Server >> Publish.
It will open the Pick a publish target window. Select App Service from the left menu. Select the Create New radio button and click on the ‚ÄúCreate profile‚Äù button. Refer to the image below:
The next window will ask you to login to your Azure account if you are not logged in. Once the login is successful, a Create App Service window will open. Refer to the image below:
The fields of this window have default values in them as per the configuration of your Azure account. However, you can change these values depending on your requirements.
You can fill the details as mentioned below:
Click on the ‚ÄúCreate‚Äù button to start the application deployment on Azure. It will take few minutes to complete depending on your internet connection speed.
After the deployment is successful, click on the ‚ÄúPublish‚Äù button to publish the app to Azure. Once the application is published successfully, the website will be launched automatically in the default browser of your machine. You can also access the website using the URL BlazorDDLDemo.azurewebsites.net.
You can see the application in your browser as shown in the image below:
In this article, we learned how to deploy and publish a Blazor application on Azure. We created a SQL database and DB server on Azure and used them in our application to handle the DB operations.
Get my book Blazor Quick Start Guide to learn more about Blazor.
You can also read my other articles here.
Originally published at https://ankitsharmablogs.com/
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
276 
276¬†claps
276 
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
We‚Äôve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://media.consensys.net/truffle-is-now-available-on-microsoft-azure-marketplace-12d646caef59?source=search_post---------72,"Truffle is a development environment, testing framework and asset pipeline for the Ethereum Blockchain, aiming to make life as an Ethereum developer easier.
Earlier in the week, Microsoft Azure launched their Truffle integration in its Marketplace, offering a suite of developer tools that now makes it easier to build on Ethereum. With Truffle, you get:
To learn more about the pricing details, visit the Microsoft Azure Marketplace.
News, insights, and education on all things‚Ä¶
144 
1
144¬†claps
144 
1
Written by
ConsenSys is the leading Ethereum software company building MetaMask, Infura, Codefi, ConsenSys Quorum, Truffle, and Diligence. Visit consensys.net
News, insights, and education on all things decentralization from leaders in the blockchain industry
Written by
ConsenSys is the leading Ethereum software company building MetaMask, Infura, Codefi, ConsenSys Quorum, Truffle, and Diligence. Visit consensys.net
News, insights, and education on all things decentralization from leaders in the blockchain industry
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/deep-learning-turkiye/azure-ml-studio-kullanarak-bitcoin-fiyat-tahminlemesi-yapmak-f5179f395811?source=search_post---------73,"There are currently no responses for this story.
Be the first to respond.
Eƒüer elinizde b√ºy√ºk bir veri k√ºmesi varsa bu veri i√ßinde bazƒ± √∂r√ºnt√ºleri (pattern) bularak bu veriyi anlamlandƒ±rabilirsiniz. Makine √∂ƒürenmesinden faydalanarak b√ºy√ºk veri i√ßindeki √∂r√ºnt√ºleri tespit edebilir ve bu ili≈ükileri yeni veri k√ºmeleri √ºzerinde kullanabilirsiniz.
Makine √∂ƒürenmesi i√ßin kendi bilgisayarƒ±nƒ±zƒ± kullanabilirsiniz ama bu durumda √ßoƒüu zaman kaynak (GPU, CPU) yetersizliƒüinden dolayƒ± modelinizi eƒüitmeniz uzun s√ºrecektir. Bu gibi durumlarda bulut servislerini kullanmanƒ±z hem b√ºy√ºk veriyle uƒüra≈ümanƒ±zƒ± kolayla≈ütƒ±racak hem de size bazƒ± ekstra kolaylƒ±klar sunacaktƒ±r. Google Colab ve Azure Machine Learning Studio bu bulut servislerinden bazƒ±larƒ±dƒ±r. Bu yazƒ±da Azure ML Studio‚Äôyu tanƒ±tacaƒüƒ±m.
Azure ML Studio, makine √∂ƒürenmesi modellerini kolaylƒ±kla eƒüitmenizi saƒülayan grafiksel √∂ƒüelerle kolayla≈ütƒ±rƒ±lmƒ±≈ü bir ara√ßtƒ±r. Bu aracƒ± kullanarak veri √∂n-i≈üleme yapabilir, veri √ºzerinde makine √∂ƒürenmesi algoritmalarƒ±nƒ± √ßalƒ±≈ütƒ±rarak denemeler yapabilirsiniz. Modelinizi test ederek istenilen sonu√ßlara ula≈ütƒ±ƒüƒ±nƒ±zda direkt olarak Microsoft Azure‚Äôda yayƒ±nlayabilirsiniz.
Azure ML Studio bu i≈ülemleri kolaylƒ±kla yapmanƒ±z i√ßin hazƒ±r veri √∂n i≈üleme mod√ºlleri, makine √∂ƒürenmesi algoritmalarƒ± ve yayƒ±nladƒ±ƒüƒ±nƒ±z modele eri≈üim sunan bir API saƒülƒ±yor. Bu sayede bir geli≈ütirici olarak eƒüittiƒüiniz makine √∂ƒürenmesi modelini hi√ß uƒüra≈ümadan yayƒ±nlayarak uygulamalara hizmet vermeye ba≈ülayabiliyorsunuz.
Azure ML Studio‚Äôya buradan ula≈üabilirsiniz. Herhangi bir kart bilgisi girmeden platformu √ºcretsiz olarak kullanabilir ve denemeler yapabilirsiniz. √úcretsiz versiyonda bir √ßalƒ±≈üma alanƒ±na en fazla 100 bile≈üen eklemenize izin veriliyor. Detaylar i√ßin burayƒ± inceleyebilirsiniz. Giri≈ü yaptƒ±ktan sonra kar≈üƒ±nƒ±za ≈ü√∂yle bir ekran gelecektir.
Burada olu≈üturduƒüunuz projeleri, denemeleri, web servisleri, Jupyter not defterlerini, Studio‚Äôya y√ºklediƒüiniz veri k√ºmelerini ve eƒüittiniz modelleri g√∂rebilirsiniz.
ƒ∞lk denemenizi olu≈üturmak i√ßin Experiments b√∂l√ºm√ºn√º se√ßip a≈üaƒüƒ±dan New‚Äôe tƒ±klayƒ±n. Daha sonra a√ßƒ±lan ekranda payla≈üƒ±lan hazƒ±r denemeleri g√∂rebilirsiniz. Biz ≈üimdilik sƒ±fƒ±rdan olu≈üturacaƒüƒ±mƒ±z i√ßin ‚Äúblank experiment‚Äùi se√ßerek devam edelim. Kar≈üƒ±nƒ±za a≈üaƒüƒ±daki gibi bo≈ü bir st√ºdyo alanƒ± gelecektir.
Bu ekranda soldaki bile≈üenleri bo≈ü alana s√ºr√ºkle bƒ±rak i≈ülemiyle ekliyorsunuz. Hazƒ±r veri k√ºmeleri, veri √∂n i≈üleme y√∂ntemleri, yeni veri k√ºmesi i√ßeri aktarma, makine √∂ƒürenmesi algoritmalarƒ±, Python ve R scriptleri ve yazƒ± analiz algoritmalarƒ± gibi her≈üeyi bile≈üen olarak kullanabiliyorsunuz.
ƒ∞lk denememiz i√ßin Data Input and Output b√∂l√ºm√ºnden Import Data bile≈üenini s√ºr√ºkleyip bƒ±rakalƒ±m. Bu bile≈üen sayesinde kullanacaƒüƒ±mƒ±z veriyi nereden elde edeceƒüimizi belirliyoruz.
S√ºr√ºklediƒüiniz Import Data bile≈üenini se√ßin solda a√ßƒ±lan men√ºde ‚ÄúLaunch Import Data Wizard‚Äù ƒ± tƒ±klayarak veriyi nereden-nasƒ±l alacaƒüƒ±nƒ±zƒ± belirleyin. Veriyi direk dosya y√ºkleyerek ekleyebileceƒüiniz gibi isterseniz herhangi bir online ortamdan da (RSS, Azure vb.) √ßekebiliyorsunuz.
ƒ∞lk √∂nce son bir yƒ±lƒ±n BTC fiyat verisini √ßekelim. Biz bu √∂rnekte datayƒ± bir siteden csv formatƒ±nda alacaƒüƒ±z bundan dolayƒ± ‚ÄúWeb URL via HTTP‚Äù se√ßerek ilerleyin. Sonraki ekranda ‚ÄúData source URL‚Äù olarak ‚Äúhttps://api.blockchain.info/charts/market-price?timespan=1year&format=csv‚Äù adresini girin. CSV veri k√ºmemizde ba≈ülƒ±klar olmadƒ±ƒüƒ± i√ßin ‚ÄúCSV or TSV has header row‚Äù b√∂l√ºm√ºn√º tiklemeyin.
Burada kullandƒ±ƒüƒ±nƒ±z adresin y√∂nlendirilmi≈ü url olmamasƒ± gerekiyor aksi takdirde ‚ÄúHttp redirection not allowed‚Äù gibi bir hata alƒ±yorsunuz. Eƒüer elinizdeki adreste bazƒ± y√∂nlendirmeler varsa nihai adresi bulmak i√ßin redirectdetective sitesini kullanabilirsiniz.
Veri √ßekme i≈ülemini ba≈ülatmak i√ßin st√ºdyonuzdaki ‚ÄúImport Data‚Äù bile≈üenini se√ßin saƒü tƒ±klayƒ±n ve ‚ÄúRun Selected‚Äùi se√ßin. ƒ∞≈ülem tamamlandƒ±ƒüƒ±nda bile≈üenin saƒüƒ±nda ye≈üil tik i≈üareti belirecektir.
ƒ∞≈ülem bittikten sonra veriyi g√∂r√ºnt√ºlemek i√ßin ‚ÄúImport Data‚Äù bile≈üenine saƒü tƒ±klayƒ±p ‚ÄúResults dataset‚Äù->‚ÄùVisualize‚Äùi se√ßin.
√áektiƒüiniz veri a≈üaƒüƒ±daki gibi g√∂sterilecektir. Burada kolon se√ßerek o kolona ait verinin detaylarƒ±nƒ± saƒüdaki b√∂l√ºmde g√∂rebilirsiniz. Bu b√∂l√ºmde kolona ait istatistiki bilgileri (ortalama, standart sapma vb.) kontrol edebilir, frekans daƒüƒ±lƒ±mƒ±nƒ± g√∂rebilir ve diƒüer kolonlarla kar≈üƒ±la≈ütƒ±rmalar yapabilirsiniz.
Bu ≈üekilde bir yƒ±llƒ±k Bitcoin fiyat verisini √ßektik. ≈ûimdi aynƒ± i≈ülemi g√ºnl√ºk Bitcoin piyasa deƒüeri (market cap) verileri i√ßin yapalƒ±m. Bu i≈ülem i√ßin yeni bir ‚ÄúImport Data‚Äù bile≈üenini s√ºr√ºkleyip bƒ±rakƒ±n. ‚ÄúData source URL‚Äù olarak ≈üu adresi girin: ‚Äúhttps://api.blockchain.info/charts/market-cap?timespan=1year&format=csv‚Äù. Bu bile≈üeni ekledikten sonra se√ßin ve saƒü tƒ±klayarak ‚ÄúRun Selected‚Äù ile √ßalƒ±≈ütƒ±rƒ±n. Daha sonra visualize butonuna tƒ±klayarak veriyi g√∂rselle≈ütirin.
Son bir yƒ±lƒ±n Bitcoin piyasa deƒüeri verisini de √ßektik. ≈ûimdi bu iki veri k√ºmesini tarih (Col1) kolonunu baz alarak birle≈ütirelim. Bu i≈ülem i√ßin birle≈ütirme (join) yapmamƒ±z gerekiyor. Sol taraftaki ‚Äúsearch experiment items‚Äù kutucuƒüuna ‚Äújoin data‚Äù yazarak bu bile≈üeni bulun ve √ßalƒ±≈üma alanƒ±na s√ºr√ºkleyin. Daha sonra a≈üaƒüƒ±daki gibi ‚ÄúImport Data‚Äù bile≈üenlerinin altƒ±ndaki noktalarƒ± ‚ÄúJoin Data‚Äù bile≈üenine baƒülayƒ±n.
Birle≈ütirme i≈üleminde hangi kolonun baƒülantƒ± kolonu olarak kullanƒ±lacaƒüƒ±nƒ± belirtmemiz gerekiyor. Bu i≈ülem i√ßin ‚ÄúJoin Data‚Äù bile≈üenini se√ßin ve saƒü taraftan ‚Äúlaunch column selector‚Äù e tƒ±klayarak ‚ÄúCol1‚Äù kolonunu yani tarih kolonunu se√ßin. Join i≈ülemi yapƒ±ldƒ±ƒüƒ±nda iki import bile≈üeninden de gelen 2 tane tarih kolonu olacak, 2. ‚Äúimport data‚Äùdan gelen tarih kolonunu tutmaya gerek yok bu y√ºzden ‚ÄúKeep right key columns‚Äù se√ßeneƒüindeki tiki kaldƒ±rƒ±n.
‚ÄúJoin Data‚Äù bile≈üenini se√ßerek ‚ÄúRun Selected‚Äù ile √ßalƒ±≈ütƒ±rƒ±n ve kontrol i√ßin ‚ÄúVisualize ‚Äù ile tabloyu a√ßƒ±n. Bu i≈ülem sonunda veri k√ºmesi a≈üaƒüƒ±daki gibi BTC fiyatƒ± ve piyasa deƒüeri aynƒ± tabloda birle≈ütirilmi≈ü ≈üekilde g√∂r√ºnecektir.
Kolon isimlerini d√ºzenlemek i√ßin ‚ÄúJoin Data‚Äù bile≈üenini altƒ±na a≈üaƒüƒ±daki gibi ‚ÄúEdit Metadata‚Äù bile≈üenini ekleyin. Kolonlarƒ± se√ßin ve ‚Äúnew column names‚Äù alanƒ±na kolon adlandƒ±rmalarƒ±nƒ± virg√ºl ile ayƒ±rarak yazƒ±n.
‚ÄúJoin Data‚Äù bile≈üeninin √ßƒ±ktƒ±sƒ±nƒ± ‚ÄúEdit Metadata‚Äù bile≈üenine baƒülayƒ±n. ‚ÄúEdit Metadata‚Äù bile≈üenini √ßalƒ±≈ütƒ±rƒ±n ve tabloyu g√∂r√ºnt√ºleyin, tablo a≈üaƒüƒ±daki gibi g√∂r√ºnecektir.
Bu veri k√ºmesindeki tarih kolonunu kullanmayacaƒüƒ±z. Tablodan kaldƒ±rmak i√ßin ‚ÄúSelect Columns in Dataset‚Äù bile≈üeni ekleyin ve ‚ÄúEdit Metada‚Äù bile≈üenine baƒülayƒ±n. Bu bile≈üeni se√ßerek saƒü taraftan fiyat ve piyasaDeƒüeri kolonlarƒ±nƒ± se√ßin.
Birle≈ütirdiƒüimiz veri k√ºmesini eƒüitime sokmadan √∂nce eƒüitim ve test verisi olarak ayƒ±racaƒüƒ±z. Bunun i√ßin ‚ÄúSplit Data‚Äù bile≈üenini kullanacaƒüƒ±z, bu bile≈üeni bularak s√ºr√ºkleyip bƒ±rakƒ±n. ‚ÄúSelect Columns in Dataset‚Äù bile≈üeninden ‚ÄúSplit Data‚Äù bile≈üenine baƒülantƒ± okunu a≈üaƒüƒ±daki gibi baƒülayƒ±n.
‚ÄúSplit Data‚Äù bile≈üenini se√ßerek eƒüitim/test verisi oranƒ±nƒ± belirleyin, verilerin ayrƒ±mƒ± bu orana g√∂re yapƒ±lacaktƒ±r. Bu oranƒ± %80 eƒüitim, %20 test verisi olarak belirleyeceƒüiz bunun i√ßin ‚ÄúFraction‚Ä¶‚Äù b√∂l√ºm√ºne yukarƒ±daki resimde g√∂sterildiƒüi gibi 0.8 girin.
‚ÄúSplit Data‚Äù bile≈üenini de √ßalƒ±≈ütƒ±rƒ±n ve altƒ±ndaki noktalara saƒü tƒ±klayarak g√∂r√ºnt√ºleme yaparak verileri kontrol edin.
Bu denemede doƒürusal baƒülanƒ±m (linear regression) kullanarak BTC piyasa deƒüerine g√∂re fiyatƒ±nƒ± tahminlemeye √ßalƒ±≈üacaƒüƒ±z. Doƒürusal baƒülanƒ±m iki veya daha √ßok deƒüi≈üken arasƒ±ndaki ili≈ükiyi anlamak i√ßin kullanƒ±lƒ±r. Mesela, elimizde insanlarƒ±n ya≈ülarƒ±nƒ± ve maa≈ülarƒ±nƒ± g√∂steren bir veri var. Bu veri k√ºmesindeki her bir ki≈üi i√ßin ya≈üƒ± ve maa≈üƒ± g√∂steren veri noktalarƒ±mƒ±z var: (50, 5000TL), (25, 3000TL) gibi. Bu veri noktalarƒ±nƒ± a≈üaƒüƒ±daki gibi bir grafik √ºst√ºnde g√∂sterildiƒüini d√º≈ü√ºnelim. Doƒürusal baƒülanƒ±m bu veri noktalarƒ±nƒ± en iyi ≈üekilde kesen a≈üaƒüƒ±daki doƒüruyu bulmamƒ±zƒ± saƒülar. Daha sonra bu doƒüruyu kullanarak elimizdeki veri k√ºmesinde bulunmayan bir veri i√ßin tahminleme yapabiliriz. Mesela 40 ya≈üƒ±ndaki bir ki≈üinini maa≈üƒ±nƒ± tahminleyebiliriz.
Arama kutucuƒüunu ‚Äúlinear regression‚Äù yazarak bile≈üeni bulun ve √ßalƒ±≈üma alanƒ±na s√ºr√ºkleyip bƒ±rakƒ±n. ƒ∞sterseniz bile≈üeni se√ßip saƒü taraftan ayarlarƒ±nƒ± deƒüi≈ütirebilirsiniz bu √ßalƒ±≈üma i√ßin default deƒüerleri kullanacaƒüƒ±z.
Bu algoritma ile model eƒüitim yapmak i√ßin ‚ÄúTrain Model‚Äù bile≈üeni eklememiz gerekiyor. A≈üaƒüƒ±daki gibi ekleyerek baƒülantƒ±larƒ± yapƒ±n. ‚ÄúSplit Data‚Äù bile≈üeninin altƒ±ndaki sol nokta eƒüitim verisini temsil ediyor, bu noktayƒ± ‚ÄúTrain Model‚Äù bile≈üenine baƒülayƒ±n.
‚ÄúTrain Model‚Äù . bile≈üenini se√ßerek saƒü taraftan eƒüitim kolonunu belirleyin, bu i≈ülem i√ßin fiyat kolonunu a≈üaƒüƒ±daki gibi saƒüdaki men√ºden se√ßin.
≈ûimdi ilk eƒüitimimizi yapacaƒüƒ±z, baƒülantƒ±larƒ± yaptƒ±ktan ve kolonu se√ßtikten sonra ‚ÄúTrain Modeli‚Äùi se√ßerek √ßalƒ±≈ütƒ±rƒ±n ve adƒ±m adƒ±m t√ºm bile≈üenlerinizin √ßalƒ±≈ümasƒ±nƒ± izleyin. Bu a≈üamada hepsinde ye≈üil tik i≈üaretini g√∂receksiniz.
Train Model bile≈üenine saƒü tƒ±klayarak model √∂zelliklerine g√∂z atƒ±n.
Eƒüitilen modeli test verisi √ºzerinde denemek i√ßin ‚ÄúScore Model‚Äù bile≈üeni ekleyin. A≈üaƒüƒ±daki gibi ‚ÄúTrain Model‚Äù bile≈üenine ve ‚ÄúSplit Data‚Äù bile≈üeninden √ßƒ±kan test veri k√ºmesine baƒülayƒ±n.
‚ÄúScore Model‚Äù bile≈üeni tahmin verilerini √ºretmek i√ßin kullanƒ±lmaktadƒ±r. Sƒ±nƒ±flandƒ±rma modelleri i√ßin sƒ±nƒ±f √ßƒ±ktƒ±sƒ± verirken, baƒülanƒ±m modelleri i√ßin tahmin edilen sayƒ±yƒ± √ºretir.
‚ÄúScore Model‚Äù bile≈üenini √ßalƒ±≈ütƒ±ralƒ±m ve g√∂r√ºnt√ºleyelim.
G√∂r√ºld√ºƒü√º gibi test veri k√ºmesine yeni bir kolon (Scored Labels) daha eklendi. Burada modelin o g√ºn i√ßin yaptƒ±ƒüƒ± fiyat tahmini bulunuyor. Bu veri k√ºmesini inceleyerek ve 2. kolon ile kar≈üƒ±la≈ütƒ±rarak modelin fiyat tahminlerini g√∂zden ge√ßirebilirsiniz.
Modelin √ºrettiƒüi fiyatlarƒ±n ne kadar tutarlƒ± olduƒüunu kontrol etmek i√ßin ise ‚ÄúEvaluate Model‚Äù bile≈üeni kullanƒ±lƒ±yor. Bu bile≈üeni de √ßalƒ±≈üma alanƒ±na ekleyin. Bu bile≈üeni de ‚ÄúScore Model‚Äù bile≈üenine a≈üaƒüƒ±daki gibi baƒülayƒ±n.
‚ÄúEvaluate Model‚Äù bile≈üenini √ßalƒ±≈ütƒ±rƒ±n, i≈ülem bittikten sonra bu bile≈üene saƒü tƒ±klayƒ±p ‚ÄúEvaluation Results‚Äù->‚ÄùVisualize‚Äùa tƒ±klayƒ±n ve modelin hata katsayƒ±larƒ±nƒ± ve hata histogramƒ±nƒ± inceleyin.
Bu deƒüerlerden hata (error) deƒüerleri modelin ne kadar yanƒ±ldƒ±ƒüƒ±nƒ± yani ger√ßek deƒüerlerle tahminlenen deƒüerler arasƒ±ndaki farklarƒ± g√∂sterir. Ortalama Mutlak Hata (Mean Absolute Error: MAE ) bu deƒüerler arasƒ± farklarƒ±n mutlak deƒüerlerinin ortalamasƒ±nƒ± g√∂sterirken, Karesel Ortalama Hata (Root Mean Squared Error: RMSE) ise bu farklarƒ±n karelerinin ortalamasƒ±nƒ±n karek√∂k√ºd√ºr. Bu hata deƒüerlerinin sƒ±fƒ±ra yakƒ±nla≈ümasƒ± modelin daha az hata yaptƒ±ƒüƒ±nƒ± g√∂sterir. Detaylar i√ßin ≈üurayƒ± inceleyebilirsiniz. Determinasyon katsayƒ±sƒ± (Coefficient of Determination) ise tahmin edilen deƒüerlerle ger√ßek deƒüerler arasƒ±ndaki korelasyonun karesidir. 0'a yakla≈ümasƒ± aradaki korelasyonun zayƒ±f olduƒüunu 1'e yakla≈ümasƒ± ise g√º√ßl√º olduƒüunu g√∂sterir.
Modelinizi bir API olarak dƒ±≈üarƒ± a√ßmak i√ßin ‚ÄúSet up Web Service‚Äù b√∂l√ºm√ºnden ‚ÄúPredictive Web Service‚Äùi se√ßin.
Azure ML Studio bu a≈üamada modelinizde bazƒ± otomatik deƒüi≈üiklikler yapacak ve yeni bir sekmede ‚ÄúPredictive Experiment‚Äù b√∂l√ºm√ºnde modelinizin son halini g√∂sterecektir.
Burada normalde webservis iki girdiyle √ßalƒ±≈üƒ±yordu yani hem fiyat hem piyasa deƒüerini alƒ±yordu. Bunu d√ºzeltmek i√ßin predictive experiment tabƒ±nda
‚ÄúSelect Columns..‚Äù altƒ±nda bir ‚ÄúSelect Columns‚Äù bile≈üeni daha ekledim ve sadece piyasadeƒüeri kolonunu se√ßtim daha sonra bunu ‚ÄúScore Model‚Äù bile≈üenine girdi olarak verdim. Bu sayede web servis sadece piyasaDegeri girdisiyle √ßalƒ±≈üƒ±r hale geldi.
D√ºzenlemeleri yaptƒ±ktan sonra bu b√∂l√ºmden modeli tekrar √ßalƒ±≈ütƒ±rƒ±n ve son kontrollerinizi yapƒ±n. Daha sonra a≈üaƒüƒ±dan ‚ÄúDeploy Web Service‚Äù i se√ßin. A√ßƒ±lan ekrandan API anahtarƒ±nƒ±zƒ± g√∂r√ºnt√ºleyebilir ve API test sayfasƒ±na eri≈üebilirsiniz.
API‚Äônizi test etmek i√ßin a≈üaƒüƒ±daki Test butonunu kullanabilirsiniz.
Buraya servisinizi besleyecek olan deƒüeri yani o g√ºn√ºn BTC piyasa deƒüerini girin ve butona basƒ±n.
Servis, modeli √ßalƒ±≈ütƒ±racak ve a≈üaƒüƒ±daki gibi BTC fiyat sonucu d√∂necektir. Bug√ºn BTC 8070 USD olacakmƒ±≈ü, yatƒ±rƒ±m tavsiyesi deƒüildir :)
Tebrikler artƒ±k makine √∂ƒürenmesiyle BTC fiyatlarƒ±nƒ± tahmin etmeye √ßalƒ±≈üan bir web servisiniz var. Piyasa deƒüeriyle BTC fiyatƒ± tahminlemek pek saƒülƒ±klƒ± bir y√∂ntem olmasa da bu rehberin basit olmasƒ± a√ßƒ±sƒ±ndan b√∂yle bir veri se√ßilmi≈ütir. Daha saƒülƒ±klƒ± bir model olu≈üturmak i√ßin indikat√∂rleri (RSI, MACD), tweet sentiment analizlerini eƒüitime dahil edip farklƒ± makine √∂ƒürenmesi algoritmalarƒ± kullanabilirsiniz.
Benim olu≈üturduƒüum st√ºdyo √∂rneƒüine buradan ula≈üƒ±p kopyalayabilirsiniz. √ñrneklerin payla≈üƒ±ldƒ±ƒüƒ± Azure yapay zeka galerisine buradan ula≈üabilirsiniz.
Not: Eƒüitim s√ºrecinde hata √ßƒ±karsa a≈üaƒüƒ±daki gibi kƒ±rmƒ±zƒ± √ßarpƒ± i≈üaretinin √ºzerine gelerek kontrol edebilirsiniz. A≈üaƒüƒ±daki hata, veri k√ºmesinde kolon ismini deƒüi≈ütirilip ‚Äútrain model‚Äù bile≈üeninde kolon adƒ± deƒüi≈ütirilmediƒüinde olu≈ümaktadƒ±r.
Azure ML galerideki √∂rnekleri incelemenizi ≈üiddetle tavsiye ederim. ƒ∞lgin√ß √∂rnekler var ve bunlarƒ± kendi st√ºdyonuza kopyalayƒ±p denemeler yapabiliyorsunuz.
Mesela;
Amerika se√ßimlerinde yatƒ±rƒ±mlarƒ±n hangi adaya gideceƒüini tahminleyebilir,
gallery.azure.ai
bir √ºr√ºn√º alan m√º≈üterilerin tweetlerinde duygu analizi yapabilir,
gallery.azure.ai
veya kanser te≈ühislerini tahminleyebilirsiniz.
gallery.azure.ai
Kaynaklar
https://api.blockchain.info/charts/market-price?timespan=1year&format=csv
https://api.blockchain.info/charts/market-cap?timespan=1year&format=csv
gallery.azure.ai
gallery.azure.ai
peter.intheazuresky.com
T√ºrkiye'nin En Etkili Yapay Zeka Topluluƒüu tarafƒ±ndan yapay‚Ä¶
610 
610¬†claps
610 
T√ºrkiye'nin En B√ºy√ºk Yapay Zeka Topluluƒüu tarafƒ±ndan yapay zekanƒ±n alt dallarƒ± olan makine √∂ƒürenmesi ve derin √∂ƒürenme alanlarƒ±ndaki T√ºrkiye'den √ºretilen i√ßerikleri bulabilirsiniz.
Written by
Articles about Deep Learning, iOS App Development, Entrepreneurship and Psychology
T√ºrkiye'nin En B√ºy√ºk Yapay Zeka Topluluƒüu tarafƒ±ndan yapay zekanƒ±n alt dallarƒ± olan makine √∂ƒürenmesi ve derin √∂ƒürenme alanlarƒ±ndaki T√ºrkiye'den √ºretilen i√ßerikleri bulabilirsiniz.
"
https://faun.pub/an-introduction-to-microsoft-azure-and-cloud-computing-54a7ebac0287?source=search_post---------74,"There are currently no responses for this story.
Be the first to respond.
You have heard lots of things about the cloud. Let‚Äôs understand each component of the cloud on this story. Also, let‚Äôs understand how the organizations are implementing various cloud models using Microsoft Azure. Before going‚Ä¶
"
https://medium.com/the-lark/azure-one-662dca9c9f7?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Floating on the waves of the ocean,the sun and the sky were above me.Burnt orange in the eyes of the shadow,pulsating into a flow of crimson red
The sky and the ocean, and the horizonin between, fading afar as I was drifting.For so long, lost in the flow of movement,wishing to‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/an-ambiverts-guide-to-azure-functions-95931976c565?source=search_post---------76,"There are currently no responses for this story.
Be the first to respond.
The following blog post will walk you through how to use Azure Functions, Twilio, and a Flic Button to create an app to trigger calls/texts to your phone. If you‚Äôre looking for a quick summary and overview on Azure Functions, I recommend starting here with our documentation, or taking 4 minutes to read in more detail how to create your first Azure Function, so you have some context on how to use functions within Azure.
If you‚Äôd like to skip straight to the code, scroll down to the Let‚Äôs get to the code!‚Äã section below.
Happy learning!
-Chloe
Hello everyone, my name is Chloe, and I‚Äôm an ambivert üëã
As a former actress (and current developer advocate), people often assume I‚Äôm an extrovert. The big secret is‚Ä¶ I‚Äôm not! I‚Äôm actually an introverted extrovert (also known as an ambivert). Day-to-day, that means I have no problem being on stage and giving a talk at a conference, socializing with attendees, doing a couple on-camera interviews, and sticking around for the schmoozing at a happy hour‚Ä¶ in fact, I love it! But afterwards I will escape to my hotel room and watch TV under a blanket for the proceeding 3 hours because I have to ‚Äúrecharge.‚Äù‚Äã
So, here‚Äôs the definition of ambivert:‚Äã
‚ÄãA person whose personality has a balance of extrovert and introvert features.‚Äã‚Äã
The best way (not so eloquent way) to describe my ambivert features would be that I equally love and hate being around other humans (only half kidding!).‚Äã I love to socialize- conferences, meet-ups, and coffee chats are right up my alley (in fact, it‚Äôs a big part of my job!)‚Ä¶. but only half of the time. I have to recharge afterwards, or even schedule a couple nights at home to counter my social interaction. I‚Äôm likely very chatty at a dinner, but opt out of the evening karaoke (and trust me- ya girl LOVES karaoke).‚Äã‚Äã
So, what I‚Äôm trying to say is, if you ever see me sticking around at a conference happy hour, I‚Äôve either had espresso later in the afternoon, I‚Äôm jetlagged, or I‚Äôm secretly dying on the inside.‚Äã
Remember‚Ä¶ I have a theatre degree. I‚Äôm very good at convincing people I‚Äôm not internally screaming inside üòê. I see my extroverted-ness almost like a button I can turn on and off depending on the situation‚Ä¶ which is a great segue into the device I‚Äôll be using to showcase how I built this.
Learning and creating ways to navigate my ambivert-ness ‚Äúin the wild‚Äù at conferences, meet-ups, and meetings has been an interesting process!‚Äã‚Äã Today, in this post, I‚Äôd like to show you one of those hacks I‚Äôve created for myself‚Ä¶ and hopefully, if you‚Äôre an introvert (or ambivert, or a human/robot looking to build cool things) you can try this demo yourself and code your way out of awkward or undesirable social situations!‚Äã
When deciding to take on this project, I considered several different devices to build my project with.‚Äã‚Äã At first, I pictured it as a wearable.‚Äã I reached out to Sara Chipps and Jennifer Wadella to see if Jewelbots‚Äô capabilities would work for my use case. Unfortunately, while very fashionable and fun, the range of the devices is limited (but if you have a young girl in your life- get them one of these. Very cool, fun way to learn programming!). I also considered FitBit as an option, but had concerns over folks thinking I was triggering my functions through it right in front of them. I needed something that was small, subtle and able to fit in my pocket.
So, the winner is‚Ä¶ a Flic button! What‚Äôs a Flic button? Great question! It‚Äôs a small button (about the size of a quarter) that acts as a Bluetooth Remote for iOS and Android. You can program these buttons to do anything from turning on Smart lights in your home, controlling music, taking photos, sending texts, making calls, posting tweets, calling Ubers, etc.
Flic has many pre-built integrations and apps you can use within their app, ranging from sending a tweet to triggering a MP3 to play a laugh track (I may or may not have set up this specific example to bug my boyfriend in our apartment for when I tell bad jokes). Suz Hinton sent me a Flic button for our mentorship sessions a few months back for an Azure exercise, and once I learned how to connect it to an Azure Function, I knew I had to build something fun with it.‚Äã While the Flic button does include a pre-built ‚Äúfake call‚Äù feature in it‚Äôs app, it doesn‚Äôt actually create a call (you use a screenshot of a call, and it plays a ringtone sound). This is why using the Twilio API was necessary for this project, and how this blog post was born.
If you‚Äôre unfamiliar, Azure Functions is a serverless compute service that enables you to run code on-demand without having to explicitly provision or manage infrastructure. Not only can you can use Azure Functions to run a script or piece of code in response to a variety of events, but it also lets you execute your code in a serverless environment without having to first create a VM or publish a web application.‚Äã
You can trigger the execution of Azure Functions in a variety of ways. Here are 5 common ones‚Äã:
‚òùüèªHTTP (which is what I‚Äôll be talking about today)‚Äã
‚úåüèª Timers (example: every day at 11am, execute this function, that starts + checks the temperature of my sous vide)‚Äã
üëåüèª You can create a function triggered when data is added to or changed in Azure Cosmos DB‚Äã
üññüèª You can create a function triggered when files are uploaded to or updated in Azure Blob storage
üñêüèª You can create a function that is triggered when messages are submitted to an Azure Storage queue
And many more! For a complete list, tutorials, documentation, and additional details of the capabilities of Azure Functions, start with the Azure Functions Documentation.
If you haven‚Äôt played much with serverless/Azure Functions, I recommend starting with reading this Azure Functions Overview and completing this Create Serverless Logic with Azure Functions module to get a better idea of how all of these pieces fit together before diving into programming your Flic button. The Microsoft docs are a great place to get free resources and lessons on how to get started!
Speaking of great documentation/getting started, I also used Twilio to create this. Twilio allows software developers to programmatically make and receive phone calls, send and receive text messages, and perform other communication functions using its web service APIs.‚Äã Their walk-through/demo code has a special Rick Astley Easter Egg that I will show you in a bit since I kept it in my demo üôÉ
Alright- let‚Äôs review our goals!
-Texting my friends an SOS message to save me‚Äã
-Triggering a call from my ‚Äúboyfriend‚Äù*
‚ÄãFor the sake of easy to understand visuals/screenshots I used the Azure portal to create this. You can also use VS Code, the Azure CLI, etc.‚Äã With Azure Functions you are given the the ability to code and test Functions locally on your machine without having to deploy to the cloud every single time you want to test (a huge time saver!).
To create an Azure function, you can just start from the Get Started menu and select (surprise!) Function App.
Then you‚Äôll need to fill in some basic info about your function here. Including the app name, the Azure subscription you‚Äôd like to use, a resource group (I‚Äôm creating a new one in this case), the Operating System you‚Äôd like to use, the hosting plan (I‚Äôm using consumption), the location I‚Äôd like to use (I‚Äôm in California, so West US 2 is usually my default), the runtime stack I‚Äôd like to use (I‚Äôm using JavaScript in this case), and I have the option to create new storage or use existing. I created a new one in this case.‚Äã‚Äã
Once I have all these filled out, I can go ahead and deploy! Wait about a minute or two, then watch for the Deployment succeeded message.‚Äã
‚Äã
Woo! If you followed those steps, we have our resource! We‚Äôll just select ‚ÄúGo to resource‚Äù to view your new Function App.‚Äã Now we‚Äôll add a new function
It typically takes about a minute to deploy and then we‚Äôll have a fresh new Azure Function waiting to be called. The default code is a simple hello world app, where if you paste the function URL into your browser‚Äôs address bar. Add the query string value &name=<yourname> to the end of this URL and press the Enter key on your keyboard to execute the request. You should see the response returned by the function displayed in the browser.‚Äã
Cool! So, we see this works now. Let‚Äôs get to the fun part‚Ä¶
My boyfriend Ty Smith works full-time as an Android Developer at Uber, and is an Android GDG and GDE, and also travels for conferences as well. Needless to say, he‚Äôs a busy guy and I didn‚Äôt want my app to call him, because maybe he‚Äôd be in a meeting/at dinner/playing the new Resident Evil game, and I wouldn‚Äôt want to disturb him (also, testing this would have been a bit of a nightmare- example can be seen in this Twitter thread).
‚ÄãSo, everyone, please meet my new fake boyfriend Twilio Smith- he‚Äôs a Twilio # that I purchased (with a Texas area code üåµü§†).‚Äã
After reviewing the Twilio API docs, I was able to get up and running pretty quickly with some sample code (shout-out to Twilio for the excellent documentation!).‚Äã
I have 2 Azure functions I needed to create and call. One for the call, and one for the texts. Please note: it‚Äôs okay to hardcode your Twilio credentials when getting started, but you should use environment variables to keep them secret before deploying to production. Check out Frank Boucher‚Äôs video on How to use Environment Variables in Azure Functions for a great 5 minute tutorial!
You‚Äôll probably notice that this function sends a text to me vs to friends/coworkers at a conference. For the sake of this demo, I‚Äôve made it so the code texts me so I can show this off in-person when I demo this on stage (plus, you‚Äôll annoy less folks with test texts while debugging‚Ä¶ again, you can learn more about that in this Twitter thread üò¨ü§¶‚Äç‚ôÄÔ∏è). But obviously, you‚Äôd replace these numbers with the numbers of your friends you wish to alert.
The code for our phone call trigger is pretty similar except we‚Äôre making a call, not a text. You‚Äôll also notice that I‚Äôm linking to something here.‚Äã.. let‚Äôs take a look at what that link is hosting.
As I mentioned earlier, one of the reasons I decided to use Twilio was to be able to have a real call come in on my phone. Twilio also gives us the capability to use TwiML to compose voice messages, as well as do things such as, oh, I don‚Äôt know‚Ä¶ play an MP3 of Rick Astley perhaps? Obviously, you can record your own voice message MP3 (I‚Äôve included several samples of my own voice as your cousin/partner/friend in the repo). You can take a look at Microsoft‚Äôs documentation on how to use Twilio for voice and SMS capabilities from Azure if you‚Äôd like to dive deeper into TwiML, or have more questions about configuring your Application to use Twilio libraries.
Now we can incorporate our Flic button. Here‚Äôs what the Flic app looks like (left). For the sake of time, I won‚Äôt walk through every step, but essentially you just add the URL of the Azure Function and click save. Flic‚Äôs app is very straightforward, and will require a simple copy/paste of the https link we created with our 2 Azure Functions.
‚Äã
‚Äã
Last, but certainly not least, I needed to add my fake boyfriend to my contacts (complete with an image) so it would look more legitimate when a call came through. Otherwise this would show up in my phone as an unknown #. So‚Ä¶. shall we go ahead and test it out?‚Äã‚Äã
As I mentioned before, I wanted to configure one of my Functions to text other people (for it‚Äôs actual use case), but I can‚Äôt really demonstrate/test that well on my own. So with this demo, my fake boyfriend is going to be texting me.
‚Äã‚Äã
So, that‚Äôs the app! As you can see, it‚Äôs pretty easy to get up and running with Azure Functions! If you‚Äôd like more instructions on how to deploy to Azure, check out the GitHub repo here. There are so many easy ways to deploy to Azure, and you can read about them in more detail in our docs.
Using simple Azure Functions just like this can open the door for a plethora of automation in your applications or even your personal life. Anything from a button for your children to press when they get home from school (to alert the bus dropped them off safely), even starting a tea kettle in the morning so your tea is ready to go while you‚Äôre groggily getting ready for work, or creating a function to check a database in your app on a timed schedule. This particular Twilio demo was created just for fun, but think about how using Azure Functions in your applications or everyday tasks could automate things for you!
So, what‚Äôs next for this project?‚Äã Well, I‚Äôd love to add a couple more features‚Äã- please check out the repo on Github if you‚Äôre interested in contributing your own features and ideas! Here are a couple that folks have suggested on Twitter:
If you‚Äôve read this far- congrats! You‚Äôve successfully learned how to get yourself out of awkward social situations using technology. If you‚Äôd like to dive deeper into any of these topics, here are some great places to get started:
Azure Functions Documentation ‚Äî a great starting point for beginners which includes 5 minute Quickstarts‚Äã to create functions that execute based on events created via:
Have any questions? Comment below, or shoot me a message on Twitter!
‚Äã
Any language.
306 
1
Thanks to Suz Hinton and chanezon.¬†
306¬†claps
306 
1
Written by
Musical theatre actress turned developer evangelist.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Musical theatre actress turned developer evangelist.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@onmyway133/machine-learning-in-ios-azure-custom-vision-and-coreml-645e93f35eee?source=search_post---------77,"Sign in
There are currently no responses for this story.
Be the first to respond.
Khoa Pham
May 23, 2018¬∑6 min read
This is a Part 2 of my Machine Learning in iOS tutorials, check Part 1 first.
You may have read my previous article about Machine Learning in iOS: IBM Watson and CoreML. So you know that Machine Learning can be intimidating with lots of concepts and frameworks to learn, not to mention that we need to understand algorithms in Python.
Talking about image labelling, you might have a bunch of images and you want to train the machine to understand and classify them. Training your own custom deep learning models can be challenging. The easiest approach is to start with the easy step and to use some cloud services, so that you don‚Äôt get discouraged at first. Imagine that the code to train the model is already written for you, all you have to do is to tag related images, and get the trained result.
In this tutorial we will take a look at Azure Custom Vision service by Microsoft, that allows us to build custom image classifier.
Custom Vision is a service that allows us to build custom image classifier, firstly announced at MSBuild 2017, you can watch the keynote here. It is one of many services in Cognitive Services in the AI + Machine Learning product in Azure. Other services include speech, language, search, bot, etc.
The Custom Vision Service is a Microsoft Cognitive Service that lets you build custom image classifiers. It makes it easy and fast to build, deploy, and improve an image classifier. The Custom Vision Service provides a REST API and a web interface to upload your images and train the classifier.
To begin your work, go to Custom Vision home page and click Get Started. You should have an Azure account and if not, just register here for free. The free tier is enough for us to get started using the service, with 2 projects and 5000 training images per project.
The usage is extremely easy. Here is the dashboard for Custom Vision, I can‚Äôt expect it to be simpler. Follow me with the next steps.
Create a new project called Avengers. For laziness and for easy compare with other cloud services, here we use the same dataset from the post Machine Learning in iOS: IBM Watson and CoreML. Just to recap: last time we made an app that recognised superheroes. And since last post, people request that they want to see more superheroes ‚ù§Ô∏è
Note that in the Domain section, you need to select General (compact). As this produces lightweight model that can be consumed in mobile, meaning that the trained model can be exported to .mlmodel, the format supported by CoreML.
Click Add images and select images for each superhero. Name a proper tag.
About data set, this What does Custom Vision Service do well? says that:
Few images are required to create a classifier or detector. 50 images per class are enough to start your prototype. The methods Custom Vision Service uses are robust to differences, which allows you to start prototyping with so little data. The means Custom Vision Service is not well suited to scenarios where you want to detect subtle differences. For example, minor cracks or dents in quality assurance scenarios.
So our images can be sufficient for this tutorial.
Click Train to start the training process. It shouldn‚Äôt take a long time as Custom Vision uses transfer learning.
Azure allows us to use Prediction API to perform prediction based on our trained model. But in this case, we just want to get the trained model to embed in our iOS apps to run offline. So click Export and select CoreML.
We use the same project as in the Machine Learning in iOS: IBM Watson and CoreML. The project is on GitHub, we use CoreML and Vision framework to perform prediction based our trained model.
We name the model to AzureCustomVision.mlmodel and add it to our project. Xcode 9 can autogenerate a class for it, so we get the class AzureCustomVision. Then we can construct Vision compatible model VNCoreMLModel and request VNCoreMLRequest, and finally send the request to VNImageRequestHandler. The code is pretty straightforward:
Build and run the app. Select your superheroes and let the app tell you who he/she is. Our dataset is not that big, but you can see the model predicts pretty well with very high confidence. For now, we have images for only 4 superheroes, but you can add many more depending on your need.
We have covered IBM Watson and Microsoft Azure Custom Vision. There are other cloud services that worth checking out. They can be as easy as uploading images and training, or more advanced with custom TensorFlow code execution or complex rules.
This post Comparing Machine Learning (ML) Services from Various Cloud ML Service Providers gives a lot of insight into some popular machine learning cloud services together with sample code, worth taking a look too.
Here are some links to get started with using cloud services, especially Azure Custom Vision:
Check out my apps https://onmyway133.com/apps
932 
932¬†
932 
Check out my apps https://onmyway133.com/apps
"
https://blog.aion.network/microsoft-azure-lists-aionnode-a563181b0fbd?source=search_post---------78,"¬© 2022 The Open Application Network. All rights reserved.
"
https://medium.com/@anthonypjshaw/azure-pipelines-with-python-by-example-aa65f4070634?source=search_post---------79,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anthony Shaw
Jan 2, 2019¬∑7 min read
In this tutorial, I‚Äôll show you -by example- how to use Azure Pipelines to automate the testing, validation, and publishing of your Python projects.
Azure Pipelines is a cloud service that supports many environments, languages, and tools. It is configured via a master azure-pipelines.yml YAML file within your project.
Your project can exist in multiple source repositories, and automated integrations are available for GitHub.
If you want to see the project layouts and how the configuration looks, everything in this tutorial is on my GitHub repository. It‚Äôs licensed as MIT, so you can do with it as you wish!
github.com
A build can have multiple jobs. You might want to segment your jobs by tasks like:
In these examples, I‚Äôve focused on testing. Because Python is an interpreted language, the build stage is typically replaced with dynamic and static checkers, known as ‚Äòlinters‚Äô.
Having automated tests for any Python project can give you fast notifications when a change to the code has broken an existing feature in your application.
Your azure pipelines build will consist of jobs, these jobs consist of a number of steps. Some steps are pre-defined and called ‚ÄúTasks‚Äù, you can find the full list on the Microsoft Azure Pipelines website.
You can also define your own tasks consisting of scripts in either Bash (Linux and MacOS) or PowerShell (Windows).
Some of the pre-defined tasks you‚Äôll need for Python are:
UsePythonVersion@0 ‚Äî Use a specific Python version (like a virtual environment)
PublishTestResults@2 ‚Äî Publish the test results to the dashboard. Test results should be in the Junit XML format.
When configuring a job, you have to pick which Virtual Machine image to use for the job agent to execute your steps. For Linux, there are many images to choose from, Ubuntu 16 has the latest versions of Python available.
To test on Windows, the best image to choose is the vs2017-win2016 image, which has Visual Studio 2017 on Windows Server 2016.
Some of the other images include:
Because steps are defined in a job, and a job can only have 1 operating system it can get very tedious to copy steps between jobs and keep the up to date.
If your build and test steps are the same across Linux and Windows, you can move the steps into a separate file (in this example, called templates/steps.yml) and include it in the job.
Once you‚Äôve configured your testing jobs to output a Junit XML file and you‚Äôve successfully configured the PublishTestResults task, you will see a detailed test dashboard. All test failures will be available here. In the examples, I‚Äôve named the Test Results based on the operating system, job name and the version of Python so it‚Äôs easier to see what‚Äôs happening. You can add any variable or label to the test names.
To test a Django application, there are 4 basic steps you need to add to a build job:
This is important as Django can behave very differently between Python versions. This example tests Python 3.6 and 3.7. You can change these for other versions, like 3.5.
The installation of Django would be via. pip . This script task updates the pip binary and then installs any dependencies in django-basic/requirements.txt, which is the example Django Project I‚Äôve set up for this tutorial.
To test a Django application you can use the python manage.py test command, or use another tool like Pytest.
Because Azure Pipelines test output can be viewed and sorted online, it‚Äôs beneficial to use the Junit XML output. Pytest is the easiest way to generate this. The pytest-django package is a plugin for Pytest that will work with your existing Django tests. You only need to make a small change. Create a pytest.ini file in your project directory with these settings:
The task itself brings together these tools to test an application:
Lastly, use the builtin task to copy the test output to the Azure Pipelines service.
Here is the azure-pipelines.yml for the example Django app which can be found on GitHub.
If you want to test your Django application against multiple versions of Django, or a particular plugin, you can add extra version numbers as variables in the build matrix.
In this example, I‚Äôm testing 2 versions of Django across 2 versions of Python. This generates multiple jobs automatically.
Because there is no built-in task for selecting the version of Django, we use the variable to control the version installed in pip.
This script task adds the django.version variable defined in the matrix to the pip install command.
Here is the final azure-pipelines.yml for the matrixed Django test
If you‚Äôre using the Flask web framework, the steps are very similar to Django, but it doesn‚Äôt require a Pytest plugin.
In the example repository, I‚Äôve simply copied the Flask example web application which comes with tests. To use this on Azure Pipelines, you need to run the setup.py command using pip install -e . from the application directory. Adding the [test] suffix will install the testing tools.
If you wanted to calculate the test coverage of your application, you combine the testing task with the coverage package.
For the flask-basic example project, you install both Pytest and coverage, then run pytest through the coverage run command.
You will be able to view the coverage data in the Azure Pipelines portal.
If you‚Äôre building Python libraries for distribution via shared files, PyPi or another artifact system, this example illustrates how to build source and binary distributions on Azure Pipelines.
Aside from the test commands that have been shown throughout this tutorial, we need to add a couple of extra tasks.
Azure Pipelines comes with an artifact publishing, hosting and indexing API that you can use through the tasks. You can also see the artifacts from a build in the web interface.
In the example project, I‚Äôve created a really simple Python package, with a setup.py and setuptools configured.
To install the dependencies, this time we‚Äôll install the package using pip install -e . once we‚Äôve changed to the source directory.
Now, to test the library we can use Pytest again to generate the Junit XML output:
Then, after the test results have been published, the sdist (source distribution, a tar.gz copy of the source files, and a binary wheel can be built and published in the artifact repository using the CopyFiles and PublishBuildArtifacts tasks:
Azure Pipelines has support for publishing the packages to PyPi, but I haven‚Äôt added that to my examples. It‚Äôs well documented on the website.
If you‚Äôre using pyproject.toml as your configuration file and using Flit to publish and build packages, there are a few changes to make to the previous example.
Finally, if you want to download the artifacts and install them as a final verification, you can add an additional job like this:
Some other pointers I learned whilst writing these examples:
Group Director of Talent at Dimension Data, father, Christian, Python Software Foundation Fellow, Apache Foundation Member.
183 
2
183¬†claps
183 
2
Group Director of Talent at Dimension Data, father, Christian, Python Software Foundation Fellow, Apache Foundation Member.
"
https://medium.com/swlh/lets-do-devops-bootstrap-aws-to-your-terraform-ci-cd-azure-devops-github-actions-etc-b3cc5a636dce?source=search_post---------80,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
Pairing Terraform with a CI/CD like Azure DevOps, Terraform Cloud, or GitHub Actions can be incredibly empowering. Your team can work on code simultaneously, check it into a central repo, and once code is approved it can be pushed out by your CI/CD and turned into resources in the cloud.
"
https://medium.com/flutter-community/nlp-chat-on-flutter-azure-676aa4768fbb?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
At this point Flutter has alot of momentum and works really well to build cross-platform apps quickly and has web support as well. On the downside the community & 3rd party support is in its infancy and the platform is changing fast. Obvious choice to use the newest and coolest tech now that its past Beta.
I wanted realtime push of messages rather than polling. Unfortunately once you choose Flutter, it has a ripple effect on what push hubs you can use. Flutter aligns with Googles Firebase and has near native support for Flutter Cloud Messaging (FCM). Integration with Azure Events does not seem straightforward hence the quick path is to use Firebase for both Events and Storage.[11]
Given Flutter and Firebase use, the natural choice is to use Google Cloud Platform rather than Azure for all hosting. I chose Azure for personal reasons ‚Äî being more experienced on it, and understanding the Azure Machine Learning environment better. I am going very ‚Äúpolyglot‚Äù and going with a Python Flask REST to host Python ML models ‚Äî very cost effectiveness of way to host services on Azure (esp if you bundle onto an existing web app ‚Äî plan B is to use Azure Functions).
There are various NLP Sentiment Analysis [2,3,4] models you can implement. The cutting edge at this point is Google‚Äôs BERT and its variants[6,7]. Microsoft has released Azure Compute compatible pre-trained models which we can fine tune [8]. It maybe worth building a few models, starting with simple ones so we can measure differences/improvements.
This was a very brief article to kickoff my new project and explain how we make some basic design decisions up front. Look for Part 2 code + walkthru as I build it soon!
References and Inspirations:
[1] Flutter Overview ‚Äî https://medium.com/swlh/why-businesses-should-start-focusing-on-googles-flutter-and-fuchsia-48e16820f2a9
[2] Nice Overview of NLP w/ code‚Äî https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72
[3] Guide to word embeddings ‚Äî https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec
[4] More on Sentiment Analysis ‚Äî https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17
[5] Azure already has a Sentiment service ‚Äî https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/?WT.mc_id=blog-medium-abornst
[6] BERT primer ‚Äî https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
[7] More on BERT ‚Äî https://medium.com/sciforce/googles-bert-changing-the-nlp-landscape-5f4a7bf65cc5
[8] Azure and BERT ‚Äî https://azure.microsoft.com/en-us/blog/microsoft-makes-it-easier-to-build-popular-language-representation-model-bert-at-large-scale/
[9] Github details on pre-training and fine tuning BERT ‚Äî https://github.com/google-research/bert
[10] Github for Azure BERT implementations ‚Äî https://github.com/microsoft/AzureML-BERT
[11] Flutter and Firebase integration ‚Äî https://medium.com/flutterpub/enabling-firebase-cloud-messaging-push-notifications-with-flutter-39b08f2ed723
Https://www.twitter.com/FlutterComm
Articles and Stories from the Flutter Community
181 
181¬†claps
181 
Articles and Stories from the Flutter Community
Written by
Tech Manager by Day, ML Hacker by Night ‚Äî founder: foostack.ai
Articles and Stories from the Flutter Community
"
https://medium.com/@maarten.goet/azure-sentinel-design-considerations-492f87fae384?source=search_post---------82,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 18, 2019¬∑11 min read
I‚Äôve written about Azure Sentinel before and how cloud SIEM‚Äôs are changing the security landscape. Microsoft provides Azure Sentinel as-a-service, which you can enable with the click of a button, only paying for the storage you use.
However, Azure Sentinel, as with any cloud services and/or SIEM, still needs some design considerations if you are putting it into production. What are these considerations? And what are the options available to me and my company?
In this article I‚Äôll show you a couple of things to consider when designing for Azure Sentinel. From foundational choices, to identity & access, to (data) connections and dashboarding; I‚Äôll share some real-world experiences.
Let‚Äôs look at the foundation first
Before we start, let‚Äôs make sure we are on the same page first and understand the fundamentals. Azure Sentinel uses a Log Analytics workspace as its backend, storing events and other information. Log Analytics workspaces are the same technology as Azure Data Explorer uses for its storage. These backends are ultra-scalable, and you can get back results in seconds using the Kusto Query Language (KQL).
The first thing to plan for is the Log Analytics workspace we‚Äôll be using. When setting up Azure Sentinel for the first time, it allows you to create a new Log Analytics workspace or to pick an existing one.
DESIGN CONSIDERATION: New or existing Log Analytics workspace?
Let‚Äôs look at why would you want to re-use an existing workspace. Of course, it would be the easy way; it is already there, you‚Äôve set up the right access to it, data is already streaming in and you can just add Azure Sentinel to it. No problem, right?
Well, access control is particularly one of the bigger reasons to potentially create a new Log Analytics workspace. That allows you to tightly control who has access to that aggregated data in Azure Sentinel, which often is a CISO requirement as we‚Äôll be discussing below.
Apart from access control reasons, you might also run into a technical challenge that forces you to create a new workspace; it is relatively hard to move an existing Log Analytics environment over to another subscription. You need to first offboard agents, remove current Solutions, before you can move it. And that might cause ‚Äòdowntime‚Äô for the monitoring solution currently using that workspace.
And of course, the last reason would be that sometimes you‚Äôve created a bit of history in your current environment; experimented with settings, have a name for your workspace that you‚Äôd like to change etcetera, so you might want to start with a ‚Äòclean slate‚Äô because of that.
DESIGN CONSIDERATION: How long do we need to store our data?
One other thing to consider is how long you will want to store the data. The default setting will be 31 days. However, you can change this workspace setting and extend to up to two years. As per the documentation:
‚ÄúThe retention period of collected data stored in the database depends on the selected pricing plan. By default, collected data is available for 31 days by default, but can be extended to 730 days. Data is stored encrypted at rest in Azure storage, to ensure data confidentiality, and the data is replicated within the local region using locally redundant storage (LRS). The last two weeks of data are also stored in SSD-based cache and this cache is encrypted.‚Äù
No, Azure Sentinel will NOT replace Azure Security Center
An often-heard remark is: ‚ÄúOh, so Azure Sentinel will replace Azure Security Center.‚Äù No, no no. Azure Security Center has its own place in the security landscape. It acts as the primary ‚Äòengine‚Äô to perform detections on Microsoft Azure, in your VM‚Äôs, in containers and on other properties such as Azure Stack, your on-premises infrastructure, etcetera.
Want to detect crypto miners in your Linux VM on Azure? Enable Azure Security Center. Want to get best practices and insights on securing your network in Azure? Enable Azure Security Center.
However, if you want to coordinate your security operations centrally, and aggregate multiple security solutions, such as Azure Security Center, Microsoft‚Äôs Cloud App Security, Azure ATP and others, you will want to enable Azure Sentinel.
By connecting all these data sources, you can start building a single pane of glass, and have one point of entry for your responders when they need to go threat hunting.
DESIGN CONSIDERATION: Which other security solutions will I be enabling alongside Azure Sentinel?
The identity and access piece is important
As pointed out above, often the CISO office will require you to tightly control who has access to that aggregated data in Azure Sentinel, because it could contain personal identifiable (PII) data. Normally only appointed security officers will be granted ‚Äòread access‚Äô.
DESIGN CONSIDERATION: Who needs access to the data in Azure Sentinel? Can we provide that access ‚Äòjust in time‚Äô to these people & roles?
Microsoft is in the process of adding RBAC features to Log Analytics workspaces as Oleg Ananiev, the group program manager for both Azure Monitor and Log Analytics, points out. This will then implicitly work for Azure Sentinel. More information can be found here.
As people come and go in a company, security offers will also likely be changing over time. We don‚Äôt want to grant access to a specific person but to the role he or she is fulfilling. We also don‚Äôt want to grant access all the time, but only when needed; for instance, when hunting for threats, or when a specific case was raised, and an investigation is opened.
This is where Azure Active Directory (AAD) Privileged Identity Management (PIM) can help. You can find more information on Azure AD PIM here. You will need either Azure AD P2 licenses or EM+S E5 licenses for those users you would like to use with Azure AD PIM.
Plan for the data connections
Azure Sentinel has a lot of possible data sources. Each and everyone of those needs a data connection and potentially a configuration.
DESIGN CONSIDERATION: Which data sources will I be connecting? What configuration does that data connection need?
I won‚Äôt be writing up each configuration of each possible data source. But I will provide you with a few ones that you need to think about, because there are things to know on how you can connect them:
.
.
How should I connect other SIEM systems?
Some enterprises will already have some sort of SIEM solution, like for instance ArcSight. And while I am NOT advocating that this is the preferred way of setting up cloud governance, your CISO office might want to hook up Azure Sentinel to that current system.
DESIGN CONSIDERATION: Will I be connecting Azure Sentinel to another SIEM solution?
If that is a requirement, you will want to consider using Azure Monitor and Event Hubs to forward your alerts to this other system. By using Event Hubs, you can do this safely and reliably; even when the receiving end is offline or malfunctioning, events get stored in the queue and Azure will release them when the system is back online.
If your system is not supported by Azure Monitor or Event Hubs, there still a fair chance to get it integrated with Azure Sentinel. There is a growing list of third parties that have built their own integrations on top of the API, that you can use. You can find the list here.
How can we support the threat hunters?
Up until now, we‚Äôve talked about getting data into Azure Sentinel. But after it gets processed and an alert gets raised, you will want to investigate. Your threat hunting colleagues need access to the data to understand what is going on.
DESIGN CONSIDERATION: What technology will the threat hunting colleagues be using? Do they prefer Jupyter? Will they require KQL access to the workspace?
One of the ways to do threat hunting is using the Kusto Query Language (KQL) and search through events quickly and easily in the workspace. They could use Azure Data Explorer, the ‚ÄòLogs‚Äô function of the Log Analytics workspace, a third-party application (such as Grafana) or the native Azure Sentinel UI in the Microsoft Azure portal.
That last option, going threat hunting from the Azure Sentinel portal UI, is a neat option. Microsoft provides you out of the box with pre-fab hunting queries and maps them back to the right Tactics category (fi: Initial Access, Lateral Movement, etcetera). Either way, consider what you would to do to provide them with the right UI, and access rights.
Another popular option among threat hunters is Jupyter. Microsoft has a free service based on Jupyter notebooks called Azure Notebooks. Through the ‚ÄòKqlmagic‚Äô extension, you can use Python to directly query the workspace using KQL queries. I‚Äôve wrote about that here. Consider if they will be using Jupyter locally (fi: in a docker container) or if they‚Äôll use Azure Notebooks. Also consider where you will be storing the notebooks; GitHub is a great option for that. And remember, Microsoft already provides you with many sample notebooks to get you jumpstarted.
Dashboards: how will we visualize the Azure Sentinel data?
Azure Sentinel provides a lot of out-of-the-box dashboards. Some of them are solution focused (Office 365), some are technically focused (Insecure Protocols) and some are geared towards third parties (F5, Palo Alto, etcetera).
Technically, these are JSON files that work in the Azure Dashboards section of your portal. You import them into your Tenant, and they will be available for everyone who can access that Tenant. Of course, you can restrict this with the built-in Azure access controls as they are just resources like any other.
Microsoft regularly updates it (GitHub) repository with new versions of the dashboards as they receive feedback from the field. You can manually update the JSON file in your Tenant or use the built-in functions in the Azure Sentinel UI. Either way, you should plan for some change management around this.
DESIGN CONSIDERATION: What are my requirements for visualizing Azure Sentinel data? How do I provide access to those programs and/or operators?
Another popular choice to visualize data from Azure Sentinel is to use open source visualization tools. Grafana is a great option, because it has a large ‚Äòstore‚Äô with visualization types (most of them free), and because Microsoft provides you with a native Log Analytics connector for Grafana.
With that connector, you can use Kusto (KQL) queries to get specific data from Azure Sentinel and map it onto one of Grafana‚Äôs visualizations. For instance, a world map with network connections, or a list of Alerts. Grafana has dashboarding features that most SOC‚Äôs will love, for instance the rotating dashboards. You will of course need to plan access from Grafana to Azure Sentinel‚Äôs data.
Escalation and notifications
All the above are technical design considerations. However, if Azure Sentinel will be powering your Security Operations Center (SOC), you will need to design your processes as well. How will your Alerts be followed up? Do we need a connection to our ticketing system? What if alerts & tickets stay open for too long? Are the right people, and potentially the management, informed in time (before breaching the SLA)?
DESIGN CONSIDERATION: What process do I need to run my Security Operations Center (SOC)? Which tools will support my Service Levels?
One of the options available to you out-of-the-box to automate the follow-up of alerts are Playbooks. In essence, these are Azure Logic Apps that can be triggered whenever a certain condition is met. For instance, an Alert with a high severity gets raised by Azure Sentinel, and you want to send this to a security engineer via text message. The logic app could contain code that connects to Twilio and sends the Alert description to a specified phone number.
But is this reliable enough? How do I know the security engineer has read it? What if he or she didn‚Äôt, and we need to escalate to the next engineer. Or worse yet, we‚Äôre approaching SLA times and we need to start informing management. This is where 3rd party solutions like SIGNL4 come in. There are a few out there, but SIGNL4 is great because it is a cloud service where you can set escalation paths, do two-way communication (to receive acknowledgement), use multiple channels (ersistent push, text and voice) and log the audit trail. They also support duty scheduling and have a 2-tier escalation model.
Key takeaways
The key takeaway from this article is that while Azure Sentinel is software-as-service, you should still plan for the implementation of the service. Gather your business / CISO requirements and consider for each subject what you should do. Also, don‚Äôt forget that it is not only a technical deployment, but you will need to plan for the process side as well.
Do you have other design considerations you are taking into account when deploying Azure Sentinel? Do you have real-world experience with Azure Sentinel? I would love to hear from you in the comments below.
Stay tuned for the next installment in my multi-part deep-dive series on Azure Sentinel!
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
79 
6
79¬†
79 
6
Microsoft MVP and Microsoft Regional Director.
"
https://medium.com/ontologynetwork/ontology-added-to-microsoft-azure-and-amazon-aws-marketplaces-92b3cb0593ec?source=search_post---------83,"There are currently no responses for this story.
Be the first to respond.
Ontology ONT_Dev_Platform has been added to the Microsoft Azure and Amazon AWS Marketplaces ‚Äî Microsoft and Amazon‚Äôs online software stores ‚Äî and is expected to enter Google‚Äôs in January 2019.
ONT_Dev_Platform is an Ontology blockchain dApp product built on a cloud service, which includes the Ontology stand-alone test environment, SmartX, and Block Explorer. It is available to users worldwide for free to deploy in just one click.
Got questions? Feel free to ask the Ontology development team and community in the #development channel in the Ontology Discord.
Are you a developer? Make sure you have joined our tech community on Discord. Also, take a look at the Developer Center on our website, there you can find developer tools, documentation, and more.
Ontology website / Ontology GitHub / ONTO website / OWallet (GitHub)
Telegram (English) / Discord
Twitter / Reddit / Facebook / LinkedIn
A high performance, open-source blockchain specializing in digital identity and data.
723 
723¬†claps
723 
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
"
https://medium.com/microsoftazure/moving-from-lambda-%C6%9B-to-azure-functions-b6d5ed5ca007?source=search_post---------84,"There are currently no responses for this story.
Be the first to respond.
‚ÄúIt‚Äôs time to migrate.‚Äù
Maybe the decision to switch cloud providers came ‚Äúdown from above.‚Äù Maybe the decision was yours. Perhaps your original solution was merely a spike to ‚Äúkick the tires‚Äù of one cloud, and now it‚Äôs time to try another. Whatever your reasons, if you are considering moving your serverless functions from AWS Lambda to Azure Functions, you‚Äôre in the right place to learn how!
‚ÄúMoving from Lambda to Azure Functions‚Äù is a six-part videos series that covers what you need to know to make the transition between cloud providers. You‚Äôll learn how to migrate your app, explore how resources in Azure relate to Amazon AWS, build a function locally, deploy it manually and learn how to push it automatically as part of a CI/CD pipeline.
The sample app is simple but does more than just echo text or print, ‚ÄúHello, world.‚Äù It computes whether or not a number is a prime and uses a cache to store the results to serve them quickly on subsequent calls. The AWS Lambda implementation uses JavaScript (Node.js) and Amazon DynamoDB.
You can view the source code for the sample app and deploy the migrated code directly to Azure with a single-click in the ‚ÄúAWSMigration‚Äù GitHub repository.
github.com
The first video provides and overview of the sample application and shows how to test and access it from the portal and the command line.
The next video walks through how to create an Azure Functions app: the solution for hosting serverless functions in Azure.
Learn how to migrate the code and move from using Amazon DynamoDB to Azure‚Äôs inexpensive and easy-to-use Azure Table Storage for the application cache.
After the app is migrated and deployed, review how Azure resources are organized and accessed compared to Amazon AWS.
Use the cross-platform Azure Functions Core Tools to create a local functions project and run it. Then, using free and cross-platform Visual Studio Code, build and debug a project in just a few short steps. After implementing the fully migrated function, deploy it to Azure directly from Visual Studio Code.
Friends don‚Äôt let friends right-click publish. That‚Äôs why in this final video we‚Äôll make your DevOps team proud by implementing continuous deployment. The function will also get a security lift in two areas: first, it will require authentication for access. Second, it will get assigned a managed identity to securely access other resources and assets.
This is a short series designed to ease your understanding of how to migrate from AWS Lambda to Azure. As always, we welcome your feedback, comments, and suggestions. If you have experienced a similar migration, please share your thoughts and tips in the comments below!
What‚Äôs next? Check out an Overview of Azure Functions.
Any language.
157 
1
157¬†claps
157 
1
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pytorch/efficient-serverless-deployment-of-pytorch-models-on-azure-dc9c2b6bfee7?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
Authored by: Gopi Kumar, Principal Program Manager at Microsoft. (@zenlytix)
Recent advances in deep learning and cloud-based infrastructure have led to innovations in models for various domains like natural language processing, computer vision, recommendations. Of course, developing the model is only half the story. Your models are mostly useful once they are served up for making predictions for consumption in in AI-driven scenarios from the end applications. It is important to do it in a cost-effective and reliable manner. However, managing infrastructure for hosting your models is challenging as it involves several aspects like maintaining your fleet, ensuring reliability, scaling, security and ongoing monitoring and management. Can we leverage serverless technologies for our model hosting?
Azure provides serverless infrastructure with the Azure Functions service that offloads many of these infrastructure management tasks and simplifies the rest. Azure Functions operates the hosting instances to run your models as small functions without the developer or operator being aware of the specific virtual machine or fleets. Depending on your application needs and cost budget, you can use a choice of several hosting plans within Azure Functions from basic instances with a consumption plan, to premium instances and dedicated hosting. We will use the ‚Äúconsumption plan‚Äù which is usually the most cost effective (often free upto 1 million monthly requests per subscription ) option for relatively low volume scenarios.
You are only charged for the duration that the function actually runs in a consumption plan. Also, as your needs change, you can easily upgrade to premium or dedicated hosting plans as the underlying technology and methodology is still the same. Details on pricing for various hosting options are here. For efficient model serving we will use the ONNX Runtime, a highly optimized, low memory footprint execution engine. With ONNX Runtime, the deployment package footprint can be upto 10x lower, allowing us to use the more cost effective plan. More details are in the section below titled ‚ÄúOptimizing the runtime footprint‚Äù.
We will walk through the steps to take a PyTorch model and deploy it into the Azure Functions serverless infrastructure, running the model prediction in the highly efficient ONNX Runtime execution environment. While the steps illustrated below are specific to a model that was built using the popular fast.ai (a convenience library built on PyTorch), the pattern itself is quite generic and can be applied to deploying any PyTorch model.
The main steps to get your models into production on Azure serverless infrastructure using the ONNX Runtime execution engine (after you have trained your model are):
The model illustrated as an example is the Bear Detector model which is one of the popular examples in fast.ai. We won‚Äôt go into the actual training process here as it is the same method you normally use. The end result of the training process is a PyTorch model object in your Python environment. PyTorch provides a built-in mechanism to export your model object in the format needed by ONNX Runtime with the following code:
The parameters for the dummy_input depends on the shape of the tensors in your model. The output will be the model written to a file called model.onnx.
You also need to create a label file (labels.json) since this is a classification model. In the Bear Detector example, the model is classifying across the three classes of bear, and hence the label file looks like this:
The classes names must match the vocabulary you used during the training.
One of the best practices for productive development is to be able to test your deployment on your development machine before you deploy it to the cloud. I use a Windows laptop with Windows Subsystems for Linux (WSL2) as my development-test environment. The instructions should also apply to development environments like a local Linux machine, Azure Cloud Shell or a Virtual machine on the cloud like the Data Science Virtual Machine or Azure Machine Learning Compute Instances.
Setting up the environment and tools
You need to have the following tools installed on your development machine.
Create an Azure Function Project
First, you need to create a project for your Azure Functions locally, which is just a directory on your machine.
Next, you must initialize the Function App and specify the runtime. We use Python runtime and the Azure Functions whose execution is triggered through a HTTP request. This means that to get a prediction from your model you send a HTTP request from the client with the desired parameter (to be described later).
Create your inferencing code
We have a convenience inference code template that is published on GitHub that you can use as boiler plate and update to your needs. Here are steps to clone the code template and adapt it for your Azure Function App project to deploy your model.
The main source files are __init__.py and predictonnx.py in start/classify directory. In the Bear detector example, it takes input from the HTTP GET request in the ‚Äúimg‚Äù parameter which is a URL to an image which will be run through the model for prediction of the type of bear. You can adapt the same easily for deploying other models.
The predictonnx.py which does the actual prediction function, expects the model file and labels file in the current directory. In this example, we also need to do the pre-processing of the input image by normalizing it and scaling it to the desired size before it can be passed onto ONNX Runtime to run the inference operation. This file contains both the pre-processing code and the code to get the model prediction with ONNX Runtime.
Copy the model.onnx and labels.json files (created in the earlier step) to the directory.
Install the dependent Python libraries locally in a virtual environment.
Deploy Azure Functions App locally and test
Now you are ready to test your Azure Functions App locally. The Azure Functions Core Tools makes this super simple. Literally you just run one command from the ‚Äústart‚Äù directory:
This will start an environment very similar to what would be in the cloud-based Azure Functions on your local machine. It listens on port 7071 and is ready for your request. For testing the Azure functions all you need is to visit the following URL in a browser or use tools like curl or invoke a Web request from your client application where you want to consume the model.
http://localhost:7071/api/classify?img=[[URL of the image to classify]]
Effectively you are pass an URL to an image that the Azure functions in the ‚Äúimg‚Äù parameter of the web request to receive predictions from the model on the type of bear with the above example model.
After you have tested with a few sample images and are satisfied that your model works fine, you are now ready to deploy it to the cloud where a client or application from anywhere is able to consume predictions from the model.
Pro Tip: If the only consumer for the model is an app running on your development machine this can be an end state.
We will use the Azure CLI to create an Azure Function App and a Storage account and put all these in a resource group for easy management.
Note: If you have not logged into Azure CLI. you must first run ‚Äúaz login‚Äù and follow instructions to log into to Azure with your credentials. In the example above, we are deploying the resources in westus2. You can choose another Azure data center/ region if that is more convenient for you. Here in this example, we set a flag to disable Application Insights on this Azure Functions App. Application Insights is a service Azure provides to help you monitor your Azure Functions and other Azure services. We recommend enabling Application Insights for production deployment and refer you to the documentation on Functions Monitoring for more information on its usage.
Finally, you run the command to publish your Azure Function App project into Azure.
After a few minutes, your model is deployed to the cloud. The last command also output the URL base include a key that can be used to make HTTP request and get predictions from the model. In case you missed the output, you can go back and fetch the URL by running the command ‚Äúfunc azure functionapp list-functions [[YOUR Function App name] ‚Äî show-keys‚Äù. For this Bear detector app, you can append ‚Äú&img=[Your Image URL]‚Äù to the InvokeUrl from above command to invoke the Azure functions and receive predictions from the model.
You can visit the Azure portal and search for your Azure Function App.
Azure Functions provides additional deployment modes. For simplicity I used the local zip deployment which essentially packages up the local project directory (including the dependent python libraries) into a zip file which is then deployed to the Azure Functions App in the cloud. Azure Functions also supports container deployment on premium and dedicated hosting.
One challenge with the consumption plan is that the instance sizes are relatively small with a maximum of 1.5GB of main memory per instance. A native PyTorch model has a bigger footprint both from an App on-disk size and the working memory size perspective. The default runtimes in popular deep learning frameworks are more optimized for model development experience as opposed to serving. Microsoft developed the ONNX Runtime, a highly optimized, low memory footprint and open source execution engine for inferencing.
Using ONNX Runtime as the execution runtime in Azure Functions helps lower the footprint of hosting your PyTorch model and enables you to deploy models on the cheaper consumption plan hosting mode of Azure Functions. The total deployment package for our example was about 75MB (including the model file, Python library dependencies). In contrast, if you are using standard PyTorch runtime, the deployment package is almost 10X bigger for the same model since the PyTorch library and dependencies has to be bundled with your model. This often requires deploying to a larger instance type for hosting. In our experience in deploying numerous models within applications in Microsoft, the ONNX Runtime is on an average 2X faster enabling to serve models at low latency and high throughput. So, ONNX Runtime is a great option to deploy your PyTorch models in most scenarios especially in low cost / low resource environments such as the Azure Functions Consumption plan instances. Hosting models in Azure Functions with HTTP interface enables you to consume the same from cross platform clients.
It should be noted that there are other technologies you can use to deploy models on Azure. Many customers use Kubernetes clusters to run their applications and host their models. Azure offers a managed Azure Kubernetes Service (AKS) that can be used to host your models. Azure Machine Learning service provides out of the box support to deploy your models to AKS.
Some of the other considerations in deploying models into production include having a streamlined development and deployment processes. This is where end-to-end machine learning services like Azure Machine Learning addresses these challenges by effectively bridging the experimentation world of data scientists who are iterating on new models and the operational world of machine learning (also known as ML Ops) where the models are served in a production environment with the appropriate SLAs, ensuring model reproducibility, versioning, monitoring and feedback loops to improve models over time. We don‚Äôt cover these here but provide pointers in the ‚ÄúLearn More‚Äù section below.
We have seen how it is quite easy to deploy PyTorch models cost-effectively to the Azure serverless infrastructure and get the benefits of offloading operational concerns like scaling, security, monitoring and infrastructure management. The tooling provided by Azure Functions enables a good local development, debugging and deployment experience. ONNX Runtime enhances PyTorch with optimized inferencing and a fast execution engine in a small footprint, making your PyTorch model inferencing highly performant. We would to love to hear your experience with serverless deployment of your models and how we can improve our tools and processes further.
An open source machine learning framework that accelerates‚Ä¶
109 
109¬†claps
109 
Written by
PyTorch is an open source machine learning platform that provides a seamless path from research prototyping to production deployment.
An open source machine learning framework that accelerates the path from research prototyping to production deployment
Written by
PyTorch is an open source machine learning platform that provides a seamless path from research prototyping to production deployment.
An open source machine learning framework that accelerates the path from research prototyping to production deployment
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-difference-between-traffic-manager-and-front-door-service-in-azure-4bd112ed812f?source=search_post---------86,"There are currently no responses for this story.
Be the first to respond.
Comparison: Traffic Manager vs Front Door in Azure.
When choosing a global load balancer between Traffic Manager and Azure Front Door for global routing, you should consider what‚Äôs similar and what‚Äôs different about the two services. Both services provide
"
https://posts.specterops.io/death-from-above-lateral-movement-from-azure-to-on-prem-ad-d18cb3959d4d?source=search_post---------87,NA
https://medium.com/@renatogroffe/hospedando-um-website-est%C3%A1tico-de-forma-r%C3%A1pida-e-barata-no-azure-storage-da65913f26df?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 5, 2019¬∑5 min read
Figurando entre as principais plataformas de cloud computing da atualidade, o Microsoft Azure conta com diversos servi√ßos para a hospedagem de aplica√ß√µes Web. Est√£o dispon√≠veis desde solu√ß√µes voltadas a projetos de m√©dio e pequeno porte, quanto alternativas recomendadas a cen√°rios de uso mais intensivo e que englobem at√© mesmo milh√µes de acessos/usu√°rios simult√¢neos.
Diante disso como poder√≠amos ent√£o realizar o deployment de um simples web site est√°tico sem grandes complica√ß√µes e com um custo reduzido?
Uma boa alternativa neste sentido √© o suporte √† hospedagem de sites est√°ticos oferecidos pelo Azure Blob Storage, um dos principais servi√ßos de armazenamento que integram a plataforma de cloud computing da Microsoft.
Na imagem a seguir (extra√≠da do site de precifica√ß√£o do Azure Storage no in√≠cio de Mar√ßo/2019) √© poss√≠vel ter uma dimens√£o dos valores envolvidos:
Ao longo deste artigo ser√° demonstrada a hospedagem de um web site est√°tico empregando o Azure Blob Storage, com isto acontecendo a partir do Visual Studio Code em um ambiente Linux (Ubuntu Desktop 18.04).
E aproveito este post para deixar aqui um convite.
Dia 06/03/2019 (quarta-feira) √†s 21:30 ‚Äî hor√°rio de Bras√≠lia ‚Äî teremos mais um evento online no Canal .NET. Desta vez farei uma apresenta√ß√£o sobre o presente e o futuro do .NET Core e do ASP.NET Core, cobrindo as vers√µes 2.2 e 3 (esta √∫ltima ainda em Preview). Al√©m disso mostrarei novidades envolvendo o Visual Studio 2019 e o C# 8.0.
Para efetuar a sua inscri√ß√£o acesse a p√°gina do evento no Meetup. A transmiss√£o acontecer√° via YouTube, em um link a ser divulgado em breve.
O primeiro passo para se proceder com a publica√ß√£o ser√° a cria√ß√£o de uma nova Storage Account no Portal do Azure:
√â fundamental que o tipo da nova conta seja StorageV2 (general purpose v2), como indicado na pr√≥xima imagem. Por se tratar de um site e se esperar a ocorr√™ncia de acessos frequentes ao mesmo foi selecionada a op√ß√£o Hot em Access tier:
Ap√≥s a cria√ß√£o da conta de armazenamento (chamada de groffestatic neste exemplo) o pr√≥ximo passo consiste na configura√ß√£o deste recurso, de forma que o mesmo seja empregado na hospedagem de um site est√°tico. Acessar para isto o item Static website:
Ativando ent√£o a op√ß√£o Enabled e informando o arquivo correspondente ao √≠ndice/p√°gina inicial do site em Index document name:
Ao acionar o bot√£o Save neste mesmo painel e, ap√≥s alguns segundos, aparecer√£o 2 endpoints para acesso ao site est√°tico:
Testes de acesso a esses 2 endere√ßos mostrar√£o que nenhum conte√∫do foi publicado at√© este momento:
Muito utilizado no desenvolvimento Web, o Visual Studio Code conta com diversas extens√µes que simplificam a utiliza√ß√£o e gerenciamento de recursos gerados a partir do Microsoft Azure.
Na imagem a seguir observamos o conte√∫do do web site a ser publicado na conta de armazenamento criada na se√ß√£o anterior (basicamente uma p√°gina HTML, que exibir√° uma imagem/banner):
OBSERVA√á√ÉO: Os procedimentos descritos nesta se√ß√£o foram executados em uma m√°quina na qual foi instalado o Ubuntu Desktop 18.04.
Para o deployment ser√° necess√°rio instalar a extens√£o Azure Storage:
Acessando o √≠cone do Azure ser√° exibido o item STORAGE, com contas de armazenamento vinculadas a uma assinatura/subscription desta plataforma. Navegando at√© Blob Containers na conta groffestatic aparecer√° um container chamado $web (n√£o confundir essa estrutura de armazenamento com containers Docker!); ser√° justamente este elemento no qual ser√° hospedado o site:
Clicar com o bot√£o direito do mouse sobre $web, acionando na sequ√™ncia a op√ß√£o Deploy to Static Website‚Ä¶:
Selecionar agora o diret√≥rio contendo o website:
O deployment ter√° finalmente in√≠cio, com uma mensagem sendo exibida ao final em caso de sucesso:
Acessando neste momento o container $web aparecer√£o ent√£o os arquivos index.html e mvpconf-banner.jpg vinculados ao mesmo:
A seguir temos o resultado no browser para o endpoint prim√°rio:
E tamb√©m um teste com o endpoint secund√°rio:
Deploy a static website to Azure
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
68 
2
68¬†claps
68 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mbellinaso/microsoft-azure-services-overview-and-notes-cfb30b95d8e?source=search_post---------89,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marco Bellinaso
May 25, 2018¬∑10 min read
This page will grow and change over time. It‚Äôs not meant to be ‚Äúdocumentation‚Äù, but rather a quick list of important notes and high-level concepts that I wrote down for myself, in pretty much random order. Feel free to suggest additions and point out mistakes in the comments below or directly highlighting the incriminated content.
Azure Services
Containers
Service Fabric supports .NET Framework apps, Core apps, Linux and Windows Docker containers.
AKS supports Core apps and Linux-based Docker containers. Win-based Docker container will be added. .NET Framework apps are not supported.
Even Azure AI models can be exported to containers that can run in AKS.
Announced at Build 2018 (and currently in private beta), Dev Spaces allows to deploy and *debug* AKS-hosted containers from Visual Studio.
Data Storage
Azure SQL DB, PostgreSQL and MySql for relational data.
Cosmos DB for unstructured data + small and large data
Azure Storage
Security
Other services
Messaging
Monitoring
Visual Studio and Tools
Who am I / what do I do? I proudly work as a Solutions Architect in the Mobile Team @ ASOS.com (iOS app | Android app), and we‚Äôre always looking for strong, friendly and talented developers that want to have an impact on how tens of millions of customers shop online. ASOS is the biggest online-only retailer in UK and, let‚Äôs be real, the best tech+fashion company in the world. Some of the technologies we use are Swift for iOS, Kotlin for Android, React and Node on the web frontend, .NET and Azure on the backend. If that sounds interesting to you, and you happen to live in beautiful London (or are willing to move here‚Ä¶after all it‚Äôs the best city in Europe except for some in Italy!), do get in touch with me!
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
164 
2
164¬†
164 
2
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
"
https://medium.com/@wtfmitchel/is-azure-stagnating-c91894e67292?source=search_post---------90,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mitchel Lewis
Jul 31, 2021¬∑7 min read
It‚Äôs not a secret that Microsoft‚Äôs future depends on Azure not only being successful, but dominant. With Google Workspace dethroning Office 365 in the cloud productivity markets, Windows needing a complete re-write since a decade ago and their consequent monopoly on exploits and ransomware attacks in the PC and Server markets, much is riding on Azure‚Äôs ability to dominate the cloud infrastructure space as Microsoft has done with the OS, productivity, and server spaces before.
One consequence of Microsoft‚Äôs dependence on Azure is that Microsoft can post its best quarter ever and investors will get spooked if Azure‚Äôs revenue growth slips in the slightest. It also doesn‚Äôt help when their CFO Amy Hood admitted that Azure slowing revenue growth still performed better than she anticipated.
‚ÄúForty-five percent was both better than we expected and driven by consumption growth, which is very good,‚Äù Hood said in an interview. ‚ÄúDemand is healthy. The overall execution was better than I expected.‚Äù -Amy Hood
Unlike before though, Microsoft isn‚Äôt starting at the top of an emerging market as it did in the OS, Productivity, and Server spaces. Instead, Microsoft was 2 years late to the market and has to compete with the likes of AWS instead and claw market-share away from them. And this is bad news for Microsoft as competing with other tech monopolies in established markets is not something that they‚Äôre especially good at; they just aren‚Äôt the same company that mothballed IBM all those years ago.
The success of Microsoft‚Äôs business model relies mostly on them being among the first movers of infant markets, becoming the industry standard, and entrenching its products throughout said industry; lock-in if you will. In turn, their products no longer need to compete on quality, cease to evolve, and stagnate no differently than the human race as they have no ecological competition. Apparently, the law of natural selection even applies to markets just as it applies to us.
In doing this, Microsoft‚Äôs frustrating, insecure, and unstable architecture renders users change and technology averse, traumatized if you will, and consequently vying to keep everything the same. Further, they can artificially inflate the switching costs of moving to their competition, derail migration efforts to their competition even if it‚Äôs better technology, and maintain dominance. Put simply, Microsoft‚Äôs products and services create a moat of sorts that keeps users in and competition out while allowing them to compete with themselves. Mitigating their defenses is much easier said than done.
Being a first-mover that optimizes their solutions for lock-in is a double whammy for Microsoft and no one seems to care; hence why they do it. This happens to be why Windows, Active Directory, Server, and Exchange are still in play today despite being legacy, expensive, complex, frustrating, and unstable for users and admins alike. It‚Äôs simply too ingrained and users/admins are rendered apathetic to change.
While Microsoft can‚Äôt exactly take credit for this brilliant aspect of their business model, they can absolutely laugh all the way to the bank at anyone who is criticizing them about their quality woes without realizing that they don‚Äôt even have to compete on quality; at least until Azure became their last hope.
One immediate problem with their tactics though is that they don‚Äôt bode well in markets that are already well-established. Nor is it easy to re-structure a company to engineer for quality when it‚Äôs structured to maximize lock-in. Although absolute genius goes into engineering products for lock-in, especially when realizing that all of their engineers are trying to do their best/ethical job, this heroin-esque approach to engineering is systemic and cannot be turned off like a light switch; quite the contrary. Any manager at Microsoft can and will affirm that Microsoft is a big ship to steer and such a restructuring could take years to fully implement.
As such and much like their founder Bill Gates, Microsoft isn‚Äôt built for fair competition, hence why they lose their ass in markets that they‚Äôre late to. And as they have shown repeatedly with cloud, mobile, social, gaming, and laptop markets, Microsoft is consistently a fish out of water when entering saturated markets because they are not optimized to compete on quality which is the only card that a new entrant has to play against the status quo; exhibit Zoom, Slack, Twitch. All of which stacks the deck further against Microsoft‚Äôs ambitions with Azure given out competitive the cloud infrastructure market is.
To highlight this and although Microsoft is doing great things in the cloud space with Office 365, they were late to the market, ironically among the last to host their own services, and are in second place while losing further ground to Google Workspace. The same is true of Azure in that it was 2 years behind AWS to the cloud infrastructure market.
And although Microsoft and analysts claim Azure to be second in the cloud infrastructure space from a revenue perspective, Microsoft has yet to corroborate this with data and is refusing to post individual performance metrics of Azure after a decade of production. Based on what little we‚Äôve seen though, AWS revenue is growing while Azure revenue growth is shrinking which is the opposite of what Azure needs to do. Meanwhile, AWS revenue grew 9% in the last year.
No matter where you look, you can find Microsoft consistently omitting all key performance indicators (KPIs) worthy of mention concerning Azure financials or usage; MAU, P&L, CPA, ARPA, RPE, etc; nada. Meanwhile, you‚Äôll find a whole host of ambiguous metrics such as vague growth rates, total user counts instead of monthly use statistics, and containers like the Intelligent Cloud averaging various offerings together. All of which takes significantly more effort than simply reporting individual performance and is frankly hard to keep under wraps for 12 years. Meanwhile, AWS has no problem reporting on AWS‚Äôs performance; they have nothing to hide.
Oddly enough though and while it‚Äôs even their policy to never report on KPIs, they definitely track them and occasionally post them but only if they exude a dominant market presence. In doing this though, Microsoft has a tell so to speak. Put simply, when products are doing fantastically, Microsoft will break protocol from time to time and report KPIs. But when products are doing horribly, Microsoft seems to hide behind their bogus policy so as to keep KPIs under lock and key while sugar-coating poor performance with ambiguities instead.
In doing this though, this being not reporting common usage and financial metrics while further hiding individual performance in the Intelligent Cloud, Microsoft has made it impossible for analysts to evaluate where Microsoft stands in the fold compared to AWS or Google Cloud. Ironically, the assessments declaring Azure to be in second place among cloud providers are speculative at best.
‚ÄúMuddy waters make it easy to catch fish.‚Äù ‚Äî Chinese Proverb
With all of this in mind, it‚Äôs easy to see why Microsoft needs investors to believe that Azure‚Äôs position is strong and why Microsoft is working so hard to keep Azure‚Äôs performance under wraps; that dog don‚Äôt hunt. Although I can only speculate, it seems as if the KPIs surrounding Azure do not exhibit dominance or a route to dominance that Microsoft needs to project in order for share prices to keep rising while its stagnant revenue growth serves as further evidence of this.
If said KPIs did exhibit Azure‚Äôs dominance or even a route to dominance, then Microsoft would have no reason to be shy and release them in the face of increasing scrutiny of their persistent refusal to report on these metrics. And their refusal to post these metrics while muddying the waters with pointless statistics/rates and odd financial containers instead isn‚Äôt exactly a good omen so far as the health of Azure is concerned; if not symptomatic of the contrary. Put simply, if Azure truly had a big ol‚Äô dong then Microsoft would have thrown it on the table by now rather than hiding it behind excuses and obscure metrics for over a decade.
To be fair though, Microsoft could indeed be shy about Azure‚Äôs performance for the past 12 years. Azure could be doing swimmingly for all I know and I‚Äôm willing to accept that reality if it bears truth. What I do know is that omission is the most common form of lying with statistics, followed by obfuscating matters with ambiguous metrics, and Microsoft just doesn‚Äôt have an incentive to resort to these squid and ink tactics if Azure is in great shape.
All stars go through an inflationary phase before they go supernova and I‚Äôm forced to wonder if this is true of Microsoft given their questionable reporting. Obviously, you‚Äôre welcome to believe otherwise though. It‚Äôs not like Microsoft has provided us with enough information to do anything else but speculate.
Further Reading:
hackernoon.com
Engineer, Farmer, and Hellion
175 
8
175¬†
175 
8
Engineer, Farmer, and Hellion
"
https://medium.com/@moments-with-bren/free-courses-on-devop-aws-azure-gcp-linux-windows-python-golang-scripting-helm-database-b146f2ca147?source=search_post---------91,"Sign in
There are currently no responses for this story.
Be the first to respond.
Momentswithbren
Dec 22, 2019¬∑3 min read
Free Courses on DevOp, AWS, Azure, GCP, Linux, Windows, Python, Golang, Scripting, Helm, Database, ML.
START LEARNING!!
DevOps - FREE Udemy Courseshttps://www.udemy.com/course/devops-crash-course-cicd-with-jenkins-pipelines-groovy-dsl/
https://www.udemy.com/course/learn-devops-with-jenkins-all-in-one-guide/
https://www.udemy.com/course/jenkins-quick-start/
https://www.udemy.com/course/jenkins-intro/
https://www.udemy.com/course/ansible-essentials-simplicity-in-automation/
https://www.udemy.com/course/devops-series-server-automation-using-ansible/
https://www.udemy.com/course/devops-beginners-guide-to-automation-with-ansible/
https://www.udemy.com/course/learn-devops-kubernetes-deployment-by-kops-and-terraform/https://www.udemy.com/course/containers-101/
https://www.udemy.com/course/kubernetes-for-developers/
https://www.udemy.com/course/jenkins-beginner-tutorial-step-by-step/
https://www.udemy.com/course/deploying-containerized-applications-technical-overview/
https://www.udemy.com/course/docker-on-windows/
https://www.udemy.com/course/learn-devops/
https://www.udemy.com/course/docker-essentials/
https://www.udemy.com/course/kubernetes-by-example/
https://www.udemy.com/course/atlassian-jira-for-beginners/
https://www.udemy.com/course/ci-cd-pinepline-devops-automation-in-1-hr/
https://www.udemy.com/course/complete-junit-5-course-for-beginners/
https://www.udemy.com/course/master-virtualbox-in-one-day/
https://www.udemy.com/course/linux-academy-devops-essentials/
https://www.udemy.com/course/git-started-with-github/
https://www.udemy.com/course/kubernetes-getting-started/
https://www.udemy.com/course/learn-git-bash/
AWS - FREE Udemy Courseshttps://www.udemy.com/course/developing-cloud-native-applications-microservices-architectures/
https://www.udemy.com/course/namrata-h-shah-aws-tutorials-dynamodb-and-database-migration-service/
https://www.udemy.com/course/aws-developer-associate-training/
https://www.udemy.com/course/aws-certified-solutions-architect-associate-in-30-days/
https://www.udemy.com/course/linux-academy-aws-essentials-2019/
https://www.udemy.com/course/learn-amazon-web-services-the-complete-introduction/
https://www.udemy.com/course/amazon-web-services-aws-v/
https://www.udemy.com/course/amazon-web-services-aws-cloudformation/
https://www.udemy.com/course/aws-developer-associate-training/
https://www.udemy.com/course/aws-certified-solutions-architect-associate-in-30-days/
https://www.udemy.com/course/learn-amazon-web-services-the-complete-introduction/
https://www.udemy.com/course/amazon-web-services-aws-v/
https://www.udemy.com/course/amazon-web-services-aws-cloudformation/
https://www.udemy.com/course/aws-appstream-20-introduction/
https://www.udemy.com/course/ec2with10labs/
https://www.udemy.com/course/launch-a-lamp-stack-and-install-wordpress-on-aws/
https://www.udemy.com/course/aws-certified-solutions-architect-associate-in-30-days/
https://www.udemy.com/course/aws-certified-security-specialty-exam-introduction/
https://www.udemy.com/course/amazon-web-services-aws/
https://www.udemy.com/course/mastering-aws-featuring-iam/
https://www.udemy.com/course/learn-amazon-web-services-aws-easily-to-become-architect/
https://www.udemy.com/course/commvault/
https://www.udemy.com/course/aws-concepts/
https://www.udemy.com/course/complete-aws-course-learn-hands-on-practically/
https://www.udemy.com/course/terraform-fast-track/
https://www.udemy.com/course/amazon-web-services-monitoring-and-analysis/
Azure - FREE Udemy Courses
https://www.udemy.com/course/maruti-microsoft-azure-step-by-step-part-1/
https://www.udemy.com/course/azure-storage-security-guide/
https://www.udemy.com/course/learn-fundamentals-of-cloud-thru-microsoft-azure/
https://www.udemy.com/course/az-900-basics-of-cloud-computing/https://www.udemy.com/course/jumpstart-docker-platform-with-microsoft-azure/
https://www.udemy.com/course/introduction-to-cloud-computing/
GCP - FREE Udemy Courseshttps://www.udemy.com/course/google-cloud-platform-overview-for-aws-professionals/¬†Linux - FREE Udemy Courseshttps://www.udemy.com/course/linux-command-line-for-beginners-42/
https://www.udemy.com/course/vi-editor/
https://www.udemy.com/course/linux-basics-for-beginners/
https://www.udemy.com/course/command-line/
https://www.udemy.com/course/ubuntu-linux-on-virtualbox-quick-setup/
https://www.udemy.com/course/command-line-arguments-for-beginners/
Windows - FREE Udemy Courseshttps://www.udemy.com/course/iis-web-server-free-course/
Python - FREE Udemy Courseshttps://www.udemy.com/course/python-for-every1/
https://www.udemy.com/course/complete-python-masterclass-learn-from-scratch/
https://www.udemy.com/course/complete-python-course-go-from-zero-to-hero/https://www.udemy.com/course/complete-python-course-zero-to-hero/
https://www.udemy.com/course/complete-python-course-learn-from-scratch/
https://www.udemy.com/course/python-regular-expressions-with-examples/
https://www.udemy.com/course/python-for-machine-learning-t/
https://www.udemy.com/course/complete-python-course-go-from-beginner-to-advanced-x/
https://www.udemy.com/course/python-for-every1/
https://www.udemy.com/course/complete-python-masterclass-go-from-beginner-to-advanced/
https://www.udemy.com/course/the-ultimate-python-course-learn-from-scratch-r/
https://www.udemy.com/course/complete-python-course-zero-to-hero-x/
https://www.udemy.com/course/complete-python-course-learn-from-scratch-k/
https://www.udemy.com/course/complete-python-course-go-from-zero-to-hero-z/
https://www.udemy.com/course/ultimate-python-course-go-from-zero-to-hero/
https://www.udemy.com/course/the-ultimate-python-course-learn-from-scratch-i/
https://www.udemy.com/course/learn-python-from-scratch-w/
https://www.udemy.com/course/complete-python-masterclass-learn-from-scratch-b/
https://www.udemy.com/course/pythonbasics/
https://www.udemy.com/course/python-fundamental-basics/
https://www.udemy.com/course/learn-python-from-scratch-m/
https://www.udemy.com/course/lets-learn-python-s/
https://www.udemy.com/course/absolute-python-basics-for-anyone/
https://www.udemy.com/course/python-programming-basic/
https://www.udemy.com/course/the-complete-python-course-beginner-to-advance/
https://www.udemy.com/course/mongodb-and-python-quickstart-with-mongoengine/
https://www.udemy.com/course/comprehensive-python-course/
https://www.udemy.com/course/complete-python-course-learn-from-scratch-c/
https://www.udemy.com/course/complete-python-course-beginner-to-advance/
https://www.udemy.com/course/technobytes-python-for-beginners/
https://www.udemy.com/course/learn-python-from-zero/
https://www.udemy.com/course/pythonforbeginnersintro/
https://www.udemy.com/course/ready-for-python-within-an-hour/
https://www.udemy.com/course/exception-handling-in-python-3-try-except-else-finally-raise/
https://www.udemy.com/course/advanced-python-memory-management/
https://www.udemy.com/course/python-3-crash-course/
https://www.udemy.com/course/complete-python-course-zero-to-hero-e/
https://www.udemy.com/course/complete-python-course-learn-hands-on-practically-i/
https://www.udemy.com/course/complete-python-course-learn-hands-on-practically/
https://www.udemy.com/course/learn-python-build-a-virtual-assistant-in-python/
https://www.udemy.com/course/getting-started-with-python/
https://www.udemy.com/course/python-for-absolute-beginners-u/
https://www.udemy.com/course/pythonforbeginnersintro/
https://www.udemy.com/course/learn-and-practice-python-programming-python-from-scratch/
https://www.udemy.com/course/python-for-complete-beginners-u/
Golang - FREE Udemy Courseshttps://www.udemy.com/course/getgoing/
Scripting - FREE Udemy Courses
https://www.udemy.com/course/practical-bash-scripting/
https://www.udemy.com/course/groovy-step-by-step-for-beginners/
https://www.udemy.com/course/introduction-to-linux-shell-scripting/
https://www.udemy.com/course/powering-you-with-powershell/
https://www.udemy.com/course/bash-shell-scripting/
https://www.udemy.com/course/powershell-command/
https://www.udemy.com/course/learn-powershell/
Helm - FREE Udemy Courseshttps://www.udemy.com/course/helm-best-practices-2019/¬†Database - FREE Udemy Courses
https://www.udemy.com/course/oracle-database-lab-setup-at-home/
https://www.udemy.com/course/sql-crash-course-postgresql-for-beginners/
https://www.udemy.com/course/complete-machine-learning-masterclass-learn-from-scratch-c/
https://www.udemy.com/course/complete-machine-learning-course-learn-from-scratch/
https://www.udemy.com/course/python-for-machine-learning-t/
https://www.udemy.com/course/machine-learning-masterclass-a-z-beginner-to-advance/
https://www.udemy.com/course/making-computers-think/
Service Design | Power Skills Training | Positive Psychology | Education, Employment and Economic Advancement | Scholarships | Knowledge Mobilization
164 
164¬†
164 
Service Design | Power Skills Training | Positive Psychology | Education, Employment and Economic Advancement | Scholarships | Knowledge Mobilization
"
https://medium.com/@renatogroffe/publicando-um-web-site-est%C3%A1tico-na-nuvem-com-docker-nginx-e-azure-container-instances-d688823bbe1b?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jan 25, 2019¬∑5 min read
A utiliza√ß√£o de Docker em aplica√ß√µes Web costuma ter como ponto de partida imagens pr√©-existentes, nas quais j√° foram devidamente configurados um runtime para a execu√ß√£o de projetos em plataformas como .NET, Java ou Node, por exemplo.
E se precisarmos fazer o mesmo com um site est√°tico, cujo conte√∫do normalmente envolve apenas arquivos HTML, imagens, JavaScript e CSS? Empregar uma imagem contendo um SDK n√£o seria a melhor op√ß√£o neste caso, j√° que esta √∫ltima traria uma s√©rie de componentes desnecess√°rios para a execu√ß√£o do site em quest√£o.
Uma √≥tima alternativa para este cen√°rio seria o uso do Nginx a partir de um container. O Nginx √© um web server open source bastante popular no que se refere √† implementa√ß√£o de mecanismos de load balancing, proxy reverso, compress√£o de dados e seguran√ßa. Outro ponto que pesa a seu favor est√° no fato de que conta com imagens Docker relativamente pequenas, sobretudo se levarmos em conta sua distribui√ß√£o baseada em Alpine Linux (na casa dos 17 MB).
E quanto √† publica√ß√£o de um site est√°tico na nuvem a partir de um container Docker, este √∫ltimo baseado em uma imagem do Nginx? Uma solu√ß√£o relativamente barata e de f√°cil utiliza√ß√£o seria o servi√ßo Container Instances que integra o Microsoft Azure.
Neste novo artigo demonstrarei a gera√ß√£o de uma imagem contendo um web site est√°tico e empregando o Nginx, com a sua posterior publica√ß√£o no Azure Container Instances.
E aproveito este espa√ßo para deixar aqui ainda um convite.
Dia 08/02/2019 (sexta-feira) √†s 21h30 ‚Äî hor√°rio de Bras√≠lia ‚Äî teremos mais um evento online gratuito no canal Coding Night. Desta vez acontecer√° um bate-papo online descontra√≠do (e com pol√™micas) entre Desenvolvedores Front e Back-end sobre a implementa√ß√£o de solu√ß√µes, frameworks JavaScript, performance e outros temas relacionados ao desenvolvimento Web.
Para efetuar a sua inscri√ß√£o acesse a p√°gina do evento no Meetup. A transmiss√£o acontecer√° via YouTube, em um link a ser divulgado em breve.
O site utilizado como exemplo neste artigo ser√° formado por um arquivo index.html, al√©m de conter uma imagem .png de divulga√ß√£o de um evento. Na imagem a seguir √© poss√≠vel observar a estrutura de arquivos (com os arquivos Dockerfile e docker-compose.yml) e diret√≥rios (o conte√∫do correspondente ao site se encontra na pasta site-meetup-59-dotnet-sp):
O arquivo Dockerfile referencia a imagem Alpine do Nginx, al√©m de constar no mesmo uma instru√ß√£o que copiar√° o conte√∫do da pasta site-meetup-59-dotnet-sp para o diret√≥rio /usr/share/nginx/html da imagem correspondente ao site est√°tico:
Na arquivo docker-compose.yml est√£o as configura√ß√µes para a gera√ß√£o da imagem meetup-59-dotnet-sp-nginx, bem como a cria√ß√£o de um container para testes baseado na mesma e que estar√° acess√≠vel atrav√©s da porta 30000:
A execu√ß√£o do comando docker-compose up -d no diret√≥rio em que se encontram os arquivos docker-compose.yml e Dockerfile ter√° como resultados:
Na pr√≥xima imagem est√£o os resultados indicando a imagem gerada (meetup-59-dotnet-sp-nginx com 17.9 MB), assim com o container gerado a partir da mesma:
Um teste de acesso via browser ao endere√ßo http://localhost:30000 mostrar√° que o site hospedado no container est√° funcionando corretamente:
O comando docker login permitir√° a autentica√ß√£o junto a uma conta/reposit√≥rio p√∫blico no Docker Hub, exigindo para isto que sejam informados um usu√°rio pr√©-existente e sua respectiva senha.
O deployment de imagem p√∫blica do site est√°tico aqui apresentado envolver√°:
Consultando o Docker Hub ap√≥s este procedimento j√° aparecer√° a imagem meetup-59-dotnet-sp-nginx:
No portal do Azure ser√° criado um novo recurso baseado no servi√ßo Container Instances:
Preencher no formul√°rio de cria√ß√£o:
Na pr√≥xima tela indicar:
Em Summary acionar o bot√£o OK, confirmando a cria√ß√£o de um novo container:
Na pr√≥xima imagem √© poss√≠vel observar o container j√° criado. O acesso ao mesmo se dar√° tanto via IP (indicado em IP address), quanto via endere√ßo especificado em FQDN (Fully Qualified Domain Name):
E nas imagens a seguir √© poss√≠vel observar o site hospedado em execu√ß√£o (acessado via IP, assim como atrav√©s do endere√ßo associado ao recurso do Azure Containers Instances):
Azure Container Instances Documentation
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
Nginx - Docker Hub
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
79 
79¬†
79 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/daily-programming-tips/how-to-deploy-a-local-ml-model-as-a-web-service-on-azure-machine-learning-studio-5eb788a2884c?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@thisiszone/azure-durable-functions-before-and-after-b7266d51ed4d?source=search_post---------94,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zone
Jun 11, 2018¬∑10 min read
Zone‚Äôs head of .NET development, Andy Butland, looks at the use of Azure Functions and Durable Functions to tackle common concerns‚Ä¶
I recently had the pleasure of attending and speaking at the Intelligent Cloud Conference in Copenhagen, where I talked on the subject of Azure Functions.
I went through some of introductory topics such as what the platform is and why we might consider using it, and then ran through a number of examples ‚Äî illustrating solving real-world problems with Azure Functions via a combination of diagrams, demos and code. The latter half of the presentation was focused on a comparison between two implementations, for two patterns, using ‚Äúclassic‚Äù (my name ‚Äî it‚Äôs too early for a ‚Äúclassic‚Äù designation I think!) Azure Functions and a newly released extension to the platform named Durable Functions.
I‚Äôm going to spin that discussion off into the following blog post ‚Äî comparing the implementation of two long-running tasks, using chaining and fan out/in techniques. Some important aspects of the code will be shown within the article, but to view the full implementation of each pattern, please see the Github repository.
As a brief introduction though, Durable Functions allow us to abstract away intermediate queues and tables that we might otherwise need to support workflows using the standard platform. Rather than going via these storage components, functions can call other functions directly, passing input and retrieving output ‚Äî which sounds obvious given the name ‚Äúfunctions‚Äù of course, but it‚Äôs much more clearly apparent when using the durable functions approach. As we‚Äôll see, we can‚Äôt completely ignore the underlying storage components that support this, but nonetheless, many scenarios can be implemented with less ‚Äúplumbing‚Äù required.
We might look to chain together multiple Azure Functions for a couple of reasons. The first is that function execution time is limited ‚Äî 5 minutes by default, extendable via configuration to 10 minutes. That‚Äôs still quite a long time and plenty for most processes but we could certainly have examples of a workflow that takes longer than that, and therefore couldn‚Äôt be completed within a single function invocation.
Even if that‚Äôs not the case, we would likely want to adhere to the single-responsibility principle (SRP) within our function applications ‚Äî for the same reasons of clean code and ease of understanding and maintenance as we would in any software development. So we would likely want to break up more complex workflows into multiple functions, each responsible for a single, smaller piece.
To do that though, we need to find a way to have the output of one function be passed to the next one in the chain. Which we can do via an intermediate storage component, most typically, a queue.
The example I looked at was a (fictitious) e-commerce workflow, where we want to carry out a number of steps following the placement of an order by a customer. We need to write to a database, make a call to an external web-service to manage inventory updates and to send a confirmation email to the customer. In order to provide a responsive experience to the user, we don‚Äôt want to wait to complete all the necessary steps in the workflow before sending a response back to them, so instead we take the order details, save them to a queue message and continue the workflow as a background process.
The first step of our implementation using the standard Azure Functions platform is to create a queue triggered function to respond to this queue message. This function has one primary job ‚Äî to update the database. Its secondary job is to create a new message, on a new queue, which in turn triggers the second function to carry out its job of calling the external web service. And so on, to the final function that sends the email to the customer.
We are able to exhibit some control and customisation implementing logic to simply not pass the message onto the next queue if part of the process fails and we want to cut short the full workflow.
The following diagram illustrates the components involved:
The implementation of the first function is as follows:
Key points to note include the incoming item parameter, which is bound via the QueueTrigger attribute and thus contains the content of our initial message. We also have a second parameter named outputQueue, which is of a collection type. Once the task of updating the database is complete, we manipulate that collection in code by adding a typed item to it ‚Äî and because of the Queue attribute applied to the parameter, this change is reflected in the storage component itself as a new message in the queue.
This is one of the nice features of Azure Functions I find, these attributes provide a lot of power when it comes to integrations, meaning we can avoid dropping down into the lower-level APIs of the storage component or service libraries themselves and focus more on implementing the logic of our business process.
Taking the same scenario with Durable Functions, we can immediately see via a compare and contrast of the two component diagrams that we have on the face of it a much simpler solution:
On the left, we have our input as before, from queue storage. And on the right, our external systems and services. In the middle though, just functions calling functions, and no (apparent) intermediate storage components.
The first function is now implemented as follows:
We have our queue trigger as before, but this time a new parameter called starter that‚Äôs of a type DurableOrchestrationClient. Via it, we can call our main workflow function ‚Äî known as an orchestration function within the framework ‚Äî passing in the name of the function to call, and the initial input (the contents of our queue message).
The orchestration function looks like this:
As input we receive a parameter of type DurableOrchestrationContext which allows us to do various things like retrieve the input to the function and then make calls to other functions ‚Äî called activity functions ‚Äî to carry out their dedicated tasks. We can pass information to those functions, as well as receive their returned output back ‚Äî and if need be, respond appropriately with standard C# control of flow code to amend the workflow. For example here, if the call to update the inventory fails, we opt not to send the confirmation email to the customer.
Finally an example of an activity function, which has a parameter of type DurableActivityContext, again allowing us to retrieve the function input (which, as is shown here, can be typed rather than simply a string):
Another pattern used for handling larger workloads is known as fan-in/out, or sharding, where we take one large input file and break it up into smaller ones that can be executed in parallel. When all are finished, we aggregate these intermediate results in order to produce an overall output.
The example I chose to illustrate this was to obtain from Kaggle a large data-set of summer Olympic data from 1896 to the present day, containing every single event and medal won. I then wanted to analyse that to determine the overall ranking of countries in terms of medal success in Olympic history.
The following diagram illustrates the components and functions that were involved in the implementation using the standard Azure Functions platform:
We start with the large dataset (a CSV file) uploaded to a blob storage container. A function, configured with a trigger for that container is then fired. It‚Äôs going to be responsible for the ‚Äúfan-out‚Äù operation, and will break the file up into smaller files, one for each year, and write them to a second blob storage container.
First though, we need somewhere to store our intermediate results for each of the fanned out operations. We use table storage for that, so populate a table with one record for each year, with an empty field reserved for population when each operation on a given year completes.
A second function set up to be triggered on the second blob storage container will be fired. At this point the scaling feature of the Azure Functions platform will kick-in, and, as we have multiple files in this container, we‚Äôll get a number of function instances triggered and run in parallel. As each completes it does two things. Firstly, it updates the record in table storage for the given year. And secondly, it writes a message to a queue ‚Äî just a simple flag indicating ‚ÄúI‚Äôm done‚Äù, that will be used to trigger the last step in the process.
The final function, responsible for the ‚Äúfan-in‚Äù operation, triggers from these queue messages. We have an issue here though, in that we can only calculate our final result once all years have been processed. So the first thing this function does is check in table storage and query to see if data for all years has been completed. If not, the function simply exits. When the final message on the queue is processed, we‚Äôll have data for all years, so can continue, aggregate the results and produce an output, which is written as a CSV file to a final blob storage container.
Code-wise, the first function looks like this:
Perhaps the most interesting thing to note here is the use of the parameter of type Binder. We need this because we are going to create multiple files in blob storage, and we don‚Äôt know for sure at compile time what the amount and names of the files will be (there‚Äôll be one for each year). So we can‚Äôt use an attribute as before. Well, in fact we can, but it‚Äôs one we have to create dynamically at run-time which can be seen in the WriteFilePerYear method. Here we dynamically create an attribute on an instance of a CloudBlockBlob object, and having done that, the changes we make to it in terms of setting properties and uploading bytes, are reflected in the blob storage container. Again, we avoid digging down into the lower-level storage API directly (though of course, if we ever need to, we can ).
The second function processes the data for each year and writes the results to table storage:
And the final function carries out the fan-in operation as described above:
We can turn to Durable Functions to implement the same pattern and, as we‚Äôll see, we get a couple of key benefits. Here‚Äôs the updated component diagram:
Once again, note the absence of intermediate storage components; we‚Äôve been able to do away with explicit use of message queues and table storage for tracking the intermediate results. We have our input and output blob storage containers, but the workflow itself is just functions calling other functions, passing input and receiving and acting on their return values.
The other valuable benefit is that our fan-in operation ‚Äî which was tricky to implement previously ‚Äî is much more straightforward, and efficient. We are no longer having to make unnecessary function invocations where we check to see if all intermediate operations are complete, and if not, exit. Each of those would have an, albeit very small, cost associated, which we can now avoid.
Our initial trigger function is now responsible for taking the initial input and transforming it into a typed structure to pass to the orchestration function:
There‚Äôs one little aside to note here, which is that although intermediate storage components are abstracted away, they still exist. Queues are used to pass data between functions, which have a 64KB limit, equating to 32KB of UTF-16 string data. If the serialised function input or output goes over this limit, currently the process will fail, often in quite odd and tricky to diagnose ways. As such currently I‚Äôm taking care to serialise and GZip compress the inputs and outputs, pass them as a string, and decompress and deserialise on the other side.
This is something the functions team are aware of ‚Äî nicely, much of the development is out in the open on Github and can be followed via issues and commits ‚Äî and hopefully in future this compression or use of an alternative storage mechanism will become something the platform takes care of and is also abstracted away from the developer. But for now, it‚Äôs something to keep an in mind.
Here‚Äôs the orchestration function:
The orchestration function, having received input, then creates multiple Tasks, each hooked up with an activity function instance being passed it‚Äôs appropriate input for a given year. Once all primed, they are triggered in parallel and the code waits for all to complete. When they all have, the final activity function is called to process the output. Note in particular here how we‚Äôve replaced all the calls to table storage for handing intermediate output with the single await Task.WhenAll(parallelTasks) line.
The two activity functions looks like this:
Over the last few months we‚Äôve been making use of Azure Functions and other server-less offerings for implementing a number of supporting, auxiliary application components for the web applications we are building. With a few provisos ‚Äî not unexpected in a new and evolving platform ‚Äî we‚Äôve been able to make good use of them, partly replacing the more traditional console apps or Windows services we might have previously employed for similar tasks. The introduction of the durable functions extension just makes that all a bit more straightforward, particularly when it comes to the more complex workflows.
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
115 
115¬†
115 
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
"
https://medium.com/hackernoon/kubernetes-adventures-on-azure-part-1-e0f68b486679?source=search_post---------95,"There are currently no responses for this story.
Be the first to respond.
This is the first article of a series of 3:
In the last month I read 3 awesome books around Kubernetes:
Now it‚Äôs time to start adventuring in the magical world of Kubernetes for real! And I will do it using Microsoft Azure.
Microsoft Azure offers a ready to go Kubernetes solution: Azure Container Service (ACS). It seems easiest way to test a Kubernetes cluster on Azure, if we don‚Äôt consider the new Azure Container Instance. It hides Kubernetes behind the scenes leaving you with simple deployments of containers that will be charged by cou, by memory and moreover by seconds!
Let‚Äôs try ACS! But first I want to highlight its current limits immediately so that you are aware of them:
Easiest way to start our ACS journey is following ‚ÄúDeploy Kubernetes cluster for Linux containers‚Äù that shows a beautiful 4 min to read on top of the page.
Note: It will guide you in using Azure Cloud Shell to create a Kubernetes cluster with Linux only nodes. Personally I installed an used a local Azure CLI following this article from Microsoft. Another article: ‚ÄúDeploy Kubernetes cluster for Windows containers‚Äù will show how to create a Kubernetes cluster with Windows only nodes. This missing hybrid deployment is a limitation for me, because I want to use an hybrid cluster with worker roles with Linux and Windows. But I know for sure that this limitation can be overcome using ACS Engine directly to manually deploy a Kubernetes cluster on Azure (another chapter in my adventure).
Main Steps to install a Linux ACS Kubernets cluster are:
After few minutes your cluster should be up and running with 1 master and 2 nodes, but I had no luck with it at first try.
Failure on step 2 (solved with a second try): on first try of step 2 I received an error, that disappeared on second run of the command, probably due to newly created app credentials in AAD not yet ready to be used. Here detailed error:
Note on step 3 (solved deleting and creating cluster again in another way): this step failed with ‚ÄúAuthentication failed‚Äù error. Maybe due to the fact that there was already an id_rsa file under my user .ssh folder?
Fast solution is deleting the cluster with following command:
and create it again, but this time we will first create an SSH key pair on our own.
From Linux/MacOS you can follow: How to create and use an SSH public and private key pair for Linux VMs in Azure to create an SSH Key pair to be stored on your machine. This is really important and needed to connect to your Kubernetes cluster.
To create SSH key pair run following command and be sure to specify a path to store your key, mine is ~/acs/sshkeys/acsivan:
Note: I changed group and cluster name to avoid conflict with pending deletion of previous groups, that has been perform asynchronously using ‚Äî no-wait argument.
Let‚Äôs try again to create our Kubernetes cluster with following commands (replace ssh key pair path with your one):
If there are no errors in console you are ready to connect to your first Kubernetes cluster on Azure!!! Hurray!
Let‚Äôs run our first kubectl command to check nodes of our cluster:
Wait‚Ä¶ v1.6.6? Latest Kubernets version on 24th August 2017 is 1.7.4. This is another limit of Azure ACS: it‚Äôs not updated on the fly to latest versions.
First of all we will deploy Azure Vote app as described in Microsoft article we are following and then we will run some commands on our cluster to play with it a bit before moving to a Windows cluster.
Now let‚Äôs play a bit on it to test some kubectl commands:
Again a super easy command will lead you to a Dashboard showing your cluster from your browser (I love Kubernetes!)
The best way to reach it is kubectl proxy that should give you an output like Starting to serve on 127.0.0.1:8001
Open a browser to http://127.0.0.1:8001/ui and you will see dashboard running.
Now we can easily delete (and stop paying) everything with this simple command: az group delete --name myAcsTest2 --yes --no-wait
I tested Azure Container Services with a Windows cluster before moving to a full hybrid cluster. You can find details in Part 2.
I will then try to scatter cluster across multiple cloud providers and onpremises location (dreaming‚Ä¶)
#BlackLivesMatter
195 
2
195¬†claps
195 
2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/hackernoon/all-your-containers-are-belong-to-us-deploying-to-microsoft-azure-2e9aa464a113?source=search_post---------96,"There are currently no responses for this story.
Be the first to respond.
Azure Container Instances enables deployment of Docker containers onto Azure infrastructure without provisioning any virtual machines or adopting a higher-level service.
Follow me on Twitter, happy to take your suggestions on topics or improvements /Chris
The year was 1989. The game Zero Ving had just been released in Japan. Little did the creators of the game, Toaplan, know that their game would be legendary in 2019 for the screen you as a user was faced with when losing the game. All your base are belong to us ‚Äî that is, you‚Äôve lost. That‚Äôs of course not the case with containers, you are very much winning using containers and even more so when you bring them to the Cloud.
It becomes more and more common today to develop as well as deliver your application in one or more containers. One of the most common containerization software‚Äôs out there is Docker. It‚Äôs a great tool making it very easy to create image as well as containers and also monitor the same. Wouldn‚Äôt it be great if we could continue using Docker and bring our app to the cloud
In this article we will do the following:
In case you missed the links we are mentioning in this article. Here they are:
Using container technology allows us to split up our application into many services. On top of that, it offers a secure and reliable option to deliver the application. Now comes the next question, where do we deliver it to, On-premise or maybe to the Cloud?
This article is about delivering your application to the Cloud so of course, we are a little bit biased. Let me explain in a few short points why we think the Cloud is a great place for your app:
You will need the following installed
We said initially we would focus more on how to deploy rather than write an application, so for that reason, we are going to use a pre-made application that you can pull down from here:
git clone https://github.com/Azure-Samples/aci-helloworld.git
Looking at it you can see that it is a very simple Node.js application running Express. There are two files of interest in the Repository for the sake of our demonstration:
Let's have a look at the app/index.js file:
Above we can see that it‚Äôs pretty standard Node.js + Express application, no magic here.
Lets now have a look at the Dockerfile:
It does the following:
All in all, this is a pretty standard looking Dockerfile.
Building an image is pre-step we need to do before our application can actually be started. The build step will pull in the OS image we ask for, download dependent library, copy our app code in its place and so on.
We can use the docker build command to build an image. The exact command we will need to use is:
docker build ./aci-helloworld -t aci-tutorial-app
The above command looks for the Dockerfile in the directory /aci-helloworld and creates an image called aci-tutorial-app. Running the command should yield an output looking like this:
Above it‚Äôs showing us all the steps that we set up in the Dockerfile like:
We can see our created image if we run the following command:
docker images
Ok, then, we have an image which means we are ready for our next step; testing it locally.
Now that we have an image, we can create a container from it, using docker run. The full command looks like the following:
docker run -d -p 8080:80 aci-tutorial-app
Let‚Äôs look at the arguments:
We can see that the external port is 8080, which means we can navigate to
http://localhost:8080 to ensure our application works.
This is the image we get, so I would say our container is working:
With the following command we can list all the running containers:
docker ps It should present the following result:
We don‚Äôt want a container running and using up resources so let‚Äôs shut it down. We want to run the command docker kill to shut down the container, however, that command needs an argument, it needs the container id. Remember when we run docker ps ? The first column was our container id. We don't need the full id though, it suffices with the 4 first characters. So let's kick off our command
docker kill [container id, 4 first characters] docker ps // it should be an empty list That‚Äôs it. Here is a screen dump of the commands we just ran:
Azure Container Registry is your private Docker registry in Azure.
We need Docker, Docker Engine and Azure CLI for this to work. We have already installed Docker at this point so let's see how we can install Azure CLI:
https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest
Before we can create said registry we will need a Resource Group. A Resource Group is a logical container in which we need to place all our resources like applications, databases and now Resource Group. Everything in the same group can easily and securely communicate.
so let‚Äôs create that first:
Once this Resource Group is created we can go back to creating our Container Registry.
The command looks like the following:
Let‚Äôs break it down a bit.
az acr create
Is the actual command to create our Container Registry. Then we need some arguments:
You should get an output looking like the following:
The important part is getting a provisionState back with value Succeeded.
We need to log in to our registry before we can push docker images to it. So let‚Äôs log in:
That should tell you Login Succeeded if all is well
Your output should look something like this:
Above you can see that I opted to call the registry chriscontainerregistry put you would have to replace that with your chosen name.
To push a container image to a private registry like Azure Container Registry, you must first tag the image with the full name of the registry's login server.
That‚Äôs something you can find out by looking at the JSON output when you created your registry. You are looking for a property called ""loginServer"". It has the format of [your registry name].azurecr.io. In my case, that would be chriscontainerregistry.azurecr.io.
So either you remember the name of loginServer, from when we created our container registry or you can always retrieve the loginServer later by calling this command:
This will give us the loginServer name printed in our terminal. Of course [container registry name] would in our case be the value chriscontainerregistry, so adjust accordingly depending on your chosen name.
Let‚Äôs now head back to Docker. We need to Tag the aci-tutorial-app image with the loginServer of your container registry.
We tag it with the following command:
docker tag aci-tutorial-app /aci-tutorial-app:v1
Let‚Äôs break it down.
So the correct command in our case, using the correct values would be:
docker tag aci-tutorial-app [container registry name].azurecr.io/aci-tutorial-app:v1
Run docker images command at this point, to verify it was correctly created. It should look something like this:
Now we can actually push the image to the repository. We do so by executing the following command:
docker push /aci-tutorial-app:v1
and with all the correct values in place, it would be:
docker push chriscontainerregistry.azurecr.io/aci-tutorial-app:v1
You may need to log in first, in which case you run the following command:
az acr login ‚Äî name [container registry name]
Carrying out the docker push should render the following result:
Ok, so now we actually want to see what images we have in there, spoiler there should be the one we just uploaded ;)
We can run the following command:
Using the correct value for acrName it would look like this:
There it is, our only pushed image :)
Now that we have our image in the repository, we can tell the repository to create a container from our image and thereby deploy our application.
To run our deploy command we first need a little info, namely the following:
This will return the password
Ok, now we come to the deploy command, that might look a little bit intimidating:
There are a ton of ways to create a container, if you are interested in other ways, have a look at this link az container create
If it takes a while to deploy you can check status meanwhile, with this command:
After a very long JSON response back, look for provisioningState: Succeded, if you have that you are good.
Let‚Äôs have a look at our container with the following command:
We can see the logs from the app by running:
This will tell us running on port 80
Once it‚Äôs deployed we can visit the app on the --dns-name-label value, like so:
We set out to deploy an app. This time we wanted to deploy a docker container. For that, we first needed to create a docker image. So we created one using docker build.
Then we realized we needed an Container Registry, cause it was from there we would deploy our image, i.e instantiate a docker container and deploy it.
To make it end up in the Container Registry we first needed to tag it with the loginServer name, after that we pushed the tagged image.
Lastly, we told the Container Registry to create a container from our image and deploy it. Once deployment was done we could go to our browser and verify the app was there, success :))
It wasn‚Äôt that many steps really. I mean let‚Äôs say our app consisted of 3 other services. We would only need to build an image for each, tag it, push, and create a container.
#BlackLivesMatter
138 
138¬†claps
138 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Google Developer Expert. Cloud Developer Advocate at Microsoft, https://twitter.com/chris_noring
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/awesome-azure/azure-application-security-group-asg-1e5e2e5321c3?source=search_post---------97,"There are currently no responses for this story.
Be the first to respond.
Introduction to Application Security Group (ASG) in Azure ‚Äî What is ASG?
Application Security Groups helps to manage the security of Virtual Machines by grouping them according the applications that runs on them. It is a feature that allows the application-centric use of Network Security Groups.
"
https://medium.com/awesome-azure/azure-vs-aws-difference-between-azure-virtual-network-vnet-and-aws-virtual-private-cloud-vpc-2e8debc3290e?source=search_post---------98,"There are currently no responses for this story.
Be the first to respond.
Comparison: Azure Virtual Network (VNet) vs AWS Virtual Private Cloud (VPC)
The journey to the cloud begins with choosing a cloud provider and provisioning private networks or extending their on-premise network. Customers looking to provision their resources in the cloud can choose from the‚Ä¶
"
https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429?source=search_post---------99,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manikanta Yadunanda
Dec 16, 2017¬∑6 min read
In this post, I will go through the happy path for setting up a deep learning machine in Microsoft Azure. This will be beneficial for people who want to set up a deep learning machine easily on Azure(Without going through all the graphics driver, CUDA, and deep learning libraries installation).The latter part of this post will be targeted for setting up fast.ai course content and libraries on Azure. I am part of the fastai deep learning 2017 course as an international fellow. This course content will be available to the general public early 2018 as a MOOC.
Virtual machine images contain all the required environment for various tasks. One can deploy them and get up and running straight away. Amazon AWS has AMI for this purpose. Similarly Azure offers virtual machine images.
The one we are interested in is Azure Deep Learning Virtual Machine(DLVM) part of the Azure Data Science Virtual Machines(DSVM). It was released mid-2017. Please go to the below page to get an idea of what it offers.
azuremarketplace.microsoft.com
In short, it has almost all the tools required to get you started in deep learning. We will refer it as VM for the rest of the article.
If you have a visual studio subscription then I have a good news for you. At the time of this writing, you can enable the benefit of monthly Azure credits. You can use those credits for your VM costs. You have to select this subscription while creating your virtual machine.
Please visit this link for more details and how to activate the benefit.
Note: Virtual machine pricing depends on region and instance types. You are advised to refer here for proper pricing information.
1. Azure Deep Learning Virtual Machines can be only deployed on Azure NC series virtual machines. By default, it‚Äôs not enabled( at least for me). So one needs to raise a support request to get them enabled on your ID.
2. Please note this is only needed if you observe the NC series options are greyed out during the process of our deployment. You might see a message like ‚ÄúNot available for the current subscription‚Äù
3. It may take some time depending on the availability of NC series machines.
4. Once NC series are enabled on your ID, you can easily create your VM.
You can manage your machines from the virtual machines tab.
You can then select the VM you want, to open a detailed view of that VM.
Using the above tab, you can control your VM. ‚ÄúConnect‚Äù will show the details required for SSH login to your instance. ‚ÄúStart‚Äù will help to boot up your VM whenever you want to use it. And make sure you use ‚ÄúStop‚Äù when you are not using it. You will be billed for the time your VM is in running state.
You can connect to your instance using ssh with the below command.
ssh -L 8888:127.0.0.1:8888 username@ip
-L 8888:127.0.0.1:8888 is used for SSH tunneling which helps us to connect to our jupyter notebook server on our remote VM from our local browser.
username@ip replace this with your details from ‚ÄúConnect‚Äù tab which was shown in the previous image.
You will be prompted for the password. Upon completion, you will have the terminal of your VM. You can play and move around to see the examples and tutorials this VM is loaded with to get you started.
As jupyter is already installed in our VM. You can straight away run the jupyter server to serve you notebooks. To start the jupyter server, you can run the following command.Note that instead of running the server from notebooks you can run from any folder location from which you want your notebooks to be served.
You should see something like the below image on the screen.
Copy the URL+Token from the command line and enter in your local browser to view the jupyter notebooks. You can play with the sample notebooks which are already present.
That‚Äôs it for the setup and should be enough to get you going :)
Note: All the above information is what I observed at the time of writing this post. These things might change in future and some of them may be wrong. You are advised to always refer to the official documents.
pip install ipykernel
python -m ipykernel install ‚Äî user ‚Äî name fastai ‚Äî display-name ‚ÄúPython (fastai)‚Äù
4. You can keep any name you like for the kernel instead of ‚ÄúPython (fastai)‚Äù.
5. The above steps would add fastai kernel to jupyter notebooks.
6. You can change the kernels of your notebook as shown in the following image.
7. With the above kernel, you will be able to run your fastai notebooks.
8. Let‚Äôs say for Deep Learning Part1 V2, you can change your directory to fastai/courses/dl1/ on your cloned repository.
9. Launch jupyter notebook from there as explained earlier and change your kernel to fastai kernel. You can then run all the lessons notebooks and experiment with them.
Fastai package
The fastai library is also available through pip. And you can install it using ‚Äúpip install fastai‚Äù as mentioned in the official readme. This will help if you want use fastai as a library in any of your environments or projects.
Update for fast.ai 2018 course
All the above mentioned steps remain same except for fast.ai installation.The github readme has updated instructions.
Happy Deep Learning :)
iOS Developer, Machine Learning Enthusiast, https://www.linkedin.com/in/manikanta-sangu-88292386
175 
9
175¬†
175 
9
iOS Developer, Machine Learning Enthusiast, https://www.linkedin.com/in/manikanta-sangu-88292386
"
https://towardsdatascience.com/azure-synapse-analytics-as-a-cloud-lakehouse-a-new-data-management-paradigm-cdcbe2378209?source=search_post---------100,"Sign in
There are currently no responses for this story.
Be the first to respond.
Korkrid Akepanidtaworn (Kyle)
Jul 2, 2020¬∑4 min read
In the early days of data repository, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a commonly known system used for reporting and data analysis, and is considered a core component of‚Ä¶
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sahityamaruvada/spinning-up-instances-with-terraform-in-azure-and-aws-38f251d462e8?source=search_post---------101,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sahitya Maruvada
Jul 18, 2019¬∑5 min read
When I first started working on Terraform with a little knowledge of AWS and Azure there were several blog posts and of course the official documentation to aid me throughout the process. This article aims to be a beginner-friendly guide giving information on what resources are required to create a virtual machine in a crisp and easy manner. The goal of this post is to give a basic idea of‚Ä¶
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/seed-digital/using-microsoft-azure-identity-with-firebase-in-a-react-native-app-c9eef0fd0af8?source=search_post---------102,"There are currently no responses for this story.
Be the first to respond.
[UPDATE] We‚Äôve updated the library with an example using Expo, if you want to see it running check out the Github
Authenticating users into your application is quite a bread and butter requirement when developing an app.
Most app users are familiar with the idea of signing into an application using an existing service like Facebook or Google OAuth, and sometimes this can be the best option when a new application requires an account setup.
Leveraging existing user data from an OAuth service can have many advantages for both users and developers when integrating with an application:
Here at Sheda we tend to use Firebase and React Native for most of our mobile development projects.
Firebase comes with some handy features such as real time-database service, user authentication and file storage which means our development process can move at a super-fast pace.
Which in turn, means that we get products to market at a faster rate and get that all-important customer feedback quicker, resulting in faster iterations and a better overall product for users.
The Firebase user authentication service allows us to integrate sign-in services from major social media platforms, like Facebook and Twitter, into the application with a few lines of code.
With this capability, our developers can focus on its core functionality instead.
Firebase authentication supports many service providers, but unfortunately, Microsoft Account is an exception.
While this is a bummer, Firebase does still allow you to register new users with a custom token which means we can handle the login mechanism by ourselves and save Microsoft Identity user credentials to Firebase for later use.
We‚Äôll explain how to get this working in your react native app.
Trying to access a Microsoft account is kind of bizarre.
Microsoft previously separated their user accounts into two different domains ‚Äî one for their cloud platform Microsoft Azure and another for general users who are using their services like Hotmail, One Drive or Xbox.
This meant developers had to use different authentication endpoints in order to authenticate users from different services.
Thankfully, they recently converged their disparate authentication service into a single service called ‚Äòv2.0 endpoint‚Äô which allows you to use OAuth authentication for whichever Microsoft service account you have.
Authenticating a user via the v2 endpoint will give us access to a custom bearer token, this token allows us to consume REST APIs from the Microsoft Graph (a single endpoint into all Microsoft services) and allows your app to request simple user data, for example, first name, last name, email and get other information like email messages, contacts and notes associated with their accounts.
As I have stated earlier, Microsoft used OAuth 2.0 protocol for user authentication.
If you are familiar with OAuth 2.0 you may skip this part. Otherwise, let‚Äôs see how this thing works to have a clear understanding of what we are going to implement.
The authentication process gets started from a web browser (WebView in this case) requests a login page from the Microsoft authentication server (http://login.microsoftonline.com/common).
In this step, when requesting to the server, we need to equip our application identification as a query string to allow the Microsoft server to identify which application we are using.
After a user enters their credentials and goes through the general login process, the Microsoft server will redirect a user back to the URL that was provided when registering an application with a temporary code, which is a crucial piece of data that we need to use in order to obtain the access token for using to consume REST APIs.
The last step in this part would be requesting an access token. Once we have a temporary code from the last step, we have to use it as a part of the payload to gain an access token.
To allow the application to interact with the Microsoft server, it needs to be registered through Microsoft Application Registration Portal. (how to register an app in Azure)
Once the app is registered, an Application ID will be generated.
In order to get the authentication to work on React Native, without using other third-party libraries, we decided to utilize React Native WebView component.
Consequently, we are going to need a secret key for communicating with the Microsoft Authentication service through the web interface.
Moving down to the Application Secrets section, click on the ‚ÄòGenerate New Password‚Äô button to get the secret key generated.
It will take two to three seconds before the secrete key will be shown on screen. Keep in mind that, it will be shown just only once ‚Äî so keep it in a safe place.
Everything seems to be pretty good, but one more thing we have to tell Microsoft is which platform we are going to integrate this app on.
Move down a little bit into a Platforms section and click on ‚ÄòAdd Platform‚Äô.
The ‚ÄòAdd Platform‚Äô dialog will be prompted, in this case we will go with Web platform as we‚Äôll be using JavaScript to access the service.
Then, define a callback URL. Make sure that it‚Äôs using HTTPS protocol if you are going to integrate this application with iOS to avoid problems that may occur.
Once everything is set, scroll down to the bottom, hit ‚ÄòSave‚Äô and we are done with the Application Register on the Microsoft Application Portal.
React Native is a JavaScript library developed by Facebook. It allows you to develop native applications for both iOS and Android by using JavaScript.
React Native has its origins and shares the same syntax and other design considerations as ReactJS, which is the web application version of the library.
One of the main design paradigms for React apps is to split each component of the application into independent, reusable pieces and think about each piece in isolation.
This allows components to be reused across a whole application where appropriate.
Apart from creating all components ourselves, developers can also integrated the other components from around the world created by other developers into their application through a package manager like NPM or Yarn.
Since we are going to reuse the Microsoft authentication service with our future projects as well, we have decided to create a separate NPM module called ‚Äòreact-native-azure-ad-2‚Äô.
The module contains three main components where:
As shown in the diagram:
After implementing the component, using it is quite easy.
First, import the ‚Äòreact-native-azure-ad-2‚Äô component.
Then create an AzureInstance by using Microsoft application credentials that we registered above.
Also, add application scope in order to ask users to consent when they login.
For more information about scope visit Microsoft blog.
After that, return AzureLoginView where you want the login WebView to be rendered and pass along with azureInstance that we have created from the last step.
With AzureLoginView, we can have a few prop options to render the view differently.
That should make it easy to integrate Microsoft Azure authentication into your React Native app. If you encounter any issues drop us a line in the comments below or on Github.
Want some help building your technology startup, growing your business and creating products/services that your customers will love? Contact Sheda and let‚Äôs see how we can help.
We use design and technology to inspire creation and‚Ä¶
176 
5
176¬†claps
176 
5
We use design and technology to inspire creation and purpose‚Ää‚Äî‚Ääcreating a future we‚Äôd like to live in. These are our ideas and musings on design, technology, business and Industry 4.0 from the crew at Sheda.
Written by
A product development & design firm. We use design & technology to help purpose-driven companies & entrepreneurs to create the future we all want to live in
We use design and technology to inspire creation and purpose‚Ää‚Äî‚Ääcreating a future we‚Äôd like to live in. These are our ideas and musings on design, technology, business and Industry 4.0 from the crew at Sheda.
"
https://medium.com/microsoftazure/5-microsoft-learn-modules-for-getting-started-with-azure-cf948c8c3caf?source=search_post---------103,"There are currently no responses for this story.
Be the first to respond.
Microsoft Learn is an free interactive sandbox for learning new skills. With over 62 Azure modules and counting, it can be hard for a new comer to know where to get started.
I was recently asked for my perspective as a Cloud Advocate, from some start ups, in my local tech ecosystem. Below is a list of 5 recommended Microsoft Learn Modules for getting up to speed with Azure Services.
I recommend you try them out in the following order:
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
If you have any questions, comments, or topics you would like me to discuss feel free to follow me on Twitter if there is a milestone you feel I missed please let me know.
Aaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
44 
44¬†claps
44 
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/blueteamlabs/using-sysmon-in-azure-sentinel-883eb6ffc431?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
Over the last couple of nights I've been playing with Azure Sentinel to see how useful it will be as a SIEM/Hunting platform. One of the fist things I wanted to do is onboard Sysmon data. Unfortunately the documentation isn't up to par yet and it took me a LOT of time and some help from Kevin Beaumont, @ashwinpatil and Maarten Goet to get this working. Thanks guys!
For instance the ""Security and Audit"" Solution has a SysmonEvent schema, this one is broken however. In order to save you the same struggle I'll give a brief outline here.
First of all you'll need to connect machines, this is relatively straight forward. Then you need to start ingesting some data;
Now it is ready to start ingesting events, to configure which ones go to ""Workspace Settings"" and then to ""Advanced Settings"". Now start adding the Data sources you require.
By now the data should be flowing into your instance. You can check this by going to Logs and use the following KQL query;
Sadly these events are unparsed. There are two options now, parse them by hand, which I don't recommend ;). Or parse them by creating a function and use that in your future queries.
Fortunately the Azure team loves Sysmon like I do and they were so kind to provide a parsing KQL over here.
I have created a OSSEM mapped equivalent of that one, since I like the consistency in the field names, I also use it in my Splunk ThreatHunting app. This parsing KQL version is available here.
Copy the entire contents of the file and paste it in an empty query box, next click Save on the top right. Choose a name and alias name and save it as a function.
In some cases it can take a few minutes to become available, make sure to remember this :D It caused me some annoyance when testing this for the first time. So next test your new function in a new query window.
And there you go, properly parsed events! The only slight inconvenience is that the syntax autocomplete function is not working since these field names are not part of a Schema, there is no way known to me to address this at this moment. Given you know what you're looking for most of the time, you'll be fine without.
Blogging platform for the BlueTeamLabs GitHub repository
74 
1
74¬†claps
74 
1
Written by
FalconForce | DFIR | Threat hunter | Data Dweller | Splunk | Sysmon | Microsoft MVP
Blogging platform for the BlueTeamLabs GitHub repository
Written by
FalconForce | DFIR | Threat hunter | Data Dweller | Splunk | Sysmon | Microsoft MVP
Blogging platform for the BlueTeamLabs GitHub repository
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@auchenberg/introducing-remote-debugging-of-node-js-apps-on-azure-app-service-from-vs-code-in-public-preview-9b8d83a6e1f0?source=search_post---------105,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kenneth Auchenberg
May 17, 2018¬∑5 min read
Finding and identifying issues with Node.js apps deployed to the cloud can be burdensome process that usually involves local debugging, the sprinkling of console.logson to your codebase, and many re-deployments to get the problem identified and solved.
Today we are changing that, as we are introducing a public preview of remote debugging for Node.js apps deployed on Azure App Service for Linux. Our remote debugging experience brings you the same great debugging experience that you already know from Visual Studio Code when debugging Node.js locally to the Azure Cloud.
Remote debugging for Azure App Service works by taking advantage of a new Azure mechanism that allows us to forward the remote debugging information from your Node process running in Azure to your local computer in a secure way. Once the connection is established we can take VS Code‚Äôs built-in Node.js debugger to attach and debug ‚Äî just like if you were debugging your Node app locally!
You can read more about how it works, and other Azure App Service announcements here on the Azure blog.
To get started you first have to prepare your Node.js app running in Azure and setup Visual Studio Code. Let me show you how.
This section assumes that you already have your Node.js app deployed to Azure using Azure App Service for Linux. If you haven‚Äôt deployed your app you can get started here or using the Azure CLI here.
In order to enable remote debugging when your app is running in the cloud, you might need a make a slight modification to your Node app, depending on how you start your app.
Per our default logic for Node Apps on Azure, we‚Äôll try to auto-detect how you start your Node app, if you use bin/www server.js app.js hosting.js or index.js as your main script file. If that‚Äôs the case, there‚Äôs some good news: You don‚Äôt have to change anything!
If you are using NPM scripts to start your Node app, you need to make a slight modification to how you start your app, as we won‚Äôt try to fiddle with your app. You know best how to launch your app, so for this public preview, we are asking you explicitly to create a new debugging specific NPM script that runs your Node app with --inspect=0.0.0.0:$APPSVC_TUNNEL_PORT.
This runtime flag tells your Node app to start in debug mode listening on the debugging port specified by Azure, which is exposed as an environment variable.Your scripts section in your package.json should look something like:
2. The next step is to go to Azure Portal, and find your deployed Node.js app on Azure App Service (Linux).
4. Go to Application settings and update your Startup File to your newly configured npm run start_azure_debug script.
It should look something like:
Your Node app is now figured to run with remote debugging enabled.
Yay! üéâüî•
Next is to get your Visual Studio Code setup going, which is an easy process:
3. If you haven‚Äôt logged into Azure from VS Code, you should now click the Azure icon in the sidebar to the left, and login to see your App Service apps.
4. Since remote debugging is a preview feature, you now have to go to your VS Code settings to enable it. You do this by clicking File > Preferences > Settings. modify your appService.enableRemoteDebugging to be true.
5. Now that you have remote debugging enabled, the next step for you is to open the source code for your Node app as your workspace in VS Code.
6. Once you have the source code open next is to find your deployed Node app in the Azure App Service list within VS Code.
Right click and select the new ‚ÄúStart Remote Debugging‚Äù option.
Once clicking on ‚ÄúStart remote debugging‚Äù, Visual Studio Code will check if remote debugging has been enabled for your app, and if it hasn‚Äôt you will be asked to confirm before enabling remote debugging on your behalf.
Once the right configurations has been set, you should now see remote debugging for your Node.js app being started (you can follow the progress in the status bar) and once the debugger is connected VS Code will enter debug mode.
Notice: You might get a prompt from your firewall on Windows, please allow the connection
Bam. That‚Äôs it. üéâüî•
The debugger is now connected, and you can remote debug your Node.js app running in Azure! Try set a breakpoint!
In the March 2018 release of VS Code, we introduced a new debugging concept called Logpoints. When combining Logpoints with remote debugging on Azure you have a powerful combination for seamless production debugging!
Try it out and read more about Logpoints here üëâhttps://code.visualstudio.com/updates/v1_22#_logpoints
We are excited to bring you real remote debugging for Node.js apps running on Azure App Service (for Linux) directly from Visual Studio Code. Giving you a simply integrated experience that allows you to easily debug and diagnose problems if they occur when running your Node.js apps in our Cloud.
We‚Äôd love to hear your feedback: Please find us in GitHub to report issues, provide suggestions, or tell us your success stories.
Happy coding!
/k
Developer Experience @stripe / Alum @code , @microsoft , @citrix , @podio , @vodafone , @zyb / @WEF Global Shaper / @coldfrontconf founder / @goog
263 
1
263¬†
263 
1
Developer Experience @stripe / Alum @code , @microsoft , @citrix , @podio , @vodafone , @zyb / @WEF Global Shaper / @coldfrontconf founder / @goog
"
https://medium.com/@renatogroffe/net-core-2-1-nosql-exemplos-utilizando-mongodb-redis-documentdb-ravendb-e-azure-cosmos-db-f9675b80df76?source=search_post---------106,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Aug 20, 2018¬∑4 min read
Neste novo post trago todos os exemplos que implementei anteriormente em artigos e palestras cobrindo NoSQL solu√ß√µes como MongoDB, Redis, DocumentDB, RavenDB e Azure Cosmos DB. As aplica√ß√µes em quest√£o j√° foram convertidas para o .NET Core 2.1 e o ASP.NET Core 2.1, estando dispon√≠veis no GitHub (com os reposit√≥rios correspondentes sendo referenciados nas pr√≥ximas se√ß√µes).
Esta √© uma atualiza√ß√£o complementando outro artigo que publiquei anteriormente sobre o uso de tecnologias NoSQL com .NET Core 2.0 e ASP.NET Core 2.0:
.NET Core 2.0 + NoSQL: exemplos utilizando MongoDB, DocumentDB e Redis
E por falar em tecnologias Microsoft, n√£o deixem tamb√©m de acompanhar o Azure Tech Nights, que acontecer√° entre os dias 20 e 28 de Agosto de 2018. Ser√° um evento NOTURNO, ONLINE e GRATUITO promovido pelo Canal .NET, com apresenta√ß√µes focadas no Microsoft Azure e cobrindo temas como microsservi√ßos, Intelig√™ncia Artificial, desenvolvimento Web, bancos de dados, NoSQL, Docker, Kubernetes e muito mais.
Entre os palestrantes teremos MVPs Microsoft, MTACs e Especialistas de Mercado.
Para efetuar a inscri√ß√£o acessem este link.
A grade com as palestras e outras informa√ß√µes podem ser encontradas no site oficial do Azure Tech Nights.
O MongoDB foi tema de um Live Demo do Canal .NET durante o ano de 2017:
Os exemplos de uso do MongoDB em .NET Core 2.1 e ASP.NET Core 2.1 est√£o no seguinte endere√ßo:
https://github.com/renatogroffe/MongoDB-DotNetCore2.1
Ao consultar o reposit√≥rio MongoDB-DotNetCore2.1 voc√™ encontrar√° os seguintes projetos:
O uso de Redis com ASP.NET Core j√° foi abordado em um evento online do Canal .NET sobre performance em APIs, em que demonstrei a implementa√ß√£o de cache distribu√≠do empregando esta solu√ß√£o NoSQL:
A demonstra√ß√£o no caso foi baseada no conte√∫do apresentado no seguinte artigo:
ASP.NET Core 2.0: implementando cache em APIs REST
O pr√≥ximo artigo abordou o uso de Redis em conjunto com Docker, Docker Compose e ASP.NET Core, atrav√©s de um exemplo de implementa√ß√£o de uma solu√ß√£o multi-container e que emprega uma API REST gratuita da Marvel Comics para a apresenta√ß√£o de informa√ß√µes (armazenadas em cache em uma inst√¢ncia do Redis):
ASP.NET Core + Docker Compose: implementando solu√ß√µes Web multi-containers
Uma demonstra√ß√£o envolvendo o projeto descrito no artigo foi tamb√©m realizada em um evento online do Canal .NET sobre Docker Compose:
Este √∫ltimo exemplo j√° foi convertido tamb√©m para o ASP.NET Core 2.1:
https://github.com/renatogroffe/ASPNETCore2.1_Docker-Compose
Um ponto importante deve ser destacado quanto ao uso de cache com Redis no ASP.NET Core 2.1. Devido √† mudan√ßa para o metapackage Microsoft.AspNetCore.App o package Microsoft.Extensions.Caching.Redis dever√° ser adicionado a um projeto a fim de tornar poss√≠vel este tipo de implementa√ß√£o:
Solu√ß√£o NoSQL multi-model que integra o Azure, o Cosmos DB √© um servi√ßo de alta disponibilidade que permite a utiliza√ß√£o de tecnologias como MongoDB, DocumentDB, Cassandra e Gremlin a partir da nuvem da Microsoft.
O Azure Cosmos DB foi inclusive tema de uma apresenta√ß√£o que fiz na edi√ß√£o 2018 do TDC S√£o Paulo. Seguem os slides descrevendo as principais caracter√≠sticas deste servi√ßo:
E foi tamb√©m abordado durante a 2a. edi√ß√£o do Azure Tech Nights:
Alternativa NoSQL da Microsoft e orientada a documentos, o DocumentDB se destaca pela possibilidade de uso de SQL (por meio de um conjunto limitado de instru√ß√µes) na consulta √†s informa√ß√µes de uma cole√ß√£o.
O site a seguir traz exemplos e permite inclusive a realiza√ß√£o de testes envolvendo o uso de SQL para a consulta a uma cole√ß√£o do DocumentDB:
Azure Cosmos DB Query Playground
Para consultar os exemplos de uso do DocumentDB com .NET Core 2.1 e ASP.NET Core 2.1 acesse:
https://github.com/renatogroffe/DocumentDB-DotNetCore2.1
No reposit√≥rio DocumentDB-DotNetCore2.1 se encontram projetos similares √†queles apresentados na se√ß√£o sobre MongoDB:
Vale destacar ainda que o DocumentDB conta com um emulador local, cuja instala√ß√£o pode ser obtida a partir do seguinte link (h√° uma alternativa a isto sob a forma de uma imagem no Docker Hub):
Use the Azure Cosmos DB Emulator for local development and testing
A seguir est√° um exemplo de consulta atrav√©s da interface disponibilizada para gerenciamento do emulador local de DocumentDB:
Tamb√©m orientado a documentos, o RavenDB √© uma alternativa NoSQL implementada em .NET, conta com uma vers√£o Community e possui a capacidade de suportar transa√ß√µes ACID (comportamento este t√≠pico de bancos relacionais). Al√©m disso √© multiplataforma, o que possibilita seu uso em ambientes como Windows, Linux, MacOS, containers Docker e Raspberry Pi.
Uma Solution envolvendo carga de um cat√°logo de produtos e servi√ßos, bem como uma API REST de consulta a tais dados, foi disponibilizado no seguinte reposit√≥rio do GitHub:
https://github.com/renatogroffe/RavenDB-DotNetCore2.1
O RavenDB juntamente com o MongoDB, o Redis, o DocumentDB e o Azure Cosmos DB foram tamb√©m tema de uma apresenta√ß√£o online que realizei pelo canal Developers-BR e cuja grava√ß√£o j√° est√° dispon√≠vel no YouTube:
.NET Core 2.1 e ASP.NET Core 2.1: Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
93 
93¬†
93 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://javascript.plainenglish.io/scan-your-code-for-vulnerabilities-with-azure-devops-tools-7db80955d4fe?source=search_post---------107,"Loopholes are everywhere. Software systems built by software engineers are no exception and we all know them as vulnerabilities. Usually, the severity increases at an exponential rate. So, if one vulnerability is not fixed it might lead to other vulnerabilities.
Ideally, we, as software engineers, want to develop software systems that are free of vulnerabilities but that‚Äôs not always the case. We do have vulnerabilities in real-world software.
To identify them, we need to scan our code by using different tools. These tools might be manual or can be integrated into a DevOps pipeline to automatically scan the code before deployment.
Today, we are going to review some of the tools that can be used for scanning the code in an Azure pipeline. Here they are.
SonarCloud is another leading online tool for code security that can be integrated into the Azure DevOps pipelines. SonarCloud supports nearly all major programming languages like JavaScript, TypeScript, C/C++, C#, VB .Net, and is free for open-source projects too.
However, If you have some closed source code then SonarCloud has a subscription fee to analyze private repositories.
With SonarCloud, you will have the advantage of automated detection of vulnerabilities and bugs across all pull requests and branches. It can also provide build tasks that you may integrate into your build definition.
However, SonarCloud gives detailed logs of all issues related to code, giving you the chance to fix them even before merging and deployment. When you are working at the project level, you will also get detailed tracking of the overall health of your application by the virtue of a dedicated widget.
The AWS Toolkit for Azure DevOps enables you to add tasks to easily build and release pipelines in Azure DevOps to seamlessly work with the vast array of AWS offerings that include AWS CodeDeploy, AWS Elastic Beanstalk, Amazon S3, AWS Lambda, Amazon Simple Queue Service, Amazon Simple Notification Service, and AWS CloudFormation.
With AWS Toolkit, you can also run commands using both AWS CLI and Windows PowerShell AWS Tools. It is available for free in the Azure DevOps category at the Visual Studio Marketplace.
Since AWS Toolkit is an open-source project, you can contribute to it, improve its docs or make a feature request too.
Ado Security Scanner is another open-source tool for code scanning in Azure DevOps pipelines by Microsoft DevLabs. This tool is specifically designed to assist organizations to manage secure Azure DevOps pipelines with the help of built-in ADO dashboard widgets through continuous scans and visualization of security issues and problems.
It assists you in keeping your Azure DevOps artifacts such as project/org settings, build/release configurations, agent pools, service connections, etc. configured securely. Also, you can run it in an Azure DevOps pipeline via a marketplace extension or maybe separately in a PowerShell console.
It manages different components of Azure DevOps and scans them like Builds, Releases, Organization, Service Connections, User, etc.
Beagle Security is an intelligent, automated, and holistic tool to make your applications free of vulnerability-related problems and issues. It provides continuous penetration testing that requires some human supervision for organizations so that they can remain safe from cyber threats and exploits.
It finds the loopholes in your application before the real hackers do. The advantage of using Beagle Security is that it allows you to perform penetration testing of web applications with both SANS and OWASP standards.
You can also download the reports in PDF and other user-friendly formats. Alongside that, you get complete support from their team of experienced security researchers in the whole process.
In a nutshell, there are many tools that you can use for scanning your code for vulnerabilities. If you are looking for comprehensive tools that work well with both open source and closed source projects, then you can try any of the above tools mentioned in this list.
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
67 
67¬†claps
67 
Written by
I help developers to be better engineers! üíª Software Engineer | üìà Data Science | üíº Entrepreneurship | üß† AI | üñãÔ∏è Writer at DEV with 250K+ views.
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
Written by
I help developers to be better engineers! üíª Software Engineer | üìà Data Science | üíº Entrepreneurship | üß† AI | üñãÔ∏è Writer at DEV with 250K+ views.
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
"
https://medium.com/@mauridb/powerbi-and-azure-databricks-2-d0ed16427d36?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
Feb 15, 2018¬∑3 min read
Just a couple of days after I published the article that describes how to connect to Azure Databricks with PowerBI via the ODBC connector, I received an email from friends (Yatharth and Arvind) in the Azure Databricks and AzureCAT team that told me that a better and easier way now available to connect PowerBI to Azure Databricks was possible.
Not only this new way is way simpler and lightweight, but it also enables usage of DirectQuery to offload processing to Spark, which is perfect when you have a real huge amount of data, that doesn‚Äôt make sense to be loaded into PowerBI, or when you want to have (near) real-time analysis.
The PowerBI connector to be used to go this way is, as you may have guessed, the Spark connector:
Configuring the connector, compared to all the setup needed with ODBC, is really a breeze. All you have to specify is the server and the protocol. Protocol must be set to HTTP:
Setting the server is just a bit tricky. Information can be found in the JDBC/ODBC pane available in the Configuration page of your Azure Databricks Spark cluster:
The url needs to be constructed following this procedure:
The final url would be:
https://eastus.azuredatabricks.net:443/sql/protocolv1/o/6132794369297039/0214-200557-viols339
Now just insert it into the Server textbox of PowerBI Spark connector configuration window, as mentioned before, and you‚Äôre almost done.
Credentials are issued and managed via the Personal Token, exactly as explained in the ODBC article. So just insert the token user name (remember that you really have to literally insert ‚Äútoken‚Äù as username) and then the generated token as password:
After that it will just works, now also with DirectQuery:
Here a glimpse of what‚Äôs happening behind the scenes, with the SQL queries sent to Spark when DirectQuery is used:
Official documentation will be updated soon, in the meantime, for all those like me who are eager to play with Azure Databricks and PowerBI, I hope this helps.
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
See all (156)
82 
5

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
82¬†claps
82 
5
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/azure-kubernetes-service-aks-observability-with-istio-service-mesh-4eb28da0f764?source=search_post---------109,"In the last two-part post, Kubernetes-based Microservice Observability with Istio Service Mesh, we deployed Istio, along with its observability tools, Prometheus, Grafana, Jaeger, and Kiali, to Google Kubernetes Engine (GKE). Following that post, I received several questions about using Istio‚Äôs observability tools with other‚Ä¶
"
https://medium.com/@renatogroffe/asp-net-core-azure-kubernetes-orquestra%C3%A7%C3%A3o-de-containers-na-nuvem-parte-2-6c922daeadab?source=search_post---------110,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 18, 2018¬∑9 min read
Esta √© a segunda parte da s√©rie em que abordo o uso combinado do ASP.NET Core, do Microsoft Azure e do Kubernetes (atrav√©s do AKS - Azure Kubernetes Service) na implementa√ß√£o de solu√ß√µes que empregam orquestra√ß√£o de containers. As pr√≥ximas se√ß√µes descrevem os passos necess√°rios para a publica√ß√£o de uma imagem Docker no Azure, bem como a cria√ß√£o de um cluster no Kubernetes e o deployment neste √∫ltimo da aplica√ß√£o de testes.
Caso n√£o tenha ainda acessado ou, at√© mesmo, deseje rever o primeiro artigo deste tutorial consulte ent√£o o link a seguir:
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem - parte 1
A imagem Docker utilizada a partir do Kubernetes ser√° gerada atrav√©s do Visual Studio 2017. Acionar para isto na IDE o menu de contexto para o projeto APIContagem e, em seguida, as op√ß√µes Add e Docker Support (este processo tamb√©m pode ser realizado durante a cria√ß√£o de um novo projeto):
Aparecer√° neste momento a janela Docker Support Options. Selecionar em Target OS a op√ß√£o Linux:
Com isto ser√£o gerados o Dockerfile e os arquivos do Docker Compose:
A imagem Docker ser√° gerada com base nas configura√ß√µes de release da aplica√ß√£o. Selecionar para isto a op√ß√£o Release no Visual Studio 2017:
Efetuar na sequ√™ncia a compila√ß√£o do projeto (a partir do menu Build > Build Solution). Ao executar o comando docker images no PowerShell aparecer√° ent√£o a imagem apicontagem:latest:
O Azure Containter Registry permite o armazenamento de imagens Docker de forma privada, representando assim uma alternativa dentro da nuvem da Microsoft ao Docker Hub. Este servi√ßo pode ser empregado em conjunto com tecnologias como Azure Container Service (com seus diferentes orquestradores - Docker Swarm, DC/OS e Kubernetes), Azure Kubernetes Service (AKS) e Azure Web App for Containers.
No portal do Azure ser√° criado um novo recurso baseado no servi√ßo Container Registry:
Informar no formul√°rio de cria√ß√£o:
Ap√≥s alguns segundos o item groffecr (um Container Registry) aparecer√° na lista de elementos que comp√µem o grupo de recursos indicado no passo anterior:
Na pr√≥xima imagem aparecem detalhes deste novo recurso, com o login para conex√£o ao servi√ßo de registro/armazenamento de imagens em destaque:
Uma tag chamada groffecr.azurecr.io/apicontagem dever√° ser criada para a imagem apicontagem:latest. Este novo elemento cont√©m o nome que ser√° gravado no Azure Container Registry (formado pela identifica√ß√£o do registro de containers + nome da aplica√ß√£o/imagem; esses dois itens est√£o separados ainda por uma barra ‚Äî ‚Äú/‚Äù). Executar para isto o seguinte comando no PowerShell:
docker tag apicontagem:latest groffecr.azurecr.io/apicontagem
O pr√≥ximo passo agora consiste em efetuar o login no recurso do Azure Container Registry criado na se√ß√£o anterior. Executar para isto o seguinte comando no PowerShell (em que ser√£o fornecidos o usu√°rio e uma senha disponibilizados pelo Microsoft Azure):
docker login groffecr.azurecr.io -u USU√ÅRIO -p SENHA
As credenciais necess√°rias est√£o na se√ß√£o Access keys do Container Registry (para o exemplo deste artigo foi empregada a senha indicada em password):
Para publicar a imagem groffecr.azurecr.io/apicontagem no Azure Container Registry ser√° utilizado o comando:
docker push groffecr.azurecr.io/apicontagem
A imagem groffecr.azurecr.io/apicontagem aparecer√° ent√£o no Azure Container Registry (se√ß√£o Repositories), logo ap√≥s a conclus√£o deste √∫ltimo procedimento:
Para a cria√ß√£o do cluster que servir√° de base para testes com o Azure Kubernetes Service (AKS) ser√° necess√°rio instalar a vers√£o 2.0 do Azure Command Line Interface (CLI).
O Azure CLI 2.0 √© um utilit√°rio de linha de comando para gerenciamento e administra√ß√£o de recursos do Microsoft Azure. Multiplataforma, h√° a possibilidade de uso desta ferramenta em ambientes Windows, Linux e macOS. Informa√ß√µes sobre como instalar o Azure CLI podem ser encontradas no seguinte link:
https://docs.microsoft.com/en-us/cli/azure/install-azure-cli
A instru√ß√£o a seguir utiliza o Azure CLI para registrar o provider que permitir√° a cria√ß√£o e gerenciamento de recursos do AKS via linha de comando:
az provider register -n Microsoft.ContainerService
Um grupo de recursos chamado TesteKubernetes dever√° ser criado, com o mesmo estando vinculado √† regi√£o Leste dos EUA (East US):
az group create --name TesteKubernetes --location eastus
J√° o pr√≥ximo comando ir√° gerar um novo recurso do AKS (ContagemService) com um cluster contando com 2 nodes, sendo que o mesmo estar√° vinculado ao grupo TesteKubernetes:
az aks create --resource-group TesteKubernetes --name ContagemService --node-count 2 --generate-ssh-keys
Na figura a seguir √© poss√≠vel observar a execu√ß√£o destas instru√ß√µes a partir do PowerShell:
Conclu√≠da esta primeira sequ√™ncia de tarefas ser√° poss√≠vel notar a exist√™ncia de 2 novos grupos de recursos no portal do Azure: TesteKubernetes e MC_TesteKubernetes_ContagemService_eastus:
No caso do grupo MC_TesteKubernetes_ContagemService_eastus ser√£o criados aqui todos os recursos relacionados √† estrutura do Master e dos Nodes que formam o cluster do Kubernetes:
O recurso ContagemService aparecer√° dentro do grupo TesteKubernetes:
Ao acessar este recurso (ContagemService) o status do mesmo constar√° como Succeeded (o que indica sucesso em sua cria√ß√£o) e na se√ß√£o Properties estar√° especificado o n√∫mero de Nodes (2, em NODE COUNT):
Para a cria√ß√£o de objetos no cluster ser√° empregado o kubectl, utilit√°rio de linha de comando para o gerenciamento de recursos do Kubernetes. O link a seguir cont√©m instru√ß√µes para a instala√ß√£o desta ferramenta:
Install and Set Up kubectl
No pr√≥ximo comando o Azure CLI ser√° utilizado com o intuito de liberar o acesso do kubectl ao cluster do AKS:
az aks get-credentials --resource-group TesteKubernetes --name ContagemService
Ser√° tamb√©m configurado o acesso do kubectl ao recurso do Azure Container Registry criado anteriormente:
kubectl create secret docker-registry contagemregistrykey --docker-server=https://groffecr.azurecr.io --docker-username=<USU√ÅRIO> --docker-password==<SENHA> --docker-email=<E-MAIL>
A seguir est√° o resultado da execu√ß√£o destas 2 instru√ß√µes:
As defini√ß√µes do objeto Deployment que ser√° criado ficar√£o em um arquivo YAML (contagem.yaml) cujo conte√∫do foi disponibilizado na listagem seguinte:
O comando kubectl create -f contagem.yaml proceder√° com a cria√ß√£o do objeto Deployment:
J√° o objeto Service funcionar√° como um Load Balancer, disponibilizando um endere√ßo de IP para acesso √† aplica√ß√£o e distribuindo o processamento entre os diferentes Pods que vierem a existir para a mesma. A listagem a seguir (arquivo service.yaml) cont√©m as defini√ß√µes necess√°rias para a gera√ß√£o desta estrutura:
https://gist.github.com/renatogroffe/a31fca447004623f8114da1b17e1fde0
A instru√ß√£o kubectl create -f service.yaml far√° com que este novo recurso seja criado:
A instru√ß√£o kubectl get deployment listar√° o objeto de deployment criado nesta se√ß√£o:
O comando kubectl get pods exibir√° os Pods dispon√≠veis (at√© o momento apenas um):
Ao executar o comando kubectl get services ser√£o listados os servi√ßos dispon√≠veis, com o IP para acesso √† aplica√ß√£o destacado em vermelho:
Um teste com a URL http://40.117.133.3/api/contador trar√° como resultado o valor do contador, a propriedade machineName com o nome do Pod que est√° executando o container e o sistema operacional do host (Unix/Linux):
O kubectl ser√° empregado agora para escalar a aplica√ß√£o via linha de comando, com a instru√ß√£o a seguir definindo o uso de 2 Pods para processamento das solicita√ß√µes enviadas √† aplica√ß√£o de testes:
kubectl scale deployment contagem-deployment --replicas=2
Com a execu√ß√£o do comando kubectl get pods ser√£o exibidos agora 2 Pods (o j√° existente e um novo que est√° destacado em vermelho):
Novos testes mostrar√£o que o Load Balancer configurado atrav√©s do objeto Service est√° em funcionamento, com o consequente direcionamento de requisi√ß√µes para os 2 Pods existentes:
O Kubernetes conta tamb√©m com uma aplica√ß√£o Web/dashboard para monitoramento e gerenciamento de um cluster. Para ativar o Kubernetes Dashboard executar a seguinte instru√ß√£o (referenciando o recurso do AKS e seu grupo correspondente):
az aks browse -g TesteKubernetes -n ContagemService
Na pr√≥ximas imagens temos detalhes da p√°gina inicial que aparecer√° ao acessar o Kubernetes Dashboard, com resumos englobando as diferentes estruturas existentes no cluster:
√â poss√≠vel inclusive escalar uma aplica√ß√£o a partir da se√ß√£o Deployment:
Para efeito de testes a aplica√ß√£o ser√° configurada a fim de empregar 4 Pods:
Com isto teremos 2 novos Pods, destacados em vermelho na pr√≥xima imagem:
A remo√ß√£o for√ßada de um destes Pods levar√° √† cria√ß√£o autom√°tica de uma nova estrutura deste tipo (o que demonstra a capacidade do Kubernetes em garantir uma alta disponibilidade da aplica√ß√£o, com a aloca√ß√£o de novos containers/inst√¢ncias/Pods caso sejam detectados problemas em algum destes elementos):
Nas pr√≥ximas imagens temos exemplos de retornos produzidos pelos novos Pods:
Os fontes do projeto de testes e scripts para cria√ß√£o de objetos no Kubernetes est√£o dispon√≠veis no GitHub:
https://github.com/renatogroffe/ASPNETCore2_Kubernetes
Para remover um recurso do Azure Kubernetes Service (AKS) e todas as estruturas associadas a este executar a seguinte linha de comando:
az aks delete --name ContagemService --resource-group TesteKubernetes --no-wait
Azure Container Service (AKS)
Docker para Desenvolvedores .NET ‚Äî Guia de Refer√™ncia
Kubernetes ‚Äî Site Oficial
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
185 
2
185¬†
185 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/building-a-diy-adhd-medication-reminder-with-azure-functions-e84e4b0611c3?source=search_post---------111,"There are currently no responses for this story.
Be the first to respond.
Lately, I‚Äôve been playing around with Azure Functions to automate different parts of my life. Just today, as I was building a demo using Functions for an upcoming work project, I looked up at the clock and discovered it was 4pm.
"
https://medium.com/@jeffhollan/ordered-queue-processing-in-azure-functions-with-sessions-c42ee21e689d?source=search_post---------112,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
May 30, 2019¬∑6 min read
Let‚Äôs chat about ordering. It‚Äôs one of my favorite topics, and something I‚Äôve blogged about extensively before. Previously ordered processing in Azure Functions was only possible with event streams like Azure Event Hubs, but today I want to show how you can preserve order for Service Bus queues and topics as well.
On the surface it seems pretty straight-forward: I want to be able to process messages from a queue in the exact order that I received them. For a simple service running on a machine, it‚Äôs pretty easy to achieve. However, how do I preserve the ordering of queue messages when I want to process at scale? With something like Azure Functions I may be processing messages across dozens of active instances, how can I preserve ordering?
Let‚Äôs use a simple example of a messaging system that deals with patients at a hospital. Imagine I have a few events for each patient:
I want to make sure I never process a message out of order and potentially discharge a patient before I‚Äôve processed their treatment!
Let‚Äôs run some quick experiments to see what happens. For this I‚Äôm going to simulate 1000 patients each sending these 4 messages (in order) and processing them (ideally in order as well).
Let‚Äôs try this with a simple Azure Function that just triggers on a queue. I‚Äôm not going to do anything special, just trigger on the queue and push the operation it‚Äôs processing to a list on Redis Cache.
After sending 1000 patients worth of data (4 messages each) to this queue, what does the Redis Cache look like after processing? Well some of the patients look great. When I lookup Patient #4 I see:
Great! All 4 events were sent for Patient 4, and got processed in order. But if I look at patient 2:
In this case it didn‚Äôt finish processing the ‚Äúpatient arrives‚Äù message until after 2 other messages had already been processed. So what happened here? Azure Service Bus does guarantee ordering, so why are my messages out of order?
Well by default, the queue trigger will do a few things. First, for every instance that spins up, it will process a set of messages concurrently. By default an instance concurrently processes 32 messages. That means it may be processing all 4 messages for a patient at the same time and they finish in different order than they were sent. Well that seems easy enough to fix, let‚Äôs just limit the concurrency to 1.
Here‚Äôs maybe the most common solution to the above problem that I see. Let‚Äôs limit the concurrency to only process 1 message at a time instead of 32. For that I modify my host.json file and set the maxConcurrentCalls to 1. Now each instance will only process 1 message at a time. I run the same test again.
First off, it‚Äôs super slow. It takes me a long time to chew through the 4000 queue messages because each instance only processes 1 at a time. And worse yet? When I check the results afterwards, some of the patients are still out of order! What‚Äôs going on here? Even though I limited the instance concurrency to 1, Azure Functions has scaled me out to multiple instances. So if I have 20 function app instances that have scaled, I have 20 messages being processed concurrently (1 per instance). That means I still get into a spot where messages from the same patient could be processed at the same time ‚Äî just on different instances. I‚Äôm still not guaranteed ordered processing.
The fix here? Many people want to limit the scale out of Azure Functions. While it‚Äôs technically possible, it would hurt my throughput even more. Now only one message globally could be processed at a time, meaning during high traffic I‚Äôm going to get a large backlog of patient events that my function may not be able to keep up with.
Wouldn‚Äôt this be such a sad blog post if I ended it here? There is a better way! Previously I would have said your best bet here may be to use Event Hubs which, because of partitions and batches, you can guarantee ordering. The challenge here though is that sometimes a queue is the right message broker for the job given its transactional qualities like retries and deadlettering. And now you can use queues and get ordering with Service Bus sessions üéâ.
So what are sessions? Sessions enable you to set an identifier for a group of messages. In order to process messages from a session, you first have to ‚Äúlock‚Äù the session. You can then start to process each message from the session individually (using the same lock / complete semantics of a regular queue). The benefit of sessions is it enables you to preserve order even when processing at high scale across multiple instances. Think of before where we had something like 20 Azure Function app instances all competing for the same queue. Rather than not scaling to 20, now all 20 instances each will ‚Äúlock‚Äù its own available session and only process events from that session. Sessions also ensure that messages from a session are processed in order.
Sessions can be dynamically created at any time. An instance of Azure Functions spins up and first asks ‚Äúare there any messages that have a session ID that hasn‚Äôt been locked?‚Äù If so, it locks the session and starts processing in order. When a session no longer has any available messages, Azure Functions will release the lock and move on to the next available session. No message will be processed without first having to lock the session the message belongs to.
For our example above, I‚Äôm going to send the same 4000 messages (4 patient events for 1000 patients). In this case, I‚Äôm going to set the patient ID as the session ID. Each Azure Functions instance will acquire a lock on a session (patient), process any messages that are available, and then move on to another patient that has messages available.
Sessions are currently available in the Microsoft.Azure.WebJobs.Extensions.ServiceBus extension using version >= 3.1.0, and at the time of writing this is in preview. So first I'll pull in the extension.
And then make the tiniest code change to my function code to enable sessions ( isSessionsEnabled = true):
I also need to make sure I‚Äôm using a session-enabled queue or topic.
And when I push the messages to the queue, I‚Äôll set the right sessionId for each patient message I send.
After publishing the function I push the 4000 messages. The queue gets drained pretty quickly, because I‚Äôm able to process multiple sessions concurrently across scaled-out instances. After running the test I check Redis Cache. As expected, I see all messages were processed, and for every single patient I see they were processed in order:
So with the new Azure Functions support for sessions, I can process messages from a Service Bus queue or topic in order without having to sacrifice on overall throughput. I can dynamically add messages to a new or existing session, and have confidence that messages in a session will be processed in the order they are received by service bus.
You can see the full sample I used for testing and loading messages in my GitHub repo. The master branch will be all in order, and the out-of-order branch is the default and out of order experiment.
Originally published at https://dev.to on May 30, 2019.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
See all (133)
234 
3
234¬†claps
234 
3
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@borakasmer/parse-website-with-go-for-processing-the-azure-net-core-web-service-result-in-microservice-a145b3b98c9c?source=search_post---------113,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bora Ka≈ümer
Sep 18, 2020¬∑10 min read
This article will perform some operations on the data of products sold on an online shopping site and save them to the DB.
‚ÄúGetting information off the internet is like taking a drink from a firehose.‚Äù ‚Äî Mitch Kapor
We will get new product data with different currency prices by using .Net Core web API. But we will convert all product prices to ‚Ç∫ currency and save on to the SqlDB. So we will parse a webpage and get Dolar, Euro, and British Pound currency value from the HTML and convert it to the ‚Ç∫ Turkish Liras with Go lang. For all these processes, we will use Microservices. For improving the performance, we will use the Redis Cache. If there is no any Currency data in Redis, we will put all product data that we get from the .Net Core service to the RabbbitMQ. After all, we will get every product data one by one from the consumer and convert all product‚Äôs price currency to ‚Ç∫ Turkish Liras with Go lang. We will save current currency data to Redis for one minute and insert all product data with the created date to the SQL DB.
Create a WebApi ‚ÄúGoService‚Äù:
Add RabbitMQ and Newtonsoft package.
Models/Exchange.cs: This is sold product data model.
With this Insert() Action, we will get Exchange data, connect to local RabbitMQ with 78.217. This is my modem IP. And we will put it on the one channel. We will use the ‚Äútest‚Äù for the username and password from the 1881 port. We will connect to RabbitMQ like this because we will try to reach my local RabbitMQ from the remote Azure server. Because in the end, we will publish this .Net Core Web Service to the Azure.
We will put the data to the RabbitMQ because we will perform some operations on the data, and it takes so long. So this process must work behind the current operations. We will create a ‚Äúproduct‚Äù channel on RabbitMQ and put this data on it to takes and process it by the consumer. ‚Äú[SwaggerOperation(Summary = ‚ÄúExchangeType is an Enum.‚Äù: We declare columns of Exchange parameter with this SwaggerOperation Attribute[].
Controller/ExchangeController.cs:
Let‚Äôs declare to config on Startup.cs:
Startup.cs: We will declare the swagger document for this service. So we add service.AddSwaggerGen() and we will add swagger declaration with c.EnableAnnotations() method. The swagger declaration is soo important. In this application, we declare Enum values of ExchangeType like above for the Insert() method.
webParserGo.go: This is our main page. Firstly we will get all the products list from the SQL. So we will create a ‚Äúsql.go‚Äù file and import it to ‚ÄúwebParser.go.‚Äù We will download the ‚Äúmssqldb‚Äù library from GitHub as below.
‚ÄúBIG problems are best solved in small pieces.‚Äù
‚Äî Frank Sonnenberg
The packaging is everything for the GoLang. As the clean code said, we must distribute the packages by its duty with folders for readability, test, and debugging.
We will take the product from the queue, convert the price to Turkish Liras ‚Ç∫, and save it to the SQL server.
‚Äúdb.Prepare‚Äù is a prepared statement that is bound to a single database connection. The typical flow is that the client sends a SQL statement with placeholders to the server for preparation; the server responds with a statement ID. In this example, we will prepare an SQL query with Products properties placeholder with ‚Äú@p1,@p2..‚Äù And finally, we will return inserted @@Identity ‚ÄúSCOPE_IDENTITY()‚Äù
. ‚Äúcontext.WithTimeout()‚Äù: We will set a one-minute timeout for SQL query..‚Äùdefer cancel() & defer stmt.close()‚Äù: It is so important for memory management..‚Äùrows := stmt.QueryRowContext(ctx, product.Name, product.Price,..‚Äù: We will set the insert product parameter and get inserted ProductID(@@identity).
webParserGo.go(Sql):
For using Sql in Go, we will get belowe library.
We will open the SQL-DB as below. And with the ‚Äúdefer‚Äù keyword, we will close it when everything is finished about this DB.
This file is used for the global declaration.
‚ÄúYour website is the window of your business. Keep it fresh, keep it exciting.‚Äù ‚ÄîJay Conrad Levinson
We will parse ‚Äúhttps://doviz.com‚Äù for getting exchanges value because there is no endpoint to calculate the Turkish lira equivalent of the incoming product price.
We will use ‚ÄúPuerkitBio/goquery‚Äù for the parsing a website.
We will not parse the web page for every product insert. It is insane and not safe for performance. So what will we do? We will keep the above exchangeList into the Redis for one minute.
We will use go-redis/redis library for using Redis in Go.
All main processes are calling from here. This is our microservice. We will use ‚Äústreadway/amqp‚Äù for the consuming data from the RabbitMQ. And we will import all packages like ‚Äúredis,sql,parser,shared.‚Äù
We will get the final all product data from the SQL.
We will publish the .Net Core WebService to the cloud, and we will call swagger from the Azure.
We will create a new App Service on Azure. We will go to http://portal.azure.com on the browser. We will set Subscription, Resource Group, which we created before, Web App unique name (rmqservices), Running platform(.Net Core3.1), and finally, Region.
The next step is downloading the publish profile. We will use this profile for publishing the .Net Core service to the Azure on Visual Studio. And that‚Äôs all.
We will right-click the project and click the Publish item. We will select publish Profile, and finally, we will Import Profile, which we downloaded from the Azure. After all, we will publish the .Net Core project to Azure ‚Äúrmqservices.azurewebsites.net.‚Äù
This beloved‚Äôs code is Azure .Net Core RabbitMQ config. The hostName is my fake modem IP, and the external Port is 1881.
We have to make some arrangements for our modem for redirection from our modem to the Local RabbitMQ machine port. When we insert a product with the swagger on Azure .Net Core WebApi service, this package will come to my ModemIP and 1881 port. We will redirect to this request to 192.168.1.1, which is RabbitMQ Pc IP and, of course, port 5672, which is RabbitMQ port as bellow picture. After all, this inserted product data will subscribe to the product channel on RabbitMQ. And local Go service will consume it and start all the process. All Setup is done.
In this application, we post new product data from .Net Core WebService with swagger. All new products‚Äô currency could be changed every request. And when we insert every product, we convert the product‚Äôs price to Turkish price currency. There is no service for the getting exchange rate while converting the Product‚Äôs Price to Turkish currency. So we parsed an exchange web site for getting rates. We used microservices technologies because of this long process. Microservice allows us to work together with different technologies. Go is a powerful programming language. The packaging is so important for clean code in Golang. So We distributed all packages according to their duty. Redis, SQL, Parser, Shared all the packages are used in the ‚Äúconsumer.go‚Äù file. We published the .Net Core Project to Azure and redirected from the external 1881 port to the internal local RabbitMQ machine with 5672 port on the local Modem Port Forwarding setup. So we could process locally consumed product data in Go, which is subscribed by remote Azure Service.
I hope this article has given you a different perspective on how different technologies work together.
‚ÄúIf you have read so far, first of all, thank you for your patience and support. I welcome all of you to my blog for more!‚Äù
Source: kb.objectrocket.com, https://github.com/masnun/gopher-and-rabbit, jenicaandpatrick.com, bsilverstrim.blogspot.com, github.com/PuerkitoBio/goquery
Source Code: https://github.com/borakasmer/GoWebParser
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
See all (22)
132 
2
132¬†claps
132 
2
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
About
Write
Help
Legal
Get the Medium app
"
https://blog.bankex.org/bankex-microsoft-azure-setup-overview-facbbadf4b18?source=search_post---------114,"What‚Äôs Microsoft Azure?
It‚Äôs a set of services which you can utilize to assemble your solution.
Why Blockchain as a Service from Azure?
As an open, flexible, and scalable platform, Azure supports a rapidly growing number of distributed ledger technologies that address specific business and technical requirements for security, performance, and operational processes.
The intelligence services, like Cortana Intelligence, provide unique data management and analysis capabilities that no other platform is able to offer. And the vast Microsoft partner ecosystem extends the capabilities of our platforms and services in a way to fit specific roles and industry needs.
Blockchain as a Service (BaaS) provides a rapid, low-cost, low-risk, and fail-fast platform for organizations to collaborate together by experimenting with new business processes ‚Äî backed by a cloud platform with the largest compliance portfolio in the industry. (taken from: https://azure.microsoft.com/en-us/solutions/blockchain/)
How does BANKEX use Azure?
In the case of BANKEX, we use the 5 most powerful services provided by Microsoft Azure:
Azure Active Directory is a IaaS (identity-as-a-service) which currently provides and manages access of our team to services that are currently available on Microsoft Azure.
Storage Account is a durable, robust and scalable storage solution which allows us to store all the essences that are deployed within our solution.
App Service Plan is a powerful tool that allows us to host our publications. In fact, it‚Äôs a set of virtual machines which allows us to deploy and manage easily the web solutions that we build at BANKEX. It can also protect our platform from DDos attacks using the Web Application FireWall.
Application Insights allows us to monitor the way our solutions are currently performing and based on these parameters we can decide if developers need to intervene. Developers can also see if any error have occurred or exceptions have been thrown. This information is given to us before the customer can see the issue.
Visual Studio team services is a code management tool which allows to encapsulate all the artefacts and modifications that happen during the lifecycle of our solution.
We host our public solutions on Visual Studio Team Services. We utilize modern dev-op solutions in order to organize a continuous delivery and integration of our solutions. This way, we want to achieve the shortest time possible between development and deployment of them.
Our solutions can be broken down into parts: private and public.
The public part is primarily smart contracts on Ethereum that can be found on our BANKEX GitHub (https://github.com/BankEx).
Microsoft Azure allows us to firstly deploy our smart contracts on the TestNet and then once all the tests are successful it can be then deployed on the Ethereum production net.
Our private part is written on Node.js and Angular and it is also deployed on Azure with all the benefits of continuous integration and delivery.
Video explaining how BANKEX uses Microsoft Azure services:
BANKEX is available at:
Website ‚Äî Telegram ‚Äî Twitter ‚Äî Facebook‚Äî Youtube ‚Äî LinkedIn ‚Äî Reddit ‚Äî GitHub ‚Äî Steemit
BANKEX is Bank-as-a-Service on blockchain, building the‚Ä¶
799 
799¬†claps
799 
Written by
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
BANKEX is Bank-as-a-Service on blockchain, building the Proof-of-asset-Protocol. www.bankex.com
Written by
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
BANKEX is Bank-as-a-Service on blockchain, building the Proof-of-asset-Protocol. www.bankex.com
"
https://medium.com/awesome-azure/azure-difference-between-azure-storage-queue-and-service-bus-queue-azure-queue-storage-vs-servicebus-3f7921b0159e?source=search_post---------115,"There are currently no responses for this story.
Be the first to respond.
Comparison ‚Äî Azure Storage Queue vs Azure Service Bus Queue
Storage Queue is a simple message queuing service to store large numbers of messages.Service Bus Queue is part of a broader messaging service that supports queuing, publish/subscribe, and more advanced integration patterns.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-create-different-environments-on-azure-22331d11ea11?source=search_post---------116,"There are currently no responses for this story.
Be the first to respond.
Every application needs different environments for different purposes and each application needs at least 3 environments. For example, we need a Development environment for the developers to push the code and test it themselves, a QA environment for testers to test the app before we‚Ä¶
"
https://towardsdatascience.com/building-snowpipe-on-azure-blob-storage-using-azure-portal-web-ui-for-snowflake-data-warehouse-f0cdd7997250?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Dec 14, 2019¬∑10 min read
Snowpipe is a built-in data ingestion mechanism of Snowflake Data Warehouse. It is able to monitor and automatically pick up flat files from cloud storage (e.g. Azure Blob Storage, Amazon S3) and use the ‚ÄúCOPY INTO‚Äù SQL command to load the data into a Snowflake table.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/iotforall/does-microsofts-azure-iot-edge-live-up-to-the-hype-6086dd5f7d5b?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
Microsoft‚Äôs Azure IoT Edge was announced at Build 2017 to much fanfare. The idea goes that the edges of a network are underutilized and devices are unnecessarily chatty with the cloud. Wouldn‚Äôt it be great if you could move some of the work being done in the cloud down to your device? That‚Äôs where Microsoft‚Äôs Azure IoT Edge comes in.
The example given at the Build highlighted work done with Sandvik Coromant, a manufacturing tools company that creates multi-million dollar machines. Before IoT Edge, Sandvik Coromant utilized the Azure‚Äôs Stream Analytics and Machine Learning platforms to predict disasters and shut down machines if one seemed pending.
There‚Äôs only one problem: communication to the cloud introduced a two-second latency and time is of the essence when it comes to an emergency shutdown. By utilizing IoT Edge, Sandvik Coromant was able to take the same logic used in the cloud and move it to the edge, reducing latency to 1/10th of a second.
There are many potential advantages to moving compute from the cloud to the edge; reducing latency is but one. Reducing bandwidth costs, simplifying development by using the same programming models in the cloud as on your devices, allowing the use of Azure‚Äôs advanced cloud platform locally, the list goes on. For some work, IoT Edge seems like a fantastic tool.
But does reality live up to the hype? Let‚Äôs delve in a little deeper.
After some investigation, I was surprised to find that Azure IoT Edge was not a completely new product. It‚Äôs a new name to reflect all of the new features that have been packed into an existing product, Azure IoT Gateway SDK.
Although I felt a little duped at first, I understand Microsoft‚Äôs reasoning. If anything else, Azure IoT Edge is a sexier name, so let‚Äôs just roll with it. Azure IoT Edge does everything that Azure IoT Gateway SDK did, and more. So other than moving compute from the cloud, what does this thing do?
Azure IoT Edge‚Äôs primary objective is connecting existing industrial devices to the cloud and each other; making previously dumb machines smarter and more connected.
The GitHub repository for Azure IoT Edge was my starting point. The sample apps that are linked to in the ReadMe seemed especially useful. Since Azure IoT Edge runs on both Linux and Windows and is able to run JavaScript, Java, and .NET modules, Microsoft is making an effort to reach out to developers of all stripes.
As a C#-loving .NET developer at heart, I decided to grab the latest and greatest .NET Standard module sample to get started. After following the instructions to download the sample app, I attempted to run it to no avail. ‚ÄúThe debug executable‚Ä¶specified in the debug profile does not exist.‚Äù
Huh?
As a software developer, this type of curveball happens a lot. I‚Äôm not married to .NET, so I just ran the JavaScript sample. Lo and behold, it worked! Well, running on my local machine logging faked data anyway.
Hooray! I had a (fake) sensor throwing some stuff at IoT Edge. But really, what was that doing? There was no cloud involved, no models I had created in Azure and put on my local device. I had to dig deeper.
Taking a different tack to learn a bit more, rather than running the standalone samples I decided to get the source for IoT Edge. Interestingly, the source for IoT Edge itself contains additional samples. I would have liked to see the main GitHub repository contain the source of IoT Edge without any samples, or just lump everything together into one repo. Having multiple repos with samples confuses the issue, I think.
I found the Microsoft documentation on Azure IoT Edge helped clarify things and I was able to getting up and running with the ‚ÄúHello World‚Äù sample.
I installed the prerequisites as instructed to build IoT Edge from source, which includes installing CMake and Python 2.7 and making sure the folders in which they reside were added to the PATH environment variable.
Then, I ran the command tools\build.cmd --disable-native-remote-modules to build the sample in the DeveloperCommand Prompt, opened through Visual Studio. Don‚Äôt be like me; make sure you have ‚ÄúDesktop environment with C++‚Äù installed through the Visual Studio Installer even if that‚Äôs not your development bread-and-butter. You‚Äôll need that.
After digging into the documentation a bit, I understood a bit more about how this thing works. You can imagine it as a mini operating system, with ‚Äúmodules‚Äù standing in as apps.
Some modules are pre-written and do common gateway functions, but you can create your own custom modules using various languages (C#, Java, JavaScript, and C). These modules ingest data from your edge devices, make some logical decisions, and communicate with other modules and the cloud. This all runs on top of a software abstraction that can live in a Linux or Windows environment.
Following the instructions to create the ‚ÄúHello World‚Äù sample, I found it utilizes a JSON configuration file. In this file, you indicate the modules you want to use and their entry point as well as any static arguments you will be needing.
It seems like a straightforward system of setting up configuration variables, especially useful for tying together a bunch of modules you don‚Äôt own. For the ‚ÄúHello World‚Äù sample, it uses two modules: ‚ÄúHello World‚Äù and ‚ÄúLogger.‚Äù
The idea is that the Hello World module just prints some messages to the Logger, whose only role is to output these messages into a file. Extrapolating out, it‚Äôs obvious the Logger module could be used by any other module you decide to create.
To run the sample, all one needs to do is enter samples\hello_world\Debug\hello_world_sample.exe ..\samples\hello_world\src\hello_world_win.json into any old command prompt. This starts the sample executable and points it to use the included default JSON configuration file. Then, Azure IoT Edge starts, runs the modules, and starts logging a ‚ÄúHello World‚Äù message to the file specified in the JSON config file every five seconds.
Can we edit this module to do our bidding?
The out-of-the-box configuration writes to a iot-edge/build/log.txt file. I wanted to test that changing the configuration file (located at iot-edge/samples/hello_world/src/hello_world_win.json) would change the output. After updating it to write to a .json file, it was in JSON format after all, I ran the Hello World sample. Yep, it logs to a bonafide JSON file now!
Seeing the promise of Azure IoT Edge and getting a handle on how to start and run a sample application is a start, but I want to dive deeper. In Part Two I‚Äôll be exploring how to connect IoT Edge to the cloud and run on an actual device.
Originally published at iotforall.com
Expert analysis, simple explanations, and the latest‚Ä¶
233 
2
233¬†claps
233 
2
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
Written by

Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
"
https://medium.com/@pjbgf/azure-kubernetes-service-aks-pulling-private-container-images-from-azure-container-registry-acr-9c3e0a0a13f2?source=search_post---------119,"Sign in
There are currently no responses for this story.
Be the first to respond.
Paulo Gomes
Nov 21, 2017¬∑4 min read
‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî Updated on Jan/2018: Another way of achieving the same results is to ensure the service principal used by your AKS server is on the Reader role of your ACR, in which case you won‚Äôt need to create a cluster secret. More information on this can be found here. ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî
Microsoft has just recently released a preview of AKS, its Managed Kubernetes offering which is a response to GKE (Google Kubernetes Engine). It is still early days but it does sound great to have that on Azure ‚Äî that is, when you don‚Äôt bump into one of the known issues.
The only problem is that all examples I found so far only use public container images from Docker Hub, which is most probably not what you would use for your own projects in production. So here I will go through a more ‚Äúreal life‚Äù scenario, based on the Microsoft walkthrough example, but provisioning the cluster, the registry and deploying a container based on a private container image.
In order to proceed you will need to have Azure CLI 2 installed, so if you haven‚Äôt done that yet, here is how you can do it. Also make sure you have logged in on azure with the CLI command az login.
Let‚Äôs start by provisioning the container registry:
Open the Azure Portal in your browser and look for all your Container Repositories. Select the one you have just created, then go to its Settings and Access Keys. Enable Amin user and copy the password.
Download Microsoft‚Äôs sample container image and tag it with the ACR address we have just created:
When you do this for your own images, just make sure you prefix them with your full registry address. Once that is done, push it onto your private registry:
If you don‚Äôt have a cluster already, create one:
Make sure that the location specified above is one of the supported ones on the preview. This bit has been quite unstable since the launch, with some regions simply not accepting new clusters after a few days. So if you start getting to many issues with your cluster, do go to the AKS GitHub account and check whether is there any known issue or existing work around for your problem.
Install the AKS CLI:
Download the configuration settings and set the current context to connect to your kube cluster:
You are almost all set now. You just need to create a ‚Äúdocker-registry‚Äù secret in the cluster, which you can then use in your yml file:
Create a file named azure-vote.yml with the contents below. Make sure you use the correct image name and secret that we have previously defined.
Deploy the application into your cluster:
During the deployment process the cluster will use the secret to connect to the private registry. To confirm all worked properly, just fire the commandkubectl get pods. The result will show all the current pods in the cluster and their respective statuses.
That‚Äôs it! All done. Hopefully this also works for you and you have saved a few bangs of your head against the wall. :)
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
179 
3
179¬†
179 
3
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
"
https://towardsdatascience.com/how-to-fail-the-azure-fundamentals-certification-e37a650a251e?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Briggs
Sep 16, 2020¬∑4 min read
AZ-900 is the most popular certification for the (almost) most popular Cloud service in the world. Azure Fundamentals acts as the starting point for many people working with the Cloud.
"
https://medium.com/devopsturkiye/azure-devops-%C3%BCzerinde-uygulama-yay%C4%B1nlama-2d37ba7720b5?source=search_post---------121,"Sign in
There are currently no responses for this story.
Be the first to respond.
Abdulkerim Karaman
Feb 11, 2019¬∑3 min read
Bildiƒüiniz gibi Azure, Microsoft tarafƒ±nda bulut teknolojilerinin √ßatƒ±sƒ±nƒ± olu≈üturmaktadƒ±r. Bu makalemizde Azure platformunun sunduƒüu DevOps yapƒ±sƒ±nƒ± inceleyeceƒüiz.
Hesabƒ±nƒ±z yoksa https://azure.microsoft.com/tr-tr/services/devops/ adresine giderek √ºcretsiz hesap olu≈üturabilirsiniz. Azure DevOps altyapƒ±sƒ±nƒ± daha iyi anlamak i√ßin bir uygulama olu≈üturarak deploy edeceƒüiz.
√ñrnek bir .Net Core uygulamasƒ± olu≈üturalƒ±m.
Yeni bir product controller olu≈üturalƒ±m ve i√ßine data girelim.
ProductController:
Product:
Uygulamayƒ± √ßalƒ±≈ütƒ±rarak test edelim.
http://localhost:5000/api/product url √ºzerinden uygulamaya eri≈üebilirsiniz.
Github √ºzerinde bir proje olu≈üturarak kodlarƒ±mƒ±zƒ± atalƒ±m.
Evet uygulama kodlarƒ±mƒ±z hazƒ±r. ≈ûimdi https://azure.microsoft.com/tr-tr/services/devops/ adresine giderek yeni bir proje olu≈üturalƒ±m.
Projemizi olu≈üturduktan sonra bizi a≈üaƒüƒ±daki gibi bir proje y√∂netim ekranƒ± kar≈üƒ±layacak.
Evet sol tarafda ki Repos men√ºs√º altƒ±ndan, isterseniz azure ‚Äòun sunmu≈ü olduƒüu repository kullanabilirsiniz. Fakat biz yine aynƒ± men√º altƒ±ndan bulunan ‚Äúimport repo‚Äù kullanarak kendi github adresimizi vereceƒüiz.
Soruce alanƒ±ndan git se√ßerek github proje adresimizi yazalƒ±m. Import dediƒüinizde a≈üaƒüƒ±daki adres ile kar≈üƒ±la≈ütƒ±ysanƒ±z tamamdƒ±r :)
Pipeline altƒ±nda builds sekmesine girerek yeni bir build paketi tanƒ±mlƒ±yoruz. Github repo se√ßerek proje kodlarƒ±mƒ±zƒ± g√∂steriyoruz.
Ardƒ±ndan proje tipini belirtiyoruz.
Se√ßtikten sonra a√ßƒ±lan ekrandan build a≈üamlarƒ±nƒ± olu≈üturuyoruz. Default olarak Restore, build, test, publish a≈üamalarƒ± geliyor. Sadece publish a≈üamasƒ±nda yer alan ‚Äúpublish web project‚Äù checkbox‚Äôƒ±nƒ± kaldƒ±rƒ±yoruz.
ƒ∞≈ülemi tamamladƒ±ktan sonra builds men√ºs√º altƒ±na gelen paketi √ßalƒ±≈ütƒ±rdƒ±ƒüƒ±mƒ±zda ba≈üarƒ±yla build olduƒüunu g√∂zlemliyoruz.
ƒ∞lgili build paketi √ºzerinde edit‚Äôe tƒ±klayarak triggers sekmesi altƒ±ndan her push i≈üleminin ardƒ±ndan bu build paketinin otomatik √ßalƒ±≈ümasƒ±nƒ± saƒülayabileceƒüiniz gibi, yine bu sekme altƒ±ndan zamanlayƒ±cƒ± tanƒ±mlayarak belli zamanlarda bu i≈ülemi yapmasƒ±nƒ± saƒülayabilirsiniz.
Evet sƒ±ra geldi Release paketi olu≈üturmaya bunun i√ßin sol men√ºden Release sekmesine girelim.
Kar≈üƒ±ma gelen ekrandan Azure service deployment‚Äôƒ± se√ßelim.
Bu a≈üamada Azure Portal tarafƒ±nda daha √∂nce olu≈üturduƒüumuz bir web app uygulamamƒ±zƒ± se√ßeceƒüiz. Bu a≈üamayƒ± https://portal.azure.com √ºzerinden kolayca yapabilirsiniz.
Evet ilgili app‚Äôi de se√ßtikten sonra olu≈üturduƒüumuz Release paketi √ßalƒ±≈ütƒ±ralƒ±m.
Evet t√ºm agent ‚Äôlarƒ±mƒ±z ba≈üarƒ±lƒ± bir ≈üekilde √ßalƒ±≈ütƒ±. Olu≈üturduƒüumuz ve deploy ettiƒüimiz uygulamaya a≈üaƒüƒ±daki url √ºzerinden ula≈üabilirsiniz.
https://productapi.azurewebsites.net/api/product
GitHub Repo:
https://github.com/akaramanapp/ProductApi
Evet olu≈üturduƒüumuz bu pipeline sayesinde github √ºzerine atacaƒüƒ±mƒ±z her kodun ardƒ±ndan, ilgili agent‚Äôlar √ßalƒ±≈üarak otomatik deploy i≈ülemi yapƒ±lmƒ±≈ü olacak.
Yararlƒ± olmasƒ± dileƒüiyle‚Ä¶
Full Stack Developer
81 
81¬†
81 
Medium independent DevOps publication. Join thousands of aspiring developers and DevOps enthusiasts
"
https://medium.com/@selcukusta/azure-functions-ile-slack-bot-yap%C4%B1m%C4%B1-8305cdcaacda?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sel√ßuk Usta
Jan 21, 2018¬∑7 min read
IT d√ºnyasƒ±nƒ±n son d√∂nemde merkezinde yer alan bir kavramƒ±n ‚Äî Serverless Architecture ‚Äî tasarƒ±m ve uygulamasƒ±nda kullanƒ±lan Azure Functions ile (ki bir d√∂nem WebJobs olarak adlandƒ±rƒ±lan bir yapƒ± idi) k√º√ß√ºk bir uygulama yaparak sistemin mantƒ±ƒüƒ±nƒ± √ß√∂zebilmek derdimiz.
Ne yazƒ±k ki T√ºrk√ße kar≈üƒ±lƒ±ƒüƒ±nƒ± tam olarak ifade edemediƒüim bir kavram Serverless Architecture. Sunucusuz mimari olarak d√ºz bir √ßeviri yapsak i√ßime sinmiyor √ß√ºnk√º en nihayetinde bulut ortamda olsa dahi bir sunucu ortamƒ± mevcut. T√ºm bu sunucu bakƒ±m ve zaman maliyetlerinin minimize edildiƒüi desek farklƒ± bulut √ß√∂z√ºmleri ile karƒ±≈üƒ±yor. Tam bu noktada Functions as a Service (FaaS) kavramƒ± kurtarƒ±cƒ± olarak hayatƒ±mƒ±za giriyor. Birim i≈ülemlerin, event-based uygulamalarla sunucu y√∂netimine gerek olmaksƒ±zƒ±n fonksiyon seviyesinde kar≈üƒ±lanmasƒ± ve i≈ülenmesi y√∂ntemi ile bu mimariyi canlandƒ±rabiliyoruz.
Yazƒ±nƒ±n ana konusuna girmeden hemen √∂nce yukarƒ±da bahsettiƒüimiz kavramƒ±n teknik detaylarƒ±na dair √ßok deƒüerli bulduƒüum √º√ß referansƒ± buraya bƒ±rakmak istiyorum:
Ekip olarak yemek yemeyi √ßok ‚Äî fazlaca √ßok ‚Äî seviyoruz. √ñƒülen yemekleri i√ßin yaptƒ±ƒüƒ±mƒ±z gurme ka√ßamaklarƒ± organizasyonlarƒ±mƒ±z da me≈ühurdur :)
Hƒ±zlƒ±ca organize olabilmek i√ßin bir mail ba≈ülatƒ±yoruz ve katƒ±lanlar, katƒ±lmayanlar +-1 ile d√∂n√º≈ü yapƒ±yorlar. Organizat√∂r gurmemiz ise son listeyi toplayƒ±p ona g√∂re planlama yapƒ±yor.
D√º≈ü√ºnd√ºm ki ‚ÄúSlack‚Äù √ºzerinde bir kanalƒ±mƒ±z olsa; ismi ‚ÄúGurmecikler‚Äù. Slack √ßalƒ±≈üma grubumuzun da bir bot‚Äôu olsa (Gurmet√∂r) ve Gurmecikler kanalƒ±na √ºye t√ºm arkada≈ülara √∂zelden ‚ÄúBu √∂ƒülen yemeƒüe gidiyoruz, geliyor musun?‚Äù diye mesaj atsa. Katƒ±lanlarƒ± da bir yere yazsa. Sonrasƒ±nda organizat√∂r arkada≈ü hi√ß sayƒ±m zahmetine girmeden bu listeden planlamayƒ± ger√ßekle≈ütirse. Hayat daha kolay ‚Äî ve lezzetli ‚Äî olmaz mƒ±ydƒ±?
Organizat√∂r genelde ben olmuyorum ama olsaydƒ±m bu olay √ßok ho≈üuma giderdi diye d√º≈ü√ºnd√ºm ve kollarƒ± sƒ±vadƒ±m (yemek i√ßin deƒüil, uygulama i√ßin :))
Yazƒ±nƒ±n devamƒ±ndaki uygulamalarƒ± √ßalƒ±≈ütƒ±rabilmek i√ßin bir takƒ±m √∂n gereksinimlerimiz mevcut. ≈û√∂yle ki;
T√ºm bunlara sahip olduƒüumuzu d√º≈ü√ºnerek yola devam ediyoruz.
Slack √ºzerinde bir √ßalƒ±≈üma grubu olu≈üturduysak bu adrese giderek ‚ÄúCreate App‚Äù butonu yardƒ±mƒ±yla bir uygulama olu≈üturmamƒ±z gerekiyor. Ben uygulamama ‚ÄúGurmet√∂r‚Äù adƒ±nƒ± verdim.
Sol taraftaki men√ºden ‚ÄúBot Users‚Äù se√ßeneƒüini se√ßiyor ve uygulamamƒ±za baƒülƒ± bir bot olu≈üturuyoruz. Display Name ve Default Username √∂zelliƒüini dilersek deƒüi≈ütirebiliriz. Bu isimler Slack ekranƒ±nda g√∂r√ºnecektir. Default Username √∂zelliƒüi T√ºrk√ße karakter barƒ±ndƒ±ramayacaƒüƒ± i√ßin bunu gurmetor olarak deƒüi≈ütiriyoruz.
Sƒ±rada uygulamaya tanƒ±mlƒ± yetkiler var. Bunun i√ßin yine sol men√ºden ‚ÄúOAuth & Permissions‚Äù se√ßeneƒüini se√ßiyoruz ve Scopes alt ba≈ülƒ±ƒüƒ±ndan yetki tanƒ±mlarƒ±nƒ± ger√ßekle≈ütiriyoruz. Bu botu bir kanala √ºye olmu≈ü kullanƒ±cƒ±larƒ±n listesini √ßekerken kullanacaƒüƒ±mƒ±z i√ßin channels:read yetkisi ile ve bu kullanƒ±cƒ±lara mesaj atabileceƒüi i√ßin chat:write:bot yetkisi ile sarmalƒ±yoruz.
Yetkilerle ilgili en √∂nemli adƒ±m, programatik olarak t√ºm bu i≈ülemleri yapacaƒüƒ±mƒ±z i√ßin bir Token‚Äôa ihtiya√ß duyuyoruz ve bunu √ºreteceƒüiz. Bunun i√ßin yine aynƒ± ekrandaki en √ºst b√∂l√ºmde yer alan Tokens for Your Workspace b√∂l√ºm√ºne √ßƒ±kƒ±yoruz ve ‚ÄúInstall App‚Äù butonu ile uygulamamƒ±zƒ± √ßalƒ±≈üma grubumuza ekliyoruz. Sonrasƒ±nda bu alanda iki token g√∂r√ºlecek. Biz bot olarak devam edeceƒüimiz i√ßin Bot User OAuth Access Token altƒ±ndaki deƒüeri bir yere not alƒ±yoruz.
Son yapmamƒ±z gereken i≈ülem ise uygulamamƒ±zda kullanacaƒüƒ±mƒ±z √∂zelliƒüi devreye almak. Bunun i√ßin Slack bir √ßok √∂zellik sunuyor (interaktif men√º, interaktif mesaj, sla≈ü komutlarƒ±, vs‚Ä¶). Bizim uygulamamƒ±z ise kullanƒ±cƒ±ya bir soru sorup butonlar yardƒ±mƒ±yla cevap alacaƒüƒ± i√ßin sol men√ºden ‚ÄúInteractive Components‚Äùi se√ßiyor ve aktif hale getiriyoruz:
Bizden iki √∂zellik istiyor ‚Äî biri zorunlu olmak √ºzere ‚Äî :
Request URL: Uygulamamƒ±z, kullanƒ±cƒ±dan gelen cevabƒ± bir webhook yardƒ±mƒ±yla burada verdiƒüimiz adrese POST edecek.
Options Load URL (for Message Menus): Mesaj men√ºleri, buton i√ßerikleri gibi alanlarƒ± dinamik y√∂netmek istersek bu alana bir adres tanƒ±mlamasƒ± yapabilir ve uygulamanƒ±n, i√ßerikleri bu alandan √ßekmesini saƒülayabiliriz.
Tahmin ettiƒüiniz √ºzere bizim doldurmamƒ±z gereken alan Request URL. Buradan bir event fƒ±rlayacak ve biz bu event‚Äôi yakalayƒ±p bir fonksiyon aracƒ±lƒ±ƒüƒ±yla yorumlayacaƒüƒ±z, i≈ülemler yapacaƒüƒ±z ve kullanƒ±cƒ±ya cevap d√∂neceƒüiz. Bu adrese sahip olmak ve fonksiyonumuzu geli≈ütirmek √ºzere Azure‚Äôa gidiyoruz.
Azure √ºzerinde iki uygulama kullanacaƒüƒ±z.
Bunlardan ilki Azure Functions. G√∂revi; Slack‚Äôten gelen isteƒüi kar≈üƒ±lamak, kullanƒ±cƒ± cevabƒ±nƒ± yorumlamak ve kullanƒ±cƒ±ya uygun cevabƒ± d√∂nmek.
Diƒüeri ise Redis Cache. G√∂revi; Azure Functions √ºzerinden aldƒ±ƒüƒ±mƒ±z kullanƒ±cƒ± cevabƒ±nƒ± saklamak.
Redis Cache kurulumu olduk√ßa basit. Yapƒ±lmasƒ± gereken Azure portal √ºzerinden New butonuna basarak a√ßƒ±lan pencereye Redis Cache yazmak ve sunulan tek √ºr√ºn√ºn kurulumunu en maliyetsiz haliyle ger√ßekle≈ütirmek.
Kurulum bittikten sonra Redis‚Äôe ula≈ümak i√ßin gerekli olan baƒülantƒ± c√ºmlesini temin etmek √ºzere ana ekran √ºzerindeki Show Access Keys linkine tƒ±klƒ±yoruz.
A√ßƒ±lan yeni penceredeki Primary connection string (StackExchange.Redis) ba≈ülƒ±ƒüƒ± altƒ±nda yer alan baƒülantƒ± c√ºmlesini bir yere saklƒ±yoruz.
Sƒ±rada uygulamanƒ±n kalbi, Azure Functions kurulumu var. Bunu Azure portali √ºzerinden kurabileceƒüimiz gibi Visual Studio 2017 √ºzerinden de Publish Profile ile kolaylƒ±kla ger√ßekle≈ütirebiliriz. Ben de bu ≈üekilde devam ediyorum. √ñncelikle yeni bir proje olu≈üturuyorum:
Sonrasƒ±nda ise a√ßƒ±lan projeme saƒü tƒ±klayarak Add > New Azure Function diyor ve ismini MessagingFunction adƒ±nƒ± verdiƒüim fonksiyonu ekliyorum.
Uygulamanƒ±n tamamƒ±na a≈üaƒüƒ±daki repodan ula≈ümanƒ±z m√ºmk√ºn. Ben tƒ±pkƒ± diƒüer yazƒ±larda olduƒüu gibi ana fonksiyona yoƒüunla≈üacaƒüƒ±m.
Kodu incelemeden hemen √∂nce belirtmem gereken nokta ≈üu ki; Slack payload body parametresi ile application/x-www-form-urlencoded tipinde a≈üaƒüƒ±daki gibi bir istek yolluyor:
Ancak Azure Functions mevcut s√ºr√ºm√ºnde application/x-www-form-urlencoded tipine √∂zel bir kabul ger√ßekle≈ütiremiyor. Dolayƒ±sƒ±yla kodda yer alan 6. satƒ±rdaki ReadAsStringAsync ifadesini ReadAsFormDataAsync olarak deƒüi≈ütiremiyoruz. Bu sebeple gelen i√ßeriƒüi olduƒüu gibi metinsel bir ≈üekilde okuyup payload= ifadesinden sonraki json ifadeyi alƒ±yor ve Newtonsoft.Json k√ºt√ºphanesi ile i≈üliyoruz.
Kodun geri kalan kƒ±smƒ±nda ise gelen cevaptaki kullanƒ±cƒ± se√ßimini (actions[0].value) ve kullanƒ±cƒ± adƒ±nƒ± (user.name) alƒ±yor, Redis‚Äôe yazma i≈ülemini ger√ßekle≈ütiriyor ve kullanƒ±cƒ±ya; se√ßimine g√∂re bir cevap d√∂n√ºyoruz.
Azure Functions √ºzerindeki hassas verileri (appsettings ve connectionstrings olarak √∂zelle≈ütirebiliriz) kod √ºzerinde tutmak yerine olu≈üturduƒüumuz Function‚Äôa √∂zel Application Settings alanƒ±nda tutmamƒ±z mantƒ±klƒ±:
Tabii hen√ºz yukarƒ±daki pencereyi g√∂rebilecek duruma gelemedik. Bunun i√ßin Azure √ºzerinde Azure Functions servisimizin kurulu olmasƒ± gerekli. Yukarƒ±da da bahsettiƒüim √ºzere bunun i√ßin Publish Profile kullanacaƒüƒ±z. Yapmamƒ±z gereken projemize saƒü tƒ±klayarak ‚ÄúPublish‚Äù butonuna basmak.
A√ßƒ±lan penceredeki saƒü √ºst k√∂≈üede Azure hesabƒ±mƒ±zla ili≈ükilendirilmi≈ü Microsoft hesabƒ±mƒ±zƒ± g√∂rmemiz gerekli. Eƒüer g√∂remiyorsak giri≈ü yapmamƒ±z yeterli olacaktƒ±r. Sonrasƒ±nda ise t√ºm alanlar otomatik olarak doluyor ve bize d√º≈üen tek ≈üey ‚ÄúCreate‚Äù butonuna basmak oluyor.
T√ºm i≈ülemler ba≈üarƒ±yla ger√ßekle≈ütikten sonra ise ‚ÄúPublish‚Äù butonu yardƒ±mƒ±yla uygulamamƒ±zƒ± Azure‚Äôa y√ºkl√ºyoruz. Y√ºkleme i≈ülemi ba≈üarƒ±yla ger√ßekle≈ütikten sonra ise iki √ºst g√∂rselde yer alan ekrana ula≈üabiliyor olmalƒ±yƒ±z. G√∂rselde de g√∂receƒüimiz √ºzere iki anahtarƒ± ekliyoruz (REDIS_CONNECTION ve REDIS_DEFAULT_DB). Bu iki deƒüer, uygulamadaki Redis‚Äôe baƒülantƒ± ger√ßekle≈ütiren RedisCacheProvider sƒ±nƒ±fƒ± i√ßerisinde kullanƒ±lacak.
Son adƒ±mda yapmamƒ±z gereken ise fonksiyona ula≈üacaƒüƒ±mƒ±z public adresi edinmek. Functions penceresinden ilgili function‚Äôƒ± buluyor ve saƒü √ºst k√∂≈üedeki ‚Äú</> Get function URL‚Äù linki ile a√ßƒ±lan pop-up‚Äôdan URL‚Äôi kopyalƒ±yoruz.
Kopyaladƒ±ƒüƒ±mƒ±z function URL‚Äôini, tekrar Slack paneline d√∂nerek Request URL olarak g√∂rd√ºƒü√ºm√ºz alana yapƒ±≈ütƒ±rƒ±yor ve aktif hale getiriyoruz.
Son durumda ihtiyacƒ±mƒ±z olan 3 sistem de (Slack, Redis, Azure Functions) hazƒ±r durumda. Yapmamƒ±z gereken son ≈üey, bot kullanƒ±cƒ±sƒ± ile istediƒüimiz kanaldaki √ºyelerin listesini almak ve her birine mesaj atmak.
Mesaj atma i≈üi i√ßin k√º√ß√ºk bir Python scripti geli≈ütirelim.
3. satƒ±r en √∂nemli kƒ±sƒ±m. Yazƒ±nƒ±n en ba≈üƒ±nda bahsettiƒüim ve bir yere not aldƒ±ƒüƒ±mƒ±z Bot User OAuth Access Token deƒüerini buraya yapƒ±≈ütƒ±rƒ±yoruz.
ƒ∞lk fonksiyon (get_users_in_channel) parametre olarak kanal id‚Äôsi alƒ±yor ve bu kanaldeki kullanƒ±cƒ±larƒ±n id‚Äôlerini d√∂nd√ºr√ºyor.
ƒ∞kinci fonksiyon ise (send_message) yine parametre olarak kanal id‚Äôsi alƒ±yor ve o kanala √∂zelle≈ütirilmi≈ü bir mesaj g√∂nderiyor. Bu kanal id‚Äôsi public bir kanal olabilir ya da private bir kanal olabilir. Ayrƒ±ca her kullanƒ±cƒ± id‚Äôsi, o kullanƒ±cƒ± ile yapƒ±lan √∂zel g√∂r√º≈ümelerdeki kanalƒ±n da id‚Äôsi olarak ifade ediliyor.
Ana fonksiyonda ise kanaldaki kullanƒ±cƒ±larda d√∂nerek her kullanƒ±cƒ±ya mesaj g√∂nderme i≈ülemi ger√ßekle≈ütiriliyor.
Gif‚Äôi kaydederken arka tarafta Python uygulamasƒ±nƒ± tetikledim, kƒ±sa bir s√ºre sonra mesaj geldi ve cevabƒ± verdim. Sonrasƒ±nda ise Azure Functions tarafƒ±ndan nihai cevap bana ula≈ütƒ±. Bir de Redis tarafƒ±nƒ± kontrol edelim dilerseniz:
Burada da gelmeyecek olarak kayƒ±t altƒ±na alƒ±ndƒ±ƒüƒ±mƒ± g√∂rebiliyoruz. Uygulama istenen kƒ±vama gelmi≈ü durumda.
Bu yazƒ±nƒ±n sonuna gelirken karnƒ±mƒ±n acƒ±ktƒ±ƒüƒ±nƒ± s√∂ylemem gerekli. Eƒüer aynƒ± etkiyi yarattƒ±ysam √∂z√ºr diliyorum :) Bir sonraki yazƒ±ya dek, bot‚Äôlardan aldƒ±ƒüƒ±nƒ±z cevaplarƒ±nƒ±z hep anlamlƒ± olsun.
github.com
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
159 
159¬†
159 
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
"
https://medium.com/javarevisited/5-best-azure-fundamentals-courses-to-pass-az-900-certification-exam-in-2020-9e602aea035d?source=search_post---------123,"There are currently no responses for this story.
Be the first to respond.
Hello Guys, If you are preparing for AZ-900 or Microsoft Azure Fundamentals exam and looking for some good online courses to pass this exam, then you have come to the right place.
In the past, I have shared the best courses to learn Azure and certifications like Azure Administrator (AZ-103), Azure Developer (AZ-303), and Azure Architect (AZ-303) and In this article, I am going to list down some of the best courses to crack the AZ-900 certification exam.
You may know that Cloud computing is becoming more and more critical, and it‚Äôs almost mandatory for both technical and non-tech IT people to know about Cloud computing and different Cloud platforms like AWS, Azure, and GCP.
You just can‚Äôt hide with Cloud anymore, you need to learn it to understand and master it to stay relevant in technology jobs. Thankfully there is a lot of learning material available to learn about the benefits of the Cloud and why companies should move to the Cloud.
Still, certifications are probably the best way to develop knowledge, skill, and get the recognition required by many Cloud jobs. If you are just getting started with Cloud Computing and Cloud platforms like Microsoft Azure, then Microsoft Azure Fundamentals (AZ-900) is probably the best certification to build foundational Cloud skills and also get the recognition. This certification is very similar to the AWS Cloud Practitioner certification and primarily designed for candidates looking to demonstrate foundational level knowledge of cloud services and how those services are provided with Microsoft Azure platforms. People, who successfully pass this exam, known as Microsoft Azure Fundamentals or AZ-900, will earn the Microsoft Certified Azure Fundamentals? Certification, which is recognized in many cloud-based jobs. You can also put the certification badge on your LinkedIn profile and your resume to attract recruiters. This exam can be also be taken as an optional first step in learning about cloud services and how those concepts are exemplified by Microsoft Azure. If you happen to attend any of Microsoft training, then you may also get some free vouchers to participate in this certification exam for free.
Here is my list of best courses you can join in cracking the AZ-00 or Microsoft Azure Fundamentals exam and earning a Microsoft Certified Azure Fundamentals certification, but before that let‚Äôs take a look at this nice Microsoft Azure Certification RoadMap which is prepared by Whizlabs, one of the leading IT certification test provider.
They also have very good study material and practice tests for Azure certification including AZ-900 (Microsoft Azure Fundamentals exam).
This is one of the best courses on Udemy to prepare for the AZ-900 or Microsoft Azure Fundamentals exam.
This course is perfectly aligned well with the AZ-900 syllabus and covers all the topics well. Scott Duffy, the instructor of this course, is not clear and concise, which makes it easy to digest the concepts. The course also comes with a lot of bonuses like you will get access to a 24-page study guide, and you can even download audio of the course to learn on the go. The best thing is the course is well up-to-date, which is very challenging given constant updates from Azure. It also contains quizzes to reinforce learning.
Here is the link to join this course ‚Äî AZ-900: Microsoft Azure Fundamentals Exam Prep -2021 Edition
It also contains a 50 questions practice test for final preparation. It‚Äôs not sufficient, but you can still take to find your strong and weak areas before the exam. If you are serious, I suggest you combine this course with Whizlab‚Äôs AZ-900 practice test for better preparation.
This is a great resource or online course to prepare for Microsoft Azure Fundamentals and AZ -900 Exam on Coursera as this is created by Microsoft itself, which means you will be learning from the source itself.
This Microsoft Azure Fundamentals AZ-900 Exam Prep Specialization consists of four courses that will provide all the fundamental knowledge you need to prepare you for the AZ-900 certification exam and for a career in the cloud computing field.
Here are the key things you will learn in this course :
The best thing about this course is that the content of this Coursera Specialization is tightly aligned to the AZ-900 exam objective domains. This is Ideal for IT personnel just beginning to work with Microsoft Azure or anyone wanting to learn about it.
Talking about social proof the program has an impressive 4.7 ratings from more than 10K people who have trusted this to start their career in Cloud. In short, one of the best Coursera courses for the AZ-900 exam and to learn Cloud computing in Microsoft Azure.
Here is the link to join this course ‚Äî Microsoft Azure Fundamentals AZ-900 Exam Prep Specialization
By the way, instead of joining these courses and specialization individually, you can also join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
coursera.com
This is one of the best courses to prepare for the Microsoft Azure Fundamentals exam. It contains all you need to know to pass the AZ-900 exam. The content is very hands-on, and you will see the instructor showing you real stuff inside the Microsoft Azure portal, where Microsoft Learn only teach you in text. If you are a beginner in both cloud technologies and Azure, then you will learn a lot more from seeing a real Azure portal than reading on Microsoft Learn documentations. Apart from lectures that cover all the topics for the AZ-900 exam like Cloud Fundamentals, Core Azure Services, Pricing, and Billing, etc., this course also provides AZ-900 practice questions to get you ready for the exam.
Here is the link to join this course ‚Äî Microsoft Azure ‚Äî Beginner‚Äôs Guide + AZ-900 preparation
At the end of the course, you will get access to two AZ-900 mock tests, containing 74 questions. Since mock tests are crucial to building speed and accuracy, I strongly suggest you go through these tests to improve your speed and accuracy. It will also help you to find your strong and weak areas to focus on before the exam.
Microsoft provides several free learning materials to prepare for their Azure certification, and it‚Äôs beneficial to prepare all topics for Microsoft Azure Fundamentals AZ-900 certification. This is a text-based document with some videos, but very well written, accurate, and full of information. If you don‚Äôt like reading too much, then you can combine this learning resource with one of the online courses I have shared above. But, if you like reading more than watching online courses, then you will love this text-based training material, which is also completely free of cost. You can take notes easily, and reading is also faster than watching courses, and in most cases, they complement your learning from online courses.
Here is the link to join this course for FREE ‚Äî Azure fundamentals by Microsoft
This is another comprehensive AZ-900 Course on Udemy to learn both Cloud Computing and Azure Fundamentals. You will not only learn about Cloud basics like differences between Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), and Software-as-a-Service (SaaS) but also core Azure services like Azure computer, storage, network, and security features. This course is taught by Thomas Mitchell, a 25-year IT industry veteran (and Microsoft Certified Trainer), and it is one the most comprehensive AZ-900 exam-prep course available on Udemy. It features 5 hours of video lectures, numerous hands-on demonstrations, slick downloadable infographics, and several quizzes. You will also get access to a 50-question practice test at the end of the course for practice.
Here is the link to join this course ‚Äî AZ-900 Azure Exam Prep: Microsoft Azure Fundamentals
Talking about social proof, this course is trusted by more than 10,000 students, and it has got on average 4.4 ratings from close to 3000 ratings, which is incredible and speaks a lot about its quality.
Overall, an excellent course to prepare for AZ-900 or Microsoft Azure Fundamental certification exam.
I am a big fan of solving practice questions before appearing for the certification exam, and this single thing has helped me to achieve high scores in most of the exams I have given so far. There is an immense benefit of solving practice questions, and when it comes to choosing the exam simulator or practice test, I always go for Whizlabs, a leader in the IT certification test. They provide quality test questions that mimic real exams in terms of difficulty level and formats. The Microsoft Azure Fundamentals or AZ-900 practice test on Whizlabs is no exception. You will get access to top-quality and unique 275 questions for practice. They are spread out into 5 full-length mock exams. There are an additional 35 questions divided into 7 section tests.
This test costs just $15, and I strongly suggest you buy this if you want to pass the AZ-900 exam on the first attempt. Btw, if you are preparing for multiple certifications or plan to give them in near future, I suggest getting a Whizlabs subscription which provides full access to all of their online training courses and practice tests for different certifications like AWS, Java, Cloud, Docker, and Kubernetes with just $99 per annum (50% discount now).
Her is the link to join this test ‚Äî Microsoft Azure Fundamentals (AZ-900) ‚Äî Practice Tests
They also provide a detailed explanation of each and every question so that you know why a particular option is correct, and why others are not correct. This helps to consolidate your learning and remove any misconceptions you have.
Apart from Whizlabs‚Äô practice test, there is also an official Microsoft test which you can buy while scheduling for the exam. They are a bit pricy though and cost around $100. I prefer Whizlabs, which is much cheaper and similar in quality. That‚Äôs all about some of the best courses to crack AZ-900: Microsoft Azure Fundamentals certification. This is the first cloud certification in several of Microsoft Azure certifications and provides a nice platform to prepare for more comprehensive and distinguished certifications like AZ-300 Microsoft Azure Solution Architect. It‚Äôs both easy to pass, and preparation will equip you with good knowledge of essential cloud technologies and Azure services like compute, storage, networking, security, and price. Since Cloud is essential in today‚Äôs world and every company is migrating on Cloud, I strongly recommend this course to all IT professionals like Programmers, BAs, Project Managers, Infra guys, and support people.  Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like these AZ-900 dumps or practice tests, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. ‚Äî If you are new to the world of Azure Cloud and looking for some free courses to learn Cloud computing then you can also, check out this list of free cloud computing courses with Microsoft Azure in 2021.
medium.com
P. S. S. ‚Äî And, if you are feeling gracious, you can also purchase my topic-wise AZ-900 Practice Test on Udemy. It contains more than 250+ high-quality questions, divided into 5 topics as per the AZ-900 curriculum, and provides you with comprehensive practice. you can use the tet to find your strong and weak areas and work on them before the actual exam.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
257 
257¬†claps
257 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@renatogroffe/desenvolvimento-serverless-com-net-core-implementando-sua-primeira-azure-function-5a3898c4cf51?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 7, 2019¬∑5 min read
A ado√ß√£o de arquiteturas serverless vem ganhando cada vez mais for√ßa em projetos de software nos mais variados cen√°rios, com o baixo custo de alternativas como Azure Functions favorecendo em muito esta pr√°tica. A possibilidade de se preocupar menos com quest√µes de infraestrutura e mais na entrega de solu√ß√µes tamb√©m constitui um fator importante e que merece ser destacado em tais casos.
Na imagem a seguir (print gerado no in√≠cio de Outubro/2019) podemos ter uma dimens√£o do baix√≠ssimo pre√ßo ao optar pelo uso de Azure Functions:
Desde a vers√£o 2.x das Azure Functions √© poss√≠vel implementar aplica√ß√µes serverless com .NET Core, o que abre caminho para o desenvolvimento multiplataforma de projetos combinando a codifica√ß√£o com estas tecnologias a partir de ambientes Windows, Linux ou Mac. Uma outra possibilidade seria a cria√ß√£o de Azure Functions atrav√©s do pr√≥prio Portal do Microsoft Azure.
Neste artigo demonstrarei os primeiros passos na constru√ß√£o de solu√ß√µes serverless fazendo uso para isto do .NET Core e do servi√ßo Azure Functions, com a implementa√ß√£o de uma aplica√ß√£o no Portal do Azure.
O primeiro passo para a implementa√ß√£o de Azure Functions na nuvem consiste na cria√ß√£o de uma Function App, recurso ao qual poder√£o estar vinculadas v√°rias fun√ß√µes:
Ao gerar uma nova Function App preencher as seguintes configura√ß√µes:
E:
Concluir a gera√ß√£o da Function App acionando o bot√£o Create.
Consultando o grupo de recursos Serverless aparecer√£o a Function App e demais itens criados para esta aplica√ß√£o:
Acessando a Function App groffefunctions podemos iniciar a implementa√ß√£o de uma nova Azure Function atrav√©s da op√ß√£o + New function:
O tipo da Azure Function a ser criada ser√° Webhook + API, o que far√° com que o disparo da mesma aconte√ßa mediante o envio de requisi√ß√µes HTTP:
Ao final deste processo aparecer√° a fun√ß√£o HttpTrigger1, com sua implementa√ß√£o default:
Neste momento muitos se perguntar√£o: como posso modificar o nome de uma Azure Function criada a partir do Portal?
Acessando o item Console veremos que a Azure Function HttpTrigger1 e seu conte√∫do est√£o organizados como um diret√≥rio vinculado √† Function App:
Podemos voltar um n√≠vel de diret√≥rio acima com a instru√ß√£o cd.., modificando em seguida o nome da fun√ß√£o com a instru√ß√£o:
rename HttpTrigger1 Saudacao
Atualizando a Function App groffefunctions ser√° listada agora a fun√ß√£o Saudacao:
Alterar agora o c√≥digo de Saudacao para o conte√∫do da listagem a seguir:
Concluir os ajustes acionando o bot√£o Save. Ser√° iniciada ent√£o a compila√ß√£o da Azure Function, com o resultado deste procedimento aparecendo em Log (a pr√≥xima imagem mostra sucesso ao compilar a fun√ß√£o):
Para testar a fun√ß√£o Saudacao acessar a op√ß√£o Get function URL:
Aparecer√° agora um popup com uma URL que dever√° ser copiada acionando o bot√£o Copy:
Um teste via browser com a URL:
https://groffefunctions.azurewebsites.net/api/Saudacao?code=t6WIPHiER0RY0LnxeT2UKAW6WF7zUFBWGKfblNmMeyVoOnO5WahAEQ==&nome=Renato
Produzir√° como retorno:
J√° um teste via Postman com o envio de um nome no corpo da requisi√ß√£o ir√° gerar como retorno:
Recentemente aconteceu tamb√©m uma live no Canal .NET sobre a implementa√ß√£o de solu√ß√µes Serverless empregando Azure Functions. A grava√ß√£o est√° dispon√≠vel no YouTube e pode ser assistida gratuitamente por todos aqueles interessados em conhecer mais sobre as tecnologias mencionadas neste artigo:
Azure Functions documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
103 
103¬†claps
103 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-cloud-market-share-2019-what-the-latest-data-shows-dc21f137ff1c?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 12, 2019¬∑5 min read
Q1 earnings are in for the ‚Äòbig three‚Äô cloud providers and you know what that means ‚Äî it‚Äôs time for an AWS vs Azure vs Google Cloud market share comparison. Let‚Äôs take a look at all three providers side-by-side to see where they stand.
Note: a version of this post was originally published in April 2018. It has been completely rewritten and updated for 2019.
To get a sense of the AWS vs Azure vs Google Cloud market share breakdown, let‚Äôs take a look at what each cloud provider‚Äôs reports shared.
Amazon reported Amazon Web Services (AWS) sales of $7.7 billion, compared to $5.44 billion at this time last year. AWS revenue grew 41% in the first quarter ‚Äî at this time last year, that number was 49%.
Across the business, Amazon‚Äôs growth rates are slowing down ‚Äî which perhaps is all that can be expected at their mammoth size. However, their profit margins are increasing, giving investors a boon of $7.09 earnings per share compared to the projected $4.72.
AWS has been a huge contributor to this growth. This quarter, AWS revenue makes up 13% of total Amazon sales, up from 10% in the fourth quarter. AWS only continues to grow, and bolster the retail giant time after time.
In media commentary, AWS‚Äôs numbers seem to speak for themselves:
While Amazon breaks out revenue from AWS separately, Microsoft has a more nebulous ‚Äúcommercial cloud business‚Äù ‚Äî which includes not only Azure, but Office 365, Dynamics 365, and other segments of the Productivity and Business Processes Division. This fact frustrates many pundits as it simply can‚Äôt be compared directly to AWS, and inevitably raises eyebrows about how Azure is really doing. Microsoft reported that the commercial cloud business grew 41% in the first three months of 2019, to $9.6 billion.
What Microsoft reported for Azure specifically is the growth rate: 73%. However, Microsoft did not specify what that growth actually represents. This time last year, the Azure growth rate was reported at 93%. Supposedly, analysts say that Azure is growing at a faster rate than AWS was at a similar size, but without specific numbers, it‚Äôs hard to say what this actually means.
Here are a few headlines on Microsoft‚Äôs reporting that caught our attention:
Like Microsoft, Google avoided reporting specific revenue numbers for its cloud business yet again. Parent company Alphabet reported $36.34 billion in revenue for the quarter, up 17% from $31.15 billion for the same quarter last year. Google Cloud Platform revenue is included in Google‚Äôs ‚Äúother‚Äù revenue category, alongside G Suite, Google Play, and hardware such as Nest. That category reported revenue of $5.45 billion for the quarter, up 25% from the same quarter last year when it was $4.25 billion.
According to Google and Alphabet CFO Ruth Porat, ‚ÄúGoogle Cloud Platform remains one of the fastest growing businesses in Alphabet with strong customer momentum reflected in particular in demand for our compute and data analytics products‚Äù. But without specifics, it‚Äôs hard to say what this means.
Further reading on Google‚Äôs quarterly reporting:
When we originally published this blog last year, we included a market share breakdown from analyst Canalys, which reported AWS in the lead owning about a third of the market, Microsoft in second with about 15 percent, and Google sitting around 5 percent.
This year they report an overall growth in the cloud infrastructure market of 42%. By provider, AWS had the biggest sales gain with a $2.3 billion YOY increase, but Canalys reports Azure and Google Cloud with bigger percentage increases.
Ultimately, it seems clear that in the case of AWS vs Azure vs Google Cloud market share ‚Äî AWS still has the lead.
Bezos has said, ‚ÄúAWS had the unusual advantage of a seven-year head start before facing like-minded competition. As a result, the AWS services are by far the most evolved and most functionality-rich.‚Äù
Our anecdotal experience talking to cloud customers often finds that true, and it says something that Microsoft and Google aren‚Äôt breaking down their cloud numbers just yet.
Others have made their own estimates. In November, a Goldman Sachs report stated that AWS, Azure, Google Cloud, and Alibaba Cloud made up 56% of the total cloud market, with that projected to grow to 84% this year. The report shows AWS far, far in the lead with 47% of the market projected for this year, with Azure and Google trailing at 22% and 8% market share, respectively.
AWS remains far in the lead for now. With that said, it will be interesting to see how the actual numbers play out, especially as Google positions itself for multi-cloud and Azure continues rapid growth rates. Perhaps this time next year will report revenue numbers broken out and we‚Äôll be able to say for sure.
Originally published at www.parkmycloud.com on April 30, 2019.
CEO of ParkMyCloud
41 
2
41¬†
41 
2
CEO of ParkMyCloud
"
https://medium.com/@mentormate/the-microsoft-azure-development-practices-you-should-know-ebf28d687e9?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
MentorMate
Feb 23, 2017¬∑14 min read
[Original Post Here]
Cloud computing is the set of technologies and infrastructure capabilities offered in a utility-based consumption model. Microsoft Cloud Computing offers Microsoft Azure, previously known as Windows Azure, to help its customers realize the benefits of cloud computing.
Applying existing development ideologies to Microsoft Azure is not straightforward, as direct one-to-one conversion is rarely possible. How should you alter your methodology and what Microsoft Azure development best practices should you use?
Cloud computing that replaces the traditional on-premise (locally built and deployed) software is broadly divided into three categories:
The development lifecycle of software that uses the Azure platform mainly follows two processes:
During the application development stage the code for Azure applications is most commonly built locally on a developer‚Äôs machine. Microsoft has recently added additional services to Azure Apps named Azure Functions. They are a representation of ‚Äòserverless‚Äô computing and allow developers to build application code directly through the Azure portal using references to a number of different Azure services.
The application development process includes two phases: 1) Construct + Test and 2) Deploy + Monitor.
In the development and testing phase, a Windows Azure application is built in the Visual Studio IDE (2010 or above). Developers working on non-Microsoft applications who want to start using Azure services can certainly do so by using their existing development platform. Community-built libraries such as Eclipse Plugins, SDKs for Java, PHP or Ruby are available and make this possible.
Visual Studio Code is a tool that was created as a part of Microsoft efforts to better serve developers and recognize their needs for lighter and yet powerful/highly-configurable tools. This source code editor is available for Windows, Mac and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js. It also has a rich ecosystem of extensions and runtimes for other languages such as C++, C#, Python, PHP and Go.
That said, Visual Studio provides developers with the best development platform to build Windows Azure applications or consume Azure services.
Visual Studio and the Azure SDK provide the ability to create and deploy project infrastructure and code to Azure directly from the IDE. A developer can define the web host, website and database for an app and deploy them along with the code without ever leaving Visual Studio.
Microsoft also proposed a specialized Azure Resource Group deployment project template in Visual Studio that provides all the needed resources to make a deployment in a single, repeatable operation. Azure Resource Group projects work with preconfigured and customized JSON templates, which contain all the information needed for the resources to be deployed on Azure. In most scenarios, where multiple developers or development teams work simultaneously on the same Azure solution, configuration management is an essential part of the development lifecycle.
Microsoft Azure provides a number of different ways to build and deploy an application. They can be grouped in several categories by Azure cloud service type:
Azure App Service can be used for easy publishing of web-based projects in one of the following categories:
Azure Web Apps are built with continuous integration and delivery in mind. They support various tools like GitHub Webhooks, Jenkins, Visual Studio Team Services, TeamCity, etc.
Azure Virtual Machines are a well-known standard in VMs with a large collection of pre-configured Windows or Linux Servers. VMs allow full control over machine configuration along with responsibility for support, updates, software installations and administration. Basically, they offer the Azure IaaS layer (infrastructure-as-a-service).
Azure Functions are the ‚Äòserverless‚Äô style response to business‚Äô needs and developers‚Äô demands to be able to write code directly through the web portal. With Azure Functions, a team can consistently and securely connect each function to different Azure services.
Azure Service Fabric is used to package, deploy and manage scalable and reliable microservices.
Azure Cloud Services is the PaaS layer of Azure, lying between App Service and Virtual Machines. Cloud Services employ the App Service approach to deliver various capabilities that enable developers to build n-tier cloud apps with more control over the OS.
This Azure feature functions as a kind of OS virtualization allowing teams to deploy applications in a more efficient and predictable way. The containerized application approach works the same way in development, in test and in production systems. Azure Container Service supports standard Docker tools to manage containers.
Microsoft has dramatically extended their source control solutions in the cloud by adding Visual Studio Team Services ‚Äî a wide range of tools and services to support DevOps work for continuous integration and delivery. With support for a range of tools including Jenkins, GitHub, Puppet, Chef, TeamCity, Ansible and VSTS for Azure services, a developer can work with existing tools enabling productivity and maximizing his/her experience.
After the application development is complete, development teams can move on to the quality assurance stage. Unit testing of the code is normally completed locally by the developer using different types of testing tools, such as automated unit tests and web tests. Azure Developer Tools contain emulators for some of the Azure environments, including cloud services and storage. These tools are provided for developers to unit test their application code before deploying it to Azure.
To ensure a smooth transition of the application to the cloud, I recommend using the following method:
Microsoft has implemented remote debugging for Visual Studio. The remote debugging process can be attached to any App Service (web or running in background) through the Visual Studio UI using Server or Cloud Explorer abilities. I recommend a hybrid approach to ensure our code will run on Azure ‚Äî test locally using emulators and then move the application to cloud.
There are a few steps to take in order to do an Azure deploy. First the code is tested and then synchronized with the source code manager. Then the engine and versions are added to all assemblies. And finally after the code is compiled, a package is created so it can be uploaded to the Azure platform.
All applications are prepared for a release and published at some point in their lifecycle. Uploading to the environment though is different in our case. We need to validate our Azure hosting environment first. Then, we need to focus on the host, management and execution of our cloud application. After that, we purchase a subscription, and finally, we can manually upload our deployment package to our staging or production environments.
Microsoft provides several tools that can help optimize and automate the deployment process. Some of them are well-known: MSBuild scripts and the Azure Service Management API. The API can be used to interface with the management portal and create a package (one config file and one package file) directly from a VS project. That package can then be uploaded to the specific cloud environment (Azure Service). An alternative method to do that is using some of the DevOps tools I referred to above in order to deploy to Azure.
With your application up and running in Azure, you need to be able to monitor performance, watch for issues and see how customers are using your app.
Azure provides several monitoring options:
Finally, a developer can use the Azure Management portal to manage an already deployed application. Each Azure service provides access to a few managed environments ‚Äî the so-called deployment slots. They can be used for Staging, Production or any other environment we want to deploy to and use in the Azure cloud. There is no rule dictating the environments we must use. We can deploy our application directly to the production environment or deploy it to staging.
Microsoft provides a SWAP procedure that will do a transition (switch) between one deployment slot and another one. It is important for the production environment IP address to stay the same. That is the Virtual IP address on Azure. Any change to the Public IP Address will lead to changes in all DNS records and name servers. Microsoft tries to set as standard practice using CNAME records when we configure our DNS servers. Azure provides a DNS service (DNS Zone) as well.
As a part of some teams‚Äô Microsoft Azure development best practices, the SWAP process can be automated to run each time new code is pushed as part of the continuous delivery procedure. It can even be used in preview mode so we can be absolutely sure that it will work after the SWAP process has finished.
From a development perspective, I recommend deploying the package first to the staging environment, so the application can be tested in a separate QA-like environment. Then, promote it to production for release. This approach will ensure the tested release of the application is ready for public use.
According to Microsoft Azure development best practices, the Deployment & Release stage involves the following two key phases.
System Testing. In this phase, different application tests are carried out. The tests can start with a basic ‚ÄúSmoke Test‚Äù to ensure that all basic functionality of the deployed services is running as it should on the cloud. This can be followed by ‚ÄúIntegration Testing‚Äù to ensure that all external touch points to the services are functioning as expected. Subsequently followed by a ‚ÄúUser Acceptance Test‚Äù, the services would be tested by a sample set of the application users. These are a representative set of tests that can be carried out as a part of the development lifecycle on the Azure platform, which an enterprise can run as a part of their standard project delivery practices.
All of these tests are carried out on the Azure platform without any investments required for procuring or setting up separate test environments on-premise. These tests can be carried out in the default staging environment, as discussed previously, or separate environments, in the case of larger projects, to carry out each test.
Production Release. After all of the test cycles have been completed, the tested Azure Service‚Äôs code can be released into production by promoting the services from the Windows Azure portal. The promoted services will execute from the production regions of the Azure data center fabric. As a part of the promotion process, the hosted services will have a public facing URL like ‚Äúhttp:// [project].cloudapp.net‚Äù, where [project] is the name of the project defined at the time of creating a new hosted instance. In the production stage, the services are configured, managed and monitored from the Windows Azure portal.
Some of the activities an administrator can control and manage are:
Unlike traditional on-premise systems, Azure does not provide full control of the computing environment and resources to administrators. Azure‚Äôs datacenter is fully managed and controlled by Microsoft. Users are only provided with private virtualized instances, packaged as services, as the unit of computation and storage for hosting their apps.
This will require a shift in the IT support operations. The Azure of today does not permit administrators to deploy some of the custom or third-party tools, utilities, agents, etc., that might be in use extensively with the existing operational processes to support and administer. Administrators use these tools to investigate production-related issues such as poor performance and crashes.
A cloud-based application can run into different problems related to environments and architecture. Microsoft works consistently to avoid most of the problems by creating and proposing specific patterns to be used during the application development process. They are grouped in following areas.
The time that the system is functional and working. It will be affected by system errors, infrastructure problems, malicious attacks and system load. It is usually measured as a percentage of uptime.
A key feature of most cloud applications is the ability to manage data under different conditions. Data is typically hosted in different locations and across multiple servers for reasons such as performance, scalability or availability, and this can present a range of challenges.
Microsoft Azure development best practices for design considerations should include consistency and coherence in component design and deployment, maintainability to simplify administration and development and the reusability of components and subsystems. Decisions made during the design and implementation phase have a huge impact on the quality and the total cost of ownership of cloud-hosted applications and services.
Cloud-based applications need a reliable communication channel that connects components and services ideally in a loosely coupled manner in order to maximize the scalability of distributed applications. Asynchronous messaging is widely used and provides many benefits, but also brings challenges such as the ordering of messages, poison message management and idempotency etc.
Most cloud applications use the PaaS layer of the cloud. It brings great flexibility, scalability and expense reduction in development and administration efforts but can make management and monitoring more difficult than an on-premises deployment. Applications must expose runtime information that administrators and operators can use to manage and monitor the system, as well as support changing business requirements and customization without requiring the application to be stopped or redeployed.
Performance is an indication of the responsiveness of a system to execute any action within a given time interval, while scalability is the ability of a system either to handle increases in load without an impact on performance or increase available resources. Cloud applications typically encounter variable workloads and peaks in activity. Predicting these, especially in a multi-tenant scenario, is almost impossible.
Resiliency is the ability of a system to gracefully handle and recover from failures. The nature of cloud hosting, where applications are often multi-tenant, 1) use shared platform services, 2) compete for resources and bandwidth, 3) communicate over the Internet and 4) run on commodity hardware. This means there is an increased likelihood that both transient and more permanent faults will arise. Detecting failures, and recovering quickly/efficiently, is necessary to maintain resiliency.
Security is the capability of a system to prevent malicious or accidental actions outside of the designed usage and to prevent disclosure or loss of information. Cloud applications are exposed on the Internet outside the trusted on-premise boundaries. They are often open to the public, and may serve untrusted users. Applications must be designed and deployed in a way that protects them from malicious attacks, restricts access to only approved users and protects sensitive data.
Rather than sharing final comments, I‚Äôve added the most-used design patterns for the cloud.
This pattern can improve the stability and resiliency of an application ‚Äî handling faults that may take a variable amount of time to rectify when connecting to a remote service or resource.
Enable multiple concurrent consumers to process messages received on the same messaging channel. This pattern enables a system to process multiple messages concurrently to optimize throughput, to improve scalability and availability and to balance the workload.
Delegate authentication to an external identity provider. This pattern can simplify development, minimize the requirement for user administration and improve the user experience of the application.
Indexes are a key feature of database management. Currently, they are presented in a new light for the cloud-based solutions.
The pre-populated view has returned to support the performance of the cloud data services. When the data is spread over couple of data tables it is better to combine it in a special view for the actual application needs.
Communication between the application in the cloud or an app in a hybrid environment is of high importance for the stability, the performance and the quality of the solution. To achieve better scalability and loose coupling, one can modify the project architecture by organizing the communication between the different layers of the solution through a message queue-like communication channel. Implementing messaging communication can minimize the impact on availability and responsiveness for both the task and the service in peak traffic situations.
To create a stable cloud application, the pattern must be applied on almost every layer of the application that communicates with external services. For example database or storage access, web services access, etc.
This pattern can reduce the requirement for potentially expensive computing instances. With the Static Host Couple Pattern, static content is deployed to a cloud-based storage service that can deliver the resources directly to the client.
Innovate with us! Click here to access all of our free resources. Authored by Alexander Dimitrov.
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
See all (1,229)
61 
2
61¬†claps
61 
2
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/text-annotation-on-a-budget-with-azure-web-apps-doccano-b29f479c0c54?source=search_post---------127,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aaron (Ari) Bornstein
Jan 14, 2019¬∑6 min read
TLDR: This post walks through how to deploy Doccano on Azure Web Apps in order to collaboratively annotate text data for natural language processing tasks.
All code for this post can be found here:
github.com
Doccano is an open source tool that provides annotation features for text classification, sequence labeling and sequence to sequence.
Recently I‚Äôve been doing work on annotating a dataset for co-reference as part of this task I had time to evaluate a couple of different text annotation platforms.
Most free open source annotation tools such as Brat and Anafora, don‚Äôt abide by modern UX principles. Doccano is the only open source annotation tool I‚Äôve seen with a modern UX experience. While other modern text annotations tools exist like Prodigy and LightTag exist but they have very expensive licenses.
However to collaborate Doccano we need to host the site somewhere to make this process easier this tutorial will show you how.
Azure App Service not only adds the power of Microsoft Azure to your application, such as security, load balancing, autoscaling, and automated management. You can also take advantage of its DevOps capabilities, such as continuous deployment from Azure DevOps, GitHub, Docker Hub, and other sources, package management, staging environments, custom domain, and SSL certificates.
If you have an existing Azure subscription can get started annotating you data just by clicking the button below to auto deploy .
Otherwise you can get a free Azure Account here and then click the deploy button above.
azure.microsoft.com
After the deployment navigate to the following url where {appname} is the appname you choose above.
https://{appname}.azurewebsites.net/login
For example in our deployment above the login url would be
https://doccana.azurewebsites.net/login
This will bring you in to the Doccano login page where you can login with the Admin_user and Admin_pass you configured in the deployment.
Now you are set to begin annotating your own data check out the instructions from the Doccano github . The following steps are taken verbatim from the tutorial.
Here we take an NER annotation task for science fictions to give you a brief tutorial on doccano.
Below is a JSON file containing lots of science fictions description with different languages. We need to annotate some entities like people name, book title, date and so on.
books.json
We need to create a new project for this task. Logging in with the superuser account.
To create your project, make sure you‚Äôre in the project list page and click Create Project button. As for this tutorial, we name the project as sequence labeling for books, write some description, choose sequence labeling project type and select the user we created.
After creating a project, we will see the ‚ÄúImport Data‚Äù page, or click Import Data button in the navigation bar. We should see the following screen:
We choose JSON file books.json to upload. After uploading the dataset file, we will see the Dataset page (or click Dataset button list in the left bar). This page displays all the documents we uploaded in one project.
Click Labels button in left bar to define our own labels. We should see the label editor page. In label editor page, you can create labels by specifying label text, shortcut key, background color and text color.
As for the tutorial, we created some entities related to science fictions.
Next, we are ready to annotate the texts. Just click the Annotate Data button in the navigation bar, we can start to annotate the documents.
After the annotation step, we can download the annotated data. Click the Edit data button in navigation bar, and then click Export Data. You should see below screen:
Here we choose JSON file to download the data by clicking the button. Below is the annotated result for our tutorial project.
sequence_labeling_for_books.json
Congratulation! You just mastered how to use doccano on Azure for a sequence labeling project.
If you have any questions, comments, or topics you would like me to discuss feel free to follow me on Twitter if there is a milestone you feel I missed please let me know. Thanks to Hironsan for the amazing work!
Aaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
See all (62)
263 
2
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.¬†Take a look.
263¬†claps
263 
2
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/vsts-is-dead-long-live-azure-devops-6f184baa7e56?source=search_post---------128,"Starting today, if you use Azure you will get DevOps, whether you want it or not!
In the past few years everyone has been talking about DevOps and how important it is for every successful software team to have proper DevOps solutions.
But before today, people who used Azure, they needed to plan for their own DevOps pipeline for CI/CD, source code repository, work item‚Ä¶
"
https://read.acloud.guru/azure-functions-wants-to-make-it-easy-for-developers-to-get-started-with-serverless-896766af985f?source=search_post---------129,"When it comes to trendy buzzwords, ‚Äúserverless‚Äù might be the most popular.
At the heart of the serverless paradigm is the Function as a Service (FaaS) model, a category of services that make it ridiculously easy to run code in the cloud without provisioning any compute infrastructure.
FaaS has truly been a game-changer: it considerably accelerated the ability to deploy complex backend services and democratized application development. Gone are the days when companies need to invest capital and resources to take their ideas to market. Now, all you need is an idea, good code, and an account with a major cloud provider. Within minutes, your application can be running on managed infrastructure, serving millions of requests, and scaling (virtually) infinitely.
This article will compare the FaaS services of Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) and offer some insight into one of the most transformative technologies of the modern cloud age!¬†
At the core of any application is the code that makes up its logic and functionality. Whether it is the latest mobile game, or your typical boring enterprise finance software, there are lines of code (sometimes thousands of lines) that need to run somewhere. This ‚Äúsomewhere‚Äù is typically a server, or groups of servers, where CPU cycles execute the logic that powers these applications.
But servers, even virtual cloud servers, are expensive and can be a pain to maintain, often requiring highly trained and experienced administrators to secure and manage them. Additionally, when no users are playing that game or using the finance software, these expensive servers will sit idly, virtually twiddling their thumbs, waiting for new ‚Äúwork‚Äù to come in.¬†
Traditional compute infrastructure can be very inefficient, and that is exactly what makes FaaS so appealing.
Instead of standing up complex infrastructure (servers, load balancers, etc), FaaS lets you run your code on a managed pool of compute resources, while paying only for the duration of execution.¬†
FaaS functions are event-driven, meaning they run in response to certain events. While not always the case, those functions are often short-lived, ephemeral, stateless and single-purpose.
The table below is a brief summary of the FaaS services offered by AWS, Azure, and GCP:
One of the main reasons for the popularity of FaaS is cost: it is dirt-cheap, and in many cases, practically free. With that being said, like everything else in the cloud, that price tag could dramatically change as the scale gets larger.¬†
The two main contributors to FaaS cost are:
Additionally, other charges like data transfer or storage costs might apply, but those depend on the specific use case.
All of the three major providers have a monthly free tier quota, and cost is incurred after that quota is reached. If you are experimenting or building a proof-of-concept, you will most likely be covered in the free tier.
The table below compares what each provider offers in terms of free tier quota, and how much additional usage costs.
Takeaway: The pricing structure is almost identical across all three providers.
AWS and Azure have identical pricing and free monthly quotas, while GCP offers an extra 1 million free requests per month, and has comparable pricing for additional requests. Both AWS and Azure round their duration to the nearest 1ms, while GCP rounds to the nearest 100ms increment, which could add to the overall cost at scale.
Get the Cloud Dictionary of PainSpeaking cloud doesn‚Äôt have to be hard. We analyzed millions of responses to ID the top concepts that trip people up. Grab this¬†cloud guide¬†for succinct definitions of some of the most painful cloud terms.
There are more programming languages out there than there are Star Wars sequels, prequels, and spin-offs. And for obvious reasons, every programming language has strengths and weaknesses, so one needs to pick the right tool for the right job. Cloud providers, for the most part, support most of the popular languages in their respective FaaS offering.
The table below shows the currently supported FaaS runtimes for AWS, Azure, and GCP:
Takeaway: Very comparable language support between the three major providers, with the only notable exceptions of GCP lacking support for PowerShell, and Azure lacking support for Go (a rather interesting observation considering the fact that Go was developed by Google and PowerShell by Microsoft!)
If your language of choice is not listed above, it is still possible to ‚Äúbring your own runtime‚Äù, which allows you to implement a provider‚Äôs FaaS in any programming language. See the table below for current support for custom runtimes.
Early in the days of FaaS, the most common use case was to ‚Äúkick the tires‚Äù or ‚Äúmock something up‚Äù before moving to a more mature solution. Those days are gone, and today it is not uncommon to find global-scale production apps running fully on a FaaS backend.
To achieve this, it is important to understand how cloud providers scale FaaS workloads in response to increased demand, and how they handle concurrent requests.
As mentioned earlier, FaaS functions are event-driven, so when an event is received, an instance of the function is spun up and the request is processed. The instance is kept alive to process subsequent events, and if none are received within a certain time frame, it is recycled.
All three providers advertise virtually unlimited and automatic scaling, although there are other factors that might be at play here, which are discussed below.
When a request is being processed by a FaaS instance and another request is received, a second instance is spun up to process this additional request (instances process only one request at a time.) Concurrency is the number of simultaneous functions that can run at any given time.
AWS is the only provider to offer highly customized concurrency management options, while Azure and GCP are a little vague on how concurrent executions are handled.
Given the relatively young age of FaaS as a technology, it has many detractors and naysayers. Their favorite critique, by far, is the notion of ‚Äúcold starts‚Äù.¬†
So what is a cold start, anyway?
Imagine this familiar action movie scene: our hero is racing down the highway at 120 mph, probably on his way to save some lives, when he zooms past an unsuspecting highway trooper, on his break, reading a newspaper. By the time the trooper fumbles to start his engine and gets up to highway speed to give chase, our hero is miles away already, and the trooper will need some time to catch up.¬†
Similarly, a FaaS instance in an inactive state will require some additional time to respond to a request. This initial delay encountered is known as a cold start.
Contrast that with an FaaS instance that is already in an active state and receives a request, in this case no initial delay is experienced and the instance can start processing the request almost instantly. Using our example, this would be analogous to a highway trooper driving down the highway at normal speed when he gets passed by a speeding driver. In that case, the trooper can very quickly accelerate and catch up in mere seconds.
While cloud providers do not publish their cold start statistics, the following table shows estimated averages as observed by industry analysts:
Cold starts can affect the performance of FaaS workloads that are very sensitive to delay but there are ways to mitigate it, and it appears to affect the Azure platform more than its two competitors. Additionally, AWS now offers ‚Äúprovisioned concurrency‚Äù as an approach to eliminate cold starts with Lambda. We discuss this feature in more detail later in the article.
Automating AWS Cost OptimizationAWS provides unprecedented value to your business, but using it cost-effectively can be a challenge. In this free, on-demand webinar, you‚Äôll get an overview of AWS cost-optimization tools and strategies.
Not all functions are created equal, so different workloads might require different settings to optimize performance.
Depending on how resource-hungry the code is, memory will have to be adjusted accordingly. If the memory allocated is too low, a function will take longer to execute and could potentially time out, but if the memory is set too high, you might end up over-paying for unused resources.
Cloud providers offer different maximum memory configurations, while CPU power is linearly and automatically configured in proportion to the amount of memory chosen.
Of note is the maximum memory that can be configured on GCP Cloud Functions: at 4096 MB, that limit is considerably lower than what AWS and Azure offer.
The other configurable aspect of FaaS is the maximum execution time. While most functions in the wild take seconds (or less) to execute, some intensive workloads can potentially take much longer, on the order of minutes, or even hours (for example, intensive machine learning or data analysis workloads).
The table below shows the maximum timeouts that each cloud provider offers:
It is important to note that increasing the timeout is not always the solution, and it should be considered in conjunction with adjusting the memory.
Post-COVID DevOps: Accelerating the FutureHow has COVID affected ‚Äî or even accelerated ‚Äî DevOps best practices for engineering teams? Watch this free, on-demand webinar panel discussion with DevOps leaders as we explore DevOps in a post-COVID world.
As discussed earlier, functions deployed on a FaaS are, by nature, stateless. In other words, functions are not aware of other functions or of the execution results of other functions.
Even invocations of the same function are completely independent of each other. This stateless paradigm is what makes FaaS so scalable and easy to provision.
While the stateless approach is excellent for executing a large number of short-lived and single-purpose functions (for example, a contact form on a website), it makes it difficult to build any meaningful complex applications that often require some sort of state management. Realizing this, cloud providers built orchestration services that integrate with these functions as ‚Äústeps‚Äù in a workflow, where the output of one step can be passed as input to another step. This enabled building fairly complex workflows in a completely serverless approach!
The following table lists what each provider offers for orchestration services. Those services are often very scalable as well and have many features that are beyond the scope of this article:
The power of FaaS services lies in the fact that they are event-driven, meaning certain ‚Äúinteresting events‚Äù can trigger an execution of the function.¬†
‚ÄúWhat are interesting events?‚Äù you might ask.
Anything from a simple cron schedule (example: run this function every day at midnight) to other services within the cloud provider‚Äôs ecosystem (for example, run this function when a file is uploaded to cloud storage). But one of the most popular scenarios is integrating FaaS with an HTTP endpoint.¬†
A static web frontend (i.e. HTML/CSS/JavaScript) with an integrated FaaS backend is a very common and popular architectural pattern for building serverless web apps.¬†
While all three providers support HTTP integration, AWS requires provisioning and configuring a separate resource, API Gateway, which is billed separately as well. Azure and GCP have a much more streamlined HTTP integration.
Given the various needs in terms of availability and latency required as discussed earlier, cloud providers reacted by providing different tiers of availability baked into their FaaS offering.
AWS Lambda historically came as one basic hosting plan, but more recently AWS started offering Provisioned Concurrency, which ensures that functions are initialized and ready to respond to events, cutting down the dreaded cold-start time to mere milliseconds. Azure offers a more complex variety of hosting options, while GCP just offers a one-size-fits-all plan.
When it comes to comparing the FaaS offerings of the three major cloud providers, one thing is very obvious: they are extremely similar and comparable, both in terms of features and cost.¬†
While AWS Lambda is the more mature and most popular of the three, Azure Functions appears to have some very similar features and in some ways, more options to accommodate edge cases. GCP Cloud Functions has a few less bells and whistles but is still fairly comparable to the other two.
The devil, as they say, is in the details. Most likely, there are other factors to consider when comparing these three FaaS services. But whatever the case is, the key takeaway here is that FaaS is here to stay, and it is a truly transformative technology that will only continue to gain adoption, so if you have not already, make sure that you are leveraging it!
Level up your cloud career
Looking to get certified or level up your cloud career? Learn in-demand cloud skills by doing with ACG‚Äôs courses, labs, learning paths and sandbox software.
Get more insights, news, and assorted awesomeness around all things cloud learning.
"
https://posts.specterops.io/introducing-bloodhound-4-0-the-azure-update-9b2b26c5e350?source=search_post---------130,NA
https://medium.com/bb-tutorials-and-thoughts/how-to-pass-microsoft-azure-dp-900-data-fundamentals-exam-180aebdc27b2?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is one of the leading cloud providers. This exam can be taken as an optional first step in learning about Databases and how those concepts are exemplified by Microsoft Azure. It can be taken as a precursor to Microsoft Azure Database Engineer. While it would be a‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/power-bi-connects-to-azure-databricks-44bea6731be7?source=search_post---------132,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Mar 6, 2020¬∑6 min read
Microsoft PowerBI is becoming more and more popular recently as a Data Analytics tool. Also, It is ubiquitous for a company to have a whole bucket of Microsoft products that include Azure.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ericsk/azure-iot-edge-azure-custom-vision-raspberry-pi-3-%E5%AF%A6%E7%8F%BE%E9%82%8A%E7%B7%A3%E9%81%8B%E7%AE%97-edge-computing-%E4%B8%8A%E7%9A%84%E9%9B%A2%E7%B7%9A%E9%9B%BB%E8%85%A6%E8%A6%96%E8%A6%BA%E6%87%89%E7%94%A8-94d6bde2b244?source=search_post---------133,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eric ShangKuan
Jul 8, 2018¬∑15 min read
Azure IoT Edge ÊòØ Microsoft Azure ‰∏äÁöÑ Edge Computing Ëß£Ê±∫ÊñπÊ°àÔºåÂÆÉÊúÉÂú® Windows/Linux ÁöÑÊ©üÂô®‰∏äÂü∑Ë°å‰∏ÄÂÄã Azure IoT Edge RuntimeÔºåÈô§‰∫Ü‰øùÊåÅËàáÈõ≤Á´ØÔºàAzure IoT HubÔºâÈÄ£Êé•ÁöÑÁÆ°ÈÅì‰πãÂ§ñÔºå‰πüËÆìËªüÈ´îÈñãÁôº‰∫∫Âì°ËÉΩÂ§†ÊääÁ®ãÂºèÁ∂ìÈÅé IoT Edge SDK ÂèäÂÖ∂Â∑•ÂÖ∑ÈèàÊâìÂåÖÊàê container image ÂæåÔºåÈÉ®ÁΩ≤Âà∞ IoT Edge Runtime ‰∏äÂü∑Ë°åÔºåÈÄôÊòØÂÆÉÂØ¶Áèæ edge computing ÁöÑÊñπÂºè„ÄÇ
Âè¶‰∏ÄÊñπÈù¢ÔºåAzure Custom Vision ÊòØ‰∏ÄÂÄãÂπ´Âä©ËªüÈ´îÈñãÁôºÂúòÈöäÔºåËÉΩÂ§†Âø´ÈÄüÊâìÈÄ†‰∏ÄÂÄãÊô∫ÊÖßÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÁöÑÂ∑•ÂÖ∑ÔºåÈÄèÈÅé Azure Custom Vision ÁöÑÂπ≥Âè∞Ôºå‰Ω†Âè™ÈúÄË¶ÅËíêÈõÜÂúñÁâá„ÄÅÊ®ô‰∏äÊ®ôÁ±§Ôºå‰∏çÈúÄË¶ÅÊí∞ÂØ´Á®ãÂºèÁ¢ºÔºåÂ∞±ËÉΩÊúâ‰∏ÄÂÄãÂ±¨Êñº‰Ω†Ëá™Â∑±ÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÔºåÈô§‰∫ÜÂèØ‰ª•ÈÄèÈÅéÂπ≥Âè∞Â∞áÊ®°Âûã host Êàê‰∏ÄÂÄã web serviceÔºå‰ΩÜ Custom Vision ÊúÄÂ§ßÁöÑÁâπËâ≤Â∞±ÊòØ‰Ω†ËÉΩÂåØÂá∫Ë®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÔºà‰∏¶‰∏çÊòØÊâÄÊúâÊ®°ÂûãÈÉΩËÉΩÂåØÂá∫ÔºâÂà∞ iOS (CoreML)„ÄÅAndroid (Tensorflow)„ÄÅWindows (ONNX)„ÄÅ‰ª•Âèä Azure IoT Edge ‰∏äÂü∑Ë°åÔºåÂØ¶ÁèæÈõ¢Á∑öÊô∫ÊÖßÈõªËÖ¶Ë¶ñË¶∫ÈÅãÁÆóÁöÑÊ¶ÇÂøµ„ÄÇ
ËÄå Raspberry Pi (RPi) ÊòØ‰∏ÄÂÄãÂÆπÊòìÂèñÂæó„ÄÅÁîüÊÖãÁ≥ªÂèàË±êÂØåÁöÑÈñãÁôºÊùøÔºåÊâÄ‰ª•ÊàëÂ∞±Êúâ‰∫ÜÊÉ≥Êää Azure IoT Edge + Azure Custom Vision Âú® RPi ‰∏äÂØ¶ÁèæÁöÑÊßãÊÉ≥ÔºåÊÉ≥ÂØ¶È©óÁúãÁúãÊòØÂê¶ÂèØË°å„ÄÇ
ÈñãÂßã‰πãÂâçÂÅö‰∫Ü‰∏Ä‰∏ãÂäüË™≤ÔºåÁôºÁèæÊàëÁöÑÂêå‰∫ãÂ∑≤Á∂ìÁôºË°®‰∫Ü‰∏ÄÂÄã‰ªñÁöÑÂØ¶È©óÊàêÊûúÂú® GitHub ‰∏äÔºåÊâÄ‰ª•Ë°®Á§∫‰∏¶‰∏çÊòØ‰∏çÂèØË°åÔºå‰ΩÜÁúüÊ≠£Êãø‰ªñÁöÑÊù±Ë•ø‰æÜÈÉ®ÁΩ≤ÈÇÑÊòØÈÅáÂà∞‰∫Ü‰∏Ä‰∫õÂ∞èÂïèÈ°åÔºåÊâÄ‰ª•ÊâçÊúâ‰∫ÜÈÄôÁØáÊñáÁ´†ÁöÑË™ïÁîü„ÄÇ
Ë¶Å‰ΩøÁî® Azure IoT Edge ‰πãÂâçÔºå‰Ω†ÂøÖÈ†àÂú® Microsoft Azure ‰∏äÂª∫Á´ã‰∏ÄÂÄã S1 Á≠âÁ¥ö‰ª•‰∏äÁöÑ Azure IoT HubÔºåÂª∫ÁΩÆÁöÑÊïôÂ≠∏ÂèØ‰ª•ÁúãÈÄôÁ≥ªÂàóÊñáÁ´†Ë™™Êòé„ÄÇ
ÊâÄÊúâË¶ÅËàá Azure IoT Hub ÈÄ£Êé•ÁöÑË£ùÁΩÆÈÉΩÂøÖÈ†àÂú® IoT Hub ‰∏äË®ªÂÜäË£ùÁΩÆË≠òÂà•ÔºåÂèÉËÄÉÈÄôÁØáÊñáÁ´†Âú® IoT Hub ‰∏äÂª∫Á´ã‰∏ÄÂÄã IoT Edge Ë£ùÁΩÆÔºåÂ¶ÇÊ≠§‰∏Ä‰æÜÊâçÂèØ‰ª•ÊãøÂà∞ÈÄôÂÄãË£ùÁΩÆË≠òÂà•ÁöÑÈÄ£Á∑öÂ≠ó‰∏≤Ôºå‰πãÂæåËÆìÊú¨Êñá‰∏≠ÁöÑ RPi ‰æÜÈÄ£Á∑ö„ÄÇ
ÊàëÈÅ∏ÊìáÂÅöÈÄôÂÄãÂØ¶È©óÁöÑÊùøÂ≠êÊòØÁî® Raspberry Pi 3 Model B+Ôºå ÊîùÂΩ±Ê©ü‰πüÊòØÁî® Pi (NoIR) Camera V2ÔºåÂü∫Êú¨‰∏äÈÄôÊ®£Â∞±ËÉΩ‰æÜÂÅö edge Á´ØÁöÑÈõªËÖ¶Ë¶ñË¶∫‰∫ÜÔºå‰ΩÜÁÇ∫‰∫ÜÁ∞°ÂñÆÊúâÊïàÂú∞Áî¢Áîü‰∏Ä‰∫õËº∏Âá∫ÊïàÊûúÔºåÊâÄ‰ª•ÊàëÂú® RPi ‰∏äÂ§öË£ù‰∏ÄÁâá Sense HATÔºå‰ΩøÁî®ÂÆÉ‰∏äÈù¢ÁöÑ 8x8 LED ‰æÜÂÅö‰∏Ä‰∫õË¶ñË¶∫ÂåñÁöÑËº∏Âá∫ÊïàÊûúÔºå‰æÜÈ°ØÁ§∫Ëæ®Ë≠òÁöÑÁµêÊûú„ÄÇ
‰ΩúÊ•≠Á≥ªÁµ±ÁöÑÈÉ®‰ªΩÔºåÂÆâË£ù‰∫Ü Raspbian StretchÔºåÈñãÊ©üÂæåÂÅöÂπæ‰ª∂‰∫ãÊää Azure IoT Edge Runtime ‰ª•ÂèäË¶ÅÂØ¶È©óÁöÑÁí∞Â¢ÉÊ∫ñÂÇôÂ•ΩÔºö
Êï¥ÂÄã Azure IoT Edge ÁöÑÈñãÁôºÈÉΩÂèØ‰ª•Âú® Visual Studio Code ‰∏äÂÆåÊàêÔºàÈÄô‰πüÊÑèÂë≥ËëóÁî® Windows„ÄÅLinux„ÄÅÊàñ Mac OS X ÈÉΩ‰∏ÄÊ®£ÔºâÔºåËÄåÁÇ∫‰∫ÜËÆìÈñãÁôºÈÅéÁ®ãÊõ¥È†ÜÊö¢ÔºåÂª∫Ë≠∞ÂÆâË£ù‰∏ãÂàó Visual Studio Code ÁöÑÊì¥ÂÖÖÂ•ó‰ª∂‰æÜËºîÂä©Ôºö
‰∏äËø∞ÁöÑÁí∞Â¢ÉÈÉΩÊ∫ñÂÇôÂ∞±Á∑íÂæåÔºåÂ∞±ÂèØ‰ª•ÈñãÂßãË¶èÂäÉÂèäÈñãÁôºÈÄôÂ•ó Edge computing ÁöÑÂØ¶È©ó‰ΩúÂìÅ‰∫Ü„ÄÇ
Êó¢ÁÑ∂ÊàëÂÄëÂ∑≤Á∂ìÊúâ‰∫ÜÂà•‰∫∫ÈñãÁôºÂ•ΩÁöÑÁØÑÊú¨ÔºåÊâÄ‰ª•ÊàëÂÄëÂèØ‰ª•Áõ¥Êé•‰æÜËß£ÊûêÂÆÉÊòØÂ¶Ç‰ΩïË¶èÂäÉÁöÑÔºåÂú®ÈÄôÂ•óÁØÑÊú¨‰∏≠ÔºåÂÆÉÂú® edge device ‰∏äË¶èÂäÉ‰∫Ü‰∏âÂÄã Edge ModuleÔºåÂÆÉÂÄëÂàÜÂà•ÁöÑËÅ∑Ë≤¨ÁÇ∫Ôºö
Edge Module ÂèØ‰ª•Ë¶ñ‰ΩúÊòØÂú® Azure IoT Edge ‰∏ä‰∏ÄÂÄãÊáâÁî®Á®ãÂºèÂñÆ‰ΩçÔºåÂÆÉÊúâËá™Â∑±ÁöÑÁîüÂëΩÈÄ±Êúü‰ª•ÂèäÂü∑Ë°åÁ©∫ÈñìÔºàÊÉ≥ÂÉèÂØ¶‰Ωú‰∏äÂÆÉÊòØ‰∏ÄÂÄã containerÔºâ„ÄÇË©≥Á¥∞Ë´ãË¶ã‰∏ãÊñπÁöÑ Azure IoT Edge Module ÈñãÁôºËàáÈÉ®ÁΩ≤Á´†ÁØÄÔºåÊàñÊòØÈÄô‰ªΩÂÆòÊñπÊñá‰ª∂„ÄÇ
‰ª•‰∏ãÂ∞±ÈáùÂ∞ç‰∏äËø∞ÈÄôÂπæÂÄã edge modules ‰ª•Âèä IoT Edge Module ÁöÑÈñãÁôºÈÉ®ÁΩ≤ÁöÑÊñπÂºèÊ∑±ÂÖ•Ë™™ÊòéÔºö
È¶ñÂÖàÂèÉËÄÉ Azure IoT Edge ‰∏äÁöÑÊñá‰ª∂Áû≠Ëß£ÂÆÉÁöÑÊû∂ÊßãÔºö
ÂÆèËßÄÂú∞‰æÜÁúãÔºåÈñãÁôº‰∏ÄÂÄã IoT Edge Module ÊúÄÂæåÊääÂÆÉË∑ëÂú® Azure IoT Edge Runtime ‰∏äÂ∞±ÊòØÂÅöÂπæ‰ª∂‰∫ãÊÉÖÔºö
È¶ñÂÖàÊòØ SDK ÁöÑÈÉ®‰ªΩÔºåÁõÆÂâç Azure IoT Edge ÂèØ‰ª•‰ΩøÁî® C# (.NET Core)„ÄÅPython„ÄÅNode.js„ÄÅ‰ª•Âèä Java ‰æÜÊí∞ÂØ´ edge moduleÔºåÁÑ∂ÂæåÊ†πÊìöÁõÆÊ®ôÁöÑ edge device ‰æÜÊâìÂåÖÊàê container image„ÄÇÁî±Êñº edge device ÂèØËÉΩÊòØ Windows Êàñ Linux ÁöÑÁí∞Â¢ÉÔºàÂõ†ÁÇ∫ Azure IoT Edge Runtime ÂèØ‰ª•Ë∑ëÂú® Windows/Linux ‰∏äÔºâÔºåËÄå device ‰∏äÁöÑ CPU ÂèØËÉΩÊòØ x86„ÄÅx64„ÄÅÊàñÊòØ ARM ÁöÑÊû∂ÊßãÔºåÊâÄ‰ª•Âú®Á∑®Ë≠ØÁ®ãÂºèÊàñÂª∫ÁΩÆ container image ÊôÇÔºåÈÉΩÂøÖÈ†àËÄÉÈáèÁõÆÊ®ô device ÁöÑÁãÄÊ≥Å‰æÜÈÄ≤Ë°å„ÄÇ
ÂØ¶Èöõ‰∏äÔºåÂõ†ÁÇ∫ edge module ÈÉΩÊòØ‰ª• container ÁöÑÂΩ¢ÂºèË∑ëÂú® IoT Edge Runtime ‰∏äÈù¢ÔºåÊâÄ‰ª•Âè™Ë¶ÅËÉΩÂåÖÊàêÂêàÈÅ©ÁöÑ container imageÔºåÁî®‰ªÄÈ∫ºÊñπÂºèÂØ´/Âü∑Ë°åÁ®ãÂºèÊ≤íÂ§™Â§ßÂ∑ÆÁï∞ÔºåÊúÄÂ§öÂè™ÊòØÊ≤íÊúâÂÆòÊñπÁöÑ Azure IoT Edge SDK ÂèØ‰ª•‰ΩøÁî®ËÄåÂ∑≤Ôºå‰πüÂèØ‰ª•Ëá™Ë°åÊ†πÊìö Azure IoT Edge runtime ÈñãÊîæÂá∫‰æÜÁöÑÁ®ãÂºèÁ¢º‰æÜËá™Ë°åÂØ¶‰ΩúËàá Azure IoT Edge Ê∫ùÈÄöÁöÑÈÉ®‰ªΩ„ÄÇ
Âú®ÈÄôÂÄãÂØ¶È©ó‰∏≠ÔºåÁõÆÊ®ô device ÊòØ RPi ‰∏¶‰∏îË∑ë Raspbian (Linux)ÔºåÊâÄ‰ª•Ë¶ÅÁ¢∫‰øù build Âá∫‰æÜÁöÑ container image ÊòØÈÅ©Áî® Linux/ARM ÁöÑÁí∞Â¢É‰ΩøÁî®„ÄÇ
‰∏ÄÊó¶Âª∫ÁΩÆÂÆå container image ÂæåÂπæ‰πéÂ∞±ÂÆåÊàê‰∫ÜÔºåÂâ©‰∏ãÁöÑÂ∑•‰ΩúÂ∞±ÊòØÊâæ‰∏ÄÂÄã container registry ‰æÜÊîæÔºå‰ª•‰æøÈÉ®ÁΩ≤ÊôÇËÉΩÊåáÂÆö URI„ÄÇÂ¶ÇÊûúÊòØÂØ¶È©óÁØÑ‰æãÊàñÊòØÂÖ¨ÈñãÁÑ°ÂÇ∑Â§ßÈõÖÁöÑ edge module ÂèØ‰ª•ÊîæÂú®Â¶Ç Docker Hub ÈÄôÊ®£ÂÖ¨ÈñãÁöÑ registry ‰∏≠ÔºåÂê¶ÂâáÊáâË©≤ËÄÉÊÖÆËá™Ë°åÊû∂Ë®≠ÁßÅÊúâÁöÑ registryÔºåÊàñÊòØ‰ΩøÁî® Azure Container Registry ÈÄôÊ®£ÁöÑÊúçÂãô‰æÜÊîæ container image„ÄÇ
Âú®Êï¥ÂÄãÂØ¶È©ó‰∏≠ÔºåÈõªËÖ¶Ë¶ñË¶∫ÁöÑÊ†∏ÂøÉÂ∞±Áî±ÈÄôÂÄãÊ®°ÁµÑ‰æÜÊèê‰æõÂäüËÉΩÔºåAzure Custom Vision ÊúçÂãôÊèê‰æõ‰∫Ü‰∏ÄÂÄãÁ∞°ÂñÆ‰∏äÊâãÁöÑÂπ≥Âè∞ÔºåËÆì‰Ω†Âè™ÈúÄË¶ÅËá™Â∑±ËíêÈõÜÂúñÁâá„ÄÅ‰∏äÊ®ôÁ±§ÔºåÂ∞±ËÉΩË®ìÁ∑¥Âá∫‰∏ÄÂÄãÂ∞àÁî®ÁöÑÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÔºåËÄåË®ìÁ∑¥ÂÆåÁöÑÊ®°Âûã‰πüÂèØ‰ª•ÂåØÂá∫Âà∞ Azure IoT Edge ‰∏ä‰ΩøÁî®„ÄÇ
Ë®ìÁ∑¥ÊôÇÁöÑ Domain ÂøÖÈ†àË¶ÅÈÅ∏ÊìáÊúâÊ®ôÁ§∫ (compact) ÁöÑÊâçËÉΩÂåØÂá∫Ë®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÔºåÊí∞ÊñáÁöÑÁï∂‰∏ã‰πüÂÉÖÊúâ Image Classification ÁöÑÂ∞àÊ°àÊâçËÉΩÂåØÂá∫„ÄÇ
‰∏çÈÅéÔºåÁõÆÂâçÂú® Azure Custom Vision ÁöÑÂåØÂá∫ÂäüËÉΩ‰∏äÔºåÂåØÂá∫Âà∞ Azure IoT Edge ÁöÑÂÖßÂÆπÊòØ‰ª• x86 ÁöÑÁí∞Â¢É‰æÜË®≠Ë®àÔºàÂèØ‰ª•ÈÅ∏Êìá Windows/LinuxÔºâÔºå‰∏çÈÅéÈÇÑÂ•ΩÂÆÉÂÖßÂÆπÁâ©ÂåÖÂê´‰∫ÜÊ®ôÁ±§ (labels.txt) ‰ª•ÂèäÊ®°Âûã (model.pb) ÁöÑÊ™îÊ°àÔºà‰∏ãËºâÁöÑÊ®°ÂûãÊòØ TensorFlow ÁöÑÊ†ºÂºèÔºâÔºåÊâÄ‰ª•ÊúâÈÄôÂÖ©ÂÄãÊ™îÊ°àÈÇÑÊòØÂèØ‰ª•Ëá™Ë°åÊí∞ÂØ´ edge module„ÄÇ
ÈÄôÂÄãÊ®°ÁµÑÂèØ‰ª•Âæû resin/rpi-raspbian ‰ΩúÁÇ∫ base image ÈñãÂßãÂª∫ÁΩÆ„ÄÇÂêåÊôÇÊàëÂÄëË¶ÅËÉΩËß£Êûê‰∏ãËºâÁöÑ TensorFlow ÁöÑÊ®°ÂûãÔºåÊâÄ‰ª•Ë¶ÅÂÆâË£ù Python 3 ÁöÑÁõ∏ÈóúÁí∞Â¢ÉÔºå‰∏çÈÅéÁõ¥Êé•Áî® pip3 ‰æÜÂÆâË£ù tensorflow ÁöÑÁâàÊú¨Â§™Ëàä‰∫ÜÔºàÂú® RPi ‰∏äÔºâÔºå‰∏çÈÅéÊàëÂÄëÂèØ‰ª•Âæû TensorFlow ÁöÑ CI server ‰∏≠ÊâæÂà∞ÈÅ©Áî® RPi Êñ∞ÁâàÊú¨ÁöÑ pip wheel ‰æÜÂÆâË£ùÔºåÊàñÊòØÂèÉËÄÉ TensorFlow ÂÆòÁ∂≤‰∏äÁöÑÊåáÂçóÂæûÂéüÂßãÁ¢ºÈñãÂßãÂª∫ÁΩÆ„ÄÇ
ÂÖ∂ÂÆÉÈúÄË¶ÅÁöÑ Python Â•ó‰ª∂ÂåÖÊã¨ pillow„ÄÅnumpy„ÄÅ‰ª•Âèä flaskÔºåÂõ†ÁÇ∫ÈÄôÂÄãÊ®°ÁµÑÂïüÂãïÁöÑ web service Â∞±ÊòØ‰ª• Flask ‰æÜÈÅã‰ΩúÁöÑ„ÄÇ
ÊâÄ‰ª•ÊúÄÂæå Dockerfile Â∞±ÊúÉÈï∑ÂæóÂÉèÈÄôÊ®£Ôºö
ÂÆåÊï¥ÁöÑÊ®°ÁµÑÁ®ãÂºèÁ¢ºË´ãÂèÉËÄÉÈÄôË£°„ÄÇ
ÈÄôÂÄãÊ®°ÁµÑÁõ∏Â∞çÂñÆÁ¥îÔºåÂ∞±ÊòØÊé•Êî∂Ë≥áÊñôÁÑ∂ÂæåÊéßÂà∂ Sense HAT ‰æÜÈ°ØÁ§∫ËÄåÂ∑≤ÔºåÊú¨‰æÜÂú® Raspbian ‰∏ãÂ∞±Âè™Ë¶ÅÈÄèÈÅé pip ÂÆâË£ù sense-hat Â•ó‰ª∂Â∞±ËÉΩÁî® Python ‰æÜÊéßÂà∂ Sense HAT ÔºåÊâÄ‰ª•Âª∫ÁΩÆ edge module ÁöÑ base image ‰πüÂèØ‰ª•ÊòØ resin/rpi-raspbianÔºå‰∏çÈÅéÂõ†ÁÇ∫ÈÄôË£°ÊúÉÁî®Âà∞ Azure IoT Hub ÁöÑ clientÔºåËÄåÁØÑÊú¨‰∏≠ÈôÑÂ•Ω‰∫Ü pre-built for RPi ÁöÑ iothub.so ÂÆÉÊòØÂü∫Êñº Python 3.5 ‰æÜÂª∫ÁΩÆÁöÑÔºåÊâÄ‰ª• base image ÊâçÈÅ∏Áî® resin/raspberrypi3-debian:stretch„ÄÇ
ÂÖ∂ÂÆÉÁöÑÈÉ®‰ªΩÔºåÂ∞±ÊòØÊ†πÊìö image-classifier-service ÂõûÂÇ≥ÁöÑÁµêÊûúÔºåÊ†πÊìöÊ©üÁéáÊúÄÈ´òÁöÑÊ®ôÁ±§ÔºåËÄå‰∏îÊ©üÁéáË∂ÖÈÅéË®≠ÂÆöÁöÑ THRESHOLD ÂÄºÂ∞±Âú® Sense HAT ‰∏äÁöÑ 8x8 LED Ááà‰∏äÁï´Âá∫ÂúñÊ°àÔºàÈÄôÂÄãÁØÑÊú¨ÊòØËæ®Ë≠òÈ¶ôËïâ„ÄÅËòãÊûú„ÄÅÊ™∏Ê™¨„ÄÅ‰ª•ÂèäÊü≥Ê©ôÔºâ„ÄÇ
ÂÆåÊï¥ÁöÑÊ®°ÁµÑÁ®ãÂºèÁ¢ºË´ãÂèÉËÄÉÈÄôË£°„ÄÇ
ÈÄôÂÄãÊ®°ÁµÑË¶ÅÂÅöÁöÑ‰∫ãÊÉÖÊØîËºÉÁâπÂà•ÔºåÂÆÉÂøÖÈ†à‰ΩøÁî® Open CV ‰æÜËôïÁêÜÊîùÂΩ±Ê©üÊãç‰∏ãÁöÑÁÖßÁâáÔºå‰ΩÜÂ¶ÇÊûúÊØèÊ¨°Âª∫ÁΩÆ edge module ÊôÇÈÉΩÂæûÈ†≠ÈñãÂßã build opencv Â∞±Êõ†Êó•ÂèàÂª¢ÊôÇÔºåÊâÄÂπ∏Â∑≤Á∂ìÊúâ‰∫∫Âª∫ÁΩÆÂ•Ω‰∫ÜÂü∫Êñº Raspbian ÂèàÂª∫Â•Ω Open CV ÂáΩÂºèÂ∫´ÁöÑ container image ‚Äî mohaseeb/raspberrypi3-python-opencvÔºåÊâÄ‰ª•Â∞±Áõ¥Êé•Êãø‰æÜÁï∂‰Ωú base image ÈñãÂßã„ÄÇ
ÂÄºÂæóÊ≥®ÊÑèÁöÑÊòØÔºåÈÄôÂÄãÊ®°ÁµÑÊòØÈÄèÈÅé VIDEO_PATH ÈÄôÂÄãÁí∞Â¢ÉËÆäÊï∏ÊãøÂà∞Áõ∏Ê©üÁöÑ‰ΩçÁΩÆÔºåÂ¶ÇÊûú‰Ω†Ë∑üÊàë‰∏ÄÊ®£ÊòØÁî® Pi CameraÔºàËÄå‰∏çÊòØÁî® USB CamÔºâÔºåËÄåÂèàÁÖßËëóÊñá‰ª∂‰∏≠Ë™™ÊòéÊääÁõ∏Ê©ü‰ΩçÁΩÆË®≠ÂÆöÊàê /dev/video0 ÁöÑË©±ÔºåÂ∞±Ë¶ÅÂú® Raspbian ‰∏äÂ§öÂÅö‰∏ãÂàóÊ≠•È©üÔºö
ÂÆåÊï¥ÁöÑÊ®°ÁµÑÁ®ãÂºèÁ¢ºË´ãÂèÉËÄÉÈÄôË£°„ÄÇ
Â¶ÇÊûú‰Ω†ÊääÈÄôÂÄãÂ∞àÊ°àÁØÑÊú¨ clone Âõû‰æÜÔºåÂèØ‰ª•Áõ¥Êé•Áî® Visual Studio Code ÊâìÈñãÈÄôÂÄãÁõÆÈåÑÔºåÂú®Âª∫ÁΩÆÊ®°ÁµÑÂâçÂÖàÂÅö‰∏Ä‰∫õ‰øÆÊîπÔºö
Â¶ÇÊûú‰Ω†ÁöÑ Visual Studio Code Â∑≤Á∂ìÊ≠£Á¢∫Âú∞ÂÆâË£ù‰∫Ü Azure IoT Edge Êì¥ÂÖÖÂ•ó‰ª∂ÔºåÈÇ£Âú® deployment.template.json Ê™îÊ°à‰∏äÊåâÂè≥ÈçµÔºåÂ∞±ÂèØ‰ª•ÈÅ∏Êìá Build IoT Edge Solution ÈñãÂßãÂª∫ÁΩÆÊï¥ÂÄãÂ∞àÊ°à„ÄÇ
È†ÜÂà©Âª∫ÁΩÆÂÆåÊàêÂæåÔºàÊ†πÊìöÊàëÁöÑÁ∂ìÈ©óÔºåimage-classifier-service Ê®°ÁµÑÊúÉËä±ÊØîËºÉ‰πÖÁöÑÊôÇÈñìÔºâÔºåÂú® config/deployment.json Ê™îÊ°à‰∏äÊåâÂè≥ÈçµÂ∞±ÂèØ‰ª•‰ΩøÁî® Create Deployment for IoT Edge Device Áõ¥Êé•ÈÉ®ÁΩ≤Âà∞ÊåáÂÆöÁöÑ device ‰∏ä‰∫Ü„ÄÇ
ÈÅé‰∏ÄÊÆµÊôÇÈñìÂæåÔºåÈÄ£ÈÄ≤ RPi Ë£°Âü∑Ë°å docker ps ÊáâË©≤ÂèØ‰ª•ÁúãÂà∞ÂÉèÊòØÈÄôÊ®£ÁöÑÁï´Èù¢ÔºåÂ¶ÇÊûúÁî® docker logs -f ÈÄ≤ÂÖ•‰ªª‰Ωï‰∏ÄÂÄã container ÈÉΩÊ≤íÊúâÈåØË™§ÁöÑË©±ÔºåÂ∞±ÂèØ‰ª•ÈñãÂßãËá™Ë°åÂØ¶È©óÂõâ„ÄÇ
ÈÄôÂÄãÂØ¶È©ó‰∏ªË¶ÅÊòØÂπ´Âä©ÈñãÁôº‰∫∫Âì°Áû≠Ëß£ Azure IoT Edge ÁöÑÈñãÁôº„ÄÅÈÉ®ÁΩ≤ÊµÅÁ®ãÔºå‰∏¶‰∏îÊê≠ÈÖç Azure Custom Vision Âø´ÈÄüÂÖúÂá∫‰∏ÄÂÄãÈõªËÖ¶Ë¶ñË¶∫Ê®°ÂûãÔºå‰∏çÈÅéÂú® RPi ‰∏äÂü∑Ë°å TensorFlow Áï¢Á´üÊØîËºÉÂêÉÂäõ‰∏Ä‰∫õÔºåÊàñË®±Âú®ÂØ¶ÂãôÈÅãÁî®‰∏äÔºåÊê≠ÈÖçÂÖ∂ÂÆÉÂä†ÈÄüË£ùÁΩÆ„ÄÅÊàñÊòØÈÅ∏Áî® powerful ÁöÑ x86 device ÂèØËÉΩÊúÉÊõ¥Â•Ω„ÄÇÊâÄ‰ª•Âú® edge computing ‰∏äÁöÑÊáâÁî®ÔºåÈáùÂ∞ç‰∏çÂêåÁöÑÊÉÖÂ¢ÉÈÅ∏Êìá‰∏çÂêåÁöÑËªüÁ°¨È´îÊê≠ÈÖçÊâçÊòØÊ≠£Á¢∫ÁöÑËßÄÂøµ„ÄÇ
DevRel | Developholic | Technical Evangelist
81 
81¬†
81 
DevRel | Developholic | Technical Evangelist
"
https://medium.com/@renatogroffe/asp-net-core-apis-rest-na-nuvem-com-docker-e-azure-web-app-4a82f9f594a5?source=search_post---------134,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
May 22, 2017¬∑8 min read
O objetivo deste artigo √© demonstrar a implementa√ß√£o de uma API REST na nuvem com o ASP.NET Core, empregando para isto containers Docker, o Docker Hub e o Azure Web App on Linux.
O deployment de softwares atrav√©s de containers vem ganhando cada vez mais for√ßa, com as mais variadas tecnologias se adequando para suportar este tipo de pr√°tica. O isolamento de aplica√ß√µes e um uso mais racional de recursos est√£o entre as principais vantagens obtidas por meio da ado√ß√£o dessas estruturas.
Um container pode ser definido, do ponto de vista t√©cnico, como uma unidade isolada para deployment de um projeto e todas as suas depend√™ncias. Dentre as tecnologias baseadas neste conceito merece destaque o Docker, uma das op√ß√µes mais populares da atualidade para a utiliza√ß√£o de containers.
A Microsoft n√£o ficou alheia a esta tend√™ncia, oferecendo suporte a containers Docker em produtos como o .NET Core, o ASP.NET Core e o SQL Server 2017. No que se refere √† nuvem, uma das possibilidades de uso do Docker est√° no Azure Web App on Linux. Esta alternativa PaaS (Plataform as a Service) viabiliza o deployment de aplica√ß√µes constru√≠das a partir de tecnologias compat√≠veis com distribui√ß√µes Linux (caso de containers Docker), al√©m de oferecer in√∫meras facilidades em termos de gerenciamento, escalabilidade, seguran√ßa, monitoramento e deploy.
Nas pr√≥ximas se√ß√µes ser√° abordado o uso em conjunto de tecnologias como ASP.NET Core, Docker e Azure. Isto acontecer√° atrav√©s de um exemplo pr√°tico, o qual dar√° √™nfase √† implementa√ß√£o e deployment de uma API REST na nuvem.
A implementa√ß√£o da API descrita neste artigo j√° foi detalhada anteriormente no seguinte post (embora utilizando a vers√£o do ASP.NET Core baseada no arquivo project.json, com o uso de tal recurso sendo posteriormente descontinuado):
ASP.NET Core: implementando uma API REST em Linux
Este projeto permitir√° a convers√£o de alturas em p√©s (medida comumente adotada na avia√ß√£o) para o equivalente em metros, considerando para isto a seguinte f√≥rmula:
Para a constru√ß√£o do exemplo de testes ser√£o utilizados:
Inicialmente ser√° criado um projeto do tipo ASP.NET Core Web Application (.NET Core) chamado APIAlturas:
Selecionar na sequ√™ncia a vers√£o 1.1 e a op√ß√£o Web API em ASP.NET Core 1.1 Templates:
A classe ConversorAlturasController ser√° respons√°vel pela convers√£o de alturas em p√©s para o equivalente em metros, com isso acontecendo atrav√©s do m√©todo Get (este √∫ltimo respons√°vel pelo tratamento de requisi√ß√µes HTTP):
Na imagem a seguir √© poss√≠vel observar o resultado da convers√£o de uma altura de 100 p√©s (30,48 metros):
Com a API de convers√£o j√° implementada o pr√≥ximo passo ser√° a cria√ß√£o e deployment no Docker Hub de uma imagem Docker, a qual permitir√° a publica√ß√£o da aplica√ß√£o sob a forma de um container no Azure Web App on Linux. Trata-se de um processo extremamente simples, gra√ßas ao Visual Studio Tools for Docker - conjunto de ferramentas de suporte ao Docker dispon√≠veis para uso no Visual Studio 2017.
Dentre as funcionalidades que integram o Visual Studio Tools for Docker destacam-se recursos para building, debugging e execu√ß√£o de containers a partir de projetos gerados com o .NET Core.
Os procedimentos descritos nesta se√ß√£o dependem da instala√ß√£o pr√©via do Docker for Windows em um ambiente de desenvolvimento baseado no Windows 10. Na interface de configura√ß√£o deste aplicativo ser√° necess√°rio acessar a se√ß√£o Shared Drives e, na sequ√™ncia, marcar o driver C: (isto permitir√° a utiliza√ß√£o das diversas funcionalidades do Visual Studio Tools for Docker):
Dentro do Visual Studio 2017 ser√° necess√°rio adicionar os arquivos de suporte ao Docker para a aplica√ß√£o criada na se√ß√£o anterior. Um dos caminhos para isto consiste em acionar o menu de contexto para o projeto APIAlturas e, em seguida, as op√ß√µes Add e Docker Support:
Como resultados desta a√ß√£o ser√£o criados os seguintes itens:
O cont√©udo do arquivo Dockerfile pode ser observado na pr√≥xima listagem:
J√° na listagem a seguir est√£o as configura√ß√µes do arquivo docker-compose.yml:
Alternativamente os arquivos de suporte ao Docker tamb√©m poderiam ser adicionados no momento de cria√ß√£o da aplica√ß√£o (selecionando para isto a op√ß√£o Enable Docker Support na tela de sele√ß√£o de templates do ASP.NET Core 1.1):
Neste momento o projeto APIAlturas j√° estar√° devidamente configurado para sua execu√ß√£o e, at√© mesmo, debugging a partir de um container Docker. O pr√≥prio √≠cone para ativar a aplica√ß√£o indica isto:
Com aplica√ß√£o APIAlturas em modo de execu√ß√£o ser√° poss√≠vel verificar a exist√™ncia de uma imagem Docker criada para efeitos de desenvolvimento, al√©m de um container baseado nessa estrutura.
Ao executar o comando docker images no PowerShell aparecer√£o as imagens microsoft/aspnetcore:1.1 e apialturas:dev:
J√° o comando docker ps -a exibir√° o container gerado para a execu√ß√£o por meio do Visual Studio:
Uma nova imagem Docker dever√° ser gerada com base nas configura√ß√µes de release da aplica√ß√£o. Selecionar para isto a op√ß√£o Release (ao inv√©s de Debug) no Visual Studio 2017:
Efetuar em seguida a compila√ß√£o do projeto (a partir do menu Build > Build Solution). Ao executar novamente o comando docker images no PowerShell constar√° agora a imagem apialturas:latest:
Uma tag chamada renatogroffe/apialturas dever√° ser criada para a imagem apialturas:latest, com este novo elemento contendo o nome a ser registrado no Docker Hub (formado por identifica√ß√£o do reposit√≥rio/login + nome da aplica√ß√£o; esses dois itens estar√£o separados ainda por uma barra - ‚Äú/‚Äù). Executar para isto o seguinte comando no PowerShell:
docker tag apialturas:latest renatogroffe/apialturas
Uma nova execu√ß√£o da instru√ß√£o docker images trar√° agora uma imagem chamada renatogroffe/apialturas:
Para registrar a imagem no Docker Hub ser√° necess√°ria uma conta para uso deste reposit√≥rio. Caso precise criar tal login e/ou obter maiores informa√ß√µes acesse:
https://hub.docker.com/
O pr√≥ximo passo ser√° logar no Docker Hub (via PowerShell) atrav√©s da seguinte instru√ß√£o - que solicitar√° usu√°rio e senha:
docker login
Para publicar a imagem renatogroffe/apialturas no Docker Hub ser√° utilizado o comando:
docker push renatogroffe/apialturas
O resultado desta a√ß√£o pode ser observado na pr√≥xima imagem:
A imagem renatogroffe/apialturas aparecer√° ent√£o no Docker Hub ap√≥s a conclus√£o deste √∫ltimo procedimento:
No portal do Azure ser√° necess√°rio criar um novo recurso baseado no servi√ßo Web App on Linux:
Informar no formul√°rio de cria√ß√£o do recurso:
Acionar ap√≥s o preenchimento destes campos a op√ß√£o Configurar cont√™iner:
Em Cont√™iner do Docker selecionar o item Docker Hub, certificando-se de que a op√ß√£o P√∫blico est√° marcada em Acesso ao Reposit√≥rio e informando no campo Imagem e marca opcional o valor renatogroffe/apialturas (correspondente √† imagem publicada anteriormente no Docker Hub):
Selecionar OK em Cont√™iner do Docker, confirmando as defini√ß√µes a serem utilizadas para a cria√ß√£o do container na nuvem. Concluir ent√£o este processo acionando o bot√£o Criar no formul√°rio em que se especificaram as configura√ß√µes da API REST que ficar√° hospedada na nuvem:
Na op√ß√£o Todos os recursos ser√° poss√≠vel constatar, ap√≥s um curto per√≠odo de tempo, a presen√ßa do recurso apialturas:
Ao acessar o item apialturas ser√° exibido um painel no qual constar√° o endere√ßo da aplica√ß√£o de testes (http://apialturas.azurewebsites.net), al√©m de outras op√ß√µes para a configura√ß√£o e gerenciamento deste recurso:
Um teste com a URL http://apialturas.azurewebsites.net/api/conversoralturas/pesmetros/1000 retornar√° o resultado da convers√£o de mil p√©s (304,8 metros):
Este artigo procurou apresentar uma das alternativas para uso do ASP.NET Core com Docker na nuvem da Microsoft: o Azure Web App on Linux. Esse servi√ßo √© uma excelente op√ß√£o n√£o s√≥ para a utiliza√ß√£o de containers, como tamb√©m para a hospedagem de aplica√ß√µes que empreguem plataformas tipicamente associadas a distribui√ß√µes Linux. O Azure Web App on Linux conta ainda com funcionalidades que facilitam sua configura√ß√£o, gerenciamento e, at√© mesmo, integra√ß√£o cont√≠nua de solu√ß√µes Web.
ASP.NET Core - Documentation
Conte√∫dos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0
Introduction to Azure Web App on Linux
Visual Studio Tools for Docker
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
73 
2
73¬†
73 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://blog.jeremylikness.com/nosql-and-azure-cosmos-db-in-20-minutes-9d0c3e0279dc?source=search_post---------135,
https://medium.com/ark-io/ark-bridgechain-azure-guide-774f5fd63333?source=search_post---------136,"There are currently no responses for this story.
Be the first to respond.
This deployment method can be used to jump-start your journey to your own BridgeChain in the process outlined here (ARK Deployer)
Pre-Requisites: Active Microsoft Azure account (Trial is OK) and basic knowledge on how to connect via SSH (E.g. PuTTY for Windows).
End result:- Ubuntu VM in Azure- Azure PublicIP and Firewall configurations pre-built- Your own BridgeChain node and ARK Explorer running in < 20 minutes- Re-usable and customizable deployment script
Script available at : ARK Azure on GitHub
Navigate to: https://github.com/ArkEcosystem/ark-azureClick on the big blue ‚ÄúDeploy to Azure‚Äù button to be taken to Portal.Azure.com. Login with your account (Trial or not, either work).
You should only need to input data for 3 empty fields (Resource Group, Admin Password, and DNS Label), but full details for this section:
Subscription ‚Äî If not already on your current one.Resource Group ‚Äî ‚ÄòCreate New‚Äô only option that will work unless you wish to edit the template yourself. We will use My-Ark-RG for this guide.Location ‚Äî Choose your desired region, ensuring it allows the subscription you chose above. If in doubt, use the default region when using a Free Trial.Admin Username ‚Äî This is the account you will sign into the server with.Admin Password ‚Äî Secure string and has high complexity requirements.Dns Label Prefix ‚Äî This is the unique DNS name that you are giving to this VM. It is mandatory that this name be unique in the Location Datacenter as a whole (It will self-check after input) **Remember this for easy SSHUbuntu OS Version ‚Äî Only option is 16.04-LTS at this time.ARKNSG Name ‚Äî This is the name for the firewall group to permit SSH as well as Ark Node/Explorer port access to this VM.
Currently, the VM produced is a Standard_A1 size VM. This is a very low-cost resource VM for tutorial purposes (can be scaled-up after deployment if desired or via template adjustment).
Click ‚ÄòAgree‚Äô, and ‚ÄòPurchase‚Äô to begin deployment. Should take 5‚Äì10 minutes.
You are welcome to explore your new VM‚Äôs Overview, etc, by clicking on ‚ÄòResource Groups‚Äô and finding your new group, and the VM inside. There are lots of configuration items here.
If you do not remember your Public DNS name or IP address (for SSH), go to: Resource Groups > My-Ark-RG > MyUbuntuVM > Overview. This has all the general information you will need.
The Public DNS Name for all VMs follows this pattern:PublicDNSname.locationid.cloudapp.azure.com
So in our instance it would be here: firstarksidechain.southcentralus.cloudapp.azure.com
SSH into your new VM using the Public DNS Name and login with the credentials used during the VM Template deployment page.
Starting here, is the default quick-installation method with the chain being named ‚ÄúMyTest‚Äù. If you wish to customize it, please see the bottom-most section of this article.
Run the following command (It‚Äôs a one-liner, copy and paste the full contents from here or from the ARK AZure Github page):
curl -o- https://raw.githubusercontent.com/ArkEcosystem/ark-azure/master/script/arkdefaultinstall.sh | bash
This script will complete all of the installation steps to get both the node and explorer running for your BridgeChain, with all the default values. Total installation time is about 10 minutes.
Just after the node gets installed, there will be 3 lines of text to record. Copy these lines outlined in red below.
This information will be used later on, as you explore ARK past the deployment phase with ARK Deployer. Copy and paste it right out of the console window for safe keeping.
This is the Public IP of your server, and the port required to view the ARK Explorer for your BridgeChain (4200). The API should be available on port 4100.
You can highlight the URL straight from the SSH window, such as: http://13.65.29.3:4200 and hit CTRL+C to copy it. Paste into a browser, and voila!
If you wish to customize your deployment of ARK within the bounds of ARK-Deployer, download a copy of:
https://raw.githubusercontent.com/ArkEcosystem/ark-azure/master/script/arkdefaultinstall.sh
Within this file, you‚Äôre welcome to edit the list of variables on lines 21‚Äì31 and personalize them. These variables all align with an optional parameter of ARK Deployer (See GitHub: https://github.com/ArkEcosystem/ark-deployer#optional-parameters)
You can then run this new version of your script against a new VM, or, you can uninstall the original node/explorer and re-install using the script again. We would recommend just rolling out a new server for ease of use, but that‚Äôs your call.
To quickly make a personalized copy of the script (GitHub account)
For more in-depth and customizable BridgeChain enjoy following along with the ARK Deployer guide going forward in your ARK journey. Welcome aboard ARK.
Special thanks to Walrusface for writing this guide and script, delegate Jarunik for sponsoring its development and our dev Alex Barnsley for testing and modifying necessary things in ARK deployer.
ARK | All-in-One Blockchain Solutions
557 
1
557¬†claps
557 
1
Written by
Vice President and Co-Founder at ARK.io
ARK | All-in-One Blockchain Solutions
Written by
Vice President and Co-Founder at ARK.io
ARK | All-in-One Blockchain Solutions
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/making-a-wearable-live-caption-display-using-azure-cognitive-services-and-ably-realtime-f4f6667a076f?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
Live captioning of speech into text has so many useful applications and Azure Cognitive Services makes it fast and easy to build captioning into your applications. Used together with Ably Realtime, it is possible to make wearable devices which can display what you‚Äôre saying, in real time. Wearable live captions!
This article will explain how to use Azure Speech and Ably Realtime and will go through building a web app that will take data from your microphone and turn it into readable text.
Check out this video to see the demo in action:
I‚Äôll admit to a personal desire to see more products like this on the market. The reason is my mother. She has been steadily losing her hearing over the last few years and relies heavily on lip reading and clear pronunciation. Two things which are denied to her when the people talking to her are wearing masks. Of course I also want to keep her safe, so I will always encourage everyone to wear a mask, but there must be ways that technology can make her, and many others, life easier. One of the frustrations with assistive technologies on phones is that they require her to be looking at her phone, rather than at the speaker, which can lead to her being treated differently, often poorly, by whoever is speaking to her.
Inspiration hit when I saw this product on sale (at Cyberdog, of all places)
It is a face mask with a wearable LED display inside. The display is incredibly small, flexible, breathable and has very low power consumption. What if I could send text to the display from my phone‚Äôs microphone? It could update the mask display to show what I am saying!
So began my journey to build a wearable live captioning demo.
The demo consists of a web app, which is used to capture microphone data and send it to Azure Cognitive Services. When it receives text back, the web app can display it to the user. It also contains a virtual representation of the hardware display to visualise what the hardware display will be showing. The app uses the Ably Javascript SDK along with their MQTT broker to send messages from the app to the wearable hardware.
The wearable part is a 32 by 8 display of neopixels (very small LEDs) connected to an Adafruit Feather Huzzah (a small, wifi enabled microprocessor) which is powered by a rechargeable USB battery.
The web app is built with HTML, CSS and JS, it will run on your phone or your computer and just requires an internet connection and a microphone.
You can see and clone the code for the entire project on github, it is open source, and I‚Äôd be delighted if you used it to create your own wearable tech projects, especially if they can help make someone‚Äôs day better! Instructions on how to set up the app and its various dependencies are in there too.
The getUserMedia() API has been in browsers for a while. It allows us to prompt the user for permission to use their microphone and or camera and, once allowed, get a stream of data from their media devices. This app uses just the audio, so it will only prompt for microphone permissions.
This app uses the Cognitive Services Speech service, which allows us to transcribe audible speech into readable, searchable text.
When a user clicks the ‚ÄúStart Listening‚Äù button on the app UI, a function called streamSpeechFromBrowser is called. This uses the Azure fromDefaultMicrophoneInput along with the fromAuthorisationToken function to authenticate with Azure and initialise a SpeechRecognizer. This is what will perform the speech recognition on the data coming from the mic. It will return an object which contains the text of what has been said.
Because the phone now has the transcription, and our microcontroller is connected to our LED display, the app needs to send the transcription to the hardware, in a format that it can understand so the code running on the hardware can convert that text into lights.
To communicate between the web app and the microprocessor, a messaging protocol is required. MQTT is a lightweight publish/subscribe protocol, designed specifically for IoT devices and optimised for high latency or unreliable networks. This is perfect for this particular project where the wearer might be on a 3g connection.
In order to use MQTT, a broker is required, this is a service which is responsible for dispatching messages between the sender (or client) and the rightful receivers. The web app is the client, in this case, and the receiver is the microcontroller. This project uses the Ably MQTT broker, which comes for free with the Ably Javascript SDK. The web app can send messages using the Ably SDK and they will be automatically sent out using MQTT too.
The microprocessor on the Adafruit Feather Huzzah is very small and therefore has limited processing power and memory, which means that the code that runs on it needs to be as efficient as possible. It is therefore necessary to avoid doing any complicated text parsing, string splitting or other similarly memory intensive tasks on the microprocessor. Parsing and splitting strings is especially costly, and involves larger buffers than would be ideal.
While at a glance, this may seem like premature performance optimisation, if all of the memory on the board is used parsing the messages as human readable strings, it decreases the amount of memory available to buffer incoming messages. To solve this problem, a binary message format is used to talk to the hardware. The browser app creates a specially coded message to send text strings to the device.
Where most systems would probably use JSON to serialize messages with properties, this app uses a binary format where the order and content of bytes as they are received is relevant to the hardware. For example, sending a command to scroll text across the display involves the browser sending a message that looks like this:
But rather than serializing this message to JSON, and sending it as text, it is packed down into a binary message that looks like this, byte by byte:
These messages are sent as raw bytes ‚Äî the control codes in the ASCII standard are used to provide some header information in the messages.
Because the message is a raw byte stream, it is not necessary to parse the message on the hardware, it can just loop over the message bytes, and run different processes depending on the byte being looped over at the time. This takes all of the complexity of parsing text away from the microprocessor, and moves it into the TypeScript code in the web app, where it is easier to debug and test.
In the table above, the byte at offset 8 would represent a single character, but the parser on the hardware is looking out for the STX and ETX start and end of text markers. What this means is that any number of ASCII characters can be added in the space between them to form a full sentence in the message.
Since the transcription from Azure Cognitive Services arrives as text, it is trivially simple to display this text to the user in a containing element within the app UI.
The microcontroller needs a way to convert the messages it receives as ASCII characters, into a format which can be shown on an 8 pixel high resolution matrix (it will support displays with a higher resolution, but not lower).
The first thing to do was design some ‚Äúpixel font‚Äù style alphanumeric characters and symbols
Which could be converted to an array of binary values. Let‚Äôs take, for example, the letter n. Which, represented visually, would look like this:
which as an array of binary values (where black = 1 and white = 0) would be:
Or, as an array of binary values:
The app uses a JavaScript image manipulation program called jimp to create this conversion from a png to one very long array containing all of the characters. The array also requires a list of indexes, which will point to the starting position of each letter and its width (you‚Äôll notice from the graphic above, that the letters and symbols differ in width). These two byte arrays are small enough to be embedded on the microcontroller.
With the ‚Äúfont‚Äù arrays embedded on the device, it is possible to write code in C/C++ (the language that the microprocessor uses) to show the letters on the display. This means that instead of computing all of the individual pixel positions in the browser app, and sending them one by one, the microcontroller will handle the translation.
LED displays, like the one used in this project are made up of addressable RGB LEDs. ‚ÄúAddressable‚Äù means that each LED on the display has its own address number, the first LED on the strip is LED 0, the second LED 1, and so on as they move along the matrix.
There is an open source Arduino library, written to interact with these LEDS, called AdaFruit NeoPixel which makes it simple to set the colour value of individual pixels with a pixel address (or ID) and a colour value, set in RGB (red, green blue).
Unfortunately, addressing the LEDs in the matrix isn‚Äôt quite as simple as I‚Äôd originally hoped. There are many different ways of wiring up an LED matrix, and every manufacturer seems to have a different preference!
Some common wiring patterns look like this:
Not only that, but the displays sometimes have connectors at both ends, which means that the numbering could go from left to right, or right to left, depending on which connector is used. These differences are mostly made to accommodate physical constraints, snaking the wires uses a lot less wiring than bringing the line back to the start of each row.
This variety of wiring patterns meant that the code running on the board needed to be able to translate from (x,y) coordinates into the appropriate pixel ID, so that developers working with this code will be able to use it with the display they own. This is done by ensuring that all of the operations in the JavaScript SDK library refer to pixel locations in (x,y) coordinates rather than pixel IDs. Once these coordinates are sent to the hardware, it will translate the coordinates into the correct pixel ID, based on hardware configuration code running on the board.
The microcontroller code has a single Configuration file included in the project that contains the display configuration, along with configuration for WiFi and MQTT settings:
You can see here that every device needs to be told the GPIO (General Purpose IO) pin the display is connected to, the dimensions of the display, and the index_mode and carriage_return_mode settings that capture the possible difference between displays.
This means that the code will be able to drive cheap displays bought from different manufacturers, without spending time changing the code.
With the microcontroller software now able to set pixels using any kind of display, and with the ‚Äúfont‚Äù arrays stored as variables in the Arduino C++ code, the missing piece is to write message handlers that process incoming binary messages, look up the ‚Äúfont‚Äù data for each letter in the message, and push the appropriate pixels to the display.
First let‚Äôs cover how static text can be written to the display -
When a message to display static text is received, the code loops through each text character present in the message and looks up that character in the embedded font. The font data that gets returned from the font is an array of bytes, with a single byte for every pixel to be set on the display, prefixed by the width of the character in pixels.
Because the font data includes the width, when the code iterates over subsequent characters, it can calculate the location it needs to start drawing at. This location is just called xPosition in the code, and each time a character is drawn on the display, its width and an additional space is added to the xPosition. As the code loops, all of the (x,y) coordinates returned from the font have this xPosition added to them making sure that the characters are drawn in sequence. This continues until the loop reaches the width of the display, where it stops drawing extra characters that would flow off the side.
Writing scrolling text is mostly the same, with one subtle change: the xPosition starts out at the right hand edge of the display. This means that when a message is first received, the xPosition ‚Äî the initial offset, will equal the display width and no pixels will be drawn.
The scrolling text implementation is relatively simple ‚Äî it is a loop that decrements that xPosition until it equals the total width of all the characters, as a negative number. So, if the text string has a total width of 100 pixels, it‚Äôll keep on drawing, and decrementing the xPosition, until the xPosition equals -100. This scrolls the text across the screen.
The code on the hardware only ever draws pixels for the characters that will be visible on the display which means that memory is saved by not converting every character into pixels ahead of time.
There is another advantage to this approach ‚Äî if you refer back to the data that is being sent as part of SetTextMessage, one of the values in the message header is a ‚Äúscroll interval‚Äù. This scroll interval is the time delay between scrolling a single pixel to the left. This is what sets the speed of the scrolling. As the animation is played, the code pauses execution every time it scrolls. This loop prevents control being returned to the main Arduino loop, which would trigger receiving more MQTT messages.
This approach makes it possible to use MQTT as a buffer without using up any memory on the device. When it finishes scrolling, it receives any subsequent messages that have been sent, and starts scrolling the next piece of text.
Deploying code to the microcontroller not only requires the device to be plugged in but also takes time for the code to be verified, compiled and then pushed to the device. This can sometimes take quite a while. In order to make debugging both the code running on the board and its support of different sized displays, the web app comes with a simulated version of the hardware display.
This is a visual, virtual representation of the display which runs transliterated code (from C++ to typescript). It contains ported versions of the Arduino libraries that the microprocessor requires (like AdaFruit_NeoPixel), which made it possible to write code that appears as though it uses the NeoPixel SDK function, but it is targeting a div in the markup, instead of a pixel on the display. This means that code for the board can be written in Typescript (a language I personally much prefer!), tested on the simulated board and, once functional, can be transliterated into C++ to be written to the board.
To validate this approach, it was necessary to transliterate a few of the NeoPixel test programs into TypeScript to run in the simulator. This helped to build confidence that the code that was being written would work similarly on the hardware devices. The simulator was close enough to the real hardware to be a good test target. Fortunately this held true, and the TypeScript code that was written transliterated across to C++ with very few changes.
The same codebase that is used here to scroll text, can be used to send pixels for interactive painting, or even send images across to the device (as long as they are sufficiently tiny to fit on the display) using the TypeScript SDK that was built to communicate with the hardware.
The message protocol that was designed to send the data over the wire can transmit any pixel data, so long as the hardware at the other end can process the messages that it receives.
The final sequence diagram for the app, hardware and services in use looks like this:
Because there‚Äôs a message being sent via Ably every time a new transcription arrives, the web app also features a sharable ‚ÄúView‚Äù page that just displays the transcription text in real time on the screen. The user can share the URL of this to anyone that wants to open the page and read along.
The app contains a Share link that will trigger the operating system‚Äôs default sharing UI (at least on Windows and Android devices), using the Web Share API. This API is paint-dripping-wet new and not supported everywhere yet, so will be replaced with a ‚ÄúCopy Shareable Link‚Äù button on unsupported devices.
You can check out the code on GitHub: https://github.com/ably-labs/live-caption-demo
The live demo is up at: https://live-caption.ably.dev/
I started the project after seeing (and buying!) that LED mask in Cyberdog. I had hoped that I‚Äôd be able to hack, or reverse engineer that mask for use in this project, but its OEM hardware and software made that impossible for my skill level, so sadly it wasn‚Äôt really fit for this project. The displays that I ended up using don‚Äôt fit inside a mask comfortably, I bought these because they are what is currently readily available from hardware providers. The micropixel displays inside the mask don‚Äôt seem to be commercially available outside of those products just yet, but they probably will be soon, and this app will be ready when they are!
Azure Cognitive Services Speech service can do translation as well as transcription, it would be possible, with few changes to the code, and perhaps a few extra characters in the font, to make the mask display language translation, which could be very useful when travelling!
I, of course, showed the display and its accompanying app to my mother the last time I visited her and she was overwhelmed with what a difference it made to how well she could understand me in a mask. Yes sometimes the transcription is imperfect, but she was able to grasp the meaning of the words that she couldn‚Äôt hear and was able to look me in the face while speaking to me and that made all the difference. I hope that someone will take this idea and really run with it, because it could make a difference to so many people during this pandemic, and into the future.
Any language.
272 
3
272¬†claps
272 
3
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Lead Developer Advocate
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/awesome-azure/azure-difference-between-azure-private-links-and-azure-service-endpoints-private-links-vs-service-endpoints-8fb0f80ca196?source=search_post---------138,"There are currently no responses for this story.
Be the first to respond.
Azure Private Links vs Azure Service Endpoints ‚Äî Comparison between Private Links and Service Endpoints
Azure Private Link (Private Endpoint) allows you to access Azure PaaS services over Private IP address within the VNet. It gets a new private IP on your VNet. When you send traffic to PaaS resource, it will always ensure traffic stays‚Ä¶
"
https://medium.com/@ymedialabs/the-pros-and-cons-of-jenkins-vs-azure-devops-469c66140b4d?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
YML
Apr 17, 2020¬∑4 min read
By Sandhya Mungarwadi, QA Engineer, YML
The following document depicts the pros and cons of CI/CD tools (Azure DevOps and Jenkins). Before that, let‚Äôs understand what CI /CD means.
Continuous Integration (CI) is a development practice where development teams make small, frequent changes to code. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible.
Continuous delivery (CD) is actually an extension of CI, in which the software delivery process is automated further to enable easy and confident deployments into production at any time.
Azure Pipelines is a service that caters the need for creating pipelines on Azure Cloud Platform. It supports continuous integration (CI) and continuous delivery (CD), hence constantly and consistently tests and builds your code and deploys it to any target by defining a pipeline.
üëçUsability (Great User Interface) ‚Äî Being a new user, it was easy to pick up and go with this tool . We can start with the classic editor which provides an intuitive GUI to create and visualize the integration steps. Later, the user you can define those very same steps in YAML.Hence, we can define the pipeline using two features i.e. Classic Editor and YAML.
üëçCategorized Built in Tasks ‚Äî The tasks are categorized based on the nature of operation. ex:Build tasks,Utility tasks,Deploy tasks etc. This makes easy for the user to add the desired/specific tasks to their pipeline.
üëçGroup Tasks ‚Äî It allows you to encapsulate a sequence of tasks, already defined in a pipeline, into a single reusable task ,just like any other task.
üëçConfiguring CI/CD Pipeline as Code ‚Äî Using YAML we can achieve this. Reference to understand the hierarchy of YAML file-
üëçRequest and Add Tasks to your Pipelines ‚Äî It has a lot of build-in tasks,yet you can download extensions/tasks from the Azure DevOps marketplace.
üëçMicrosoft Hosted Agents ‚Äî Azure Pipelines offers cloud hosted build agents for Linux, Windows, and macOS builds. You can have a look as to what software are installed on the agent using the following reference link.
üëçAny language, any platform, any cloud ‚Äî Build, test, and deploy Node.js, Python, Java, PHP, Ruby, C/C++ , .Net, Android, and iOS apps. Run in parallel on Linux, macOS and Windows. Deploy to Azure, AWS, GCP or on-premises
üëçAzure Pipelines ‚Äî Provide unlimited build minutes to all open source projects and up to 10 concurrent jobs across Windows, Linux and macOS.
üëçAzure Pipeline ‚Äî Analytics are provided at the end of each run with parameters like rate and duration of run.
üëéIntegration with non-Microsoft is difficult ‚Äî Azure DevOps should provide easier integration with other product lines to improve acceptability.
üëéAzure Pipeline ‚Äî Workflow is straightforward (can‚Äôt set if-else or switch-case constructions). This makes it more difficult to develop complex workflows.
üëéThe deprecated tasks/extensions ‚Äî They are not removed from the marketplace.
üëéDocuments are not Up To Date.
A Jenkins pipeline is a suite of plugins that supports implementing and integrating continuous delivery pipelines into Jenkins.
üëçJenkins is open source, easy to install and free.
üëçPlatform Independent ‚Äî Available for all platforms and different operating systems, whether Mac OS X, Windows or Linux.
üëçRich Plugin ‚Äî Jenkins comes with a wide range of plugins.
üëéOutdated UI ‚Äî Its interface seems a bit outdated and not user friendly as it doesn‚Äôt follow modern design principles.
üëé Scripted Pipelines ‚Äî Must be programmed in Groovy.
üëéThough it‚Äôs rich in Plugin, a lot of plugins are not straightforward or unstable ‚Äî Even for a basic tasks, plugins needs to be installed.
üëéThere‚Äôs no YAML interface for Jenkins Pipelines.
üëéJenkins doesn‚Äôt provide any analytics (there are plugins but they are not enough) at the end of each run.
üëéNeeds better documentation.
It all depends on the Team/Project need . We know Jenkins is more flexible to create complex workflow indeed Azure DevOps is faster to adapt. It‚Äôs very rare that a single CI tool will suffice for all scenarios.
If we decide to use both the tools, then we should know that Azure Pipelines supports integration with Jenkins.
YML is a design and digital product agency. We create digital experiences that export Silicon Valley thinking to the world.
See all (1,516)
139 
4
139¬†claps
139 
4
YML is a design and digital product agency. We create digital experiences that export Silicon Valley thinking to the world.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/connect-azure-devops-to-aws-b89120599103?source=search_post---------140,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
Azure DevOps (ADO) is a CI/CD platform from Microsoft ($MSFT). It permits a great deal of flexibility in the type of code run, the structure and permissions sets applied to jobs, and many other items of your creation and management of resources and automated jobs. However, support for other cloud providers is (perhaps obviously) weaker than at $MSFT‚Äôs native Azure Cloud.
"
https://medium.com/avmconsulting-blog/azure-kubernetes-service-aks-d1e71c7ecbe6?source=search_post---------141,"There are currently no responses for this story.
Be the first to respond.
(Highly available, secure, and fully managed Kubernetes service)
Azure Kubernetes Service (AKS) manages your hosted Kubernetes environment, making it quick and easy to deploy and manage containerized applications.
Create AKS cluster using Azure Portal
Sign in to the Azure portal at https://portal.azure.com.
If you don‚Äôt have an account, sign up for the free tier. You will get a $ 200 credit with 1-month validity.
Step 1: In the top search bar, search with AKS and click on ‚ÄúKubernetes Service‚Äù and click on ‚ÄúAdd‚Äù
Step 2: To create an AKS cluster, complete the following steps:
2. Authentication: Configure the following options:
Step 3: Select Review + create and then Create when validated successfully.
It will take some to provision the AKS cluster for you. Once deployment is completed, click on ‚ÄúGo to resources‚Äù.
It will take you to the AKS clusters page.
Step 4: Connect to the cluster
Cloud shell is pre-loaded with kubectl. Open cloud shell using the button on the top right-hand corner of the Azure portal.
To assemble kubectl to attach to your Kubernetes cluster, use the az aks get-credentials command.
To verify the association to your cluster, use the kubectl get command to come to an inventory of the cluster nodes.
Now your cluster is ready to deploy your application.
az login
Note: This login method is configured using the OAuth DeviceProfile flow.
3. If you have multiple subscriptions in Azure, you might need to use az account list and az account set ‚Äìsubscription <Your Azure Subscription ID> to make sure you‚Äôre working on the right one:
4. Create a resource group
After several minutes the command completes and returns JSON-formatted information about the cluster.
Important:
Save the JSON output during a separate computer file, as a result of you would like the ssh keys later during this document.
Note:
Note: If you get the below error when running the on top of az aks, create command, then re-run an equivalent command once more
Note:
While creating AKS, internally a new resource group is created (like MC_<Resource Group Name>_<AKS Name>_<Resource Group Location>) which is consists of a Virtual machine, Virtual network, DNS Zone, Availability set, Network interface, Network security group, Load balancer and Public IP address etc.‚Ä¶
Connect kubectl to your Kubernetes cluster by using the az aks get-credentials command and configure accordingly. This step downloads credentials and configures the Kubernetes user interface to use them.
You should also check that you are able to open the Kubernetes dashboard by running
This will launch a browser tab with a graphical representation:
Thanks
For Reference
kishore1021.wordpress.com
azure.microsoft.com
www.ais.com
üëã Join us today !!
Ô∏èFollow us on LinkedIn, Twitter, Facebook, and Instagram
If this post was helpful, please click the clap üëè button below a few times to show your support! ‚¨á
AVM Consulting‚Ää‚Äî‚ÄäClear strategy for your cloud
486 
We are developing blogging to help community with Cloud/DevOps services¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
486¬†claps
486 
Written by
Vineet Sharma-Founder and CEO of Kubernetes Advocate Tech author, cloud-native architect, and startup advisor.https://in.linkedin.com/in/vineet-sharma-0164
AVM Consulting‚Ää‚Äî‚ÄäClear strategy for your cloud
Written by
Vineet Sharma-Founder and CEO of Kubernetes Advocate Tech author, cloud-native architect, and startup advisor.https://in.linkedin.com/in/vineet-sharma-0164
AVM Consulting‚Ää‚Äî‚ÄäClear strategy for your cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/move-data-from-on-premise-sql-server-to-azure-blob-storage-using-azure-data-factory-bbf67e4e5fde?source=search_post---------142,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Nov 18, 2019¬∑8 min read
Although there is an official tutorial for copying data from on-premise SQL Server to Azure Blob Storage (https://docs.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-portal), this article will focus on some details that were not covered in that tutorial. For example,
"
https://medium.com/@renatogroffe/docker-azure-devops-build-e-deployment-automatizado-de-aplica%C3%A7%C3%B5es-15cec26174c3?source=search_post---------143,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Dec 23, 2019¬∑14 min read
Plataforma com foco em DevOps e mantida pela Microsoft, o Azure DevOps √© uma solu√ß√£o bastante abrangente. Dentre as funcionalidades oferecidas por este servi√ßo est√£o controle de vers√£o do c√≥digo-fonte de projetos de software (GIT ou TFVC), gest√£o de tarefas baseando-se para isto em Scrum ou Kanban, integra√ß√£o cont√≠nua e deployment automatizado, automa√ß√£o de testes e dashboards para monitoramento de um projeto como um todo.
Neste tutorial demonstrarei como efetuar o build de imagens Docker e o deployment automatizado empregando containers, fazendo uso para isto dos seguintes recursos:
Ao optar pela utiliza√ß√£o de Docker em conjunto com o Azure DevOps conseguimos simplificar em muito o processo de build e deployment, j√° que essa integra√ß√£o nos isenta da necessidade de instalar runtimes espec√≠ficos e realizar in√∫meros ajustes de configura√ß√£o (as defini√ß√µes presentes no arquivo Dockerfile costumam ser mais do que suficientes para a compila√ß√£o do projeto e o build da imagem correspondente).
E aproveito este espa√ßo tamb√©m para um convite‚Ä¶
Que tal aprender mais sobre Azure Functions e desenvolvimento de solu√ß√µes Serverless, em um workshop que acontecer√° durante um s√°bado (dia 18/01/2020) em S√£o Paulo Capital e implementando um case na pr√°tica? Acesse ent√£o o link a seguir para efetuar sua inscri√ß√£o com um desconto especial de 25%: http://bit.ly/anp-serverless-blog-groffe
E caso voc√™ precise conhecer mais sobre o Microsoft Azure como um todo, n√£o deixe de aproveitar o pre√ßo promocional de lan√ßamento da primeira turma online do treinamento Azure na Pr√°tica que acontecer√° dia 15/02/2020 (tamb√©m um s√°bado). Aproveite para conhecer mais sobre dezenas de servi√ßos e possibilidades oferecidas pelo Azure e, o melhor, no conforto de sua casa ou ambiente de trabalho! Acesse o link a seguir para informa√ß√µes e efetuar sua inscri√ß√£o: http://bit.ly/anp-online-blog-groffe
O projeto ASP.NET Core 3.1 que servir√° de base para esse tutorial j√° foi disponibilizado no GitHub:
https://github.com/renatogroffe/ASPNETCore3.1-API-REST_Docker-Alpine
Acabei optando por duplicar este reposit√≥rio, j√° que nesta c√≥pia ser√° gravado um arquivo com configura√ß√µes de build do Azure Pipelines.
J√° na listagem a seguir est√° o conte√∫do do arquivo Dockerfile, em que est√£o referenciadas as imagens Alpine do SDK do .NET Core 3.1 (para restaura√ß√£o de pacotes e build da aplica√ß√£o) e do runtime do ASP.NET Core 3.1 (com o ambiente necess√°rio para a execu√ß√£o da API REST a partir de um container):
O Azure Containter Registry permite o armazenamento de imagens de containers de maneira privada, representando assim uma alternativa dentro da nuvem da Microsoft ao Docker Hub.
Nesta se√ß√£o demonstrarei a cria√ß√£o de um novo recurso baseado no Container Registry, a fim de que no mesmo constem as imagens geradas atrav√©s do build automatizado do projeto via Azure DevOps. Acionar para isto no Portal do Azure a op√ß√£o Create a resource:
Em Containers selecionar Container Registry:
E finalmente preencher em Create container registry:
Confirmar finalmente a gera√ß√£o do Container Registry clicando no bot√£o Create:
Projetos do Azure DevOps estar√£o agrupados em Organizations. Para o exemplo descrito nesse tutorial ser√° criada uma Organization chamada groffe-demos. Acessar para isto a op√ß√£o New organization:
Ser√° solicitado ent√£o um aceite dos termos de utiliza√ß√£o do Azure DevOps:
Informar em seguida o nome da Organization, bem como o Data Center em que novos recursos no Azure DevOps estar√£o hospedados; concluir este procedimento acionando a op√ß√£o Continue:
Na tela inicial da Organization groffe-demos ser√° solicitada a cria√ß√£o de um novo projeto, no qual constar√£o as defini√ß√µes para build e release/deployment da aplica√ß√£o ASP.NET Core. Preencher para isto os campos Project name e Description, selecionando ainda a op√ß√£o Private , Git em Version control, Basic em Worker item process e finalmente clicando na op√ß√£o Create project:
A pr√≥xima imagem traz o projeto APIContagem j√° criado; acionar na sequ√™ncia a op√ß√£o Project settings:
Aparecer√£o agora diversas op√ß√µes para configura√ß√£o do projeto:
Deslocar a barra de rolagem para baixo at√© Pipelines, clicando ent√£o na op√ß√£o Service connections:
Teremos agora que criar uma nova conex√£o para o recurso do Azure Container Registry gerado anteriormente. Em New service connection selecionar a op√ß√£o Docker Registry:
Em New Docker Registry service Connection:
Confirmar este procedimento acionando a op√ß√£o Save:
Na imagem a seguir podemos observar a conex√£o ao Azure Container Registry j√° criada:
Ser√° por meio do Azure Pipelines que definiremos o processo automatizado (pipeline) de build de imagens Docker para o projeto APIContagem. Retornando √† se√ß√£o Summary deste projeto acessar na barra lateral Pipelines > Pipelines, como indicado na imagem a seguir:
Acessar agora a op√ß√£o Create Pipeline:
Em Where is your code? selecionar a op√ß√£o GitHub YAML:
J√° em Select a repository definir o reposit√≥rio do GitHub ao qual estar√° atrelado o pipeline de build:
Ser√° solicitado neste momento que o usu√°rio se autentique junto ao GitHub. Realizado este procedimento, aparecer√° agora uma tela com informa√ß√µes do Azure Pipelines:
Descer ent√£o com a barra de rolagem at√© o final da p√°gina, certificando-se de que o reposit√≥rio a ser utilizado est√° selecionado em Repository access. Confirmar esta escolha acionando o bot√£o Approve and install:
Em Configure your pipeline selecionar a op√ß√£o Starter pipeline, a fim de iniciar a montagem do pipeline com um m√≠nimo de configura√ß√µes:
Neste momento aparecer√° a tela Revise your pipeline YAML. Posicionar agora o cursor ao final do conte√∫do do arquivo YAML com defini√ß√µes de pipeline:
Acionar na sequ√™ncia o bot√£o Show assistant:
Localizar agora a task Docker e clicar sobre a mesma:
A partir deste ponto ser√£o preenchidas as configura√ß√µes da task correspondente ao build de uma imagem Docker. Em Container registry selecionar a conex√£o criada anteriormente para o Azure Container Registry:
Em Container repository informar o nome da imagem a ser gerada (apicontagem-azuredevops para o exemplo aqui descrito):
J√° em Commands:
Concluir o processo acionando o bot√£o Add:
Na pr√≥xima imagem podemos observar o conte√∫do do YAML do pipeline de build j√° atualizado:
Foram acrescentadas as defini√ß√µes destacadas na imagem a seguir (em vermelho):
Concluir a configura√ß√£o deste pipeline acionando a op√ß√£o Save and run:
Em Save and run:
Acionar finalmente o bot√£o Save and run:
Neste momento ter√° in√≠cio um Job para build da imagem Docker da aplica√ß√£o de testes (√≠cone azul):
Clicando sobre este Job podemos observar o andamento do mesmo:
Ap√≥s algum tempo o status do Job indicar√° que o processo de gera√ß√£o da imagem Docker teve sucesso (√≠cone verde):
Observando o reposit√≥rio no GitHub ser√° poss√≠vel constatar a presen√ßa do arquivo azure-pipelines.yml, gerado via Azure DevOps e no qual constam as defini√ß√µes de build:
J√° na pr√≥xima imagem temos o conte√∫do do arquivo azure-pipelines.yml, com as configura√ß√µes especificadas nos passos anteriores desta se√ß√£o:
Acessando o recurso do Azure Container Registry criado na se√ß√£o anterior ser√° poss√≠vel notar a presen√ßa da imagem apicontagem-azuredevops em Repositories:
Clicando sobre esta imagem ser√£o listadas as diferentes tags/vers√µes existentes para a mesma (11 e latest, que basicamente correspondem √† mesma imagem):
A grava√ß√£o de uma altera√ß√£o na branch master far√° com que o processo de build das imagens apicontagem-azuredevops seja disparado automaticamente. A fim de simular isto farei aqui uma altera√ß√£o no arquivo ContadorController.cs:
Uma nova execu√ß√£o do build ter√° in√≠cio e poder√° ser consultada via Azure DevOps (detalhes como o que acontece no Job envolvido neste processo podem ser obtidos seguindo as orienta√ß√µes da se√ß√£o anterior):
Ap√≥s algum tempo o processo concluir√°, indicando sucesso na gera√ß√£o de uma nova imagem:
Voltando ao recurso do Azure Container Registry poderemos constatar a presen√ßa de uma nova tag de n√∫mero 12 (a tag latest √© a mais atual, juntamente com 12):
Nesta se√ß√£o demonstrarei a cria√ß√£o de um novo recurso baseado no Azure Web App for Containers, o qual far√° uso da imagem apicontagem-azuredevops (hospedada no Container Registry groffeazuredevops) para a execu√ß√£o de inst√¢ncias da API de testes a partir de containers na nuvem Microsoft.
Utilizando o Portal do Azure incluir um novo recurso, utilizando para isto a op√ß√£o Web App for Containers:
Em Web App informar:
Dar andamento √† cria√ß√£o do recurso acionando a op√ß√£o Next: Docker>.
Informar na se√ß√£o Docker as defini√ß√µes da imagem utilizada no deployment da aplica√ß√£o:
Clicar em seguida no bot√£o Review + create:
Ap√≥s a revis√£o acionar finalmente o bot√£o Create, para assim finalizar este processo com a gera√ß√£o de um novo recurso do Azure Web App for Containers:
Na pr√≥xima imagem podemos observar o recurso j√° criado, com o endere√ßo de acesso ao mesmo destacado em vermelho:
Um teste de acesso √† API REST trar√° no resultado a altera√ß√£o realizada na branch master:
Ainda no Azure Pipelines iremos configurar o pipeline para release/deployment automatizado da aplica√ß√£o, fazendo uso para isso de imagens do Azure Container Registry e do recurso do Azure Web App for Containers descrito na se√ß√£o anterior.
Acessar para isso em Pipelines a op√ß√£o Releases, clicando em seguida no bot√£o New pipeline:
Em Select a template acionar a op√ß√£o Azure App Service deployment, posicionando o mouse sobre a mesma e efetuando um clique no bot√£o Apply:
Preencher o campo Stage name em Stage com o valor Deploy Web App:
Retornando ao diagrama do Pipeline clicar na op√ß√£o 1 job, 1 task do item Deploy Web App:
Preencher agora as seguites configura√ß√µes para a execu√ß√£o do stage Deploy Web App (se√ß√£o Parameters):
Acionar ent√£o o bot√£o Save, a fim de confirmar as configura√ß√µes realizadas:
Um coment√°rio poder√° ser preenchido na janela Save; concluir este procedimento clicando no bot√£o OK:
Clicar agora na op√ß√£o + Add em Artifacts:
Em Add an artifact:
Na sequ√™ncia teremos que ativar o deployment automatizado. Acionar para isto a op√ß√£o Continuous deployment trigger em Artifacts (√≠cone com o sinal de um raio):
Marcar ent√£o o item Continuos deployment trigger como Enabled:
Neste momento um sinal de check aparecer√° do lado do √≠cone com o raio em Artifacts, indicando que o deployment cont√≠nuo/automatizado foi devidamente configurado. Concluir os ajustes clicando no bot√£o Save:
Informar se necess√°rio um coment√°rio na janela Save, confirmando os ajustes realizados atrav√©s de um clique no bot√£o OK:
Sucessivos commits na branch master resultar√£o no build de novas imagens da APIs REST (como j√° demonstrado anteriormente). A novidade agora est√° tamb√©m no deployment automatizado ap√≥s os √∫ltimos ajustes, fazendo uso de tais imagens para publica√ß√£o no recurso baseado no Azure Web App for Containers.
Considerando uma altera√ß√£o como a indicada a seguir:
Logo depois do build das imagens acontecer√° in√≠cio o deployment automatizado, como indicado na pr√≥xima imagem:
Na imagem a seguir podemos observar o deployment em andamento:
E finalmente conclu√≠do:
Analisando o recurso do Azure Container Registry veremos que 18 foi a tag da √∫ltima imagem gerada:
Verificando a se√ß√£o Container settings do recurso do Azure Web App for Containers √© poss√≠vel constatar que o mesmo j√° foi atualizado para trabalhar com containers baseados nesta tag (18):
Acessando a API para testes notaremos que no retorno da mesma j√° consta a altera√ß√£o efetuada na classe ContadorController.cs:
Azure DevOps Services
Canal Julio Arruda - MVP
Azure DevOps Sprints - Canal Vinicius Moura - MVP
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
83 
1
83¬†claps
83 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/aris-recommended-azure-dev-ops-resources-a6d819c4935e?source=search_post---------144,"There are currently no responses for this story.
Be the first to respond.
Tldr; Recently I was asked to put together a list of resources by a member of the local Israeli developer ecosystem, for getting started with Azure DevOps for someone who is familiar with the cloud but not with Azure here is that list.
azure.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
If you need help from the League of Extraordinary Cloud DevOps Adocates just use #LoECDA. It is like our bat signal. Members are @AbelSquidHead @damovisa @DonovanBrown @StevenMurawski and @jldeen
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
If you have any questions, comments, or topics you would like me to discuss feel free to follow me on Twitter.
About the AuthorAaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
51 
1
51¬†claps
51 
1
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@renatogroffe/asp-net-core-2-0-deployment-na-nuvem-com-docker-azure-container-registry-e-azure-web-app-on-linux-17a7dc80f723?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 18, 2017¬∑8 min read
O objetivo deste tutorial √© demonstrar o deployment de um projeto ASP.NET Core 2.0 no Microsoft Azure, fazendo uso para isto de containers Docker e de servi√ßos de hospedagem que integram esta plataforma de cloud computing (Azure Container Registry e Azure Web App on Linux).
A aplica√ß√£o que ser√° utilizada - um site MVC - j√° foi descrita no artigo indicado a seguir, o qual abordou o consumo de APIs REST disponibilizadas gratuitamente pela NASA (Ag√™ncia Espacial Norte-Americana):
Consumo de APIs em .NET Core: utilizando APIs REST da NASA
Esse projeto est√° dispon√≠vel inclusive no GitHub:
https://github.com/renatogroffe/ASPNETCore2_NASA-Open-APIs
Caso queira saber mais sobre o Azure Web App on Linux e o uso de Docker em aplica√ß√µes ASP.NET Core consulte tamb√©m o seguinte post:
ASP.NET Core: APIs REST na nuvem com Docker e Azure Web App
Para gerar uma imagem Docker contendo a aplica√ß√£o mencionada neste artigo ser√£o utilizados recursos do pr√≥prio Visual Studio 2017. Isto acontecer√° por meio de um conjunto de ferramentas conhecidas como Visual Studio Tools for Docker, as quais possibilitam o building, o debugging e a execu√ß√£o de containers a partir de projetos gerados com o .NET Core.
No Visual Studio 2017 ser√° necess√°rio adicionar os arquivos de suporte ao Docker no projeto SiteDadosNASA. Acionar ent√£o o menu de contexto para esse site e, em seguida, as op√ß√µes Add e Docker Support (este processo tamb√©m pode ser realizado durante a cria√ß√£o de um novo projeto):
Aparecer√° neste momento a janela Docker Support Options. Selecionar em Target OS a op√ß√£o Linux:
Este conjunto de a√ß√µes produzir√° como resultados os seguintes itens:
O cont√©udo do arquivo Dockerfile est√° na pr√≥xima listagem:
J√° a listagem a seguir traz as configura√ß√µes do arquivo docker-compose.yml:
Ap√≥s estes procedimentos o projeto SiteDadosNASA j√° estar√° devidamente configurado para execu√ß√£o e, at√© mesmo, debugging a partir de um container Docker. O pr√≥prio √≠cone do Visual Studio para iniciar a aplica√ß√£o indica isto:
Com a aplica√ß√£o SiteDadosNASA em modo de execu√ß√£o ser√° poss√≠vel constatar a exist√™ncia de uma imagem Docker criada para testes de desenvolvimento, al√©m de um container baseado em tal estrutura.
Ao executar o comando docker images no PowerShell aparecer√£o as imagens microsoft/aspnetcore:2.1 e sitedadosnasa:dev:
J√° o comando docker ps -a trar√° o container gerado para utiliza√ß√£o a partir do Visual Studio:
Ser√° preciso agora gerar uma nova imagem Docker com base nas configura√ß√µes de release da aplica√ß√£o. Selecionar para isto a op√ß√£o Release (ao inv√©s de Debug) no Visual Studio 2017:
Efetuar na sequ√™ncia a compila√ß√£o do projeto (a partir do menu Build > Build Solution). Ao executar novamente o comando docker images no PowerShell aparecer√° ent√£o a imagem sitedadosnasa:latest:
OBSERVA√á√ÉO: os procedimentos descritos neste artigo foram realizados a partir do Visual Studio 2017 Update 15.3, utilizando ainda o Docker for Windows em um ambiente de desenvolvimento baseado no Windows 10.
O Azure Containter Registry permite o armazenamento de imagens de containers de maneira privada, trazendo assim uma alternativa dentro da nuvem da Microsoft ao uso de planos pagos do Docker Hub. Este servi√ßo pode ser empregado em conjunto com tecnologias como Azure Container Services (com seus diferentes orquestradores - Docker Swarm, DC/OS e Kubernetes) e Azure Web App on Linux.
No portal do Azure ser√° criado um novo recurso baseado no servi√ßo Azure Container Registry (em portugu√™s Registro de Cont√™iner do Azure):
Informar no formul√°rio de cria√ß√£o:
Ap√≥s alguns segundos o item renatogroffe (um Container Registry) aparecer√° na lista de recursos dispon√≠veis:
Uma tag chamada renatogroffe.azurecr.io/sitedadosnasa dever√° ser criada para a imagem sitedadosnasa:latest. Este novo elemento cont√©m o nome que ser√° gravado no Azure Container Registry (formado pela identifica√ß√£o do registro de containers + nome da aplica√ß√£o/imagem; esses dois itens est√£o separados ainda por uma barra - ‚Äú/‚Äù). Executar para isto o seguinte comando no PowerShell:
docker tag sitedadosnasa:latest renatogroffe.azurecr.io/sitedadosnasa
Ao acionar novamente a instru√ß√£o docker images no PowerShell aparecer√° uma imagem chamada renatogroffe.azurecr.io/sitedadosnasa:
O pr√≥ximo passo agora ser√° efetuar o login no recurso do Azure Container Registry criado na se√ß√£o anterior. Executar para isto o seguinte comando no PowerShell (em que ser√£o fornecidos o usu√°rio e uma senha disponibilizados pelo Microsoft Azure):
docker login renatogroffe.azurecr.io -u USU√ÅRIO -p SENHA
As credenciais necess√°rias est√£o na se√ß√£o Access keys do Container Registry (√© poss√≠vel utilizar qualquer uma das senhas indicadas em password e password2):
A imagem a seguir mostra que a autentica√ß√£o teve sucesso:
Para publicar a imagem renatogroffe.azurecr.io/sitedadosnasa no Azure Container Registry ser√° utilizado o comando:
docker push renatogroffe.azurecr.io/sitedadosnasa
O resultado desta a√ß√£o pode ser observado na pr√≥xima imagem:
A imagem renatogroffe.azurecr.io/sitedadosnasa aparecer√° ent√£o no Azure Container Registry (se√ß√£o Repositories), logo ap√≥s a conclus√£o deste √∫ltimo procedimento:
Mais uma vez ser√° criado um novo recurso no portal do Azure, desta vez baseando-se no servi√ßo Web App on Linux:
Preencher no formul√°rio de cria√ß√£o do recurso:
Acionar ap√≥s informar o conte√∫do destes campos a op√ß√£o Configurar cont√™iner:
Em Cont√™iner do Docker selecionar o item Registro privado, especificando na sequ√™ncia as seguintes configura√ß√µes:
Acionar ent√£o o bot√£o OK em Cont√™iner do Docker, confirmando as defini√ß√µes a serem utilizadas para a gera√ß√£o do novo container. Concluir este processo clicando sobre o bot√£o Criar, a partir do formul√°rio em que se especificaram as configura√ß√µes do site a ser hospedado na nuvem:
A op√ß√£o Todos os recursos listar√°, ap√≥s um curto per√≠odo de tempo, o recurso consultadadosnasa:
Acessando o item consultadadosnasa aparecer√° um painel no qual constar√° o endere√ßo da aplica√ß√£o de testes, bem como op√ß√µes para a configura√ß√£o e gerenciamento deste recurso:
Testes com a URL http://consultadadosnasa.azurewebsites.net/ trar√£o resultados similares aos encontrados nas imagens a seguir:
Muito embora este artigo tenha focado no deployment de aplica√ß√µes ASP.NET Core 2.0 na nuvem, o uso de servi√ßos como Azure Container Registry e Azure Web App on Linux n√£o est√° restrito √† nova plataforma de desenvolvimento Web da Microsoft. Projetos implementados em linguagens como Node.js, Java, Ruby, Python e PHP tamb√©m podem se valer dos recursos aqui descritos, sendo que isto √© poss√≠vel gra√ßas ao suporte que tais tecnologias oferecem para o trabalho com containers Docker.
ASP.NET Core - Documentation
Azure Container Registry - Documentation
Conte√∫dos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0
Introduction to Azure Web App on Linux
Visual Studio Tools for Docker
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
62 
2
62¬†claps
62 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/serverless-from-the-beginning-using-azure-functions-azure-portal-part-i-7334a083b8f0?source=search_post---------146,"Follow me on Twitter, happy to take your suggestions on topics or improvements /Chris
Serverless is Cloud-computing execution model in which the cloud provider runs the server, and dynamically manages the allocation of machine resources. So essentially you can focus on writing code as your Cloud Provider does the rest
This article has been moved to https://softchris.github.io/pages/serverless-one.html
"
https://medium.com/@thisiszone/image-recognition-using-the-azure-custom-vision-service-c0bfc74a3343?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zone
Nov 22, 2018¬∑9 min read
Zone‚Äôs head of .NET development, Andy Butland, explores the AI and machine learning possibilities offered by Azure‚Äôs Custom Vision Service‚Ä¶
In recent years, the availability of machine learning algorithms, made available as services, has transformed our ability to add artificial intelligence features to applications. Functionality that was once the remit of hardcore AI experts can now be accessed by a much wider range of developers armed with just a cloud subscription.
I‚Äôve recently been working with one such service: the Custom Vision Service, part of the suite of Cognitive Services, provided by Microsoft and hosted at Azure.
The Cognitive Services offer several specific AI or machine learning tasks, provided as services via an API, that we can integrate into web, mobile and desktop applications, using JSON-based REST calls over HTTP or, in many cases, a higher-level client library.
Payment for the services is via a pay-as-you-go model where cost will depend on usage, with generous free tiers that can be used for experimentation, development and low-volume production requirements.
There are two services offering features related to image recognition. The first ‚Äì and more established ‚Äì is Computer Vision, which uses a pre-built model created and maintained by Microsoft. The Custom Vision service, currently available in preview, allows you to build and train your own model, dedicated to a given image domain.
Although there‚Äôs more work involved in sourcing images, tagging them, training and refining the model, the advantage of a dedicated one is that we‚Äôll likely get improved accuracy when using only images from our chosen subject. This is due to the model being able to make finer distinctions between images and to avoid distractions from superficially similar but in reality quite different image subjects.
To create a Custom Vision Service Model, you‚Äôll need an Azure subscription. Sign in and, after typing ‚Äúcustom vision‚Äù into the search box, you‚Äôll find a link to the service. Similar to almost all other Azure resources, you‚Äôll be presented with a blade where you need to select the name, pricing tier, location and some other details, following which the service will be instantiated.
Pricing details can be found here. Payment is based on usage and a small amount for storage, but for investigating the service the free tier is perfectly adequate and gives access to all features, though you are limited to a single project.
The location can currently only be a single Azure region in the US, but I‚Äôd expect the service to be rolled out to more data centres as it comes out of preview, reducing latency for accessing it from websites and applications hosted in other areas of the world.
Although some aspects common with other Azure services are managed from the standard Azure portal, in most cases we work in a custom portal dedicated to the service, accessed via the ‚ÄúQuick start‚Äù menu.
Once in this custom portal we can create a project, giving it a name, description, selecting between single and multiple subject per images options, and selecting a domain. The section here is important if you want to use the model in offline contexts, which I‚Äôll discuss later in this article, but for now only if one of the selections (such as ‚Äúlandmarks‚Äù or ‚Äúfood‚Äù) matches our chosen subject matter should we choose anything other than ‚ÄúGeneral‚Äù.
With the project created we then need to provide some pre-classified source images that we can upload via the interface and associate them with the required tag. Make sure to keep some back from the training set of images for each tag, so we have some images the model hasn‚Äôt seen directly to evaluate with.
The next step is to train the model via the green button at the top, which, once complete, will show us some statistics of the expected model performance.
We can continue to tweak the model to look to improve these numbers by providing further images. We should aim for at least 50 in each category, with a balanced number across each, and look to provide the range of viewpoints, backgrounds and other settings in which test images may be provided when the model is in use. Each time we do this we can retrain and get a new iteration of the model with an updated set of statistics.
Up to now we‚Äôve interacted with the Azure portal interfaces for the purposes of creating and working with our model. While we might continue to use this method for initial training of the model, and ongoing refinement of it, the real power of the service comes from the API ‚Äì known as the prediction API ‚Äì that allows us to integrate it within our own applications.
Before doing that though, we need to extract three pieces of information from the portal, which we‚Äôll need to pass for identification and authorisation purposes in the API requests. These are the project ID, the project URL and the prediction API key.
There are actually two ways to access the prediction API. The first, and only if working cross-platform, is via REST services called over HTTP, passing requests and parsing response in JSON. If on Windows, we also have the option of leaning on a client library, that abstracts away some of the underlying details of the HTTP requests and allows us to work at a higher, and more strongly-typed, level.
Although the latter is simpler, the lower-level, HTTP-based approach is also quite straightforward. The key code sample for using this is shown below:
As we‚Äôll interact with the API by making REST calls over HTTP, first we need to create a HttpClient object.
Authorisation of access to our model is carried out by the checking of a passed header value, which we need to set before we make the request. The name of the header is Prediction-Key and the value is that which we obtained from the custom vision portal for our project. We‚Äôve stored this in a settings file, which, within Azure function projects from which the above sample is taken, are surfaced in the form of environment variables.
Similarly, we‚Äôve stored and can access the prediction URL end point of our model that we‚Äôre going to make our request to.
We‚Äôre using the API option where we provide the image as part of the body of our request ‚Äì rather than providing a URL to the test image which is also supported ‚Äì so need to convert the uploaded file from the stream we have access to, to an array of bytes, which is then wrapped within a HttpContent instance.
Having set the content type header to application/octet-stream we can then make our HTTP request using the POST method.
The response comes back as a JSON string in the example you can see, which we can deserialise into an instance of a strongly typed class we‚Äôve also created. As well as some detail about the project and iteration used, we get back a list of predictions ‚Äì one for each tag, and for each tag a probability of how confident the model is that the provided image matches the tag.
With the Windows client library, the code is a little simpler:
We‚Äôre leaning here on two NuGet packages. First Microsoft.Cognitive.CustomVision.Prediction, which is the client library itself, and then Microsoft.Rest.ClientRuntime ‚Äì another library it depends on used for wrapping the REST HTTP calls.
From these, we have a model class provided, so we don‚Äôt need to create our own and ensure that it matches with the expected JSON response. We are also abstracted away from the HTTP calls, so there‚Äôs no HttpClient to work with, and need to set headers and parse responses.
Rather we just create a new endpoint object, providing the prediction key that we store in settings and is made available via an environment variable. We then make a call using the PredictImageAsync method, passing the project ID ‚Äìalso held in settings ‚Äì and a stream representing the image file itself. We get back a strongly typed response that we can then handle as we need.
The Custom Vision Service provides a second API ‚Äì known as the training API ‚Äì that we can use for integrating features of building and training models into our applications. While this won‚Äôt always be useful ‚Äì and we can of course continue to use the portal for such tasks ‚Äì there could be cases where maintaining a model becomes part of a business process and as such makes sense to provide such features within a custom application.
We work with the training API in a similar way to that shown for the prediction API, though I‚Äôd suggest in this case, although we could use the lower-level HTTP calls, the nature of the training features mean we‚Äôll likely be making a number of different requests to the API, and as such, would likely be wanting to create some form of wrapper to keep our code maintainable. Rather than doing that though, it makes sense to use the open-source client library that‚Äôs already available, at least if supported on the platforms you need to work with.
Sometimes, a real-time request over the internet isn‚Äôt feasible or appropriate ‚Äì perhaps for a mobile or desktop application where network connectivity may not be permanently available. The Custom Vision Service supports scenarios like this allowing us to export the model into a file we can embed and use within our applications.
I mentioned earlier when creating a Custom Vision Service project that we had the option to select a domain. We had a choice between various high-level subject matters and also the option to select a ‚Äúcompact‚Äù option, and this is what we need to use for a model we want to be able export and use offline. Fortunately, we don‚Äôt have to start creating our model all over again if we‚Äôve already created it with a standard domain; we‚Äôre able to change the domain of an existing model and then retrain it to create a new iteration using the selected domain.
Only ‚Äúcompact‚Äù domains can be exported, but in choosing this we do have to accept a trade-off in a small reduction in accuracy, due to some optimisations that are made to the model for the constraints of real-time classification on mobile devices.
When we export and download a model iteration, we have the choice of a number of formats, each of which are suitable for different application platforms. For example, we can select CoreML for IoS, TensorFlow for Android and Python or ONNX for Windows UWP.
The primary reason I‚Äôve been investigating and working with the Custom Vision Service is due to working on authoring a course for Pluralsight, where I go into the subject in a lot more detail than covered in this article, illustrate demos and sample applications, walk through code and discuss more background of image classification theory and how we can measure and refine our models.
If you have a Pluralsight subscription, or are interested in taking out a free trial, you can access the course here.
As part of working on this course I‚Äôve released the code samples as open-source repositories on GitHub, that you can access here:
Further useful links and references I used in authoring the course, samples and writing the article are:
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
105 
105¬†claps
105 
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/building-rest-api-with-python-flask-and-azure-sql-4e99e2b53180?source=search_post---------148,"There are currently no responses for this story.
Be the first to respond.
Azure SQL has native JSON support which is a key factor to simplify a lot ‚Äî and make developer-friendly ‚Äî the interaction between the database and any service that needs to handle data in even the most exotic way.
"
https://medium.com/swlh/azure-devops-yml-terraform-pipeline-and-pre-merge-pull-request-validation-6352b841376a?source=search_post---------149,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
tl;dr: Here‚Äôs YML code that will build an Azure DevOps pipeline that can be run automatically as part of pull request validation (pre-merge) and requires manual approval by definable admin groups in order to proceed to modifing resources.
Microsoft‚Äôs Azure DevOps (ADO) is an incredibly powerful CI/CD platform that is being rapidly‚Ä¶
"
https://medium.com/wavesprotocol/microsoft-azure-cloud-features-waves-smart-assets-and-smart-accounts-1a71b3c23c2b?source=search_post---------150,"There are currently no responses for this story.
Be the first to respond.
Waves steps up integration with Microsoft Azure, making its Smart Asset and Smart Account functionality available to developers on the cloud platform.
Waves‚Äô smart accounts and smart assets are now available to developers on Azure Marketplace in a special extension for Visual Studio Code, which simplifies the process of creating and operating smart contracts.
Two new virtual machine templates in Azure can be used for setting up new options on a public or private blockchain.
‚ÄòUnlike other similar solutions, non-Turing complete smart contracts from Waves offer increased security and protection from vulnerabilities due to the option of restricting functionality to avoid undesirable use‚Äô, comments Sasha Ivanov, Founder and CEO of Waves Platform.
‚ÄòA special extension for Microsoft Visual Studio Code editor will simplify the process of writing smart contracts for developers, allowing them to work in complete integration with Microsoft Azure cloud,‚Äô he continues. ‚ÄòI am confident that users will hugely appreciate the variety of opportunities and possibilities this opens up for them!‚Äô
‚ÄòWe believe that supporting open-source platforms and facilitating the widespread availability of contemporary technological tools for the developer community is very important,‚Äô says Konstantin Goldstein, Microsoft Russia‚Äôs principal technical evangelist.
‚ÄòWaves is one of world‚Äôs leading blockchain platforms, which contributes substantially to the development of an ecosystem for global decentralised solutions, and we are glad to support the company in its development,‚Äô he adds.
Waves Platform has been deployed in Azure cloud since 2017.
The functionality of Waves‚Äô smart accounts and smart assets can be applied in a variety of use cases, including supply chain management, verification of documentation, organization of internal transactions, filing accounting reports and game development.
Read more about Waves Smart Accounts and Smart Assets.
Join Waves CommunityRead Waves News channelFollow Waves TwitterSubscribe to Waves Subreddit
Open blockchain protocol and development toolset for Web 3.0 applications and decentralized solution
303 
303¬†claps
303 
Waves is an open blockchain protocol and development toolset for Web 3.0 applications and decentralized solutions, aiming to raise security, reliability and speed of IT systems. It enables anyone to build their apps, fostering mass adoption of blockchain.
Written by
Waves Tech is a powerful blockchain-agnostic ecosystem focused on inter-chain DeFi, the embodiment of technological freedom for blockchain-based finance.
Waves is an open blockchain protocol and development toolset for Web 3.0 applications and decentralized solutions, aiming to raise security, reliability and speed of IT systems. It enables anyone to build their apps, fostering mass adoption of blockchain.
"
https://medium.com/@renatogroffe/asp-net-core-azure-web-app-for-containers-escalando-uma-api-rest-com-containers-docker-f16bca26a019?source=search_post---------151,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 15, 2018¬∑6 min read
Preocupa√ß√£o muitas vezes relegada a um segundo plano, a necessidade de escalar uma API REST pode resultar em grandes esfor√ßos em termos de prepara√ß√£o e configura√ß√£o de recursos de infraestrutura em ambientes on-premises (locais). Como podemos ent√£o simplificar e tornar mais r√°pido este processo, de forma a atender a uma demanda crescente de acessos em nossas APIs?
Uma resposta a isto seria a utiliza√ß√£o de algum servi√ßo de hospedagem na nuvem. Alternativas do tipo PaaS (Plataform as a Service) permitem uma maior √™nfase na constru√ß√£o e entrega de uma solu√ß√£o, dispensando os envolvidos no projeto de grandes preocupa√ß√µes no que se refere √† montagem de toda a infraestrutura requerida. Esta √© sem sombra de d√∫vidas uma grande vantagem, mas ainda assim existe o risco de se ficar atrelado a uma tecnologia espec√≠fica disponibilizada por um provider de cloud computing. E como superar tal limita√ß√£o?
A resposta a esta segunda quest√£o passa pelo uso de Docker. A escolha por servi√ßos compat√≠veis com containers Docker contribui para que um projeto fique menos dependente de tecnologias propriet√°rias na nuvem, tornando assim vi√°vel a troca de um fornecedor de servi√ßos de cloud por outro (frente a condi√ß√µes econ√¥micas ou t√©cnicas que estimulem tal movimento).
No caso espec√≠fico do Microsoft Azure, temos diversas op√ß√µes para a hospedagem de aplica√ß√µes containerizadas. Uma destas alternativas √© o Azure Web App for Containers, um servi√ßo que permite a publica√ß√£o de APIs e sites a partir de imagens Docker e que conta inclusive com suporte a solu√ß√µes de CI/CD (Continuous Integration/Continuous Deployment) como Visual Studio Team Services (VSTS), Jenkins, Maven e Travis CI.
Este artigo aborda a publica√ß√£o de uma API REST a partir de uma imagem Docker empregando para isto o Azure Web App for Containers, bem como o processo de escalar horizontalmente a aplica√ß√£o correspondente a fim de que 2 inst√¢ncias (containers) atendam a solicita√ß√µes HTTP enviadas √† mesma. Trata-se de um cen√°rio comum a aplica√ß√µes de m√©dio e pequeno porte, sendo que o uso do Web App for Containers prov√™ toda a infraestrutura necess√°ria sem grandes complica√ß√µes no que se refere ao deployment e gerenciamento de recursos.
E aproveito este espa√ßo para deixar aqui um convite.
Dia 25/07/2018 (quarta-feira) √†s 21h30 - hor√°rio de Bras√≠lia - teremos mais um hangout no Canal .NET. Confira esta apresenta√ß√£o online com o MVP Elemar J√∫nior e aprenda mais sobre a modelagem de Microsservi√ßos com base em processos de neg√≥cio.
Para efetuar a sua inscri√ß√£o acesse a p√°gina do evento no Facebook ou ent√£o o Meetup. A transmiss√£o acontecer√° via YouTube, em um link a ser divulgado em breve.
Para os testes descritos neste artigo ser√° utilizada uma API REST baseada no ASP.NET Core. Esta aplica√ß√£o produzir√° como retorno a quantidade de acessos √† API, al√©m de exibir o nome do host/m√°quina e do sistema operacional utilizado pelo container Docker. O projeto em quest√£o j√° foi detalhado no seguinte artigo:
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem - parte 1
Esta mesma aplica√ß√£o conta com uma imagem (renatogroffe/apicontagem) no Docker Hub e que servir√° de base para a publica√ß√£o da API no Azure:
Os fontes empregados para a gera√ß√£o desta imagem (incluindo o arquivo Dockerfile) j√° foram disponibilizados no GitHub:
https://github.com/renatogroffe/ASPNETCore2.1_Docker
Importante destacar que a imagem em quest√£o faz uso da distribui√ß√£o Alpine Linux. Maiores detalhes sobre a utiliza√ß√£o desta alternativa com Docker e ASP.NET Core 2.1 podem ser encontrados no artigo a seguir:
ASP.NET Core 2.1: Release Candidate 1 e Docker Alpine
Outras informa√ß√µes sobre o uso de Docker com ASP.NET Core e Azure podem ser encontrados no seguinte post, no qual est√£o todos os conte√∫dos que venho produzindo sobre o uso conjunto dessas tecnologias:
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
No portal do Azure ser√° necess√°rio criar um novo recurso baseado no servi√ßo Web App for Containers:
Informar no formul√°rio de cria√ß√£o do recurso:
Acionar ap√≥s o preenchimento destes campos a op√ß√£o Configure container:
Em Container Settings selecionar Single Container e o item Docker Hub, certificando-se de que a op√ß√£o Public est√° marcada em Repository Access e informando ainda no campo Image and optional tag o valor renatogroffe/apicontagem (correspondente √† imagem publicada anteriormente no Docker Hub):
Selecionar OK em Container Settings, confirmando as defini√ß√µes a serem utilizadas para a cria√ß√£o do container na nuvem. Concluir ent√£o este processo acionando o bot√£o Create no formul√°rio em que se especificaram as configura√ß√µes da API REST que ficar√° hospedada na nuvem:
O recurso apicontagem e o Service Plan correspondente aparecer√£o ap√≥s alguns segundos dentro do grupo de recursos TesteAPIEscalavel:
Ao acessar o item apicontagem ser√° exibido um painel no qual constar√° o endere√ßo da aplica√ß√£o de testes (https://apicontagem.azurewebsites.net/), al√©m de outras op√ß√µes para a configura√ß√£o e gerenciamento deste recurso:
O valor do machineName para um primeiro teste com a URL https://apicontagem.azurewebsites.net/api/contador foi b2594f7d5e8d, com o mesmo correspondendo ao nome do container gerado ao publicar a aplica√ß√£o atrav√©s do Azure Web App for Containers:
A op√ß√£o Scale up (App Service plan) possibilita escalar verticalmente a aplica√ß√£o, atrav√©s da sele√ß√£o de configura√ß√µes do hardware a ser empregado tais como n√∫mero de cores, mem√≥ria e storage (este conjunto de funcionalidades n√£o ser√° abordado neste artigo):
J√° a op√ß√£o Scale out (App Service plan) permite escalar horizontalmente a API publicada no Azure Web App for Containers. Este procedimento pode tanto ser efetuado manualmente, atrav√©s do campo Override condition em Configure:
Quanto de forma autom√°tica, por meio da op√ß√£o Enable autoscale. Neste √∫ltimo caso temos a possibilidade de sele√ß√£o de uma m√©trica envolvendo crit√©rios como mem√≥ria, uso de CPU e volume de dados para a configura√ß√£o do processo de escalonamento da aplica√ß√£o (este tipo de configura√ß√£o tamb√©m n√£o ser√° coberto neste artigo):
Ser√£o definidas em Override condition 2 inst√¢ncias para esta API de testes, acionando-se na sequ√™ncia a op√ß√£o Save para confirmar esta configura√ß√£o:
Ap√≥s alguns segundos uma nova inst√¢ncia/container j√° estar√° dispon√≠vel (machineName = 8cd8c75151e7), al√©m daquela vinculada ao primeiro container (machineName = b2594f7d5e8d). √â o que demonstram as imagens a seguir, obtidas ap√≥s o envio de diversas requisi√ß√µes ao endere√ßo da API de testes:
Esses testes demonstram o qu√£o f√°cil √© escalar uma aplica√ß√£o Web atrav√©s do Azure Web App for Containers, dispensando assim os respons√°veis pelo deploy de um projeto da necessidade de configura√ß√£o de mecanismos de load balancer e outros recursos associados a um cen√°rio com m√∫ltiplas inst√¢ncias.
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
35 
3
35¬†
35 
3
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/azure-synapse-for-data-analytics-create-workspaces-with-cli-bd5ef90fd489?source=search_post---------152,"There are currently no responses for this story.
Be the first to respond.
One of the challenges of large scale data analysis is being able to get the value from data with least effort. Doing that often involves multiple stages: provisioning infrastructure, accessing or moving data, transforming or filtering data, analyzing and learning from data, automating the data pipelines, connecting with other services that provide input or consume the output data, and more. There are quite a few tools available to solve these questions, but it‚Äôs usually difficult to have them all in one place and easily connected.
If this article was helpful or interesting to you, follow @lenadroid on Twitter.
This is the first article in this series, which will cover what Azure Synapse is and how to start using it with Azure CLI. Make sure your Azure CLI is installed and up-to-date, and add a synapse extension if necessary:
What is Azure Synapse?In Azure, we have Synapse Analytics service, which aims to provide managed support for distributed data analysis workloads with less friction. If you‚Äôre coming from GCP or AWS background, Azure Synapse alternatives in other clouds are products like BigQuery or Redshift. Azure Synapse is currently in public preview.
Serverless and provisioned capacityIn the world of large-scale data processing and analytics, things like autoscale clusters and pay-for-what-you-use has become a must-have. In Azure Synapse, you can choose between serverless and provisioned capacity, depending on whether you need to be flexible and adjust to bursts, or have a predictable resource load.
Native Apache Spark supportApache Spark has demonstrated its power in data processing for both batch and real-time streaming models. It offers a great Python and Scala/Java support for data operations at large scale. Azure Synapse provides built-in support for data analytics using Apache Spark. It‚Äôs possible to create an Apache Spark pool, upload Spark jobs, or create Spark notebooks for experimenting with the data.
SQL supportIn addition to Apache Spark support, Azure Synapse has excellent support for data analytics with SQL.
Other featuresAzure Synapse provides smooth integration with Azure Machine Learning and Spark ML. It enables convenient data ingestion and export using Azure Data Factory, which connects with many Azure and independent data input and output sources. Data can be effectively visualized with PowerBI.
At Microsoft Build 2020, Satya Nadella announced Synapse Link functionality that will help get insights from real-time transactional data stored in operational databases (e.g. Cosmos DB) with a single click, without the need to manage data movement.
Prepare the necessary environment variables:
Create a resource group as a container for your resources:
Create a Data Lake storage account:
The output of this command will be similar to:
Retrieve the storage account key:
Retrieve Storage Endpoint URL:
You can always check what your storage account key and endpoint are by looking at them, if you‚Äôd like:
Create a fileshare:
Create a Synapse Workspace:
The output of the command should show the successful creation:
After you successfully created these resources, you should be able to go to Azure Portal, and navigate to the resource called $SynapseWorkspaceName within $ResourceGroup resource group. You should see a similar page:
What‚Äôs next?
You can now load data and experiment with it in Synapse Data Studio, create Spark or SQL pools and run analytics queries, connect to PowerBI and visualize your data, and many more.
Stay tuned for next articles in this series to learn more! Thanks for reading!
If this article was interesting to you, follow @lenadroid on Twitter.
Any language.
145 
145¬†claps
145 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Solution Architecture. Distributed systems, big data, data analysis, resilient and operationally excellent systems.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/aws-certified-user-group-turkey/azure-devops-server-2019-rc-1-ile-ci-cd-s%C3%BCre%C3%A7lerine-genel-bak%C4%B1%C5%9F-b4277e9a730a?source=search_post---------153,"There are currently no responses for this story.
Be the first to respond.
Merhabalar tekrardan daha √∂nceki yazƒ±larƒ±mƒ±zda Atlassian √ºr√ºn ailesine ait ara√ßlarƒ± kullanarak DevOps s√ºre√ßlerini u√ßtan uca kurgulayarak yazƒ±lƒ±m takƒ±mlarƒ±nƒ±n √ßevik s√ºre√ßlere dahil olmalarƒ±nƒ± ve √ºretilen √ßƒ±ktƒ±larƒ±n s√ºrekli entegrasyon kavramƒ± ile kaliteli ve izlenebilir bir hale getirilmesini ele almƒ±≈ütƒ±k. Bu yazƒ±mƒ±z ile bu mimari kurguyu Microsoft tarafƒ±ndan √ßok yakƒ±n zamanda duyurulan Azure DevOps Server 2019 RC1 √ºzerinde kurgulayacaƒüƒ±z.
Microsoft firmasƒ± √ßok yakƒ±n bir zaman √∂nce DevOps s√ºre√ßlerini y√ºr√ºtmek √ºzere kullanƒ±cƒ±lara sunduƒüu TFS √ºr√ºn√º yeniden g√∂zden ge√ßirerek t√ºm ara√ßlarƒ±nƒ± ve yeni √∂zelliklerini sunduƒüu Azure DevOps Server 2019 RC1 uygulama sunucusunu duyurdu. Artƒ±k t√ºm devops s√ºre√ßlerinizi aynƒ± yapƒ± √ºzerinden kurgulayarak bir izleme ekranƒ± ile y√∂netme ve geri izleme yapabilme ≈üansƒ±na sahipsiniz.
Azure DevOps Server ile ilgili detaylara bu makale √ºzerinden ula≈üabilirsiniz. Bu sunucu ile birlikte aslƒ±nda Microsoft tarafƒ±nda daha √∂nceden kullandƒ±ƒüƒ±mƒ±z TFS ile birlikte azure hizmetleri olarak sunulan Azure Boards , Azure Repos , Azure Pipelines , Azure Test Plans ve Azure Artifacts servislerini on-prem bir sunucuda kullanabiliyoruz.Bu servisler ile ilgili genel bilgileri ilgili link √ºzerinden takip edebilirsiniz.
Yazƒ±mƒ±zƒ±n genel konusu s√ºre√ßlerin tasarƒ±mƒ± ve kullanƒ±mƒ± olduƒüu i√ßin servislerin genel bilgilerini ilgili yerlerde kƒ±saca √ºzerinden ge√ßerek anlatmaya √ßalƒ±≈üacaƒüƒ±m. Bu s√ºre√ßleri tasarlamak adƒ±na Azure DevOps Server 2019 RC1 kurulum dosyasƒ±nƒ± indirerek gerekli kurulum a≈üamalarƒ±nƒ± tamamlamƒ±≈ü olmamƒ±z gerekmektedir.
Kurulum ile ilgili olarak ben tek sunucu √ºzerinde bir kurguyu tercih ettim. Detaylƒ± kurulum adƒ±na ilgili medium yazƒ±sƒ±nƒ± incelemenizin yararlƒ± olacaƒüƒ±nƒ± d√º≈ü√ºn√ºyorum.
Sunucu i√ßin azure VM modellerinden B2ms ile hemen gerekli alt yapƒ±mƒ±zƒ± olu≈üturuyoruz.
√úzerinde windows server 2019 i≈ületim sistemi kurulu olan sanal makinamƒ±zda √∂nceden indirdiƒüimiz Devops Server 2019 RC 1 uygulamasƒ±nƒ± kurmamƒ±z gerekmektedir. En basit ≈üekilde kurulumu a≈üaƒüƒ±daki linkten takip ederek ger√ßekle≈ütirebilirsiniz.
Sunucu kurulum a≈üamalarƒ± :https://1drv.ms/b/s!AskWoAU3NqUug_VU9vKkn8mjTriUmQ
Azure repos , Microsoft √ºr√ºn ailesinin bizlere sunduƒüu istediƒüimiz IDE, d√ºzenleyici veya Git istemcisinden Git depolarƒ±na g√ºvenli bir ≈üekilde baƒülanabildiƒüimiz , kod geli≈ütirme ve branch stratejilerimizi y√ºr√ºtt√ºƒü√ºm√ºz ve kontrol ettiƒüimiz bir web depolama servisidir.
Burada √∂ncelikli olarak git hesabƒ±mda bulunan bir √∂rnek uygulama reposundan nasƒ±l kodlarƒ±mƒ±zƒ± azure repos‚Äôa aktardƒ±ƒüƒ±mƒ±zƒ± g√∂stermek istiyorum.
Git hesabƒ± √ºzerinden DevOpsCore adƒ± altƒ±nda bir repo olu≈üturarak , Visual Studio ile olu≈üturmu≈ü olduƒüum √∂rnek uygulama kodlarƒ±nƒ± bu repoya g√∂nderiyorum.
Sonrasƒ±nda ise kurulumunu ger√ßekle≈ütirdiƒüim devops server √ºzerindeki olu≈üturduƒüum ‚ÄúDevOpsTest‚Äù √ºzerinde ‚ÄúTestProject‚Äù isimli projeyi olu≈üturarak artƒ±k t√ºm ilgili devops s√ºre√ßlerimi bu proje √ºzerinden y√ºr√ºtmeyi planlƒ±yorum.
Burada bulunan repoya , git √ºzerindeki kodlarƒ±mƒ± √ßekerek artƒ±k devops server √ºzerinde versiyonlama i≈ülemlerimi y√ºr√ºtebileceƒüim alt yapƒ±yƒ± hazƒ±rlamƒ±≈ü oluyorum.
Azure Repos kullanƒ±m a≈üamalarƒ± :https://1drv.ms/b/s!AskWoAU3NqUug_VSCB6u_Xp6KnuvtA
Azure repos √ºzerinde olu≈üturduƒüumuz kodlarƒ±mƒ±zƒ± bir takƒ±m kriterlere tabi tutarak build etmemizi saƒülayacak ve s√ºrekli entegrasyon kavramƒ±nƒ±n mimari yapƒ±mƒ±zdaki kar≈üƒ±lƒ±ƒüƒ± olan azure pipeline kurgusunu olu≈üturmaya ba≈ülayabiliriz.
√ñncelikle Devops Server 2019 RC 1 kurulumunu yaptƒ±ƒüƒ±mƒ±z sunucu √ºzerinde kod derleme ve kontrol etme a√ßƒ±sƒ±ndan Visual Studio Code ve Git uygulamalarƒ±nƒ± kurarak ba≈ülƒ±yorum.
Uygulamalarƒ± kurduktan sonra bir pipeline olu≈üturmak istediƒüimde , bu pipeline i√ßin hangi repositoryde √ßalƒ±≈ümam gerektiƒüini se√ßtiƒüim bir ekran ile kar≈üƒ±la≈üƒ±yorum.
Gerekli bilgileri doldurup , artƒ±k nasƒ±l bir i≈ülemler zinciri olu≈üturmam gerektiƒüine karar verip adƒ±mlarƒ± uygulayabildiƒüim aray√ºz ile i≈ülemlerime ba≈ülƒ±yorum. Burada kendi kurgunuzu olu≈üturabileceƒüiniz gibi devops server‚Äôƒ±n bizlere sunduƒüu hazƒ±r ≈üablonlarƒ± da kullanabiliriz. Ben uygulama kodu build edilirken nuget paketlerinin g√ºncellendiƒüi , build i≈üleminin ger√ßekle≈ütiƒüi , test senaryosunun i≈ülediƒüi ve build √ßƒ±ktƒ±sƒ±nƒ±n release i√ßin kullanƒ±labilecek bir uygulama olduƒüu hazƒ±r bir ≈üablon se√ßerek i≈üleme ba≈ülƒ±yorum.
Burada build i≈ülemlerimiz, birer pipeline √ºzerinde bulunan job ve i√ßlerindeki adƒ±mlarƒ± temsil eden tasklar sayesinde i≈ülemektedir. Her task bir adƒ±m gibi √ßalƒ±≈üarak yapƒ±lmasƒ± gereken i≈ülemleri kurgulamamƒ±zƒ± saƒülamaktadƒ±r.
Bizim senaryomuzda ≈üimdilik bir test i≈ülemi ger√ßekle≈ütirmeyeceƒüimiz i√ßin ben test adƒ±mlarƒ±nƒ± pasif konuma getirerek i≈ülemlere devam ediyorum.
Olu≈üturmu≈ü olduƒüumuz pipeline i√ßin i≈ülemleri bizim adƒ±mƒ±za y√ºr√ºtecek bir agent kurulumu ger√ßekle≈ütirmemiz gerekmektedir.ƒ∞lgili Proje ayarlarƒ±nda pipeline sekmesi altƒ±nda bulunan agent pool sekmesi ile nasƒ±l bir yol izlememiz gerektiƒüine ve bir agent isosu indirebileceƒüimiz linklere ula≈üabiliriz.
Buradaki adƒ±mlarƒ± sunucuya kurmu≈ü olduƒüumuz visual studio code terminal ekranƒ±nƒ± kullanarak ger√ßekle≈ütiriyoruz.
Tam bu kƒ±sƒ±mda bizden bir PAT (Personel Access Token) istiyor. Gerekli i≈ülemleri kullanƒ±cƒ± ayarlarƒ± men√ºs√º altƒ±nda bulunan personel access token sekmesinden olu≈üturmamƒ±z gerekmektedir.
Olu≈üturmu≈ü olduƒüumuz PAT ile i≈ülemlerimize kaldƒ±ƒüƒ±mƒ±z yerden devam ediyoruz.
Ve en sonunda bizim i√ßin t√ºm i≈ülemleri y√ºr√ºtmekle g√∂revli olan agentƒ±mƒ±zƒ± bir servis olarak ayaƒüa kaldƒ±rmƒ±≈ü bulunmaktayƒ±z.
Bu kƒ±sƒ±mda tasklarƒ±n i√ßinde bulunan build i≈ülemini ger√ßekle≈ütirebilmek adƒ±na sunucumuza Visual Studio 2017 Build tools kurulumu ger√ßekle≈ütirerek gerekli yetkinliƒüi pipeline i√ßerisinde bulunduruyoruz.
Bu build tool ile gelen √∂zelliklerin agent tarafƒ±ndan algƒ±lanmasƒ± i√ßin servislere (services.msc) ula≈üarak VSTS Agent Servisini tekrardan ba≈ülatma i≈ülemini yapmamƒ±z gerekmektedir.
T√ºm bu i≈ülemler tamamlandƒ±ktan sonra ilgili pipeline √ºzerinde build kurgumuzu bir kez elle tetikleyerek i≈ülemlerin doƒüru ve kriterlere uygun ≈üekilde tamamlandƒ±ƒüƒ±nƒ± kontrol ediyoruz.
Manuel tetikleme ile birlikte s√ºrecimiz belirlediƒüimiz ≈üekilde i≈üleyerek gerekli buildi tamamlamƒ±≈ü ve uygulama √ßƒ±ktƒ±sƒ±nƒ± bizler i√ßin belirlemi≈ü olduƒüumuz konuma olu≈üturmu≈ütur.
Artƒ±k build i≈üleminin sonucunda bizlere saƒülanan √ßƒ±ktƒ±ya artifacts sekmesnden ula≈üabiliriz.
Azure Pipeline kullanƒ±m a≈üamalarƒ± :https://1drv.ms/b/s!AskWoAU3NqUug_VXY8NBx1qScoOgHQ
Build mekanizmasƒ± ve s√ºrekli entegrasyon adƒ±mlarƒ±nƒ±n d√ºzg√ºn ≈üekilde i≈ülediƒüini manuel olarak test ettikten sonra asƒ±l kurgulamak istediƒüimiz mimari adƒ±na k√º√ß√ºk bir test senaryosu uygulamaya ba≈ülayabiliriz. √ñncelikli olarak Azure Boards kullanarak geli≈ütirmekte olduƒüumuz yazƒ±lƒ±m i√ßin bir feature isteƒüi gelmi≈ü gibi bir g√∂rev olu≈üturalƒ±m .
Bu istek adƒ±na ekimizdeki g√∂revi alan geli≈ütirici i≈ülemi development sekmesinden √ºzerine alƒ±r.
Gerekli deƒüi≈üiklikler i√ßin bir feature branch olu≈üturarak ilgili g√∂rev ile ili≈ükisini olu≈üturur.
Geli≈ütirici kod deƒüi≈üikliklerini yapar ve master branch √ºzerine merge etmek adƒ±na bir adet pull request olu≈üturur.
Pull request isteƒüi takƒ±m i√ßindeki ilgili sorumlu yada ba≈üka bir takƒ±m arkada≈üƒ± tarafƒ±ndan kontrolden ge√ßirilerek merge i√ßin izin verilir.Bu senaryoda tek ba≈üƒ±ma bir i≈ülem ger√ßekle≈ütirdiƒüim i√ßin merge kontrol√ºn√ºde ben yapmak durumundayƒ±m.
Sonrasƒ±nda ise tamamlanan merge ile birlikte pipeline √ºzerindeki build kurgumuz devreye otomatik ≈üekilde girer.
Build i≈ülemi saƒülƒ±klƒ± ≈üekilde √ßalƒ±≈üarak gerekli adƒ±mlarƒ± tamamlamƒ±≈ü bulunmaktadƒ±r.
Gerekli i≈ülemler tamamlandƒ±ktan sonra tekrardan board √ºzerinde g√∂reve tƒ±klayarak artƒ±k t√ºm geli≈ütirme s√ºrecini takip etmemiz m√ºmk√ºnd√ºr.
Test Senaryosu a≈üamalarƒ± :https://1drv.ms/b/s!AskWoAU3NqUug_VV55n70XsZWwCu2A
Devops s√ºre√ßlerinde sƒ±klƒ±kla kullanƒ±lan g√∂rev takibi , yazƒ±lƒ±m ya≈üam d√∂ng√ºs√º izlenebilirliƒüi ve versiyon kontrol sistemlerini bir arada kurgulayabildiƒüimiz bir yazƒ±mƒ±zƒ±n sonuna daha geldik. Ba≈üka bir yazƒ±da g√∂r√º≈ümek dileƒüi ile.
‚Äú √áevik s√ºre√ßler s√ºrd√ºr√ºlebilir geli≈ütirmeyi te≈üvik etmektedir.Sponsorlar, yazƒ±lƒ±mcƒ±lar ve kullanƒ±cƒ±lar sabit tempoyu s√ºrekli devam ettirebilmelidir.‚Äù ‚Äî Agile Manifesto Madde 8
AWS Certified Users Group Turkey experience sharing page.
41 
41¬†claps
41 
Written by
Muzur bir oƒülan babasƒ±, hayvan sever, Harry Potter hayranƒ±, bazen maceracƒ± d√ºz yazƒ±lƒ±mcƒ±.
AWS Certified User Group dedicated certification training for cloud services provided by Amazon. In this group AAI‚Äò s and certified pro‚Äôs are going to share articles about training and certification on AWS, useful tips for AWS services, news and features announced from AWS.
Written by
Muzur bir oƒülan babasƒ±, hayvan sever, Harry Potter hayranƒ±, bazen maceracƒ± d√ºz yazƒ±lƒ±mcƒ±.
AWS Certified User Group dedicated certification training for cloud services provided by Amazon. In this group AAI‚Äò s and certified pro‚Äôs are going to share articles about training and certification on AWS, useful tips for AWS services, news and features announced from AWS.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/azure-machine-learning-deployment-workflow-475fc939e96?source=search_post---------154,"There are currently no responses for this story.
Be the first to respond.
This blog post is co-authored by Jaya Mathew and Francesca Lazzeri, data scientists at Microsoft.
The Artificial Intelligence Conference in London is a relatively addition to the list of conferences hosted by O‚ÄôReilly worldwide. The aim of this conference is to create a forum for the ever-growing AI community to explore the most essential issues and innovations in applied AI. In the conference the various talks covered topics ranging from practical business applications of AI, to compelling AI enabled use cases, to various technical trainings and deep dive into successful AI projects etc.
In our session ‚ÄúA day in the life of a data scientist in an AI company‚Äù, we presented a scientific framework to help organizations to systematically discover opportunities to create value from data, qualify new opportunities and assess their fit and potential, then how to build a team to smoothly implement end-to-end advanced analytics pilots and projects, and produce sustainable ongoing business value from data. Specifically we shared a few important concepts, such as the Machine Learning Workflow and the Team Workspace.
In the session, we also introduced the audience to Azure AI‚Äôs latest offering Azure Machine Learning Service. Azure Machine Learning Service (Preview) is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning Service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.
The Machine Learning workflow is an agile, iterative data science framework to deliver predictive analytics solutions and intelligent applications efficiently. It helps improve team collaboration and learning. It contains a distillation of the best practices and structures that facilitate the successful implementation of data science initiatives.
The goal is to help companies fully realize the benefits of their analytics program. The life cycle outlines the major stages that projects typically execute, often iteratively:
The following diagram provides a view of the tasks (in blue) associated with each stage of the life cycle:
In Azure Machine Learning Service, the workspace represents a central location for a team to collaborate and it manages access to compute targets, data storage, models created, docker images created, webservices deployed and it keeps track of all the experiment runs that were performed with it. Data scientists can manage the authorization and creation of workspaces and experiment from the Python SDK.
You can use Python to get started with Azure Machine Learning. In the snippet above, we are creating a workspace called ‚ÄúDemo‚Äù in the resource group ‚ÄúContoso‚Äù which resides in the given subscription. The workspace will be created in the Azure region ‚ÄúeastUS2‚Äù.
You can create multiple workspaces, and each workspace can be shared by multiple people. When sharing a workspace, control access to the workspace by assigning the following roles to users:
When you create a new workspace, it automatically creates several Azure resources that are used by the workspace:
With Azure Machine Learning Service, once the data scientist builds a satisfactory model, the trained model can be easily put into production and monitored.
The following diagram illustrates the complete deployment workflow:
In the next few paragraphs, we will show how to perform the following steps:
In this second step, you need to create your scoring script, your environment file and your configuration file.
You can deploy registered images into the cloud or to edge devices. here we deploy it to Azure Container Instances, that offers a simple way to run a container in Azure, without having to provision any virtual machines and without having to adopt a higher-level service.
In this blog post, we outline the various aspects that need to be addressed from data collection to metrics for a successful AI model to be used in a production environment. In particular, we introduced the audience to Azure‚Äôs latest cloud analytics environment that makes it easy to collect data, analyze, experiment, and build a model for any organization to use. In the last part, we showed how to use Azure Machine Learning Service to deploy your models to to Azure Container Instances.
Any language.
111 
2
111¬†claps
111 
2
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/visual-studio-code-the-swiss-army-knife-for-threat-hunting-with-azure-sentinel-503e7ef38c96?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Sep 30, 2019¬∑6 min read
Jupyter is a great platform for threat hunting where you can work with data in-context and natively connect to Azure Sentinel using Kqlmagic, but adding Visual Studio Code to the mix will give you even more superpowers!
When working with Visual Studio Code and Jupyter you get intellisense, debugging, a variable and data explorer, and live sharing; making the life of security analysts a bit easier. In this blog I‚Äôll show you how.
Threat Hunting with python, Jupyter and Kusto
Jupyter Notebook, formerly called IPython, is an open-source application that allows you to create and share documents that contain live code, equations, visualizations and narrative text through markdown. It is already broadly used in cybersecurity for threat hunting, and has support for lots of programming languages such as R, Python, etc. The multi-user version of Jupyter is called JupyterHub.
Microsoft created a magic extension for Jupyter called Kqlmagic that allows you to work with Kusto-based workspaces such as Log Analytics, Azure Security Center, Azure Sentinel and more from a Jupyter notebook using KQL (Kusto Query language).
PRO TIP: I‚Äôve written a blog earlier this year that helps you get started and provides a step-by-step tutorial to connect to Azure Security Center and Azure Sentinel. Read it here.
How do I get Jupyter up and running?
Microsoft‚Äôs offers a free hosted service called Azure Notebooks to develop and run Jupyter notebooks in the cloud with no installation. Although it is a free service, each project is limited to 4 Gb of memory and 1 Gb data. However, if the Azure Active Directory account you sign in with is associated with an Azure subscription, you can connect a Data Science Virtual Machine (DSVM) instances. DSVM‚Äôs are available from the Azure Marketplace and provide you with better processing power and removes any of those limits.
If you want to run Jupyter locally, you can install it using pip. However, I strongly recommend installing Python and Jupyter using the Anaconda distribution, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.
For even more convenience, Jupyter is available as a Docker image. The Jupyter team maintains a set of Docker image definitions in the https://github.com/jupyter/docker-stacks GitHub repository. Not sure which Docker image to choose? Here‚Äôs documentation describing them, and their images and relationships. My suggestion: use scipy-notebook.
PRO TIP: Use this command to load a Docker-based Jupyter container on port 8888: docker run -p 8888:8888 ‚Äî name jupyter jupyter/scipy-notebook
Why use Visual Studio Code?
Jupyter is a great platform for threat hunting where you can work with data in-context and natively connect to security backends in Microsoft Azure using Kqlmagic, but adding Visual Studio Code to the mix will give you even more superpowers!
Microsoft released a Python extension for Visual Studio Code in the extension marketplace that now supports Jupyter and works both on Windows and MacOS. If you are working on Windows and want an isolated environment for working with Python, the Windows Subsystem for Linux (WSL) is a great option. You can enable WSL and install a Linux distribution on your Windows machine, completely isolated from your normal development environment and use the VS Code Remote ‚Äî WSL extension.
When working with Visual Studio Code and Jupyter you get:
A familiar interface
You might already be using Visual Studio Code for other things, and can just add this to the list of great things to do with VSCode :-)
IntelliSense
The Python Interactive window has full IntelliSense ‚Äî code completions, member lists, quick info for methods, and parameter hints. You can be just as productive typing in the Python Interactive window as you are in the code editor.
Live Sharing
The Python Interactive window also supports Visual Studio Live Share for real-time collaboration. Live Share lets you co-edit and co-debug while sharing code, terminal, comments and more.
Variable Explorer and Data Viewer
Within the Python Interactive window, it‚Äôs possible to view, inspect, and filter the variables within your current Jupyter session. By expanding the Variables section after running code and cells, you‚Äôll see a list of the current variables, which will automatically update as variables are used in code.
Debugger
The Visual Studio Code debugger lets you step through your code, set breakpoints, examine state, and analyze problems. Using the debugger is a helpful way to find and correct issues in notebook code. Open the command palette (Ctrl+Shift+P) and run the Python: Debug Current File in Python Interactive Window command.
Show me the money!
To work with Jupyter notebooks, you must activate an Anaconda environment in VS Code, or another Python environment in which you‚Äôve installed the Jupyter package. To select an environment, use the Python: Select Interpreter command from the Command Palette (Ctrl+Shift+P).
To connect to the Jupyter server running in the Docker container:
PRO TIP: You define Jupyter-like code cells within Python code using a #%% comment.
Run Cell applies to only the one code cell. Run Below, which appears on the first cell, runs all the code in the file. Run Above applies to all the code cells up to, but not including, the cell with the adornment. You would use Run Above, for example, to initialize the state of the runtime environment before running that specific cell. Selecting a command starts Jupyter (if necessary, which might take a minute), then runs the appropriate cell in the Python Interactive window:
When you‚Äôve activated an environment with Jupyter installed, you can import a Jupyter notebook file (.ipynb) in VS Code as Python code. Once you‚Äôve imported the file, you can run the code as you would with any other Python file and also use the VS Code debugger. When you open a notebook file, the Python extension prompts you to import the notebook as a Python code file:
To export content from VS Code to a Jupyter notebook (with the .ipynb extension), open the command palette (Ctrl+Shift+P) and select Python: Export Current Python File as Jupyter Notebook. It creates a Jupyter notebook from the contents of the current file, using the #%% and #%% [markdown] delimiters to specify their respective cell types.
PRO TIP: You define Jupyter markdown text cells within Python code using the #%% [markdown] comment.
Conclusion
While Jupyter, together with Azure Sentinel, already provides a powerful combination for threat hunting, adding Visual Studio to the mix supercharges it even more. Intellisense, debugging, live sharing and the variable viewer are very useful if you‚Äôre a security analyst working with notebooks.
Happy hunting!
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
42 
2
42¬†claps
42 
2
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/create-a-private-ethereum-consorium-blockchain-in-azure-3667185276b5?source=search_post---------157,"In this tutorial, I demonstrate how to create a private Ethereum Consortium Blockchain network in Azure using one of the Azure Marketplace templates.
To begin the process, login to Azure portal and click on the ‚Äú+‚Äù icon on top left corner.
"
https://medium.com/@renatogroffe/azure-devops-guia-de-refer%C3%AAncia-gratuito-bac4fba3ff5?source=search_post---------158,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 4, 2020¬∑2 min read
Neste post agrupo os conte√∫dos gratuitos que venho produzindo sobre Azure DevOps, o que inclui artigos e v√≠deos (em alguns casos de eventos/lives que ajudei a organizar) sobre o uso desta plataforma no build e deployment automatizado de aplica√ß√µes.
Voc√™s poder√£o encontrar aqui materiais cobrindo o uso de tecnologias como .NET Core/ASP.NET Core, Azure App Service/Web App for Containers, Docker, Kubernetes, Azure Container Registry e Azure Kubernetes Service (AKS). Meu intuito √© manter este guia sempre que poss√≠vel atualizado, adicionando ao mesmo novos conte√∫dos gratuitos que vier a disponibilizar.
E para aqueles interessados em se aprofundar ainda mais nos recursos do Azure DevOps, deixo listados aqui os seguintes canais do YouTube:
Docker + Azure DevOps: build e deployment automatizado de aplica√ß√µes
Kubernetes + Azure DevOps: build e deployment automatizado de aplica√ß√µes
Azure DevOps Pipeline + Azure Container Registry + Azure Web App for Containers + Environment Variable
Azure DevOps + Azure Container Registry + Azure Kubernetes Service
Azure DevOps Pipeline + Azure Container Registry + Azure Web App for Containers
Docker - Guia de Refer√™ncia Gratuito
Kubernetes - Guia de Refer√™ncia Gratuito
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
42 
42¬†claps
42 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/a-new-look-at-azure-durable-functions-aa43f37955ff?source=search_post---------159,"There are currently no responses for this story.
Be the first to respond.
Zone‚Äôs principal architect, Andy Butland examines how the durable functions framework has matured over the past couple of years‚Ä¶
A couple of years ago I had opportunity to speak, write and work with what was then a new serverless Azure technology, Durable Functions, based on top of the Azure Functions framework for the purposes of handling long-running or multi-stage tasks. As part of investigating the technology I built a couple of sample applications using both durable functions and the standard functions framework by itself, and compared and contrasted the‚Ä¶
"
https://medium.com/@bankexcom/bankex-protocol-a-trusted-instrument-of-crowdfunding-d4226aa75750?source=search_post---------160,"Sign in
There are currently no responses for this story.
Be the first to respond.
BANKEX
Jul 24, 2017¬∑2 min read
On the 27th of March 2017, BANKEX team got $120.000 from Microsoft Azure. BANKEX team is grateful for the trust displayed to us.
Azure Active Directory is a IaaS (identity-as-a-service) which currently provides and manages access of BANKEX team to services that are currently available on Microsoft Azure. Storage Account is a durable, robust and scalable storage solution which allows us to store all the essences that are deployed within our solution.
As an open, flexible, and scalable platform, Azure supports a rapidly growing number of distributed ledger technologies that address specific business and technical requirements for security, performance, and operational processes.
In the case of BANKEX, we use the five most potent services provided by Microsoft Azure:
- Azure Active Directory
- App Service Plan
- App Service
- Applications Insights
- Storage Account
Microsoft Azure allows us to firstly deploy our smart contracts on the TestNet and then once all the tests are successful it can be then deployed on the Ethereum production net.
Microsoft Azure Setup Overview
BANKEX is available at:
Website: https://bankex.com/Telegram: https://t.me/bankexFacebook: https://www.facebook.com/BankExchange/GitHub: https://github.com/BankExSlack: http://bit.ly/slack-bankex
Reddit: https://www.reddit.com/r/bankex/
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
200 
200¬†
200 
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
"
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-152edbcc1f2c?source=search_post---------161,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 3, 2020¬∑1 min read
Para aqueles que colocaram como meta aprender algo novo na area de programa√ß√£o em 2020, eu estou disponibilizando alguns m√≥dulos do meu curso: Criando API‚Äôs RESTful utilizando TypeScript, Node e mongoDB.
Segue abaixo as primeiras video aulas liberadas :)
Introdu√ß√£o ao curso:
Ambiente de desenvolvimento:
Instala√ß√£o do Node.js
Instala√ß√£o do TypeScript
Aproveitando, segue link para download das ferramentas que n√≥s iremos utilizar nesse curso abaixo:
Espero que gostem :)
Enjoy your life
131 
131¬†
131 
Aqui voc√™ vai encontrar os principais conte√∫dos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/awesome-azure/difference-between-scale-set-and-availability-set-in-azure-9b2da03b891c?source=search_post---------162,"There are currently no responses for this story.
Be the first to respond.
Comparison: Azure VM Scale Sets vs Availability Sets in Azure.
Availability Set consists of a set of discrete VMs.VM Scale Set consists of a set of identically configured VMs.
Availability Set consists of a set of discrete VMs which have their own names and individual‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-free-microsoft-azure-fundamentals-az-900-online-courses-for-beginners-in-2021-efd01d8be403?source=search_post---------163,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for the AZ-900 certification or Azure Fundamentals exam and looking for free online courses to start your preparation then you have come to the right place.
In the past, I have shared both free and paid courses to learn AWS, Azure, and Google Cloud as well as the best AZ-900 courses and practice tests, and today, I am going to share the free AZ-900 courses to pass the Azure Fundamentals exam.
If you are thinking to learn Cloud Computing and Microsoft Azure platform then this is the best cloud certification to start with.
Microsoft is among the top cloud service providers; it is popular for its reliability and security. Besides this, the cloud industry is the most rapidly growing industry of this era, and in the coming years, it will join the league of top industries.
To lead a successful career as a cloud engineer, you must have certifications of expertise from the big players of cloud computing. If you have plans to appear in certification exams like Azure Fundamentals AZ-900 then congratulations, you made a great decision. However, if you are still wondering about the certificates, then you must hurry, time is a limited entity.
In this guide, I‚Äôll share seven free courses that will help you pass the Azure Fundamentals AZ-900 exam with flying colors. All the below-listed courses are created by the industry experts, and each of them is picked after hours of research. None of the courses will disappoint you.
This particular certification exam is designed specially to test the fundamental knowledge of the candidates. Therefore even if you are a beginner and have no prior knowledge, you can clear this exam, but the only catch here is that you need to pursue each of the courses thoroughly.
And, if you don‚Äôt mind spending few bucks to acquire a useful skill like Azure and Cloud Computing and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out the AZ-900: Microsoft Azure Fundamentals Exam Prep course by Scott Duffy on Udemy. It‚Äôs one of the best online courses to prepare for the AZ-900 certification exam in 2021.
udemy.com
Here is a list of the best free online AZ-900 courses to become a certified Azure practitioner. You can join these courses to pass the Azure Fundamentals certification exam in the very first attempt.
These online courses have been created by experts and trusted by thousands of developers. You can also join them to learn online and become a certified Azure professional. Remember, they are absolutely free and you don‚Äôt need to pay for anything.
If you are running short on time and need something that can help you prepare for the Azure Fundamental AZ-900 Exam in a limited time, then this course is for you. It is a 2-hour long video course available on Udemy created by Maruti Makwana, and so far over 6 thousand students have enrolled in this course.
In this course, you‚Äôll learn about the basics of the Azure platform and how you can use it for your day to day projects. Besides this in this course, the instructor will give a brief introduction to other Azure concepts like the Security and Deployment of Apps.  Graphics and illustrations are smartly combined with each other to make the understanding process easy for the students, apart from this, the instructor will also perform some live implementations and ask the students to follow his steps.
If you are looking for a course that can teach you all the concepts with practical implementation, then this course is a perfect pick for you.
Here is the link to join this free Azure course ‚Äî AZ-900 Microsoft Azure Fundamentals
This is another free course that is great to learn essential Microsoft Azure concepts and perfect for anyone preparing for the AZ-900 exam and cloud computing.
The course begins with fundamental concepts such as, what is the cloud? From here it focuses on explaining little by little how you can work on azure etc. This course is strictly visual and conceptual. You‚Äôll get the conceptual knowledge of the cloud and its services which would help you in further learning of advanced features, which is important to pass the AZ-900 certification exam.   Here are the key things you will learn in this course:
This free online Azure course works wonders by providing you frame-of-reference learning which is highly effective to solve complex problems in advanced azure learning.
Here is the link to join this free AZ-900 course ‚Äî Microsoft Azure Concepts
Also, it provides bonus features and access cards to a hands-on lab to enhance learning. The course is regularly updated so all your queries will be answered.
If you are interested in a practical approach, then this course is for you. It is available on the Pluralsight website, though it is a paid platform but using the 10-day trial option, and you can quickly complete this course.
It is created by Matt Milner, and through this course, the instructor will introduce you to the advantages, limitations, and restrictions of cloud computing, and later in the timeline, you‚Äôll be taught about various aspects of Azure Security and SQL.
Every second of this course is informative, and the instructors have used the graphics quite intelligently. Through these graphics, even challenging concepts seem relatively easy.
Here is the link to join this course ‚Äî Microsoft Azure Fundamentals
Therefore if you are looking for an easy to learn and practical course, then this course is for you. By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount).
I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-trial to watch this course for FREE.
pluralsight.pxf.io
If you have an adequate amount of time and can attend in-depth explanatory videos, then this cloudacademy.com course is for you. In this 6-hour long video course, you‚Äôll learn about every element that is linked with the Azure platform.
This course is created by Guy Hummel; he is an Azure expert. So far, he has created 60 different courses regarding cloud computing and taught over 54 thousand students. He has teaching experience of more than 25 years, thus understanding the concepts won‚Äôt be a problem for you.
One of the fascinating features of this course is that it has special lectures from other industry experts, and the instructor is regularly updating the course content. Therefore, you‚Äôll get the latest information about the Azure platform.
This is another quality free resource to prepare for Microsoft Azure Fundamentals certifications. Created by Andrew Brown, a leading AWS and Cloud expert, this 3-hour course is perfect to prepare for Azure Fundamentals certification in quick time.
The Azure Fundamentals exam is an opportunity to prove knowledge of cloud concepts, core Azure services, Azure pricing, SLA, and lifecycle, and the fundamentals of cloud security, privacy, compliance, and trust. You will learn about all these things in this course.
Here is the youtube link to watch this course for free:-
If you are looking for a course that covers a wide range of topics related to the Azure Fundamental AZ-900 exam in a very limited time without compromising with the information, then this course is for you.
It is available on Udemy, and over seven hundred students have enrolled in this course. It is among the latest courses uploaded on Udemy but its instructor ‚ÄúXaas Technologies‚Äù has a long history with cloud computing.
So far ‚ÄúXaas Technologies‚Äù have taught more than 83 thousand students through their Udemy courses. This is fast-paced, and to the point course that deals with the following concepts of Azure:
All these concepts are taught in-depth using appropriate graphics and illustrations. You must enroll in this course if you need more output in less time.
Here is the link to join this free course ‚Äî AZ-900 Prep ‚Äî Azure Virtual Machines (VMs) Master Class
If you are just starting with Azure and want to learn everything in detail, then this Coursera course is a perfect pick for you. In this course, over 9 thousand students have enrolled so far, and it is rated as a top seller on Coursera.
It is a 20 hours long video course, and throughout this course, the instructor will assign different tasks to the students, and they‚Äôll only be promoted to the next videos if they complete those tasks on time. Though the tasks have flexible deadlines, you can‚Äôt skip them. In this course, you‚Äôll be taught about the followings;
Through the course, you‚Äôll have practical implementations of various concepts, and the rest will be taught with the help of simple yet engaging graphics.
Here is the link to join this Azure course ‚Äî Getting Started with Azure
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking a Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
It cost around $399/year but it‚Äôs completely worth your money as you get unlimited certificates.
click.linksynergy.com
That‚Äôs all about the free AZ-900 courses to prepare for Azure Fundamentals certification. All the above-listed courses are designed for the beginner, and gradually throughout the instructor, enhance the level.
If you are just starting with the preparation for the Azure Fundamental AZ-900 exam, then you must enroll in all the courses and attend at least a few of the videos, and then decide which one is more understandable for you. If you come across some courses that aren‚Äôt listed here but deliver quality information, then do let me know.
Other Azure Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure courses useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P.S. ‚Äî If you want to pass the Microsoft Azure platform in the first attempt and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out the AZ-900: Microsoft Azure Fundamentals Exam Prep course on Udemy. It‚Äôs the best online course to prepare for the AZ-900 certification exam in 2021.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
128 
128¬†claps
128 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@gmusumeci/getting-started-with-terraform-and-microsoft-azure-a2fcb690eb67?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 12, 2020¬∑13 min read
In this post, we will explore the basics (and a few tips of many years of experience) to configure your development environment to start writing Terraform code for Microsoft Azure.
Installing Terraform is very simple. We just need to download the file from https://www.terraform.io/downloads.html and extract the‚Ä¶
"
https://medium.com/javarevisited/how-to-prepare-for-microsoft-azure-fundamentals-certification-az-900-exam-in-2021-4a258b3006a?source=search_post---------165,"There are currently no responses for this story.
Be the first to respond.
Hello there, if you are aiming for Microsoft Azure Fundamentals certification in 2021 but not sure how to prepare for it then you have come to the right place. Earlier, I have shared the best Azure courses, and today, I am going to share a complete guide to prepare for the AZ-900 certification exam, with links to books, tutorials, guides, whitepapers, and online courses.
Cloud computing skills are in demand and companies are increasingly looking for people who know and worked in public cloud platforms like AWS, GCP, and Microsoft Azure.
If you are looking to get started in Cloud Computing, particularly on the Microsoft Azure side then AZ-900 or Azure Fundamentals certification is probably the best way to start.
You will not only learn about Azure but also learn essential Cloud Computing fundamentals like storage, network, compute, and memory among all things.
You will also learn about things like IaaS (Infrastructure as a Service), PaaS (Platform as a Service) and SaaS (Software as a Service), Sales and Pricing, which are very important for both technical and non-technical IT professionals.
Earlier, I have shared a few tips, courses, and practice tests to pass the AZ-900 or Microsoft Azure Fundamentals exam and today, I‚Äôll talk about how to crack the Microsoft Azure Fundamental certification (AZ-900) in 2021. But, before that, let‚Äôs first understand what is Microsoft Fundamentals or AZ-900 certification exam? Microsoft has established a certification in Azure‚Äôs new role-based certification path, specifically ‚ÄúMicrosoft Certified Azure Fundamentals‚Äù.
This exam is intended for people with non-technical training, as well as technical people who wish to test their basic knowledge of Azure cloud services. To obtain this certification, it is compulsory to pass the AZ-900 exam.
Now that you know What is Microsoft Azure Fundamental certification and why should you get certified for it, it‚Äôs time to make a plan to succeed in this prestigious exam.
Here is your step by step guide to crack Microsoft‚Äôs new AZ-900 or Azure Fundamentals certification in 2021:
Azure AZ-900 is a light exam compared to many other Microsoft role-based exams. This exam does not require experienced cloud professionals or programming assistants.
If you are an individual involved in buying and selling cloud services, this review will benefit you even if it comes from a non-technical experience. In addition, this exam will be useful for people who want to validate their basic knowledge of cloud services or solutions. Again, it will be helpful if the exam candidate has general computer knowledge or experience before starting formal preparation for the AZ-900 exam. Let‚Äôs say we want to pursue a career in one of Azure‚Äôs key roles, including developer, solution architect, administrator, etc. and you don‚Äôt know where to start your journey. Here come this exam and certification. This exam can confirm your mastery of the Azure basics and make you competent enough to undertake future Azure certification activities. This warranty may reflect the results of the test when other preparations for the Microsoft Azure certification exam are also taken.
We recommend that you know the basic details of this exam before you begin preparing for the AZ-900 exam. Microsoft has a defined policy regarding exams.
Here, each type of exam has specific details depending on the role or function. There will be several details for Microsoft MCSA exams, role-based exams, etc. However, let‚Äôs see how these details apply to our AZ-900 exam.
As with most Azure exams, you can expect 40 to 60 questions on an AZ-900 exam and you will get 85 minutes.
And there will be a different question format like a case study, a short answer, repeated answer options, a build list, a hotspot, a multiple-choice, a brand review, a review screen, an active screen, better answer, drag, and drop, etc. during your exam
You can answer any number of exam questions. There will be no penalty for giving incorrect answers. Just don‚Äôt check the wrong answers, that‚Äôs all.
It is mandatory to check the exam price before starting preparation for the AZ-900 exam. These exam fees are subject to change depending on where the exam is taken. If you live in the United States, the exam will cost $ 99.
You will benefit from a reduced rate if you are a member of the Microsoft Imagine Academy program, a member of the Microsoft Partner Network program, or a Microsoft Certified Trainer. Students are also entitled to a fee reduction.
You have to get a total of 700 points to pass this exam. The candidate who receives votes below this number will be considered bankrupt. You can find out the results of your exam a few minutes after the end of the exam. However, to get a detailed dashboard, you have to wait a few days.
The dashboard can contain many details, including general exam performance, pass / fail status, a bar chart showing performance in key areas of the exam, and instructions on how to interpret. The results of your exam.
Many candidates expressed doubts about the exam repetition policies before starting preparation for the AZ-900 exam or during preparation. If you are trying this exam and cannot pass it for the first time, you must wait at least 24 hours before repeating the exam.
If this happens to you a second time, the waiting time for the next exam can be increased to 14 days. This way you can take a maximum of 5 reps in a year.
If you cancel or reschedule an appointment at least 6 working days before the exam, the cancellation will not be invoiced. If the cancellation/rescheduling takes place within 5 working days, you will be charged a small fee.
However, if you do not reschedule/cancel the appointment within 24 hours or if you cannot participate in the exam, all exam fees will be forfeited.
It is very important to prepare well if you want to take the AZ-900 certification exam. Since the exam is new, you need to follow the correct preparation procedure to complete the exam the first time. Below is the complete guide to preparing for the AZ-900 exam.
This portal serves as a command center for all Microsoft exams and certifications. You can find a list of all Microsoft certifications on the Microsoft Learning page.
Without any difficulty, you can also find the AZ-900 page on this portal. Frankly, this portal should be the starting point for preparing for the AZ-900 exam. This is because you can find everything you need to know about this exam on this portal, including the option to register for the exam, details on AZ-900 exam modules, links to the AZ-900 Microsoft Azure Fundamentals study materials, exam prerequisites, exam group study, exam links, exam policies, and structure, etc. On top of all that, this is one of the few places where you can get important updates on the exam schedule, module changes, price changes, etc. You can find links to schedules available only on this portal for the AZ-900 Exam, it is almost certain that there is no way that this portal will go unnoticed while preparing for the exam AZ-900.
Online training courses play an important role in preparing for Azure Fundamentals or AZ-900 certification exam. While Microsoft also provides official training it's very expensive and there are many affordable options available to people who are preparing for Azure Fundmatnals or AZ 900 exam.
Here are some of the recommended course from Udemy and Pluralsight to prepare for Azure Fundamentals or AZ-900 certification exam:
This is one of the most popular Udemy courses to prepare for AZ-900 or Azure Fundamentals exam. Created by Scott Duffy, this course is updated to cover the latest exam curriculam. The course also includes a practice test for better preparation.
udemy.com
2. AZ-900 Exam-Prep: Microsoft Azure Fundamentals (JAN 2021)
This one is another awesome course from Udemy which I highly recommend for developers going for Azure Fundamentals or AZ 900 exam. This course is not just up-to-date and covers all exam topics it also comes with Interactive Labs, Flash Cards, and a 50-Question Practice Exam.
udemyy.com
3. Microsoft Azure Fundamentals (AZ-900) Path on Pluralsight
If you are a Pluralsight member then you should be happy that Pluralsight has a complete Path and array of courses to prepare for the Azure Fudnemtnals certification exam in 2021.
This is an excellent starting point for those looking to get started in working with cloud services and solutions in the Azure space.
pluralsight.pxf.io
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount). I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
If you have a reliable Azure book, your impact would be very high in preparing for the AZ-900 exam. But unfortunately, most of the Azure books available do not meet the expectations of the necessary readers or meet industry standards.  However, we have shared some of the Azure books which can be very helpful in preparing for the AZ-900 exam.
Practice with Simulator is the best way to analyze your skills and abilities in a particular area. You can also find many websites during the AZ-900 exam preparation that claim they can provide the best Simulator for the AZ-900 exam. Never trust these websites because they don‚Äôt provide a real exam environment and valid questions. Sometimes using these simulators can even result in the main exam being failed. Therefore, always keep these difficulties in mind when preparing for the AZ-900 exam. I highly recommend David Mayer‚Äôs Microsoft Azure Fundamentals AZ-900 Simulator because they are providing a real exam environment and valid questions.
You can also try with these free 10 questions and then purchase a premium version to get ready for the exam and check your preparation level.
If you need more options, Here are some more AZ-900 Practice questions from Udemy and Whizlabs.
Three complete timed practice tests for AZ-900 Azure Fundamentals exam, 150 questions, 100% original material
udemy.com
2. AZ-900 Practice Tests | Microsoft Azure Fundamental | Jan 21
This practice exam is completely up-to-date with new requirements. New questions added for the latest changes to the exam. It contains 6 full-length practice tests to build speed and accuracy.
udemyy.com
3. Microsoft Azure Exam AZ-900 Certification
This is one of my favorite practice tests. It‚Äôs high quality and provides in-depth explanations. It contains 6 Full-Length Mock Exams (325 Unique Questions) and 7 Section Tests (55 unique questions). You also get reports to assess strengths & weaknesses.
Here is the link to join this practice test ‚Äî Microsoft Azure Exam AZ-900 Certification
This step can always be ignored when preparing for the AZ-900 exam. This is as important as related books or documents because these things are published by Microsoft itself.
From these sources, you can find many relevant Azure documents. Also, be sure to subscribe to Azure Notifications to help you stay up to date on the latest Azure updates.
azure.microsoft.com
That‚Äôs all about how to crack Microsoft Azure Fundamentals (AZ-900) Cloud Certification in 2021. If you want to start working with Microsoft Azure or any other cloud platform then this is a really good certification to get started.
You will not only learn about Microsoft Azure Fundamenta but also Cloud Computing fundamentals which apply to all kinds of cloud providers like AWS, GCP, and Microsoft Azure.   Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like these AZ-900 dumps or practice tests, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. ‚Äî If you are new to Azure Cloud Platform and looking for free online training courses to learn Azure basics and prepare for Azure Fundamentals certification then you can also see Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It‚Äôs completely free and all you need is a free Udemya account to join this course online.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
217 
1
217¬†claps
217 
1
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/microsoftazure/10-azure-ml-code-examples-every-cloud-ai-developer-should-know-a2899bf2f8bd?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
TLDR; The Azure ML Python SDK enables Data scientists, AI engineers,and MLOps developers to be productive in the cloud. This post highlights 10 examples every cloud AI developer should know, to be successful with Azure ML.
If you are new to Azure you can get a free subscription using the link below.
azure.microsoft.com
The scripts in this example are used to classify iris flower images to build a machine learning model based on scikit-learn‚Äôs iris dataset the code can easily be adapted to any scikit-learn estimator.
Code:
docs.microsoft.com
This example shows you how to run your TensorFlow training scripts at scale using Azure Machine Learning‚Äôs TensorFlow estimator class. This example trains and registers a TensorFlow model to classify handwritten digits using a deep neural network (DNN) and MNIST but can be scaled to other more complex models.
Code:
docs.microsoft.com
PyTorch the example scripts in this article are used to classify chicken and turkey images to build a deep learning neural network based on PyTorch‚Äôs transfer learning tutorial and can be adapted to more complex projects.
Code:
docs.microsoft.com
This example demonstrates how to deploy a production model on Azure Kubernetes Service (AKS). AKS is good for high-scale production deployments. Use AKS if you need one or more of the following capabilities:
Code:
docs.microsoft.com
Data in the wild is dynamic, this example walks through how to monitor changes in you data distribution and model performance over it‚Äôs production lifespan so that you can be alerted and update it more readily. ‚Äò
Code:
docs.microsoft.com
This code example walks through using BERT model for question and answering in an end to end pipeline on the AzureML platform. From how to fine tune it from scratch using the distributed training with Horovod and how to optimize model performance with Azure ML Hyper Drive.
Code:
github.com
This example shows how to use Azure Machine Learning to run distributed training using Distributed Data Parallel in Pytorch for extractive summarization.
Code:
github.com
The automation of detecting anomalous events in videos is a challenging problem that currently attracts a lot of attention by researchers, but also has broad applications across industry verticals. This code example provides an an end to end template for creating Video Anomaly Detection service with Azure ML and AML Pipelines.
Code:
github.com
At its best, AI advances society through critical high-impact applications such as Heathcare, Security and Self Driving Cars. However at its worst AI can amplify existing societal biases with unintended consequences, such as ethnic, gender or racial discrimination. Model interpretability is a critical component of the Machine Learning Engineering process. This code example shows how to use the interpretability package of the Azure Machine Learning Python SDK to better understand why your model made its predictions.
docs.microsoft.com
For more information about Shapley Values one of the key interpretability measures check out my previous post on the topic.
medium.com
This End To End Notebook demonstrates how to train a custom estimator in Azure ML using the Intel NLP Architect Open Source Aspect Based Sentiment model. This model enables more granular insight into sentiment analysis as well contains best practices for configuring custom estimators from remote GitHub branches and custom environmental variable settings.
github.com
See also:
medium.com
Now that you have all the code you need to get started for your own production Azure ML project check out my previous posts on 9 Advanced Tips for Production Machine Learning and how Setting up AML Notebook VM.
medium.com
medium.com
Aaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
47 
47¬†claps
47 
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/getting-started-with-vuejs-nodejs-and-azure-cosmosdb-35dadbf2dd3c?source=search_post---------167,"Looking at the Frontend frameworks that showed up in the past few years, it seems like we are down to very few main frameworks that people are talking about.
I believe, for the enterprise solutions, it seems like the battle is between Angular and React. Of course there are other ones out there that have their community and market (like EmberJs, or BackboneJs, etc), but I‚Ä¶
"
https://medium.com/microsoftazure/the-power-of-azure-devops-projects-for-java-apps-cda6c69bef0e?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
If you are unfamiliar with Azure DevOps Projects, create right now an Azure Trial account to try this beauty. It is one of the coolest, most amazing features of Azure and many developers are missing out. #FOMO. Because what you really want is commit to production.
If you like this article, please clap for it down below or to the left side.
This is the ability of being able to commit a change to a project on Git or GitHub, and see automated continuous integration, testing, and deployment being triggered automatically.
This article covers exactly how to import, build, deploy, and test two Java applications, one written with Spring Boot Reactive, and another with Eclipse MicroProfile using Payara Micro as the runtime.
Your key take away? You can now scaffold Java projects on Azure extremely easily and fast, and independent from the Cloud.
Before diving in, quick pause.
As you can see above, I posted the content of this article originally as a series of tweets, and someone said I invited ""tutorials via Twitter"".
I guess I just invented the #Twitorial. :P ‚Ä¶ Moving on!
Assuming that you by now should already have an Azure Trial account to experiment this, let's dive right in!
Both projects are hosted on GitHub, and you can clone and run them locally just to see them in action before you push them to you Azure account:
Clone either project to your computer (or both). They have equal building and running instructions except for which Maven plugin to call, and it should be pretty easy to get done with this, as long you of course have the necessary tools installed (JDK 8, Maven 3+, Docker is optional)
This is a neat feature in recent versions of Docker and has helped me circumvent some limitations and deliver a developer experience that is equally awesome on your local computer as well on the CI/CD platform.
The basic springboot-hello-azure/Dockerfile contains the following instructions, and it is useful for building/testing the Docker image locally.
Dockerfile:
Dockerfile.cicd:
The second one above as stated earlier on, has the multi-state build surpport and configuration. This is the version we will use later in this article to import/create Docker image on Azure DevOps project wizard.
Go right now to the Azure Portal, and hit Create DevOps Project. If you don't see this tile on your dashboard, search for DevOps on the left menu inside ""All Services"". You must find ""DevOps Projects"". Once you open it, you should be able to Add a new project.
The steps below will walk you through the project creation wizard, so stay tuned as we check up to 8 steps.
Make sure you select Bring your own code
Here you have two options again: either go with MicroProfile or Spring Boot. Below the links to the Git repositories to facilitate your copy/paste.
We will be building and deploying using Docker, so doesn't matter the framework or platform, we will build a Docker image. Therefore, here you select Dockerized application.
Make sure you mark the option that says Web App for Containers.
This is a simple service that can spin up dockerized web applications with a load balancer in front and other perks for scaling. It costs less than Kubernetes but of course won't provide the entire feature set of Kubernetes.
I did not test this with AKS (Azure Kubernetes Service), but please feel free to try it out and post in the comments!
Since this is a Dockerized application, it simplifies things if we also use Docker to run the entire build cycle of this project. For that, we have Dockerfile.cicd for both projects which you can use to compile/package the Maven project, and then right away build a Docker image with the artifact produced by Maven.
This is exactly what we want.
The deployment will be in progress, so keep an eye and wait for everything to be complete.
Once the deployment is ready, here's the amazing dashboard you will have for your DevOps project, and all of its resources created for you automatically:
Your application is now running in the Cloud ‚Äî Microsoft Azure to be more precise! Visit http://<your-app>.azurewebsites.net/api/hello once deployment is done.
Try this now. And please let me know in the comments what you think. And don't forget to clap if you liked it :-)
Any language.
45 
45¬†claps
45 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Brazilian, Product and Program Manager for Java at Microsoft. Promoting great developer technologies to the world. Previously at Oracle.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/ursium-blog/slock-it-working-with-microsoft-to-bring-its-dapp-to-the-azure-cloud-c7a39720fdb3?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
Stephan Tual
Mar 15, 2016¬∑4 min read
Business analytics, data management, identity and messaging: meet Slock.it.
We‚Äôre pleased to announce that we‚Äôre working with Microsoft to bring the Slock.it ƒêapp to the Microsoft Azure Cloud, powered by Ethereum.
Microsoft has heavily invested in their cloud infrastructure, now leveraged by thousands of large corporate partners to deploy countless applications, ranging from machine learning to data warehousing ‚Äî and now, interacting with the decentralized sharing/collaborative economy.
For Slock.it UG, this means unprecedented exposure to ‚Äòmainstream IT‚Äô and new potential partners in our mission to grow a DAO ecosystem. For the DAO to which we will make a Proposal, it would means expanding its serviceable available market to include thousands of new channels across a variety of verticals it could only have dreamed of a few short weeks ago.
This will also make it very easy for developers to build ƒêapps for the Ethereum Computer, by simplifying retrieving data from existing services (e.g. Oracles) or triggering hosted applications and devices based on messages and payments made on the blockchain. We‚Äôll soon make available an Ubuntu core image bundling the Ethereum framework and the Slock.it Snappy ƒêapp for developers to build on. These ƒêapps can then be deployed without modifications to the Ethereum Computer, making the Azure cloud the perfect environment to experiment with Slock.it and smart objects such as the Samsung SmartThings line.
Some of you may wonder if there is added value of deploying a blockchain on top of a cloud service ‚Äî an infrastructure which is by definition centralized. The Ethereum ‚ÄúBlockchain as a Service‚Äù allows Microsoft customers and partners to test and integrate blockchains as part of their network easily, within an environment they can understand and control. It‚Äôs a sandbox, a playground. It might not make sense for the hacker who enjoys spending 3 days compiling the Dev branch on CAELinux. But if your name is GE, Pearson or 3M ‚Äî it‚Äôs the ideal way to get started ‚Äî with a one-click deployment.
We‚Äôre particularly excited about what this will mean for companies that are keen to explore Slock.it in order to decentralize access control, but are hesitant with regards to how this integration might impact existing services.
Think about a warehouse provider wanting to:
I didn‚Äôt choose this example at random ‚Äî this is paraphrased from a conversation with a company currently exploring our technology. Big chunks of their access control can be automated and decentralized with the Slock.it App, and now with Azure integration, it can be done as part of an incremental transition toward decentralization, eliminating the need to call an expensive integrator or spending months experimenting with costly PoCs.
For more information, please see the Microsoft Azure website, or join the discussion on our Slack channel at https://slock.it:3000
About the Author
Stephan Tual is the Founder and COO of Slock.it.
Previously CCO for the Ethereum project, Stephan has three startups under his belt and brings 20 years of enterprise IT experience to the Slock.it project. Before discovering the Blockchain, Stephan held CTO positions at leading data analytics companies in London with clients including VISA Europe and BP.
His current focus is on the intersection of blockchain technology and embedded hardware, where autonomous agents can transact as part of an optimal ‚ÄúEconomy of Things‚Äù.
Twitter: @stephantualContact: stephan@slock.it
If you enjoyed reading this, please log in and click ‚ÄúRecommend‚Äù below.This will help to share the story with others.
CEO at Genomics Startup 3BP. Former CCO Ethereum, Blockchain & Smart Contracts expert. Moonlights as Indy Film Producer.
See all (15)
28 
1
28¬†claps
28 
1
Epigenomics, Blockchain, Smart Contracts
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/mensageria-na-nuvem-com-rabbitmq-net-core-e-azure-functions-7c2a4f890448?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 2, 2020¬∑5 min read
Uma d√∫vida bastante recorrente entre aqueles que optam pelo Microsoft Azure para a implementa√ß√£o de solu√ß√µes na nuvem √© quanto ao suporte oferecido ao uso de RabbitMQ. Como trabalhar com esta que √© atualmente uma das solu√ß√µes de mensageria?
Temos algumas op√ß√µes para a cria√ß√£o de ambientes baseados no RabbitMQ no pr√≥prio Portal do Azure, como demonstrado na imagem a seguir:
Existe ainda a possibilidade de configurarmos em produ√ß√£o a utiliza√ß√£o do RabbitMQ em um cluster Kubernetes, com isto acontecendo por meio do AKS (Azure Kubernetes Service).
Mas e no que diz respeito √† implementa√ß√£o de aplica√ß√µes que processar√£o as mensagens enviadas a uma fila/queue do RabbitMQ?
At√© recentemente o melhor caminho para isto seria a implanta√ß√£o de uma aplica√ß√£o em uma m√°quina virtual ou, at√© mesmo, a execu√ß√£o desta em um container criado com o Azure Container Instances ou via cluster Kubernetes.
A grande novidade agora est√° no fato de que poderemos tamb√©m trabalhar com RabbitMQ a partir de aplica√ß√µes baseadas em Azure Functions, abrindo caminho assim para a implementa√ß√£o de solu√ß√µes em arquitetura serverless com um custo reduzido. Entranto, √© necess√°rio ressaltar que o package para .NET Core ainda se encontra em vers√£o Beta no momento da publica√ß√£o deste artigo (in√≠cio de Mar√ßo/2020).
O an√∫ncio sobre o suporte a RabbitMQ est√° descrito no seguinte post:
dev.to
Neste artigo trago um exemplo simples de uso de RabbitMQ em uma Function App, atrav√©s da implementa√ß√£o de um projeto Serverless utilizando .NET Core, Azure Functions 3.x e o Azure SQL/SQL Server na manipula√ß√£o de cota√ß√µes de moedas estrangeiras.
Para ficar por dentro das novidades do Azure Functions 3.x acesse o artigo a seguir:
.NET Core 3.x + Serverless: configura√ß√£o, dicas e exemplos com Azure Functions 3.x
E aproveito este espa√ßo para deixar aqui um convite.
Que tal aprender mais sobre Docker, Kubernetes e a implementa√ß√£o de solu√ß√µes baseadas em containers utilizando o Microsoft Azure, em um workshop que acontecer√° durante um s√°bado (dia 04/04/2020) em S√£o Paulo Capital e implementando um case na pr√°tica?
Acesse ent√£o o link a seguir para efetuar sua inscri√ß√£o (inclui camiseta, emiss√£o de certificado e almo√ßo para todos os participantes) com desconto:http://bit.ly/anp-docker-blog-groffe
A aplica√ß√£o detalhada nesta se√ß√£o √© uma varia√ß√£o do seguinte projeto:
.NET Core + Azure Functions 3.x + Queue Storage + Azure SQL/SQL Server + Queue Trigger + HTTP Trigger
Detalhes sobre a cria√ß√£o de Function Apps a partir do Visual Studio Code podem ser encontrados no artigo:
.NET Core + Serverless: implementando jobs com Azure Functions e o VS Code
Ser√° necess√°rio incluir no novo projeto o package Microsoft.Azure.WebJobs.Extensions.RabbitMQ, utilizando para isto uma de suas vers√µes Beta compat√≠veis com Azure Functions 3.x:
Adicionar ainda a biblioteca Dapper:
E o novo provider de acesso a dados do .NET Core 3.x para bases do SQL Server/Azure SQL:
No arquivo local.settings.json estar√£o as strings de conex√£o para o SQL Server (BaseCotacoes) e o RabbitMQ (BrokerRabbitMQ):
Teremos neste projeto a classe Cotacao representando uma cota√ß√£o de moeda estrangeira
Um HttpTrigger implementado pelo tipo CotacoesHttpTrigger, o qual permitir√° consultas ao valor de cota√ß√£o mais recente para uma moeda (definidos na tabela dbo.Cotacoes):
E finalmente na classe MoedasRabbitMQTrigger estar√° o c√≥digo que define a Function de mesmo nome, estrutura essa respons√°vel por processar mensagens direcionadas a uma fila do RabbitMQ:
O c√≥digo deste projeto j√° se encontra no GitHub:
https://github.com/renatogroffe/DotNetCore-AzureFunctions3x-RabbitMQ-Moedas
Para a cria√ß√£o de um ambiente de testes com RabbitMQ utilizei a tag/vers√£o 3-management-alpine da imagem oficial do RabbitMQ no Docker Hub (chamada rabbitmq) e o Azure Container Instances. A string de conex√£o foi alterada, a fim de apontar para o broker representando por este recurso do Azure Container Instances.
Uma Function App chamada groffemoedas tamb√©m foi criada:
O deployment das Azure Functions detalhadas na se√ß√£o anterior aconteceu por meio do pr√≥prio Visual Studio Code:
Caso queira conhecer mais sobre a publica√ß√£o no Microsoft Azure de uma Function App acesse:
.NET Core + Serverless: publicando uma Azure Function via VS Code
Criei para os testes uma Console Application chamada CargaMoedasRabbitMQ, a qual tamb√©m j√° foi disponibilizada no GitHub:
https://github.com/renatogroffe/DotNetCore3.1-RabbitMQ-CargaCotacoes
Ao executar via PowerShell o primeiro teste da Console Application poderemos constatar que a cota√ß√£o √© processada pela Function App publicada no Azure:
Enviando uma requisi√ß√£o √† fun√ß√£o HttpTrigger teremos como resultado:
Na imagem a seguir temos o segundo teste da Console Application, com o processamento da mensagem correspondente:
J√° a fun√ß√£o HttpTrigger trar√° como resposta:
Serverless + Azure Functions: Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
95 
95¬†claps
95 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-provision-infrastructure-on-azure-with-terraform-4065430a3d72?source=search_post---------171,"There are currently no responses for this story.
Be the first to respond.
Terraform is an infrastructure as a code tool that makes it easy to provision infrastructure on any cloud or on-premise. Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom‚Ä¶
"
https://medium.com/forloop/meet-with-microsoft-azure-experts-at-2019-forloop-summit-eb76d5d7ac19?source=search_post---------172,"There are currently no responses for this story.
Be the first to respond.
We are glad to announce that Microsoft will join us at this year‚Äôs Forloop Summit.
Microsoft Azure Experts will be on hand to share information on the cloud society and how Azure increases developers‚Äô productivity with the platform‚Äôs support of over one hundred services ‚Äî including 1st and 3rd party developer languages and tools.
There will also be an Azure DevOps and Microsoft Intelligent Kiosk demo at the event ‚Äî a collection of demos that leverage Microsoft Cognitive Services and other Artificial Intelligence tools running on Azure.
Visit the Microsoft stand to see all the exciting features.
As companies rapidly turn to the cloud and seize the opportunities it brings, we are excited to arm our developer community with the latest thinking, tools and systems to support enterprises in their cloud journeys.
See you at the event!
forLoop developers publications
183 
183¬†claps
183 
forLoop developers publications
Written by
Developer & Technology Advocate. Writer of all things Technical & Magical. Software Craftsman.
forLoop developers publications
"
https://medium.com/@tsuyoshiushio/azure-devops-yaml-pipeline-with-net-core-and-coverage-report-b84b5b282a95?source=search_post---------173,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Sep 24, 2018¬∑4 min read
I played with the new feature of Azure Pipeline. For the initial setup, you can see this video on the announcement. As my personal project, I developing a tool which use Go lang as CLI and Azure Function V2(C#) as backend. I‚Äôd like to share how we can create Continuous Integration pipeline with .NET Core with Coverage Report.
azure.microsoft.com
We have a good instruction from the Azure DevOps official documentation. However, it tightly coupled with Visual Studio. My project is much Open Source Project, so I don‚Äôt want to limit the contributor someone who has Visual Studio.
docs.microsoft.com
Other problem is, this pipeline never show the Coverage Report on the website. We need to download and click the detail report on the Visual Studio.
For someone who want to know the pipeline I‚Äôd like to share the yaml file, first.
This is the target project for the CI.
github.com
I use coverlet for collecting coverage. We can use it not only for Windows but also Linux and Mac.
github.com
For generating HTML report, I use ReportGenerator.
github.com
To Generate the coverage report, you can write like this. You need to add nuget package `coverlet.msbuild` on your test project. Then you can use /p:CollectCoverage=true parameter. Also you can choose the format of the coverage like cobertura , opencover , etc. However, for the Azure DevOps, I recommend to use cobertura since the following task request cobertura or jacoco as a format to upload coverage. StrikesLibrary.Test is the target test project. Unfortunately, We need to specify one by one. By default the report is written under the target test project directory called coverage.cobertura.xml
Since we need to specify several test projects, we need to merge the result.
This script test the second test project called StrikesRepository.Test You need to specify the report which is specified the last one. /p:MergeWith=....\coverage.cobertura.xml will merge the report and write it under the target test directory as coverage.cobertura.xml as well
For generating, the report, you need to install report generator.
When you use this on your client machine, you don‚Äôt need --tool-path however, on the CI environment, I recommend to specify it for find the tool. Because the shell should be reloaded.
Execute the command with specifying the report and target directory. In this case, I output the HTML report on the .\results directory.
The point is the reporttypes: Use HTMLInLine for enabling the output on the Azure DevOps page. Azure DevOps Coverage page show index.html on the web. However, the CSS and Javascript should be included. For this purpose we can use HTMLInline to include CSS and Javascript on the index.html.
Using YAML, we can use full functionality of the VSTS tasks. Now you can see the Unit testing and coverage report on the web. IMO. The important thing is getting feedback quickly and notice an issue. That is why I want make it available on the web. This YAML feature which is the pipeline as code works very well and we can include it on our repository. I love this new feature.
Some useful links.
docs.microsoft.com
docs.microsoft.com
You can see both YAML and GUI configration
docs.microsoft.com
If you can‚Äôt find the spec, you can see the task.json on this repo. This link is an example of the script.
github.com
Senior Software Engineer ‚Äî Microsoft
123 
4
123¬†
123 
4
Senior Software Engineer ‚Äî Microsoft
"
https://medium.com/@tsuyoshiushio/how-to-validate-request-for-azure-functions-e6488c028a41?source=search_post---------174,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Sep 1, 2018¬∑2 min read
You might want to validate the HttpRequest using Azure Functions. However, we don‚Äôt have any implementation on the Azure Functions now. I see the issue , however it is not yet.
This is just a small idea to achieve validation on Azure Functions. I discussed with my Master Sensei, Kazuki Ota. I‚Äôll share with the outcome.
I want to validate a model object. C# have a great functionality of Validation in System.ComponentModel.DataAnnotations. You can validate using this library.
We need to call Validator.TryValidateObject(movie,...) However, I don‚Äôt write it on my Functions everytime. Ideally, I want Validate(model) method on my Functions. Since Functions is static class, it might be difficult. I eventually, write an extension method on HttpRequest. It might be second best. However, if you know better way, please let me know.
This class represent a body of HttpResponse<T>. This method deserialize the target object with model validation. IsValid method will return if the model is valid or not. Also it contains, deserialized object and Validation Results.
This class extend GetBodyAsync<T>() method which returns HttpResponseBody . Inside the method, it validate the object.
Then, Azure Functions looks like this.
Since Azure Functions bindings have converter, If we send pull request for HttpTrigger, then we might directory return the HttpRequestBody object. However, I don‚Äôt know whether people loves this idea or not.
I share this blog for thinking about the best way to handle validation for Azure Functions. If you have better idea please let me know.
This is the whole sample code.
github.com
Senior Software Engineer ‚Äî Microsoft
108 
4
108¬†
108 
4
Senior Software Engineer ‚Äî Microsoft
"
https://doublepulsar.com/detecting-dns-cve-2020-1350-exploitation-attempts-in-azure-sentinel-1f2efb26422e?source=search_post---------175,"In my personal honeypot, BluePot, I‚Äôve built out detection for a wide variety of situations ‚Äî from BlueKeep exploitation to SMB MS17‚Äì010 abuse that lead to WannaCry.
"
https://itnext.io/getting-started-with-graph-databases-azure-cosmosdb-with-gremlin-api-and-python-80e57cbd1c5e?source=search_post---------177,"In this article I will give a gentle introduction to Graph Databases using Azure Cloud Platform.
I will start by giving a quick intro to graph databases explaining their use cases and the pros and cons.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/net-core-serverless-implementando-jobs-com-azure-functions-e-o-vs-code-f0d657f64824?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 28, 2019¬∑7 min read
Um equ√≠voco bastante comum em cloud computing √© considerar que a implementa√ß√£o de solu√ß√µes Serverless esteja limitada √† constru√ß√£o de APIs REST. No caso espec√≠fico das Azure Functions, este tipo desenvolvimento faria uso de HTTP Triggers.
Contudo, esta n√£o √© a √∫nica op√ß√£o dispon√≠vel ao trabalharmos com esta tecnologia (felizmente!). Triggers de execu√ß√£o peri√≥dica (Timer Trigger), baseadas em mensageria (Queue, Service Bus Queue e Service Bus Topic Triggers) e atreladas a atualiza√ß√µes em uma base do Cosmos DB est√£o entre as possibilidades oferecidas via Microsoft Azure.
Tal fato aumenta o escopo de utiliza√ß√£o das Azure Functions, ao mesmo tempo que constitui uma escolha que prioriza uma r√°pida entrega de solu√ß√µes e descarta as preocupa√ß√µes envolvendo diferentes aspectos de infraestrutura como: a cria√ß√£o de VMs, configura√ß√µes destes ambientes, instala√ß√£o de componentes necess√°rios √† execu√ß√£o das aplica√ß√µes‚Ä¶.
Neste novo artigo demonstrarei a implementa√ß√£o de um job/rotina de processamento disparado periodicamente e empregando Azure Functions + Timer Trigger, .NET Core e o Visual Studio Code. A fun√ß√£o em quest√£o ir√° checar se uma base de dados se encontra no ar, logando o resultado disto em uma Table Storage e na eventualidade de um problema enviando um alerta para um canal do Slack via Azure Logic App. Num segundo post detalharei a publica√ß√£o deste projeto no Microsoft Azure.
Caso queira conhecer mais sobre Azure Functions j√° abordei este tema anteriormente no seguinte artigo:
Desenvolvimento Serverless com .NET Core: implementando sua primeira Azure Function
E aproveito este espa√ßo tamb√©m para um convite‚Ä¶
Que tal aprender mais sobre Azure Functions e desenvolvimento de solu√ß√µes Serverless, em um workshop que acontecer√° durante um s√°bado (dia 18/01/2020) em S√£o Paulo Capital e implementando um case na pr√°tica? Acesse ent√£o o link a seguir para efetuar sua inscri√ß√£o com um desconto especial: http://bit.ly/aznp-devops-blog-groffe
A cria√ß√£o do projeto descrito neste artigo acontecer√° dentro de um diret√≥rio chamado ServerlessMonitorBD. Acionar ent√£o neste local o Visual Studio Code atrav√©s da instru√ß√£o code . (no exemplo a seguir foi utilizado o PowerShell, mas tal procedimento aconteceria de maneira id√™ntica utilizando Bash e uma distribui√ß√£o Linux como Ubuntu Desktop):
Ser√° preciso que no Visual Studio Code esteja instalada a extens√£o Azure Functions:
Outros complementos necess√°rios para a emula√ß√£o deste servi√ßo do Azure num ambiente de desenvolvimento s√£o o Microsoft Azure Storage Emulator e as Azure Functions Core Tools. Maiores informa√ß√µes sobre estas ferramentas podem ser encontradas nos links a seguir:
Use the Azure storage emulator for development and testing
Work with Azure Functions Core Tools
Para efeitos de desenvolvimento e testes esse artigo simular√° o comportamento da Azure Logic App empregada na integra√ß√£o com o Slack, a partir de uma requisi√ß√£o HTTP do tipo POST (a gera√ß√£o do recurso real acontecer√° no segundo post). Como tal recurso ser√° exposto como um endpoint de uma API REST, temos a possibilidade de ‚Äúmockar‚Äù gratuitamente essa estrutura via portal Mockable.io:
https://www.mockable.io/
Na imagem a seguir est√° destacado em vermelho o bot√£o + REST Mock do Mockable; acionar essa op√ß√£o para prosseguir com a cria√ß√£o do mock:
Em REST MOCK preencher as seguintes configura√ß√µes:
Fornecer ent√£o uma descri√ß√£o detalhando o objetivo do mock em Description:
E finalmente acionar o bot√£o Save:
Neste momento o mock gerado para testes ter√° seu status como Stopped:
Acionando o bot√£o Stopped o mesmo entrar√° em execu√ß√£o (status Started):
Consultando o mock teremos acesso aos endpoints gerados para testes com o mesmo (com e sem HTTPS):
Um teste via Postman mostrar√° que o mock gerado possui o comportamento esperado, gerando uma resposta do tipo 202 ‚Äî Accepted quando do envio de uma requisi√ß√£o HTTP do tipo POST:
Acessar agora o √≠cone do Microsoft Azure; ser√° a partir desta op√ß√£o que acontecer√° a cria√ß√£o de novas Function Apps e Azure Functions por meio do VS Code:
Em FUNCTIONS acionar a op√ß√£o Create New Project‚Ä¶ (um √≠cone representado por uma pasta e um raio):
Neste momento ser√° solicitado o diret√≥rio que servir√° de base para a gera√ß√£o do Function App; o projeto em quest√£o assumir√° o nome desta pasta (neste exemplo ServerlessMonitorBD):
Definir na sequ√™ncia linguagem (C#) a ser utilizada para a implementa√ß√£o de Azure Functions:
A escolha de um template tamb√©m ser√° solicitada para a cria√ß√£o de uma primeira Azure Function; acionar para isto a op√ß√£o TimerTrigger:
E uma cron expression, a fim de determinar a periodicidade de execu√ß√£o da fun√ß√£o DBCheckTimerTrigger. Para este exemplo ser√° utilizada a express√£o a seguir, a qual far√° com que a Azure Function seja disparada a cada 30 segundos:
Maiores informa√ß√µes sobre a montagem de cron expressions (um padr√£o origin√°rio de ambientes UNIX) podem ser encontradas na Wikipedia:
en.wikipedia.org
Ser√° solicitado ainda a utiliza√ß√£o de uma storage account; para este exemplo selecionar a op√ß√£o Use local emulator:
Passados alguns segundos ser√£o gerados o projeto ServerlessMonitorBD e seus diversos arquivos:
No arquivo local.settings.json dever√£o ser inclu√≠das as seguintes defini√ß√µes:
Adicionar tamb√©m ao projeto ServerlessMonitorBD uma refer√™ncia para o package System.Data.SqlClient (neste caso optei pela vers√£o est√°vel 4.6.0):
A classe LogEntity herda do tipo TableEntity (namespace Microsoft.WindowsAzure.Storage.Table) e representa os dados de log a serem gravados no Table Storage:
E por fim temos a implementa√ß√£o da classe DBCheckTimerTrigger e seu m√©todo Run:
Os testes descritos nesta se√ß√£o ser√£o conduzidos a partir da execu√ß√£o do projeto ServerlessMonitorBD no Visual Studio Code:
Os primeiros testes ser√£o executados com servidor SQL Server fora do ar, o que resultar√° em falhas:
Com o SQL Server em execu√ß√£o os resultados finalmente ser√£o satisfat√≥rios:
Consultando os dados via Microsoft Azure Storage Explorer teremos registros indicando falhas e sucessos no acesso a BaseIndicadores:
Recentemente aconteceu tamb√©m uma live no Canal .NET sobre a implementa√ß√£o de solu√ß√µes Serverless empregando Azure Functions. A grava√ß√£o est√° dispon√≠vel no YouTube e pode ser assistida gratuitamente por todos aqueles interessados em conhecer mais sobre as tecnologias mencionadas neste artigo:
Azure Functions documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
99 
99¬†claps
99 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devopsturkiye/grafana-ile-azure-kaynaklar%C4%B1n%C4%B1z%C4%B1-monitor-edin-d5e01d88253a?source=search_post---------179,"Sign in
There are currently no responses for this story.
Be the first to respond.
Emre √ñzkan ‚òÅÔ∏è üêß üê≥ ‚ò∏Ô∏è
Apr 10, 2019¬∑3 min read
Azure‚Äôun kendi Monitor servisi ve Log Analytics servisi dƒ±≈üƒ±nda, kaynaklarƒ±nƒ±zƒ± diƒüer uygulamalar √ºzerinden de monitor edebiliyorsunuz.Ara≈ütƒ±rmalarƒ±m sonrasƒ±nda hem entegrasyon kolaylƒ±ƒüƒ± hem de kullanƒ±mƒ± kolaylƒ±ƒüƒ± nedeniyle OpenSource olan Grafana‚Äôyƒ± deneme fƒ±rsatƒ± buldum.
Marketplace √ºzerinden uygulamayƒ± deploy ederek ba≈ülƒ±yorum.
Yaratƒ±lan kaynaklar a≈üaƒüƒ±daki gibidir.
Kurulum tamamlandƒ±ktan sonra yaratƒ±lan makineden serial console alƒ±p ilk giri≈ü i√ßin Bitnami tarafƒ±ndan yaratƒ±lan ≈üifreyi temin etmemiz gerekiyor.
Sonrasƒ±nda browser √ºzerinden Grafana aray√ºz√ºne(3000 port)ula≈üƒ±yorum.
Kullanƒ±cƒ± adƒ± olarak ‚Äúadmin‚Äù ve yukarda temin ettiƒüim ≈üifre ile login oluyorum.
Login olduktan sonra bir data source eklemem gerekiyor.Datasource sekmesini a√ßtƒ±ƒüƒ±mda entegre edebileceƒüim bir√ßok √ºr√ºn√º g√∂rebiliyorum.
Ben bu makalede Azure Monit√∂r‚Äô√º se√ßerek ilerliyorum.
Entegrasyonu yapmam i√ßin a≈üaƒüƒ±daki bilgileri temin etmem gerekiyor.ƒ∞sterseniz browser √ºzerinden cloud shell‚Äôi kullanarak isterseniz de CloudShell komut aracƒ±nƒ± bilgisayarƒ±nƒ±za kurarak alabilirsiniz.
Subscription id ve Tenant id bilgisini almak i√ßin alttaki komutu √ßalƒ±≈ütƒ±rƒ±yorum.
Subscription id=45435kjhjh-bvb7-dfxf-09sk-xcvjhdzfsd98
tenant id=23423√∂bsf-789t-jkh8‚Äì675g-fsdhafh435kh
Sonrasƒ±nda Client id ve Client Secret‚Äôi ister aray√ºzden istersenizde komut satƒ±rƒ±ndan temin edebilirsiniz.
Client id==> Azure Active Directory- App Registrations-Choose your app-Application id
Client Secret ==> Azure Active Directory- App Registrations-Choose your app-Keys
Ben alttaki komut satƒ±rƒ±nƒ± √ßalƒ±≈ütƒ±rƒ±yorum.
Client id==>xcvxcvtret34‚Äì3qwq-49b8‚Äì8250-fgzsdfd879
Client Secret==> xdfsdfsdfsd-342o-23423-a657-asdadlkd23
Aynƒ± bilgiler ile LogAnalytics Kƒ±smƒ±nƒ± da ekleyebiliyoruz.
Sonrasƒ±nda yeni bir dashboard olu≈üturup grafiklerimizi olu≈üturuyoruz.
Data kaynaƒüƒ± olarak Azure Monitor servisini se√ßip grafiƒüimizi olu≈üturuyoruz.
Diƒüer bir se√ßenek Azure Log Analytics servisini kullanƒ±yorsanƒ±z ,Grafana √ºzerinde ilgili query ‚Äòleri yazarak grafiƒüe d√∂kebilirsiniz.Altta CPU ve Memory ile alakalƒ± basit anlamda 2 query √∂rneƒüi payla≈ütƒ±m.
Diƒüer yazƒ±larƒ±ma g√∂z atmak isterseniz;
medium.com
Ki≈üisel Bloguma g√∂z atmak isterseniz;
sysaix.com
Solution Architect at @RedHat CK{A | AD | S} https://www.linkedin.com/in/emreozkann/
See all (56)
44 
Haftalƒ±k olarak yayƒ±mƒ±nƒ±zdan alacaƒüƒ±nƒ±z Email B√ºlteni¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
44¬†claps
44 
Medium independent DevOps publication. Join thousands of aspiring developers and DevOps enthusiasts
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-difference-between-azure-sql-database-and-azure-sql-managed-instance-sql-mi-2e61e4485a65?source=search_post---------180,"There are currently no responses for this story.
Be the first to respond.
Comparison ‚Äî Azure SQL Database vs SQL Managed Instance (MI).
SQL Managed Instance (SQL MI) provides native Virtual Network (VNet) integration while Azure SQL Database enables restricted Virtual Network (VNet) access using VNet Endpoints.
"
https://towardsdatascience.com/azure-data-studio-or-ssms-which-should-i-use-1db49824a39?source=search_post---------181,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nikola Ilic
Jun 15, 2020¬∑6 min read
Question from the title already became extremely popular and it will become more and more as time passes by. Since Microsoft made Azure Data Studio generally available in September 2018 and investing‚Ä¶
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-manage-azure-key-vault-with-terraform-943bf7251369?source=search_post---------182,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Jan 14, 2020¬∑2 min read
The purpose of Azure Key Vault is to store cryptographic keys and other secrets used by cloud apps and services in a HSM (Hardware security module). A HSM is a physical computing device that safeguards and manages digital keys for strong authentication and provides cryptoprocessing.
In this example, we will create a Terraform module to manage an Azure Key Vault.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-pass-microsoft-azure-ai-900-ai-fundamentals-exam-c5aec07e3078?source=search_post---------183,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is one of the leading cloud providers. This exam can be taken as an optional first step in learning about Machine Learning and Artificial Intelligence and how those concepts are exemplified by Microsoft Azure. It can be taken as a precursor to Microsoft Azure AI‚Ä¶
"
https://medium.com/analytics-and-data/setting-up-airflow-on-azure-connecting-to-ms-sql-server-8c06784a7e2b?source=search_post---------184,"There are currently no responses for this story.
Be the first to respond.
Setting up Airflow on Azure, isn‚Äôt quite as easy as on Google Cloud, where Airflow exists as a managed service called ‚ÄúCloud Composer‚Äù.Microsoft has, however, come up with a quick-start template to setup Airflow on Azure, this template sets up both a web-app hosting an Airflow instance and the postgres database backing it up, make it easier to deploy to the cloud.
"
https://medium.com/@lizrice/azure-container-instances-with-multiple-containers-512c022c04ec?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Liz Rice
Sep 14, 2017¬∑4 min read
Earlier this summer Microsoft announced Azure Container Instances (ACI), a very interesting concept allowing users to quickly spin up containers on demand. You no longer have to provision a virtual machine for the containers to run on ‚Äî ACI takes care of the infrastructure for you. I have taken to describing ACI as ‚Äúlike Lambda for containers‚Äù in the sense that you can create and destroy containers quickly, and be billed for them by the second.
Microsoft have a quick-start guide that walks you through deploying a single container instance. This gets you a simple, single-container hello world server that shows a web page. What if you want to use multiple Azure Container Instances together?
There are two quite different approaches at present. The first is an ACI Connector for Kubernetes project (described as ‚Äúexperimental‚Äù at time of writing this post) which allows your Kubernetes cluster to spin up containers within ACI rather than on pre-provisioned virtual machines. This is a really neat idea as it means your cluster (and your billing) can use resources as required, without having to provision, scale or indeed pay for unused VM resources.
The other approach is Container Groups, which looks a lot like the concept of a pod in Kubernetes. You create a container group by writing a JSON-format template and then deploying that template.
Let‚Äôs walk through that process using a marginally more complex hello-world. This has a Postgres container to store a count of page hits, and a web server container that displays that count (oh, and it also shows a joke. Sadly, it‚Äôs always the same joke.) We‚Äôll deploy both those containers in one Azure Container Instance Container Group.
I created a JSON deployment template called hellodeploy.json. I already have an Azure resource group in place called lizRgWest and I‚Äôm going to deploy my container group within it.
Initially I got error messages about a lack of resources. I sorted this out by limiting the total requested by my container group to 1 CPU and 1Gb RAM in total. I‚Äôm not sure if this is a deliberate or documented limit, so your mileage may vary.
Once the containers have had a chance to get up and running (bearing in mind that this includes pulling the images to whichever machines Azure is going to run them on) you can use az container show to see them.
This gives output like this:
You can see both containers listed in the Image column, and there is also an address and port where the web server can be located. This is available because the deployment template specified exposing a public IP address.
Should you need more detail on each of the running containers, simply omit -o table from the command.
You get to specify the container port number(s) that should be exposed, although as far as I can tell so far, you can‚Äôt map to a different port number. My web server app expects to serve on port 8080 so that‚Äôs the port I have to hit in my browser.
My very simple webserver code expects to connect to a Postgres database with a hardcoded address at localhost:5432. This matches the port specified in the deployment template definition of the database container.
As you might expect from a container group, the web server container is indeed able to connect to the database container on localhost. I know it‚Äôs working because the server increments a counter and stores the result in the database every time it serves a page, and I can see that number increasing when I refresh the page.
When you‚Äôre done with the container group you can delete it with a single command:
Regular readers will know that I‚Äôm part of the Aqua Security team, and our product helps enterprises secure their containerized deployments wherever they run. Even on ACI? Well‚Ä¶let‚Äôs just say I‚Äôm quietly confident‚Ä¶
Containers / security / open source @isovalent @ciliumproject / cycling / music @insidernine
See all (475)
123 
2
123¬†claps
123 
2
Containers / security / open source @isovalent @ciliumproject / cycling / music @insidernine
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/reactive-programming-with-event-grid-and-azure-functions-52b24003935d?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jan 28, 2018¬∑6 min read
I play with the Azure Event Grid . Azure Event Grid is a fully-managed intelligent event routing service that allows for uniform event consumption using a publish-subscribe model. Azure Functions have some integration with Event Grid. I‚Äôd like to share some of the tips which you to enable Event Grid.
At the first time, you might think what you can use it for? I also thought the same thing. Now I gradually understand several benefits.
According to the Pricing page .
It costs 0.3$ / millions operations with 100, 000 operations for free for a month. It cost around 5 USD per month. Compared with Service Bus Standard plan, which has Publish-Subscribe service, is $10 for month for 5 million operations. For the Event Grid, it is $2.97. Service Bus is very good for highly reliable system like banking system. Event Grid is fit for Event Driven Architecture with scalability. It supports millions events per second.
Event Driven Architecture is not good at Low Latency scenario. If you use Queue for messaging method, it requires Polling/Long Polling. It aims for consistency rather than low latency. However, sometimes, You want to build event-driven serverless system with Low latency. If you create a printing system with event-driven architecture with Queue, it is difficult to print less than 1 min from the request. Also, If you want to move from Microservices architecture to Serverless architecture, you might need Low Latency scenario.
Since Event Grid using Pub-Sub model, Event Handlers need to subscribe a Topic. When The Topic gets an message, message is routed event Handlers, Event Grid allow you to filter the message. You can chose which event you receive. For example, If you use a blob storage, you can filter via directory/file patterns.
Once an Event Grid Topic accept a message and if it fails to send message to event handler, it will retry with exponential back off method. It is much reliable than just use http request. You can substitute some use cases that you use queue for sending message for asynchronous messaging.
You can use Event Grid with Azure Functions. Azure Functions support EventGrid Trigger. However, I can‚Äôt find explicit document until now. So, I‚Äôd like to share some learning.
Currently Event Grid is preview. The Location which you can use with Azure Functions is limited. Now West US 2 and West Central US is for the Blob Integration. You can check which location you can use in here.
To understand the behavior of EventGrid, let‚Äôs start with Custom Topic. On the Azure Portal, create Event Grid Topic.
Create an Event Grid Topic from the Azure Portal.
Get the Topic Endpoint and Topic Key for sending messages from the Event Topic.
Now ready to use Topic.
Let‚Äôs write code for Event Grid Trigger. I wrote an sample with C#. You can find whole code in here. You need to install this nuget package.
Microsoft.Azure.WebJobs.Extensions.EventGrid
You might want to know the EventGridEvent definition. You can get a lot of meta data from this. You can get the message from ‚ÄúData‚Äù attribute from the senders.
Then just upload your code to a Function App.
You can create an Event Subscription from the Azure Portal. Go to the Function App which you deploy your code. Just click this link.
You will see this screen. Choose the Event Grid Topic name which you want to subscribe. Then save.
Now you are ready to accept Event Grid message at the Azure Functions.
Sending message is also simple. Just install Microsoft.Azure.EventGrid package as nuget package. You need topicHostName and topicKey which you can get already. One tips, you can pass Json Serializable Poco as the data object. I don‚Äôt know why but if you pass Anonymous Json Serializable Object, it will cause an error.
Additional NOTE (30th Jan, 2018):
The data object is not restricted to JObject by EventGrid runtime. It should be cast to JToken as the API level. Now have some issue for Event Grid Trigger. see https://github.com/Azure/azure-functions-eventgrid-extension/issues/25.
Send message using this method. Then you will see the Event Grid Trigger works.
Also, you can filter by Event Type and Subject. When you configure Event Subscription like this, it will trig by only ‚Äúsome/‚Äù prefix subject. This feature is handy.
Trigger is almost the same as calling a webhook. If you create a simple webhook function with Event Subscription like this,
Open the Event Grid Topic page, then click Event Subscription
Then create webhook with the HttpTrigger function‚Äôs URL.
Then send message to the topic, the HttpTrigger function is also triggered.
I recommend to use EventGrid Trigger. It provides the endpoint url verfication with Event Grid. If you use HttpTrigger, you probably will see 3 initial requests, they are verfication errors, in preview they are just ignored.
You can check if your messages are routed as expected. on the Event Grid Topic page. You can see the Metrics page for each Event Grid Subscription.
One of the difficulty of the debugging is, if you‚Äôve got an error during messaging, sometimes you can‚Äôt see the error messages. It looks just not delivered the messages since this is still preview. I encounter that issue three times. Once it was just an shortage of Azure specific region.
The second was I tried to send EventGridEvent with passing non-json-serializable object to the data field. At that time, I didn‚Äôt use the Microsoft.Azure.EventGrid package. Just send message with HttpClient. Since now you can use EventGrid package, I recommend to use this. It prevent these issue.
The third was, The medhod name is not match with Function Name. This is the WRONG sample. :)
Also if you encounter the issue which you can‚Äôt see an error on your portal and Metrics page, you can ask support request on Azure Portal.
Event Gird is new way to Reactive Programming world. Enjoy the Event Grid with Azure Functions.
Senior Software Engineer ‚Äî Microsoft
55 
55¬†
55 
Senior Software Engineer ‚Äî Microsoft
"
https://medium.com/@maarten.goet/threat-hunting-in-the-cloud-with-azure-notebooks-supercharge-your-hunting-skills-using-jupyter-8d69218e7ca0?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Feb 20, 2019¬∑8 min read
Robert M. Lee has a great quote: ‚ÄúThreat hunting exists where automation ends‚Äù. Threat hunting is large manually, performed by SOC analysts, trying to find a ‚Äòneedle in the haystack‚Äô. And in the case of cybersecurity, that haystack is a pile of ‚Äòsignals‚Äô.
These analysts often use separate tools for querying the data, manipulating the data set, reversing the potential malware, etcetera. What if we could provide an environment where you can perform all these tasks in context, and share the outcome with your team?
Azure Notebooks, with a little KQL magic sauce, is exactly that. Let‚Äôs supercharge your hunting skills with Azure, Jupyter, Python and KQL!
Kusto Query Language (KQL)
Kusto Query Language or KQL in short is the default way to work with data in Azure Data Explorer powered services such as Log Analytics, Azure Security Center, Azure Monitor and many more. It is a powerful yet easy to learn language.
Robert Cain, a Microsoft MVP, has written a 4-hour long course on Pluralsight that you can take for free, to learn the language all the way up to the advanced queries. KQL skills is something you‚Äôll need if you will be doing threat hunting in Azure; most of the security data will be in Log Analytics workspaces.
Jupyter
Jupyter Notebook, formerly called IPython, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text through markdown. It is already broadly used in data science, and has support for lots of programming languages such as R, Python, etc. The multi-user version of Jupyter is called JupyterHub.
The cool thing is that you can share your notebook with others, and that you can produce interactive output using HTML etc. and display that through a so called ‚Äúpresentation mode‚Äù. This makes it great for threat hunting and sharing signals within the SOC team.
On GitHub you‚Äôll find ready-to-run Docker images containing Jupyter.
Azure Notebooks
Azure Notebooks is currently in public preview and is a free hosted service to develop and run Jupyter notebooks in the cloud with no installation. Azure Notebooks is a freeservice, but each project is limited to 4GB memory and 1GB data to prevent abuse. Legitimate users that exceed these limits see a Captcha challenge to continue running notebooks.
However, if the Azure Active Directory account you sign in with is associated with an Azure subscription, you can connect to any Azure Data Science Virtual Machine (DSVM) instances within that subscription. DSVM‚Äôs can be found in the Azure Marketplace. With these dedicated DSVM‚Äôs you can add better processing power and remove any of those limits.
PRO TIP: You need to deploy the Ubuntu version of the DSVM. The Windows version of DSVM does not contain JupyterHub by default. The Ubuntu template of DSVM has an extra bonus: it will open up the right ports by default in your NSG!
In the case of Azure Notebooks, it allows you to share your notebooks using GitHub.
Pandas, KQLMagic and other libraries
One of the things you will find out early using Jupyter is that you will want to manipulate data. This is where a library called Pandas comes in. Pandas is an open source Python framework, maintained by the PyData community and mostly used for Data Analysis and Processing.
Another library you will need is KQLMagic. Michael Binshtock, who works at Microsoft, wrote this and allows you to directly query Log Analytics-based workspaces in Azure, for instance when working with data in Azure Monitor, Azure Security Center, etcetera. The great thing about this library is that it uses the Kusto Query Language (KQL). Which means that you can use your favorite KQL queries directly in Jupyter.
The big picture
Putting all the pieces together you get something like this:
Real-world threat hunting
Let‚Äôs look at a real-world example. In this case we have a number of virtual machines running in Microsoft Azure, and Azure Security Center is turned on at the subscription level to capture relevant security events.
We‚Äôre suspicious of a machine called APPSERVER, based on an Alert we got fromAzure Security Center, and want to do some investigation.
We go to Azure Notebooks and login:
We create a new Project called ‚ÄòThreat Hunting‚Äô:
We create a new Notebook called ‚ÄòAzure threat hunting‚Äô:
We will use the Free Compute option and open the notebook:
The docker image that the Free Compute option provides *already contains* the Kqlmagic library that we will need. If you‚Äôre using a dedicated DSVM, or are running Jupyter locally, you should run the install command to get the library installed:
PRO TIP: The Free Compute is a docker container in a shared compute environment and therefore it will take a couple of minutes before the library loads. You can look ‚Äòbehind the scenes‚Äô by using the Terminal button:
Through the terminal window you can issue commands such as ‚Äòps‚Äô or ‚Äòtop‚Äô:
Now we will need to authenticate to the Log Analytics workspace we will be using. In this case we will be connecting to Azure Security Center:
We can now run KQL queries to look at the data being captures by Azure Security Center for this machine. In this case we‚Äôll have a look at the network connections it has:
PRO TIP: While in the KQL query interface in Azure you‚Äôll be using the double quote character for specifying input, you‚Äôll be using the single quote in Jupyter. Make sure to change your queries so that they work properly in Jupyter.
If you want to go multi-line to make things better readable, you need to use double %. As our application server is in The Netherlands, I will apply a filter and only show the connections that are going to IP addresses that our outside of our country:
Let‚Äôs see if any of these IP addresses match a TOR node. There is a current list of TOR nodes and their IP addresses at https://www.dan.me.uk/torlist. We can load that into our notebook using Pandas:
The next step is to compare the two lists to see if there are any matches:
Another great way of visualizing your data is taking the Longitude and Latitude points from the KQL query and putting them on a world map. Add the following line to your KQL query:
And use the following Python code in Jupyter:
From this point on you‚Äôll likely want to do some more investigation and assess whether or not there is a real threat. Use your own hunting skills for that ;-)
Sharing your findings
An unique feature of Jupyter is the Presentation mode. It allows you to easily share key items from your audience to other people in a visual friendly way, without having to copy/paste data to another application.
You can use Markdown text to annotate your notebook. Enable the Slide picker by going to the View menu, Cell Toolbar, then Slide Show. Go to any row and on the right-hand side select to Skip it, be part of a Slide, etcetera.
Lastly, click on ‚ÄòEnter/Exit RISE Slideshow‚Äô to share your findings:
I‚Äôve published this Jupyter notebook on my GitHub repository. Another great thing about Azure Notebooks is that you can clone any repository and turn it into a Jupyter project:
Other examples
John Lambert, distinguished engineer at Microsoft‚Äôs Threat Intelligence Center, has some other great examples on threat hunting with Jupyter which he has shared here:
There is also a sample notebook on MyBinder that shows you step-by-step which Kqlmagic commands are available, and how to use them.
Conclusion
Jupyter is a great platform for threat hunting. You can work with data in-context and natively connect to security backends in Microsoft Azure using Kqlmagic.
Best of all, using Azure Notebooks and Azure Security Center, we didn‚Äôt spend a dollar and got our threat hunting platform for free :-)
Start learning KQL, Python and Jupyter today and supercharge your hunting skills!
Thank you
A big thank you goes out to Michael Binshtock, John Lambert, Giovanni Lanzani and Paul Shealy. They have been invaluable on providing support, and background information, while I wrote this article.
Happy hunting!
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
40 
1
40¬†claps
40 
1
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/threat-hunters-forge/azure-sentinel-to-go-b5f6848d3c61?source=search_post---------188,"There are currently no responses for this story.
Be the first to respond.
Recently, I started working with Azure Sentinel, and as any other technology that I want to learn more about, I decided to explore a few ways to deploy it. I got a grasp of the basic architecture and got more familiarized with it. As a researcher, I also like to simplify deployments in my lab environment and usually look for ways to implement the infrastructure I work with as code. Therefore, I started to wonder if I could automate the deployment of an Azure Sentinel solution via a template or a few scripts. Even though, it made sense to expedite the deployment of the solution, I realized I still did not have data or other resources to play with. Then, I wondered If I could integrate the deployment of an Azure Sentinel instance and other resources through the same scripts or templates covering different scenarios.
In the end, this approach allows me to also share the process with others in the community üåé in a more practical way.
This post is part of a four-part series where I will show you how to deploy your own Azure Sentinel solution in a lab environment via Azure Resource Management (ARM) templates along with a custom logs ingestion pipeline to consume pre-recorded datasets and other resources such as network environments for research purposes.
In this post, I show you how to use ARM templates to deploy an Azure Sentinel solution and ingest pre-recorded datasets via a python script, Azure Event Hubs and a Logstash pipeline.
The other parts of this series can be found in the following links:
Microsoft Azure Sentinel is a scalable, cloud-native, security information event management (SIEM) and security orchestration automated response (SOAR) solution. An Azure service that empowers organizations to bring disparate data sources from resources hosted both on-premises and in multiple clouds and be able to detect, investigate and respond to threats.
If you want to learn more about Azure Sentinel, I would recommend to explore this Microsoft Azure document page. Also, if you want to know what you can do with it, make sure you read the articles available in the Microsoft Tech Community Sentinel blog and take a look at these awesome webinars.
Technically, all we need to do to deploy an Azure Sentinel solution is:
That basic set up allows you explore all the main features of Azure Sentinel as well as preloaded out-of-the-box resources such as queries, visualizations, response playbooks, and notebooks. You could also upload other resources and even enable data connectors in Azure Sentinel via code. Javier Soriano blogged about it in this post, and it is a great reference for production deployments.
One of the things I wanted to do different for this post was execute Azure Sentinel On-boarding steps, but in a declarative way with Azure Resource Manager (ARM) templates without having to run Powershell commands.
To implement infrastructure as code for your Azure solutions, use Azure Resource Manager templates. The template is a JavaScript Object Notation (JSON) file that defines the infrastructure and configuration for your project. The template uses declarative syntax, which lets you state what you intend to deploy without having to write the sequence of programming commands to create it.
The Azure Resource Manager is the deployment and management service for Azure and below you can see some of the ways you could interact with it.
A few things that I like about ARM templates are the orchestration capabilities to deploy resources in parallel which makes it faster than serial deployments, and also the feature to track deployments via the Azure portal.
Now that we know a little bit more about Azure Resource Manager services, we are ready to deploy Azure Sentinel. One document that I recommend to get familiar with to learn more about Azure resources mapped to ARM template resource types is this one. In this section, we are going to deploy a Log Analytics workspace and enable Azure Sentinel. Remember that I provide the template for you so that you can follow along.
A Log Analytics workspace can be found under the Microsoft.OperationalInsights resource types as Microsoft.OperationalInsights/workspaces
I created an initial template with some parameters to make it modular for anyone to use. This is the initial template:
Next, I needed to define the Azure Sentinel solution and enable it on the top of the Log Analytics workspace. You can do it with a resource type found under the Microsoft.OperationsManagement resource types as Microsoft.OperationsManagement/solutions .
I added that to our initial ARM template and this is the final result:
That‚Äôs it! You can download it and use it for the next steps.
There are a few ways to execute ARM templates, and it all depends on how comfortable you are with the Azure portal and Azure tool-kits (e.g. Azure CLI)
If you want to use one command to deploy an ARM template, then this option is for you. The Azure command-line interface (CLI) is Microsoft‚Äôs cross-platform command-line experience for managing Azure resources. It can be installed in Windows, macOS and Linux environments. In addition, there is a PowerShell version of it and also an interactive, authenticated, browser-accessible option known as the Azure Cloud Shell.
We can start using Azure CLI and create a Resource Group if you have not done it yet. Run the following command to create one in a specific location:
Next, you can run the following command to execute the ARM template:
Track your deployment: Azure Portal>Resource Group Name>Deployments
That‚Äôs it! once your deployment completes, you will be able to access the main Azure Sentinel interface. Before we do that, let me show you another way to execute our ARM template.
It takes a few more clicks to do it via the Azure portal, but it is easy to follow:
That‚Äôs it! once your deployment completes, you will be able to access the main Azure Sentinel interface.
Search for ‚ÄúAzure Sentinel‚Äù
Select the Azure Sentinel workspace that you just created.
You will be taken to the main Azure Sentinel interface üçª That was easy right?
‚ÄúWhy do I have to do all that with ARM templates when I can just follow these instructions and with a few clicks I can deploy one too?‚Äù
Deploying the solution while working in a lab environment is not enough. You need to have other resources and data to start exploring and learning about all the capabilities Azure Sentinel provides. That will take more than just a few clicks. What if we can take the ARM template that we just used and run other nested templates in parallel to deploy other resources and even ingest pre-recorded data for additional research?
Azure-Sentinel2Go is an open source project developed to expedite the deployment of an Azure Sentinel lab along with other Azure resources and a data ingestion pipeline to consume pre-recorded datasets for research purposes.
Azure-Sentinel2Go is part the Blacksmith project
The Blacksmith project focuses on providing dynamic easy-to-use templates for security researches to model and provision resources to automatically deploy applications and small networks in the cloud.
Azure Sentinel2Go is a work in progress, and I welcome feedback on what it is that you would like to see being deployed along with an Azure Sentinel solution and datasets you would like to work with in your lab environment.
One of the features that I have noticed security analysts get interested the most while using Azure Sentinel for the first time is the Log Analytics capabilities. Log Analytics is the primary tool in the Azure portal for writing log queries written in Kusto Query Language (KQL) to quickly retrieve, consolidate, and analyze security events. Therefore, I decided to find a way for researchers to learn about KQL with pre-recorded datasets.
Fortunately, the Log Analytics workspace allows the collection of custom logs via its HTTP Data Collector API. If you want to learn how to do it with code, there are some basic examples in Azure docs for Powershell, C# and Python.
In this section I will share a few of my favorite ways to send pre-recorded datasets a Log Analytics workspace custom log table.
This is one of the simplest ways to send data directly to the log analytics workspace. I took the basic example available here, and extended it a little bit to be able to read from a JSON file or a folder, show a progress bar, and send smaller sized chunks of 5MB per POST request. Make sure you read the Data Limits while using a similar approach. I also extended the PowerShell script available and created a proof of concept here.
The script has the following options and all the information you need from the your log analytics workspace can be found in Azure Portal>Log Analytics Workspace>Advanced Settings.
Next, we need a data sample for these exercises. Therefore, the project comes with a few data samples in this folder. Download the dataset-sample-small.tar.gz to your local computer and decompress it.
Next, send it over by running these commands in your local computer:
Once it completes go to your Azure Sentinel interface and click on Logs. You can see that there are no events yet. It usually takes from 5‚Äì10 mins.
You can see a new table under customs logs with the event schemas. Remember that not every event will have the same schema. Make sure you understand the schema of your events before running queries.
Based on the event schemas, we can run the following query to see what events we are working with:
That‚Äôs it! This is a very practical way to ingest custom logs, but might not scale with larger files or hundreds of files in a loop. Therefore, I wanted to also provide another option that would allow me to send events to a more robust pipeline and let it handle the whole process. This is a proof of concept and works very well in a lab environment.
I like to use existing tools that are proven to work at scale and this is not the exception. TL;DR ‚Äî I use Kafkacat to read json files stored locally and send them over to an Azure Event Hub. Next, Logstash reads them from Azure Event Hub, and sends them over to a Log Analytics workspace.
In more details the following is happening in the image above:
I already provide the following configurations as part of Azure Sentinel2Go:
This is the Logstash input config file to consume events from the Azure Event Hub. The plugin used is the Logstash Azure Event Hubs input plugin.
I do not use the input codec => ""json"" property because I do not want to unpack the event Message field and exceed the max number (500) of custom fields per data type in the Log Analytics workspace.
This is the Logstash output config file to send the events that it collects from the Azure Event Hub to the Log Analytics workspace. The plugin used is the Azure Log Analytics output plugin for Logstash developed by Microsoft.
One thing I added to the Azure Sentinel2Go repository is a ‚ÄúDeploy to Azure‚Äù badge used on Azure quick-start templates to upload the ARM template directly to the Azure portal. Very convenient! Click on the badge!
You will be taken to the interface to set deployment parameters. Set the Deploy Pipeline parameter to Logstash-EventHub. One thing to pay attention to is the virtual machine size. If you are in westus, you need to switch it to Standard_A3 . Also, make sure you set the AllowedIPAddresses parameter to restrict access to the Logstash box. Add your company or your house Public IP address.
Monitor your deployment. It should take around 8‚Äì10 minutes.
Once it completes, you should be able to send prerecorded data from your local computer to the Azure Event Hub.
First, create a local Kafkacat configuration file to define a few properties to be able to access the Azure Event Hub. I created one for you as shown below.
You will need to get the following values and paste them in the config file.
Second, we need a sample dataset to send over to our Azure Event Hub. We can use the same dataset we used earlier with the Python script.
Next, in your local computer, run Kafkacat in Producer mode as shown below.
Once you run that command, you can check the events flowing through the Event Hub. Go to Azure Portal > Resource Group Name > Event Hub Namespace and filter the Show Metrics view to show Messages only. It might take a few minutes for the view to update.
The Azure Sentinel view also will take a a few mins to update.
As you already know, click on Logs (Log Analytics) to explore the custom logs and their schema. One thing to remember is that the events flowing through this pipeline are packed inside of the Message field. This is to avoid exceeding the max number (500) of custom fields per data type in case you send a lot of events with different schemas. I will show you an example later.
You can unpack the Message field and get to specific nested field with the Kusto Query function parse_json(). This function interprets a string as a JSON value and returns the value as dynamic .
Remember that not every event will have the same schema. Make sure you understand the schema of your events before running queries.
Azure Sentinel2Go also comes with the option to load pre-recorded datasets right at deployment time. It leverages the same Logstash VM for the data ingestion. You do not have to send anything from your local computer.
I use the following commands to download and decompress all small mordor datasets. The commands are part of the deployment and are executed inside of the Linux VM when you choose to add the item ‚Äúmordor-small-datasets‚Äù to the Add to cart parameter while deploying Azure Sentinel2Go. You do not have to run anything in your local computer.
If you choose to add the item ‚Äúmordor-large-apt29‚Äù to your Add Mordor Dataset parameter while deploying Azure Sentinel2Go, the following commands are executed inside of the Linux VM.
This is the additional Logstash input config to read all the JSON files. The plugin used is the Logstash File Input plugin.
If you have resources running from the earlier deployment, I recommend to delete them (Lab environment). Similar to our previous deployment, go to Azure-Sentinel2Go > grocery-list > custom-log-pipeline. Select Logstash for the Deploy Custom Logs Pipeline parameter as shown below and add a mordor dataset to your cart (Add Mordor Dataset) . For this example, we are going to use the mordor-small-datasets. Also, once again, make sure you set the AllowedIPAddresses parameter to restrict access to the Logstash box. Add your company or your house Public IP address.
Monitor the deployment. It might take around 8‚Äì10 minutes for it to be done. When it is complete, go to your Azure Sentinel interface. Give it 2‚Äì3 mins for events to start showing. You will start getting thousands of events (200K+)
What I do while I wait for all the events (300k+) to be ingested üòÜ üê∂ ‚ù§Ô∏è
Take advantage of the time you have and stretch a little bit! Take a break! ‚ù§Ô∏è
We can do the same as before and explore a few events to understand the event schemas. Also, since those events were generated as part of the Mordor project, you could focus on datasets mapped to specific ATT&CK tactics and techniques. The project comes with a Navigator View for the specific platforms that it supports (Currently only Windows).
One thing that I like to look for when looking for lateral movement techniques is processes created under logon sessions that were initially created as part of a network authentication event (Logon Type 3). One example can be adversaries leveraging the Windows Management Instrumentation (WMI) and its Win32_Process class to execute command over the network. This behavior would generate something similar to this:
We can use KQL and its JOIN operator to look for a similar behavior without filtering on the parent process wmiprvse.exe. We can use events 4624 (An account was successfully logged on) and 4688 (A New process has been created) from the Microsoft-Windows-Security-Auditing event provider.
Use the following query and run it in log analytics as shown below
As you can see in the image below, that query got some hits from a few datasets that were created after emulating adversaries using WMI and Powershell Remoting to execute commands over the network üèπ
That‚Äôs it for this first part! I hope you enjoyed it and found the design and deployment of Azure Sentinel2Go helpful. In the next post, I will show you how to deploy additional resources along with an Azure Sentinel solution to focus on a few use cases that go beyond just using the Log Analytics features. I want to make sure Azure Sentinel2Go also allows the exploration of other capabilities provided by Azure Sentinel.
Azure Sentinel2Go Link: https://github.com/OTRF/Azure-Sentinel2Go
https://mordordatasets.com/introduction
https://docs.microsoft.com/en-us/azure/azure-monitor/faq
https://docs.microsoft.com/en-us/azure/azure-monitor/terminology
https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-platform
https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-sources#custom-sources
https://docs.microsoft.com/en-us/azure/sentinel/overview
https://techcommunity.microsoft.com/t5/azure-sentinel/deploying-and-managing-azure-sentinel-as-code/ba-p/1131928
https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/overview
https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview
https://docs.microsoft.com/en-us/azure/azure-monitor/insights/solutions
https://azuremarketplace.microsoft.com/en-us/marketplace/apps/Microsoft.SecurityOMS?tab=Overview
https://azure.microsoft.com/en-us/pricing/details/azure-sentinel/
https://azure.microsoft.com/en-us/pricing/details/monitor/
https://www.elastic.co/guide/en/logstash/current/plugins-inputs-azure_event_hubs.html
https://azure.microsoft.com/en-us/services/event-hubs/
https://github.com/yokawasa/logstash-output-azure_loganalytics
https://docs.microsoft.com/en-us/azure/kusto/query/
Threat Hunting, Data Science & Open Source Projects
49 
49¬†claps
49 
Threat Hunting, Data Science & Open Source Projects
Written by

Threat Hunting, Data Science & Open Source Projects
"
https://medium.com/@renatogroffe/docker-kubernetes-azure-devops-it-experts-superdigital-b6d436632e9b?source=search_post---------189,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 14, 2019¬∑4 min read
No dia 15/04/2019, uma segunda, meu amigo Milton C√¢mara e eu (Renato Groffe) participamos como palestrantes do It Experts, um evento promovido pela Superdigital (uma empresa do grupo Santander) em S√£o Paulo-SP.
Deixo aqui meu agradecimento ao Natanael S√° pelo convite e por todo o apoio para que este evento acontecesse.
E aproveito este espa√ßo e o grande interesse por Docker tamb√©m para um convite.
Tem interesse em conhecer mais sobre Docker? Que tal ent√£o fazer um curso completo, cobrindo desde fundamentos a diferentes possibilidades de uso de containers com tecnologias em alta no mercado? Adquira conhecimentos profundos sobre Docker, evolua e se diferencie no mercado, seja voc√™ um profissional DevOps, um Desenvolvedor ou um Arquiteto de Software!
Acompanhe o portal Docker Definitivo para ficar por dentro de novidades a serem anunciadas em breve!
Site: https://dockerdefinitivo.com/
Esta iniciativa contou com as seguintes apresenta√ß√µes:
P√∫blico: 25 pessoas
A seguir est√£o os slides que utilizamos como base para as apresenta√ß√µes:
As demonstra√ß√µes realizadas tomaram como base o seguinte projeto:
ASP.NET Core + Docker + Alpine Linux + Docker Hub + Kubernetes + AKS (Azure Kubernetes Service)
A implementa√ß√£o deste projeto, bem como a cria√ß√£o de objetos para deployment em um cluster Kubernetes foram abordadas por mim no seguinte artigo:
Orquestra√ß√£o de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
O post a seguir agrupa traz estes materiais e outras refer√™ncias sobre a utiliza√ß√£o de Docker e diversas outras tecnologias (principalmente .NET Core):
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
Demonstrei tamb√©m o deployment de aplica√ß√µes empregando containers Docker na nuvem no seguinte v√≠deo do Canal .NET, em que cobri a utiliza√ß√£o do servi√ßo conhecido como Azure Web App for Containers:
O uso do Kubernetes foi tema ainda de 2 eventos online gratuitos do Canal .NET, com a grava√ß√£o dos mesmos estando dispon√≠vel no YouTube e abordando desde os primeiros passos ao deployment automatizado via Azure DevOps (antigo VSTS):
A seguir est√£o fotos gerais do evento e de cada uma das palestras realizadas.
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
58 
58¬†claps
58 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/mensageria-net-core-3-1-exemplos-com-rabbitmq-kafka-azure-service-bus-e-azure-queue-storage-c594bf30c091?source=search_post---------190,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 12, 2020¬∑4 min read
Recentemente participei de algumas apresenta√ß√µes no Canal .NET em que foi abordado o uso de mensageria em solu√ß√µes constru√≠das com .NET Core 3.1, com exemplos envolvendo tecnologias como Azure Queue Storage, Azure Service Bus, RabbitMQ e Apache Kafka.
Neste novo post trago diversos exemplos que implementei e utilizei durante estes eventos (al√©m de projetos adicionais), bem como o link com a grava√ß√£o de cada uma destas lives. Na se√ß√£o de Refer√™ncias foram inclu√≠dos tamb√©m artigos cobrindo algumas das solu√ß√µes de mensageria aqui mencionadas.
E aproveito este espa√ßo para um convite‚Ä¶
Caso voc√™ precise conhecer mais sobre o Microsoft Azure como um todo, n√£o deixe de aproveitar o pre√ßo promocional (apenas R$ 255,00) da segunda turma online do treinamento Azure na Pr√°tica com foco em Desenvolvimento Web que acontecer√° dia 20/06/2020 (um s√°bado). Aproveite para aprender mais sobre as possibilidades oferecidas por servi√ßos do Microsoft Azure como App Service (para hospedagem de Web Apps, com suporte a ASP.NET Core e Docker), Azure Storage, Azure Functions + Logic Apps (solu√ß√µes serverless), Azure App Configuration + Key Vault‚Ä¶ E o melhor, no conforto de sua casa! Acesse o link a seguir para informa√ß√µes e efetuar sua inscri√ß√£o:
https://bit.ly/anp-web3-blog-groffe
.NET Core 3.1 + Console Application + Envio de Mensagens + RabbitMQ + Azure Service Bus (Queues e Topics) + Apache Kafka
.NET Core + Azure Functions 3.x + Consumo de Mensagens + Queue Storage + RabbitMQ + Azure Service Bus (Queue)
.NET Core + Azure Functions 3.x + Azure Service Bus + Apache Kafka + Topics
ASP.NET Core 3.1 + API REST + Apache Kafka + T√≥pico (Envio de Mensagens)+ Manipula√ß√£o de A√ß√µes
.NET Core + Azure Functions 3.x + Azure Cosmos DB + Apache Kafka (T√≥pico/Consumo de Mensagens) + HTTP Trigger + Cota√ß√µes de A√ß√µes
.NET Core + Azure Functions 3.x + MongoDB + Apache Kafka (T√≥pico/Consumo de Mensagens) + Cota√ß√µes de A√ß√µes
.NET Core 3.1 + Console Application + Apache Kafka (T√≥pico/Consumo de Mensagens) + Redis + Serilog + Manipula√ß√£o de A√ß√µes
.NET Core + Azure Functions 3.x + RabbitMQ (Consumo de Mensagens)+ Azure SQL/SQL Server + RabbitMQ Trigger + HTTP Trigger
.NET Core 3.1 + Console Application + RabbitMQ (Consumo de Mensagens) + Moedas Estrangeiras
ASP.NET Core 3.1 + API REST + Azure Service Bus + T√≥pico (Envio de Mensagens) + Manipula√ß√£o de A√ß√µes
.NET Core + Azure Functions 3.x + Azure Cosmos DB + Azure Service Bus (Topic/Consumo de Mensagens) + Queue Trigger + HTTP Trigger + Cota√ß√µes de A√ß√µes
.NET Core + Azure Functions 3.x + Azure Service Bus (Topic/Consumo de Mensagens) + SQL Server + Dapper + HTTP Trigger + Cota√ß√µes de A√ß√µes
.NET Core 3.1 + Console Application + Azure Service Bus (Topic/Consumo de Mensagens) + Redis + Serilog + Cota√ß√µes de A√ß√µes
Nesta live utilizei o projeto a seguir para o envio de mensagens a uma fila ou t√≥pico (permitindo assim o teste de tecnologias como Azure Queue Storage, Azure Service Bus, RabbitMQ e Apache Kafka):
.NET Core 3.1 + Console Application + Envio de Mensagens + RabbitMQ + Azure Service Bus (Queues e Topics) + Apache Kafka
O consumo de mensagens associadas a filas criadas com Azure Queue Storage, Azure Service Bus e RabbitMQ envolveu um projeto baseado em Azure Functions + .NET Core 3.1:
.NET Core + Azure Functions 3.x + Consumo de Mensagens + Queue Storage + RabbitMQ + Azure Service Bus (Queue)
E um segundo projeto tamb√©m criado com Azure Functions + .NET Core 3.1 e vinculado a t√≥picos do Azure Service Bus e Apache Kafka:
.NET Core + Azure Functions 3.x + Azure Service Bus + Apache Kafka + Topics
A grava√ß√£o est√° dispon√≠vel no YouTube:
Para esta apresenta√ß√£o utilizei um projeto ASP.NET Core 3.1 que possibilita o envio de mensagens a um t√≥pico do Apache Kafka:
ASP.NET Core 3.1 + API REST + Apache Kafka + T√≥pico (Envio de Mensagens)+ Manipula√ß√£o de A√ß√µes
E ainda implementei 3 projetos para o consumo de mensagens vinculadas ao t√≥pico em quest√£o:
.NET Core + Azure Functions 3.x + Azure Cosmos DB + Apache Kafka (T√≥pico/Consumo de Mensagens) + HTTP Trigger + Cota√ß√µes de A√ß√µes
.NET Core + Azure Functions 3.x + MongoDB + Apache Kafka (T√≥pico/Consumo de Mensagens) + Cota√ß√µes de A√ß√µes
.NET Core 3.1 + Console Application + Apache Kafka (T√≥pico/Consumo de Mensagens) + Redis + Serilog + Manipula√ß√£o de A√ß√µes
A seguir temos a grava√ß√£o do YouTube:
Nesta live um dos assuntos foi AMQP (Advanced Message Queuing Protocol), um dos protocolos mais populares entre solu√ß√µes de mensageria. O Azure Service Bus constitui um bom exemplo de alternativa com suporte a este padr√£o, sendo que fiz uso desta tecnologia em uma aplica√ß√£o ASP.NET Core 3.1 para o envio de mensagens a um t√≥pico:
ASP.NET Core 3.1 + API REST + Azure Service Bus + T√≥pico (Envio de Mensagens) + Manipula√ß√£o de A√ß√µes
Os pr√≥ximos projetos exemplificam o consumo de mensagens vinculadas ao t√≥pico do Azure Service Bus utilizado na aplica√ß√£o anterior:
A grava√ß√£o desta live tamb√©m se encontra no YouTube:
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
.NET + Apache Kafka: Guia de Refer√™ncia
.NET Core + Serverless: melhorando a experi√™ncia de Desenvolvimento com Azure Functions 3.x | pt 1
.NET Core 3.x + Serverless: configura√ß√£o, dicas e exemplos com Azure Functions 3.x
Serverless + Azure Functions: Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
48 
48¬†
48 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/no-code-data-enhancement-with-azure-synapse-analytics-and-azure-auto-ml-cb9d97fb0c26?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
TLDR; This post will walk through how to train and evaluate Azure ML AutoML Regressions model on your data using Azure Synapse Analytics Spark and SQL pools.
Before we get started let‚Äôs make sure we are all on the same page with the core Azure concepts needed to take your data to the next level.
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
Azure Synapse Analytics is an integrated service that accelerates extracting insightful across data warehouses and big data systems.
Azure Synapse ties together traditional relational SQL enterprise data warehousing, unstructured data stores and serverless Apache Spark , to enable limitless pipelines for ETL and ELT operations. Furthermore Synapse Studio provides a unified interface for data monitoring, coding, and security.
Azure Machine Learning (Azure ML) is a cloud-based service for creating and managing machine learning solutions. It‚Äôs designed to help data scientists and machine learning engineers to leverage their existing data processing and model development skills & frameworks.
In Azure Machine Learning, Auto ML allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality.
With Auto ML we can transform Synapse Analytics Data into actionable baseline models to enrich datasets at scale without writing a single line of machine learning code.
Regression is used to build models to forecast numeric values such as taxi fares based on learned input features.
In the next section, I will walk you through an end to end example of how to enrich you Synapse data by training and evaluating a model with the NYC Taxi Dataset. Once you get preform these steps you‚Äôll be able to train and run your own Auto ML on models on any tabular dataset of your choosing.
First if you do not have them already we need to create our Azure ML and Synapse workspaces.
Once we have deployed our two workspaces we need to link them. Full steps for linking Azure ML and Synapse Workspaces can be found here
To actually ingest and process our data we need to use pools. Azure Synapse Analytics offers various analytics engines to help you ingest, transform, model, analyze, and serve your data.
For this tutorial since we are using a toy dataset we can use the cheapest pools available for you own data you may want to configure your pools accordingly.
The serverless Apache Spark pools offers open-source big data compute capabilities. This is where the majority of our data processing and Auto ML code will run.
A dedicated serverless SQL pool offers T-SQL based compute and storage capabilities. We will use this pool to store the data we want to enhance with our AutoML model.
One key advantage of Azure Synapse Analytics is if you configure a time out you only pay for the compute when it‚Äôs in use.
Once we have our serverless Spark and SQL pools up and running we can now ingest our data setup our Spark and SQL tables for training and testing respectively.
Download this Spark Create-Spark-Table-NYCTaxi- Data.ipynb notebook and import it into your workspace.
Once the notebook is uploaded, change the sql_pool_name value to match the name of your sql pool and then select the desired spark pool and run click all.
Once the data is ingested we can use our spark nyc_taxi and Spark Pool to train an AutoML regression model for forecasting taxi fares.
Follow the three steps in wizard below to train your model.
This will kick off your Auto ML Regression training job. It should take about 2hrs to run when it is complete we can then evaluate it on our SQL test table.
Once we have the best model we can now evaluate it on our test SQL table using our SQL Pool.
First we need to select the table we want to enhance with the model we just trained.
Then we select our new Auto ML model, map the our input table columns to what the model is expecting and choose or create a table for storing our model locally.
The wizard will generate a T-SQL script that evaluate our model against the test data and outputs the fare predictions.
There you have it all you need to know to train and test you own AutoML models and make them actionable.
Additional Synapse Documentation and Walkthroughs worth checking out can be found below:
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
techcommunity.microsoft.com
Now that you‚Äôve finished the steps above it is time to try them out on you own synapse data. Feel free to post in the comments if you have any questions and to share the cool models you make!
Look forward to seeing what AutoML and Azure Synapse can do for you!!
Thanks to Nellie Gustafsson, Yifan Song and Chang Xu from the Azure Synapse product team for their great documentation and support during the writing this post.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with the Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
154 
2
154¬†claps
154 
2
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/azure-na-pratica/azure-na-pr%C3%A1tica-gratuito-2-docker-saiba-como-foi-conte%C3%BAdos-gratuitos-1720aa45005?source=search_post---------192,"There are currently no responses for this story.
Be the first to respond.
Neste √∫ltimo s√°bado (06/06/2020) o Azure na Pr√°tica promoveu seu segundo minicurso online e gratuito, com foco desta vez em Docker. Al√©m de uma introdu√ß√£o englobando conceitos b√°sicos e benef√≠cios da ado√ß√£o de containers, ao longo deste treinamento foi coberto tamb√©m o uso de Docker em conjunto com tecnologias como Docker Hub, Docker Compose, Docker Desktop, Windows, Linux, macOS (com o aux√≠lio do Ericson da Fonseca) SQL Server, PowerShell, Bash, .NET 5/ASP.NET 5, NGINX, RabbitMQ, MongoDB, Microsoft Azure, Azure Web App for Containers e Visual Studio Code.
Caso voc√™ queira ter acesso ao conte√∫do do primeiro treinamento (que aconteceu em Maio/2020) gratuitamente ou, at√© mesmo, deseje rev√™-lo, acesse o link a seguir:
Azure na Pr√°tica Gratuito #1 - Desenvolvimento Web: saiba como foi + conte√∫dos gratuitos
Fui instrutor e um organizadores desta iniciativa, juntamente com meus amigos Milton C√¢mara (Microsoft MVP, MTAC) e Ericson da Fonseca (Microsoft MVP). O resultado geral foi muito al√©m de nossas expectativas iniciais, com 4269 inscri√ß√µes via Sympla:
E um pico de 1100 pessoas nos assistindo ao longo da live no YouTube, com mais de 4 mil visualiza√ß√µes at√© o momento da publica√ß√£o deste post:
A grava√ß√£o j√° est√° dispon√≠vel no canal Azure na Pr√°tica no YouTube e pode ser assistida gratuitamente (aproveitamos para convidar voc√™ que est√° lendo esse post para que se inscreva no YouTube):
Mais uma vez tivemos espectadores de outros pa√≠ses, como Portugal, Angola, Argentina e Samoa (al√©m claro de participantes de norte a sul do Brasil). Recebemos in√∫meros agradecimentos em redes sociais (YouTube, Facebook, LinkedIn) por este esfor√ßo, algo que sempre nos motiva para seguir em frente com esse tipo de iniciativa. Seguem alguns feedbacks:
Os slides utilizados est√£o dispon√≠veis no SlideShare:
A seguir temos o reposit√≥rio do GitHub com os exemplos usados durante o minicurso:
https://github.com/azurenapratica/ANP-Docker-Minicurso
Aproveitamos para agradecer:
A Microsoft disponibilizou brindes sob a forma de conte√∫dos gratuitos e que podem ser acessados no link:
http://bit.ly/anp0606
Nas pr√≥ximas se√ß√µes temos descontos para os pr√≥ximos cursos pagos do Azure na Pr√°tica, al√©m de conte√∫dos gratuitos sobre Docker.
O canal do Azure na Pr√°tica no YouTube √© uma excelente fonte de conte√∫dos, com grava√ß√µes gratuitas incluindo mesas redondas, dicas e truques na utiliza√ß√£o de Docker e servi√ßos do Microsoft Azure com suporte a containers:
www.youtube.com
Temos ainda o Canal .NET e o Coding Night, canais em que o uso de containers Docker √© abordados com frequ√™ncia:
www.youtube.com
www.youtube.com
A seguir est√£o tamb√©m diversos artigos abordando a utiliza√ß√£o de Docker(h√° v√≠deos sendo referenciados em alguns destes posts):
Docker ‚Äî Guia de Refer√™ncia Gratuito
Docker + Bancos Relacionais: cuidados importantes na cria√ß√£o de containers
Docker + Portainer: gerenciando containers a partir de um browser
MongoDB + mongo-express + Docker Compose: montando rapidamente um ambiente para uso
Kubernetes ‚Äî Guia de Refer√™ncia Gratuito
Docker para Desenvolvedores .NET ‚Äî Guia de Refer√™ncia
ASP.NET Core + Docker: trabalhando com vari√°veis de ambiente
Microservices: alternativas para a implementa√ß√£o no Microsoft Azure
Docker + Azure DevOps: build e deployment automatizado de aplica√ß√µes
Kubernetes + Azure DevOps: build e deployment automatizado de aplica√ß√µes
Docker + GitHub Actions ‚Äî parte 1: build automatizado de aplica√ß√µes
Docker + GitHub Actions ‚Äî parte 2: deployment automatizado de aplica√ß√µes
Como o Microsoft Azure pode simplificar a publica√ß√£o de suas Web Apps? ‚Äî Dica R√°pida
E concluo este post com os links para os blog de cada um dos respons√°veis por este minicurso:
Renato Groffe ‚Äî Blog
Ericson da Fonseca ‚Äî Blog
Milton Camara Gomes ‚Äî Blog
Al√©m de convidar a todos para que acompanhem e sigam o blog Azure na Pr√°tica:
Azure na Pr√°tica ‚Äî Blog
Blog do Azure na Pr√°tica
60 
60¬†claps
60 
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Pr√°tica
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Pr√°tica
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/aplicando-testes-automatizados-com-selenium-e-azure-devops-mvpconf-latam-2019-33b2d5840a7?source=search_post---------193,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 28, 2019¬∑3 min read
Nos dias 12 e 13 de Abril/2019 (sexta e s√°bado) aconteceu o MVPConf Latam 2019, o maior evento t√©cnico brasileiro de experts Microsoft.
Em sua segunda edi√ß√£o e organizado pela comunidade de MVPs Microsoft no Brasil, o MVPConf teve car√°ter beneficente (o valor arrecadado foi repassado a institui√ß√µes beneficentes das 5 regi√µes do Brasil: Sudeste, Sul, Centro-Oeste, Nordeste e Norte) e contou com mais de 2 mil pessoas inscritas de todo o pa√≠s.
Realizei com meu amigo Milton C√¢mara Gomes (Microsoft MVP) 2 apresenta√ß√µes durante o evento, focando na implementa√ß√£o e execu√ß√£o automatizada de testes de aplica√ß√µes Web utilizando o Selenium WebDriver e o Azure DevOps:
Gostaria de deixar neste post meu muito obrigado ao Andr√© Dias (Microsoft MVP) que coordenou a trilha DevOps pela oportunidade em participar da mesma como palestrante.
Deixo aqui tamb√©m meus agradecimentos ao Robson Ara√∫jo (Microsoft MVP) e ao Thiago Adriano (Microsoft MVP) pelas fotos tiradas durante a apresenta√ß√£o. E tamb√©m ao Fernando Seguim que atuou no staff por toda a ajuda durante minhas palestras.
Os slides utilizados durante a apresenta√ß√£o foram disponibilizados no SlideShare:
E no pr√≥ximo link est√° o projeto de exemplo utilizado durante a minha demonstra√ß√£o:
Selenium WebDriver + .NET Core 2.2 + .NET Standard 2.0 + xUnit
J√° descrevi anteriormente esta solu√ß√£o no seguinte artigo:
.NET Core + Selenium WebDriver: testes em modo headless com Firefox e Chrome
O assunto implementa√ß√£o de testes de aplica√ß√µes Web com Selenium WebDriver foi tema tamb√©m de um evento online no Canal .NET. A grava√ß√£o est√° no YouTube e pode ser assistida gratuitamente:
A seguir est√£o as fotos tiradas em cada uma das 2 sess√µes.
12/04/2019 ‚Äì 1a. Palestra
13/04/2019 ‚Äì 2a. Palestra
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
92 
92¬†
92 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/t-t-software-solution/1-virtual-academy-for-azure-fundamentals-%E0%B9%82%E0%B8%94%E0%B8%A2-aipen-studio-3f60db031b0f?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
‡∏ú‡∏°‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏°‡∏≠‡∏á‡∏´‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö Microsoft Certified: Azure Fundamentals (AZ-900) (‡∏Ç‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏¢‡πà‡∏≠‡πÜ‡∏ß‡πà‡∏≤ AZ-900 ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö) ‡∏°‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏ô‡∏õ‡∏µ ‡πÅ‡∏ï‡πà‡πÇ‡∏ä‡∏Ñ‡∏£‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠ COVID-19 ‡∏à‡∏ô‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏ß‡πâ‡∏°‡∏µ‡∏≠‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ñ‡∏£‡∏±‡∏ö
‡∏à‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á‡πÑ‡∏î‡πâ‡∏°‡∏≤‡πÄ‡∏à‡∏≠ Post ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì Issaret Prachitmutita ‡πÉ‡∏ô FB Group ‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå ‡∏à‡∏∂‡∏á‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏•‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡πÑ‡∏ß‡πâ 2 ‡∏≠‡∏¢‡πà‡∏≤‡∏á
‡∏Ç‡∏≠‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì ‡∏Ñ‡∏∏‡∏ì ‡∏≠‡∏¥‡∏®‡πÄ‡∏£‡∏® ‡∏õ‡∏£‡∏∞‡∏à‡∏¥‡∏ï‡∏ï‡πå‡∏°‡∏∏‡∏ó‡∏¥‡∏ï‡∏≤ ‡πÅ‡∏•‡∏∞‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô AiPEN Studio ‡∏ó‡∏∏‡∏Å‡πÜ‡∏ó‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡πÜ ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏ß‡∏¥‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå ‡∏ó‡∏≠‡∏á‡∏†‡∏π‡πà ‡∏°‡∏≤‡∏Å‡πÜ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏ô‡∏≤‡∏°‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå ‡∏ó‡∏µ‡πà‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Cloud ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå‡∏ú‡πà‡∏≤‡∏ô Microsoft Teams ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡πÄ‡∏™‡∏≤‡∏£‡πå 14:00‚Äì16:00 ‡∏ô. ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ 6 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå ‡πÅ‡∏•‡∏∞‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ô Video ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏î‡πÑ‡∏ß‡πâ‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å AZ-900 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ Cloud ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Å‡∏±‡∏ö Cloud Provider ‡πÄ‡∏à‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡πÄ‡∏ä‡πà‡∏ô AWS ‡∏´‡∏£‡∏∑‡∏≠ GCP ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÅ‡∏Ñ‡πà‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô Azure ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô
‡∏ú‡∏°‡∏≠‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö Microsoft Azure Certifications ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡πÉ‡∏ö Certificate ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà 4 ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏£‡∏±‡∏ö ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ
www.microsoft.com
2. Exam AZ-204 -> Azure Developer Associate ‚òÖ‚òÖ
3. Exam AZ-400 -> Azure DevOps Engineer Expert ‚òÖ‚òÖ‚òÖ
‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡πà‡∏≤‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ö‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡πá‡πÄ‡∏ä‡πà‡∏ô
‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á 5 ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö (Five characteristics of cloud computing) ‡πÄ‡∏ä‡πà‡∏ô Microsoft, Amazon ‡πÅ‡∏•‡∏∞ Google
‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏•‡∏¥‡πâ‡∏á‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
www.saladpuk.com
Azure ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏£‡∏±‡∏ö‡∏Å‡∏±‡∏ö Compliance ‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏≤‡∏Å‡∏• ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô GDPR ‡πÉ‡∏ô‡∏™‡∏´‡∏†‡∏≤‡∏û‡∏¢‡∏∏‡πÇ‡∏£‡∏õ, HIPAA, ISO ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á PDPA ‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà
docs.microsoft.com
‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á Cloud Computing ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£ ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏™‡∏≠‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà
Capital Expenditure (CapEx)
‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á On-Premises Data Center ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏õ‡∏Å‡∏±‡∏ö Physical Infrastructure ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏Å‡∏±‡∏ö Server, Storage, Network, Backup, Disaster Recovery, Data Center Infrastructure ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á CapEx ‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏£‡∏≤‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏à‡πà‡∏≤‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏£ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô‡∏ï‡∏≤‡∏°‡∏£‡∏≠‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì
Operational Expenditure (OpEx)
‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏ö‡∏ô Cloud Computing ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô CapEx
‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏°‡∏µ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô, ‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô CPU, RAM, IOPS ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bandwidth ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ ‡πÄ‡∏ä‡πà‡∏ô backup traffic ‡πÅ‡∏•‡∏∞ data recovery traffic
‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏°‡∏±‡πà‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏™‡∏°‡∏≠ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏π‡∏ç‡πÄ‡∏õ‡∏•‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á OpEx ‡∏Ñ‡∏∑‡∏≠ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡∏µ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏•‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏•‡∏î‡∏•‡∏á
‡∏Ñ‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ö‡∏ô Cloud Computing ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏Ç‡∏≠‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏° 3 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ñ‡∏∑‡∏≠ IaaS, PaaS ‡πÅ‡∏•‡∏∞ SaaS
On-Premise‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏•‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏ó‡∏±‡πâ‡∏á Hardware ‡πÅ‡∏•‡∏∞ Software ‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Network ‡∏¢‡∏±‡∏á‡πÑ‡∏á ‡∏à‡∏∞ Patch OS ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á ‡πÑ‡∏ü‡∏î‡∏±‡∏ö ‡πÄ‡∏ô‡πá‡∏ó‡∏´‡∏•‡∏∏‡∏î harddisk ‡πÄ‡∏™‡∏µ‡∏¢ ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ã‡∏∂‡πà‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏•‡πÄ‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏•‡∏¢
Infrastructure as a Service (IaaS)‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ö‡∏ô Cloud Cumputing ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ã‡∏¥‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ Cloud Provider ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏• Physical Infrastructure ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏∞ Patch OS ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà ‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏á Antivirus ‡πÇ‡∏î‡∏¢
‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏¢‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å On-Premises ‡∏°‡∏≤‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‚ÄúLift & Shift‚Äù)
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£: Container Service, Virtual Machines, Azure Storage Accounts
Platform as a Service (PaaS)‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏≠‡∏≥‡∏ô‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏á‡∏ß‡∏•‡∏Å‡∏±‡∏ö Infrastructure ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡πÄ‡∏ô‡πâ‡∏ô‡∏î‡∏π‡πÅ‡∏Ñ‡πà ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏ó‡∏≤‡∏á Azure ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏î‡∏π‡πÅ‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô Windows ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≠‡∏¢ update ‡∏´‡∏£‡∏∑‡∏≠ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£: Azure App Service, Azure SQL Database, Azure Functions
Software as a Service (SaaS)‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡πÉ‡∏´‡πâ End Customer ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô‡∏Ñ‡∏•‡∏≤‡∏ß‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡∏¢‡∏∏‡πà‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏•‡∏¢
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£: Dynamics 365, Outlook, Office 365
‡∏ñ‡πâ‡∏≤‡πÉ‡∏Ñ‡∏£‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Cloud Services ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ú‡∏°‡∏Ç‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏™‡∏•‡∏±‡∏î‡∏ú‡∏±‡∏Å‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
www.saladpuk.com
Public Cloud‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏• Hardware ‡πÄ‡∏≠‡∏á ‡πÉ‡∏´‡πâ Cloud Provider ‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏î‡∏π‡πÅ‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ Internet ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏£‡πå‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏î‡πâ‡∏ß‡∏¢
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Public Cloud ‡∏Ñ‡∏∑‡∏≠ ‡∏à‡πà‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÑ‡∏°‡πà‡∏°‡∏µ CapEx (‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤), ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏£‡∏∞‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß, ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡∏ô‡∏±‡∏Å
Private Cloud‡πÄ‡∏£‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Cloud Environment ‡∏•‡∏á‡πÉ‡∏ô Data-center ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤ ‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏£‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏ö‡∏≥‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤ Hardware ‡πÅ‡∏•‡∏∞ Software
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Private Cloud ‡∏Ñ‡∏∑‡∏≠ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏≠‡∏á‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà, ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ Network ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô, ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å, ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
Hybrid Cloud‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏™‡∏°‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Public ‡πÅ‡∏•‡∏∞ Private Cloud ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Ç‡∏≠‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£
‡πÄ‡∏ä‡πà‡∏ô ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô Public Cloud ‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢‡∏ï‡πà‡∏≠‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á Host Web Application ‡∏ö‡∏ô Public Cloud ‡πÅ‡∏ï‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏°‡∏≤‡∏¢‡∏±‡∏á Database ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Private Cloud ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Hybrid Cloud ‡∏Ñ‡∏∑‡∏≠ ‡∏°‡∏µ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô (Economies of Scale) ‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ Public Cloud ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Private Cloud ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà Public Cloud ‡∏°‡∏µ‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤, ‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏î‡πâ‡∏≤‡∏ô Security, Compliance ‡πÅ‡∏•‡∏∞ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Legacy Applications ‡∏Ç‡∏≠‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£
Digital Skill ‚Äî Azure Fundamentals (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
ExamTopics ‚Äî AZ-900 Exam Actual Questions
Facebook ‚Äî Data TH.com ‚Äî Data Science ‡∏ä‡∏¥‡∏•‡∏ä‡∏¥‡∏• (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
Github ‚Äî Microsoft Certified Azure Fundamentals (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
Medium ‚Äî Azure AZ-900 Exam Preparation Guide: How to pass in 3 days
Medium ‚Äî ‡∏ß‡∏µ‡∏ò‡∏µ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏™‡∏≠‡∏ö AZ-900 Online ‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Azure Exam Voucher
Medium ‚Äî AZ-900 ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÅ‡∏ô‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡∏•‡∏á‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏™‡∏≠‡∏ö
Medium ‚Äî AZ-900 ‡∏™‡∏£‡∏∏‡∏õ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏∏‡∏î‡πÜ
Microsoft Learn-Azure Fundamentals
Udemy ‚Äî Microsoft Azure ‚Äî Beginner‚Äôs Guide + AZ-900 (‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢)
WhizLabs ‚Äî AZ-900 (‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢)
Workshop ‡πÄ‡∏•‡πá‡∏Å‡πÜ‡∏à‡∏≤‡∏Å Microsoft ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AZ-900 ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏π‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô Cloud Computing ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á Cloud ‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏ô‡πÉ‡∏à‡∏à‡∏∞‡∏™‡∏≠‡∏ö Microsoft Certified: Azure Fundamentals (AZ-900) ‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏°‡∏≤‡∏Å
‡πÉ‡∏ô‡∏ö‡∏ó‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Azure ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Ç‡∏≠‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì ‡∏Ñ‡∏∏‡∏ì ‡∏≠‡∏¥‡∏®‡πÄ‡∏£‡∏® ‡∏õ‡∏£‡∏∞‡∏à‡∏¥‡∏ï‡∏ï‡πå‡∏°‡∏∏‡∏ó‡∏¥‡∏ï‡∏≤ , ‡∏ó‡∏µ‡∏°‡∏á‡∏≤‡∏ô AiPEN Studio ‡∏ó‡∏∏‡∏Å‡πÜ‡∏ó‡πà‡∏≤‡∏ô‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡∏ß‡∏¥‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå ‡∏ó‡∏≠‡∏á‡∏†‡∏π‡πà ‡∏°‡∏≤‡∏Å‡πÜ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡πÜ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏°‡∏≠‡∏£‡πå‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡πÜ‡∏ó‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡πÜ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ^^
‡∏ô‡∏≤‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô
https://www.tt-ss.net/
31 

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
31¬†claps
31 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-lock-azure-resources-to-prevent-changes-45be494b135c?source=search_post---------195,"There are currently no responses for this story.
Be the first to respond.
Preventing Azure Resource Deletion or Unexpected Changes using Locks.
Resource Manager Locks provide a way for administrators to lock down Azure resources to prevent deletion or changing of a resource. These locks sit outside of the Role Based Access Controls (RBAC) hierarchy and, when applied, will place restrictions on the resource for all‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/battle-for-the-king-of-public-cloud-aws-vs-azure-9c5505dabb1e?source=search_post---------196,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ryan Gleason
Jun 17, 2020¬∑6 min read
The public cloud is taking over whether you like it or not. And what‚Äôs not to like about it?
Cloud computing has made it incredibly easy to manage all of our computer system resources. It has provided‚Ä¶
"
https://medium.com/syntropynet/microsoft-adds-noia-platform-to-azure-marketplace-4dd3bf961e76?source=search_post---------197,"There are currently no responses for this story.
Be the first to respond.
As we approach launch, our team is working hard to secure placements on every major cloud marketplace. By adding our solution to these marketplaces, existing cloud users can start using Syntropy Stack in a few clicks.
We‚Äôve had a collaborative relationship with Microsoft for some time. Earlier this year, they added us to their FastTrack program, granting us direct access to Azure engineers to collaborate on development and deployment of the Syntropy.
Now, we‚Äôre proud to announce that Microsoft is adding our technology to the Azure marketplace, making Syntropy accessible to millions of cloud users worldwide. Users can now launch an instance with our agent installed, and we‚Äôll soon be distributing API keys and tutorials to guide them through initial deployment and usage.
‚ÄúWith the Syntropy Sta in the Azure marketplace, customers worldwide will benefit from Syntropy‚Äôs next-generation technology and solutions, accessed and used in just a few clicks.‚Äù
‚Äî Tibbs Pereira, Principal PM Manager, Microsoft Azure FastTrack
The Azure marketplace is a hand-picked collection of certified apps and services that can be deployed seamlessly to any client‚Äôs existing infrastructure. Adding our tech to the marketplace will accelerate adoption of our network.
‚ÄúMicrosoft Azure is one of the biggest and most respected cloud providers on the market. By having our technology certified by Microsoft and listed directly on the marketplace, driving adoption becomes a lot less burdensome.‚Äù
‚Äî Jonas Simanavicius, CTO at Syntropy
Our ultimate goal is to make our technology stack as seamless as possible, from initial adoption all the way through billing. We‚Äôve had a very productive relationship with Microsoft, and we look forward to working with them further to match our technology with their global user base.
Please join our Telegram Channel to learn more.
Rethinking the Internet for Everyone.
478 
478¬†claps
478 
Syntropy is an open project providing next-generation connectivity technology for the Internet, powered by $NOIA ‚ö°Ô∏è
Written by
Co-founder of NOIA Network
Syntropy is an open project providing next-generation connectivity technology for the Internet, powered by $NOIA ‚ö°Ô∏è
"
https://blog.jeremylikness.com/azure-event-grid-glue-for-the-internet-e770d94cc29?source=search_post---------198,
https://medium.com/cognitiveclouds/aws-vs-microsoft-azure-vs-google-cloud-cbdb03e0281e?source=search_post---------199,"There are currently no responses for this story.
Be the first to respond.
While collectively, these three cloud providers dominate the space, their approach to cloud computing is dictated strongly by their background. Amazon has immense know-how when it comes to collating and aggregating massive amounts of data, Google‚Äôs heritage stems from an analytical background, and Microsoft‚Äôs strength comes from computing. This comparison will underline their strengths and weaknesses. As we proceed, tie these activities to your business objectives to find the right fit. Also, it‚Äôs important to note that your best fit may not turn out to be a single cloud provider.
Your reasoning for picking one service over another will differ from another user. However, there are particular aspects of competing clouds that offer benefits in certain circumstances. That can always be compared. So, let‚Äôs advocate for and against each now.
AWS continues to lead the way, in very broad terms, regarding maturity and offering the widest range of functionality. However, the gap is certainly closing. Its expansive list of services and tools, along with its enterprise-friendly features make it an attractive proposition for big organizations. While its massive, continuously growing infrastructure offers economies of scale that allow aggressive price cuts.
Now it appears, Microsoft has begun to bridge that gap between the two. With its plans to strengthen ties with its on-premise software and ongoing investment in building out the Azure cloud platform, it will continue to bridge that gap. Microsoft Azure will continue to be a strong proposition for organizations already heavily invested heavily in Microsoft regarding technology and developer skills, of which there are undoubtedly many. With Google offering a slightly different proposition, it has made inroads with certain users. Even so, to become a viable enterprise choice, it has a lot of work to do. It might carve a niche for itself in advanced use cases based on machine learning and big data, but whether Google is willing to relinquish the key IaaS market to its largest competitors is another subject.
Originally published on Product Insights Blog from CognitiveClouds: Top Web App Development Company
Tips, advice and insights from our digital product‚Ä¶
43 
1
43¬†claps
43 
1
Written by
See every interaction with a customer ‚Äî across all digital channels ‚Äî & quickly determine how to delight your audience with personalization and recommendations.
Tips, advice and insights from our digital product strategy, design and development experts.
Written by
See every interaction with a customer ‚Äî across all digital channels ‚Äî & quickly determine how to delight your audience with personalization and recommendations.
Tips, advice and insights from our digital product strategy, design and development experts.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/asp-net-core-docker-aplica%C3%A7%C3%B5es-na-nuvem-com-azure-container-instances-49a61db2d79f?source=search_post---------200,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 25, 2017¬∑5 min read
Em posts anteriores abordei o uso de containers Docker em conjunto com o ASP.NET Core, empregando para isto servi√ßos como Docker Hub, Azure Web App on Linux e Azure Container Registry. Caso deseje saber mais a respeito acesse os links a seguir:
ASP.NET Core: APIs REST na nuvem com Docker e Azure Web App
ASP.NET Core 2.0: deployment na nuvem com Docker, Azure Container Registry e Azure Web App on Linux
A Microsoft anunciou em Julho/2017 um novo servi√ßo chamado Azure Container Instances. Ainda em modo Preview, esta √© uma alternativa que simplifica a utiliza√ß√£o de containers Docker na nuvem. Embora dispense o uso de orquestradores (abordagem comum em cen√°rios complexos, nos quais quest√µes como um uso mais intensivo e escalabilidade s√£o preocupa√ß√µes centrais), o Azure Container Instances pode mesmo assim ser integrado a solu√ß√µes como o Kubernetes.
As pr√≥ximas se√ß√µes demonstram como utilizar este servi√ßo, partindo dos exemplos apresentados nos posts indicados no in√≠cio deste artigo.
Este primeiro exemplo faz uso de uma API REST para convers√£o de alturas em p√©s para o equivalente a metros. Para maiores detalhes sobre a implementa√ß√£o desse projeto baseado no ASP.NET Core 1.1 acesse este link.
A imagem p√∫blica que ser√° empregada est√° dispon√≠vel no Docker Hub como renatogroffe/apialturas:
No portal do Azure ser√° criado um novo recurso baseado no servi√ßo Azure Container Instances:
Acionar ent√£o o bot√£o Criar:
Informar no formul√°rio de cria√ß√£o:
Na pr√≥xima tela ser√° poss√≠vel definir o tipo do sistema operacional (Linux em OS Type), n√∫mero de cores/n√∫cleos, mem√≥ria, o uso ou n√£o de um IP p√∫blico (Yes em Public IP address) e a porta (normalmente 80 em se tratando de aplica√ß√µes Web):
Em Resumo clicar no bot√£o OK, confirmando a cria√ß√£o de um novo container:
Ap√≥s alguns segundos o item apialturascontainer-api1 constar√° na lista de recursos dispon√≠veis:
Ao acessar este item aparecer√° ent√£o o recurso apialturarcontainer, com o IP p√∫blico para acesso a este container tamb√©m destacado em vermelho:
Efetuando um teste com a URL http://13.90.204.247/api/conversoralturas/pesmetros/1000 ser√° retornado o valor da convers√£o de 1000 p√©s para o equivalente em metros (304,8):
OBSERVA√á√ÉO: Al√©m do portal de Azure, novos recursos baseados no servi√ßo Azure Container Instances podem ser criados via Azure CLI 2.0. Para saber mais a respeito consulte este link.
Para o exemplo descrito nesta se√ß√£o ser√° utilizada uma imagem privada chamada renatogroffe.azurecr.io/sitedadosnasa, a qual se encontra armazenada em um recurso do Azure Container Registry. Trata-se de um site implementado com o ASP.NET Core 2.0 e que acessa uma API gratuita de consulta a imagens da NASA (Ag√™ncia Espacial Norte-Americana):
As credenciais necess√°rias para utiliza√ß√£o desta imagem est√£o na se√ß√£o Access keys do Container Registry:
OBSERVA√á√ÉO: para maiores informa√ß√µes sobre este site de exemplo e o Azure Container Registry acesse este link.
Preencher no formul√°rio de cria√ß√£o:
OBSERVA√á√ÉO: Os demais procedimentos de cria√ß√£o s√£o id√™nticos ao exemplo descrito na se√ß√£o anterior.
Na pr√≥xima imagem √© poss√≠vel observar o item dadosnasacontainer-dad4 em destaque:
Ao acessar este recurso aparecer√£o o IP p√∫blico gerado e o container dadosnasacontainer:
Um teste com a URL http://40.76.21.242/ produzir√° um resultado similar ao da imagem a seguir:
ASP.NET Core - Documentation
Azure Container Instances - Documentation
Azure Container Registry - Documentation
Conte√∫dos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
96 
96¬†claps
96 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/change-data-capture-with-azure-postgresql-and-kafka-4598dbf0b57a?source=search_post---------201,"Change Data Capture (CDC) can be used to track row-level changes in database tables in response to create, update and delete operations. It is a powerful technique, but useful only when there is a way to leverage these events and make them available to other services.
Using Apache Kafka, it is possible to convert traditional batched ETL processes into real-time, streaming mode. You can do-it-yourself (DIY) and write good old Kafka producer/consumer using a client SDK of your choice. But why would you do that when you‚Äôve Kafka Connect and it‚Äôs suite of ready-to-use connectors?
Once you opt for Kafka Connect, you have a couple of options. One is the JDBC connector which basically polls the target database table(s) to get the information. There is a better (albeit, a little more complex) way based on change data capture. Enter Debezium, which is a distributed platform that builds on top of Change Data Capture features available in different databases. It provides a set of Kafka Connect connectors which tap into row-level changes in database table(s) and convert them into event streams that are sent to Apache Kafka. Once the change log events are in Kafka, they will be available to all the downstream applications.
Here is a high-level overview of the use-case presented in this post. It has been kept simplified for demonstration purposes.
Data related to Orders is stored in the PostgreSQL database and contains information such as order ID, customer ID, city, transaction amount. time etc. This data is picked up the Debezium connector for PostgreSQL and sent to a Kafka topic. Once the data is in Kafka, another (sink) connector sends them to Azure Data Explorer allow or further querying and analysis.
The individual components used in the end to end solution are as follows:
Data pipelines can be pretty complex! This blog post provides a simplified example where a PostgreSQL database will be used as the source of data and a Big Data analytics engine acts as the final destination (sink). Both these components run in Azure: Azure Database for PostgreSQL (the Source) is a relational database service based on the open-source Postgres database engine and Azure Data Explorer (the Sink) is a fast and scalable data exploration service that lets you collect, store, and analyze large volumes of data from any diverse sources, such as websites, applications, IoT devices, and more.
Although Azure PostgreSQL DB has been used in this blog, the instructions should work for any Postgres database. So feel free to use alternate options if you‚Äôd like!
The code and configuration associated with this blog post is available in this GitHub repository
Apache Kafka along with Kafka Connect acts as a scalable platform for streaming data pipeline ‚Äî the key components here are the source and sink connectors.
The Debezium connector for PostgreSQL captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database, generates data change event records and streams them to Kafka topics. Behind the scenes, it uses a combination of a Postgres output plugin (e.g. wal2json, pgoutput etc.) and the (Java) connector itself reads the changes produced by the output plug-in using the PostgreSQL‚Äôs streaming replication protocol and the JDBC driver.
The Azure Data Explorer sink connector picks up data from the configured Kafka topic, batches and sends them to Azure Data Explorer where they are queued up ingestion and eventually written to a table in Azure Data Explorer. The connector leverages the Java SDK for Azure Data Explorer.
Most of the components (except Azure Data Explorer and Azure PostgreSQL DB) run as Docker containers (using Docker Compose) ‚Äî Kafka (and Zookeeper), Kafka Connect workers and the data generator application. Having said that, the instructions would work with any Kafka cluster and Kafka Connect workers, provided all the components are configured to access and communicate with each other as required. For example, you could have a Kafka cluster on Azure HD Insight or Confluent Cloud on Azure Marketplace.
Check out these hands-on labs if you‚Äôre interested in these scenarios
Here is a breakdown of the components and their service definitions ‚Äî you can refer to the complete docker-compose file in the GitHub repo
The Kafka and Zookeeper run using the debezium images ‚Äî they just work and are great for iterative development with quick feedback loop, demos etc.
The Kafka Connect source and sink connectors run as separate containers, just to make it easier for you to understand and reason about them ‚Äî it is possible to run both the connectors in a single container as well.
Notice that, while the PostgreSQL connector is built into debezium/connect image, the Azure Data Explorer connector is setup using custom image. The Dockerfile is quite compact:
Finally, the orders-gen service just Go application to seed random orders data into PostgreSQL. You can refer to the Dockerfile in the GitHub repo
Hopefully, by now you have a reasonable understanding of architecture and the components involved. Before diving into the practical aspects, you need take care of a few things.
Finally, clone this GitHub repo:
To begin with, let‚Äôs make sure you have setup and configured Azure Data Explorer and PostgreSQL database.
During the ingestion process, Azure Data Explorer attempts to optimize for throughput by batching small ingress data chunks together as they await ingestion ‚Äî the IngestionBatching policy can be used to fine tune this process. Optionally, for the purposes of this demo, you can update the policy as such:
Refer to the IngestionBatching policy command reference for details
3. Create a Service Principal in order for the connector to authenticate and connect to Azure Data Explorer service. If you want to use the Azure Portal to do this, please refer to How to: Use the portal to create an Azure AD application and service principal that can access resources. The below example makes use of Azure CLI az ad sp create-for-rbac command. For example, to create a service principal with the name adx-sp:
You will get a JSON response:
Please note down the appId, password and tenant as you will be using them in subsequent steps
4. Add permissions to your database
Provide appropriate role to the Service principal you just created. To assign the admin role, follow this guide to use the Azure portal or use the following command in your Data Explorer cluster
You can setup PostgreSQL on Azure using a variety of options including, the Azure Portal, Azure CLI, Azure PowerShell, ARM template. Once you‚Äôve done that, you can easily connect to the database using you favourite programming language such as Java, .NET, Node.js, Python, Go etc.
Although the above references are for Single Server deployment mode, please note that Hyperscale (Citus) is another deployment mode you can use for ‚Äúworkloads that are approaching ‚Äî or already exceed ‚Äî 100 GB of data.‚Äù
Please ensure that you keep the following PostgreSQL related information handy since you will need them to configure the Debezium Connector in the subsequent sections ‚Äî database hostname (and port), username, password
For the end-to-end solution to work as expected, we need to:
If you‚Äôre using Azure DB for PostgreSQL, create a firewall rule using az postgres server firewall-rule create command to whitelist your host. Since we‚Äôre running Kafka Connect in Docker locally, simply navigate to the Azure portal (Connection security section of my PostrgreSQL instance) and choose Add current client IP address to make sure that your local IP is added to the firewall rule as such:
To change the replication mode for Azure DB for PostgreSQL, you can use the az postgres server configuration command:
.. or use the Replication menu of your PostgreSQL instance in the Azure Portal:
After updating the configuration, you will need to re-start the server which you can do using the CLI (az postgres server restart) or the portal.
Once the database is up and running, create the table. I have used psql CLI in this example, but feel free to use any other tool. For example, to connect to your PostgreSQL database on Azure over SSL (you will be prompted for the password):
Use the below SQL to create the table:
The purchase_time captures the time when the purchase was executed, but it uses VARCHAR instead of a TIMESTAMP type (ideally) to reduce the overall complexity. This is because of the way Debezium Postgres connector treats TIMESTAMP data type (and rightly so!)
Over the course of the next few sections, you will setup the source (PostgreSQL), sink (Azure Data Explorer) connectors and validate the end to end pipeline.
Starting up our local environment is very easy, thanks to Docker Compose ‚Äî all we need is a single command:
This will build (and start) the order generator application container along with Kafka, Zookeeper and Kafka Connect workers.
It might take a while to download and start the containers: this is just a one time process.
To confirm whether all the containers have started:
The orders generator app will start inserting random order events to the orders_info table in PostgreSQL. At this point you can also do quick sanity check to confirm that the order information is being persisted - I have used psql in the example below:
This will give you the five most recent orders:
To stream the orders data to Kafka, we need to configure and start an instance of the Debezium PostgreSQL source connector.
Copy the JSON contents below to a file (you can name it pg-source-config.json). Please ensure that you update the following attributes with the values corresponding to your PostgreSQL instance: database.hostname, database.user, database.password.
At the time of writing, Debezium supports the following plugins: decoderbufs, wal2json, wal2json_rds, wal2json_streaming, wal2json_rds_streaming and pgoutput. I have used wal2json in this example, and it's supported on Azure as well.
To start the connector, simply use the Kafka Connect REST endpoint to submit the configuration.
Notice that port for the REST endpoint is 9090 - this is per service port mapping defined in docker-compose.yaml
Let‚Äôs peek into the Kafka topic and take a look at the change data capture events produced by the source connector.
You will be dropped into a shell (inside the container). Execute the below command to consume the change data events from Kafka:
Note that the topic name myserver.retail.orders_info is as a result of the convention used by the Debezium connector
Each event in topic is corresponding to a specific order. It is in a JSON format that looks like what‚Äôs depicted below. Please note that the payload also contains the entire schema which has been removed for brevity.
So far, we have the first half of our pipeline. Let‚Äôs work on the second part!
Copy the JSON contents below to a file (you can name it adx-sink-config.json). Replace the values for the following attributes as per your Azure Data Explorer setup - aad.auth.authority, aad.auth.appid, aad.auth.appkey, kusto.tables.topics.mapping (the database name) and kusto.url
Notice that Kafka Connect Single Message Transformation (SMT) have been used here ‚Äî this is the ExtractNewRecordState transformation that Debezium provides. You can read up on it in the documentation
It removes the schema and other parts from the JSON payload and keeps it down to only what's required. In this case, all we are looking for the order info from the after attribute (in the payload). For e.g.
You could model this differently of course (apply transformation in the source connector itself), but there are a couple of benefits to this approach:
To install the connector, just use the Kafka Connect REST endpoint like before:
Notice that port for the REST endpoint is 8080 - this is per service port mapping defined in docker-compose.yaml
The connector should spin into action, authenticate to Azure Data Explorer and start batching ingestion processes.
Note that flush.size.bytes and flush.interval.ms are used to regulate the batching process. Please refer to the connector documentation for details on the individual properties.
Since the flush configuration for the connector and the batching policy for the Orders table in Azure Data Explorer is pretty aggressive (for demonstration purposes), you should see data flowing into Data Explorer quickly.
You can query the Orders table in Data Explorer to slice and dice the data. Here are a few simple queries to start with.
Get details for orders from New York city;
Get only the purchase amount and time for orders from New York city sorted by amount
Find out the average sales per city and represent that as a column chart:
The total purchase amount per city, represented as a pie chart:
Number of orders per city, represented as a line chart:
How do purchases vary over a day?
How does it vary over a day across different cities?
Learn how to visualize data with Azure Data Explorer dashboards
To stop the containers, you can:
To delete the Azure Data Explorer cluster/database, use az cluster delete or az kusto database delete. For PostgreSQL, simply use az postgres server delete
Kafka Connect helps you build scalable data pipelines without having to write custom plumbing code. You mostly need to setup, configure and of course operator the connectors. Remember that Kafka Connect worker instances are just JVM processes and depending on your scale and requirements you can use choose to operate them using Azure Kubernetes Service. Since Kafka Connect instances are stateless entities, you‚Äôve a lot of freedom in terms of the topology and sizing of your cluster workloads!
If you want to explore further, I would recommend
ITNEXT is a platform for IT developers & software engineers‚Ä¶
80 
80¬†claps
80 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://faun.pub/microservices-in-azure-an-introduction-ba2a0fcc0385?source=search_post---------202,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is a powerful cloud computing platform designed to make big businesses agile and infinitely scalable for a fraction of the cost.
While often overlooked, Azure is used by 90% of Fortune 500 companies to build low latency, high data-volume apps. Between Azure clients like Linkedin, News Corp, and Wikipedia, you probably use an Azure app at least once per day without knowing it. The secret to these popular apps lies in Azure‚Äôs microservices support, which allows apps to be both resilient and reactive.
Today, we‚Äôll give you a quick introduction to Azure and its impressive microservices functionalities.
Here‚Äôs what we‚Äôll cover today:
Microsoft Azure, or simply Azure, is a cloud computing platform created by Microsoft. It was launched in 2010 to compete with the Google Cloud Platform and Amazon‚Äôs AWS.
Azure includes all the standard cloud services like Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS), which each offer a different scale of cloud implementation to fit each company‚Äôs unique needs.
While relatively late to the cloud platform scene, Azure offers unique functionalities:
These factors combine to make Azure a great cloud platform choice for existing mid to large-scale companies looking to get the most out of their cloud service and embrace modern DevOps techniques.
One way companies can get the most out of their cloud is to design web applications with event-driven architecture (EDA).
An event-driven architecture uses events to trigger and communicate between decoupled services. Components send events anytime a state is updated within their scope such as a user adding an item to their cart or posting a comment on a post.
This structure makes each component responsible for notifying other areas of the system about state changes. Event-driven architecture is the opposite of traditional request-driven architecture, in which the state of a component is only revealed when another component asks for its current state.
Event-driven architecture is common in systems built with microservices. Microservices architecture is a way to break down an application into separate, independent components that make it easier to maintain and scale.
In Azure, you can either use Microsoft‚Äôs Azure Service Fabric or the open-source Azure Kubernetes Service (AKS) for building microservice applications as well as handling deployment lifecycle and container orchestration.
Event-driven architecture synergizes with a microservice architecture to further increase scalability because it allows the separate components to communicate reactively regardless of their size or separation.
Some benefits of an event-driven architecture over traditional monolithic applications are as follows:
Event-driven architecture is becoming a popular design pattern because of how perfectly it meets modern needs. However, it can be difficult to get an event-driven system started because all event-handling systems must be built upfront. It‚Äôs best to only use event-driven architecture when its unique strengths fit the product‚Äôs design.
Use event-driven architectures when:
In Azure, event-driven systems are best implemented with three components: an event grid (EG), one or more FIFO queue, and a collection of Azure Functions
The event grid is the primary event processor and essentially acts as a post office that intakes, evaluates, manipulates, and redistributes any events traveling from event producer to handler. Event grids have a few major parts:
Event grids can be configured very differently based on what type/frequency of event should be allowed through. All event grids will at least have a dead-letter queue and some kind of event filter.
The dead-letter queue is a FIFO queue data structure that stores the events which did not successfully reach their handler. Once stored, the events can be resent and analyzed by developers to find possible bugs.
The event filter is a set of criteria that the Topic uses to decide which events move on to their handler and which will be blocked. It can also manipulate the event to a different type if the desired handler can only handle certain types.
If an event filter is working correctly, only actionable and applicable events will be delivered to a particular handler.
Building applications with EDAs in Azure is a great way to learn the platform. To tackle this project yourself, you‚Äôll need to complete the following 4 steps.
1. Set up an Azure Account
You‚Äôll need an Azure account to create an Azure environment and create a DevOps project workspace to practice in. You can get a free trial of Azure on Microsoft‚Äôs official site.
2. Build the foundational structure
Next, you‚Äôll need to create resource groups, an automation account, and a database.
Resource groups are containers that we can use to gather all our resources in one place. They allow you to organize how resources are managed and grouped based on your needs.
Azure accounts act as containers for your source files. Automations are a container for all your runbook, runbook executions (jobs), and the assets that your runbooks depend on.
A runbook is a collection of routine operations that help you evaluate the system.
Finally, you can create a database using the Azure-supported mySQL database software. Once you install Az.mySQL, you can enter New-AzMySqlServer into your command-line or PowerShell to initialize the server.
3. Create Event Storage
Your application needs a way to store the events it can‚Äôt complete right away. Queues are a great option because they store events in the order they‚Äôre received and ensure events aren‚Äôt skipped.
A 3 queue system is very effective for event-driven applications:
4. Add Azure functions
These functions act as an intermediary between your runbook and Event Grid. They also perform event validation and process messages.
You‚Äôll at least need the following:
This is just the first step on your journey to learn Azure. While tricky to get started, learning Azure will set you up for success when you learn other cloud platforms.
As you continue your journey, check out these intermediate topics:
Happy learning!
üëã Join FAUN today and receive similar stories each week in your inbox! Ô∏è Get your weekly dose of the must-read tech stories, news, and tutorials.
Follow us on Twitter üê¶ and Facebook üë• and Instagram üì∑ and join our Facebook and Linkedin Groups üí¨
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
84 
84¬†claps
84 
Written by
Coding is like skateboarding: you can‚Äôt learn new skills just by watching someone else. Master in-demand coding skills through Educative‚Äôs interactive courses.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium‚Äôs largest DevOps publication.
Written by
Coding is like skateboarding: you can‚Äôt learn new skills just by watching someone else. Master in-demand coding skills through Educative‚Äôs interactive courses.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium‚Äôs largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mauridb/automatic-import-of-csv-data-using-azure-functions-and-azure-sql-63e1070963cf?source=search_post---------203,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
May 30, 2017¬∑2 min read
Lately .csv (or related format, like .tsv) became very popular again, and so it‚Äôs quite common to be asked to import data contained in one or more .csv file into the database you application is using, so that data contained therein could be used by the application itself.
In the era of the Cloud, what can we do to simplify such popular requirement so that, for example, the user can just drop a file into a folder, and have it automagically imported into Azure SQL? (With small changes the same principle could be used for any database)
Azure Functions (that I love more and more every day) are the answer. Azure functions support the concept of trigger, which is an event that (as the name implies) when happens, will have an Azure Function executed, passing to that function information about the event that triggered it.
One of the available trigger is the Blob Trigger. So 40% of the work that is needed to implement what the title of this article says is already done.
The other 40% is provided by a new feature that has been added to the BULK INSERT command in Azure SQL: that ability to use a Blob Storage as a source.
The remaining 20% is the code that needs to be written to ‚Äúglue‚Äù these two functionalities together. And this is the beauty of Azure Functions! Out of 100% of the code you would have had to write, you actually have to care only for 20% of it!
The full solution, with detailed instruction on how to set it up using, when possible, the new Azure CLI 2.0, is here:
github.com
Each time a file will be saved into the Azure Blob Store‚Äôs ‚Äúcsv‚Äù folder, within a couple of seconds, if the format is the expected one, data will be available in Azure SQL for you to be used as you wish. Yeah, is that simple and easy, really!
Enjoy!
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
54 
6
54¬†
54 
6
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://posts.specterops.io/azure-privilege-escalation-via-service-principal-abuse-210ae2be2a5?source=search_post---------204,"On-prem Active Directory is here to stay, and so is Azure Active Directory. While these products have similar names, they in fact work very differently from one another. There are hints of vanilla AD‚Äôs discretionary access control model in Azure, but permissions in Azure are mostly controlled through granted roles. Regardless of these differences, configuration-based attack primitives are alive and well in Azure, and researchers discover new attack primitives in Azure all the time.
In this blog post, I‚Äôll explain how a particular kind of attack path can emerge in Azure based on Azure‚Äôs RBAC system ‚Äî an attack path we have seen in the vast majority of Azure tenants we‚Äôve gotten access to. I‚Äôll show you how the attack primitive works, how to protect yourself against it, and offer my own commentary on the opportunity this presents to Microsoft.
This work is not new ‚Äî in fact, Microsoft‚Äôs own documentation describes these attack paths. Additionally, some great researchers have discussed this attack primitive at length:
Dirk-jan Mollema wrote about abusing the Application Administrator role to add new secrets for service principals and escalate privileges here: https://dirkjanm.io/azure-ad-privilege-escalation-application-admin/
Karl Fosaaen spoke about Azure privilege escalation techniques in this talk: https://www.netspi.com/webinars/lunch-learn-webinar-series/adventures-in-azure-privilege-escalation/
Emilian Cebuc and Christian Philipov describe abusing service principals to escalate in an Azure tenant in this talk: https://www.youtube.com/watch?v=QwVApszlIdY
Before we talk about the Service Principal-based privilege escalation, I want to describe Azure‚Äôs built-in attack path prevention system. Azure Active Directory has a built-in system to protect against the emergence of attack paths, particularly around password reset privileges. When looking at the documentation for administrator roles that provide password reset privileges, you will often see wording like this:
Source: https://docs.microsoft.com/en-us/azure/active-directory/roles/permissions-reference#helpdesk-administrator
In particular, note the highlighted text: ‚ÄúWhether a Helpdesk Administrator can reset a user‚Äôs password and invalidate refresh tokens depends on the role the user is assigned.‚Äù
This, to me, is confusing. So I set out to understand all the different possibilities for password reset privileges in Azure, and wound up creating this table:
This brings a little more clarity, but look at what happens when we model these permissions using a graph:
This is where the brilliance of this system finally becomes clear. Looking at the top row, we can see that Global Admins and Privileged Authentication Admins can reset each others‚Äô, plus all other users‚Äô passwords, but the opposite is not true. In other words, only GA and PAA users can reset a GA password. This is part of the back-end Azure system and something that Azure admins have no control of.
I love this system because it provides a highly effective, non-configurable, frictionless safety rail for admins that prevents the emergence of attack paths that include resetting a Global Admin‚Äôs password. Azure admins can safely dole out the ‚ÄúPassword Admin‚Äù role without worrying about the safety of their Global Admins ‚Äî at least within the context of this particular attack primitive.
When you create or register an application in your Azure tenant, an application object is created with a unique application (client) ID:
Here, the app ‚ÄúMyCoolApp‚Äù has a unique identifier starting with d6f118bc. Below that, you can see the ID of my tenant as well. This app object ‚Äúresides‚Äù within my tenant. Let‚Äôs start to think of this in terms of a graph and see how the attack path emerges:
Because the app ‚Äúresides‚Äù in my tenant, anyone with control of my tenant has control of the app. Let‚Äôs model this with showing how my own Global Admin user has control of the tenant:
Nothing scary here: my user is a global admin against the tenant, the tenant contains the app, therefore the ‚Äúpath‚Äù here is that I have control of the app. Now let‚Äôs extend this model to include service principals.
Azure apps needing to authenticate to the tenant to perform some action do so using an object called a Service Principal. Service Principals work kind of like users ‚Äî you authenticate to the tenant with a ‚Äúusername‚Äù (object id) and a ‚Äúpassword‚Äù (a certificate or secret). You can see the object id of the app‚Äôs service principal in this screenshot:
Let‚Äôs go ahead and add that to our graph model:
Again, nothing surprising here: my Global Admin user has control of everything here.
Service Principals can have admin roles in Azure just like users. For example, we have seen several Azure environments where at least one Service Principal has the Privileged Role Admin role. Let‚Äôs recreate that in my own tenant:
And let‚Äôs extend the graph model to include this:
Again, nothing scary: an attack path from a Global Admin to the PRA role isn‚Äôt a problem. Let‚Äôs bring another user into the mix: Alice App Admin. Like the user‚Äôs name suggests, we will grant this user the ‚ÄúApplication Administrator‚Äù role in my tenant:
Let‚Äôs also add this user and their role assignment into our graph model:
And‚Ä¶ there it is. Our attack path has emerged, connecting the Alice App Admin user to the PRA role. The attack path works like this:
Here‚Äôs a video of the attack path in action, escalating from ‚ÄúApplication Administrator‚Äù to ‚ÄúGlobal Administrator‚Äù:
As discussed earlier, Azure has built-in mechanisms to prevent privilege escalation through role-based attack paths. Only those users with Global Admin or the Privileged Authentication Administrator role can reset a Global Admin‚Äôs password. This simple, brilliant mechanism provides a highly effective safety rail to protect Azure admins from unknowingly introducing attack paths into their tenants. But this mechanism could go further.
I believe the best possible prevention against the privilege escalation technique described in this blog post is for Microsoft themselves to extend this mechanism to cover service principals as well. Just as the system provides built-in protections for Global Admin users, Azure could (perhaps should) also provide built-in protection for Global Admin service principals. I don‚Äôt know the inner-workings of this system and whether this extension is even technically feasible, but I believe Microsoft can completely eliminate this attack primitive by extending the system to cover service principals, creating an even more secure platform for all of their customers.
In the meantime, Azure administrators should audit roles held by service principals, and determine the exposure of those service principals to the rest of their identities.
Azure admins can prevent this attack path by auditing roles held by service principals and comparing those roles to the other identities with control of apps. The most dangerous role in Azure is ‚ÄúGlobal Admin‚Äù, so let‚Äôs start there. Open the Azure portal, navigate to your tenant, then click ‚ÄúRoles and Administrators‚Äù:
Scroll down to and click on ‚ÄúGlobal Administrator‚Äù. Here, you will see the list of identities with this role currently activated. In the ‚Äútype‚Äù column, look for ‚ÄúServicePrincipal‚Äù entries:
There are two service principals in my tenant with the Global Admin role: ‚ÄúCanYouAddASecretToMe‚Äù and ‚ÄúThisAppHasGlobalAdminRole‚Äù. These are the names of the service principals, and they are also the names of the apps associated with these service principals.
This is your first prevention opportunity: determine whether these apps actually need Global Admin rights in order to function. If these are third party apps, work with your vendor to get this level of privilege reduced to only the level required for the application to function.
If these roles must persist, then your next step is to look at the identities in your tenant with the tenant-level roles that grant a principal the ability to abuse service principals. Those roles are:
Additionally, these legacy/hidden roles can be abused to take over service principals:
Let‚Äôs audit the ‚ÄúApplication Administrator‚Äù role by navigating to our tenant, clicking ‚ÄúRoles and Administrators‚Äù, then click on the Application Administrator role:
Here we see all users and service principals with this role currently active. This is your second opportunity for prevention: do all these users need the ability to manage credentials for all service accounts in your tenant?
We also need to check the app-level roles and per-app owners. Earlier we saw that the service principal, ‚ÄúThisAppHasGlobalAdminRole‚Äù, was currently a Global Admin. Navigate to your tenant, click on ‚ÄúApp registrations‚Äô‚Äô, click on ‚ÄúAll Applications‚Äù, then find the app associated with that Service Principal:
First, click on ‚ÄúOwners‚Äù, which will show you all principals that own the app (there can be more than one):
Do you trust Matt Nelson to add a secret to an app with Global Admin powers? I wouldn‚Äôt.
Next, click on ‚ÄúRoles and administrators‚Äù, which will show the app-level roles particular to this app. Click on ‚ÄúCloud Application‚Äù administrator, for example, which will show the principals that have this role against the app both explicitly (scoped to the app), and where that role inherits down from the tenant:
These are your third and fourth opportunities for prevention: do you trust all these principals to have control of a service principal with Global Admin rights?
You should repeat this process for Service Principals that hold the following roles:
This is also the point where the built-in password reset protections against Global Admins can break, creating attack paths from low-privilege users all the way to Global Admin by abusing control of service principals. In the real world, this is the most common avenue of escalating to Global Admin we have seen without needing to pivot down to on-prem AD first.
Going through this process manually, you may find yourself asking, ‚ÄúIs there one screen that tells me all roles held by any Service Principals?‚Äù To my knowledge, no such screen in Azure Portal exists, but you can use the Microsoft-authored PowerShell cmdlets to answer this question:
This will produce output like this:
This can make it much easier to quickly identify Service Principals with dangerous admin roles.
Microsoft is continually making improvements to Azure, whether those improvements are related to uptime, features, or security. There is an opportunity here for Microsoft to extend its existing design choices related to password reset rights to also cover Service Principals. I do not know the inner-workings of how that system works, or whether it‚Äôs even feasible to extend that system to cover Service Principals. Either way, this is a great opportunity for Microsoft to protect all of its customers and make Azure an even more secure platform.
Posts from SpecterOps team members on various topics‚Ä¶
38 
38¬†claps
38 
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
"
https://medium.com/@tsuyoshiushio/enable-different-configuration-for-multi-agent-pipeline-with-azure-devops-dca5b5a786ff?source=search_post---------205,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Mar 6, 2019¬∑4 min read
Some project has a testing that we can not run parallel. In my case, one test create and remove a blog and others refer that. Obviously it can‚Äôt run at the same time. However, you might want to reduce the testing time with multi agent strategy. You can change the connection string per agent.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-pass-the-microsoft-azure-fundamentals-az-900-exam-854f93de5541?source=search_post---------206,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is one of the leading cloud providers. This exam can be taken as an optional first step in learning about cloud services and how those concepts are exemplified by Microsoft Azure. It can be taken as a precursor to Microsoft Azure or Microsoft cloud services exams. While it‚Ä¶
"
https://medium.com/bb-tutorials-and-thoughts/building-an-angular-app-with-azure-static-web-apps-service-8fe84ebe4709?source=search_post---------207,"There are currently no responses for this story.
Be the first to respond.
Nowadays there are so many ways to build and deploy Angular apps such as angular with Java, Angular with Nodejs, serverless, etc. Building with Azure Static Web apps service is one of them and it is recently released by Microsoft Azure and it's in the preview mode. With this service, you can‚Ä¶
"
https://medium.com/hackernoon/azure-sql-transient-errors-7625ad6e0a06?source=search_post---------208,"There are currently no responses for this story.
Be the first to respond.
One thing people will notice when migrating to using cloud services are intermittent errors occur at a higher rate than you‚Äôre used to when running on premise.
These errors are often called transient errors and can be mitigated by just retrying the operation. While some operations might be fine to retry in all cases i.e. fetching data, others like creating orders, deducting money, etc. might not or at least need a little more fine grain control. Also, some errors makes no sense to retry and as a result it‚Äôs better to fail fast.
Writing the code to handle can easy turn to spaghetti code and it isn‚Äôt code you want to copy paste as it‚Äôs easy to get wrong and hard to maintain ‚Äî so you really do want a framework to assist you with these scenarios.
This is how the authors describe Polly:
Polly is a .NET resilience and transient-fault-handling library that allows developers to express policies such as Retry, Circuit Breaker, Timeout, Bulkhead Isolation, and Fallback in a fluent and thread-safe manner.
Which really sums up what it does, it lets you create policies using a fluent API, policies which then can be reused and can range from being generic to target very specific scenarios.
Polly is available as NuGet package and works with both old .NET and shiny .NET Core, which will be what I‚Äôm using in this post.
Also worth noting Polly is open source available on GitHub.Polly‚Äôs also member of the .NET Foundation.
Defining a policy with Polly is really straightforward in the Polly namespace you Policy class which is what you utilize to create your policies
Above we‚Äôve via the Handle<TException>method defined a policy that if a TimeoutException occurs it‚Äôll wait 3 seconds and retry the operation 5 times before failing on the 6th time which is defined using the WaitAndRetry method.
Because the delay of each retry attempt in this case is retrieved using a Func<int, TimeSpan> delegate it‚Äôs really easy to implement more advanced scenarios like a sliding delay
to let our policy handle more exceptions we can use one to manyOr<TException> method after theHandle<TException> call
you can also make your policy even more granular by inspecting the exception with an exception predicate
In this case we‚Äôll fail fast if it‚Äôs a .tmp file that‚Äôs not found. You can also set policies based on result values, besides retries there‚Äôs also support for circuit-breaker, timeout, fallback, composing policies together and more, so the policy ‚Äúengine‚Äù is very flexible.
We now have our policy in place and want to put it into good use, here the policies Execute method comes in play
You can also create asynchronous policies to and then we can utilize the policies ExecuteAsync method
It‚Äôs almost too good to be true, there‚Äôs a lot happening under the hood ‚Äî but in my opinion ‚Äúhidden‚Äù under a very easy to understand and maintainable API.
When is comes to SQL server there‚Äôs a few errors known to be safe to retry, so we explicitly look for those, so if we begin with it could look something like this:
where SqlRetryCount is just a constant for how many retries to do and ExponetialBackoff if a method that exponentially increases the delay between each attempt
TimeoutException:s will always be retried but SqlExceptions will be passed to AnyRetryableError method for assessment if it‚Äôs retryable or not
AnyRetryableError will iterate all errors and with the RetryableError method check if it‚Äôs an error known to be retriable and here is where the magic happens
we will switch on SqlException error number and use a few known constants to determine if it should fail fast or retry.
In this case, I‚Äôm going to do a simple API that extends SqlCOnnection with WithRetry methods, i.e. open SQL connection
giving us an open sync and async method, which in its simplest form usage could look something like this
combining with a simple SQL to .NET object mapper like Dapper results in pretty clean code
the above code will open the sql connection using the retry policy, but the query will be executed without retry, which could be what you want for some operations, but for a select it‚Äôs often safe/what you want to retry the whole operation. For that we add an overload that lets us execute code within the policy boundary
this will open the connection and then invoke and return the result of Func you pass to it all within the scope of policy and just small refactoring of the calling code
and async variant usage could look something like this
Polly is really powerful, yet really easy to get started with and also fairly easy to retrofit into an existing application ‚Äî so I definitely think you should take it for a spin.
I‚Äôve created a complete sample repository on GitHub with the code from this post, thanks for reading! ‚ù§
github.com
Hacker Noon is how hackers start their afternoons. We‚Äôre a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don‚Äôt take the realities of the world for granted!
#BlackLivesMatter
28 
3
28¬†claps
28 
3
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Partner & Technical fellow at WCOM AB. Microsoft Azure & Developer Technologies MVP. Been coding since I 80‚Äôs (C128 & Amiga). Father of 2, husband of 1.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/microsoftazure/credit-card-fraud-detection-with-azure-data-science-virtual-machine-a121dc2fd1b5?source=search_post---------209,"There are currently no responses for this story.
Be the first to respond.
This blog post is co-authored by Jaya Mathew and Francesca Lazzeri, data scientists at Microsoft.
People from across the data world came together in New York last week for the Strata Data Conference. In our session ‚ÄúA day in the life of a data scientist: How do we train our teams to get started with AI?‚Äù, we presented a scientific framework to help organizations improve data science skill set, systematically discover opportunities to create value from data, qualify new opportunities and assess their fit and potential, smoothly implement end-to-end advanced analytics pilots and projects, and produce sustainable ongoing business value from data.
We also walked through a detailed credit card fraud detection use case, from how the data typically gets collected to data wrangling, building a model, tuning the model, and operationalizing the model for a business to use in their production environment.
The goal of this blog post is to share with you more details on this end-to-end credit card fraud detection solution that we built using Python and Azure Data Science Virtual Machine.
Recent advancements in computing technologies along with the increasing popularity of eCommerce platforms have radically amplified the risk of online fraud for financial services companies and their customers. Failing to properly recognize and prevent fraud results in billions of dollars of loss per year for the financial industry. This trend has urged companies to look into many popular artificial intelligence (AI) techniques, including deep learning for fraud detection. Deep learning can uncover patterns in tremendously large data sets and independently learn new concepts from raw data without extensive manual feature engineering. For this reason, deep learning has shown superior performance in domains such as object recognition and image classification.
For this solution we used a sample data set from Kaggle that contains transactions made by credit cards in September 2013 by European cardholders. These transactions occurred in two days:
The data set can be summarized as follow:
¬∑ Features V1, V2, ‚Ä¶ V28: are the principal components obtained with PCA, the only features which have not been transformed with PCA are ‚ÄòTime‚Äô and ‚ÄòAmount‚Äô
¬∑ Feature Time: contains the seconds elapsed between each transaction and the first transaction in the dataset
¬∑ Feature Amount: is the transaction Amount
¬∑ Feature Class: is the response variable and it takes value 1 in case of fraud and 0 otherwise
For this scenario we used a specific type of neural network called Autoencoder. This neural network is trained to attempt to copy its input to its output. Internally, it has a hidden layer h that describes a code used to represent the input.
The network may be viewed as consisting of two parts:
¬∑ an encoder function h = f(x)
¬∑ a decoder that produces a reconstruction r = g(h)
We optimize the parameters of our autoencoder model in such way that a special kind of error, reconstruction error is minimized.
To build our solution, we used a Data Science Virtual Machine, that is a Windows Azure virtual machine (VM) image. It is preinstalled and configured with several tools that are used for data analytics and machine learning. The Data Science Virtual Machine jump-starts your analytics project. You can work on tasks in various languages including R, Python, SQL, and C#.
To create an instance of the Microsoft Data Science Virtual Machine, follow these steps:
¬∑ Navigate to the virtual machine listing on the Azure portal. You may be prompted to login to your Azure account if you are not already signed in.
¬∑ Select the Create button at the bottom to be taken into a wizard.
¬∑ The wizard that creates the Microsoft Data Science Virtual Machine requires input. The following input is needed to configure each of the steps shown on the right of the figure:
a. Basics:
i. Name. The name of the data science server you‚Äôre creating.
ii. VM Disk Type. Choose SSD or HDD. For an NC_v1 GPU instance like NVidia Tesla K80 based, choose HDD as the disk type.
iii. User Name. The admin account ID to sign in.
iv. Password. The admin account password.
v. Subscription. If you have more than one subscription, select the one on which the machine is to be created and billed.
vi. Resource Group. You can create a new one or use an existing group.
vii. Location. Select the data center that‚Äôs most appropriate. For fastest network access, it‚Äôs the data center that has most of your data or is closest to your physical location.
b. Size. Select one of the server types that meets your functional requirements and cost constraints. For more choices of VM sizes, select View All.
c. Settings:
i. Use Managed Disks. Choose Managed if you want Azure to manage the disks for the VM. If not, you need to specify a new or existing storage account.
ii. Other parameters. You can use the default values. If you want to use nondefault values, hover over the informational link for help on the specific fields.
d. Summary. Verify that all the information you entered is correct. Select Create.
We developed our model using Python and Azure Notebooks. First of all, you need to prepare your environment and import the necessary components:
You can now enter the credentials to access the data from the cloud and then download the file for analysis:
Import the credit card data set:
For the modelling piece, you first need exclude the variable ‚ÄòTime‚Äô. Since the spread of the variable ‚ÄòAmount‚Äô is large, this variable is standardized. Then you have to define the framework for the autoencoder and then compile and fit using the training data:
Finally, you can save your model:
In this blog post, we dived into a specific credit card fraud detection use case. Most importantly, we showed how the right cloud analytics environment, such as an Azure Data Science Virtual Machine, makes it easy to collect data, analyze, experiment, and build a model for any organization to use in a production environment.
¬∑ Strata Data NYC Slide Deck: https://www.slideshare.net/FrancescaLazzeriPhD/a-day-in-the-life-of-a-data-scientist-how-do-we-train-our-teams-to-get-started-with-ai
¬∑ Data Science Virtual Machine: https://aka.ms/AzureDSVM
¬∑ Data Source for demo: https://www.kaggle.com/mlg-ulb/creditcardfraud
¬∑ Blog Post by Venelin Valkov: https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd
¬∑ Deep Learning Book by Ian Goodfellow, Yoshua Bengio, Aaron Courville: http://www.deeplearningbook.org/
Any language.
50 
1
50¬†claps
50 
1
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pluralsight/featured-channel-everything-net-developers-should-know-about-azure-8df78684f4b6?source=search_post---------210,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pluralsight
Dec 8, 2017¬∑1 min read
We are experiencing a monumental shift towards cloud computing. As more data is moved to the cloud, Microsoft Azure‚Äôs footprint increases. Pluralsight expert and Microsoft MVP Scott Allen has created a custom channel to help .NET developers get up to speed on all things Azure.
Channel courses:
Microsoft Azure for Node.js Developers ‚Äî Cloud Patterns and Architecture
Microsoft Azure for Node.js Developers ‚Äî Building Secure Services and Applications
Developing with .NET on Microsoft Azure ‚Äî Getting Started
Follow Scott‚Äôs channel now.
Channels offer Pluralsight users a focused, guided way to learn. With the ability to create, follow and share channels, skilling up and accomplishing goals becomes easier and more customizable. Learn more about channels and the other products that comprise our platform.
We are *the* technology learning platform, dedicated to helping teams create the future.
114 
1
114¬†
114 
1
We are *the* technology learning platform, dedicated to helping teams create the future.
"
https://medium.com/avalanche-hub/setting-up-an-avalanche-node-with-microsoft-azure-cheaper-than-aws-2683c5d3d197?source=search_post---------211,"There are currently no responses for this story.
Be the first to respond.
Running a validator and staking with Avalanche provides extremely competitive rewards of between 9.69% and 11.54% depending on the length you stake for. The maximum rate is earned by staking for a year, whilst the lowest rate for 14 days. There is also no slashing, so you don‚Äôt need to worry about a hardware failure or bug in the client which causes you to lose part or all of your stake. Instead with Avalanche you only need to currently maintain at least 60% up time to receive rewards. If you fail to meet this requirement you don‚Äôt get slashed, but you don‚Äôt receive the rewards. You also do not need to put your private keys onto a node to begin validating on that node. Even if someone breaks into your cloud environment and gains access to the node, the worst they can do is turn off the node.
Not only does running a validator node enable you to receive rewards in AVAX, but later you will also be able to validate other subnets in the ecosystem as well and receive rewards in the token native to their subnets.
You only need modest hardware requirements of 2 CPU cores, 4 GB Memory and 40 GB SSD to run a validator and it doesn‚Äôt use enormous amounts of energy. Avalanche‚Äôs revolutionary consensus mechanism is able to scale to millions of validators participating in consensus at once, offering unparalleled decentralisation.
Currently the minimum amount required to stake to become a validator is 2000 AVAX (which can be reduced over time as price increases). Alternatively, validators can also charge a small fee to enable users to delegate their stake with them to help towards running costs. You can use a calculator here to see how much rewards you would earn when running a node, compared to delegating.
I encourage everyone to run their own validators where possible, but for those that don‚Äôt meet the minimum staking requirements and want to delegate I am currently running a node which you can find below:
avascan.info
In this article we will step through the process of configuring a node on Microsoft Azure. This tutorial assumes no prior experience with Microsoft Azure and will go through each step with as few assumptions possible.
At the time of this article, spot pricing for a virtual machine with 2 Cores and 8 GB memory costs as little as $0.01060 per hour which works out at about $113.44 a year, a saving of 83.76%! compared to normal pay as you go prices. In comparison a virtual machine in AWS with 2 Cores and 4 GB Memory with spot pricing is around $462 a year.
docs.avax.network
Now that we have a node, it‚Äôs time to set up our node to be a validator on the network. There‚Äôs already a great tutorial on that written by Rado Minchev, so we‚Äôll head over there next and declare our node to be a validator for Avalanche!
medium.com
Please see this excellent article / tool from community member Burcusan, on how to set up real-time alerts from your Avalanchego Validator Node. You can receive Telegram alerts if there is an issue with your node and further action is required.
medium.com
To make your node more recognisable for potential delegators to select to trust their stake with, consider verifying the identity of your node with the following providers Avascan, VScout, and Stakingrewards
Avalanche, a Revolutionary Consensus Engine and Platform. A Game Changer for Blockchain
Avalanche Consensus, The Biggest Breakthrough since Nakamoto
Comparison between Avalanche, Cosmos and Polkadot
Why Avalanche (AVAX) has the potential to be an incredible store of value
Avalanche 101: An Overview of the Internet of Finance
Avalanche Hub
316 
1
316¬†claps
316 
1
Avalanche Hub is the Community Growth Platform: from expansion efforts to open-source development, members directly contribute to vital research, education, and engineering initiatives. The Avalanche Hub is also on Twitter -> https://twitter.com/AvalancheHub
Written by
DLT Enthusiast and Writer. Interoperability is key for DLT to achieve its true potential. Avalanche $AVAX, Injective Protocol $INJ and Quant $QNT
Avalanche Hub is the Community Growth Platform: from expansion efforts to open-source development, members directly contribute to vital research, education, and engineering initiatives. The Avalanche Hub is also on Twitter -> https://twitter.com/AvalancheHub
"
https://medium.com/s-c-a-l-e/azure-cto-on-how-linux-and-devops-are-remaking-microsoft-s-cloud-d1a6436f1fce?source=search_post---------212,"There are currently no responses for this story.
Be the first to respond.
Mark Russinovich rose to fame in the 1990s as an expert in Microsoft Windows, helping users get the most out of the operating system, while also uncovering some serious issues with it. In 2006, he joined Microsoft to help improve the operating with which he was so familiar.
Now, Russinovich is CTO of Microsoft Azure. In this interview, he discusses what makes a good cloud platform and how new cloud-native application architectures are forcing enterprises to once again view IT as a competitive advantage
SCALE: How did you make the move from working on Windows ‚Äî Microsoft‚Äôs desktop and server operating system ‚Äî to working on Azure, its cloud operating system?
MARK RUSSINOVICH: I worked on Windows through the end of Vista and then Windows 7. I was looking at Windows 8, but at that point in time I stuck my head up and was talking with people in the company and seeing the changes that the industry was starting to go through.
The first change was the mobile change, which was happening while I was in Windows. I was working on making sure that Windows could adapt to that mobile change. Some of the things that you‚Äôre seeing, like Windows show up on small devices, that was some of what I was doing back then.
When I stuck my head up, I saw that there was another aspect to the mobile transformation that we‚Äôre going through, and that was the cloud. You have this periphery of mobile devices ‚Äî not just phones, but devices that are now called Internet of Things devices, and headless devices that are sensors and controllers. All of these devices need to connect and store and process data someplace off the device. That is the cloud.
As I was talking with the people around the company, including some who had been in Windows and left to start this project called ‚ÄúRed Dog,‚Äù which became Azure, I realized there was a huge opportunity be part of that side of the disruption ‚Äî to be at the center of the constellation of devices. That‚Äôs what attracted me to come over to Azure.
They might appear different, but Azure is what I consider an operating system for the cloud. It is a platform for the cloud. A lot of the learnings that I had for how to design a platform for a single system, like Windows, are directly applicable to a cloud platform.
Now, there‚Äôs a lot of things that are different, of course. You‚Äôre talking about massive-scale, distributed systems. Essentially the same problems that a technology like Mesos addresses. That‚Äôs also what made it exciting to me.
‚ÄúI‚Äôve heard people sour on the term DevOps in the last few months, and talk about it being an empty buzzword. I actually believe that it‚Äôs a meaningful word.‚Äù
What‚Äôs the defining feature of a cloud platform? Is it something like scale or ‚Äúcomposability?‚Äù
I think scale-out, not scale-up, is one. Designing for failure is another one. If I was to lead with two principles for what cloud is, from a technology perspective, it would be those.
What‚Äôs the co-engineering like between the cloud and on-premises worlds within Microsoft?
A big strength of Microsoft is the hybrid play. The fact is that we do have these technologies that customers are using on-premises, and we‚Äôre using the exact same core pieces to bring those technologies up into the cloud. The SQL engine that runs behind Azure Database is the exact same one that runs the SQL Server boxed product. The engineering team is consolidated.
This kind of sharing of technologies allows us to take things back and forth from the on-premises to the cloud. Actually, more and more, it‚Äôs cloud-first. We have a ‚Äúcloud-first‚Äù mantra at the company, which means we take more and more things back to the on-prem world from the cloud.
A place where you see us actually doing that is something we call Azure Stack, which is taking a subset of core Azure services and making them available for customers to deploy on-premises. We actually think that having the boxed-product technologies, and the connections to the companies using those, is a huge strength for us.
Does Microsoft have customers actually using these hybrid tools in production?
Absolutely. Not Azure Stack yet, because it‚Äôs not released but the predecessor to it called Azure Pack. We‚Äôve had that out for about a year and a half, and we‚Äôve got thousands of customers using it.
‚ÄúWhat you‚Äôre hearing now is that every business segment is being disrupted by somebody like an Airbnb or an Uber, and to be competitive, the companies that aren‚Äôt necessarily born in the cloud need to operate like the ones that were.‚Äù
I think this is another benefit that we‚Äôve got in Azure, actually. Not just things like machine learning, but first-party applications like Dynamics and Office 365, which are SaaS applications. If you think about any platform you‚Äôre building, it‚Äôs not just about supporting operating system services. What really drives a great platform are first-party applications where you‚Äôre getting direct feedback from people building big apps. In this case, something like Office 365 is much bigger and more demanding than the vast majority of our customers will ever be.
By making sure those applications work right on the platform, that makes sure that any applications that most of these enterprises or CSVs are going to be building will work great on the platform.
‚ÄúI‚Äôve heard people sour on the term DevOps in the last few months, and talk about it being an empty buzzword. I actually believe that it‚Äôs a meaningful word.‚Äù
Can walk me through the history of those principles at Microsoft? I don‚Äôt Azure was the company‚Äôs first experience with building for scale.
I think that to be able to design for hyperscale, and be able to adapt to failures and deliver services, you need to adopt a DevOps way of development and operations. I‚Äôve heard people sour on the term DevOps in the last few months, and talk about it being an empty buzzword. I actually believe that it‚Äôs a meaningful word that really represents the transformation that Microsoft, starting with groups like Azure, has undergone over the last 5 to 10 years. Which is to move from a boxed-product way of operating to a world where we develop and operate the software, and have to do that in a very agile way to get new features and functionality out.
That requires culture, that requires tooling, and that requires the processes that support that ‚Äî which are very different than the equivalents that you have for operating efficiently in the boxed-product world.
‚ÄúWhat you‚Äôre hearing now is that every business segment is being disrupted by somebody like an Airbnb or an Uber, and to be competitive, the companies that aren‚Äôt necessarily born in the cloud need to operate like the ones that were.‚Äù
One of my criticisms early on was that companies were just rebuilding old architectures on top cloud platforms. Do you see customers now coming around to the idea of building applications in new ways?
I think that you saw cloud providers like us realize it, but it isn‚Äôt just in the context of cloud providers. It‚Äôs in internet service providers that have been living in this world for a long time. That forces you to discover these things because, at the kind of hyperscale that we‚Äôre getting to, you can‚Äôt operate any other way. We were forced to adapt to a new way of operating in order to accomplish the things that we needed to accomplish.
If you take a look at the early adoption of cloud among enterprises, a lot of it was taking what they had and moving it to the cloud. There‚Äôs not a lot of major new architecture, necessarily, going on with a lot of those projects. But enterprises are also starting to realize that to stay competitive in this world where they are going to have a competitor that‚Äôs taking advantage of a DevOps model, they need to also adopt a DevOps model. If you‚Äôve got a competitor that is in the segment that you‚Äôre in, that is adopting things like containers and orchestration, and DevOps delivery of those applications, they‚Äôre going to have an advantage against you if you‚Äôre still operating in this traditional IT way of doing things.
That recognition is driving enterprises to take these things seriously and adopt them. What you‚Äôre hearing now is that every business segment is being disrupted by somebody like an Airbnb or an Uber, and to be competitive, the companies that aren‚Äôt necessarily born in the cloud need to operate like the ones that were.
‚ÄúLast fall, it was one in five virtual machines were Linux virtual machines. Now, it‚Äôs up to one in four.‚Äù
Is there a challenge for Microsoft trying keep competitive with a company like Amazon Web Services, which runs primarily Linux instances and has a perception of catering to a new generation of developers?
For one, I want to point out that our public cloud, Azure, is not a Windows-centric public cloud. It‚Äôs a system-agnostic one. That‚Äôs the way that we‚Äôre operating, and we‚Äôve been operating that way for a long time. The use of Linux in Azure is reflecting that. Last fall, it was one in five virtual machines were Linux virtual machines. Now, it‚Äôs up to one in four.
We are being recognized as a good place to run Linux, and the usage is reflecting that. From our perspective, if people want to run Linux and want to build their hyperscale applications on Linux, that‚Äôs great. We want Azure to be a fantastic place to do that. If they want to do it on Windows, we want to make sure that Azure is a great place to do that, as well.
And we also want to make sure that our Windows customers can take advantage of microservices and DevOps and containers the same way that someone that chooses Linux can take advantage of them. That‚Äôs why we‚Äôre building container technologies into Windows and why we‚Äôre doing these partnerships with Docker and Mesosphere to bring those technologies to the Windows space.
Microsoft and Mesosphere have successfully ported Mesos onto Windows. Why is this significant?
It‚Äôs significant because the orchestration layer is the place where we‚Äôre seeing a lot innovation and need for tooling and support for these new cloud application models. Mesos is a very popular option here, and bringing Mesos‚Äôs capabilities to Windows unlocks this new style of orchestration for Windows developers. And any organization that has applications mixed between Linux and Windows servers can now allocate a pool of resources and them manage them holistically using the same interfaces and the same deployment technologies. So we‚Äôre really excited about bringing that to the Windows world.
We‚Äôve already supported Mesos on top of Azure, something that was done using the resource group templates that we co-developed with Mesosphere. That‚Äôs an example of bringing Linux technologies and Mesos onto the Azure platform because our customers were asking for it.
What considerations are you looking as you continue to evolve Azure? Are there certain technologies or use cases driving its evolution?
It‚Äôs actually kind of challenging because we‚Äôve got very diverse use cases and very diverse applications that are taking advantage of the cloud. You mentioned one, which is what we call lift and shift, which is enterprises taking their existing workloads and architectures and moving them into the cloud. That requires a very special architecture, because you have to make the cloud look a lot like an on-premises environment.
Then, we also have the new, modern, cloud-native applications. They are hyperscale, scale-out, very much designed for resilience, and are taking advantage of things like microservices and the agility that those provide. Those have very different requirements, as well.
‚ÄúWhile we do have a commitment to match some of the core infrastructure pricing with Amazon, we don‚Äôt hear pricing come up in conversations with customers.‚Äù
How much does data, or big data, as a use case affect your day-to-day decision making and your future planning?
Data is a key part of the cloud scenarios. It‚Äôs also another great example of how we got very diverse types of workloads requiring different types of data technologies. From the lift-and-shift or OLTP type workloads that enterprise potentially use, to the massive scale-out, NoSQL-type solutions, to the large data stores like we‚Äôve got with our data lake offering. There is just a full spectrum of different technologies addressing all of these that we‚Äôve got to make sure we support and offer.
One of the big story lines in the past few years has been these ‚Äúprice wars‚Äù among the cloud providers. Is that something that‚Äôs going to come to an end at some point?
There was a lot of focus on pricing about a year and a half ago. A lot of news about price wars and this cloud provider dropping, another one matching. And while we do have a commitment to match some of the core infrastructure pricing with Amazon, we don‚Äôt hear pricing come up in conversations with customers, and we don‚Äôt see the industry at large ‚Äî or analysts or customers ‚Äî looking and focusing on that. It‚Äôs more about features and high-value services at this point.
‚ÄúPeople talk about how every enterprise is now a data company, every enterprise now needs data scientists.‚Äù
Is that because people have just become familiar with how the cloud operates and how to architect for it, or with the differences among providers?
I think we‚Äôll be seeing a lot more of the cluster-oriented microservices applications, as far as compute goes. And we‚Äôll see more and more of the connected services in pipelines. IoT‚Äôs another area that we see a huge amount interest and momentum in, where you‚Äôve got a whole bunch of cloud services connected together to deliver an IOT solution ‚Äî from event-ingestion to live stream analysis, to dumping huge amounts of data into some store. You can then come along later and do data processing on top of it, and machine learning on top after that, to drive feedback that goes out to those devices.
We talked about the data services. Does the cloud provider have all of the data services that I‚Äôm going to be needing? What are the SLAs behind them? Can they support the hybrid scenarios that I‚Äôve got in mind? These are the high-order bits, much more so than pricing at this.
If you were to look at Microsoft, the average Windows workload a decade ago and then project out a few years, what‚Äôs the difference in terms of what they might look like?
Absolutely. I think if you just take a look at that website-plus-database model, it came about because that‚Äôs the way that IT has created those applications. Once you can add these other services, like data analytics services, across your databases, websites and the different properties that your enterprise has, then you unlock a lot more learning.
What I just described there touches on a whole bunch of services. Everything from infrastructure-as-a-service to platform-as-a-service to machine learning. We‚Äôre going to see a lot more of those kinds of applications.
So if the past was about a web server talking a database, the future is more about data pipelines and connected systems?
Absolutely. AI think if you just take a look at that website-plus-database model, it came about because that‚Äôs the way that IT has created those applications. Once you can add these other services, like data analytics services, across your databases, websites and the different properties that your enterprise has, then you unlock a lot more learning.
People talk about how every enterprise is now a data company, every enterprise now needs data scientists. It goes to that, which is all my applications, even if they‚Äôre a web database, can make use of data analysis to drive better value. I mean, even if I‚Äôm just looking at the traffic on the website, or looking at the access patterns to the data, I can learn something about my customers, how to optimize my application and how to optimize my business.
What‚Äôs next in computing, told by the people behind the‚Ä¶
26 
1
26¬†claps
26 
1
What‚Äôs next in computing, told by the people behind the software
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
What‚Äôs next in computing, told by the people behind the software
"
https://medium.com/hackernoon/migrating-wemove-co-from-azure-to-ubuntu-vm-on-digital-ocean-96280ce24777?source=search_post---------213,"There are currently no responses for this story.
Be the first to respond.
Back in July 2016, when I started writing the code of what will become WeMove.co, I chose to use ASP.NET Core because C# is my go-to static language (I blogged about how I came upon that choice here: http://ezeokoyecelestine.blogspot.com.ng/2016/07/microsofts-confusing-dilemma-with.html). ASP.NET Core was new and very edgy at the time, I was using it at version 1.0 and I faced some really interesting issues. As I alluded in the post, Microsoft did sort out the issues and yesterday, I successfully migrated our website from Azure to Ubutu VM on Digital Ocean.
Long story short: To cut down expenses.
On January 2nd, WeMove Technologies moved into our new office at Bode Thomas Surulere ‚Äî a thriving business unit in Lagos. Running a business in Lagos, it‚Äôs imperative that we cut cost as much as possible. So it‚Äôs important I think medium term on how we can fix costs. One of the major strategies for me was either joining the Microsoft BizSpark program or getting rid of Azure, as the cost had started piling.
I‚Äôd applied for BizSpark since November and having not gotten any response, I decided to start shopping for an alternative cloud service. Azure‚Äôs PaaS is my go-to hosting service because of the ease of setting-up and deployment, directly from my BitBucket repo. Letting go meant I had to set-up a build pipeline while moving.
Even though the primary driver is reducing the cost of business, a close number 2 reason for migration was reduction of error in the future.
For all past projects and for WeMove.co, I‚Äôd preferred hosting code at BitBucket. Primarily because I get a lot of free private repository and my development team hasn‚Äôt exceeded 5. However, as we make plans for new hires, I decided that we need to upgrade our plan on BitBucket.
Then I discovered GitLab.
I went to GitLab to checkout the free CI/CD, but ended up using it because of the unlimited number of team members available (Sorry BitBucket, I still ‚ù§Ô∏è you). Ended up not using their CI/CD, instead I rolled out my own build server.
For the build server, we got a Windows Server box at Interswitch Cloud (https://cloud.interswitch.com). I chose Jenkins for the build server because that‚Äôs what I have the most experience with and trust.
It took me 1 week (of switching between coding and running the business), 71 builds & copying a folder from my development PC to the server, to successfully get Jenkins to deploy my nuget server as I want it. The major issue was getting Jenkins to make MSBuild to work with Web Deploy (the Nuget server is an ASP.NET 4.5 project). I‚Äôm not even sure I remember all the steps involved, so I apologise for not doing a step-by-step guide.
After doing this successfully, it was a lot easier to set-up scripts for the Vehicle Hire ASP.NET Core app.
Notice that on line 2, I told dotnet to publish for ubuntu, using the -r ubuntu.16.04-x64 option. Yes, I used the JDK ‚Äújar‚Äù utility to create a zip file on the third line. Did you see what I did there? Also, the curl command on line 4 only works after you download curl for windows from https://dl.uxnr.de/build/curl/curl_winssl_msys2_mingw64_stc/curl-7.57.0/curl-7.57.0.zip
Now I need to go to the Ubuntu server and set it up to accept the ZipPackage.zip. I decided to do this using PHP. That meant writing a little PHP script, which accepts the request and unzip the zip file to a specified /var/www/ path, depending on the action specified for ?tasktoperform=<action>.
For the web server, we acquired a VM from Digital Ocean. It took a little over 10 minutes from registration to the point of setting up the SSH keys. I was super impressed, to be honest.
I created a PHP script which receives the zip from the build server. The port it runs on is only open to the build server‚Äôs IP address, configured using the ufw firewall utility. It takes the file and unzips it into the appropriate directory. It also copies the appropriate appsettings.json file for the ASP.NET Core. I‚Äôll explain why below under Gotchas.
The following links contain in-depth, step-by-step guides on how to set-up ASP.NET Core for Ubuntu and configure it to work with Nginx. Then installing as a service, so it starts up with the server. So I won‚Äôt bore you by repeating them just follow the links:
docs.microsoft.com
blog.bobbyallen.me
‚Äî ‚Äî
Also, I went ahead to set-up Let‚Äôs Encrypt as the SSL provider for WeMove. The link below tells you how to do that:
www.digitalocean.com
These are the things that could kill your time if you decide on following these set-up to get ASP.NET Core running on your Ubuntu server.
You might be wondering that with this complex set-up, how is it that we‚Äôre saving more money than just pushing to BitBucket and letting Azure PaaS build it into an App Service. The answer is below:
Not only can it listen to multiple ports, it can also serve multiple (sub-) domains. That means if it‚Äôs not being maxed-out or broken in any way, I ain‚Äôt setting-up another server to serve WeMove sub-domains.
For each ASP.NET Core application we deploy to Azure, we need to set-up a new App Service. To get SSL, it has to be a tier (or two) above entry payment tier. Average monthly use for us per App Service per tier is between $25 and $30. Other services like SQL Azure raises the cost way higher, especially when you have multiple databases for staging and production.
A Digital Ocean droplet, running Nginx lets us utilise the same server for all our web apps, at a $20 charge. The Nginx lets us reverse proxy to each ASP.NET Kestrel server, depending on the request that comes in.
Our database is still in Azure and I plan to keep it that way for a long time.
Thanks for taking out time to read this.
We‚Äôre launching a new service next week, lookout for it.
If you want to hire any vehicle for anything in Lagos, Nigeria, please use our service https://wemove.co. Follow us everywhere on social media: @WeMoveCo
twitter.com
www.instagram.com
www.facebook.com
#BlackLivesMatter
47 
2
47¬†claps
47 
2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Rebel ‚Ä¢ Founder & CEO, WeMove Technologies (owners of WeMove.co) ‚Ä¢ Follow @WeMoveCo
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.datadriveninvestor.com/tips-for-az-104-microsoft-azure-administration-certification-9689c5925cc8?source=search_post---------214,"Recently Microsoft made quite few changes in Azure certifications. Earlier you will need to pass 2 exams Az-100 and AZ-101 to be a certified Azure administrator. Microsoft combined both skills into one exam which was AZ-103.
AZ-104 is new version of AZ-103. This tests the candidates knowledge on implementing, managing, and monitoring an organization‚Äôs Microsoft Azure‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gavinlewis/multi-cloud-architectures-for-the-enterprise-part-2-41df71d959e3?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gavin Lewis
Nov 4, 2019¬∑5 min read
A few months back I wrote about Multi-Cloud Architectures for the Enterprise: Part 1; summarized, it was an example of how you could connect AWS, Azure and On-Premise using IPSec VPNs. Part 2 was intended to talk about platform services, however, after a few months working with Azure outside of my sandbox environment and seeing what some of the clients I work with are doing, I thought I‚Äôd pivot a little and talk about the big three cloud providers‚Ä¶
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/spiritual-tree/spring-azure-324f8f6b61b5?source=search_post---------216,"There are currently no responses for this story.
Be the first to respond.
A poem for early Spring
A sliver of skymore violet than blueThe Spring Azure driftslike a petal in view.
Sailing on the currents,floating through the flowersas early song birds calland buds slip off their cowls.
Tiny blue butterflywrestling the wind,what spirit lifts and leads yousafely home again?
"
https://medium.com/awesome-azure/azure-reading-application-settings-in-azure-functions-asp-net-core-1dea56cf67cf?source=search_post---------217,"There are currently no responses for this story.
Be the first to respond.
Reading settings from a Azure Functions in .NET.
For the second (v2) and third (v3) generation of Azure Functions, the logic changes a bit. Not quite as straightforward, but more flexible. ConfigurationManager is not used anymore. Instead, you‚Äôre supposed to use ConfigurationBuilder. OR
"
https://medium.com/awesome-azure/azure-difference-between-azure-sql-database-and-sql-server-on-vm-comparison-azure-sql-vs-sql-server-vm-cf02578a1188?source=search_post---------218,"There are currently no responses for this story.
Be the first to respond.
Comparison ‚Äî Azure SQL Database vs SQL Server on Virtual Machine
Azure SQL Database offers Database-as-a-service (DBaaS-PaaS). With SQL Database, you don‚Äôt have access to the machines that host your databases. In contrast, Azure Virtual Machine offers Infrastructure-as-a-service (IaaS). Running SQL Server on an‚Ä¶
"
https://medium.com/dotnet-hub/use-azure-key-vault-with-net-or-asp-net-core-applications-read-azure-key-vault-secret-in-dotnet-fca293e9fbb3?source=search_post---------219,"There are currently no responses for this story.
Be the first to respond.
Store app settings in Azure Key Vault for .NET 5.x or ASP.NET Core 3.x applications
Azure Key Vault is a service that you can use to store secrets and other sensitive configuration data for an application. It allows you to define settings that can be shared among multiple apps, including apps running in App Service.We will use‚Ä¶
"
https://towardsdatascience.com/how-to-deploy-web-apps-with-azure-52ca340b41b9?source=search_post---------220,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Briggs
Jul 17, 2020¬∑8 min read
Azure ‚Äî or any other cloud service ‚Äî has made sharing the fruits of our creative labor easier than ever.
We can take an app built with Angular (an incredibly easy framework to pick-up) and deploy it to the world quicker than the time it takes to make coffee.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-deploy-static-angular-website-with-azure-devops-46546b536aeb?source=search_post---------221,"There are currently no responses for this story.
Be the first to respond.
There are a lot of deployment strategies when you deploy your angular applications to production and your deployment strategy is entirely depends on your application architecture. For example, If you are using Java or Nodejs with Angular you need to deploy your application on respective environments. If‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/protecting-personal-identifiable-information-with-azure-ai-aa4b7afc4839?source=search_post---------222,"There are currently no responses for this story.
Be the first to respond.
TLDR; The following post will outline both first party and open source techniques for detecting PII with Azure.
Personally Identifiable information (PII), is any data that can be used used to identify a individuals such as names, driver‚Äôs license number, SSNs, bank account numbers, passport numbers, email addresses and more. Many regulations from GDPR to HIPPA require strict protection of user privacy.
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
Azure Cognitive Search is a cloud solution that provides developers APIs and tools for adding a rich search experience to their data, content and applications. With cognitive search you can add cognitive skills to apply AI processes during indexing. Doing so can add new information and structures useful for search and other scenarios.
The Azure PII Detection skill (Currently in Preview) extracts personally identifiable information from an input text and gives you the option to mask it from that text in various ways. This skill uses the machine learning models provided by Text Analytics in Cognitive Services.
docs.microsoft.com
In addition to the first party cognitive search Microsoft also provides an open source PII detection tool for Azure called Presidio which was developed by the Microsoft Commercial Software Engineering team in Israel.
github.com
Presidio is open-source, transparent, and scalable. Presidio allows developers and data scientists to customize or add new PII recognizers via API or code to best fit your anonymization needs. Presidio leverages docker and kubernetes for workloads at scale.
Presidio automatically detects Personal-Identifiable Information (PII) in unstructured text, annonymizes it based on one or more anonymization mechanisms, and returns a string with no personal identifiable data. For example:
For each PII entity, presidio returns a confidence score:
Text anonymization in images (beta)
Presidio uses OCR to detect text in images. It further allows the redaction of the text from the original image.
Check out a public demo to try out on your own data with the link below.
presidio-demo.azurewebsites.net
Installation Steps
More information can be found on the github repo and near one click deployment options for Azure are coming soon!
Additional deployment options can be found here.
In this post you learned two of my favorite options for detecting PII in your data with Azure. If you are interested in Azure and AI be sure to check out my other posts and the Azure medium blog.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
158 
1
158¬†claps
158 
1
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/hackernoon/kubernetes-installing-mongodb-replicaset-on-azure-using-helm-20f6bfcf92b3?source=search_post---------223,"There are currently no responses for this story.
Be the first to respond.
It‚Äôs time to install a MongoDB ReplicaSet on a Kubernetes cluster on Azure and try to kill it in all possible ways!Starring:
Let‚Äôs install a Cluster with 1 Master and 3 Nodes all running Linux using ACS Engine with following commands.
I described detailed steps on ACS Engine usage for installing a cluster in my Kubernetes Adventures on Azure ‚Äî Part 3 (ACS Engine & Hybrid Cluster) article. Please refer to it for details. Here I list quickly commands to be used.
This is needed to group all resources for this tutorial in a single logical group, to be able to delete everything with a single command at the end.
I usually create an ssh pair for my test on acs. Please check my article here on how to do it. I will change the examples/kubernetes.json file to use previously created ssh pair, dnsPrefix and servicePrincipalProfile (following Deploy a Kubernetes Cluster suggestions from Microsoft).
my kubernetes.json file with changes in bold is:
Create your cluster with:
Wait for the cluster to be up running:
Connect to it using kubeconfig file generated during deployment in _output folder.
Following commands can be used to determine when cluster is ready:
Now you can open Kubernetes Dashboard if you want to use a UI to check you cluster status: kubectl proxy and then open a browser at http://127.0.0.1:8001/ui
Helm is the Package Manager for Kubernetes. It simplifies installation and maintenance of products and services like:
We will use it to install and configure a MongoDB Replica Set.
Prerequisites, installation steps and details can be found in the Use Helm to deploy containers on a Kubernetes cluster article from Microsoft.
With all prerequisites in place, Helm installation is as simple as running command:
Let‚Äôs clone charts repository to be able to examine and change MongoDB chart files before deploying everything on our cluster:
Now go in the /charts/stable/mongodb-replicaset folder. Here you will find all artifacts composing an Helm Chart. If needed you can change values.yaml file to tailor installation based on your needs. For now let‚Äôs try a standard installation.
Run following command: helm install . and wait for following output:
MongoDB Replicaset is up and running! Helm is ultra easy and powerful!
From output of helm install pick up the NAME of your release and use it in the following command:
Here we follow a different path from Helm Chart. Let‚Äôs open an interactive shell session with remote Mongo server!
In theory Pod 0 should be the master as you can see rom rs:PRIMARY> prompt. If this is not the case tun following command to find the Master:
Take note of the primary Pod because we are going to kill it soon and connect to it using last kubectl exec used above.
We need to create some data to check persistence across failures. We are already connect to the mongo shell, creating a document and leaving the session is as simple as:
Use following command to monitor changes in replica set:
Here it is: kubectl delete pod $RELEASE_NAME-mongob-replicaset-0
MongoDB will start an election and another Pod will become master:
And in the meantime Kubernetes will immediately take corrective actions instantiating a new Pod 0.
Now we have to simulate a real disaster: let‚Äôs kill all Pods and see the StatefulSet magically recreating everything with all data available.
After few minutes our MongoDB replicaset will be back online and we can test it again to see if our data are still there.
Run following command to verify that key created is still there:
As always you can delete everything with a simple Azure CLI 2 command: az group delete --name k8sMongoTestGroup --yes --no-wait
This is a topic for a future post. It seems trivial as a kubectl expose but it is not so easy. If you expose the existing service, it will be LoadBalanced on 3 nodes behind it and this is wrong. We need 3 LoadBalancers, exposing 3 services, 1 for each Pod in the StatefulSet. Moreover we have to activate Authentication and SSL to ensure best practice from security perspective.
I will find best way to do it, while playing with Heml, Kubernetes, MongoDB and Azure!
#BlackLivesMatter
94 
5
how hackers start their afternoons. the real shit is on hackernoon.com.¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
94¬†claps
94 
5
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/azure-messaging-service-in-a-picture-f8113cec54cd?source=search_post---------224,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jul 12, 2018¬∑4 min read
When you start serverless on Azure, you might be wondering which messaging services to use. I create a simple diagram how to choose the messaging service.
If you have a asynchronous message between Azure functions, the Storage Queue comes as a first choice. If you find Queue messaging service, Storage Queue have these characteristics
azure.microsoft.com
If you find reliable solution for Queue and Pub-Sub with enterprise reliability you can choose Service Bus.
Before moving on the detail, I‚Äôll share the Queue and Pub-Sub in a picture.
A queue is simple messaging service for sending messaging asynchronously. A function send a queue, then other function polling the queue then consume the queue. Since it is polling, it is not very fast.
As for publish-subscribe, a message receiver is a subscriber. A subscriber subscribe a topic. If topic receive a message, it send message to the subscribers. Subscriber can filter the messages. It is the overview of the Publish-Subscribe.
In case of Service bus, it has these charactersitics
You can use Service Bus for reliable use case. E.g. Financial / Banking solution.
azure.microsoft.com
Event Grid is interesting offer. It has these characteristics
You can use Event Grid
azure.microsoft.com
If you need to ingest a lot of messaging at a time, you need to choose EventHub or IoTHub.
azure.microsoft.com
azure.microsoft.com
This is very good to read for EventHub with a lot of messages.
blogs.msdn.microsoft.com
medium.com
This is not asynchronous, but, if you want to have bi-directional messaging, like server to client, you can pick SignalR Service. It is serverless offer for the SignalR. SignalR is based on websocket. However, you don‚Äôt need to create a websocket server, also it scale!
azure.microsoft.com
If you want to send message through company to company with firewall, you can choose Auzre Relay. It support WebSocket and WCF. This service is also serverless and it is designed for hybrid communication between intranet through the internet.
docs.microsoft.com
I use these service to communicate between hololens and hololens through the internet.
Notification Hub is used for mobile push notifications. If you want to communication a lot of clients, you can consider this.
azure.microsoft.com
This article is written for a guy wants to try Azure Functions. It is good to know the messaging services. :) This is quick summary, however, you can learn more detail, please refer these articles.
docs.microsoft.com
docs.microsoft.com
Senior Software Engineer ‚Äî Microsoft
See all (200)
90 
1
90¬†claps
90 
1
Senior Software Engineer ‚Äî Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/azure-na-pr%C3%A1tica-gratuito-1-desenvolvimento-web-saiba-como-foi-conte%C3%BAdos-gratuitos-5908938c85e6?source=search_post---------225,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
May 11, 2020¬∑5 min read
Neste √∫ltimo s√°bado (09/05/2020) o Azure na Pr√°tica promoveu seu primeiro minicurso online e gratuito, com foco em Desenvolvimento Web e utilizando o Microsoft Azure. Al√©m de uma breve introdu√ß√£o a conceitos de cloud computing, ao longo deste treinamento foi coberto tamb√©m o uso de tecnologias como Azure Storage, Azure Functions, Azure App Service, Application Insights, ASP.NET Core, .NET Core e Visual Studio Code.
Fui um dos instrutores e organizadores desta iniciativa, juntamente com meus amigos Milton C√¢mara (Microsoft MVP, MTAC) e Ericson da Fonseca (Microsoft MVP). O resultado geral excedeu em muito nossas expectativas e nos, j√° que contamos com 1819 inscri√ß√µes via Sympla:
E um pico de 609 pessoas nos assistindo ao longo da live no YouTube, com mais de 3 mil visualiza√ß√µes at√© o momento da publica√ß√£o deste post:
A grava√ß√£o j√° est√° dispon√≠vel no canal Azure na Pr√°tica no YouTube e pode ser assistida gratuitamente (aproveitamos para convidar voc√™ que est√° lendo esse post para que se inscreva no mesmo):
Foi surpreendente para n√≥s 3 ao longo da live constatarmos que pessoas residentes em Portugal, Angola, It√°lia e Reino Unido estavam nos assistindo (al√©m claro de participantes de norte a sul do Brasil). Recebemos in√∫meros agradecimentos em redes sociais (YouTube, Facebook, LinkedIn) por este esfor√ßo, algo que nos enche de orgulho e motiva√ß√£o para seguir em frente mesmo em uma √©poca dif√≠cil para todos. Eis alguns feedbacks:
Os slides que utilizamos foram disponibilizados no SlideShare:
Aproveitamos para agradecer:
Nas pr√≥ximas se√ß√µes est√£o avisos incluindo conte√∫dos gratuitos sobre o Microsoft Azure, eventos online gratuitos nos pr√≥ximos dias cobrindo esta plataforma e descontos para os pr√≥ximos cursos pagos do Azure na Pr√°tica.
O canal do Azure na Pr√°tica no YouTube √© uma excelente fonte de conte√∫dos, com grava√ß√µes gratuitas incluindo mesas redondas, dicas e truques na utiliza√ß√£o do Microsoft Azure:
www.youtube.com
Temos ainda o Canal .NET e o Coding Night, canais em que servi√ßos do Microsoft Azure s√£o abordados com frequ√™ncia:
www.youtube.com
www.youtube.com
Uma iniciativa promovida anualmente pelo Canal .NET √© o Azure Tech Nights, evento online e gratuito cobrindo diferentes tecnologias que integram a nuvem Microsoft. A edi√ß√£o 2020 aconteceu recentemente (Fevereiro a Abril), com os links da grava√ß√£o de cada palestra podendo ser encontrados no seguinte post:
Azure Tech Nights 2020: saiba como foi - V√≠deos Gratuitos
Diversos projetos exemplificando o uso de servi√ßos do Azure est√£o no reposit√≥rio do Azure na Pr√°tica no GitHub:
https://github.com/azurenapratica
E tamb√©m no meu GitHub:
https://github.com/renatogroffe
A seguir est√£o tamb√©m diversos artigos abordando diferentes servi√ßos do Azure (h√° v√≠deos sendo referenciados em alguns destes posts):
Docker - Guia de Refer√™ncia Gratuito
Kubernetes - Guia de Refer√™ncia Gratuito
GitHub Actions - Guia de Refer√™ncia Gratuito
Azure DevOps - Guia de Refer√™ncia Gratuito
Serverless + Azure Functions: Guia de Refer√™ncia
Serverless √© muito mais que apenas APIs REST!
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
Microservices: alternativas para a implementa√ß√£o no Microsoft Azure
ASP.NET Core + Azure App Configuration: manipulando configura√ß√µes de forma mais inteligente
Como o Microsoft Azure pode simplificar a publica√ß√£o de suas Web Apps? - Dica R√°pida
GitHub + Azure App Service: deployment automatizado e sem complica√ß√µes de Web Apps na nuvem
Application Insights + Logic Apps + Aplica√ß√µes Web: enviando alertas de monitoramento via Slack
ASP.NET Core + Entity Framework Core: monitoramento descomplicado via Application Insights
ASP.NET Core + Application Insights: monitorando a disponibilidade dos BDs de uma aplica√ß√£o
E concluo este post com os links para os blog de cada um dos palestrantes do minicurso:
Renato Groffe ‚Äî Blog
Ericson da Fonseca ‚Äî Blog
Milton Camara Gomes ‚Äî Blog
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
85 
1
85¬†claps
85 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/azure-na-pratica/azure-na-pr%C3%A1tica-gratuito-3-azure-devops-saiba-como-foi-conte%C3%BAdos-gratuitos-314df31d447c?source=search_post---------226,"There are currently no responses for this story.
Be the first to respond.
Neste √∫ltimo s√°bado (18/07/2020) o Azure na Pr√°tica promoveu seu terceiro minicurso online e gratuito, focando desta vez nos primeiros passos com Azure DevOps. Al√©m de uma introdu√ß√£o englobando conceitos de automa√ß√£o e DevOps em geral, ao longo deste treinamento foi coberto tamb√©m o uso de Azure DevOps em conjunto com tecnologias como Docker, Azure App Service, Azure Container Registry, ASP.NET Core e Application Insights.
Caso voc√™ queira ter acesso ao conte√∫do do primeiro e do segundo minicursos (promovidos em Maio e Junho/2020, respectivamente) de forma gratuita ou, at√© mesmo, deseje rev√™-los, acesse os links a seguir:
Azure na Pr√°tica Gratuito #1 - Desenvolvimento Web: saiba como foi + conte√∫dos gratuitos
Azure na Pr√°tica Gratuito #2 - Docker: saiba como foi + conte√∫dos gratuitos
Fui instrutor e um organizadores desta iniciativa, juntamente com meus amigos Milton C√¢mara (Microsoft MVP, MTAC) e Vinicius Moura (Microsoft MVP). O resultado geral foi muito al√©m superou mais uma vez nossas expectativas e os 2 minicursos anteriores, com 5738 inscri√ß√µes via Sympla:
Tendo um pico de 1252 pessoas nos assistindo ao longo da live no YouTube, com mais de 5,2 mil visualiza√ß√µes at√© o momento da publica√ß√£o deste post:
Os testes de acesso que pedimos gentilmente √†queles que assistiam ao evento motraram participantes nas seguintes cidades (141 diferentes ao todo, com dados capturados atrav√©s do uso do Azure Application Insights na aplica√ß√£o que serviu de base para a demonstra√ß√£o): Adamantina, Aguas de Lindoia, Altinopolis, Araraquara, Barbacena, Barcelona, Barra do Pira√≠, Barra Mansa, Barueri, Bayeux, Bel√©m, Belford Roxo, Belo Horizonte, Benavente, Betim, Birigui, Blumenau, Boa Esperanca, Braganca Paulista, Bras√≠lia, Brumado, Cacapava, Cachoeirinha, Cachoeiro de Itapemirim, Caetanopolis, Caetite, Cajamar, Cajazeiras, Camaqua, Campina Grande, Campinas, Campo Grande, Carapicuiba, Caxias do Sul, Columbus, Conselheiro Lafaiete, Contagem, Cotia, Crici√∫ma, Cuiab√°, Curitiba, Diadema, Divin√≥polis, Duque de Caxias, Embu, Espirito Santo do Pinhal, Ferraz de Vasconcelos, Florian√≥polis, Fortaleza, Funchal, Garanhuns, Goi√¢nia, Guaratingueta, Guarulhos, Ibipora, Indaiatuba, Ipatinga, Itapevi, Itaquaquecetuba, Itu, Ivaipora, Jaboatao dos Guararapes, Jo√£o Pessoa, Joinville, Juiz de Fora, Jundia√≠, Lima, Lisbon, Lorena, Luanda, Macap√°, Manaus, Manhuacu, Marica, Mar√≠lia, Maring√°, Matao, Matozinhos, Maua, Mogi das Cruzes, Mogi Mirim, Montevideo, Montreal, Mossoro, Nantes, Navegantes, Niter√≥i, Nova Europa, Nova Luzitania, Novo Hamburgo, Osasco, Pacajus, Paraguacu Paulista, Passo Fundo, Paulinia, Pedreira, Piracicaba, Po√ßos de Caldas, Pomerode, Ponta Grossa, Porto, Porto Alegre, Porto Velho, Presidente Prudente, Qu√©bec, Recife, Ribeir√£o Preto, Rio das Pedras, Rio de Janeiro, Salvador, Santa Fe do Sul, Santa Luzia, Santa Maria, Santo Andr√©, Santos, S√£o Bernardo do Campo, S√£o Carlos, Sao Goncalo, Sao Jose, Sao Jose do Rio Preto, S√£o Jos√© dos Campos, Sao Lourenco da Mata, S√£o Lu√≠s, S√£o Paulo, Sao Roque, Sao Vicente, Seattle, Sete Lagoas, Silvares, Suzano, Taboao da Serra, Taquara, Uba, Uberl√¢ndia, Vargem Grande Paulista, Videira, Vila Velha, Vit√≥ria, Votuporanga, Waterloo, Woodstock.
Uma an√°lise destes resultados mostra que esta iniciativa atingiu n√£o apenas cidades do Brasil de norte a sul, como tamb√©m contou com p√∫blico em pa√≠ses como pa√≠ses Angola, Canad√°, Espanha, Estados Unidos, Fran√ßa, Peru, Portugal e Uruguai:
O projeto utilizado na demonstra√ß√£o, assim como as queries para an√°lise da origem (cidade, pa√≠s/regi√£o) das requisi√ß√µes e processamento (por qual container Docker as solicita√ß√µes foram processadas) j√° est√£o nos seguintes reposit√≥rios do GitHub:
ASP.NET Core 3.1 + Razor Pages + Docker + Simula√ß√£o de Falhas + Azure Application Insights
Application Insights + Kusto Query Language + Queries com informa√ß√µes de requisi√ß√µes por containers Docker
Application Insights + Kusto Query Language + Queries com informa√ß√µes de requisi√ß√µes por cidade de origem
Application Insights + Kusto Query Language + Queries com informa√ß√µes de requisi√ß√µes por pa√≠s/regi√£o de origem
A grava√ß√£o j√° est√° dispon√≠vel no canal Azure na Pr√°tica no YouTube e pode ser assistida gratuitamente (aproveitamos para convidar voc√™ que est√° lendo esse post para que se inscreva no mesmo):
Os slides que utilizamos foram disponibilizados no SlideShare:
Aproveitamos para agradecer:
Recebemos in√∫meros agradecimentos em redes sociais (YouTube, Facebook, LinkedIn) por este esfor√ßo, algo que sempre nos motiva para seguir em frente com esse tipo de iniciativa. Seguem alguns feedbacks:
Nas pr√≥ximas se√ß√µes est√£o avisos incluindo conte√∫dos gratuitos sobre o Microsoft Azure, eventos online gratuitos nos pr√≥ximos dias cobrindo esta plataforma e descontos para os pr√≥ximos cursos pagos do Azure na Pr√°tica.
No blog Azure na Pr√°tica temos v√°rias postagens semanais, cobrindo o uso de tecnologias como Azure DevOps, Docker, Kubernetes e diversos servi√ßos do Microsoft Azure. Deixamos o convite para que voc√™ se inscreva aqui, recebendo assim notifica√ß√µes de nossos conte√∫dos gratuitos:
medium.com
O canal do Azure na Pr√°tica no YouTube tamb√©m √© uma excelente fonte de conte√∫dos, com grava√ß√µes gratuitas incluindo mesas redondas, dicas e truques na utiliza√ß√£o do Microsoft Azure:
www.youtube.com
Temos ainda o Canal .NET e o Coding Night, canais em que servi√ßos do Microsoft Azure s√£o abordados com frequ√™ncia:
www.youtube.com
www.youtube.com
Uma iniciativa promovida anualmente pelo Canal .NET √© o Azure Tech Nights, evento online e gratuito cobrindo diferentes tecnologias que integram a nuvem Microsoft. A edi√ß√£o 2020 aconteceu recentemente (Fevereiro a Abril), com os links da grava√ß√£o de cada palestra podendo ser encontrados no seguinte post:
Azure Tech Nights 2020: saiba como foi - V√≠deos Gratuitos
E os canais dos MVPs Vinicius Moura (Azure DevOps Sprints) e Julio Arruda:
www.youtube.com
www.youtube.com
No link a seguir reuni diversos conte√∫dos gratuitos (artigos, v√≠deos, exemplos) sobre Azure DevOps, incluindo a utiliza√ß√£o desta solu√ß√£o em conjunto com tecnologias como Azure App Service/Web App for Containers, Docker, Kubernetes, Azure Container Registry e Azure Kubernetes Service (AKS):
Azure DevOps ‚Äî Guia de Refer√™ncia Gratuito
A seguir est√£o tamb√©m diversos artigos e projetos de exemplo abordando diferentes servi√ßos do Azure (h√° v√≠deos sendo referenciados em alguns destes posts):
Sobrevoando os servi√ßos do Azure
Docker ‚Äî Guia de Refer√™ncia Gratuito
Kubernetes ‚Äî Guia de Refer√™ncia Gratuito
GitHub Actions ‚Äî Guia de Refer√™ncia Gratuito
ASP.NET Core + Application Insights: monitorando o uso de Dapper, Entity Framework e NHibernate
.NET Core + Serverless: melhorando a experi√™ncia de Desenvolvimento com Azure Functions 3.x | pt 1
.NET Core + Serverless: melhorando a experi√™ncia de Desenvolvimento com Azure Functions 3.x | pt 2
Mensageria + .NET Core 3.1: exemplos com RabbitMQ, Kafka, Azure Service Bus e Azure Queue Storage
Serverless + Azure Functions: Guia de Refer√™ncia
Serverless √© muito mais que apenas APIs REST!
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
Microservices: alternativas para a implementa√ß√£o no Microsoft Azure
ASP.NET Core + Azure App Configuration: manipulando configura√ß√µes de forma mais inteligente
Como o Microsoft Azure pode simplificar a publica√ß√£o de suas Web Apps?- Dica R√°pida
GitHub + Azure App Service: deployment automatizado e sem complica√ß√µes de Web Apps na nuvem
Application Insights + Logic Apps + Aplica√ß√µes Web: enviando alertas de monitoramento via Slack
ASP.NET Core + Entity Framework Core: monitoramento descomplicado via Application Insights
ASP.NET Core + Application Insights: monitorando a disponibilidade dos BDs de uma aplica√ß√£o
Blog do Azure na Pr√°tica
72 
72¬†claps
72 
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Pr√°tica
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Pr√°tica
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/monitorando-recursos-com-o-asp-net-core-3-0-health-checks-azure-logic-apps-e-o-slack-241bd61b8835?source=search_post---------227,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 14, 2019¬∑8 min read
Em um post anterior demonstrei a implementa√ß√£o de uma aplica√ß√£o Web voltada ao monitoramento de recursos de infraestrutura. Utilizei para isto o ASP.NET Core 2.2, Health Checks e diversos packages que integram o projeto open source Xabaril/AspNetCore.Diagnostics.HealthChecks. Dentre as tecnologias poss√≠veis de serem acompanhadas e oferecidas por esta solu√ß√£o est√£o servidores do SQL Server, Oracle, PostgreSQL, MySQL, MongoDB e Elasticsearch, aplica√ß√µes Web, projetos SignalR, brokers de mensageria baseados em RabbitMQ e Kafka, clusters do Kubernetes, diversos servi√ßos do Microsoft Azure e AWS‚Ä¶
Aqueles interessados em conhecer mais a respeito deste exemplo podem faz√™-lo acessando o link a seguir:
ASP.NET Core + Health Checks: implementando rapidamente uma solu√ß√£o de monitoramento
Neste novo artigo retomo esse projeto (SiteMonitoramento), fazendo agora uso de uma nova vers√£o do mesmo criada com o ASP.NET Core 3.0 e j√° disponibilizada no GitHub:
ASP.NET Core 3.0 + Health Checks + Dashboard + Monitoramento de Recursos
A inten√ß√£o desta vez √© ir al√©m, criando um Worker Service que consumir√° um endpoint com os status dos diferentes Health Checks/depend√™ncias cadastrados em SiteMonitoramento. Worker Services s√£o uma nova op√ß√£o oferecida pelo .NET Core 3.0 e que contam com um template que simplifica a implementa√ß√£o de solu√ß√µes como Windows Services e processos, valendo-se para isto de parte da infraestrutura e do runtime do ASP.NET Core.
Este Worker Service ir√° analisar periodicamente cada Health Check, enviando notifica√ß√µes de alerta para um canal em um grupo do Slack. Essa comunica√ß√£o com o Slack ocorrer√° por meio de uma Logic App criada no Azure. Logic Apps s√£o um tipo de recurso baseado em workflows e disponibilizado na nuvem Microsoft, simplificando em muito a integra√ß√£o com diversos servi√ßos corporativos: Microsoft Teams, SAP e Office 365 tamb√©m constituem exemplos de tecnologias suportadas em uma Logic App.
E aproveito este espa√ßo para um convite.
Dia 22/10/2019 (ter√ßa) a partir das 21:30 ‚Äî hor√°rio de Bras√≠lia ‚Äî teremos mais uma live no Canal .NET. Desta vez ser√° abordado o uso da biblioteca Polly, solu√ß√£o esta que possibilita um melhor tratamento de falhas em projetos .NET e contribui assim para a obten√ß√£o de aplica√ß√µes mais est√°veis.
Para efetuar a sua inscri√ß√£o acesse a p√°gina do evento no Meetup. A transmiss√£o acontecer√° via YouTube, em um link a ser divulgado em breve.
Conforme indicado anteriormente, a aplica√ß√£o SiteMonitoramento faz uso de packages que comp√µem o projeto Xabaril/AspNetCore.Diagnostics.HealthChecks no monitoramento de recursos atrav√©s de Health Checks. Na imagem a seguir est√° o dashboard disponibilizado por esta aplica√ß√£o, com a checagem dos status de inst√¢ncias do SQL Server, Redis e MongoDB, um site publicado no Microsoft Azure, al√©m de uma conta de armazenamento do Azure e do DocumentDB/Cosmos DB:
Para facilitar o consumo de informa√ß√µes envolvendo o status de Health Checks defini o endpoint /status-monitoramento na classe Startup, atrav√©s de uma chamada ao m√©todo UseHealthChecks em Configure (a partir da linha 65). Ajustes foram realizados de maneira a formatar o resultado deste endpoint, com o retorno gerado contendo a identifica√ß√£o do endpoint, seu status e a descri√ß√£o de um eventual erro; o resultado em quest√£o foi serializado por meio da classe JsonSerializer (namespace System.Text.Json):
O resultado do endpoint /status-monitoramento pode ser observado na pr√≥xima listagem, com nenhuma das depend√™ncias apresentando qualquer tipo de problema:
Visando a integra√ß√£o com o Slack ser√° criada uma nova Logic App no portal do Azure:
Informar um nome para a Logic App (AlertasMonitoramentoInfra-LogicApp neste exemplo), definindo ainda um Resource Group e uma Location. Confirmar este procedimento acionando o bot√£o Create:
Conclu√≠do o processo de cria√ß√£o, a Logic App estar√° dispon√≠vel para se configurar o workflow de integra√ß√£o com o Slack:
Para implementar o workflow acessar ent√£o a op√ß√£o Logic app designer. Um v√≠deo estar√° dispon√≠vel apresentando detalhes sobre este tipo de servi√ßo:
Avan√ßando com a barra de rolagem aparecer√° a op√ß√£o Start with a common trigger. Ser√° necess√°rio selecionar um trigger/gatilho. Para o tipo de solu√ß√£o que estamos implementando ser√° escolhido When a HTTP request is received:
A primeira parte do workflow estar√° montada, conforme indicado na pr√≥xima imagem:
Acionar a op√ß√£o Use sample payload to generate schema, a fim de definir um contrato com os campos contendo informa√ß√µes a serem recebidas pela Logic App (os dados em quest√£o servir√£o de base para o envio de uma mensagem ao Slack):
Aparecer√° agora o popup Enter or paste a sample JSON payload. Preencher o campo correspondente com o conte√∫do JSON indicado na imagem a seguir, em que foram especificadas as propriedades dependency e message; concluir este processo clicando no bot√£o Done:
Em Request Body JSON Schema constar√° a estrutura do objeto a ser recebido via solicita√ß√£o HTTP do tipo POST pela Logic App. Um pouco abaixo deste trigger estar√° a op√ß√£o + New step, a qual permitir√° incluir a integra√ß√£o com o Slack como uma nova a√ß√£o no workflow (atrav√©s do assistente Choose an action):
O Azure oferece diversas a√ß√µes e conectores pr√©-definidos (se√ß√£o All) para a implementa√ß√£o de uma Logic App, como demonstrado na pr√≥xima imagem:
A pesquisa desta funcionalidade possibilitar√° encontrar as a√ß√µes dispon√≠veis para integra√ß√£o com o Slack. Selecionar para este exemplo o item Post message:
Na a√ß√£o que possibilita a integra√ß√£o com o Slack aparecer√° o bot√£o Sign in, que dever√° ser acionado para permitir a conex√£o com um grupo e um canal desta ferramenta de colabora√ß√£o (o ideal para a realiza√ß√£o deste procedimento √© que j√° se tenha conectado anteriormente ao grupo do Slack):
Ao clicar em Sign in ser√° exibida a tela a seguir, que solicitar√° a permiss√£o de acesso da Logic App ao grupo do Slack; acionar ent√£o a op√ß√£o Allow:
Um canal (Channel Name) e uma mensagem (Text Message) dever√£o ser especificados para o envio da notifica√ß√£o:
Preencher ent√£o o Channel Name:
E o campo Message Text, utilizando para isto os itens dependency e message dispon√≠veis em Add dynamic content:
Concluir este procedimento acionando o bot√£o Save. Neste momento um endere√ßo ter√° sido gerado em HTTP POST URL; esta configura√ß√£o servir√° de base para o envio de mensagens ao Slack atrav√©s do Worker Service de monitoramento:
O Visual Studio 2019 √© uma das alternativas para a cria√ß√£o de projetos baseados no template Worker Service:
Temos ainda a possibilidade de gerar um projeto deste tipo via linha de comando (dotnet new), fazendo uso para isto da op√ß√£o worker:
J√° abordei anteriormente o uso de Worker Services com o .NET Core 3.0 no seguinte artigo:
Novidades do .NET Core 3.0: Worker Services
Nesta se√ß√£o ser√° detalhada a implementa√ß√£o de uma aplica√ß√£o do tipo Worker Service chamado MonitoramentoInfraProcess. Os fontes deste projeto j√° foram disponibilizados no GitHub:
.NET Core 3.0 + Worker Service + Monitoramento de Recursos + Health Checks + Envio de Alertas para Azure Logic App/Slack
No arquivo appsettings.json est√£o os itens:
A classe StatusHealthCheck corresponde √† representa√ß√£o dos dados com status dos Health Checks configurados em SiteMonitoramento:
J√° na classe Worker (gerada quando da cria√ß√£o do projeto) est√° a implementa√ß√£o do processo de monitoramento:
Supondo que as inst√¢ncias do Redis e do MongoDB n√£o estejam no ar, como indicado na imagem a seguir:
E tamb√©m no endpoint a ser consumido pelo Worker Service:
Ao executar o Worker Service de monitoramento ser√° poss√≠vel visualizar que notifica√ß√µes foram enviadas ao Slack:
E dentro do canal monitoramento no Slack estar√£o as mensagens com os Health Checks e seus respectivos erros:
.NET Core 3.0 e ASP.NET Core 3.0: Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
78 
78¬†
78 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/asp-net-core-2-1-protegendo-uma-api-rest-com-jwt-identity-core-e-azure-key-vault-6a80d94ceb63?source=search_post---------228,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 22, 2018¬∑2 min read
Recentemente (09/10/2018) realizei uma apresenta√ß√£o em um meetup promovido pelo grupo Azure Brasil e que abordou a implementa√ß√£o de APIs REST seguras na nuvem. Os slides que utilizei est√£o dispon√≠veis no SlideShare:
Um dos exemplos que abordei nesta apresenta√ß√£o era uma API REST implementada em ASP.NET Core 2.1 e que fazia uso do SQL Server/Azure SQL, Entity Framework Core, Entity Framework Core InMemory, Identity Core, JWT (JSON Web Token), Bearer Authentication e Azure Key Vault. O projeto em quest√£o j√° foi inclusive disponibilizado no GitHub:
https://github.com/renatogroffe/ASPNETCore2.1_JWT-Identity-Azure_Key_Vault
Este mesmo exemplo foi descrito em detalhes durante a demonstra√ß√£o pr√°tica na palestra que realizei durante o Azure Tech Nights 2018. A grava√ß√£o se encontra no YouTube e pode ser assistida gratuitamente (todos os passos para configura√ß√£o e uso do Azure Key Vault foram detalhados neste v√≠deo):
O uso de tokens em APIs REST √© um tema recorrente entre os conte√∫dos que venho produzindo, sendo que diversas informa√ß√µes sobre a ado√ß√£o de JWT (JSON Web Tokens), Bearer Authentication e o ASP.NET Core Identity podem ser encontradas nos seguintes posts:
ASP.NET Core + JWT: Guia de Refer√™ncia
ASP.NET Core 2.0: JWT + Identity Core na autentica√ß√£o de APIs
Quanto ao Azure Key Vault, este servi√ßo que integra a plataforma de cloud computing da Microsoft permite o armazenamento, o gerenciamento e o controle de acesso a configura√ß√µes consideradas sens√≠veis em um projeto. Tais segredos ficar√£o hospedados e vinculados a um recurso criado na nuvem, estando acess√≠veis apenas para aplica√ß√µes devidamente registradas via Azure Active Directory.
Todo o processo de configura√ß√£o para uso do Azure Key Vault est√° indicado no v√≠deo mencionado neste post. No projeto exemplo espec√≠fico aqui apresentado uma string de conex√£o ficar√° armazenada em um recurso do Key Vault, sendo necess√°rio indicar no arquivo appsettings.json:
Altera√ß√µes tamb√©m ser√£o realizadas na classe Program e Startup para que o projeto se integre ao Azure Key Vault.
.NET Core 2.1 e ASP.NET Core 2.1: Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
22 
22¬†
22 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/georgian-impact-blog/comparing-google-cloud-platform-aws-and-azure-d4a52a3adbd2?source=search_post---------229,"There are currently no responses for this story.
Be the first to respond.
A starting point to help you choose the right platform for your ML project.
By Jing Zhang
If you‚Äôre looking for an end-to-end machine learning (ML) platform, you‚Äôre spoiled for choice. There are three main choices for cloud providers: Google Cloud Platform (GCP), Amazon Web Service (AWS) and Microsoft Azure Platform (Azure). The question is: how do you choose between the three? What functionality do they provide to build ML pipelines? We set out to answer these questions in a recent hackathon.
The R&D team at Georgian, where I work as an ML Engineer, decided to organize a hackathon to explore how each provider can help with the workflow. Our team builds machine learning software components ourselves, but we‚Äôre typically working with the growth-stage software companies in the Georgian family to deploy, so we wanted to familiarize ourselves with the different platforms and to be able to adapt to their workflows faster.
In this blog post, we‚Äôll give a high-level overview of the ML platform solutions provided by GCP, AWS and Azure based on our experience during the hackathon. This list is not exhaustive but shows the results of our hackathon research.
Specifically we‚Äôll cover:
We hope this can offer help as a starting point to help you choose the right platform for your ML project.
To help compare the different platforms, we chose a project that we could run on all three so we could compare apples to apples. Our project was to build a binary classification model with a given dataset and an end-to-end pipeline on a cloud provider. We used a dataset of companies that we use for our own machine learning platform, Spring. Spring helps us identify companies that fit our investment profile.The dataset contained general information about companies including their management team, financial information and office locations. The goal was to identify whether a company fit well into our investment profile (labeled 1) or not (labeled 0).
Since we used our own data for this project, we can‚Äôt share the specifics here, but to give you a sense what we were working with, here‚Äôs an overview of the dataset:
We also intentionally injected noise and error in the dataset so that we could run some tests on the data cleaning functionality. Specifically, we introduced:
One of the goals of this hackathon was to explore and assess each aspect of the end-to-end ML pipeline shown in the figure below. We decided to approach the challenge this way so that we would be able to assess the needs of a given project against the providers. For example, if explainability is important to a project, which is the best choice? Or, which provider performs the best if you want to run heavy testing?
Specifically, we wanted to assess each platform‚Äôs functionality in these areas:
Preprocess
Discover
Develop
Train
Test & Analyze
Deploy
Since we were building a model, we needed to think about model performance. The metric we tracked was Area Under the Receiver Operating Characteristic Curve (ROC_AUC). We weren‚Äôt, however, using model performance to make a judgment on which platform was better, so long as the results were roughly comparable.
Fairness in ML has emerged as an enormous issue area for policy makers, industry and the public. It‚Äôs important to have tools at your disposal to verify that your model is not treating any group and individual unfairly. To be able to address fairness, explainability tools are important too. These tools allow you to query why a model reached a certain decision. We were looking for the availability of the tools rather than assessing their relative performance.
Unlimited budgets are nice to dream about, but the reality is that cost is a constraint we should always keep in mind, so we tracked the costs for each provider.
As you would expect with these three platforms, there is a product or service for each step in the ML pipeline. The differences between them are in how well the services integrate with each other to build an end-to-end ML pipeline experience.
We have summarized the available services in the areas we‚Äôre interested in the table below. Going into detail is beyond the scope of this blog post. You can use this summary table as a starting point to see what‚Äôs available at each step for each provider.
For our team, the most important features are the model development and model deployment sections and, to a lesser extent, data visualization and QA. We were glad to see that all three providers have hosted notebook services, experiment tracking and version control, and easy deployment methods.
As for AutoML, all three providers have developed their own offerings. We used it to build our initial models and check whether there were valuable signals in the dataset. It would be great if the explainability component could be integrated so we could understand how the models are built for further model analysis and development.
Speaking of model explainability, while all three platforms all provide some tools, the functionality varies. If you have specific requirements, make sure you check if the current functionality satisfies your needs.
GCP uses a package called ‚Äúthe what-if tool‚Äù. You can integrate it with your notebook and play with the model by changing threshold or feature value for a given example. This allows you to check how certain changes affect the prediction result.
If you are using the Sagemaker debugger, it allows you to analyze how feature engineering and model tuning are done.
Azure provides a built-in module in their SDK, which seems to have the best integration.
So how did the models perform? As measured by ROC_AUC, performance was comparable across all three platforms. Azure and GCP scored slightly higher than AWS. This doesn‚Äôt necessarily mean one platform is better than another. It matched our expectation that the scores vary but should be close.
Cost, on the other hand, was more interesting. The cost on AWS was considerably lower than GCP and Azure. Please note this isn‚Äôt strictly an apples-to-apples comparison. The Azure team on our hackathon explored more services since Azure was a completely new platform to our team and we wanted to use the opportunity to learn more about it, so that may account for some of the cost difference.
One question we asked was: ‚ÄúIs it worth spending 4 to 6 times of money to get a 5% performance improvement?‚Äù The answer depends on the problem you‚Äôre addressing. For example, we‚Äôre looking for companies that could potentially generate large returns for our fund, so it may be worth spending the extra few hundred dollars. If you have budget concerns and the spending doesn‚Äôt justify the return, then it‚Äôs a different story.
Based on our observations, all three cloud providers cover the aspects of the ML workflow we care about.
Two hot topics in the industry right now are the rise of AutoML and the need for an end-to-end machine learning workflow in one place to provide a frictionless experience. As we mentioned earlier, AutoML products are already available on all the platforms but the end-to-end pipeline experience doesn‚Äôt seem to be mature yet.
On GCP and AWS, you‚Äôll need to assemble multiple products together to get to the desired outcome. Azure, on the other hand, provides a machine learning designer service with drag and drop UI. This might imply that they are targeting different customer bases.
With the drag and drop UI interface, Azure‚Äôs machine learning designer may be more friendly to those new to data science, with little coding and technical background, to try out machine learning projects and evaluate whether it brings value to the business. Many corporations already use Microsoft products, so it might be an easy choice for teams at larger companies who are looking to try machine learning for the first time.
AWS and GCP seem to be more developer-focused. Though it‚Äôs a little more work to assemble a pipeline, they‚Äôre more customizable with the different components available. These components and the connection of the pipeline are often developed through code and configurations rather than an user interface. Companies who are more familiar with the options and know what they want to achieve may prefer this option.
We certainly learned a lot from this Georgian hackathon and it puts us in a better position to undertake projects on any of these three platforms in future. We hope that this is useful to you and helps you pick the right platform to start your next project.
A blog focused on machine learning and artificial‚Ä¶
139 
139¬†claps
139 
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
Written by
Investors in high-growth business software companies across North America. Applied artificial intelligence, security and privacy, and conversational AI.
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
"
https://medium.com/@renatogroffe/apis-seguras-com-asp-net-core-jwt-docker-e-azure-azure-talks-junho-2016-3c80f0c08ab3?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 25, 2018¬∑3 min read
No dia 19/06/2018 (ter√ßa) participei como palestrante e organizador de mais um evento presencial do Azure Talks (meetup do qual sou coordenador, juntamente com o Luigi Tavolaro).
Desta vez realizei uma apresenta√ß√£o sobre o desenvolvimento seguro de APIs REST na nuvem, utilizando para isto as seguintes tecnologias:
Este evento aconteceu no audit√≥rio do Bradesco Inovabra Habitat, contando ainda com outras apresenta√ß√µes sobre tecnologias oferecidas pelo Microsoft Azure ao longo da noite de ter√ßa.
Gostaria de deixar neste post meu muito obrigado ao Rodrigo Guerra (Microsoft) por todo o suporte para a realiza√ß√£o deste meetup. E tamb√©m ao Edson Vieira, ao Milton C√¢mara Gomes e ao Ewerton Rodrigues Jord√£o pelas fotos tiradas durante as apresenta√ß√µes.
Os slides da apresenta√ß√£o j√° est√£o no SlideShare:
A aplica√ß√£o utilizada como base para a demonstra√ß√£o tamb√©m foi disponibilizada no GitHub:
https://github.com/renatogroffe/ASPNETCore2.1_JWT-Identity-Docker
Caso desejem ainda outras refer√™ncias sobre o uso de Docker com .NET Core/ASP.NET Core acessem tamb√©m o seguinte post:
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
O uso do Azure Web App for Containers foi tamb√©m assunto de uma apresenta√ß√£o recente que fiz no Canal .NET. A grava√ß√£o est√° dispon√≠vel no YouTube e pode ser assistida gratuitamente:
Se deseja saber mais sobre o suporte a HTTPS no ASP.NET Core 2.1 (ativado por default) consulte tamb√©m o seguinte artigo:
ASP.NET Core 2.1: instalando o Preview 1 e suporte a HTTPS
Refer√™ncias adicionais sobre o uso de JWT com ASP.NET Core podem ser encontradas no seguinte post que escrevi recentemente (inclui exemplos com implementa√ß√µes customizadas de controle de acesso, ASP.NET Core Identity, Refresh Tokens e consumo de APIs que empreguem JWT):
ASP.NET Core + JWT: Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
40 
1
40¬†
40 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-8db4be7d1c6a?source=search_post---------231,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 19, 2020¬∑1 min read
4-Parte: Cria√ß√£o da primeira rota do projeto
Dando continuidade a libera√ß√£o dos m√≥dulos do meu curso: Criando API‚Äôs RESTful utilizando TypeScript, Node e mongoDB, hoje eu irei demonstrar como criar a primeira rota do nosso projeto.
Caso tenha interesse em ver os primeiros passos dessa serie, segue link de cada um deles abaixo:
Cria√ß√£o da primeira rota do projeto
Nessa v√≠deo aula n√≥s vimos como criar a nossa primeira rota e as configura√ß√µes iniciais para rodarmos o projeto em um ambiente local.
Espero que tenham gostado e at√© um pr√≥ximo artigo pessoal ;)
Enjoy your life
132 
132¬†
132 
Aqui voc√™ vai encontrar os principais conte√∫dos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/windows-developer/develop-a-mind-reading-twitter-client-with-azure-cognitive-services-f73f0c937671?source=search_post---------232,"There are currently no responses for this story.
Be the first to respond.
Adding machine learning, artificial intelligence and cognitive abilities to your Windows applications is easier than ever, thanks to Microsoft Azure services. Imagine a Twitter client that practically checks the user‚Äôs state of mind before tweeting, potentially saving them an embarrassing tweet made in anger. With the Emotion and Text Analytics APIs from Azure Cognitive Services, not only is this possible, it‚Äôs easy to do, analyzing both the text of a tweet and the user‚Äôs facial expressions, before the send button is clicked.
Azure Cognitive Services includes a portfolio of machine learning powered APIs that fall into five primary areas:
¬∑ Vision ‚Äî Image/video processing and classification services, etc.
¬∑ Speech ‚Äî Speech-to-text, voice verification, etc.
¬∑ Language ‚Äî Natural language processing including translations, linguistics, spell check, etc.
¬∑ Knowledge ‚Äî Recommendation systems, academic knowledge, etc.
¬∑ Search ‚Äî Bing Search API that covers web, media, news, etc.
To access these services, API keys are necessary:
1. Navigate to the Cognitive Services API Console.
2. Login or create a free account.
3. Select the Vision tab.
4. Locate the row for Emotion API and click Get API Key button.
5. Select the Language tab.
6. Locate the row for Text Analytics API and click the Get API Key button.
7. Make a note of the endpoint and API keys.
To get started creating the Safe Tweet app, follow these simple steps to set up your project:
¬∑ Open Visual Studio 2017
¬∑ Choose File -> New -> Project from the main menu
¬∑ Select Windows Universal from the left navigation pane
¬∑ Select Blank App (Universal Windows) from the right pane
¬∑ Enter the name of your app in the Name field (we‚Äôre using ‚ÄúSafeTweet‚Äù)
¬∑ Enter a Location, or keep the default
¬∑ Select the OK button
After creating the initial project, Visual Studio will prompt you to select the target and minimum platform versions.
¬∑ Ensure the Target version is set to Windows 10 Fall Creators Update (10.0; Build 16299)
¬∑ Set the Minimum version to Windows 10 Fall Creators Update (10.0; Build 16299)
We‚Äôre setting both to the Windows 10 Fall Creators Update because some of the packages we‚Äôre using have a dependency on .Net Standard 2.0.
The Text Analytics API is a great algorithm for determining the overall sentiment of a block of text to determine its positivity or negativity.
Microsoft provides a package via NuGet that simplifies integrating the Text Analytics API into a UWP app. Install the NuGet package from the NuGet console:
With this package installed into the project, a piece of text can be analyzed with the following steps:
1. Create an instance of the SentimentClient class using the API key previously obtained for the Text Analytics API in the Cognitive Service Console.
2. Create a list of SentimentDocument objects containing the text to be analyzed.
3. Call the GetSentimentAsync() method providing the list of SentimentDocument objects as a parameter.
4. Process the collection of Document objects returned from GetSentimentAsync().
Note, SentimentDocument contains an Id property that correlates a request to the corresponding response. In the sample Twitter client app, the Id happens to be unnecessary since only one line of text is being sent for analysis.
The Emotion API uses facial expression recognition to detect eight different emotions: anger, contempt, disgust, fear, happiness, neutral, sadness and surprise. This algorithm can detect multiple faces in a single image and provide confidence measurements for each emotion for each face.
Microsoft provides a package via NuGet that simplifies integrating the Emotion API in UWP Apps. Install the NuGet package from the NuGet console:
After installing this package, analyze the emotions in an image with the following steps:
1. Create an instance of the EmotionServiceClient class using the API key previously obtained for the Emotion API in the Cognitive Service Console.
2. Call the RecognizeAsync() method passing in the image itself as a Stream, or as a String that represents a Url to the image.
3. Process the collection of Emotion objects returned from RecognizeAsync().
The following code demonstrates these steps:
Note: The EmotionServiceClient class implements IDisposable, and therefore the instance needs to be disposed. This is different behavior than the SentimentClient class from the Text Analytics API.
Processing the collection of the returned Emotion objects is also straightforward. For every face detected in the image, the collection will contain a corresponding Emotion object. Each Emotion object corresponds to a face recognized in the image and includes:
¬∑ A Rectangle object with the coordinates and size of the face found
¬∑ An EmotionScores object, with a numeric score ranging from 0 to 1 for each emotion detected (the sum of which will equal 1).
The ToRankedList() helper method from the EmotionScores object can be used to rank the scores for easier consumption.
The following code demonstrates consuming the Emotions API and determines the primary emotion:
The above Emotion API sample uses a Stream of the image to analyze. There are several methods in the UWP to obtain an image ‚Äî reading it from disk or directly from a web cam. For the Safe Tweet app, reading directly from the web cam would create a seamless user experience. A good starting place to learn more about using the web cam in a UWP app can be found here.
Since UWP are sandboxed and have restricted rights, developers need to request access to specific capabilities:
¬∑ In Visual Studio, open the project manifest and select the Capabilities tab.
¬∑ Select the Webcam capability.
The MediaCapture class is a mechanism used to capture video and photos from a webcam and provides a more seamless approach than using the CameraCaptureUI class (which simply uses the Windows built-in camera app).
Before using the MediaCapture class, it must be initialized, as follows:
Note, failure to set the capture mode to StreamingCaptureMode.Video value, MediaCapture will default to both video and audio and will require adding the Microphone capability in the project manifest.
Once MediaCapture is initialized it can be used to take a photo as follows:
Converting the captured photo to a Stream that the Emotion API can use is a little more involved:
While not part of the Cognitive Services API, the sample app wouldn‚Äôt be complete without demonstrating how to connect to Twitter and post the tweet.
Start by visiting https://apps.twitter.com/app/ to create and register the app.
Note: The callback URL is optional and does not need to be provided.
After submitting the above form, click the Create my access token button and select the level of access needed (read only, read and write, or read, write and access direct messages). For this app, select Read and Write.
Once completed, the Twitter registration page will present a summary of the app‚Äôs setting including the consumer key, the consumer secret as well as the access token and access token secret.
This consumer key and consumer secret will be needed to connect the UWP app to the Twitter infrastructure (note: the consumer secret should never be human-readable in your app or checked into source code repositories).
Back in Visual Studio, install the UWP Community Toolkit Services NuGet package.
This package not only allows developers to easily integrate with Twitter, but many other services including Bing, Facebook, LinkedIn, OneDrive, and more.
The Twitter Service provides a rich API allowing consuming apps to tweet, retrieve the user profile or timeline, or live-stream tweets. The following snippet shows how to connect to the Twitter service and send a text-based tweet.
There may come a day when most apps will perform cognitive tasks previously thought only possible by humans. Saving us from ourselves may just be a small component of the larger set of capabilities available with a few lines of code and Azure Cognitive Services.
We‚Äôve only scratched the surface of what is possible in a UWP app consuming Azure Cognitive Services and leveraging the UWP Community Toolkit. Check out the links below to explore more opportunities.
¬∑ Find sample source code on GitHub.
¬∑ Documentation
¬∑ Source
¬∑ Text Analytics Documentation
¬∑ Emotions Documentation
¬∑ 20-minute introduction video
¬∑ Check out the hard-hitting online training series on Microsoft Cognitive Services at Microsoft Virtual Academy
¬∑ Where would you use Cognitive Services? In Games, Mixed reality apps or maybe in beautiful Fluent apps.
Learn more about developing for Windows 10 here.
Everything you need to know to develop great apps, games‚Ä¶
60 
60¬†claps
60 
Everything you need to know to develop great apps, games and other experiences for Windows on PCs, phones, IoT, Xbox & HoloLens. wndw.ms/dev
Written by
Everything you need to know to develop great apps, games and other experiences for Windows.
Everything you need to know to develop great apps, games and other experiences for Windows on PCs, phones, IoT, Xbox & HoloLens. wndw.ms/dev
"
https://medium.com/house-of-haiku/azure-9fae0b67ad9?source=search_post---------233,"There are currently no responses for this story.
Be the first to respond.
Frictionless azure,dangling sparrows in time, are,prism refraction,infused color, spins madly, define blue synonyms, such.
Anna Rozwadowska 2020
"
https://medium.com/@jaychapel/7-ways-to-get-azure-credits-2df14d3a228f?source=search_post---------234,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 18, 2020¬∑4 min read
Azure credits are a perk offered by Microsoft that help you save money on your cloud bill. Like a gift card for a retail store, credits are applied to your account to help cover costs until they are exhausted or expire. In a sense, these credits act as a spending limit because any usage of resources or products that are not free will be deducted from the credit amount. We found 7 different ways that you can earn credits and start saving on your Azure bill.
If you‚Äôre a Visual Studio subscriber, you get monthly Azure credits that can be used to explore and test out different Azure services. The amount of Azure credits that you receive will depend on the type of Visual Studio subscription that you have.
With a Visual Studio Enterprise subscription, you get a standard of $150 in monthly credits. For subscriptions through MSDN Platforms you get $100 a month. For Visual Studio Professional and Visual Studio Test Professional, you get $50 a month.
Full-time students at an accredited, two or four-year educational institution in a STEM-related field are eligible for these credits.
When a student signs up with their school email address, Microsoft gives them $100 in credit in order to help them further their career and build their skills in Azure thanks to the free access to learning paths, labs, and professional developer tools.
With a free account, you get access to a number of popular Azure services for no cost. In addition to access to free services, you‚Äôll also get a $200 credit. It‚Äôs important to note that while the free account lasts for 12 months, your credits must be spent in the first 30 days.
Whether you‚Äôre just getting started in Azure or are looking to further your knowledge, a free account is always a great way to test the waters without having to make a long term commitment.
In the Partner Network, those that are members of Microsoft‚Äôs Action Pack program receive $100 of Azure credits every month. Based on your computing needs, you can use these credits for any Azure service; some examples include, Virtual Machines, Web Sites, Cloud Services, Mobile Services, Storage, SQL Database, Content Delivery Network, HDInsight, Media Services, and more.
The great part about this is that there are a handful of usage scenarios that won‚Äôt consume all of the $100 credit ‚Äî you can use this pricing calculator to estimate how much you could use with a $100 credit.
Any of the unused monthly credits can‚Äôt be carried over to succeeding months or transferred to other Azure subscriptions, so make sure to use it while you can!
This global program is designed to help startups as they build and scale their organizations. Part of the technical enablement features that are always free and available to all startups is $200 of Azure credits that can be used towards any service for 30 days. This is a great option for startups because it‚Äôs free and gives you the ability to explore all the different offerings without having to spend any money.
With Azure for Education, users are given access to the learning resources and developer tools that educators and students need in order to build cloud-based skills. This program is available to students, educators and institutions ‚Äî once signed up, educators get $200 of Azure credits.
Whether you‚Äôre teaching advanced workloads, interested in building cloud-based skills, or just getting started in your Azure learning journey, this program provides guidance and resources for individuals looking to further their knowledge in Azure.
In an effort to make their technology more affordable and accessible for nonprofit and nongovernmental organizations, Microsoft offers donated and discounted products. Each year, approved organizations receive $3,500 in Azure credits which can be used to purchase all Azure workloads created by Microsoft (excluding Azure Active Directory, which is licensed under EM+S).
No matter the industry you‚Äôre in or learning level you‚Äôre at, there are a wide variety of credits and resources offered that can help make Azure an affordable option for you.
Further Reading:
Top 3 Ways to Save Money on Azure
How to Save Money with Microsoft Azure Enterprise Agreements
9 Ways to Get AWS Credits
Originally published at www.parkmycloud.com on February 11, 2020.
CEO of ParkMyCloud
52 
1
52¬†
52 
1
CEO of ParkMyCloud
"
https://medium.com/@gmusumeci/how-to-create-an-azure-remote-backend-for-terraform-67cce5da1520?source=search_post---------235,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 24, 2020¬∑5 min read
For simple test scripts or for development, a local state file will work. However, if we are working in a team, deploying our infrastructure from a CI/CD tool or developing a Terraform using multiple layers, we need to store the state file in a remote backend and lock the file to avoid mistakes or damage the existing infrastructure.
"
https://koukia.ca/introduction-to-azure-event-grid-87b058408c70?source=search_post---------236,"Azure Event Grid is a managed event routing platform which enables you to react in real-time to changes that are happening in your applications hosted in Azure or any Azure resources that you own.
Today to get notified of a stage change in a resource, such as a new record in a database, or when someone creates a Virtual Machine‚Ä¶
"
https://medium.com/@maarten.goet/supercharge-your-powershell-defenses-with-azure-sentinel-mitre-att-ck-and-sigma-714e1e1825d3?source=search_post---------237,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
May 6, 2019¬∑8 min read
Jeff White from Palo Alto Networks recently wrote: PowerShell has continued to gain in popularity over the past few years as the framework continues to mature, so it‚Äôs no surprise we‚Äôre seeing it in more attacks. PowerShell offers attackers a wide range of capabilities natively on the system and with a quick look at the landscape of malicious PowerShell tools flooding out; you have a decent indicator of its growth.
Enter stage left ‚Äî the PowerShell ‚Äò-EncodedCommand‚Äô parameter. This command intends to take complex strings that may otherwise cause issues for the command-line and wrap them up for PowerShell to execute. By masking the ‚Äúmalicious‚Äù part of your command from prying eyes you can avoid strings that may tip-off the defense.
How can we detect this? MITRE‚Äôs ATT&CK framework, Sigma‚Äôs open source project and Azure Sentinel can be teamed up to supercharge your defenses against these types of attacks. Let‚Äôs look at how we can do this.
Malicious PowerShell usage
Quite a number of droppers use these Base64 encoded PowerShell commands. For instance, they might try and abuse a Word document through a macro that will try and run cmd.exe which in turn runs PowerShell.exe which then parses an encoded command starting with a $.
Here‚Äôs a sample PowerShell script for you to try out encoding some sample command yourself:
PRO TIP: My good friend and PowerShell MVP Ben Gelens pointed me at a Microsoft blog here that describes how PowerShell 5+ now supports the ‚ÄúBlue Team‚Äù better.
MITRE ATT&CK
One of the first things you might ask yourself is how do I know which tactics, techniques and procedures (TTP‚Äôs) my adversaries are using? Unless you‚Äôve been involved with research on bad actors and/or have been working on the ‚ÄúRed‚Äù (offensive) side yourself, you might not have all the information available on what people are using to attack you.
This is why the InfoSec community is about sharing information. Each might have a piece of the puzzle, and putting them together will provide all of us with the bigger picture, and will allow us to up our defenses in a bigger way. There are numerous conferences, blogs and other ways to get those learnings, but the MITRE ATT&CK project is one well worth mentioning.
What is MITRE? They describe themselves as:
‚ÄúMITRE ATT&CK is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations.‚Äù
It‚Äôs a community project where many people with hands-on experience contribute. For instance Christiaan Beek, lead at McAfee, shares his learnings on TTP‚Äôs actively with MITRE.
The information MITRE ATT&CK provides can be consumed in many ways. They provide website which allows you to search the information, there is a ‚Äòattack navigator‚Äô that gives you an interactive way to work with the TTP‚Äôs, and there is a programmatic way (API) to retrieve the information as well.
If we query the ATT&CK framework, we find a TTP with number T1086 called ‚ÄúPowerShell‚Äù. MITRE provides information about the attack vector, which APT groups typically use it, and information on which phase of the ‚Äòkill chain‚Äô it maps to: Execution.
Two paragraphs in the TTP information MITRE provides could be of interest to you: Mitigation (how to defend against it) and Detection (monitoring, and possibly hunting):
‚Äú[..] If proper execution policy is set, adversaries will likely be able to define their own execution policy if they obtain administrator or system access, either through the Registry or at the command line. This change in policy on a system may be a way to detect malicious use of PowerShell. [..]‚Äù
Sigma
So you‚Äôve learned about the TTP‚Äôs that your adversaries might use. But we still need to ‚Äòtranslate‚Äô this approach to some real detection logic for our SIEM system. We need to provide the query the alert rules will use, and/or need to capture the relevant data to use for hunting later on.
Again, if you have been working on the offensive side, you might already have this information readily available. But most defenders will need to investigate the attack vector and come up with a plan as to which data sources to monitor, what to look for, and how the alerting logic would need to be configured.
And if you do find out what to monitor for, you‚Äôll be most likely configuring it directly into your SIEM configuration and maybe not first write down the abstract of what you‚Äôre trying to achieve. (This is also the reason that replacing SIEM system is tough; no history of why something is configured there in the first place ‚Äî but that‚Äôs a topic for another blog).
Florian Roth started a community project called Sigma which aims to provide a Generic Signature Format for SIEM systems. The detection logic is written in YAML format, and these .yml files can then be converted/translated into queries or rules for your specific SIEM systems (fi: Azure Sentinel, ArcSight, etc).
‚ÄúSigma is for log files what Snort is for network traffic and YARA is for files.‚Äù
The Sigma project has fully embraced MITRE‚Äôs ATT&CK framework as a way to classify the attack vector. If you look at the rule specification, in the tags section you‚Äôll find fields called ‚Äúattack.<number>‚Äù and ‚Äúattack.<tactic>‚Äù. For our PowerShell TTP this would translate to attack.t1086 and attack.execution.
On Sigma‚Äôs Github repository you‚Äôll find a folder called ‚ÄòRules‚Äô that contain quite a number of Sigma rules that Florian, other members of this project, or the broader community have developed and contributed. Under /rules/windows/powershell you‚Äôll find a lot of rules for detecting PowerShell attack vectors.
PRO TIP: Interested in what MITRE ATT&CK coverage the public Sigma rules repo covers? Florian posted an overview here.
There are many ‚Äòimplementations‚Äô of abusing PowerShell, but the one we were looking for is the one that uses Base64 encoded commands. The Sigma rule/definition for this one can be found here. A big thank you to Florian Roth and Markus Neis for the research and for sharing it to the Sigma repo.
PRO TIP: Want to get some technical details of Markus‚Äô and Florian‚Äôs research? In the YAML file they point to this URL which contains some great information!
Azure Sentinel
Now that we have the Sigma rule, you will want to put that detection logic into your Cloud SIEM, Azure Sentinel. However, we need to take a final step: converting that YAML file into something that Azure Sentinel can read/process.
Guess what, Florian and his team also thought of that! It‚Äôs called SIGMAC as in Sigma Converter. In the /tools folder of the GitHub repo you will find it as a Python script. The scripts can output to a couple of common SIEM formats, fi: Splunk, Kibana, Arcsight and one that is called ‚Äúala‚Äù.
The target called ‚Äòala‚Äô stands for Azure Log Analytics. This is the correct one, as Azure Sentinel uses Azure Log Analytics as its backend as you could read in my Azure Sentinel Design Considerations blog. Choosing ‚Äòala‚Äô as the target in the Sigma Converter will produce the right KQL query as the result:
If you want to know more about Event ID 4688 which this signature uses, you can have a look on UltimateWindowsSecurity.com and get detailed information.
PRO TIP: Don‚Äôt forget to check the basics ‚Äî am I collecting the events from the security eventlogs on my Windows servers? Is auditing properly enabled and configured in the OS? Are my Microsoft Monitoring Agents healthy and reporting into Azure Sentinel?
Putting it all together
Open up Azure Sentinel in the Azure portal and click on ‚ÄòAnalytics‚Äô. Select ‚ÄòAdd‚Äô to create a new rule. Provide a name, description, severity, and paste in the KQL query. In the Alert Simulation section you‚Äôll see if the query would have triggered.
PRO TIP: Make sure you select the right fields in the Entity Mapping section because you will need this later for Hunting. Then select the alert scheduling and alert suppression parameters, and Save the rule.
Now sit back and relax, and wait for the rule to kick in when a malicious Powershell command is run in your environment:
Because you‚Äôve used the Entity Mapping, the alert maps back to the corresponding server, IP address, etcetera, which will assist you in Hunting. Not sure how to do this? Read my blog on using Jupyter and KQL to go Threat Hunting with Azure Sentinel.
Summary
Having a common open framework and taxonomy around TTP‚Äôs such as MITRE ATT&CK helps a great deal to organize our defense efforts. Combining that with a generic SIEM format to define the logic needed to detect these TTP‚Äôs such as SIGMA is really helpful. Having a community that actively contributes to these SIGMA rules is probably the best thing since sliced bread!
Downloading the alert rules and converting them to be used in Azure Sentinel has never been easier thanks to the community efforts of MITRE and SIGMA. Supercharge your cloud SIEM today!
A big shout out to Florian Roth (SIGMA), Jeff White (Powershell), Markus Neis (Alert Rule), Christiaan Beek (TTP),the people behind MITRE and many many others in the community who generously provide this to all of us!
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
41 
41¬†claps
41 
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jeffhollan/using-entity-framework-with-azure-functions-a32d09382b94?source=search_post---------238,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
May 28, 2019¬∑4 min read
One of the exciting developments from Build this year was support for dependency injection in Azure Functions. This means you can register and use your own services as part of functions. While you‚Äôve been able to use Entity Framework Core in the past, the pairing with dependency injection makes it a much more natural fit. Let‚Äôs create a simple Azure Function that can interact with stateful data using Entity Framework Core. I‚Äôm going to create a very simple API that can get and set blog data in an Azure SQL Database.
First up, I created a brand new v2 (.NET Core 2) Azure Functions project. I selected the HTTP Trigger template to start with. In order to use dependency injection I need to verify two things:
While we‚Äôre at it, let‚Äôs install the Entity Framework libraries. ‚ö†Ô∏è These versions are very particular with the version of .NET you will be running. So choose your version of these libraries thoughtfully. At the time of writing this Azure is running .NET Core 2.2.3 so that‚Äôs what I‚Äôll use.
I also want to take advantage of the design-time migration, so I‚Äôll install the tools needed to drive that.
Next, I‚Äôll create a very simple DbContext for the data models I want to interact with. Let's just stick with the simple Blog and Posts entities.
I need to verify that I‚Äôm using constructors in my function classes so I can inject in the right dependencies (in this case, the entity framework context). This means getting rid of static and adding a constructor. I‚Äôll also modify the constructor to accept a BloggingContext created above (which itself will need a DbContextOptions<BloggingContext> injected).
To register services like BloggingContext, I create a Startup.cs file in the project and implement the FunctionsStartup configuration. Notice I use an attribute to signal to the functions host where it can run my configuration.
As mentioned, I want to use the design-time DbContext creation. Since I‚Äôm not using ASP.NET directly here, but implementing the Azure Functions Configure method, entity framework won't automatically discover the desired DbContext. So I'll make sure to implement an IDesignTimeDbContextFactory to drive the tooling.
There‚Äôs one last thing I need to do. When you build an Azure Functions project, Microsoft.NET.Sdk.Functions does some organizing of the build artifacts to create a valid function project structure. One of the .dlls it moves to a sub-folder is the project .dll. Unfortunately, for the design-time tools like entity framework migration, it expects the .dll to be at the root of the build target. So to make sure both tools are happy, I'll add a quick post-build event to copy the .dll to the root as well. I added this to my .csproj file for the project.
I went into the Azure Portal and created a simple Azure SQL database. I copied the connection string into the local.settings.json file with a new value for SqlConnectionString. You can see in my previous code samples I used that as the environment variable that would have the connection string. This way I don't have to check it into source control ü•Ω.
To make sure the connection string is also available to the CLI tooling for migrations, I‚Äôll open my Package Manager Console and set the environment variable before adding a migration and updating the database.
When browsing to the SQL database in the Azure Portal, I can see the tables were successfully created!
That‚Äôs all the heavy lifting. Now I just write a few simple HTTP triggered functions that can return and add data to the BloggerContext I injected in. When running the app, I can then call the different GET and PUT methods to validate data is being persisted and returned correctly.
After publishing, I just make sure to set the appropriate application setting for SqlConnectionString with my production SQL connection string, and we're in business!
You can see the full project code on my GitHub account
Originally published at https://dev.to on May 28, 2019.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
87 
2
87¬†
87 
2
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
"
https://koukia.ca/faster-build-and-deploy-by-using-kubernetes-and-azure-dev-spaces-within-visual-studio-bc850ecd9810?source=search_post---------239,"One of the issues that you will have while working with container images and Kubernetes, is the build process when you are coding.
Like when you have everything running in a Kubernetes Cluster in Azure and you just want to make some changes and push it to the cluster, normally you would have to compile the code, build‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/bootstrapping-azure-cloud-to-your-terraform-ci-cd-f06ddd951f69?source=search_post---------240,"There are currently no responses for this story.
Be the first to respond.
Pairing Terraform with a CI/CD like Azure DevOps, Terraform Cloud, or GitHub Actions can be incredibly empowering. Your team can work on code simultaneously, check it into a central repo, and once code is approved it can be pushed out by your CI/CD and turned into resources in the cloud.
"
https://medium.com/@jeffhollan/serverless-doorbell-ring-com-and-azure-functions-part-2-98bc8fb43e3c?source=search_post---------241,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Dec 21, 2017¬∑5 min read
This is part 2 in a series of how I have been using Azure Serverless to extend and connect the functionality of my ring.com video doorbell. Part 1 is here.
During my first afternoon I got my doorbell sending events to Azure Event Grid whenever it detected motion or was pressed. The next step was to create subscribers to those events. At the time of writing this I‚Äôve created two subscribers ‚Äî create a log entry in Cosmos DB, and analyze the recording with Azure Cognitive Services. I‚Äôll save the Cognitive Services for part 3. This blog will go into detail of wiring-up Event Grid to Azure Functions, and how to use Cosmos DB bindings in the new v2 runtime to create a document.
This was my first opportunity to build a function trigger from Event Grid and I learned a lot in the process. The first is that you actually have 2 options for triggers when working with event grid.
The first option is to use the Event Grid extension with the eventGridTrigger type in your function.json file. This is the method that is used if you select the Event Grid template in the Azure Functions portal experience. There are a few aspects worth calling out with this method:
You can also just use and manually register any HTTP triggered function to fire on events from Event Grid. The difference is pretty much the counter to each of the points above (requires manual debatch, wouldn‚Äôt leverage queues in the same way, but can locally test).
For purposes of easier local debugging I went with an HttpTrigger function which will create a document in Cosmos DB about the ring event.
Azure Functions has a concept in addition to triggers called ‚Äòbindings.‚Äô A binding is a small bit of standard code to allow for faster integration with different services. Bindings include things like Azure Storage (pull in or push out blobs), Azure Queues, Cosmos DB, and about 20 more.
For my ring.com Azure Functions, I have been developing them in VS Code on my MacBook, which means I‚Äôve been using the current preview (and cross-platform) v2 Function Runtime. This is relevant because in v2 a few changes happened in relation to the Cosmos DB (previously DocumentDB) integration with Azure Functions.
The first big difference when developing my Cosmos DB ring.com functionality is I had to manually install the Cosmos DB binding extension into my project. One of the goals of the v2 runtime is keep the base image minimal, so unused extensions wouldn‚Äôt exist on each function instance. This means I need to specify which NuGet packages to include in the function app that the host will include when running.
The experience isn‚Äôt quite as seamless as we want it to be yet, but for now here‚Äôs how I enabled Cosmos DB:
Once the setup was complete, I could add the Cosmos DB binding to the function.json config file for my HttpTrigger function.
NOTE: There are some differences between this binding and the v1 DocumentDB binding, like connectionStringSetting and the type
Once setup was complete, the function code is actually extremely simple. It will trigger on an Event Grid event, pull out the first event, and add it to the Cosmos DB collection via the binding configured above.
The final step was to deploy the function, and create an event subscription in the Event Grid topic created in part 1. Now whenever my doorbell gets pressed or motion is detected, an event is automatically stored in Cosmos DB for me.
In hindsight doing something this simple would have likely only taken me 1/10th of the time to build in Azure Logic Apps with the Event Grid trigger and Cosmos DB connector, but was worth building this way to learn some of the ins and outs of the Azure Functions v2 Cosmos DB binding.
In the next part I will be extending my serverless solution to include Logic Apps to orchestrate some powerful video AI whenever someone approaches the doorbell.
Continue to Part 3 ‚Äî Facial Recognition powered by Cognitive Services and Logic Apps
https://github.com/jeffhollan/functions-node-ring-doorbell
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
66 
66¬†
66 
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
"
https://medium.com/@ben11kehoe/funamentally-i-look-at-envoy-istio-or-what-azure-provides-inside-service-fabric-and-i-dont-b7a9f740624e?source=search_post---------242,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben Kehoe
May 30, 2017¬∑2 min read
Paul Johnston
Funamentally, I look at Envoy + Istio, or what Azure provides inside Service Fabric, and I don‚Äôt see the ability to achieve the majority of those features through serverless platforms in AWS, Azure, or GCP. I can build a service myself (using serverless architecture) to provide those features, but what I want is the serverless platform (for me, AWS) to provide the service mesh (discovery, incremental rollouts, circuit breaking, etc.), and at lower levels than needing an API Gateway per microservice.
Just to be clear, using an API Gateway per microservice does not provide the features of a service mesh today anyway, and I also don‚Äôt want API Gateway to be the focus of such features.
There are two reasons I don‚Äôt want to use an API Gateway in front of every microservice. The first is performance: it introduces latency without providing much inherent benefit. The reason there is little inherent benefit is the second reason: that within my application, everything understands AWS resources anyway, so the benefit of an HTTP API is reduced. It‚Äôd be great to have a way to communicate the expected input format for a Lambda, but for internal invocations that‚Äôs more documentation than anything else.
I‚Äôve already accepted a large amount of vendor lock-in by virtue of building a serverless application dependent on the FaaS and SaaS services in the platform ‚Äî and, as I‚Äôve argued, this lock-in should not be seen as troublesome. But the lock-in means that inside the system, I don‚Äôt need to abstract over the details of the platform.
API Gateways, to me, are useful exactly as gateways: entry points for external clients to access the system, whether that‚Äôs an IoT device, a mobile app, a web client, or a 3rd party application. And for those clients, I want to present a single endpoint to access. If every microservice is fronted by its own API Gateway, that means putting another API Gateway in front of those, adding additional latency.
Cloud Robotics Research Scientist at @iRobot
86 
86¬†
86 
Cloud Robotics Research Scientist at @iRobot
"
https://medium.com/@serhatcan/aws-microsoft-azure-google-cloud-gibi-bulut-bilisim-servisleri-pahali-mi-e191928f89e3?source=search_post---------243,"Sign in
There are currently no responses for this story.
Be the first to respond.
Serhat Can
Dec 9, 2017¬∑3 min read
10 - 100 milyon kullanƒ±cƒ± seviyesinde olan ve insanlarƒ±n √ßok vakit ge√ßirdiƒüi SnapChat, Slack, AirBnb gibi firmalardan tutun, devasa end√ºstri devi GE, Siemens gibi firmalar bile kendi sunucularƒ±nƒ± y√∂netmek yerine bulut bili≈üim servislerini kullanƒ±yorlar, kullanmak zorundalar. Peki bu firmalar i√ßin bile bulut bili≈üim pahalƒ± deƒüilse, size neden pahalƒ± geliyor?
Bulunduƒüunuz √ºlkenin yasa ve yasaklamalarƒ±ndan dolayƒ± kullanmamanƒ±z dƒ±≈üƒ±nda bulut bili≈üimin sƒ±nƒ±rsƒ±z kaynaklarƒ±ndan neden faydalanmadƒ±ƒüƒ±nƒ±zƒ± a√ßƒ±klamanƒ±z √ßok zor hale geldi.
Bulut bili≈üim dediƒüimiz zaman aynƒ± ≈üeyi anladƒ±ƒüƒ±mƒ±zdan emin olmak i√ßin √ßok kƒ±sa bulut bili≈üim servislerinin genel √∂zelliklerini ≈ü√∂yle sƒ±ralamak istiyorum:
Amazon‚Äôda 1 GB RAM ve 1 CPU √ßekirdeƒüi sunan standard ve g√º√ßs√ºz diyebileceƒüimiz Linux sunucu bile aylƒ±k 8.5$ (30+ TL).
Haklƒ±sƒ±nƒ±z, olaya sadece b√∂yle bakarsak pahalƒ± gelebilir. OpsGenie‚Äôde ilk i≈üe ba≈üladƒ±ƒüƒ±mda, AWS‚Äôin saƒüladƒ±ƒüƒ± nimetlerin farkƒ±na varmadan √∂nce, bulut bili≈üim servisleri bana da pahalƒ± g√∂r√ºn√ºyordu.
Peki aslƒ±nda ne zaman pahalƒ±:
G√ºvenlik ve g√ºvenilirlikten bahsetmek istememin sebebi bulut bili≈üim denilince insanlarƒ±n aklƒ±na gelen ilk soru i≈üaretinin bu konular ile ilgili olmasƒ±. Teknik detaya girmek istemediƒüim i√ßin bu konularda ≈üu servis ≈üunu saƒülar bu servis bunu saƒülar diye yazmayacaƒüƒ±m. Ama bulut bili≈üim servislerinin saƒüladƒ±ƒüƒ± imkanlarƒ± kendiniz saƒülamanƒ±z g√ºn√ºm√ºzde neredeyse imkansƒ±z. √úcret a√ßƒ±sƒ±ndan bakarsak bulut bili≈üim servislerinin saƒüladƒ±ƒüƒ± g√ºvenlik sertifikalarƒ±nƒ± almanƒ±z veya uygulamanƒ±zƒ±n problemlere kar≈üƒ± daha dayanƒ±klƒ± olmasƒ± i√ßin d√ºnyanƒ±n deƒüi≈üik yerlerinde √ßalƒ±≈ümasƒ±nƒ± saƒülamanƒ±z ise √ßok pahalƒ± i≈üler.
Yani bulut bili≈üim sanƒ±lanƒ±n aksine √ßok √∂zel bir uygulama alanƒ±nƒ±z (√∂rneƒüin savunma sanayi) yoksa √ßok daha az paraya g√ºvenlik ve g√ºvenilirlik saƒülayabileceƒüiniz bir ortam.
Nasƒ±l faydalanacaƒüƒ±nƒ±z ve y√∂neteceƒüinizi biliyorsanƒ±z bulut bili≈üim servisleri √ßok √ßok ucuz ve her ge√ßen g√ºn daha da ucuz hale geliyor.
Ula≈ümak ve ileti≈üimde kalmak isterseniz sosyal medya hesaplarƒ±m:
Medium: https://medium.com/@serhatcanTwitter: https://twitter.com/srhtcnLinkedin: https://www.linkedin.com/in/serhatcan/
Ba≈üka yazƒ±larƒ±mƒ± okumak isterseniz:
medium.com
medium.com
linkedin.com/in/serhatcan/
80 
80¬†
80 
linkedin.com/in/serhatcan/
"
https://medium.com/microsoftazure/creating-a-massively-scalable-wordpress-site-on-azures-hosted-bits-db6e95bb35b0?source=search_post---------244,"There are currently no responses for this story.
Be the first to respond.
I had yet another discussion about WordPress the other day with a friend here at my co-work space (Impact Hub in Honolulu). I told them I was working on a deployment script for spinning up WordPress and Ghost on Azure‚Äôs infrastructure and their response was what I‚Äôm used to:
WordPress? Dude‚Ä¶ is this new gig warping your mind?
It might be. My response was a variation of the same one I always give:
WordPress powers 30% of the entire web. I think supporting it well on Azure is a no-brainer.
I use WordPress. I like it and yes there are things that, as a developer, I wish could be different. The simplicity of the thing, however, is what really attracts me to it. So, if you‚Äôre a person who uses WordPress and you also use Azure, this post might prove useful!
Disclosure: I work at Microsoft in Developer Relations. My job is to figure things like this out and then 1) tell the product teams what could work better and 2) report to you how I managed to get it all to work. If this is helpful, hooray!
There are other ways to do this, but I like ‚Äúscripted recipes‚Äù. There are ARM (Azure Resource Management) templates as well that allow you to click a button and ‚Äúdeploy to Azure‚Äù which I think is great, but I‚Äôm a bit of a control freak and dig my shell scripts (which may not be the best so sound off if you see improvements).
The other reason I like shell scripts is that you can add comments and use them as a bit of a learning tool. At least that‚Äôs what I‚Äôm hoping to do here ‚Äî learning this Azure stuff for me hasn‚Äôt been easy and hopefully my notes help some of you.
If you want to jump right to it, here‚Äôs a gist you can read through. Yes, I know, I could just use a Docker Compose file and be done. I wanted a better database solution and I also wanted the challenge to‚Ä¶ back under the bridge with you!
Finally ‚Äî if you want to play along, make sure you have the Azure CLI installed and that you‚Äôre logged in with a default subscription set.
The first thing we need to do is declare our variables, etc:
At the very top I‚Äôm setting the resource group as RGbecause it‚Äôs used everywhere. I‚Äôm then setting the APPNAMEand LOCATION, the latter with a default of ‚ÄúCentral US‚Äù.
When you create things in Azure, you use ‚Äúresource groups‚Äù for logical groupings. This is extremely helpful when you want to wholesale remove everything when you‚Äôre playing around, which I am. It also keeps project-based stuff nice and tidy.
The name of your website needs to be unique across Azure as well. I usually try to name mine with the resource group appended to it somehow. I‚Äôll talk about the user name and password stuff later.
The final step is to create the resource group. Now let‚Äôs create MySQL.
This, to me, is the main benefit of using Azure. You get a super-scalable MySQL instance to back your fully-managed website. Most one-click installs you see online give you a single VM with MySQL installed right next to WordPress. This usually works fine (such as with this blog you‚Äôre reading) but if you‚Äôre planning on ramping up, it can be a bit scary.
The first thing to decide is the size of our database. You can ramp this thing to the moon if you want, but I suggest keeping it small to start. I‚Äôve listed out the SKU structure in the script:
This code creates a smaller MySQL instance ‚Äî in fact it‚Äôs the smallest one you can create. I ran into some issues with certain SKUs not being supported in certain regions, but if you get an error just drop the resource group and try again.
We then spin up MySQL. The one big thing to notice here is --ssl-enforcement Disabledwhich caused me some headaches. WordPress doesn‚Äôt use SSL out of the box, which you can change with a plugin or config changes (surprise surprise). If you‚Äôre behind a firewall or part of a VPN, this might not be a huge deal. You can change this later on, however, when the plugin is installed.
The final thing to point out is the admin-user and admin-passwordsettings. I am not a fan of leaving the root level stuff to the user, but right now that‚Äôs the way it is. To softly get around this, the script creates a random user name and generates a GUID for the password, which you can see in the first script above.
We‚Äôre now ready to open a firewall and create our WordPress database.
Our server is up and running but is locked down completely, which I think is marvy. We need to be explicit about who/how/what can access our server, so we need to open a few holes in the firewall. The first will be for us, the second will be for our website later on:
The first hole is for my local IP address, which I‚Äôm grabbing using curl. This is a subshell that pings ipinfo.io and reads the response, setting it to a variable. I use that response to create the first firewall rule: AllowMyIP.
The next rule looks a bit scary. I‚Äôm creating a firewall rule that allows any service within Azure to ping my server. This is what the Azure docs have to say about it (emphasis mine):
To allow applications from Azure to connect to your Azure SQL server, Azure connections must be enabled. When an application from Azure attempts to connect to your database server, the firewall verifies that Azure connections are allowed. A firewall setting with starting and ending address equal to 0.0.0.0 indicates these connections are allowed. If the connection attempt is not allowed, the request does not reach the Azure SQL Database server.
ImportantThis option configures the firewall to allow all connections from Azure including connections from the subscriptions of other customers. When selecting this option, make sure your login and user permissions limit access to only authorized users.
Azure SQL Database and SQL Data Warehouse firewall rules
This might seem suboptimal when it comes to security because it is. However, this is how every service works within any data center. We can, if we want, further lock this thing down later on when we know the IP of our site (assuming it won‚Äôt change).
The final step in the code above is to use the mysqlclient to create the remote database. I couldn‚Äôt find a way around this. I know that most people don‚Äôt have MySQL installed locally, so this could be a problem for you. I‚Äôm hoping to find a better solution someday.
I‚Äôm a control freak and I like to have access to my database. I also hate trying to remember login information so to help myself out I‚Äôll setup a .envfile with the relevant bits:
If you don‚Äôt know, a .envfile typically contains environment variables and other things intended to help you with development. They might contain local database configuration, etc. Here, I‚Äôm outputting some environment variables and I‚Äôm also setting up an alias so I can connect to prodas needed.
Side note: if you use zshell and oh-my-zsh! there‚Äôs a plugin called ‚Äúdotenv‚Äù that will automatically load any .env file in a directory when you navigate to it.
You don‚Äôt want this checked into source control!
We want our WordPress site to be fully managed, from the database all the way down to the website itself. This will allow us to flip a few bits in the future to add more cores to either the site or the database.
This is not a VM. Well, at least as far as you‚Äôre concerned. All of that is abstracted into Azure‚Äôs ‚Äúservice fabric‚Äù. That said, the first thing you need to do is to pick the size of your VM in the form of a plan :p:
You can think about this plan in terms of its capacity and compute power. B1 is the lowest but won‚Äôt work if you‚Äôre using Linux. If your WordPress site is going to power some serious traffic, you might consider a plan with a bit more horsepower to it. It‚Äôs not the most straightforward thing, but you can have a read about plans here.
Now we get to spin up our WordPress site. We could pull down the WordPress source (PHP) and then send it up to Azure, specifying deployment and runtime settings etc. Or we could simply specify the image on DockerHub:
When you create a webappon Azure, it needs to know 1) how to run it and 2) how to deploy it. You can specify a Python runtime, for instance, and ‚Äúlocal Git‚Äù as the deployment option. This would allow you to push from your local machine and have your Flask app ‚Äújust run‚Äù.
If you‚Äôre using Docker, however, the runtime is built into the image. All that Azure needs to do is to know where to get it. It‚Äôll use your plan to create and run a container. In the code above, I‚Äôm telling Azure to use the wordpress image, which Azure knows to pull down from DockerHub.
Having a look at the image description on DockerHub, you can see there are a number of environment variables that I can set for the image, namely the database connection bits. This is perfect for our needs, but how do you pass this through from Azure?
The good news is that Azure will pass whatever app settings you have for your webapp directly to your container:
The environment variables are expected by the WordPress container, and I‚Äôm setting them to the MySQL database that I created above.
The last setting, WEBSITES_PORT, tells Azure which port to forward to in the container. I think the default is port 80, but I‚Äôm setting it here just for clarity.
The final thing we need to do is a bit of a bugger for me: we need to explicitly turn on logging. I wish this just happened by default, like with Heroku, but it doesn‚Äôt so we need to:
I‚Äôm outputting everything here because I want to know exactly what happens when Azure tries to spin up this container. I‚Äôll be honest: debugging the container stuff has been difficult.
As I mention, I‚Äôm a huge fan of laziness and I like to add things to .env files when I can. Let‚Äôs do that now so we can tail our application logs with a simple command:
I‚Äôm setting the alias for the current shell and I‚Äôm also appending it to .envso I can access it later on.
We‚Äôre ready to go! The only thing left to do is to navigate to our new site. The first request will take a while as Azure is pulling down our image and running it for the first time. You can see this if you tail the logs while the site is loading, which is the very last step of our script:
I‚Äôm opening our new URL, loading our new .env file into the shell and then calling our new alias, logs. This will allows us to see what‚Äôs happening at Azure as WordPress spins up. Hopefully, after a minute or so, you see something like this:
It‚Äôs a good question, one I‚Äôve asked myself repeatedly over the last few days. The answer is simple: this WordPress site is massively scalable. That‚Äôs kind of a big deal. It‚Äôs running right on the Azure ‚Äúmetal‚Äù, so to speak and I don‚Äôt need to worry about upscaling a VM.
As I mention: I really like WordPress and I run my business on it. Having it all on Azure would be a nice peace of mind.
As for costs, you‚Äôll have to take a look at your subscription and decide if something like this is right for you. I have a regular subscription that I pay for and I can tell you that this setup is comparable with what I‚Äôve seen for WordPress hosting.
Every time I do stuff like this I end up with a big writeup for the product teams to look over. It‚Äôs important to note that this isn‚Äôt a situation where I ‚Äújust send an email‚Äù or file a bug. My team works closely with the product teams and they like (I think) that we‚Äôre beating on their stuff.
I‚Äôll be letting them know my thoughts on the MySQL stuff (admin user/pass, executing a remote command, etc) and a few more things that I need to get clarity on first. That‚Äôs a bit of a problem I have right now as I step into all of this: learning it is not straightforward.
I suppose that will be my biggest point of feedback. There‚Äôs so much to learn! It‚Äôs hard to get going and often I find myself doing things the ‚Äúold way‚Äù. It‚Äôs also my job to help get this stuff in order, which I like.
Originally published at rob.conery.io on January 9, 2019.
Any language.
51 
2
51¬†claps
51 
2
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author of The Imposter‚Äôs Handbook, founder of bigmachine.io, Cofounder of tekpub.com, creator of This Developer's Life, creator of lots of open source stuff.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@renatogroffe/asp-net-core-kubernetes-azure-community-bootcamp-abril-2018-115455a486e6?source=search_post---------245,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 5, 2018¬∑2 min read
Nesta √∫ltima quarta (04/04/2018) participei como palestrante do Community Bootcamp, realizando uma apresenta√ß√£o sobre o uso do Kubernetes no Microsoft Azure como solu√ß√£o para a orquestra√ß√£o de containers Docker (tomando como base imagens de aplica√ß√µes ASP.NET Core).
Este evento aconteceu em um dos audit√≥rios da Microsoft em S√£o Paulo-SP, contando ainda com apresenta√ß√µes sobre TypeScript e MongoDB ao longo da noite.
Gostaria de deixar neste post meu muito obrigado √† Cynthia Zanoni (Microsoft) pelo convite e ao Marcelo Mendon√ßa Miranda (Microsoft) por todo o suporte para que eu pudesse participar como palestrante. Deixo aqui tamb√©m meus agradecimentos ao Thiago Adriano (Microsoft MVP) e ao Milton C√¢mara Gomes pelas fotos tiradas durante a palestra.
Os slides da apresenta√ß√£o j√° est√£o no SlideShare:
Disponibilizei tamb√©m o projetos e os scripts utilizados durante a demonstra√ß√£o pr√°tica no GitHub:
https://github.com/renatogroffe/ASPNETCore2_Kubernetes
Caso desejem ainda outras refer√™ncias sobre o uso de Docker com o ASP.NET Core acessem tamb√©m o seguinte artigo:
ASP.NET Core + Docker Compose: implementando solu√ß√µes Web multi-containers
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
92 
1
92¬†
92 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@gmusumeci/how-to-bootstrapping-azure-vms-with-terraform-c8fdaa457836?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 26, 2020¬∑15 min read
Note: this story was updated on May 23, 2020 to add Complex PowerShell Scripts using Template_File and Variables (point 6).
A very common task when we deploy Azure Virtual Machines using Terraform is deploying applications and/or code into the machine at the boot time.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hashmapinc/doing-devops-for-snowflake-with-dbt-in-azure-db5c6249e721?source=search_post---------247,"There are currently no responses for this story.
Be the first to respond.
by Venkatesh Sekar
I recently wrote about the need within our Snowflake Cloud Data Warehouse client base to have a SQL-centric data transformation and DataOps solution. In my previous post, I stepped through how to create tables using custom materialization with Snowflake.
Continuing in that vein, I was recently asked by a customer to provide a path for them to do database DevOps for Snowflake. In general, database DevOps has involved quite a bit of complexity and ongoing tweaking to try and get it right. There are some tools available in the market today including:
But, other than Sqitch, they don‚Äôt support Snowflake yet, although, with the amount of momentum that Snowflake has in the market, I expect they will provide support in the not too distant future.
Enter dbt
Having used dbt as a data transformation and Jinja template-based tool, I was interested to see if it could potentially be the key to help unlock database DevOps for Snowflake.
As noted above, I was able to create the ‚Äòpersistent_table‚Äô materialization which provided an answer for creating ‚Äòsource tables‚Äô in DBT, and having done that I next developed a simple CI/CD process to deploy database scripts for Snowflake with dbt in Azure DevOps.
Stay with me and I‚Äôll step you through how to setup dbt to deploy the scripts. As always, the code is available in my git repo venkatra/dbt_hacks.
Here is a glimpse into the tools and solutions that I am using to make this happen‚Ä¶
dbt is a command line tool based on SQL and is primarily used by analysts to do data transformations. In other words, it does the ‚ÄòT‚Äô in ELT.
It facilitates writing modular SQL Selects and takes care of dependencies, compilation, and materialization in run time.
Azure Devops provides developer services to support teams in planning work, collaborating on code development, and building and deploying applications.
Organizations across industries rely on Snowflake for their cloud data warehousing needs ‚Äî net new data warehouses, migrations from legacy DW appliances (Netezza, Teradata, Exadata, etc.), and migrations from traditional Hadoop and Big Data platforms (Hive, HBase, Impala, Drill, etc.). Our clients are also using Snowflake for high-value solution areas such as Security Analytics and Cloud Visibility Monitoring.
Snowflake is fully relational ANSI SQL cloud data warehouse and allows you to leverage the tools that you are used to and familiar with while also providing instant elasticity, per second consumption-based pricing, and low management overhead across all 3 major clouds ‚Äî AWS, Azure, and GCP (GCP is in private preview).
The Continous Integration (CI) process is achieved using Azure Pipelines within Azure DevOps. This pipeline is typically invoked after the code has been committed, and the pipeline tasks generally handle:
In the case of a database scripts file there isn‚Äôt a great deal of validation that can be done, other than the following:
Snowflake currently does not have a tool that validates the script before execution, but it can validate during deployment, so in the Build phase I typically do these checks:
Given the set of all scripts, it‚Äôs essential to determine which scripts were added or updated. If these scripts can‚Äôt be identified, you will end up re-creating the entire database, schema, etc. which is not desired.
To solve this issue, I‚Äôll use the Azure DevOps Python API. Going through the docs, you will see different REST endpoints and determine detailed information on what was committed, when it was committed, and who committed it, etc.
The python script IdentifyGitBuildCommitItems.py has been developed in response to this. Its sole purpose is to get the list of commits that is part of the current build and their artifacts (the files that were added/changed). Once identified it would write them into a file ‚ÄòListOfCommitItems.txt‚Äô during execution.
I‚Äôll review the results in the below sections.
During the course of development, the developer might have created scripts for table creation as well as developed transformation models, markdown documentation, shell scripts, etc. The ‚ÄòListOfCommitItems.txt‚Äô that was created earlier would contain all of these scripts. Note that if a file was committed multiple times, the script will not de-dup the commits.
To keep things modular, the script FilterDeployableScripts.py was created. Its responsibilities are to:
The build pipeline is a series of steps and tasks:
These are captured in azure-build-pipeline.yml
The following screenshot highlights the list of artifacts that get published by the build. It also provides a sample output of ‚ÄòListOfCommitItems.txt‚Äô which was captured in the initial run.
Notice that the ‚ÄòDeployableModels.txt‚Äô file contains only the CONTACT table definition file, and ignores all other files that are not meant to be run.
Now take a look at the next screenshot from a different build run ‚Äî during this build run we saw the following:
Again, the ‚ÄòDeployableModels.txt‚Äô file contains only the ADDRESS table definition file and is not concerned with any other files that are not meant to be run.
A Continuous Deployment (CD) process is achieved with Azure Release Pipelines. The pipeline we are working on is geared towards the actual deployment to a specific snowflake environment, e.g. Snowflake Development Environment.
The ‚ÄúStage‚Äù section is usually specific to the environment in which the deployment needs to happen. It consists of the following tasks as seen below:
This is a Bash task with inline code below:
This is a Bash task with inline code below:
This sets up the various env configurations needed for dbt and used as part of the execution.
The ‚ÄòENV_‚Äô are variables that will be substituted at run time. They need to be defined in the variable section as below:
Upon release, the table will be created in Snowflake. Here is a screenshot of the successful run and the logs of the DBT_RUN task:
and below is the artifact in Snowflake:
Keep these limitations in mind when leveraging dbt for CI/CD with database objects‚Ä¶
I hope you‚Äôll try to replicate this simple CI/CD process to deploy database scripts for Snowflake with dbt in Azure DevOps. While there are some limitations, the potential is there to add value to your data ops pipelines.
You should also check out John Aven‚Äôs recent blog post (a fellow Hashmapper) on Using DBT to Execute ELT Pipelines in Snowflake.
If you use Snowflake today, it would be great to hear about the approaches that you have taken for Data Transformation, DataOps, and CI/CD along with the challenges that you are addressing.
I hope you‚Äôll check out some of my other recent stories also‚Ä¶
medium.com
medium.com
medium.com
Feel free to share on other channels and be sure and keep up with all new content from Hashmap here.
Venkat Sekar is Regional Director for Hashmap Canada and is an architect and consultant providing Data, Cloud, IoT, and AI/ML solutions and expertise across industries with a group of innovative technologists and domain experts accelerating high-value business outcomes for our customers.
Innovative technologists and domain experts helping‚Ä¶
87 
1
87¬†claps
87 
1
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/i-think-google-cloud-is-the-best-best-tech-best-pricing-best-support-best-roadmap-and-best-4b4e17856505?source=search_post---------248,"There are currently no responses for this story.
Be the first to respond.
I think Google Cloud is the best ‚Äî best tech, best pricing, best support, best roadmap, and best people. I work for Google Cloud, so I may be biased. So when folks ask me why Google Cloud is better than it‚Äôs competitors, perhaps it‚Äôs better for real users to speak for the platform themselves.
So I put together a list of customer opinions ‚Äî folks who live and breathe cloud every day, folks who take the time to understand the differences between vendors, folks who knows performance and cost characteristics because they actually run real-life workloads and deal with scaling, billing, and maintenance. Here it goes!
(this is just a small quick gathering ‚Äî send me more, and I‚Äôll be sure to update the list!)
Quizlet compares Google and Amazon VMs based on networking, performance, and price. Guess who comes out on top!
2. May 2016 ‚Äî ‚ÄúThe future of cloud computing: Google Cloud!‚Äù
Obulpathi from Monsanto opines on Easy of Use aspects of Google Cloud Platform and Amazon Web Services.
3. May 2016- ‚Äú5 unique features of Google Compute Engine that no cloud provider could match‚Äù
This Forbes editorial discusses what sets apart Google‚Äôs VM service, Compute Engine (TL;DR: sustained use discounts, preemptible VMs, custom VM shapes, online disk resizing, shared storage).
4. June 2016 ‚Äî ‚ÄúGCE vs AWS in 2016: Why you should NEVER use Amazon!‚Äù
A rather one-sided piece by thehftguy that talks about limitations of Amazon EC2 and how Google Compute Engine‚Äôs architecture overcomes such limitations.
5. July 2016- ‚ÄúWhy we moved from Amazon Web Services to Google Cloud‚Äù
Michael Lugassy of adtecho.com explains his reasons for migrating from AWS to Google Cloud, and some downsides of Google as well!
6. February 2016 ‚ÄúAnnouncing Spotify Infrastructure‚Äôs Googley future‚Äù
Spotify describes reasons for their move to Google Cloud ‚Äî level of innovation, big data technologies, and people. And, remember folks, next time someone tells you ‚ÄúSpotify chose GCP because they got a good deal‚Äù: when the market is not commoditized, it‚Äôd be foolish for companies to make decisions on cost.
7. September 2016 ‚ÄúTop 5 Advantages of Choosing Google Cloud‚Äù
A nice quick recap, although ‚ÄúGoogle Cloud Hosting‚Äù doesn‚Äôt have the same ring to it!
When you do the math, Google‚Äôs SSDs are 10x-ing the competition on the price-performance spectrum.
2. October 2016 ‚Äî ‚ÄúA Survival Guide for Containing your Infrastructure‚Äù
Tripstr switched from AWS to GCP, containerized, and saved 75% on their infra bill. Not bad!
3. November 2016 ‚ÄúGoogle is 50% cheaper than AWS‚Äù
A comparison of GCE and EC2 prices.
Google Cloud Storage comes out ahead on throughput, but loses on latency. Mr. Johnson correctly points out that the latency tradeoff is due to Google Cloud Storage being a multi-region service by default, unlike Amazon/Azure options, which are closer to ‚ÄúGCS regional buckets‚Äù.
2. October 2016 ‚ÄúGoogle Cloud Storage vs AWS S3‚Äù
Jishnu from Vuclip finds GCS from 4x to 20x faster than S3 for uploads while being 35% cheaper than S3.
This O‚ÄôReilly blog compares EMR to Google Cloud Dataproc.
2. September 2016 ‚Äî ‚ÄúFun with .. Google BigQuery‚Äù
Mr. Alvarez is entirely blown away by BigQuery, and leverages the GHCN Public Dataset to come up with some fun conclusions.
Google Cloud community articles and blogs
85 
2
85¬†claps
85 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
VP of PM at Firebolt. All views are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/javarevisited/my-favorite-free-microsoft-azure-cloud-courses-for-beginners-to-learn-in-2020-3418524bb531?source=search_post---------249,"There are currently no responses for this story.
Be the first to respond.
Hello guys, Cloud computing is an in-demand skill and essential for Software developers, but with clouds also comes doubt. Among, AWS, Google Cloud, and Microsoft Azure, which one should you learn first?
Does learning Azure make sense given AWS is the most popular public cloud platform? Well, I think learning the Azure cloud makes a lot of sense especially if you are looking for a job in the technical and financial world as Microsoft has the largest market share in the corporate world.
Microsoft has done a great job with their cloud services; they have successfully created services such as testing, deploying, building, and managing applications through data centers. Microsoft is changing different elements in terms of cloud computing with all these data centers and high-speed internet, we are exploring new boundaries. It‚Äôs uncanny how things are progressing, but we need to keep with new technology. If you want to learn Microsoft Azure concepts and services and looking for free online training courses and classes then you have come to the right place. In the past, I have shared free and paid courses to learn Microsfot Azure, AWS and Google Cloud Platform and today, I am going to share free courses to learn the Microsoft Azure Platform. We have sorted out and handpicked the best and free Microsoft Azure online courses from places like Udemy, Youtube, and Pluralsight. These courses are going to provide you with great insight into Microsoft Azure Cloud services and functioning. Each course has focused on a certain area of learning, so it‚Äôs of utmost importance that you take a look at all those courses personally as well. We will be providing you with a short description of the course that can provide you with the summary, of course, their content, and the vision behind them.
By the way, if you don‚Äôt mind paying few bucks for learning a useful skill like the Microsoft Azure platform and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out AZ-300/AZ-303 Azure Architecture Technologies Exam Prep 2021 course by Scott Duffy on Udemy. It‚Äôs one of the most comprehensive courses to learn the Azure Cloud platform.
udemy.com
Without wasting any more of your time, her is the best and free online course to learn the Microsoft Azure cloud platform from Udemy, Pluralsight, Youtube, and other popular online learning websites. We have sorted out the best out of all the free course for learning, here‚Äôs our list:
This course is designed for novices in the cloud computing section. To start learning this course you‚Äôll need basic knowledge about how things work in the IT industry, to which you don‚Äôt need to be a part of one.   You can easily gain this knowledge on the internet by talking to people who are in the industry.  Course content:
This course is going to behave as a launchpad for the people who are willing to learn. Guys who are looking to go in sales can also benefit from this course, the course focus on explaining the technical concept of working so guys in sales can look like a veteran with all the information available.
Here is the link to join this course ‚Äî Getting Started with Cloud Computing using Microsoft Azure
This course is developed to provide basic conceptual learning to enthusiasts. It doesn‚Äôt have any prerequisites, and you can get started now if you want.  The course begins with fundamental concepts such as, what is the cloud? From here it focuses on explaining little by little how you can work on azure etc. This course is strictly visual and conceptual. You‚Äôll get the conceptual knowledge of the cloud and its services which would help you in further learning of advanced features.  Content of Course:
This course works wonders by providing you frame-of-reference learning which is highly effective to solve complex problems in advanced azure learning. Also, it provides bonus features and access cards to a hands-on lab to enhance learning. The course is regularly updated so all your queries will be answered.
Here is the link to join this course for FRE ‚Äî Microsoft Azure Concepts
The course focuses on pure learning with short video lectures. Theoretical parts are not the main focus here you will be applying your knowledge. This course would be more meaningful if you have knowledge of dot net technologies and C.  The target audience of this course is people who are looking to make a career in cloud computing and web development.  Course Content:
This course is going to provide you the ease of learning that you‚Äôll end up using azure for your day to day purpose. You‚Äôll learn the sense of coding from a certified teacher who has worked many hours in the IT industry. The course is divided into 13 lectures which mainly focuses on visual learning.
Here is the link to sign up this course for FREE ‚Äî AZ ‚Äî 900 Microsoft Azure Fundamentals
Cloud ranger network on youtube has solely dedicated the channel to provide you the course for the azure cloud. They have all sets of the playlist from the beginners to the advance. The focus on visual learning and implementation. You‚Äôll be able to learn a lot from real-time implementations. For the beginners we would like to recommend watching the discovery series, it‚Äôs going to give you a great start as you go on from learning about basics and service to advanced all through the course. It‚Äôll give a classroom experience as you can keep in touch with all the users from different professions. They are not targeting any audience but focusing on deep learning, hope you found that useful. It‚Äôs a guided course from beginners to advance, you‚Äôll get to learn all the latest work in this technology.
This is a free Coursera course to learn the basics of the Microsoft Azure platform online. In this introductory course, you will learn about essential Microsoft Azure services related to computing, storage, network, and memory. You will also gain familiarity with core Azure topics and practice the implementation of infrastructure components.  Here are the main things you will learn in this course
Overall a nice free online course from Coursera to learn key skills like cloud computing, cloud platforms, Microsoft Azure, and cloud databases. The course is offered by Learn2Quest, and delivered by instructor Kenny Mobley.
Here is the link to join this course for FREE ‚Äî Getting Started with Azure
And, if you find Coursera courses and certifications useful, then I also suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its complete worth of your money as you get unlimited certificates.
In this course, you‚Äôre going to learn about all the services that you can use in your solution and the ambition of business. This course is really helpful for the web developers as well as business enthusiasts who are targeting to go big with their ideas. Microsoft has done a terrific job with their cloud service, learning about these services can help you solve any modern-day problem. This course is going to provide a great start for beginners as it covers a lot of topics.  Content of course:
These visual presentations will give you a bigger picture and a point of reference to help things put into perspective. This course is taken from PluralSight which generally provides courses on a premium basis only, try learning that within the free trial period.
Here is the link to join this course ‚Äî Microsoft Azure: The Big Picture
Btw, you would need a Pluralsight membership to watch this course which costs around $29 per month or $299 per year. If you can afford then I highly recommend this subscription to you as it not only gives access to this course but more than 7000+ others. Alternatively, you can also use their 10-day-free-trial to watch this course for FREE.
That‚Äôs all about free online courses to learn Microsoft Azure and become a certified Cloud developer in 2021. All the courses are chosen after taking feedback from a large group of learners and tech enthusiasts.
We tried to cover all the zones of interest of learners by providing you with the best courses available. Hope you found them useful and we would like to suggest you open those personally to get better clarity about the courses.  By the way, if you don‚Äôt mind paying few bucks for learning a useful skill like the Microsoft Azure platform and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out Microsoft Azure ‚Äî Beginner‚Äôs Guide + AZ-900 ‚Äî UPDATED 2021 course on Udemy.
udemy.com
This course will not only help you to learn Azure basics and core services but also prepare you for Azure Fundamentals ‚Äî AZ 900 exam, which is a great way to mark your entry into the cloud computing world. You can also buy this course on Udemy sales at just $10, which happens every now and then.
Other Azure Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure courses useful, then, please share it with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P.S. ‚Äî If you are serious about learning the Microsoft Azure platform and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out the Microsoft Azure ‚Äî Beginner‚Äôs Guide + AZ-900 ‚Äî UPDATED 2021 course on Udemy.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
181 
181¬†claps
181 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://itnext.io/how-a-naughty-docker-image-on-aks-could-give-an-attacker-access-to-your-azure-subscription-6d05b92bf811?source=search_post---------250,"DISCLAIMER: the contents below are for general information purposes only. The exploit is only to emphasise the need to secure your AKS clusters and not to use untrusted docker images. Use of information contained here is therefore strictly at your own risk.
I will walk-through an exploit I created that allows a simple container to find the Azure service principal details from an AKS cluster and send that to a remote URL.
Most of the things used here are based on the taking over AKS clusters post I wrote a while ago. And a fundamental piece to achieve this is the Kubelet API trick, which allows you to poke around any running pod within a node, giving you get extra powers even if your pod runs as a non-root/least-privileged user.
To understand a bit better how we will get access to the Azure Subscription, you need to know that AKS is based on the ACS engine, which is an open-source project that eases the provisioning of clusters on Azure. It internally uses a file to store the Service Principal Name (SPN) so it can interact with the Azure API. That file exists in every single cluster node and is located in the path below:
There are a few different ways to get the contents of this file, but in essence, we will need to mount the host (node) volume onto a privileged container.
The sneakiest way I found to do this, is to manipulate the kube-proxy daemonset which already runs as privileged and has a mount to /etc/kubernetes/certs. As people tend to not pay much attention to what goes on within the kube-system namespace, the change we will make can easily be overlooked.
Depending on how your image is scheduled, you may or may not get an API Server token mounted in your pod. To make this more generic, we will get the token from a kube-proxy container:
Notice that we are leveraging the unauthenticated access to Kubelet API on IP 10.240.0.4. That is generally the first node within an AKS cluster. However, a cleverer exploit could find this dynamically.
The kube-proxy requires access to the path /etc/kubernetes/certs. We will amend that, so we actually mount its parent folder. That simple change will keep the system working, as the certs mapping will still be available, however, it will also provide us access to the azure.json file. This are the full contents we will get access to:
In theory, that could be done by exporting its yaml file, replacing kubernetes/certs with /kubernetes, then applying the change:
By default, the changes on daemonsets are only rolled-out on delete. So in the actual exploit we do it in three steps: 1. export, 2. remove existing kube-proxy, 3. apply changes.
Here‚Äôs the full exploit.sh:
When executed it will print the Azure SPN and also post it to an external URL.
With that information, anyone could login onto your Azure Subscription through Azure CLI.
Query its permissions:
By default, that SPN would be a contributor of the cluster resource group. However, if the person who provisioned the cluster did a bad job, this SPN may be a subscription owner, contributor, etc.
Either way, from this point an attacker have enough access to disrupt your services, or spin up new resources in the cluster resource group to do whatever they want ‚Äî did I just hear crypto-mining?
I have shared a similar exploit with the Microsoft Security team back in February 2018, and they are working on fixing it. In the mean time, make sure you only use trusted docker images in your clusters.
UPDATE 08/07/2018: Really good news here, this has since been fixed by Microsoft. The commands below will now result in HTTP 401 ‚Äî Not Authorized:
ITNEXT is a platform for IT developers & software engineers‚Ä¶
79 
1
79¬†claps
79 
1
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://blog.jeremylikness.com/migrating-azure-functions-from-v1-net-to-v2-net-standard-b2d724f9faf?source=search_post---------251,
https://medium.com/microsoftazure/training-your-first-distributed-pytorch-lightning-model-with-azure-ml-f493d370acb?source=search_post---------252,"There are currently no responses for this story.
Be the first to respond.
TLDR; This post outlines how to get started training Multi GPU Models with PyTorch Lightning using Azure Machine Learning.
Full end to end implementations can be found on the official Azure Machine Learning GitHub repo.
github.com
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
PyTorch Lighting is a lightweight PyTorch wrapper for high-performance AI research. Lightning is designed with four principles that simplify the development and scalability of production PyTorch Models:
Once you do this, you can train on multiple-GPUs, TPUs, CPUs and even in 16-bit precision without changing your code which is perfect for taking advantage of distributed cloud computing services such as Azure Machine Learning.
github.com
Additionally PyTorch Lighting Bolts provide pre-trained models that can be wrapped and combined to more rapidly prototype research ideas.
github.com
Azure Machine Learning (Azure ML) is a cloud-based service for creating and managing machine learning solutions. It‚Äôs designed to help data scientists and machine learning engineers to leverage their existing data processing and model development skills & frameworks.
Azure Machine Learning provides the tools developers and data scientists need for their machine learning workflows, including:
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
You can even use external open source services like MLflow to track metrics and deploy models or Kubeflow to build end-to-end workflow pipelines.
Check out some AzureML best practices examples at
github.com
github.com
With the advantages of PyTorch Lighting and Azure ML it makes sense to provide an example of how to leverage the best of both worlds.
Connect to the workspace with the Azure ML SDK as follows
docs.microsoft.com
To run PyTorch Lighting code on our cluster we need to configure our dependencies we can do that with simple yml file.
We can then use the AzureML SDK to create an environment from our dependencies file and configure it to run on any Docker base image we want.
Create a ScriptRunConfig to specify the training script & arguments, environment, and cluster to run on.
We can use any example train script from the PyTorch Lighting examples or our own experiments.
For GPU training on a single node, specify the number of GPUs to train on (typically this will correspond to the number of GPUs in your cluster‚Äôs SKU) and the distributed mode, in this case DistributedDataParallel (""ddp""), which PyTorch Lightning expects as arguments --gpus and --distributed_backend, respectively. See their Multi-GPU training documentation for more information.
We can view the run logs and details in realtime with the following SDK commands.
Now that we‚Äôve set up our first Azure ML PyTorch lighting experiment. Here are some advanced steps to try out we will cover them in more depth in a later post.
This example used the MNIST dataset from PyTorch datasets, if we want to train on our data we would need to integrate with the Azure ML Datastore which is relatively trivial we will show how to do this in a follow up post.
docs.microsoft.com
In this example all our model logging was stored in the Azure ML driver.log but Azure ML experiments have much more robust logging tools that can directly integrate into PyTorch lightning with very little work. In the next post we will show how to do this and what we gain with HyperDrive.
github.com
github.com
In this example we showed how to leverage all the GPUs on a one Node Cluster in the next post we will show how to distribute across clusters with the PyTorch Lightnings Horovod Backend.
In this example we showed how to train a distributed PyTorch lighting model in the next post we will show how to deploy the model as an AKS service.
docs.microsoft.com
If you enjoyed this article check out my post on 9 tips for Production Machine Learning and feel free to share it with your friends!
medium.com
I want to give a major shout out to Minna Xiao from the Azure ML team for her support and commitment working towards a better developer experience with Open Source Frameworks such as PyTorch Lighting on Azure.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with the Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
114 
1
114¬†claps
114 
1
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/aplica%C3%A7%C3%B5es-escal%C3%A1veis-com-asp-net-core-docker-e-o-microsoft-azure-net-sp-julho-2018-fec27463e34d?source=search_post---------253,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 29, 2018¬∑3 min read
No dia 26/07/2018 (quinta-feira) participei como palestrante em mais um meetup do grupo .NET S√£o Paulo (grupo do qual tamb√©m sou organizador), realizando uma apresenta√ß√£o sobre a constru√ß√£o de aplica√ß√µes Web escal√°veis com ASP.NET Core, Docker e o Microsoft Azure.
Gostaria de deixar neste post meu muito obrigado √† Eliane Barbosa (Accesstage) e ao Diogo Brito (Accesstage) por todo o apoio para que este evento pudesse acontecer, o que incluiu a disponibiliza√ß√£o do audit√≥rio utilizado e o coffee break para os participantes. E tamb√©m √† Cynthia Zanoni (Microsoft) e √† Silene Velasco (Microsoft) pelos brindes fornecidos para sorteio.
Os slides da apresenta√ß√£o j√° est√£o no SlideShare:
Esta palestra focou no uso de solu√ß√µes escal√°veis na nuvem como Azure Web App for Containers e Azure Kubernetes Service (AKS). Caso tenha interesse em conhecer mais sobre as mesmas acesse os artigos a seguir:
ASP.NET Core + Azure Web App for Containers: escalando uma API REST com containers Docker
ASP.NET Core + Azure + Kubernetes: Guia de Refer√™ncia
E tamb√©m as grava√ß√µes de 2 eventos recentes realizados no Canal .NET:
Termino este post agradecendo mais uma vez ao Diogo Brito e ao Luigi Tavolaro, agora pelas fotos tiradas durante a apresenta√ß√£o.
Refer√™ncias
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
ASP.NET Core + Azure + Kubernetes: Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
33 
33¬†
33 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/bb-tutorials-and-thoughts/how-to-get-started-with-azure-aks-275fe5d2db40?source=search_post---------254,"There are currently no responses for this story.
Be the first to respond.
AKS is Microsoft Azure‚Äôs managed Kubernetes solution that lets you run and manage containerized applications in the cloud. Since this is a managed Kubernetes service, Microsoft takes care of a lot of things for us such as security, maintenance, scalability, and monitoring. This makes us quickly deploy our‚Ä¶
"
https://faun.pub/lets-do-devops-auto-approve-safe-terraform-apply-in-azure-devops-ci-cd-f1b41afaf4c8?source=search_post---------255,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
Hey all!
I‚Äôve written a series of blogs about running an Azure DevOps Terraform CI/CD in an enterprise environment (for more info please see my profile). One item my business very much wanted, and which CI/CDs twist themselves up in knots to support is manual approvals for particular stages or steps.
"
https://medium.com/awesome-azure/azure-organize-and-manage-multiple-azure-subscriptions-and-resources-with-azure-management-groups-3472170b1407?source=search_post---------256,"There are currently no responses for this story.
Be the first to respond.
Introduction to Azure Management Groups ‚Äî Azure Subscriptions Planning and Designing Best Practices, Benefits, and Use Cases.
Azure Management Groups, Subscriptions, and Resource Groups are used together to establish the entire organizational structure in Azure, and they‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/data-science-video-series-to-get-started-with-machine-learning-on-azure-28d7c67b02eb?source=search_post---------257,"There are currently no responses for this story.
Be the first to respond.
Data science is about extracting knowledge from data. Data science is an important area of study because it is a tool that data scientists leverage to gain insights from data and prepare it for the machine learning modeling phase. By ‚Äúdoing data science‚Äù, data scientists actually apply techniques, such as data pre-processing and cleaning, feature engineering and descriptive statistics, to their data in order to understand it and start building AI solutions.
In this sense, data science has become an area of study that universities and companies should look at as a first step to start their machine learning journey:
You can read the full article here: https://techcommunity.microsoft.com/t5/educator-developer-blog/data-science-video-series-to-get-started-with-machine-learning/ba-p/1521995
Additional resources:
More videos coming:
The month of July 2020 is Data month for the Microsoft Reactors global live streams on Twitch! For the middle two weeks of July, you will get to dive even deeper on these and similar concepts with Sarah, Francesca, and other Cloud Advocates and Microsoft employees!
Any language.
33 
33¬†claps
33 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@jthake/introducing-azure-bot-service-5daaaaf8af7f?source=search_post---------258,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeremy Thake
Nov 22, 2016¬∑2 min read
I‚Äôve been working on the Azure Bot Service for the last few months internally as it got ready for its preview launch at Connect(); event last week. Andrea Orimoto who is a Program Manager in engineering did a great 7 min video that highlighted the benefits of the service.
channel9.msdn.com
I did a great 45 min video with Lars Liden who is a program manager in the team. The video shows how easy it is to get started and shows how the Language Understanding Intelligence Service (LUIS) can help make your bot seem more natural in conversation.
channel9.msdn.com
For me personally, this is really exciting as it allows developers to get started with a bot in a matter of minutes. My 3 top things:
This is a preview and there is room for improvement. My personal wish list is:
Microsoft Graph Team, Senior Program Manager at Microsoft.
10 
10¬†
10 
Microsoft Graph Team, Senior Program Manager at Microsoft.
"
https://blog.jeremylikness.com/deploy-angular-and-net-core-2-1-to-the-azure-cloud-part-four-d68594807c7a?source=search_post---------259,
https://medium.com/@gmusumeci/how-to-use-packer-to-build-a-windows-server-image-for-azure-52b1e14be2f2?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Apr 6, 2020¬∑7 min read
Packer is an open-source tool used to create virtual machine templates from a .json file.
In this story, we will learn how to use Packer to define and build custom images in Azure, and then use this image to build a Windows virtual machine using Terraform.
About
Write
Help
Legal
Get the Medium app
"
https://posts.specterops.io/azure-privilege-escalation-via-azure-api-permissions-abuse-74aee1006f48?source=search_post---------261,"Microsoft‚Äôs Azure is a complicated system of principals, securable objects, and the various ways access is granted to those objects. Some privileged actions are tightly controlled by Azure AD roles, while other actions are controlled by roles and object ownership. Many objects in Azure are subject to distinct permissions systems, which can make effective access auditing very difficult.
In this post, I will explain how one of those permissions systems can be abused to escalate to Global Admin. I‚Äôll explain how you as an attacker can abuse this system, and I will also explain how you as a defender can find, clean up, and prevent these abusable configurations.
This is not wholly original work. A few folks in the Azure world have spoken about this risk and possible mitigations already:
In the Azure offensive security world, at least two people have discussed API permissions abuse:
In the Azure defensive security world, Doug Bienstock, Juraj Sucik, and Jacob Skiba have created a tool called Mandiant Azure AD Investigator to help find evidence of adversaries abusing Azure app roles.
I also found Marius Solbakken‚Äôs blog to be an absolute treasure trove of Azure information which really helped me understand some of the more nuanced details of Azure.
Azure AD uses the concept of ‚Äúroles‚Äù to dole out privileges to principals. For example, ‚ÄúGlobal Admin‚Äù is an Azure AD directory role. Azure API permissions are a wholly distinct, parallel set of permissions that can be granted to Azure service principals. There is some overlap between Azure AD directory roles and Azure API permissions, but I think it‚Äôs best to think of them as parallel privilege systems.
These parallel systems can be used to control access to the same objects. They can be used to grant access to the same objects. But the particular privileges granted within the Azure API permissions system are only taken into account when a principal is operating against the target object through that API:
Before we go any further, let‚Äôs establish some vocabulary here. These systems are very complex and it‚Äôs easy to get confused when learning about this.
Principal ‚Äî an identity that can be authenticated. In Azure-land, a principal can be a user or service principal. When logging in with a username and password, you are authenticating to Azure with your user principal.
Azure AD App Registration ‚Äî an application object residing in an Azure tenant. Azure Apps are where all the nifty configurations happen, where you grant users access to the app and have the app do ‚Äúthings‚Äù.
Service Principal ‚Äî the identity an Azure App uses when it needs to authenticate to Azure. Service Principals can authenticate with a username and password ‚Äî just like a user can. And just like a user, Service Principals can have control of other objects in Azure.
API Permission ‚Äî an atomic, uniquely identifiable privilege, scoped to a particular Azure App. API Permissions come in two flavors: ‚ÄúDelegated‚Äù and ‚ÄúApplication‚Äù. API Permissions describe what particular privilege is granted to the Azure App.
API Permissions in the MS Graph API are written in ‚ÄúResource.Operation.Constraint‚Äù format. Example: ‚ÄúDirectory.ReadWrite.All‚Äù means that the principal granted this permission can Read and Write to All objects in the Directory.
App Role ‚Äî a permission granted by the Azure App, directly usable by the principal it has been granted to.
Delegated Permissions ‚Äî a permission granted by the Azure App, but only usable on behalf of a user that has authenticated to the app. Principals cannot use Delegated roles themselves, but they can impersonate a logged on user who *does* have that role, using the role on the user‚Äôs behalf.
Application App Role ‚Äî a permission held by the Azure App itself. The app can use this role without a user needing to log into the app first.
Resource App ‚Äî the uniquely identified service principal associated with the application that your Azure App accesses. App Roles are defined per Resource App.
Depending on the context, all of these terms can refer to the same object: Service Principal, Enterprise Application, Resource App, and First Party Application.
Confused yet? I sure as hell was. Let‚Äôs get visual and explain these moving parts and how they connect to form attack paths.
One of the most common Resource Apps you will interact with as an Azure admin, defender, or attacker is Microsoft Graph. Basically, all of the abusable administrative actions you‚Äôd ever want to take are possible through the Microsoft Graph API.
Every Azure tenant has a Microsoft Graph Resource App. You can find it in your own tenant by searching for its (current) display name, ‚ÄúGraphAggregatorService‚Äù. In my tenant (and yours, and every other tenant), the ‚ÄúApplication ID‚Äù for Microsoft Graph is 00000003‚Äì0000‚Äì0000-c000‚Äì000000000000:
Why the same ID? Because this app actually lives in a Microsoft-controlled Azure AD tenant. Let‚Äôs start thinking about these things in terms of a graph:
The app resides in the Microsoft tenant, but is instantiated as a service principal (or ‚Äúresource app‚Äù) in the SpecterDev tenant. These objects have the same display name, but they are different objects with different IDs. Additionally, the trust boundary represented in blue above means a Global Admin in the Microsoft tenant can‚Äôt control the Resource App in the SpecterDev tenant, and that a Global Admin in the SpecterDev tenant can‚Äôt control the Azure App in the Microsoft tenant.
Now let‚Äôs add some app roles into the mix. App roles are specific to each resource app. For the sake of explaining one of the privilege escalation possibilities here, we are going to focus on two app roles: AppRoleAssignment.ReadWrite.All and RoleManagement.ReadWrite.Directory:
At this point, these app roles are merely available for an admin to grant to a service principal, but no one actually has these permissions yet. Let‚Äôs go ahead and grant the ‚ÄúAppRoleAssignment.ReadWrite.All‚Äù app role to another service principal. We are going to make this an ‚ÄúApplication App Role‚Äù (as opposed to a ‚ÄúDelegated Permission‚Äù), so that the service principal itself has this privilege:
And let‚Äôs not forget that our ‚ÄúMyCoolAzureApp‚Äù service principal is associated with the Azure App, ‚ÄúMyCoolAzureApp‚Äù:
The stage is now set for ‚ÄúMyCoolAzureApp‚Äù to turn itself or anyone else into a Global Admin. To understand this, let‚Äôs discuss what these two particular app roles allow our service principal to do:
The Microsoft documentation says that the ‚ÄúAppRoleAssignment.ReadWrite.All‚Äù permission:
‚ÄúAllows the app to manage permission grants for application permissions to any API (including Microsoft Graph) and application assignments for any app, without a signed-in user.‚Äù
What does this mean? This means that ‚ÄúAppRoleAssignment.ReadWrite.All‚Äù lets you grant yourself whatever API permission you want. This particular role also bypasses the manual, human-involved admin consent process. Having this role means that ‚ÄúMyCoolAzureApp‚Äù can grant itself ‚ÄúRoleManagement.ReadWrite.Directory‚Äù:
And what can we do with ‚ÄúRoleManagement.ReadWrite.Directory‚Äù? The documentation says:
‚ÄúAllows the app to read and manage the role-based access control (RBAC) settings for your company‚Äôs directory, without a signed-in user. This includes instantiating directory roles and managing directory role membership, and reading directory role templates, directory roles and memberships.‚Äù
In other words, you can grant yourself any directory role you want, including Global Admin:
And with that, our attack path has emerged:
Here is a video of this attack path in action:
And here is the example attack code from the demo above:
In our last post about Azure privilege escalation, we provided some commentary around what Microsoft themselves may be able to do to mitigate or eliminate that particular attack path. An existing mechanism protects Global Admins from having their passwords reset by someone without the Global Admin or Privileged Authentication Admin role. We see that mechanism as potentially shutting down the entire attack primitive discussed in that post.
But this situation is different. It‚Äôs not immediately clear what Microsoft would be able to do to prevent privilege escalation via Azure App role abuse. These app roles are used by various organizations to support automated processes, and so the fact that the AppRoleAssignment.ReadWrite.All role allows you to bypass the admin consent experience really can‚Äôt be seen as a flaw, but as a crucial feature that enables full automation of privileged actions.
So what can we as defenders do about this? First, it‚Äôs prudent to proactively audit which, if any, of your apps have one of these two very dangerous app roles. As with everything in Azure, there are several ways to do this, but my favorite method is definitely using PowerShell.
To use this script, you will need a valid user and password for the Azure AD tenant. This user does not need any special roles or permissions ‚Äî every authenticated principal can read this information. Populate the $AzureTenantID variable with your tenant ID, the $AccountName variable with your username, and the $Password variable with your plain-text password. Then, you can copy and paste the entire script into a PowerShell console, or dot invoke it:
Here‚Äôs the script in action:
This is your first prevention opportunity. The output of that script is telling us that two apps, ‚ÄúThisAppHasOAuth2Permissions‚Äù and ‚ÄúMyCoolAzureApp‚Äù have app roles allowing those apps to promote themselves or another principal to Global Admin. Carefully determine whether those apps actually require those app role assignments, whether they can be changed to delegated permissions, or removed in favor of less powerful app roles.
If these app role assignments must remain, then recall from our last post that certain roles allow other users to create new credentials for service principals. The tenant-level and app-level ‚ÄúApplication administrator,‚Äù ‚ÄúCloud application administrator,‚Äù and ‚ÄúHybrid identity administrator‚Äù roles directly or indirectly allow for this. Additionally, owners of application objects and their associated service principals can add new credentials for the app. Finally, any service principal with the Application.ReadWrite.All app role can add new credentials to any other service principal.
Don‚Äôt forget that Privileged Identity Management (PIM) means you need to audit principals with roles currently active, and users who can grant themselves a role.
Here‚Äôs what all those different possibilities look like in a graph:
Seems intimidating, right? Don‚Äôt forget that security groups can have roles now, so the above illustration can actually be rather simple in comparison to what‚Äôs actually possible.
This is your second (and third, fourth, fifth, etc.) prevention opportunity. Carefully audit the principals that can control highly privileged service principals and remove role assignments, ownership, and app roles where possible.
I very strongly recommend investing your efforts into removing the most dangerous configurations you can whenever possible. If an adversary abuses these configurations to escalate to Global Administrator in your tenant, it can be very expensive to completely remove the various persistence mechanisms available to the adversary with this level of privilege. Even then, new research is coming out all the time about novel persistence tradecraft in Azure, so you may not even be able to guarantee you‚Äôve identified and removed all persistence installed by the adversary.
Even so, you may find yourself in a situation where these dangerous configurations must remain. There is also value in detecting when administrators legitimately add these dangerous configurations to your environment. There are a couple of options for detection particular to dangerous API permission grants.
First, you have the built-in ‚ÄúAudit-Logs‚Äù feature in Azure, which will by default log every API permission grant, telling you who made the change, who the permission was granted to, and what the permission and its scope are:
Next, you have the option of building a log analytics workspace and viewing all relevant events in a roll-up view in a monitoring workbook. Here‚Äôs how you set this up:
First, you need a Log Analytics workspace. You can either use an existing workspace or create a new one. Then, in our Azure tenant view, select ‚ÄúWorkbooks‚Äù under ‚ÄúMonitoring‚Äù to scope your Workbooks to your Log Analytics Workspace:
Then, under the ‚ÄúTroubleshoot‚Äù section, you can see an example workbook which includes new app role assignments. This workbook is called ‚ÄúSensitive Operations Report‚Äù:
Click this workbook, then select ‚ÄúNew permissions granted to service principals‚Äù, which will show you whenever a service principal has been granted an app role:
Azure is a complicated and dynamic system, with new attack paths appearing, disappearing, and reappearing over time. There are very well-designed mechanisms in Azure that prevent the emergence of some attack paths, but those mechanisms are not guaranteed to exist or go unchanged in the future. Microsoft has some insanely clever people working for them, but at the end of the day, we must remember two things:
https://samcogan.com/provide-admin-consent-fora-azure-ad-applications-programmatically/
https://graphpermissions.merill.net/permission
https://winsmarts.com/automating-application-permission-grant-while-avoiding-approleassignment-readwrite-all-554a83d5b6f5
https://docs.microsoft.com/en-us/graph/permissions-reference#permissions-availability-status
https://dirkjanm.io/azure-ad-privilege-escalation-application-admin/
https://techcommunity.microsoft.com/t5/azure-active-directory-identity/understanding-quot-solorigate-quot-s-identity-iocs-for-identity/ba-p/2007610
https://docs.microsoft.com/en-us/azure/active-directory/develop/consent-framework
https://tech.nicolonsky.ch/exploring-the-new-microsoft-graph-powershell-modules/
https://practical365.com/connect-microsoft-graph-powershell-sdk/
https://www.inversecos.com/2021/10/attacks-on-azure-ad-and-m365-pawning.html
https://docs.microsoft.com/en-us/graph/resolve-auth-errors
https://tech.nicolonsky.ch/explaining-microsoft-graph-access-token-acquisition/
Thank you Stephen Hinck and Matt Hand for reviewing this post.
Posts from SpecterOps team members on various topics‚Ä¶
32 
32¬†claps
32 
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
"
https://medium.com/@mauridb/azure-sql-managed-instances-and-azure-data-factory-a-walk-through-bfb93e79ac0c?source=search_post---------262,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
May 11, 2018¬∑5 min read
Now that Azure SQL DB Manages Instances are here, a lot of companies are trying to finally migrate their complex (multi-database, multi-dependency and database-centric) SQL Server database solutions to Azure SQL DB.
Once you have your Azure SQL DB Managed Instance running, you may also want to load or extract data from it. On-prem you may have used SQL Server Integration Services, and you may well continue doing so since SSIS Packages can run in the cloud thanks to Azure Data Factory:
docs.microsoft.com
but maybe you just want to create a shiny new Azure Data Factory pipeline to orchestrate your data curation and data movement activities using SQL MI along with other Azure technologies (Spark, HDInsight, Azure Data Lake or anything else Azure offers), without having to resort to SSIS Packages.
Well, in order to allow Azure Data Factory be able to connect to an Azure SQL MI there are some steps involved that may be not obvious at the first time. This post is here to help.
The Integration Runtime (IR) is the engine that allows Azure Data Factory to perform all its activities. The default IR doesn‚Äôt provide VNet support and thus it can‚Äôt be used to connect to SQL MI VNet, which means that it can‚Äôt be used to move data in and out of a SQL MI.
Using VNet is possibile on via the a Self-Hosted Integration Runtime. Self-Hosted means that you can install the IR engine in an Azure VM that has been configured so that in can connect to the SQL MI and thus can be used as a bridge between the SQL MI VNet and the outside world. There are two options to do that. The first is to create the VM in a SQL MI VNet subnet as described here (section ‚ÄúCreate a new subnet in the VNet for a virtual machine‚Äù)
docs.microsoft.com
The other is to use an Azure VM in a separate VNet and peer it to the SQL MI VNet (look for the ‚ÄúConnect an application inside different VNet‚Äù section):
docs.microsoft.com
Once you have the VM ready you can install the IR Engine. Create an Azure Data Factory (v2) resource and click on the ‚ÄúAuthor & Monitor‚Äù link to open the Azure Data Factory portal. Then click on the pencil icon on the left to open the ‚ÄúAuthoring‚Äù pane and from there click on the ‚ÄúConnections‚Äù link you can find at the bottom left of the screen. The ‚ÄúConnections‚Äù tab will open, and there you‚Äôll have also the ‚ÄúIntegration Runtimes‚Äù section. Click on it and you should see something like the following:
In my Azure Data Factory I can see also the ssis-runtime IR since I‚Äôve enabled SSIS support. If you‚Äôve just created the ADF resource and haven‚Äôt enabled SSIS support you won‚Äôt see that line.
It‚Äôs now time to create a Self-Hosted IR. Click on the ‚ÄúNew‚Äù link an then select the first option: ‚ÄúPerform data movement and dispatch activities to external computers‚Äù:
The next option will ask you if the IR needs to access private resources. Of course it will, so make sure to select the ‚ÄúPrivate Network‚Äù option:
then give your IR a name and a description and finally you‚Äôll see a page where you can download the integration runtime.
Download the installer via ‚ÄúOption 2: Manual Setup‚Äù, copy it to the VM you have created before and run it.
After the installer has finished its work it will ask for a key. Pick one of the key generated for you at the end of the creation of a Self-Hosted IR process and use it to register the engine:
after a couple of seconds you should receive the confirmation that the IR has been registered correctly:
and you should also be able to see it in the ‚ÄúIntegration Runtimes‚Äù list in the portal:
It‚Äôs now time to create a pipeline that uses it!
The process is now just the same you follow to create a pipeline to load data into an Azure SQL Database with the exception that when creating the Linked Service that will allow pipelines to connect to Azure SQL you have to specify the newly created runtime:
Or, if you‚Äôre using the Copy Data Wizard you have to select ‚ÄúVNET in Azure Environment‚Äù as value for the ‚ÄúNetwork Environment‚Äù option in order to be able to choose which IR to use:
Once this is done, your pipeline will run just fine, moving data from a Blob Store to your database in the SQL MI:
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
34 
1
34¬†claps
34 
1
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/is-azure-profitable-3531a14f6233?source=search_post---------263,"There are currently no responses for this story.
Be the first to respond.
No one wants to buy into faltering products that are here today and gone tomorrow and the same is true of businesses with regard to their IT infrastructure. Given the productivity dependencies, a traditionally slow rate of return, and steep financial burden of IT infrastructure, CTOs and the like around the world often look to key performance indicators (KPI) including revenue, profitability, customer counts, and so forth, much like a prospective shareholder, in order to anticipate the viability of infrastructure as a service (IaaS) platforms such as AWS and Azure prior to making long-term investments in them.
For example, if a service provider is breaking close to even or operating at a loss after 8 years, this could be indicative of operational inefficiencies, architectural oversights, and looming changes which increase the likelihood of costly outages. All of which may also highlight the possibility of rate hikes down the road or that the platform in question may not be standing up to the test of time. But if revenue and profit are on point, this is indicative of long-term stability and much less risk. Comparatively, it is like being given the choice of paddle boarding on a calm summer morning or in the middle of a winter snow storm.
Needless to say, you can tell a lot about a solution by measuring its profitability. This is why common frontrunners such as Amazon, Apple, and Google are not bashful about disclosing individual revenue, profit, and user counts of their products. But this is also why the competition behind them tends to get creative rather than simply reporting on the same metrics. And when considering the lengths that Microsoft goes though in order to suppress the individual merits of Azure, I am forced to question just how profitable Azure truly is.
Before anything else though, it‚Äôs worth highlighting that any refresher course focused on lying with statistics would remind you that omission is by far the easiest way to lie with them. Along with shying away from helpful metrics while promenading with misleading and less valuable metrics instead, essentially leveraging data like an octopus jetting its ink, their application in unison is a formidable and proven recipe for statistical gaslighting that companies behind the frontrunner of their industry love to resort to. Put simply, some businesses choose to plea ignorant and resort to diversionary tactics rather than throwing it on the table so to speak and shining themselves in a negative light with the truth; Microsoft is no exception to this. Apple is also guilty of this as of late.
As mentioned before and as the #1 cloud infrastructure provider, Amazon happily reports on the individual merits of AWS by posting revenue, profit, and user counts. Why hide being the best? As the #2 cloud infrastructure provider though, Microsoft opts to bundle Azure‚Äôs earnings into a container called Intelligent Cloud which averages Azure‚Äôs revenue, profits, and losses with legacy server software such as Windows and SQL Server, Active Directory, Hyper-V and so on, making it impossible to compare the two platforms on equal ground.
Azure reports total users but not revenue or profit outside of the Intelligent cloud (crutch?). Meanwhile, LinkedIn reports revenue, profit, and total users while omitting monthly usership statistics such as monthly active users (MAU). Although it‚Äôs Microsoft policy not to report on helpful metrics such as MAU, hence why LinkedIn no longer reports it, they seem to have no problem reporting it for Azure AD, Office 365, Windows, Edge, Cortana, Bing, Skype (ooops now they don‚Äôt), XBox, Minecraft, and other services that perpetuate the narrative of having a strong foothold in their market. With the above in mind, you can see how Microsoft seems to flop their policies tactically, but you can also see a clear trend of omissions being a tell just the same while seemingly trying to mask them with arbitrary policies.
Sometimes not to speak is to speak with statistics and omitting data along with creative bundling tactics which Microsoft is leveraging at present are not exceptions to this, but the billion-dollar standard. Although they claim to be changed now, they still have the same general council, Brad Smith, that they‚Äôve had since their laughable antitrust days, and we can also see Microsoft putting in extra effort into muddying the waters so to speak with run rates, obscure metrics, massive marketing overspending rather than simply presenting their data; as they have done historically.
However, we can still speculate by giving Azure the benefit of the doubt and simply assume that it is solely responsible for all of the Intelligent Cloud‚Äôs revenue in FY18 Q4, which was $9,606,000,000, so that we can have a look at Azure‚Äôs average revenue per account (ARPA) on its best hypothetical day and compare it to AWS. While we‚Äôre at it, let‚Äôs also assume that Azure has 13 million accounts rather than their 12.8 million just to account for growth since this data is over a year old. So let‚Äôs take $9,606,000,000/13 million accounts = $738.92 average revenue per account for their latest quarter. Not bad. Good job Hypothetical Azure.
AWS, on the other hand, has reported a paltry 1 million accounts subscribing to it at the moment while only generating 6.68 billion in revenue, 2.1 billion of which was profit, in FY18 Q3. So we can take $6,680,000,000/1 million accounts = $6,680 average revenue per account for their latest quarter. For the sake of comparison, we can then divide AWS‚Äôs ARPA by Azure‚Äôs ARPA ($6,680/$738.92) which shows us that AWS is monetizing its accounts 9.04x more effectively than Azure is. Also and if AWS could maintain this ARPA with Azure‚Äôs account-base (6,680*13,000,000), then it would be generating $86,840,000,000 in revenue per quarter. üò≥
AWS being 31.4% efficient (profit/revenue) while being more 9.04x more efficient than Azure when measured by its ARPA also indicates that Azure efficiency could be as low as 3.5% or $359,320,000 in Q4 which in itself would easily rationalize bundling it into a container such as the intelligent cloud comprised of more efficient products. This would also mean that Azure could be generating could be generating as little as $27.64 average profit per account compared to the $2,100 average profit per account that AWS is seeing at present which is still 76x more than Azure; when it is given a significant benefit of the doubt.
These differences only become greater if Azure represented half of the intelligent cloud‚Äôs revenue at $4.8 billion though. If that were the case, then they would be making averaging $369.23 in revenue per account and netting anywhere in between $163 million on the low end to $1.5 billion in profit if they‚Äôre as efficient as Amazon this quarter. If accurate, this would also show AWS to be as much as 18x more efficient than Azure from the perspective of ARPA. But I digress.
In summary, when a data-driven technology company as equipped as Microsoft turns their pockets inside out instead of posting basic KPIs such as itemized profits or MAU while reporting on metrics that no one asked for instead with regard to a service that‚Äôs been in production for 8 years now, it is usually a consequence of these KPIs being contradictory of the narrative that they‚Äôre selling, not because they are not readily available. In lieu of these metrics and even when giving Azure the benefit of the doubt and compared to AWS as done above, there appears to be more disparity between AWS and Azure than Microsoft would like us to believe after being in production for 8 years. Azure may indeed be profitable, but when considering the disparity in operational efficiency between AWS and Azure and the exhaustive effort that Microsoft makes towards suppressing its individual performance metrics, I am given no option but to ask how profitable Azure is or whether it is profitable at all.
#BlackLivesMatter
94 
how hackers start their afternoons. the real shit is on hackernoon.com.¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
94¬†claps
94 
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-best-courses-for-az-104-microsoft-azure-administrator-associate-certification-exam-in-2021-7b620d61dcd8?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for the AZ-104 exam or Microsoft Azure Administrator certification and looking for the best online courses then you have come to the right place. Earlier, I have shared the best AZ-900 courses and best AZ-204 courses, and today, I am going to share the best courses to crack the AZ-104 exam in 2021.
Acquiring skills in Cloud computing is fast becoming an opportunity and asset since most companies are applying cloud technologies in all their operations or looking for ways to learn how to do that. This is why becoming certified as a Microsoft Azure Administrator is now a big step towards advancing one‚Äôs carrier.
With the current industry trend, it is safe to say learning how to create applications that have some if not all of their components hosted in the cloud has become a skill that is very high in demand.
Candidates, who are considering studying for this course, must have a working knowledge of cloud infrastructure, ARM templates, Azure Portal, PowerShell, virtualization, storage structures, networking, command Line Interface, and operating systems.
If you‚Äôre not sure if this certificate would benefit you, then ask yourself if you are an Azure Administrator who works with cloud computing like computing cloud capabilities and services like storage, security, networking, etc. If you do, then having this certificate will be a massive step towards a more advanced career. The Microsoft Certified Azure Administrator ‚Äî Associate Exam AZ-104 (newer version of exam) tests your ability to manage Azure subscriptions and resources. They want to make sure you can use the storage and virtual machines (VMs), configure virtual networks, and handle identities. Below are courses that will help you prepare for your Microsoft Azure exam.
Without wasting any more of your time, here is a list of the best online training courses to prepare for Microsoft Certified Azure Administrator certification with code AZ-104.
These courses are created by expert trainers and certified Azure administrators. You can join them on Udemy and Pluralsight, two of the most reputed online learning platforms.
This is the best online course for Microsoft Azure Administrator Certification or AZ-104 certification. Created by Scott Duffy this course is termed a best seller on Udemy and has almost 80,000 students enrolled.
This course is fully updated to cover recent changes on the AZ-104 exam and it's also one of the most comprehensive courses for the Azure Administrator exam.
The course content covers:
Its target audience is senior technical people with exposure to Azure, operations teams who want to learn more about implementing cloud solutions, and people taking the Azure AZ-104 test. It, however, does not cover Azure development or architecture
Here is the link to join this awesome AZ-104 course ‚Äî AZ-104 Microsoft Azure Administrator Exam Certification 2021
This course is very good for mastering the fundamentals of Azure. It covers the concept of portal navigation and is an excellent choice for beginners in MS Azure. It, however, demands you to have some knowledge of PowerShell / CLI commands. Additional materials are recommended after this course if you are a newbie. It has more than 6000 students enrolled with a rating of 4.5 and is taught in the English language. The course was created by Anand Nednur. Before you take it, you need an intermediate understanding of Azure resources such as virtual networks, virtual machines, Azure AD, storage, and subscriptions and access to a paid or trial Azure to complete assignments. It also covers various Microsoft Azure administrator concepts from how to manage Azure subscriptions to how to integrate them on-premises networks with Azure virtual networks.
Here is the link to join this full course ‚Äî AZ-104: Microsoft Azure Administrator
If you have registered for the Microsoft Certified Azure Administrator ‚Äî Associate Exam AZ-104, then this course will help you pass it. In this course, you will learn how to manage subscriptions and resources on Azure and secure storage, implement and manage virtual machines and advanced virtual networking  This course is divided into three sections so you can go straight to the level you belong to at the moment. It has beginners, intermediate and advanced levels.  The beginners‚Äô section curriculum covers:
Here is the link to join this AZ 104 training path ‚Äî Microsoft Azure Administrator (AZ-104)
It also has an Intermediate section that covers the following key skills from the Azure Administrator exam point of view:
And, the Advanced section curriculum covers:
By the way, to access this course you would need a Pluralsight membership which costs around @29 per month or $299 per year. Alternatively, you can also try their 10-Day free trial to check out these courses.
pluralsight.pxf.io
This course contains well-structured videos that give an overview of each topic but still get specific to help you learn the material.
It is available in English, and more than 13,748 students enrolled with a rating of 4.4 out of 5.0. The target audiences for this course are Azure Administrators and Engineers and Systems Administrators, looking to expand into Azure. You need basic knowledge in Networking, OS (Windows/Linux) Background with experience in PowerShell Client on macOS or Windows and Azure Trial from Microsoft or Paid Subscription. This course follows the Microsoft Certified Azure Administrator ‚Äî Associate Exam AZ-104 outline that will enable candidates to study specifically for the exam with Skylines Academy SKYLABS Guide to practice even when you are offline.
There are an ARM and Automation bonus course included which, although not mandated, is a very solid foundation for administering Azure.
Here is the link to join this Azure course ‚Äî Microsoft AZ-104: Azure Administrator Exam Certification
This is one of the highest-rated Azure administrator course from Udemy. created by Alan Rodrigues this course prepares you to take the Microsoft Azure Administrator Exam AZ-104 and It has a rating of 4.6, with 26,444 students enrolled.
The course is very engaging and you will find both quizzes, exercise as well as lab tasks to learn essential Azure concepts from the Azure Administrator's point of view.   The course content reveals the following topics.
But before you take this course, an understanding of on-premises virtualization technologies, network configuration, Active Directory concepts, and resilience and disaster recovery is required.
Here is the link to join this AZ-104 online course ‚ÄîAZ-104 Microsoft Azure Administrator
This is another set of practice questions that I highly recommend you to check before you go for Microsoft AZ 104 exams. This AZ 104 Exam simulator is offered by David Mayer of Certification-questions.com and it contains around 250 questions and most importantly it is always updated to cover recent exam changes.
Here are some important details about this exam simulator:
- Microsoft AZ-104 Practice Exam: Microsoft Azure Administrator
- Number of Questions: 246
- Exam Tests: 5
- Last Update: 2021‚Äì01‚Äì10
Here is the link to join this AZ 104 exam simulator ‚Äî Microsoft AZ-104 Questions
Practice tests are a very important part of your Azure Certification preparation strategy as you can use this to build the speed and accuracy required to solve exam questions within the stipulated amount of time.
This Practice AZ-104 Azure Administrator exam on Udemy has 250 high-quality questions with detailed explanations so that you can pass AZ-104 with confidence!
There are a lot of courses out there that are claiming that their courses are fully updated, but they‚Äôre actually not! This practice test on Udemy contains the latest additions on the new AZ-104 topics, the exam received an update in Jan 2021.
These Microsoft Azure Administrator practice tests are patterned after the latest exam format and were crafted based on the feedback of their 40,000+ students that have enrolled in our Azure preparation courses.
Each question has detailed explanations at the end of each set that will help you gain a deeper understanding of the Azure services.
Here is the link to join this AZ 104 Practice test ‚Äî AZ-104 ‚Äî Microsoft Azure Administrator Practice Tests 2021
That‚Äôs all about some of the best courses to learn the Microsoft Certified Azure Administrator exam. These online training courses are very good to prepare for this prestigious exam and they cover all exam topics and provide structured learning experienced.
This means you can prepare for this exam in less time and also score high at the same time. Though, You don‚Äôt need to join all these courses, you can do well even if you join one or two courses from this list.   Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure Administrator courses for the AZ-104 exam useful, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. ‚Äî If you are keen to pass the Microsoft Azure Administrator certification but looking for free online training courses for your preparation then you can also see Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It‚Äôs completely free and all you need is a free Udemy account to join this course online.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
222 
2
222¬†claps
222 
2
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://towardsdatascience.com/full-stack-machine-learning-on-azure-f0f6b77be07e?source=search_post---------265,"Sign in
There are currently no responses for this story.
Be the first to respond.
Doug Foo
Oct 29, 2019¬∑20 min read
Guide to building a Full Stack Machine Learning site on Azure using MERN (Mongo, Express, React (Node.js))
Having gotten rusty on the latest tech, I took it upon myself to concurrently learn a whole bunch of things at once and build http://diamonds.foostack.ai which is now live. (Basic machine learning app to price diamonds).
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-run-and-deploy-react-with-nodejs-backend-on-azure-app-services-b853f6e5234f?source=search_post---------266,"There are currently no responses for this story.
Be the first to respond.
If you want to deploy your application on the managed platform by selecting the runtime, An App Service is the right choice. You can create a function app, web app, logic app, etc under Azure App Services. When it comes to React‚Ä¶
"
https://medium.com/hashmapinc/business-wont-wait-migrating-to-azure-for-data-analytics-15736d2bc59?source=search_post---------267,"There are currently no responses for this story.
Be the first to respond.
by Ed Fron, Enterprise Architect
I recently worked with an industrial company that was retiring an on-premise, traditional, Big Data platform that was operational, but it just wasn‚Äôt keeping up with business scenarios that spanned multiple business areas or data domains, and it was also struggling with handling data volumes and data loading requirements without requiring significant administrative overhead and budget.
In this post, I‚Äôll highlight the solution set they chose and discuss some of the benefits they are realizing already in moving to a new approach that takes full advantage of the cloud.
Importantly, they weren‚Äôt looking for a like-for-like style replacement (moving to another traditional Big Data platform or IaaS-based solution), but were focused on reviewing the range of SaaS-based, cloud-native solutions that were available in Microsoft Azure (their chosen cloud partner) so that they could either eliminate or significantly reduce the infrastructure effort associated with operating and maintaining existing analytics applications and delivering new apps at a more rapid pace.
It was also compelling to move to cloud solutions that would eliminate static limitations on usage, configuration, and capabilities while providing a consumption-based approach and paying only for the services that they used versus a static capital sunk cost model ‚Äî an IaaS cloud model wasn‚Äôt going to cut it.
Before jumping straight into the cloud solution set they chose, below is a quick recap on how the existing traditional solution was being used. They had centralized a significant number of structured and semi-structured datasets into their Hadoop-based enterprise data lake for original data fidelity and then worked with a combination of Apache Hive as a consumption zone and a big data analytics workbook solution that ran on the core platform that helped their business analytics parse through the datasets to extract value and insights.
From a data flow perspective, analysts would create workbooks which would be processed in the Hadoop environment and then exported to Hive tables for broader use by data consumers that were using the workbooks for data interaction or PowerBI for visualization. Workbooks sourced data from other workbooks or from datalinks, and a series of joins, calculations, etc. produced a final sheet containing all required data for export within a single workbook.
This architecture and overall process, while it worked, came with a corresponding amount of overhead and limitations.
When we got the call to assist the client in their cloud migration and deployment in Azure, they reviewed with us the core solution components that they had selected (after an in-depth review and testing process) and that we would be working with during the project ‚Äî I‚Äôll quickly highlight those below to give you a sense for the architecture and how each component is being used.
Below is a high level architectural diagram with each component slotted in.
Moving from left to right on the diagram you‚Äôll see the common data sources such as on-prem data shares, cloud storage, on-prem data warehouses, and both Oracle and SQL databases.
Azure Data Factory is a managed orchestration service that allows moving data using multiple data source connectors from a source into Azure Blob Storage for original format storage. Once the data lands in Blob Storage it‚Äôs then available for processing by the Azure Databricks Spark engine. The file ingestion into Snowflake‚Äôs Cloud Data Warehouse was defined in an Azure Databricks Notebook and orchestrated using Azure Data Factory pipeline.
Dropping down to the lower part of the diagram, for relational data the Attunity Data replication solution was used to simplify and accelerate the process of migrating and consolidating data from different internal and external database sources into Snowflake. Attunity moves data from Oracle and SQL databases into Snowflake without having to write complex ETL code and works in for both batch and real time. Once the lands in Snowflake it can be picked up and processed by the Azure Databricks Spark engine.
From that point, each ‚Äútraditional‚Äù Big Data workbook was analyzed and refactored into one or more Azure Databricks Notebooks. Each Azure Databricks notebook read data from Snowflake tables into a Spark dataframe, and then executed transforms (aggregations, grouping, joins, filters, calculations, etc.) to ultimately produce the desired final Spark dataframe which is published to Snowflake for highly concurrent, interactive data consumption by PowerBI users (interactive dashboards and reporting) and also other Azure Databricks notebooks. Azure Data Factory provided an orchestration service for the pipelines to ensure that the schedule was enforced and to track any runtime dependencies.
That‚Äôs the basic pattern that was used to migrate well over 1,000 existing Big Data analytics workbooks to Spark jobs in Azure Databricks.
I‚Äôd say that the biggest challenge during the cloud migration was interpreting how each individual business analyst wrote their individual workbooks since the tool that was used previously did not enforce and normalize concepts on breaking the workbooks down into smaller workbooks, transforming them, and creating the final workbook ‚Äî there were alot of one-offs that required fairly significant business analysis.
Looking back, here‚Äôs my perspective as an implementation and consulting partner on some of the high level benefits that were realized by the client:
I hope this gave you a sense for the solution used to retire a significant base of on-prem, traditional Big Data infrastructure and associated workbooks using a combination of high value, complementary Microsoft Azure services.
Although you should always work from your own desired business outcomes and use case requirements keeping in mind restrictions around organization, existing technology frameworks, data access, etc., in general, the approach above can be used as a template for other data and analytics cloud migrations and the benefits that can be expected.
We continue to be asked to assist clients with this type of cloud-first solution approach while building and engineering end-to-end pipelines to derive quicker, more cost-effective value from their data. We look forward to seeing more customers succeed and doing a lot more together in the near future!
If you‚Äôd like additional assistance in this area, Hashmap offers a range of enablement workshops and assessment services, cloud migration services, and consulting service packages ‚Äî we would be glad to work through your specific requirements ‚Äî please reach out.
www.hashmapinc.com
Feel free to share on other channels and be sure and keep up with all new content from Hashmap by following our Engineering and Technology Blog and subscribing to our IoT on Tap podcast.
Ed Fron is an Enterprise Architect with Hashmap providing Data, Cloud, IoT, and AI/ML solutions and consulting expertise across industries with a group of innovative technologists and domain experts accelerating high value business outcomes for our customers. Connect with Ed on LinkedIn.
In you enjoyed this story, here are some other recent posts from Ed for quick access:
medium.com
medium.co
Innovative technologists and domain experts helping‚Ä¶
44 
44¬†claps
44 
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
"
https://medium.com/microsoftazure/autoscaling-azure-sql-hyperscale-d6a30efb1f95?source=search_post---------268,"There are currently no responses for this story.
Be the first to respond.
Azure SQL Hyperscale is the latest architectural evolution of Azure SQL, which has been natively designed to take advantage of the cloud. One of the main key features of this new architecture is the complete separation of Compute Nodes and Storage Nodes. This allows for the independent scale of each service, making Hyperscale more flexible and elastic.
In this article I will describe how it is possible to implement a solution to automatically scale your Azure SQL Hyperscale database up or down, to adapt to different workload levels dynamically and automatically without requiring any manual intervention.
Now, first things first. I will not go into details here, as they are well described in the official doc, but it is important to understand how the SQL Server engine has been refactored into Azure SQL Hyperscale, as it is the foundation for a fast scale up and down of the selected service tier. Here is a picture that explains it very well:
One of the main benefits of this architecture is that scaling up or down the Service Level Objective, which means cores and memory, is something completely unrelated to the size of the database. Let‚Äôs say, for example, that you want to move from an HS_Gen5_2 to HS_Gen5_4. This is an overview of what is happening:
As there is no data movement involved, this workflow happens quickly. In several tests I ran lately, I had a constant value of a maximum of 2 minutes to spin up a new compute node of the Service Level Objective you are asking for, and something around 12 seconds to switch the control to the newly created node.
For all the time needed to spin up the new compute node, all existing connections still are open and transactions against the database are processed as usual. It‚Äôs only during the 12 seconds of switch that, at some point during that time range, connections will be moved to the new compute node. If you have a long-running transaction being executed at the time of cut-off, it will be rolled back: thanks to the new Accelerated Database Recovery, this operation will be almost immediate, and it will not prevent the scaling action from being completed. Of course, it is always good to avoid re-running transactions, as they may be resource-intensive, so follow the best practice to keep the connection open only for the time needed. If you are used to working with HTTP and REST, you are already doing this: just keep doing the same with Azure SQL too. If you are using some tool like Dapper, you don‚Äôt even have to bother about managing connections, unless you have some specific requirements. It will automatically open the connection just before you run a command, and automatically close it once the command is done. This is all you need to write:
Azure SQL databases provides a useful DMV, sys.dm_db_resource_stats, to monitor resource usage:
It keeps data for about 1 hour, aggregated by 15-seconds buckets. With such data, it would be possible to do some fancy stuff using machine learning and some cool forecasting algorithms, but for the sake of simplicity, we will use a very simple moving average to smooth out peaks and figure out if we need to scale up our Azure SQL Hyperscale database. If you are interested in better algorithms to decide if you need to scale up or down, you will find some links and resources at the end of this post.
For now, we can use a window function to calculate the moving average over the last minute.
Aside from the moving average, the query will also return the number of data points used to calculate the average. This is useful when determining if scale up or down action should really happen. In fact, just after a change of Service Level, say from 2vCores to 4vCores, the used DMV will be completely empty. So, for one minute, you will have an average calculated on way less data that what you want‚Ä¶and this could lead you to make wrong assumptions about the real resource usage. Due to the application reconnecting to the database, the first seconds could be even without load‚Ä¶and you should not take any action (for example scaling down) based on that incomplete set of data. For this reason, the sample solution will ignore all actions suggested by a dataset that doesn‚Äôt have at least a minute of data.
If you want to calculate the moving average over a longer time duration, to smooth out very short spikes of workload that could trigger unwanted scaling actions, you can just increase the number of data points, keeping in mind that each data point represents 15 seconds of workload. This is the code you want to look at:
As you can see, it now takes the current data point plus the 4 earlier data points (as data is ordered in descending order by time).
In case you need more than one hour, you can use the sys.resource_stats view, where data is stored, with a 5-minute granularity, for 14 days.
Look at OVER Clause documentation to understand what other options you have and how it works.
Now that we have a simple algorithm that will tell us when scale-up is needed or when scale-down is recommended, we just have to implement a monitoring job that will get such data and kick off the scaling operation.
This can be done using Azure Functions, as they support Timer Triggers, which will allow us to do exactly what we want.
Every 15 seconds the Azure Function will wake up and will get the current performance data from an Azure SQL Hyperscale database, along with the 1-minute moving average. If the moving average is within some defined boundaries it will go back to sleep. Otherwise, autoscaling action will be started. Here‚Äôs an example of such Azure Function in action, logging into dbo.Autoscaler table so that it is possible to check what happened and why:
As visible in the picture above, in the red box (box ‚Äú1‚Äù), around 22:12 GMT a workload spike began, bringing Azure SQL Hyperscale CPU usage close to 80%. After a minute of this sustained workload, the moving average went over the 70% boundary and thus the autoscaler started to request a bump in the Service Objective, asking for a scale up to 4 vCores.
The scale-up happened within 30 seconds and then autoscaler waited for the warmup of the new Service Objective to finish (the NULL values in the RequestedSLO column), as during warmup too few data points were available to make any decision. Once the number of data points was 5 again, it started to evaluate the workload and found that 4 cores were enough to handle the existing workload. In fact, in the yellow box (box ‚Äú2‚Äù), you can see that CPU usage is half of the value existed before, when the workload spike started.
One thing that is important to keep in mind, is that every time that scale up or down happens, the local RPBEX cache will start fresh. If the workload is critical, this can have an impact on initial performance, as data needs to be pulled from page servers into the local compute node cache. To help the cache to be warm as fast as possible you may want to execute some specific query as part of the autoscaler solution, just to make sure the most-used data is in the local cache. This is not a requirement and really depends on the workload you have but is some cases can really help a lot to boost the performances after the new Service Level Objective is ready to be used, so keep this option in mind if you see slower than expected performances just after scaling up or down happens.
At a later point, around 22:20 GMT the workload spike ends and, in a minute, the autoscaler detects that 4 vCores are now even too much for the current workload and initiates a scale-down to 2 cores (green box, number ‚Äú3‚Äù).
To simulate a simple workload, you can use powerful load testing tools like Locust, K6 or Apache JMeter where you can simulate even the most complex and realistic workload. To create a simple workload, a nice tool originally written by Adam Machanic is also available: SQL Query Stress.
Albeit quite old now, it allows you to execute a query using the desired number of parallel threads, so that you can create some workload in the database. It is very easy to use, configurable enough and is great to quickly test the autoscaling sample. In fact, a .json configuration file is also included in autoscaler‚Äôs GitHub repo, so you can get started right away.
Sample source code is available here. If you want to contribute, by adding some of the improvements described below, please feel free to do so. I‚Äôll be more than happy to accept contributions to this repo.
Using the moving average to figure out if Azure SQL Hyperscale tier should be scaled up or down can be a little bit too simplistic if you have a scenario where workload can vary with very complex patterns. It works well when you have a situation where the workload spike endures for a consistent amount of time, and if your workload fits that use case, great! Just keep in mind that in the sample we‚Äôre only monitoring CPU usage, but there are also data and log IO usage that could be monitored. As you can guess, what you want to monitor, and how you want to use that data to activate the scale up or down, is up to you and very specific to your workload scenario.
As mentioned already, this is just a sample, and it should be used a starting point to create something there fits your own use case. But that‚Äôs the only improvement you can think of.
In fact, the current sample just reacts to an increased or decreased workload‚Ä¶which means that it will starts to take an action when the workload has already started to change. If the workload spike is huge, the allocated resources may not be able to satisfy it within the desired performance SLA you must guarantee, as scaling will happen because of that workload change, and sometimes even causing a larger impact due to the necessary failover during the spike.
Another approach would be to try to be proactive and predict the workload so that the Azure SQL Hyperscale tier could be changed before the spike happens, and thus always guaranteeing the best performance possible.
Machine learning is an option in this case, of course, as well other existing prediction algorithms. A nice blog post on how Netflix approaches to this problem is available on their blog: Predictive Auto Scaling Engine.
More in general the problems fall under the umbrella of ‚ÄúDemand Forecasting‚Äù and there are already some cool algorithms in Azure Machine Learning services:
If you want even more options this blog post is amazing: Time Series in Python, Exponential Smoothing and ARIMA.
Azure Machine learning can be easily integrated with Azure Functions, and you can really create spectacular solutions using Azure SQL Hyperscale, Azure Machine Learning and Azure Functions.
There are two alternatives to the proposed sample. The first is to use Azure Alerts, where you can do the same as described here, but without writing code. Just be aware that the telemetry log available to Azure Alerts have some latency ‚Äî usually between 2 and 5 minutes ‚Äî which means that response to workload spike will be not as fast as polling the database DMV as the sample does. If you don‚Äôt need to respond to workload spikes in less than 5 minutes, Azure Alerts can be a great, simpler, alternative.
Another ‚Äúsort of‚Äù alternative is to use Azure SQL Serverless database, which incorporates scale up and down natively.
I said ‚Äúsort of‚Äù because the Serverless option is not available to Azure SQL Hyperscale now so it is not really an alternative, but if you are fine using the General Purpose service tier, you‚Äôll get autoscaling right away, just by configuring it.
Autoscaling a service is a challenging job, especially if the workload is not easy predictable. But thanks to Azure SQL Hyperscale elasticity and quick scalability this can be done using an Azure Function, with minimal service interruption, especially if following the best practices and implementing correct connection retry logic. Remember also to use the latest .NET driver as they already implement some reconnection logic that in several common cases will make the reconnection completely transparent.
Any language.
96 

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
96¬†claps
96 
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.cloudboost.io/how-cloudboost-uses-docker-kubernetes-and-azure-to-scale-60-000-apps-d54d7eaf02c9?source=search_post---------269,"CloudBoost.io is a server-less platform + backend as a service (BaaS) which helps developers build apps in half less time by taking care of all the mundane tasks like Authentication, Notifications, Emails, Managing and Scaling your Database, Files, Cache, and a whole lot more. We use MongoDB and Redis clusters for data store and NodeJS powers most of our micro-services. CloudBoost is completely open source under Apache 2 License, so you can modify it the way you like and install it on your own servers for free. You can check out our GitHub here : https://github.com/cloudboost/cloudboost (Pull Requests are a LOT appreciated!)
Before I dive into how we use Docker in production, for readers who are very new to containers. Let me explain what a ‚Äúcontainer‚Äù actually is. If you‚Äôre someone who knows about Docker and containers. Please feel free to skip to the implementation part of this post.
Before containers, we used to install our pieces of our application stack on a VM. The problem with that approach was ‚Äî Let‚Äôs say you need to install MongoDB to power your app then you would have three (or more) machines running MongoDB and you would literally name them by what‚Äôs installed on each of these machines ‚Äî mongo-1, mongo2, mongo-3. You could not have any other piece of your stack (let‚Äôs say Redis) installed on the same VM because the memory and compute were not isolated between these two.
Think of containers as tiny self-contained isolated packages which has enough software to run a particular service. For example: MongoDB container has enough software backed in to run MongoDB and nothing else. Containers can then be installed on a VM (or bare metal) and you can have as many containers you want on a machine. Containers help you isolate these services, so you can literally install multiple services on one machine.
Consistency and Isolation : The biggest advantage is consistency and isolation. Whatever works in your test ‚Äî will work in staging and production because you‚Äôre literally using the same container image for those environments. It also helps you have consistent environment across your entire ops pipeline. All of the containers are isolated with each other so you can run multiple containers on one machine.
Resource Utilisation: We‚Äôve had problems with resource utilisation before ‚Äî For example if your MongoDB uses just 10% of compute on a VM. 90% of compute would be rendered as waste which would be a LOT of compute and resources. Containers help us pack one or more services and isolates them from each other which actually helps us utilise the majority of the compute on a machine before scaling out.
Before deploying our stack on containers (when we were on plain vanilla VM‚Äôs), our average cluster utilisation was under 15% which was a HUGE waste of compute.
Scale: Scale was hard. Really hard. We used tools like Chef which was responsible for managing and installing software on a VM. The problem with these tools was they were clunky, they did not work well and had a huge learning curve. Most of the times, they needed multiple modules to be installed and configuring each one of them was tough and even if we could configure them ‚Äî they were not reliable. Installation failed a lot of times, and we had to try over and over again for it to succeed. Looking at Chef forums and StackOverflow, a lot of other DevOps team faced similar issues and not just us. Ultimately, working with Chef created too many internal and external conflicts for us to produce significant ROI.
Container ecosystem has better tooling by far and is built for scale (like Kubernetes or Docker Swarm) which helped us scale our services flawlessly to many machines. More on this later.
Micro-services : Microservices are a very attractive DevOps pattern because it helps teams deploy and scale each of these services independently which tremendously increases speed to market. Engineers responsible to work with few services do not need to know about the code in other services which also helps a new hire get started fairly quickly and keeps the codebase for each of these services small. With each micro-service being developed, deployed, run, and maintained independently (often using different languages and technology stacks), these allow companies like us to ‚Äúdivide and conquer,‚Äù and scale teams and applications more efficiently. When the pipeline is not locked into a monolithic configuration ‚Äî of either toolset, component dependencies, release processes, or infrastructure ‚Äî there is a unique ability to better scale development and operations. It also helps companies easily determine what services don‚Äôt need scaling in order to optimise resource utilisation saving them cloud expenses.
Containers (like Docker) are just packages which has your service (and everything else that you need to run that service baked in) and they need to be installed on a VM / Bare Metal for it to run. On a production workload ‚Äî you would literally have many machines (sometimes thousands of machines) that form a cluster and need an orchestrator like Kubernetes / Marathon / Docker Swarm to make sure containers are installed properly, can talk to each other and can scale across the entire cluster.
Orchestrator basically takes a pool of servers and makes it into one giant machine which has the combined compute and storage of all the VM‚Äôs in a cluster. You can now install any number of containers on this giant machine, and orchestrator will ‚Äúorchestrate‚Äù containers for you ‚Äî It takes care of all the heavy lifting for you ‚Äî like which underlying machine to actually install a particular container on, takes care of updating these containers and moving it to new machines when a machine fails, and a whole lot more.
CloudBoost uses a 50 node Kubernetes Cluster (D2 VM‚Äôs) on Azure Container Service. Why not Marathon (DC/OS) or Docker Swarm? We did review all of these options back when we started migrating to Docker a year ago. Docker Swarm was too early back then and now to all its fairness has matured a lot and can be used in production. DC/OS is a lot more than an orchestrator. Its a Datacenter OS. We just needed an orchestrator to work with and Kubernetes was being used in production by a number of companies and was a perfect match for us back then (and it still works like magic!).
Data Store: CloudBoost uses MongoDB containers configured in a replica-set and Redis containers which form a cluster for our data store layer. MongoDB containers need a Persistent Volume (and a claim) so the data which is hosted on these containers are retained even if container crashes and restarts. We‚Äôve plugged into Azure File Storage for elastic storage. We also have a container that is basically connects to MongoDB cluster and backs up data every single day and retains the last 7 days of data for every app that‚Äôs hosted on the platform. Backup container is hooked into Azure File Storage as well.
Micro-services: CloudBoost uses number of micro-services. CloudBoost Core which serves API calls and data to apps that are hosted on our platform. User Service which handles authentication and session management. Analytics measures API analytics for every single app, and more. All of these are packaged on NodeJS Docker containers which you can find here. All of these micro-services have K8s replication controller attached to it which is responsible for self-healing, and scaling a particular service which we then hook K8s service and Azure Load Balancer ‚Äî to expose these to the outside world.
User Interface: User Interface containers serve static HTML, CSS and JS out to clients. They‚Äôre plain express containers which serve static files. They‚Äôre usually React apps for most part and we use CloudFlare on top, to CDN and cache most of our content which saves us a lot of bandwidth every single month.
Build Pipeline: We use TravisCI to build all of our docker images, run tests on it, push it to docker hub if all the tests succeed. Once out on Docker Hub our build pipeline runs rolling-update on our K8s cluster to update all of our containers in production. You can check out our Travis file here. We also do a GitHub release whenever our build succeeds.
Cloud Expenses: We save 45‚Äì48% of cloud expenses after migrating to Docker because of better resource utilisation.
Scale: Scale is painless. You would just ask kubernetes to scale a particular container for you and it does it seamlessly. K8s team also launched auto-scaling which takes care of this automatically. We used to get paged 7‚Äì10 times/month before moving to Kubernetes + Docker, now its just once in a month or two.
Simplified Builds: Code bases are MUCH smaller and understandable. Build runs and code gets deployed to production in parallel between services. Each engineer can scale and focus on one or two services instead of a huge codebase and more importantly ‚Äî when we receive a pull request from our open source contributors and customers, we ship their code into production in minutes instead of weeks which surprises most of them and makes them smile. :)
We also have a Docker Cloud / Compose file here which you can use to install CloudBoost on any cloud or machine you want. We‚Äôre also working closely with Docker Store team to launch CloudBoost on the store for enterprises which contains advanced enterprise features like granular control of your team, cluster admin controls and more. More on this in another post.
The Realtime JavaScript Backend.
78 
2
78¬†claps
78 
2
Written by
Founder, HackerBay.io
The Realtime JavaScript Backend.
Written by
Founder, HackerBay.io
The Realtime JavaScript Backend.
"
https://medium.com/@wuehler/global-peering-on-ethereum-with-azure-3034fb31f568?source=search_post---------270,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Wuehler
Jun 21, 2017¬∑2 min read
At INFURA, one of the primary tenets of our mission is to be a source of truth to the decentralized world. To be that reliable source of truth, some of the most fundamental data elements we want to provide are accurate and timely block and transaction data. As an infrastructure pillar, we aim to peer as widely as we can across the global Ethereum network so that we are receiving the block and transaction data as fast as possible and relaying it as effectively as we can. To address these goals, we implemented the following:
The default peer count limits for the two popular Ethereum clients take into account reasonable P2P limits so as not to overdo network traffic. For geth, the default is a max of 25 peers, for Parity, the default max is 50 peers. These max values are adequate for a single local node relaying transactions occasionally to the network. However, for a platform built for high throughput, such as INFURA, we target as many as several hundred peers to position us as widely as possible.
With a goal of achieving several hundred peers, at times we found it difficult under the existing libp2p discovery mechanism to achieve that goal. To address this, we decided to deploy a single node under our own control in data centers around the world. We reached out to our partners at Microsoft and were able to identify 32Azure data centers as targets:
Using the Azure CLI, we scripted the deployments making this a relatively painless process. As these nodes start, we record the enode for each and feed it into a central list. Then, as we start our primary INFURA nodes, we can feed all of the 26 remote enodes to our primary using the command-line argument --bootnodes [comma-separated list of nodes] in geth. The effect is that we instantly have bootstrapped with 26 nodes from every corner of the planet. Following this strategy, we have been able to achieve and sustain a significantly higher peer count. We continue to fine tune the approach and are using our remote nodes for our Fukurou project which is an entire network crawler. The goal of Fukurou is to understand the topography of the entire Ethereum network.
Over time, we expect these strategies will reinforce the INFURA service offering as a robust and solid source of truth for the Ethereum network as a whole. We continue to invest in our strategy as an infrastructure powerhouse to serve the entire Ethereum ecosystem as it matures.
For more information about INFURA and to start using our service immediately for free, please visit https://infura.io.
Infura Co-Founder, ConsenSys, Ethereum NYC Founder
69 
1
69¬†
69 
1
Infura Co-Founder, ConsenSys, Ethereum NYC Founder
"
https://blog.jeremylikness.com/lift-and-shift-your-net-app-to-azure-41c1fd6a9e43?source=search_post---------271,
https://medium.com/@renatogroffe/azure-tech-nights-2020-saiba-como-foi-v%C3%ADdeos-gratuitos-5e8e33d438e?source=search_post---------272,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 20, 2020¬∑4 min read
Entre os meses de Fevereiro a Abril/2020 aconteceu a terceira edi√ß√£o do Azure Tech Nights, um evento online, gratuito e noturno do Canal .NET focado no uso de tecnologias e servi√ßos que integram o Microsoft Azure.
Seguem alguns dados sobre o Azure Tech Nights 2020:
Aproveito este post para agradecer a todos os organizadores, palestrantes e ao p√∫blico que nos acompanhou por todos o apoio para que esta iniciativa fosse um sucesso! Deixo muito obrigado ainda ao Jackson Feij√≥ (Microsoft) pelo aux√≠lio divulgando diversas das sess√µes do evento.
A seguir est√£o alguns prints mostrando os picos de espectadores simult√¢neos ao longo do evento:
Pensando naqueles que n√£o puderam acompanhar ou, at√© mesmo, gostariam de rever alguma apresenta√ß√£o, foram agrupados neste post os links (indicados abaixo) de todas as palestras realizadas ao longo dos 9 dias de evento:
E aproveito este espa√ßo para deixar aqui um convite‚Ä¶
Caso precise conhecer mais sobre Azure DevOps, n√£o deixe de aproveitar o desconto de 15% para inscri√ß√µes na segunda turma online do treinamento promovido pelo Azure na Pr√°tica e que acontecer√° dia 23/05/2020 (um s√°bado). Aproveite para ficar por dentro do build e deployment automatizado de aplica√ß√µes utilizando diversos servi√ßos oferecidos pelo Microsoft Azure e, o melhor, no conforto de sua casa! Acesse o link a seguir para informa√ß√µes e efetuar sua inscri√ß√£o: https://bit.ly/anp-devops2-blog-groffe
E para finalizar, ainda n√£o segue o Canal .NET nas redes sociais? Fa√ßa sua inscri√ß√£o ent√£o, para ficar por dentro de novidades sobre eventos, tecnologias Microsoft e outros conte√∫dos gratuitos:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
44 
44¬†claps
44 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/orquestrando-containers-na-nuvem-com-o-azure-kubernetes-service-thorarch-17-ita%C3%BA-unibanco-70f7f69bd91c?source=search_post---------273,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 29, 2019¬∑3 min read
No dia 27/06/2019 (quinta-feira) participei como palestrante do ThorArch #17, um encontro t√©cnico com profissionais de Arquitetura de Software do Ita√∫ Unibanco em S√£o Paulo-SP e no qual tive a oportunidade de realizar uma apresenta√ß√£o focada no uso do Azure Kubernetes Service na orquestra√ß√£o de containers Docker.
Deixo aqui meu agradecimento ao Marco Oliveira (Ita√∫ Unibanco) e √† Paula Santana Rosa (Ita√∫ Unibanco, Devs Java Girl) e pelo convite, pelo presente (uma garrafa da iniciativa ThorArch) e por todo o apoio para que este evento acontecesse, sendo que tivemos um excelente p√∫blico mesmo com as palestras acontecendo durante o hor√°rio de expediente (tarde de quinta, com 60 pessoas presentes!).
Os slides da apresenta√ß√£o j√° est√£o no SlideShare:
Os materiais que utilizei como refer√™ncia para esta apresenta√ß√£o podem ser encontrados no seguinte post, em que est√£o agrupadas refer√™ncias gratuitas como artigos, v√≠deos e projetos de exemplo:
Azure Kubernetes Services - AKS: refer√™ncias gratuitas e dicas para solu√ß√£o de problemas comuns
E aproveito este espa√ßo para deixar aqui ainda um convite.
Dia 02/07/2019 (ter√ßa) a partir das 21:30 ‚Äî hor√°rio de Bras√≠lia ‚Äî teremos mais uma live no Canal .NET. Desta vez eu (Renato Groffe) e o MVP Thiago Adriano faremos uma apresenta√ß√£o demonstrando o uso do Azure Application Insights, Logic Apps e do Slack no monitoramento de aplica√ß√µes Web (com exemplo em ASP.NET Core e Node).
Para efetuar a sua inscri√ß√£o acesse a p√°gina do evento no Meetup. A transmiss√£o acontecer√° via YouTube, em um link a ser divulgado em breve.
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
47 
47¬†
47 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/github-azure-app-service-deployment-automatizado-e-sem-complica%C3%A7%C3%B5es-de-web-apps-na-nuvem-4c0a0439e096?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 16, 2019¬∑5 min read
Principal plataforma online na atualidade para gerenciamento de c√≥digo-fonte em projetos de software, o GitHub foi concebido a partir do Git e nos disponibiliza todas as vantagens oferecidas por esta solu√ß√£o de versionamento. Grandes iniciativas open source, projetos corporativos, desenvolvedores individuais e o pr√≥prio mundo acad√™mico est√£o entre os usu√°rios do GitHub.
E se pud√©ssemos tirar proveito de todos os benef√≠cios do GitHub e do uso da nuvem na publica√ß√£o de nossas aplica√ß√µes Web, aplicando ainda boas pr√°ticas de DevOps?
Seja um reposit√≥rio privado ou p√∫blico, temos a op√ß√£o de configurar sem grandes dificuldades o deployment automatizado de um projeto Web no Azure App Service e empregando para isto um reposit√≥rio do GitHub. O App Service √© uma alternativa para hospedagem de aplica√ß√µes que conta hoje com suporte √†s seguintes plataformas de desenvolvimento: .NET Core, ASP.NET cl√°ssico (Web Forms, MVC, Web API), Java, Node, PHP, Python e Ruby.
Neste novo artigo demonstrarei como configurar uma aplica√ß√£o Web com a automa√ß√£o de seu deployment, fazendo uso para tanto do Azure App Service e de um reposit√≥rio do GitHub (um projeto ASP.NET Core 2.2 neste caso).
Caso deseje conhecer mais sobre o Azure App Service acesse ent√£o os seguintes conte√∫dos:
Como o Microsoft Azure pode simplificar a publica√ß√£o de suas Web Apps? ‚Äî Dica R√°pida
Como posso utilizar Linux e o Microsoft Azure para hospedar minhas aplica√ß√µes? - Dica R√°pida
Para o exemplo demonstrado neste post ser√° criado um novo recurso do App Service a partir do Portal do Azure:
Para associar um reposit√≥rio do GitHub a um App Service acessar a op√ß√£o Deployment Center:
Al√©m de reposit√≥rios do GitHub, o Azure App Service tamb√©m pode trabalhar em conjunto com op√ß√µes como Azure Repos (Azure DevOps), Bitbucket, Local Git e FTP. Selecionar GitHub para prosseguir com a configura√ß√£o do deployment:
Um popup ser√° ent√£o exibido, solicitando a conex√£o a uma conta do GitHub:
A conta selecionada aparecer√° agora na op√ß√£o GitHub (destacada em vermelho na imagem), sendo necess√°rio acionar a op√ß√£o Continue para prosseguir com as configura√ß√µes:
Em BUILD PROVIDER selecionar a op√ß√£o App Service build service e clicar em Continue:
Em CONFIGURE indicar qual o reposit√≥rio do GitHub (o reposit√≥rio que utilizei pode ser acessado em https://github.com/renatogroffe/ASPNETCore2.2_GitHub), bem como a Branch (selecionei a branch master para efeitos de testes); acionar na sequ√™ncia o bot√£o Continue:
Na imagem a seguir podemos observar este reposit√≥rio no GitHub:
E em SUMMARY concluir este procedimento clicando em Finish:
Ap√≥s alguns segundos aparecer√° em Deployment Center uma notifica√ß√£o de que o deployment teve sucesso:
Testes
Um acesso a https://groffegithub.azurewebsites.net/api/contador trar√° como retorno os dados gerados pela API REST de testes:
Ser√° agora efetuada uma altera√ß√£o a partir do pr√≥prio GitHub utilizando a branch master e no arquivo ContadorController.cs, como indicado a seguir (ajuste destacado em vermelho). Confirmar esta altera√ß√£o acionando a op√ß√£o Commit changes:
Um novo deployment acontecer√° de forma autom√°tica, sendo poss√≠vel observar que o mesmo teve sucesso na se√ß√£o Deployment Center:
Ao acessar novamente a API de testes via browser a mudan√ßa realizada na branch master j√° constar√° no resultado gerado:
E para concluir este post aproveito a oportunidade para um convite‚Ä¶
Que tal aprender mais sobre o Azure App Service na pr√°tica, em um workshop que acontecer√° durante um s√°bado (dia 21/09) em S√£o Paulo Capital e implementando na pr√°tica um case que combina o uso deste servi√ßo com outras tecnologias como Azure SQL, Azure Storage, Azure Functions e Application Insights? Acesse ent√£o o link a seguir para efetuar sua inscri√ß√£o com um desconto especial: http://bit.ly/anp-promo-azure-github
Deixo aqui ainda algumas refer√™ncias gratuitas que podem ser √∫teis para que voc√™ conhe√ßa um pouco mais sobre os diversos servi√ßos do Microsoft Azure que podem ser utilizados para a hospedagem de aplica√ß√µes Web:
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
Hospedando projetos Web no Azure: de um site est√°tico a um cluster Kubernetes
Orquestra√ß√£o de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
Kubernetes: do Pod ao Deployment Automatizado [V√≠deo]
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
Azure Kubernetes Services - AKS: refer√™ncias gratuitas e dicas para solu√ß√£o de problemas comuns
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
77 
1
77¬†
77 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@selcukusta/azure-time-series-insight-%C3%B6n-i%CC%87nceleme-4ede1e2b09a3?source=search_post---------275,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sel√ßuk Usta
Jun 3, 2017¬∑5 min read
Hen√ºz Preview a≈üamasƒ±nda olan, Microsoft Azure ailesinin yeni √ºyesi Time Series Insight servisine g√∂z atmakta fayda var dedim ve yaptƒ±ƒüƒ±m √∂n √ßalƒ±≈ümayƒ± da payla≈ümanƒ±n keyifli olacaƒüƒ±nƒ± d√º≈ü√ºnd√ºm, ≈üimdi de bu satƒ±rlarƒ± yazƒ±yorum.
Adƒ±ndan da anla≈üƒ±lacaƒüƒ± √ºzere bu servis, bir TSDB (Time Series Database) servisi. Bu veritabanƒ± sistemiyle uƒüra≈üanlar konsepte olduk√ßa hakimdir ancak kƒ±saca a√ßƒ±klamak gerekirse anlƒ±k olan geli≈üen milyonlarca metrik veriyi g√ºvenli bir ≈üekilde muhafaza eden ve saƒüladƒ±ƒüƒ± aray√ºz desteƒüi ile son kullanƒ±cƒ±ya sunan veritabanlarƒ± olarak ifade edebiliriz. ‚ÄúYazƒ±lƒ±mlarƒ±mƒ±zƒ± Nasƒ±l ƒ∞zliyor ve Kontrol Altƒ±nda Tutuyoruz?‚Äù ba≈ülƒ±klƒ± yazƒ±mda da bu kavrama kƒ±saca deƒüinme fƒ±rsatƒ±na sahip olmu≈ütum.
Aslƒ±nda yukarƒ±daki kƒ±sa tanƒ±mdan da anla≈üƒ±labileceƒüi √ºzere metrik verilerinizi tutmak i√ßin bi√ßilmi≈ü kaftan diyebilirim. H√ºrriyet evreninde biz de bu veritabanƒ± sistemini aktif olarak kullanƒ±yoruz. Sunuculara d√º≈üen anlƒ±k istek sayƒ±larƒ±, sunuculardan gelen anlƒ±k hata sayƒ±larƒ±, CPU sƒ±caklƒ±k deƒüerleri, hafƒ±za kullanƒ±mƒ± gibi dƒ±≈ü parametrelere baƒülƒ± olarak anlƒ±k deƒüi≈üen yƒ±ƒüƒ±n datalarƒ± tutuyor ve bir aray√ºz e≈üliƒüinde g√∂zlemliyoruz.
IOT kavramƒ±nƒ±n hayatƒ±mƒ±za girmesiyle birlikte de bu veritabanƒ± sistemleri √∂nemini g√ºn ge√ßtik√ße arttƒ±rmakta. Odanƒ±zƒ±n anlƒ±k sƒ±caklƒ±k ya da nem deƒüerlerini bir Raspberry cihazƒ±ndan okuyup TSDB veritabanƒ±nda tutabilirsiniz. Maƒüazanƒ±zƒ±n √∂n√ºnden ge√ßen ki≈üilerin raflarƒ±nƒ±za ne kadar s√ºre baktƒ±ƒüƒ±nƒ± algƒ±layan bir cihaz kurabilir ve cihazdan gelen verileri yine bu veritabanƒ± sistemi √ºzerinde saklayabilirsiniz. Senaryolarƒ± ihtiya√ßlar doƒürultusunda arttƒ±rmak i≈üten bile deƒüil hayal ettiƒüiniz √ºzere.
Kendilerinin de ifade ettiƒüi gibi hen√ºz en ba≈üƒ±nda. InfluxDB, OpenTSDB hatta aynƒ± √ßer√ßevede deƒüerlendirilmese dahi Elasticsearch gibi teknolojiler ≈üu anda aktif olarak yukarƒ±da sayƒ±lan senaryolar i√ßin aktif olarak profesyonel sahnede yer alƒ±yor. Bu sahneye √ßƒ±kƒ±≈ü adƒ±mƒ± olarak da ge√ßtiƒüimiz g√ºnlerde Time Series Insights adƒ±nƒ± verdikleri servisi kullanƒ±ma sundular.
Bu yazƒ±da i≈üin maliyeti, diƒüer Cloud √ß√∂z√ºmlerinin sunduƒüu alternatif servislerle kar≈üƒ±la≈ütƒ±rma gibi kalemlere deƒüinmeyeceƒüim. Amacƒ±mƒ±z yazƒ±nƒ±n sonunda bu servisi kullanarak lokal ortamda bir uygulama geli≈ütirmek ve aray√ºz √ºzerinden g√∂ndermi≈ü olduƒüumuz yƒ±ƒüƒ±n verileri g√∂zlemleyebilmek.
Bir sonraki paragrafta ba≈ülayacaƒüƒ±mƒ±z √∂rnek √ßalƒ±≈üma i√ßin;
yeterli olacaktƒ±r. Ben de bu gereksinimleri kar≈üƒ±ladƒ±ƒüƒ±nƒ±zƒ± d√º≈ü√ºnerek √∂rneƒüime ba≈ülƒ±yorum:
√ñncelikle nasƒ±l bir √ßalƒ±≈üma modelimiz olduƒüunu tartƒ±≈ümakta fayda var. Elimde bir adet TSDB √ß√∂z√ºm√º var. Bu √ß√∂z√ºme veri saƒülamakla sorumluyum. Global √ß√∂z√ºmde bir TSDB i√ßerisinde farklƒ± metrik deƒüerleri tutmanƒ±z m√ºmk√ºn. Yani CPU takibi i√ßin ayrƒ±, y√ºk oranƒ± i√ßin ayrƒ± bir veritabanƒ±na ihtiyacƒ±m yok. Azure bunun kar≈üƒ±lƒ±ƒüƒ± olarak ‚ÄúEvent Hubs‚Äù servisini kullanƒ±yor. Bu hub ‚Äî bundan sonra kanal olarak ifade edeceƒüim ‚Äî √ºzerinden olay kayƒ±tlarƒ±nƒ± veritabanƒ±na ileteceƒüim. Bunun i√ßin yeni bir Event Hubs servisi olu≈üturuyorum:
Azure √ºzerinde olu≈üturduƒüunuz servisler bir Resource Group i√ßerisinde tutulmalƒ±dƒ±r. Eƒüer bir grubunuz varsa bunu se√ßebilir ya da benim √∂rneƒüimde olduƒüu gibi bu panel √ºzerinden yeni bir grup olu≈üturabilirsiniz. Olmazsa olmazƒ±mƒ±z ise ‚ÄúName‚Äù alanƒ±nda belirttiƒüimiz servis adƒ±. Bunu bir yere not edelim, ileride kullanacaƒüƒ±z.
Servisi olu≈üturduktan sonra a√ßƒ±lan pencereden ‚ÄúShared access policies‚Äù men√º elemanƒ±na tƒ±klƒ±yor ve saƒüa doƒüru a√ßƒ±lan yeni pencereden ‚ÄúPRIMARY KEY‚Äù kolonundaki deƒüeri kopyalƒ±yoruz. Bunu da not etmeyi unutmayalƒ±m:
Adƒ±ndan da anla≈üƒ±lacaƒüƒ± √ºzere Event Hubs, kanallarƒ±n tutulduƒüu bir container servisi aslƒ±nda. Dolayƒ±sƒ±yla bu depoya a√ßƒ±lan k√º√ß√ºk k√º√ß√ºk kanallar a√ßabilir ve bu kanallar √ºzerinden verilerinizi iletebilirsiniz. Biz de ‚Äúcpu-usage-event-hub‚Äù adƒ±nƒ± vereceƒüimiz kanalƒ±mƒ±zƒ± olu≈üturuyoruz. Bu kanal √ºzerinden veritabanƒ±na CPU kullanƒ±m y√ºzdesini anlƒ±k olarak g√∂ndereceƒüiz.
Artƒ±k olaylarƒ± g√∂ndereceƒüimiz kanalƒ±mƒ±z hazƒ±r. Sƒ±ra bu kanalƒ± dinleyen time-series veritabanƒ±mƒ±zƒ± olu≈üturmakta. Bunun i√ßin de yeni bir servis ekleme alanƒ±na gidiyor ve ‚ÄúTime Series Insights Preview‚Äù servisini se√ßiyoruz.
Bu veritabanƒ±n bir aray√ºz√º olacak elbette. Bu aray√ºze eri≈üim saƒülayacak, aray√ºz √ºzerinde sorgu √ßalƒ±≈ütƒ±rabilecek, sorgularƒ± global olarak ya da kendine √∂zel kaydedebilecek kullanƒ±cƒ±lara ihtiyacƒ±mƒ±z var. Hadi ≈üimdi bunlarƒ± tanƒ±mlayalƒ±m ve servis penceresindeki ‚ÄúData Access Policies‚Äù men√º elemanƒ±nƒ± se√ßelim. A√ßƒ±lan pencereden ‚ÄúSelect User‚Äù se√ßeneƒüi ile eri≈üim saƒülayacak kullanƒ±cƒ±yƒ± se√ßiyoruz. ‚ÄúSelect Role‚Äù se√ßeneƒüi ile de bu kullanƒ±cƒ±nƒ±n okuyucu ve katkƒ± saƒülayƒ±cƒ± yetkilere sahip olmasƒ± gerektiƒüini ifade ediyoruz:
Daha sonra ise yine servis penceresine d√∂n√ºyor ve verinin alƒ±nabilmesi i√ßin gerekli ayarlarƒ±n yapƒ±ldƒ±ƒüƒ± ‚ÄúEvent Sources‚Äù men√º elemanƒ±na tƒ±klƒ±yoruz:
Burasƒ± ilk etapta otomatik olarak doluyor (her bir servisten zaten bir tane tanƒ±mlƒ± olduƒüu i√ßin). Kƒ±saca a√ßƒ±klamak gerekirse;
Artƒ±k TSDB‚Äôimiz hazƒ±r. Aray√ºz adresini de TSDB servisi penceresinden alƒ±yoruz:
A≈üaƒüƒ±da vermi≈ü olduƒüum Github repomdan uygulamayƒ± indirdikten sonra testini yapabilirsiniz. NOT: main.py dosyasƒ±nda yer alan; ‚ÄúSERVICE_NAMESPACE‚Äù, ‚ÄúSHARED_ACCESS_KEY_NAME‚Äù, ‚ÄúSHARED_ACCESS_KEY_VALUE‚Äù, ‚ÄúEVENT_HUB_NAME‚Äù deƒüi≈ükenlerinin deƒüerlerini kendi hesap ve servis bilgileriniz ile deƒüi≈ütirmeyi unutmayƒ±n.
github.com
Uygulamayƒ± √ßalƒ±≈ütƒ±rdƒ±ƒüƒ±nƒ±zda her ≈üey yolunda ise a≈üaƒüƒ±dakine benzer bir konsol √ßƒ±ktƒ±sƒ± alƒ±yor olmanƒ±z gerekiyor:
Son olarak aray√ºze gidiyorum ve bir s√ºreliƒüine g√∂ndermi≈ü olduƒüum deƒüerlerin yansƒ±yƒ±p yansƒ±madƒ±ƒüƒ±nƒ± kontrol ediyorum:
Bu ekranda (1) ile i≈üaretlenmi≈ü alandan, √ßizim i√ßin kullanƒ±lacak metriƒüi se√ßmeyi; (2) ile i≈üaretlenmi≈ü alandan √ßizilecek alana bir isim ve renk vermeyi; (3) ile i≈üaretlenmi≈ü alandan verileriniz arasƒ±ndaki s√ºreyi; (4) ile i≈üaretlediƒüim alandan ise t√ºm i≈ülemleri yaptƒ±ktan sonra grafiƒüi g√ºncellemeyi de unutmayƒ±n.
Azure her ne kadar yolun ba≈üƒ±nda olsa da sunmu≈ü olduƒüu kolay aray√ºz, i≈ülem adƒ±mlarƒ±nƒ±n net olmasƒ± ve servislerin kolay bir ≈üekilde birbirine baƒülanmasƒ± geleceƒüe dair umutlarƒ±mƒ± ye≈üertti.
Sizler de yorum alanƒ±ndan TSDB alanƒ±ndaki deneyimlerinizi payla≈üƒ±rsanƒ±z mutlu olurum. Tekrar g√∂r√º≈üene dek, yƒ±ƒüƒ±n datalarƒ±nƒ±z hep g√ºvenle saklƒ± olsun :)
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
24 
24¬†
24 
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
"
https://medium.com/@borakasmer/automated-publish-from-tfs-to-azure-b3eb6a7ff869?source=search_post---------276,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bora Ka≈ümer
Aug 18, 2021¬∑4 min read
Hi, Today we will talk about Publishing from any Source Control to any Cloud Automatically. As a Microsoft MVP, I prefered TFS for Source Control and Azure as a Cloud :)
If you‚Äôve ever been a bit interested in TDD or DDD, you must figure out the importance of automation. Why do we try to Automate everything? Because of saving time and making fewer mistakes. So let‚Äôs publish a website from the specific branch on TFS to Azure automatically.
‚ÄúSoftware is the language of automation.‚Äù ‚ÄïJensen Huang
We will create the AutoDeployTFS project as an Asp .Net Core Web App with .Net 5.0 framework.
When the project is created, you could see the default index page as below.
1-) Now let‚Äôs Create the Azure DevOps Project (https://dev.azure.com) for source repo (TFS). Firstly, if you didn‚Äôt create any organization before, create a new one as below.
2-) Secondly, create a project under the organization as below.
3-) Create an empty folder. Open Visual Studio 2019 and Connect to Azure DevOps. Enter the credentials.
4-) Connect to AutoDeploy project on TFS and Map&Get the empty folder.
5-) Now, move all the project files from the AutoDeploy to this empty folder. And ‚ÄúCommit All‚Äù files. And finally ‚ÄúPush‚Äù.
‚ÄúAutomation is going to cause unemployment, and we need to prepare for it.‚Äù ‚ÄïMark Cuban
Your AutoDeploy project must be seen as below on TFS (Azure DevOps Project).
Now we created the AutoDeploy project on Visual Studio. We created AutoDeploy repo on TFS (Azure DevOps Project) and pushed the project to this Repo.Now we need the server to publish this website. So let‚Äôs create WebSite on Azure. https://portal.azure.com/
Go to ‚ÄúCreate Resource => Web App‚Äù and select it.
Now fill the necessary fields below in the form.
After creating the Website on Azure, now let‚Äôs connect it with TFS (Azure DevOps).
Select AutoDeploy => Deployment Center from the menu. In the below picture, we create a webhook from Azure Web Apps to TFS.
Source: You can select different kinds of sources(GitHub, BitBucket). For TFS, we select ‚ÄúAzure Repos‚Äù.
Now let‚Äôs setup the TFS connection configurations.
When someone pushes the final Web Application code to the ‚Äúmaster‚Äù branch of ‚ÄúAzure DevOps‚Äù, it automatically publishes the Azure. So you don‚Äôt worry about the IIS publish for new updates anymore.
‚ÄúAutomation is driving the decline of banal and repetitive tasks.‚Äù ‚ÄïAmber Rudd
Conclusion:
In this article, we talked about Automation importance. DevOps is the most needed expertise these days. When we change and push the application to the specific branch, we don‚Äôt need the updated Azure Web Application because it will happen automatically. But we ignored the test for this scenario, and we can get live some broken pages accidentally on Azure. So firstly, we do not push the application directly to the master branch. And before pushing the brunch, we have to run some tests automatically. Maybe we can use some tools like Jenkins or TravisCI for running tests. And if everything is ok, we can push the source to the dev TFS brunch.
I hope this article helps you understand how to publish Azure automatically from the TFS and the importance of Automation. See you later until the next article or video. Bye.
‚ÄúIf you have read so far, first of all, thank you for your patience and support. I welcome all of you to my blog for more!‚Äù
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
See all (22)
133 
133¬†claps
133 
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devopsturkiye/azure-automation-servisi-ile-paran%C4%B1z-cebinizde-kals%C4%B1n-721f30255659?source=search_post---------277,"Sign in
There are currently no responses for this story.
Be the first to respond.
Emre √ñzkan ‚òÅÔ∏è üêß üê≥ ‚ò∏Ô∏è
Apr 18, 2019¬∑3 min read
ƒ∞htiya√ß doƒürultusunda Azure √ºzerinde mesai saatleri dƒ±≈üƒ±nda kullanƒ±lmayan kaynaklarƒ± otomasyon ile a√ßma kapama √∂zelliƒüini deneyimleme fƒ±rsatƒ± buldum. Maliyet a√ßƒ±sƒ±nda sizlerin cebini de rahatlatacaƒüƒ±nƒ± d√º≈ü√ºnd√ºƒü√ºmden sizlere de g√∂stermeye √ßalƒ±≈üacaƒüƒ±m.
ƒ∞lk √∂nce bir automation account‚Äôu yaratarak ba≈ülƒ±yoruz.
A≈üaƒüƒ±daki bilgileri kendimize g√∂re uyarlƒ±yoruz.
Yarattƒ±ktan sonra a≈üaƒüƒ±daki gibi g√∂r√ºnmesi gerekiyor.
ƒ∞√ßine girip credentials sekmesine tƒ±klƒ±yoruz.
Burada runbook‚Äôu √ßalƒ±≈ütƒ±racak bir kullanƒ±cƒ± yaratmamƒ±z gerekiyor.
Sonrasƒ±nda Automation account i√ßinden Runbook Gallery sekmesine gelip sanal makineleri ba≈ülatƒ±p kapatƒ±lacak daha √∂nceden yazƒ±lmƒ±≈ü Powershell scriptini hesabƒ±mƒ±za import ediyoruz.
Script detayƒ±nƒ± a≈üaƒüƒ±daki ≈üekilde inceleyebilirsiniz.
ƒ∞mport ettikten sonra Runbook sekmesinden ilgili runbook i√ßine giriyoruz.
Runbook‚Äôu √ßalƒ±≈ütƒ±rmadan √∂nce ‚Äúedit‚Äù sekmesinden ‚ÄúPublish‚Äù etmem gerekiyor.
ƒ∞sterseniz bu aray√ºz √ºzerinden Test pane adƒ±mƒ±ndan test edebilirsiniz.Publish ettikten sonra ‚ÄúStart‚Äù sekmesi aktif oldu.
‚ÄúStart‚Äù adƒ±mƒ±ndan sonra 3 adet input olarak subscription id‚Äôsini, makine ismini ve ‚ÄúStart‚Äù ve ‚ÄúStop‚Äù se√ßeneklerinden birini se√ßmemizi istiyor.
Sonrasƒ±nda takip edebileceƒüiniz bir aray√ºze y√∂nlendiriyor.
ƒ∞sterseniz de bunu bir schedule haline getirip istediƒüiniz saatte a√ßƒ±p istediƒüiniz saatte kapatabilirsiniz.‚ÄùLink to schedule‚Äù se√ßeneƒüine tƒ±klƒ±yoruz.
ƒ∞sterseniz g√ºnl√ºk olarak isterseniz de haftanƒ±n belirli g√ºnlerini se√ßebilirsiniz.
Daha √∂nce belirttiƒüim input larƒ± tekrar giriyoruz.
Paranƒ±zƒ±n cebinizde kalmasƒ± dileƒüiyle
Diƒüer makalelerime profilime tƒ±klayarak ula≈üabilirsiniz.
medium.com
Ki≈üisel Bloguma g√∂z atmak isterseniz;
sysaix.com
Solution Architect at @RedHat CK{A | AD | S} https://www.linkedin.com/in/emreozkann/
See all (56)
17 
1
Haftalƒ±k olarak yayƒ±mƒ±nƒ±zdan alacaƒüƒ±nƒ±z Email B√ºlteni¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
17¬†claps
17 
1
Medium independent DevOps publication. Join thousands of aspiring developers and DevOps enthusiasts
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/2b-1-better-2day/microsoft-azure-dddf42264fcd?source=search_post---------278,"There are currently no responses for this story.
Be the first to respond.
”®–Ω–≥”©—Ä—Å”©–Ω –∂–∏–ª Flutter –∂–æ–æ—Ö–æ–Ω —Å–æ–Ω–∏—Ä—Ö–æ–æ–¥ –≥–∞–Ω—Ü 2 —Ö–∏—á—ç—ç–ª Udacity –¥—ç—ç—Ä “Ø–∑—Å—ç–Ω —é–º. –¢—ç–≥–∂ –±–∞–π—Ö–¥–∞–∞ 1 –æ–Ω–ª–∞–π–Ω —Ç—ç—Ç–≥—ç–ª—ç–≥–∏–π–Ω –∑–∞—Ä –æ–ª–∂ —Ö–∞—Ä–≤–∞–∞.
–ò–π–º—ç—Ä—Ö“Ø“Ø —é–º–∞–Ω–¥ –¥—É—Ä—Ç–∞–π, –±–∞—Å 2020 SMART goal –¥–æ—Ç–æ—Ä —Å—É—Ä–≥—É—É–ª—å —Ç—ç—Ç–≥—ç–ª—ç–≥—Ç ”©”©—Ä–∏–π–≥”©”© —Å–æ—Ä–∏—Ö –≥—ç—Å—ç–Ω –∑–æ—Ä–∏–ª—Ç —Ç–∞–≤—å—Å–∞–Ω –±–∞–π—Å–∞–Ω –±–æ–ª–æ—Ö–æ–æ—Ä —à—É—É–¥ –ª –±“Ø—Ä—Ç–≥“Ø“Ø–ª—ç–≤. (2020/9/16) –•–∞—Ä–∏—É –Ω—å 3 —Å–∞—Ä—ã–Ω –¥–∞—Ä–∞–∞ 12 —Å–∞—Ä–¥ –∏—Ä—Å—ç–Ω–¥—ç–≥.
–ú–∏–Ω–∏–π —Ö—É–≤—å–¥ Data, AI, Cloud –≥—ç—Å—ç–Ω 3-–Ω —Ç”©—Ä–ª–∏–π–Ω —á–∏–≥–ª—ç–ª—ç—ç—Å ”©”©—Ä—Ç –∏–ª“Ø“Ø –æ–π—Ä –±–æ–ª–æ—Ö–æ–æ—Ä –Ω—å Cloud Track-–≥ –Ω—å —Å–æ–Ω–≥–æ—Å–æ–Ω —é–º. –•–∏—á—ç—ç–ª–∏–π–Ω —Ö—É–≤—å–¥ –¥–æ—Ç—Ä–æ–æ “Ø–Ω–¥—Å—ç–Ω 5 —Å—ç–¥—ç–≤—Ç—ç–π, —Ö–∏—á—ç—ç–ª–∏–π–Ω —Ç–∞–ª–∞–∞—Ä—Ö –º—ç–¥—ç—ç–ª—ç–ª –Ω—å Google Site –¥—ç—ç—Ä, —Ö–∞—Ä–∏–ª—Ü–∞–∞ —Ö–æ–ª–±–æ–æ –Ω—å Slack-–∞–∞—Ä —è–≤–∞–≥–¥–¥–∞–≥ –º–∞—à —Å–∞–π–Ω –∑–æ—Ö–∏–æ–Ω –±–∞–π–≥—É—É–ª–∞–ª—Ç—Ç–∞–π —Ö”©—Ç”©–ª–±”©—Ä –±–∞–π–Ω–∞ –ª—ç—ç.
Cloud Track –Ω—å Microsoft Azure-–Ω —Ç–∞–ª–∞–∞—Ä –±”©–≥”©”©–¥
–≥—ç—Å—ç–Ω —Ö–∏—á—ç—ç–ª“Ø“Ø–¥—Ç—ç–π. –≠—Ö–Ω–∏–π —à–∞—Ç–∞–∞ –¥–∞–≤–∂ —á–∞–¥–≤–∞–ª –¥–∞—Ä–∞–∞—á–∏–π–Ω $1356-–Ω —Ç”©–ª–±”©—Ä—Ç—ç–π Become a Developer for Microsoft Azure - Nanodegree Program-–≥ “Ø–∑—ç—Ö —Ü–∞–∞—à–ª–∞–∞–¥ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—Ç–∞–π –±–æ–ª–æ—Ö –±–æ–ª–æ–º–∂—Ç–æ–π —é–º –±–∏–ª—ç—ç.
–•–∏—á—ç—ç–ª –Ω—å –±–æ–≥–∏–Ω–æ –±”©–≥”©”©–¥ –∞–º–∞—Ä—Ö–∞–Ω –±–∞–π—Å–∞–Ω —Ç—É–ª “Ø—Ä–≥—ç–ª–∂–ª“Ø“Ø–ª—ç–Ω Linux Academy-–Ω AZ-900 Microsoft Azure Fundamentals (13 —Ü–∞–≥), Microsoft Official Learning Path ‚Äî Azure Fundamentals (13 —Ü–∞–≥) —Ö–∏—á—ç—ç–ª–∏–π–≥ “Ø–∑—ç—ç–¥ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã–Ω —à–∞–ª–≥–∞–ª—Ç ”©–≥”©—Ö”©”©—Ä —à–∏–π–¥—Å—ç–Ω —é–º.
“Æ“Ø–ª—ç–Ω —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏ —É—á—Ä–∞–∞—Å AWS-—Ç–∞–π —Ç”©—Å—Ç—ç–π. –ì—ç—Ö–¥—ç—ç –º—ç–¥—ç—ç–∂ ”©”©—Ä–∏–π–Ω –æ–Ω—Ü–ª–æ–≥–∏–π–≥ —à–∏–Ω–≥—ç—ç—Å—ç–Ω —è–ª–≥–∞–∞–Ω—É—É–¥ –±–∏–π. Region pair, Azure Active Directory –≥—ç—Ö –º—ç—Ç.
–î—ç–¥ –±“Ø—Ç—Ü–∏–π–Ω —Ö—É–≤—å–¥ —Ç–æ–º–æ–æ—Ä –Ω—å —Ö–∞—Ä–≤–∞–ª Orchestrator –≥—ç–¥—ç–≥ –Ω”©—Ö”©—Ä –±“Ø—Ö —Ö“Ø—Å—ç–ª—Ç–∏–π–≥ –∑–æ—Ö–∏—Ü—É—É–ª–∂ ”©–≥–¥”©–≥ —Ö–∞—Ä–∏–Ω —Ç“Ø“Ø–Ω–∏–π–≥ –Ω—å —ç—Ü—Å–∏–π–Ω —à–∞—Ç –±—É—é—É hardware —Ç“Ø–≤—à–∏–Ω–¥ —Å–µ—Ä–≤–µ—Ä —Ä–∞–∫ –¥—ç—ç—Ä—Ö Hypervisor-–ª—É—É Fabric controller –≥—ç–¥—ç–≥ –∑“Ø–π–ª —Ö—É–≤–∞–∞—Ä–∏–ª–∂ ”©–≥–¥”©–≥ –±–∞–π–Ω–∞.
–ú—ç–¥—ç—ç–∂ —à–∏–Ω—ç —Ö—ç—Ä—ç–≥–ª—ç–≥—á ”©”©—Ä–∏–π–Ω account-–≥ “Ø“Ø—Å–≥—ç—Ö –±”©–≥”©”©–¥ 1 account –¥–æ—Ç–æ—Ä –æ–ª–æ–Ω subscription “Ø“Ø—Å–≥—ç–∂ —Ö—ç—Ä—ç–≥–ª—ç—Ö –±–æ–ª–æ–º–∂—Ç–æ–π.- Free (—Ç–æ–¥–æ—Ä—Ö–æ–π —Å–µ—Ä–≤–∏—Å“Ø“Ø–¥–∏–π–≥ “Ø–Ω—ç–≥“Ø–π —Ö—ç—Ä—ç–≥–ª—ç—Ö –±–æ–ª–æ–º–∂—Ç–æ–π)- Temporary (Sandbox –±—É—é—É –∑”©–≤—Ö”©–Ω —Ç—É—Ä—à–∏–ª—Ç —Ö–∏–π–∂ –∑–æ—Ä–∏–ª–≥–æ—Ç–æ–π)- Pay as you go (–•—ç—Ä—ç–≥–ª—ç—Å–Ω—ç—ç—Ä—ç—ç —Ç”©–ª”©—Ö —Ö—ç—Ä—ç–≥–ª—ç–≥—á ‚Äî –∏—Ö—ç–Ω—Ö)- Member offer (”®”©—Ä ”©”©—Ä —Ö—è–º–¥—Ä–∞–ª—ã–Ω –±–∞–≥—Ü—É—É–¥)
Hierarchy-–Ω —Ö—É–≤—å–¥ –¥–∞—Ä–∞–∞—Ö –±–∞–π–¥–ª–∞–∞—Ä –∑–æ—Ö–∏–æ–Ω –±–∞–π–≥—É—É–ª–∞–ª—Ç —Ö–∏–π–¥—ç–≥ —é–º –±–∏–ª—ç—ç.(Management Group ‚Üí Subscriptions ‚Üí Resource Group ‚Üí Resource)
Region pair : AWS-–∞–∞—Å –Ω—ç–≥ —è–ª–≥–∞–∞—Ç–∞–π –∑“Ø–π–ª –Ω—å region —Ö–æ—Å–ª–æ–ª—ã–≥ —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–¥–æ–≥ –≥—ç—Å—ç–Ω. –ò—Ö—ç–≤—á–ª—ç–Ω region –¥–æ—Ç–æ—Ä—Ö 2‚Äì3 —à–∏—Ä—Ö—ç–≥ AZ-–≥ –∞—à–∏–≥–ª–∞–∂ disaster recovery, failover-–≥ —à–∏–π–¥–¥—ç–≥ –±–∞–π—Å–∞–Ω –±–æ–ª –æ–¥–æ–æ –∏–ª“Ø“Ø —Ç–æ–º —Ö“Ø—Ä—ç—ç–Ω–¥ –∑–æ—Ö–∏–æ–Ω –±–∞–π–≥—É—É–ª–∞—Ö –Ω—å —Ö—è–ª–±–∞—Ä –±–æ–ª—Å–æ–Ω –±–∞–π–Ω–∞.
–ù—ç—Ä –±–æ–ª–æ–Ω –±–æ–ª–æ–º–∂ –Ω—å –ª –±–∞–≥–∞ –∑—ç—Ä—ç–≥ —è–ª–≥–∞–∞—Ç–∞–π –±–æ–ª–æ—Ö–æ–æ—Å –∏–∂–∏–ª—Ö—ç–Ω —Å–µ—Ä–≤–∏—Å“Ø“Ø–¥ –±–∞—Å –∑”©–Ω–¥”©”©. “Æ“Ø–ª —Ä“Ø“Ø —à–∏–ª–∂–∏—Ö—ç—ç—Å ”©–º–Ω”©—Ö —Ç–æ–æ—Ü–æ–æ–ª–æ–ª —Ö–∏–π—Ö TCO, —Ö—ç—Ä—ç–≥–ª—ç—ç–≥—ç—ç —Ö—è–Ω–∞—Ö Advisor, —É—Ä—å–¥—á–∏–ª–∂ –±–∞–≥–∞ –º”©–Ω–≥”© —Ç”©–ª”©—Ö Reservations –≥—ç—Ö –º—ç—Ç. –¢“Ø“Ø–Ω—ç—ç—Å –≥–∞–¥–Ω–∞ Azure Preview –≥—ç–∂ –æ–ª–æ–Ω –Ω–∏–π—Ç—ç–¥ –≥–∞—Ä–≥–∞–∞–≥“Ø–π —á —Ç—É—Ä—à–∏–ª—Ç—ã–Ω —Å–µ—Ä–≤–∏—Å“Ø“Ø–¥–∏–π–≥ –∞—à–∏–≥–ª–∞—Ö –±–æ–ª–æ–º–∂ –±–∞–π–¥–∞–≥.
Remote —Ö“Ø—á—ç—ç –∞–≤—á –±–∞–π–≥–∞–∞ —ç–Ω—ç “Ø–µ–¥ –±–∞—Å –Ω—ç–≥ —Ö—ç—Ä—ç–≥—Ç—ç–π –∑“Ø–π–ª –Ω—å –æ–Ω–ª–∞–π–Ω–∞–∞—Ä Windows –∫–æ–º–ø—å—é—Ç–µ—Ä –∞—à–∏–≥–ª–∞—Ö —é–º. –ó“Ø–≥—ç—ç—Ä —á –Ω—ç–≥ —Ö—ç—Ä—ç–≥–ª—ç—Ö–≥“Ø–π 1 –º–∞—à–∏–Ω—ã–≥ –æ–ª–æ–Ω —Ö“Ø–Ω –∞—à–∏–≥–ª–∞—Ö –±–æ–ª–æ–º–∂ –æ–ª–≥–æ–¥–≥–æ–æ—Ä–æ–æ –¥–∞–≤—É—É —Ç–∞–ª—Ç–∞–π –±”©–≥”©”©–¥ –≤—ç–± —Ö”©—Ç”©—á –±–∞–π—Ö–∞–¥ –ª —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π.
–•—É—Ä–¥, –∞—é—É–ª–≥“Ø–π –±–∞–π–¥–∞–ª, –¥–∞—Ç–∞ —Ö—ç—Ä—ç–≥–ª—ç—ç –≥—ç—Ö –º—ç—Ç –æ–ª–æ–Ω –∞—Å—É—É–¥–ª—ã–≥ —à–∏–π–¥–¥—ç–≥.
–¢—É–Ω —á—É—Ö–∞–ª —Å—ç–¥—ç–≤ –±–æ–ª–æ—Ö Security-–Ω —Ö—É–≤—å–¥ Firewall, DDoS protection, Security Center (AWS : Trusted Advisor) —Ç—ç–≥—ç—ç–¥ –±“Ø—Ä —É—Ö–∞–∞–ª–∞–≥ SIEM —Ç”©—Ä–ª–∏–π–Ω Sentinel –≥—ç—Ö —Å–µ—Ä–≤–∏—Å—Ç—ç–π. –ï—Ä –Ω—å —Ç—ç–≥—ç—ç–¥ –º”©–Ω–≥”©”© –ª —Ç”©–ª—á–∏—Ö–≤”©–ª –±“Ø—Ö –∑“Ø–π–ª–∏–π–≥ –Ω—å ”©–º–Ω”©”©—Å –Ω—å —Ö–∏–π–≥—ç—ç–¥ ”©–≥—á–∏—Ö–¥”©–≥ –±–æ–ª—á–∏—Ö–æ–æ–¥ –±–∞–π–Ω–∞ –¥”©”©.
AWS Cognito —à–∏–≥ Azure Active Directory (Cloud version of AD) –≥—ç–∂ –±–∞—Å –±–∏–π. –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–≥ —Ö—ç–Ω –≥—ç–¥–≥–∏–π–≥ —Ç–∞–Ω–∏—Ö, —é—É —Ö–∏–π–∂ —á–∞–¥–∞—Ö—ã–≥ —à–∞–ª–≥–∞—Ö –±“Ø—Ö–∏–π –ª –±–æ–ª–æ–º–∂—É—É–¥—Ç–∞–π —é–º –±–∏–ª—ç—ç. (SSO, MFA, Conditional Access)
21Vianet –Ω—å Microsoft –±–æ–ª–æ–Ω –•—è—Ç–∞–¥—ã–Ω –∑–∞—Å–≥–∏–π–Ω –≥–∞–∑–∞—Ä —Ö–∞–º—Ç–∞—Ä—Å–∞–Ω –∫–æ–º–ø–∞–Ω–∏ —é–º –±–∏–ª—ç—ç. –•—è—Ç–∞–¥—ã–Ω —Ö—É—É–ª–∏–π–Ω –¥–∞–≥—É—É Microsoft-–Ω “Ø–π–ª—á–∏–ª–≥—ç—ç–≥ “Ø–∑“Ø“Ø–ª–¥—ç–≥ region –≥—ç–∂ —Ö—ç–ª–∂ –±–æ–ª–Ω–æ. –ö–æ–º–ø–∞–Ω–∏–π–Ω 51 —Ö—É–≤–∏–π–≥ –•—è—Ç–∞–¥—ã–Ω –∑–∞—Å–≥–∏–π–Ω –≥–∞–∑–∞—Ä —ç–∑—ç–º—à–¥—ç–≥ –±”©–≥”©”©–¥ –∞—Å—É—É–¥–∞–ª –≥–∞—Ä–≤–∞–ª —à–∏–π–¥–≤—ç—Ä–∏–π–≥ –¥–∞–Ω–≥–∞–∞—Ä–∞–∞ –≥–∞—Ä–≥–∞—Ö —á–∞–¥–∞–ª—Ç–∞–π –≥—ç—Å—ç–Ω “Ø–≥. –•—è—Ç–∞–¥ —è–∞–ª—Ç —á “Ø–≥“Ø–π –∏—Ö –≥“Ø—Ä—ç–Ω —é–º –∞–∞.
–ò–Ω–≥—ç—ç–¥ —Å–µ—Ä–≤–∏—Å“Ø“Ø–¥–∏–π–Ω —Ç–∞–ª–∞–∞—Ä—Ö —Ö—ç—Å–≥—ç—ç –¥—É—É—Å–≥–∞—è. –ë“Ø—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π —Ç–∞–ª–∞–∞—Ä –¥—É—Ä–¥–∞–∂ —á–∞–¥–∞—Ö–≥“Ø–π —Ç—É–ª ”©”©—Ä—Ç —Å–æ–Ω–∏—Ä—Ö–æ–ª—Ç–æ–π —Å–∞–Ω–∞–≥–¥—Å–∞–Ω —Ö—ç–¥–∏–π–≥—ç—ç –ª –±–∞–≥—Ç–∞–∞–ª–∞–∞. –û–¥–æ–æ —Ö–∞—Ä–∏–Ω —à–∞–ª–≥–∞–ª—Ç—ã–Ω —Ç–∞–ª–∞–∞—Ä —è—Ä–∏–ª—Ü–∞—Ü–≥–∞–∞—è.
–®–∞–ª–≥–∞–ª—Ç—ã–Ω —Ö—É–≤—å–¥ 4 —Ç”©—Ä–ª–∏–π–Ω –Ω–∏–π—Ç 40‚Äì60 –∞—Å—É—É–ª—Ç—Ç–∞–π. 40‚Äì60 –≥—ç–¥—ç–≥ –Ω—å —Ç—É—Ö–∞–π–Ω “Ø–µ—ç—ç—Å —Ö–∞–º–∞–∞—Ä–∞–∞–¥ ”©”©—Ä ”©”©—Ä –±–∞–π–¥–∞–≥ —é–º –±–∏–ª—ç—ç. –ú–∞–≥–∞–¥–≥“Ø–π –∞—Å—É—É–ª—Ç—ã–Ω —Ö—ç–ª–±—ç—Ä—ç—ç—Å —à–∞–ª—Ç–≥–∞–∞–ª–¥–∞–≥ –±–∞–π—Ö.
Linux Academy-–Ω —Ö–∏—á—ç—ç–ª—ç—ç “Ø–∑—ç–∂ –¥—É—É—Å–≥–∞–∞–¥, —Ö—ç—Å—ç–≥ –±–æ–ª–≥–æ–Ω—ã —Ç–µ—Å—Ç“Ø“Ø–¥, –¥–∞—Å–≥–∞–ª –∞–∂–ª—ã–≥ —Ö–∏–π–≥—ç—ç–¥, —Ç”©–≥—Å–≥”©–ª–∏–π–Ω —Ç–µ—Å—Ç–∏–π–≥ 5 —É–¥–∞–∞ —Ö–∏–π—Ö—ç–¥ 60, 83, 90, 85, 90 –∞–≤—Å–∞–Ω.
–ë–∞—Å IT exams –±–æ–ª–æ–Ω Whizlabs –¥—ç—ç—Ä—Ö —Ç–µ—Å—Ç–∏–π–≥ —Ö–∏–π—Å—ç–Ω. LinuxAcademy-–Ω —Ç–µ—Å—Ç–∏–π–≥ –±–æ–¥–≤–æ–ª –∏–ª“Ø“Ø –±–æ–¥–∏—Ç —Ç–µ—Å—Ç –±–∞–π–Ω–∞ –ª—ç—ç.
”®–º–Ω”©—Ö Oracle-–Ω —à–∞–ª–≥–∞–ª—Ç—É—É–¥—Ç–∞–π –∞–¥–∏–ª —Ö“Ø–Ω–≥“Ø–π, –∏–ª“Ø“Ø –¥—É—Ç—É—É –Ω–æ–º —Å–æ–Ω–∏–Ω –±–∞–π—Ö–≥“Ø–π, —á–∏–º—ç—ç–≥“Ø–π –æ—Ä—á–∏–Ω–¥ —à–∞–ª–≥–∞–ª—Ç–∞–∞ ”©–≥—Å”©–Ω –±”©–≥”©”©–¥ ID —à–∞–ª–≥—É—É–ª–∞—Ö–¥–∞–∞ –∂–æ–ª–æ–æ–Ω—ã “Ø–Ω—ç–º–ª—ç—Ö—ç—ç –∞—à–∏–≥–ª–∞—Å–∞–Ω.
MS : AZ900 —Ö—É–≤—å–¥ —à–∞–ª–≥–∞–ª—Ç —ç—Ö–ª—ç—Ö—ç—ç—Å ”©–º–Ω”© —Ç—É—Ö–∞–π–Ω —à–∞–ª–≥–∞–ª—Ç—ã–Ω —Å—ç–¥–≤“Ø“Ø–¥–∏–π–≥ —Ö—ç—Ä —Å–∞–π–Ω –º—ç–¥—ç—Ö –≤—ç –≥—ç–∂ –∞—Å—É—É–∂ –±–∞–π–Ω–∞ –ª—ç—ç. –Ø–≥ —à–∞–ª–≥–∞–ª—Ç—ã–Ω –∞—Å—É—É–ª—Ç —è–º–∞—Ä –±–∞–π—Ö —ç—Å—ç—Ö—ç–¥ –Ω”©–ª”©”©–ª–¥”©–≥ —é–º—É—É –≥“Ø–π —é–º —É—É –º—ç–¥—ç—Ö–≥“Ø–π —é–º. –ë–∏ –±“Ø–≥–¥–∏–π–≥ –Ω—å Moderate –±—É—é—É –¥—É–Ω–¥–∞–∂/–µ—Ä”©–Ω—Ö–∏–π–¥ –Ω—å –≥–∞–¥–∞—Ä–ª–∞–Ω–∞ –≥—ç—ç–¥ –±”©–≥–ª”©”©–¥ —Ç–µ—Å—Ç—ç—ç —ç—Ö–ª“Ø“Ø–ª—Å—ç–Ω –¥—ç—ç.
–ù–∞–¥–∞–¥ –∏—Ä—Å—ç–Ω 42 –∞—Å—É—É–ª—Ç–∞–∞ 18 –º–∏–Ω—É—Ç–∞–¥ –¥—É—É—Å–≥–∞–∞–¥, —Ç—ç–º–¥—ç–≥–ª—ç—Å—ç–Ω (mark) 4 –∞—Å—É—É–ª—Ç–∞–∞ –¥–∞—Ö–∏–∂ —Ö–∏–π–≥—ç—ç–¥, —ç—Ö–Ω—ç—ç—Å –Ω—å –±“Ø–≥–¥–∏–π–≥ –Ω—å review-–¥—ç—ç–¥ —à–∞–ª–≥–∞–ª—Ç–∞–∞ –¥—É—É—Å–≥–∞—Ö–∞–¥ —è–≥ 30 –º–∏–Ω—É—Ç –±–æ–ª–∂ –±–∞–π–Ω–∞ –ª—ç—ç. –•–∞—Ä–∏—É –Ω—å —á –±–∞—Å —Ç–µ—Å—Ç—ç—ç –¥—É—É—Å–≥–∞–∞–¥ –ª —à—É—É–¥ –≥–∞—Ä–∞–∞–¥ –∏—Ä–¥—ç–≥.
820 –æ–Ω–æ–æ–≥–æ–æ—Ä –¥–∞–≤—Å–∞–Ω.
AWS-—Ç–∞–π —Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö–∞–¥ portal –Ω—å ”©”©—Ä, –≥–∞—Ä —É—Ç–∞—Å–Ω—ã –∞–ø–ø—Ç–∞–π, –∑–∞—Ä–∏–º —Å–µ—Ä–≤–∏—Å“Ø“Ø–¥ –Ω—å –∏–ª“Ø“Ø —é–º —à–∏–≥ —Å–∞–Ω–∞–≥–¥—Å–∞–Ω. –ó–∞—Ö –∑—ç—ç–ª–∏–π–Ω —Ö—É–≤—å–¥ —Ö—É—É—á–∏–Ω Microsoft-–Ω —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–¥ –±–∞–π–≥–∞–∞ —É—á–∏—Ä —Ç—ç–¥ –Ω–∞—Ä—ã–≥–∞–∞ —Ç–∞—Ç—Å–∞–Ω “Ø–π–ª—á–∏–ª–≥—ç—ç –≥–∞—Ä–≥–∞–∂, —É–ª–∞–º —Ç–æ–º—Ä–æ–æ–¥ –±–∞–π–≥–∞–∞ –≥—ç–∂ –±–æ–ª–æ—Ö–æ–æ—Ä. –•—ç—Ä–≤—ç—ç –±–∏ Microsoft-–Ω —Ö—ç—Ä—ç–≥–ª—ç–≥—á –±–∞–π—Å–∞–Ω –±–æ–ª —è–º–∞—Ä —á —ç—Ä–≥—ç–ª–∑—ç—ç–≥“Ø–π Mcrosoft : Azure –ª“Ø“Ø —à–∏–ª–∂–∏—Ö –±–∞–π—Ö –∞–∞.
–≠–Ω—ç —Ö“Ø—Ä–≥—ç—ç–¥ –Ω–∏–π—Ç–ª—ç–ª—ç—ç –¥—É—É—Å–≥–∞—è. –î–∞—Ä–∞–∞—á–∏–π–Ω —É–¥–∞–∞ —Ö–∞—Ä–∏–Ω Alibaba-–Ω “Ø“Ø–ª–∏–π–≥ —É—Ö–∞–∂ —Ç”©–Ω—Ö”©–Ω”© ”©”©. Where is Jack Ma?
To be +1% better today¬†: Learn & Try & Share (LTS)
159 
1
159¬†claps
159 
1
Written by
I am who I am... || ”©”©—Ä–∏–π–Ω—Ö”©”©—Ä”©”© –±–∞–π—Ö–∞–∞—Å –∏—á–∏—Ö–≥“Ø–π
To be +1% better today¬†: Learn & Try & Share (LTS)
Written by
I am who I am... || ”©”©—Ä–∏–π–Ω—Ö”©”©—Ä”©”© –±–∞–π—Ö–∞–∞—Å –∏—á–∏—Ö–≥“Ø–π
To be +1% better today¬†: Learn & Try & Share (LTS)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/daylightnightlite/azure-af4bd5610411?source=search_post---------279,"There are currently no responses for this story.
Be the first to respond.
Azure pure as the driven snow
Blue is beautiful
Coruscated and dancing
Daring and wonderful
Everyone here is different
Feelings can be fickle
Give an inch take a mile
"
https://medium.com/@eekayonline/connecting-atlassian-sourcetree-with-your-azure-devops-git-repo-b88aeab91b00?source=search_post---------280,"Sign in
There are currently no responses for this story.
Be the first to respond.
Edwin Klesman
Jan 31, 2020¬∑3 min read
In this short post I will show you how you can setup Atlassian SourceTree so it will connect with Microsoft Azure DevOps project‚Äôs GIT repo.
I like using SourceTree in my development teams since it is an easy interface, is available for both Windows and Mac OS, and it supports the‚Ä¶
"
https://koukia.ca/push-docker-images-to-azure-container-registry-ed21facefd0c?source=search_post---------281,"In the previous post I demonstrated how to create docker images using docker-compose. Now I will show you how you can push those images into Azure Container Registry.
I will be using Azure CLI version 2.0 to interact with Azure from the command prompt.
"
https://medium.com/@tsuyoshiushio/writing-unit-test-for-azure-durable-functions-80f2af07c65e?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Mar 8, 2018¬∑2 min read
One of the Frequently Asked Questions related Azure Functions is How to do a unit testing. On this article, I‚Äôd like to explain how to write a unit testing for C# dll v2 application.
You can find the unit testing example on the Azure-Samples / functions-unittesting-sample.
Writing a unit test of Azure Functions is quite easy. Just mock the parameter of a function.
For example, this is the code of HttpTrigger template. You can ignore the Attributes for the unit testing.
Now I try to write a unit test for this method.
Helper method HttpRequestSetup create a mock object using Moq. I wrote a helper method to enable test makes easier. You can refer the whole test in here. The unit test is just mock the parameters and set the values. Then the assert the output.
Sometimes, Azure Functions use custom class, like IAsyncCollector<T>. You also mock this by Poco or Moq.
I also create a simple Mock object on the FunctionTest.cs which is just a helper of unit testing.
If you want to create a Mock object by yourself, you may want to know the original code for that.
You might see the code by right-click the Class on your Visual Studio and select `Go to definition`.
If you can‚Äôt see that, you can find it on the following github.
For the Function App V1, you can find these at Azure WebJobs SDK.
e.g. if you want to find ServiceBus, you can go `src/Microsoft.Azure.WebJobs.ServiceBus/Bindings`.
For the Durable Functions testing, you might encounter the issue of mocking. The parameter classes are sealed class. Which means you can‚Äôt mock it. Then how to mock these?
Make sure if your Durable Functions version is beta3 or upper.
The Azure Functions production team get an issue about this and add base class for mocking. Then you can change the signature like this.
Now you can test it. On this part, I only test the Orchestrator responsibility.
You can write Unit Tests for Azure Functions simply mock the parameter. In case of Durable Functions, you need to use base class for it. If you find an issue which prevent you to write unit testing, please let us know as an issue of GitHub.
Senior Software Engineer ‚Äî Microsoft
See all (200)
81 
81¬†claps
81 
Senior Software Engineer ‚Äî Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/10-best-microsoft-azure-courses-for-beginners-and-experienced-developers-d41a454834c0?source=search_post---------283,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn about the Microsoft Azure Cloud Platform in-depth and looking for the best online training courses then you have come to the right place. In the past, I have shared the best courses to prepare for Azure Fundamentals, Azure Administrator, and Azure Architect certifications, and today I am going to share the best online course to learn Azure platform for Beginners.
These are top-quality courses from expert trainers and instructors and picked from websites like Udemy, Coursera, Pluralsight, and edX.
You can use these courses to not only learn Microsoft Azure core services but also to prepare for different Microsoft Azure certifications like AZ-900, AZ-300, and AZ-200 or Azure developer associate certification.
Microsoft Azure, is a cloud computing service created by Microsoft meant for building, testing, deploying, and managing applications and services through Microsoft-managed data centers. It provides software, platform, and infrastructure as a service and supports many programming languages, tools, and frameworks, including both Microsoft-specific and third-party software and systems. This makes it a very good platform because of its flexibility. You can have different projects in the same interface so you don‚Äôt have to be changing or learning the peculiarities of every different platform. So if you have different projects or you work with different programming languages is a good option to optimize your time and resources. Also, demand for Azure certified developers, administrators, and cloud architect is increasing exponentially as more and more companies are using Azure for their migrating their on-premise software and application. This means it‚Äôs the best time to learn Azure and boost your career profile.
At the same time, Microsoft Azure is a huge collection of services for building applications in the cloud and can often be overwhelming.
That‚Äôs why choosing the best online training courses is crucial and that‚Äôs where this article will help you. here you will find the best Azure Cloud Computing courses from sites like Udemy, Coursera, Pluralsight, and edX which is developed by experts and trusted by thousands of people like you.
Without wasting any more of your time, here you have a top 5 of the best tutorials to learn about Azure. These are the best courses from Udemy, Coursera, and edX and you can use them to learn Azure and prepare for different Azure certifications like Azure Fundamentals and Azure Architecture TEchnology.
This is another best course to learn Azure on Udemy. If you want to be an expert with Azure this is an online course you should join.  This course has a total of 12 sections, during the course you will see:
As you can see it comprehends a lot of subjects. All the course includes more than 30 hours of video, 6 articles, and 103 downloadable resources, which makes this is one of the most comprehensive Azure course on Udemy.
Here is the link to join this course ‚Äî Microsoft Azure cloud ‚Äî Beginner Boot camp
This is another Udemy Azure course which is great to learn Microsoft Azure from scratch. As its name suggests this course is for beginners but to start this course you‚Äôll need some basic IT knowledge on networks, databases, and how Web servers work.  You will learn:
By the end of the course, you‚Äôll be prepared for taking the AZ-900 certification exam. The course includes 2 simulation exams for you to practice. Apart from that, it includes 10 and a half hours of video tutorials, 13 articles, and 12 resources.
Here is the link to join this course ‚Äî Microsoft Azure ‚Äî Beginner‚Äôs Guide
This is the best Azure cloud course from Udemy and you can use this to learn Azure in-depth as well as to prepare for the AZ-300 exam. This course is meant for someone that has knowledge in more than one language and want to be more efficient.
During the course you will learn how to:
The course consists of 20 and a half hours of explanation videos that you can consult the times you want. By the end, you will master the Azure environment and you will be considerably more efficient.
Here is the link to join this Azure course ‚Äî AZ-300 Azure Architecture Technologies Exam Prep 2021
This course focuses on the Fundamentals of Azure Infrastructure. It begins with understanding the subscription system, configuring the security, and acquiring storage. Then you‚Äôll build virtual machines and VNETS to start working.  While doing the tutorial you will be able to:
It takes 30 hours to complete the course, and by the end, you will have a certificate. It is meant for an advanced profile, that already knows some different languages and wants to unify all his projects together on the same platform.
Here is the link to join this course ‚Äî Azure Infrastructure Fundamentals by Learn Quest
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking the Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but it's completely worthy of your money as you get unlimited certificates.
coursera.com
This course‚Äôs focus is to use virtual machines in Azure for workloads, emphasizing basic configuration, planning, deployment, and management tasks.
You‚Äôll be introduced to how to secure and make your virtual machines highly available and scalable. It is a good start for entering the Azure world but not having to see all its features.  Here are the key things you will learn in this course:
This course can be done completely free, and then when you finish you can pay for the certificate of finalization if you desire to.
Here is the link to join this course ‚Äî Microsoft Azure Virtual Machines by Microsoft
This is one of the best beginner-level Azure course from Pluralsight, another leading online portal for programmers and developer.s
In this course, you will learn foundational knowledge to begin planning solutions using Microsoft Azure. First, you will learn about cloud computing and the different ways to run your application code.
After that, you will discover the data storage, processing, and analysis capabilities in Azure. Finally, you will explore how to create networks; integrate, manage, and secure your applications; and develop for Azure.
After completing this online Azure course, you will have the skills and knowledge of Microsoft Azure needed to begin working on your cloud solutions.
Here is the link to join this course ‚Äî Microsoft Azure: The Big Picture
This is another great Pluralsight course to learn Microsoft Azure from scratch. Created by Matt Milner, this course will introduce you to the new world of cloud computing and how to build on the Windows Azure Platform.
You will not only learn about Windows Azure compute and storage and SQL Azure but also learn about cloud computing tradeoffs and help you understand the constraints and limitations imposed by the cloud computing model offered by the Windows Azure Platform today.
Matt is one of the Microsoft Tech experts and he has a knack for making things simple which really helps to understand complex cloud computing concepts.
here is the link to join this course ‚Äî Microsoft Azure Fundamentals
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount).
I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
pluralsight.pxf.io
This is an excellent Udemy course for both beginners and experienced software developers and tech leads who want to learn Learn how to plan, Manage, and Deploy Your Very First Azure DevOps Application Through Hands-On Demos and Use Cases.
In this Azure DevOps Fundamentals course, Microsoft Certified Trainer and bestselling Udemy instructor Brian Culp take you on a hands-on tour of Azure DevOps and shows you how to manage development teams, code, and application deployments.
Best of all, Brian understands that students taking a Fundamentals course may not yet be familiar with all the vocabulary and terms and technologies that are part of the DevOps landscape, hence he explains all of those things whenever they come in like Agile, Scrum, App Containers, Commits, Code Requests, Sprints, Unit Testing, Code Artifacts, and other DevOps jargons.
Since this course is designed for people who are new to DevOps terms and concepts, I highly recommend this to both beginners and experienced software developers to join this course.
Here is the link to join this course ‚Äî Azure DevOps Fundamentals for Beginners
As the title suggests, this Azure course is for people who want to become Data Engineer and wants to work on the Microsoft Azure Cloud platform. Microsoft Azure Date Engineering is one of the fastest-growing and in-demand fields for Data Science practitioners.
In this course, you will learn about Azure data technologies like Microsoft Azure SQL Database, Data Lake, Data Factory, Synapse Analytics, Cosmos DB, Databricks, and HDInsight.
This is a massive course with 34.5 hours of content and over 50 articles to learn all essential data technologies you want to learn in Azure. This course is also good for DP-200 and DP-2001.
Here is the link to join this Azure course ‚Äî Azure Data Engineer Technologies for Beginners
This is another course that I recommend to an experienced developer who wants to master the Azure platform. In this course, you will learn about essential DevOps tools and technologies like Docker, AKS, Azure Disks, DevOps, Virtual Nodes, ACR, DNS Zones, Active Directory, Kubernetes, Ingress, and Terraform.
This course is ideal for Azure Architects, Sysadmins, or Developers who are planning to master Azure Kubernetes Service (AKS) for running applications on Kubernetes.
The best thing about this course is that it is full of hands-on experience with clear explanations which really helps to understand what, why, and how part of any concept. In short, a great course to master Azure DevOps, particularly AKS or Azure Kubernetes Service.
Here is the link to join this course ‚Äî Azure Kubernetes Service with Azure DevOps and Terraform
That‚Äôs all about the best courses to learn the Microsoft Azure Cloud Platform in-depth in 2021. These online courses are also good for various Azure certifications like Azure Fundamentals and Azur Cloud Architect. If you are thinking to go cloud native then Microsoft Azure is a very good alternative to improve your workflow and make things easier for you. It is very extensive, so you have the articles explaining what they offer, so you can choose the one that adapts better to what you are looking for.  Other Azure Cloud Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure courses useful, then, please share it with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. ‚Äî If you are keen to learn Microsoft Azure Platform from scratch but looking for free online training courses to start your journey then you can also see Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It‚Äôs completely free and all you need is a free Udemy account to join this course online.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
155 
3
155¬†claps
155 
3
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://koukia.ca/can-you-implement-a-chat-bot-with-microsoft-bot-framework-without-azure-1446ce8333bf?source=search_post---------284,"Microsoft Bot Framework is an amazing framework that people are using it more and more to build Bots these days, because it is so simple to put together a Bot very quick.
I have also published a post here a while ago, that you can go through and learn a bit more about this Microsoft Bot Framework if you are not familiar with it, but today, I‚Ä¶
"
https://medium.com/hackernoon/azure-brute-farce-17e27dc05f85?source=search_post---------285,"There are currently no responses for this story.
Be the first to respond.
Azure AD is the de facto gatekeeper of Microsoft cloud solutions such as Azure, Office 365, and Enterprise Mobility. As an integral component of their cloud ecosystem, it is serving roughly 12.8 million organizations, 950+ million users worldwide, and 90% of Fortune 500 companies on a growing annual basis. Given such a resume, one might presume that Azure Active Directory is secure, but is it?
Despite Microsoft itself proclaiming ‚ÄúAssume Breach‚Äù as the guiding principle of their security strategy, if you were to tell me a week ago that Azure or Office 365 was vulnerable to rudimentary attacks and that it could not be considered secure, then I probably would have even laughed you out of the room. But when a client of ours recently had several of their Office 365 mailboxes compromised by a simple brute-force attack, I was given no alternative but to question the integrity of Azure AD as a whole instead of attributing the breach to the services merely leveraging it and what I found wasn‚Äôt reassuring.
After a simple ‚ÄúOffice 365 brute force‚Äù search on google and without even having to write a line of code, I found that I was late to the party and that Office 365 is indeed susceptible to brute force and password spray attacks via remote Powershell (RPS). It was further discovered that these vulnerabilities are actively being exploited on a broad scale while remaining incredibly difficult to detect during or after the fact. Skyhigh Networks named this sort of attack ‚ÄúKnock Knock‚Äù and went so far as estimating that as many as 50% of all tenants are actively being attacked at any given time. Even worse, it seems as if there is no way to correct this within Azure AD without consequently rendering yourself open to denial of service (DOS) attacks.
In fact, this sort of attack is so prevalent that it happens to be one of the biggest threats to cloud tenant security at Microsoft according to Mark Russonivich (CTO of Azure) and is among several reasons that Microsoft itself advises their customers to enable multi-factor authentication (MFA) for all users and implement advanced threat intelligence available only to E5 subscription levels or greater; basically requiring companies to give Microsoft more money to secure their own solutions. But MFA also doesn‚Äôt impede hackers from cracking passwords or protect businesses from a DOS attack nor does it help those that are unaware of its necessity as many tenants are at present.
Further, since RPS does not work with deferred authentication (DAP) and MFA, partners consisting of consultants, managed services and support providers also cannot use their partner credentials to connect to the tenants of their clients via RPS for advanced administration and scripting. Even though they can easily manage their clients via a browser-based admin center with MFA, they often have to resort to creating admin accounts within Office 365 tenant itself instead, but others do it simply for ease of access to the admin console or for when they are not the Partner On Record. These accounts are precisely what many of these attacks are targeting, often unbeknownst to admins, and Deloitte‚Äôs breach is a perfect example of such a scenario.
Unfortunately, these accounts are often stripped of MFA security to make them more convenient and accessible for the multitude of support and operations staff to use while working for various companies offering support services and they seldom expire or change upon company exit. By default in Office 365 and on top of being vulnerable to being cracked and breached, the password expiration policy is further set to a 730-day expiration and further disabled, rendering accounts vulnerable to a prolonged breach at that. Needless to say, they are ripe for attack and this exact scenario is what enabled a hacker to have unabridged administrative access to Deloitte‚Äôs Exchange Online tenant for 6+ months.
Complicating matters even further, the natural solution to this problem renders the tenant vulnerable to DOS attacks by virtue of being able to lock users out of their accounts for a fixed duration imposed by Azure AD; but this is still in preview phases. For example, by default Azure AD Smart Lockout (Preview Stage), which is still in preview, is configured to allow 10 password attempts before subjecting the account to a 60-second lockout, giving attackers a theoretical limit of 14,400 attempts per account/per day. You could decrease the threshold to 5 and increase the duration to 5 minutes protect against breaches, reducing attempts to 1,440 per day, but this would create the potential for downtime for users whenever their accounts are being attacked with brute force and password spray attacks.
However, Tyler Rusk at CSSI also called out that Microsoft doesn‚Äôt seem to throttle or limit authentication attempts made through RPS. As shown, Tyler was able to surpass the theoretical 14,400 per day limit listed in Azure AD Smart Lockout Preview without added logic, moving at a rate of 48,000 per day had he let it run for a 24 hour period or an est. 17,520,000 attempts over 365 days. However, there are obvious ways to optimize these efforts even further through via background jobs (start-job cmdlet) by essentially running attacks asynchronously instead of synchronously while optimizing for custom lockout limits, max attempts, and minimal detection. The possibilities are endless with regard to password spray attacks for obvious reasons. To be fair to Tyler and CSSI though and in my opinion, they didn‚Äôt need to leverage such measures to validate their concern.
If their lockout feature were to work though and if you were able to reduce the threat surface in the manner above, you would then have to contend with the hard countdown of the duration time. It‚Äôs immutable which means that users have to wait for it expire in order to render the account accessible again. The unlock cannot be expedited administratively at present. As such, it can just as easily result in an intentional DOS for end users if they or an unintentional DOS while running the possibility of exposing the attack; that is when/if it starts actually working. Obviously protecting from breach takes precedent over downtime, but becoming prone to DOS attacks is hardly a consolation prize.
Banned passwords nor MFA cannot protect against DOS or brute-force attacks either, only against the breach itself. In fact, when brute forcing an account protected by MFA, the MFA challenge itself can be treated as confirmation of a valid cracked username and/or password. In turn, they can then begin to try these credentials in other places which may not be protected by MFA as users and admins alike tend to keep them as similar as possible in multiple directories so that they‚Äôre easy to remember. I‚Äôll defer to Ned Pyle of Microsoft as to whether this applies to his employer and their partners.
Summarizing matters thus far, you can brute force accounts housed in Azure AD via RPS. Obvious solutions for this such as MFA, customized password blocking, and advanced threat intelligence are either ineffective, insufficient, paywalled, and/or generate significantly more overhead in order to offset these vulnerabilities. Further, these solutions are often ignored by lazy admins, consultants, and managed services providers and many may be oblivious to this threat entirely; possibly even to breaches of their own. Deloitte has proven that this can even hit the best of them.
As offensive as all of this may seem though, it‚Äôs important to remember that AD was never designed to be public facing, quite the opposite. It has actually always been inherently vulnerable to brute-force, password spray, and DOS attacks by design. AD has always been designed to be implemented in conjunction with various other counter-measures in order to maintain its integrity. This includes but certainly is not limited to relying on physical security measures such as controlled entry and limiting the ability to access the domain to those that make it past physical security measures successfully; with the obvious exception of VPN users. This is nothing new.
That said, AD was never, ever, meant to be the sole source of security for IT infrastructure and is fundamentally dependent on other security measures in order to be effective. Consequently, AD becomes markedly more vulnerable when other pre-emptive methods fail or are non-existent. Put simply, such breaches should be the expectation when depending on Azure AD alone for IT security, and this sadly applies to any Office 365 tenant with its default security settings. However, understanding its limitations helps us illuminate ways to harden Azure AD and mitigate these problems just the same.
It almost goes without saying, but none of the measures necessary to patch these vulnerabilities are free to companies leveraging these services at present. Even if Microsoft were to fix this, who is to say that something else just as simplistic and embarrassing isn‚Äôt hiding around in the corner or already being used? That said, avoiding products backed by a 20-year-old security system streamlined for vendor lock-in seems like a viable solution to avoiding this problem in the first place.
Before anything else, I truly think that the onus is on Microsoft to ensure that their baseline configuration for cloud accounts doesn‚Äôt expose their tenants unnecessarily. Sure, we could blame ignorant users and lazy admins, but I don‚Äôt think that this is fair given the scope of this vulnerability, which is essentially 46% of AzureAD‚Äôs user-base (password hash sync + cloud only = 46%). It is unknown how many have MFA enabled and the scope of this is ultimately an unknown both with regard to those who are vulnerable to it, actively being attacked, and/or those already breached though. But as a former tier 3 support engineer for Exchange Online at Microsoft, I can confirm that a significant amount of individuals as well as small-medium businesses are relying on Azure AD exclusively without further counter-measures and that they account for a sizable amount of Office 365‚Äôs user-base. That said, telling customers that pay you to secure their mailboxes or to disable basic auth to address this doesn‚Äôt cut it.
Microsoft has clearly acknowledged this problem, but rather than hardening their tenants from such attacks as other cloud services have, they have offered solutions only available to their high tier plans so as to capitalize on this problem rather than fixing it. As expensive as they are to migrate away from now, or sticky as they like to call it, their products are just going to become more costly to manage, vulnerable, and difficult to migrate away from over time. This is the malady of any legacy solution.
One easy way for Microsoft to mitigate such attacks is to update their RPS module to support DAP and develop other creative avenues for admins and the like to efficiently and securely manage their clients‚Äô tenants. They should also extend their threat intelligence and advanced customizations available only to costly, high tier license subscribers to all license levels, at least until proper solutions are implemented for all tenant levels.
As an immediate mitigation step though, Microsoft could simply swap the order of authentication. Rather than requiring a password prior to doing a two-step verification on your phone, they could require the phone verification through authenticator app or a third party MFA app such as Duo as the initial means of authentication. By deferring their password in Azure AD as the second step instead of the first, they could buffer its weak password security at present and buy time to implement a proper solution. However, this only applies to users and tenants with MFA enabled and in-use.
Just as Active Directory seems to create necessity for other costly ancillary solutions, Microsoft seems to have built AzureAD to generate further necessity for more costly solutions coincidentally offered by them just the same. On top of this and if they had their way, their solution to enable MFA would also require employers to buy phones and mobile plans for two-step verification for all of their employees which can cost more on an annual basis than any of their plans.The same can be said of the costs associated with a proper MFA solution and/or an on-premises or hosted ADFS solution (if none exist) as they drastically complicate the solution as a whole while consequently inflating the ownership costs associated with it. As complexity increases, stability falters while costs skyrocket. All of which is why I recommend avoiding their solutions entirely.
But if a company is entrenched with Microsoft products and migration is out of reach, there are options. One solution that companies can implement is ADFS which defers authentication attempts to your own domain controllers on-premise rather than Azure AD while immediately granting more granular control of password policies with Active Directory on-premise and as much protection as money can buy on the network layer. All of which can be quite costly from a licensing perspective alone, let alone the hardware, network infrastructure, and labor required to implement it all let alone the staff to maintain it. This creates a single point of failure, often on-premise, for a cloud solution unless implemented in a highly available manner though.
They can also implement an MFA solution as well but there still remains added exposure and vulnerabilities which may require further consideration. But as mentioned before, there are also added costs and MFA may not protect accounts entirely. Users tend to manually synchronize their passwords across multiple platforms for the sake of remembering it, but not all of them have the same protections, MFA or otherwise. Similar to ADFS, access to your mailbox and other apps are restricted when MFA services are degraded, also becoming a single point of failure, as shown today by Azures MFA outage. So if you go with an MFA solution, diversify with a 3rd party MFA provider.
While the existence of dirsync can do little to protect against brute-force attacks, enforcing a strong password policy including a customized banned password list on premise can be mirrored in the cloud. Customers with dirsync already pay for this functionality with Active Directory on premise and can simply have it be mirrored in the accounts synced to the Azure AD forest. Although this cannot protect from brute force, password spray, or denial of service attacks, it can absolutely harden accounts against prolonged breaches.
I suppose they could also call support to complain about it and see if they‚Äôll fix it, but you will likely be met by someone difficult to understand without experience on such matters. Or maybe they could even get a technical account manager to yell into the void or possibly even find someone with half of an ass on your behalf if you have deep enough pockets for a premier membership. While you‚Äôre at it, maybe you could upgrade your E3 plan to an E5 plan at almost double your monthly cost of E3 just to pay Microsoft to compensate for its own vulnerabilities.
In summary, Microsoft services built on Azure AD along with the businesses leveraging them are vulnerable to brute-force and password spray attacks which can be carried out by anyone with the capacity to run a script in RPS. Also, there isn‚Äôt an adequate means of hardening these services without incurring significant financial burden and paying for more of Microsofts services. All of which has probably been the case for as long as the ability to access tenants via RPS has been widely available to admins and ultimately why you would be wise to assume breach with Microsoft cloud solutions just as Microsoft does. Entities can absolutely mitigate these vulnerabilities, but Office 365 and Azure would cease to function as true cloud solutions while generating significantly more overhead costs in the process. All things considered though, it seems as if there is no way to harden Azure AD or the services such as Azure or Office 365 when leveraged by itself without incurring significant costs in addition to the aforementioned introduction of further complexity, points of failure, and on-premise dependencies for your cloud architecture.
By default , Azure AD is more of a security problem than a cloud. This is not to say that Azure cannot be made to be secure but it comes at a cost while sacrificing cloud resiliencies. Although they advise others to assume breach, Microsoft seems to be omitting this reality from Office 365 and Azure advertisements and such inconsistencies are indicative of this stance being more of a cop out than a tenable security strategy because of this. Rather than hardening the vulnerabilities inherent to Active Directory and Azure AD which makes them susceptible to some of the oldest tricks in the book, Microsoft seems to be attempting to capitalize on them instead while exposing those unaware to a haunting amount of risk.
#BlackLivesMatter
95 
95¬†claps
95 
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/levvel-consulting/saml2-vs-jwt-apigee-azure-active-directory-integration-a-jwt-story-a3eb00769a1f?source=search_post---------286,"There are currently no responses for this story.
Be the first to respond.
In our next SAML2 vs JWT post, we are going to use a JWT with a very simple API that is proxied through Apigee Edge Public Cloud. The JWT token will be an OAuth2 access token generated by Azure Active Directory. In the last post in this series, we explored what JSON Web Tokens (JWTs) are and the information it contains. This post builds on the capabilities presented by Dino Chiesa at the Apigee I Love APIs conference in October, 2015. The example presented in this post is available in this github repository. This post briefly describes how to adapt the solution to working with other Azure Active Directory tenants, but primarily focuses on the details of making work with the example tenant setup here ‚Äî keeping a functioning AAD tenant up and available for this example costs money and presents some security issues.
The figure below shows the system actors that will be involved in this story.
An API Consumer will obtain an access token (a JWT) by authentication against Azure Active Directory using an OAuth2 Resource Owner Password Credentials Authorization Grant. The JWT is then placed into the Authorization header of an API request and sent to the Apigee Edge API Gateway that we‚Äôve constructed. The API Proxy pulls down and caches the federation metadata that is published by Azure Active Directory. The API Proxy attempts to validate the JWT token included in the request; the token is determined to either be valid or not. If it is valid, then the JWT is removed from the request and passed back to the API Provider. If the token is not valid, then an error is returned to the API Consumer.
First, we need an API. Let‚Äôs use https://timeapi.org. This will create a nice example that is harmless and mostly pointless (there are probably easier ways to find the time). Without the API Gateway layer, this API call will look something like:
Request:
Response:
The full documentation of this API is available at https://www.timeapi.org.
There are numerous API interface documentation paradigms out there. I like to use Swagger 2.0 or OpenAPI v2.0. Apigee is built around this interface language, I work for an Apigee partner, and it is simple to setup for most APIs. I created a simple Swagger definition that describes the functionality of this API that we are using. It can be found here. Apigee has the ability to define an API Proxy based upon the structure of an existing Swagger interface definition. This can definitely eliminate some of the busy work associated with creating a new API Proxy on Apigee Edge, but it will lead to a much more complex example for what we are trying to show.
Now, our goal is to offload the authentication and authorization functions of this API onto the API Gateway layer. To do this securely, we would normally establish a trust relationship between the API Gateway and the API Provider. Unfortunately, timeapi.org is a public API that doesn‚Äôt take any steps to provide such a mechanism. So, we won‚Äôt be able to do that for this example. If we did control the API Provider, that trust relationship could be accomplished through a variety of mechanisms including, but not limited to:
The authentication step will be the validation of a JWT token (per the spec) including:
The Authorization step could be quite elaborate, but for our purposes, we will rely upon the OAuth2/JWT authorization mechanisms that are provided to us in the form of the audience (‚Äúaud‚Äù parameter). As long as the this field contains the expected string, it is accepted ‚Äî a very coarse grained authorization policy. The audience parameter in the JWT token, multiple roles described by its values, and a non-trivial mapping of API endpoints to roles provides the basis of a rich Role-Based Access Control (RBAC) policy. But, again, for our purposes, as long as the JWT token included with the request contains the expected audience value, the request will be considered authorized.
First, let‚Äôs construct a simple API Proxy that can proxy requests back to https://www.timeapi.org. This can be done with this tutorial.
The resulting API Proxy will look something close to the API Proxy in the image above. To access this API endpoint, one must call:
If your organization name is rcbjBlueMars (see upper right-hand corner), environment is called test (see upper right-hand corner), and the base path is ‚Äú/time‚Äù, then this would be:
Since Apigee automatically does a translation of the front-end path and backend path,
will be translated to:
To have this solution look a little more professional and have the DNS names and OAuth2 scope information match it, we will setup a DNS CNAME entry called rcbj0001-api.rcbj.net that points at rcbjbluemars-test.apigee.net. This particular domain is registered at godaddy.com. So, that is done through standard configuration mechanisms in its domain management application. Apigee Support would need to add this name to the Virtual Host configuration in Apigee Edge Public Cloud. I‚Äôm doing this in the community edition; so, this isn‚Äôt available, but in any of the paid-for versions of the platform, this can be done quite easily. Then, the API request could be sent to:
The response looks just like the original API call response above. The Apigee Trace (think of it as an API processing policy debugger) session for this request looks like the following:
Obviously, there isn‚Äôt much going on here at this point. We are going to change that.
Now, consider this presentation from the Apigee I Love APIs conference in October, 2015, describing the use of JWT with Apigee API Proxies. This presentation and accompanying code were created by Dino Chiesa who I did two webcasts with in 2016. The code that accompanies this presentation can be found here. Apigee doesn‚Äôt have out of the box support for JWT token generation or validation. But, this project contains custom Java Code that can be used to validate JWT tokens from a variety of sources (Google, Azure Active Directory, SalesForce, etc). Instructions for how to deploy the sample API Proxy are included on the GitHub repository; I‚Äôm not going to cover the build and deploy process for the sample proxy.
I extracted the conditional rule that can validate JWTs generated by Azure Active Directory. We are going to use AAD as the Identity Provider that generates JWT tokens. Azure Active Directory uses JWT as the OAuth2 access token, which works out well for our goals. The details of how an Azure AD tenant was configured to work with this tutorial can be found here.
If we add the ‚Äúparse + validate alg=RS256-ms‚Äù conditional rule to our sample proxy from above, we have something that looks similar too:
I added additional actions that extract the JWT token from Authorization header and, if valid, remove the Authorization header before forwarding the API request to the API Provider endpoint.
CORS has also been added to this project following the instuctions outlined here.
This API Proxy can be downloaded here.
The original example from the Apigee conference requires the APigee Edge enterprise pricing plan. To get this to run in a non-Enterprise Apigee organization, a couple of tweaks had to be made to the Java code. The source code and instructions for building the modified libraries are available in the github repository.
To run this in your own environment a couple of configuration changes must be made to the API Proxy including:
For an API Consumer, we will use the following script that runs a series of curl commands. The first curl command is the call to AAD to obtain an access token (JWT); the second curl command is the call to the API.
This script does not require any arguments. If it is called test-client.sh (as it is in the github repository), then one would simply run:
If you are trying to get this to work in your own environment and your own AAD tenant, then the CLIENT_ID, USERNAME_, PASSWORD_, RESOURCE_URL, and TENANT_ID variables must be updated to match the details for your tenant.
The first curl command (OAuth2 call) response looks something like:
The access_token property is a JWT token. This can be copied directly into the Authorization HTTP Request Header (per RFC 6750) as ‚ÄúBearer JWT‚Ä¶‚Äù.
The payload of this JWT token looks like:
The details of what is in this token are explored in our last post.
An application such as a web application, SPA app, or Mobile App could cache the access token and refresh token. The access token can be used across API calls until it expires. The refresh token can be used to obtain a new access token ‚Äî more on this later. Eventually, the user would have to log into the app again; this would be a configurable set of parameters. Applications such as these would likely use an interactive login with the OAuth2 Authorization Code Authorization Grant or Implicit Authorization Grant.
The trace session associated with running the test script will look similar to the following:
The first policy (colored box) in the request is a Cache Lookup Policy. This looks in an Apigee Cache called signer-cert for the cached copy of the Azure Active Directory signer certificate included in the Federation Metadata for the AAD tenant that this API Proxy trusts. Since this isn‚Äôt the first time this API Proxy has been run in the past hour, the certificate is found and assigned to a flow variable called ‚Äúcached.ms.cert‚Äù. Since there was an entry in the cache, the next four policies are skipped (as indicated by the arced arrow in the upper, left-hand corner of each. Had the cached metadata not been found, the second policy (a Service Callout Policy) would have made a call out to:
The metadata document would be written to a flow variable called ‚ÄúmsCert‚Äù.
The third policy (which also didn‚Äôt run because the metadata was found in the cache) is a Javascript callout that removes the XML Declaration from the XML document stored in the ‚ÄúmsCert‚Äù flow variable. The fourth policy (also didn‚Äôt run this time) is an Extract Variable policy that reads the signer certificate value from the retrieved XML document via an XPath expression and would assign the certificate value to the ‚Äúms.certificate‚Äù flow variable. The fifth policy (ran this time) is a Populate Cache Policy that writes the ‚Äúms.certificate‚Äù flow variable to the signer-cert Apigee Cache. The sixth policy (also didn‚Äôt run this time) writes the value stored in the ‚Äúms.certificate‚Äù flow variable to the signer-cert cache so that it can be efficiently retrieved later. The seventh policy, which always runs, extracts the JWT token from the Authorization header and places it in ‚Äúauthn.jwt‚Äù flow variable. The eighth policy is a Java Callout that runs the code from the original GitHub project and validates the JWT token. The next policy is a Raise Fault Policy that only runs if an error condition flow variable was set in the Java Callout Policy that just ran ‚Äî any failure of JWT validation returns a 401. The final policy is a Javascript Callout that removes the Authorization header and the JWT token it contains.
The response processing only runs one Policy, which sets several CORS-related HTTP Response Headers.
Another way to test this API is to use Swagger Editor. Load the Time API Swagger document into the Swagger Editor by choosing File->Import File from the menu. Then, click the ‚ÄúChoose File‚Äù. Load the $REPOSITORY_HOME/swagger/swagger.json. Finally, click the Import button. You will see a screen similar to the following.
Click the ‚ÄúAuthentication‚Äù button.
Run the test-client.sh script again. Use the ASSERTION variable value as the access token value that is put in this pop up‚Äôs field. Then, click the Authenticate button.
Now, scroll down to the ‚ÄúTry this operation‚Äù button for the GET on /{timezone}/{when}.json. The screen expands to show fields to put values in for all varibles.
Make sure the check box next to ‚Äúoauth2ResourcePasswordGrant‚Äù is checked. In the ‚Äútimezone‚Äù field, add a mainland US timezone (as defined in the swagger interface definition) such as ‚Äúpst‚Äù. In the ‚Äúwhen‚Äù field, add a natural language time description as defined by the chronic documentation (such as ‚Äúnow‚Äù). The request should look similar to the following at this point.
Click the ‚ÄúSend Request‚Äù button. The response will look something like the following.
With that, we have covered this JWT-based authentication and authorization example end-to-end. As always, please leave comments and questions. For more from Robert and the Levvel team, visit our website.
Ask us how we can help transform your business.
We help big companies innovate like startups, & small‚Ä¶
69 
1
69¬†claps
69 
1
Written by
My focus within Information Technology is API Management, Integration, and Identity‚Äìespecially where these three intersect.
We help big companies innovate like startups, & small businesses scale like industry leaders. Ask us how we can help you transform your business.
Written by
My focus within Information Technology is API Management, Integration, and Identity‚Äìespecially where these three intersect.
We help big companies innovate like startups, & small businesses scale like industry leaders. Ask us how we can help you transform your business.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/extracting-form-data-to-json-excel-pandas-with-azure-form-recognizer-160488a2d11e?source=search_post---------287,"There are currently no responses for this story.
Be the first to respond.
TLDR; This post shows how to extract data from table images for pandas and Excel using the Azure Form Recognizer Service in Python.
The Azure Form Recognizer is a Cognitive Service that uses machine learning technology to identify and extract text, key/value pairs and table data from form documents. It ingests text from forms and outputs structured data that includes the relationships in the original file.
There is a free tier of the service which provides up to 500 call a month which is more than enough to run this demo.
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
The Azure Form Recognition Service can be consumed using a REST API or the following code in python.
docs.microsoft.com
However this code returns the result in JSON format with a lot of additional information not relevant to the actual processing of the form data.
The following code sample will show you how to reformat this JSON code with python into a pandas DataFrame so it can processed in a traditional data science pipeline or even exported to Excel.
We will use the pre-trained receipt model for this tutorial. End to End Code Can be Found in the following gist.
Once your data is an pandas DataFrame it can be converted to CSV to process with Excel in just one line of code.
Hope you enjoyed this demo of the power of the Azure Form Recognizer Cognitive Service. Check out the next steps to see how to train your own custom models and then use this code to extract them to pandas and or Excel.
docs.microsoft.com
docs.microsoft.com
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
38 
1
38¬†claps
38 
1
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/azure-sentinel-helping-your-soc-with-investigation-and-hunting-ba1a8442deaa?source=search_post---------288,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Jul 15, 2019¬∑5 min read
Separating the wheat from the chaff in cybersecurity is hard. Often you find yourself handling enormous volumes of events. And more than not, data quality is an issue. False positives often lead to triage fatigue.
Being able to do triage quickly is important. The time window to respond when under attack is short, advanced adversaries typically only need hours to gain access, elevate privileges and exfiltrate data.
Azure Sentinel just released their Investigation feature (as a preview). But what is the difference between investigating and hunting? And how exactly does Azure Sentinel help my SOC with both?
How does a SOC operate?
Before we dive into Azure Sentinel‚Äôs new investigation features, let‚Äôs rewind and first look at how a Security Operations Center (SOC) operates. Most SOC‚Äôs will have two critical functions: (1) setting up and maintaining security monitoring and related tooling. And (2) find suspicious or malicious activity by analyzing alerts.
For the latter, typically a SOC will have a 3-tier model. Tier 1 is where security analysts do the triaging; they review the latest alerts to determine relevancy and urgency. Alerts that signal an incident will be forwarded to Tier 2 analysts. The second tier reviews those cases and uses threat intelligence (IOC‚Äôs, etc.) to identify affected systems and the scope of the attack. Tier 3 are often the most experience people on the team, mostly referred to as Threat Hunters. They explore the environment to identify stealthy threats and conduct continuous vulnerability tests.
In most SOC‚Äôs the Tier 2 folks will do the investigations, and Tier 3 folks will do the hunting. And while there is no clear line, most people refer to the term Investigation when they are following up with a (by Tier 1 forwarded) case. Robert M. Lee has a great quote on this:
‚ÄúThreat hunting exists where automation ends‚Äù. Threat hunting is large manually, performed by SOC analysts, trying to find a ‚Äòneedle in the haystack‚Äô. And in the case of cybersecurity, that haystack is a pile of ‚Äòsignals‚Äô.
Investigation UI in Azure Sentinel
Microsoft just released an investigation experience in Azure Sentinel as a preview. Before, you would find an investigation UI in Azure Security Center, but as Azure Sentinel is becoming the central place to aggregate security the investigations will likely happen from there, and therefore Microsoft is deprecating the investigation UI in ASC.
To get to the new Investigation Experience in Azure Sentinel you will need navigate to a Case. For every Case you‚Äôll find two buttons: View Full Details and Investigate. Previously, the Investigate button would show a placeholder page of Coming Soon, but today you‚Äôll be launched into a new window.
The investigation experience window has three sections: the top will show the Case name, and other Case details. On the right you‚Äôll find four buttons: Timeline, Info, Entities and Help. The main window will show all the entities related to this Case in a graph style manner.
Clicking an entity will show the details, hovering over the entity will give you some quick actions, for instance these: Related Alerts, Hosts the Account Failed on, Hosts which the Account Logged On to.
Clicking these will show these results as extra entities in your graph, expanding on your search.
There is also the opportunity to dive into the raw results, pivoting from the graph to a KQL query window:
The timeline button on the right allows you to ‚Äòbookmark‚Äô items/results during your investigation and have them readily available as information on this ‚Äònotebook‚Äô.
Entity mapping is important!
When you‚Äôre creating Alert Rules in Azure Sentinel, that will then trigger Cases when the criteria is met, you have the option to do Entity Mapping. From the underlying KQL query, you can pick any field and map it into either ‚ÄòAccount‚Äô, ‚ÄòHost‚Äô or ‚ÄòIP addresses.
This allows Azure Sentinel to recognize that data as such and provide the right Quick Investigation items, and more importantly link data/Cases together. More entities are coming soon, but for now these three are available.
OK, so what about hunting and supporting Tier 3 SOC analysts?
I wrote about that in a previous blog called ‚ÄòThreat Hunting in the cloud with Azure Notebooks‚Äô. I talked about how you could take the ‚ÄòKqlmagic‚Äô extension that Microsoft wrote, and use KQL queries in Jupyter notebooks to hunt for malicious actors.
Something I did not mention in that blog is the built-in support for Hunting that Azure Sentinel has. Microsoft provides you with pre-compiled KQL queries to find known indicators in your environment. These are available in the Hunting node under the Threat Management section and are mapped back to the Tactics of the MITRE ATT&CK framework. You can add your own favorite Hunting to the workspace as well.
Directly from this section of Azure Sentinel you can run the query by using the Run Query button.
When running the query, you can expand (one of) the results and use the [..] button to access the Bookmark function. This saves results and allows you to relate them to an ongoing campaign by using the Tags field.
You‚Äôll find these Saved Queries in the Hunting section of Azure Sentinel under the Bookmarks tab.
Conclusion
The new Investigation Experience in Azure Sentinel is an easy way to start your investigations, for instance by your Tier-1 SOC analysts. It visualizes your Case in a graph, which makes it easy to find connections between data points.
It will replace the investigation functionality Azure Security Center, but compliments the fact that your Tier-2 an Tier-2 SOC analysts can use their favorite investigation and hunting tools in combination with Azure Sentinel, like for instance Jupyter Notebooks.
Happy investigating!
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
79 
1
79¬†claps
79 
1
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-host-a-react-static-website-on-azure-438e0a915295?source=search_post---------289,"There are currently no responses for this story.
Be the first to respond.
There are a number of ways you can build a website with React such as Java with React, NodeJS with React, NGINX serving React, etc. For the single-page applications, all you need to do is to load the initial index.html. Once you load the index.html the React library kicks in and do the rest of the job like loading‚Ä¶
"
https://koukia.ca/azure-documentdb-vs-sql-azure-performance-comparison-revised-178a90038146?source=search_post---------290,"A while ago I wrote some code to compare the performance of Azure DocumentDb and SQL Azure and the conclusion was:
"
https://koukia.ca/managed-kubernetes-on-azure-aks-8514ac0cd8aa?source=search_post---------291,"In a previous post, I showed you how to create a Kubernetes Cluster, on Azure Container Service (Considered an IaaS offering, since you manage all the resources) and as you could see , it was pretty straightforward.
Today I will walk you through a new service on Azure called Managed Kubernetes or AKS, and this is even easier.
"
https://medium.com/@renatogroffe/azure-functions-dicas-e-truques-no-desenvolvimento-serverless-parte-1-77f033a95576?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Dec 2, 2019¬∑4 min read
Neste novo artigo inicio uma s√©rie com diversas dicas e truques envolvendo o desenvolvimento de aplica√ß√µes serverless baseadas em Azure Functions. Sempre que poss√≠vel darei andamento a esta iniciativa, com novos conte√∫dos e orienta√ß√µes √∫teis.
E como Azure Functions √© o assunto deste post, deixo aqui um link com todos os conte√∫dos gratuitos (artigos, v√≠deos, projetos de exemplo) que venho criando sobre esta tecnologia:
Serverless + Azure Functions: Guia de Refer√™ncia
Aproveito este espa√ßo tamb√©m para um convite.
Que tal aprender mais sobre Serverless e Azure Functions em um treinamento inteiramente pr√°tico e com um pre√ßo super camarada j√° que estamos em Black FÃµrÃµiÃµdÃµaÃµyÃµ Week? Acesse o link a seguir para obter um desconto de 25% na primeira edi√ß√£o do Azure na Pr√°tica em 2020, no dia 18/01 (s√°bado) em S√£o Paulo-SP: http://bit.ly/black-week-blog-groffe
Corra, porque este pre√ßo promocional √© por um tempo bem limitado!
Independente do sistema operacional, ferramenta de implementa√ß√£o ou linguagem escolhidos, a configura√ß√£o de um ambiente local de desenvolvimento baseado em Azure Functions depender√° da instala√ß√£o de um conjunto de ferramentas conhecido como Azure Function Core Tools. Maiores informa√ß√µes podem ser encontradas no link a seguir:
Work with Azure Functions Core Tools
O pr√≥prio Visual Studio Code emitir√° alertas para a atualiza√ß√£o deste complemento sempre que uma nova vers√£o estiver dispon√≠vel:
Atualmente temos como ferramentas para desenvolvimento de solu√ß√µes com Azure Functions o pr√≥prio Portal do Azure (alternativa mais limitada), IDEs como Visual Studio (atualmente na vers√£o 2019) e Visual Studio for Mac, al√©m do Visual Studio Code. Exceto pelo Portal do Azure, todas essas alternativas dependem da instala√ß√£o das Azure Functions Core Tools. √â justamente este complemento que fornecer√° todo o suporte para a implementa√ß√£o de Function Apps (projetos contendo uma ou mais Azure Functions), incluindo a execu√ß√£o local de aplica√ß√µes deste tipo.
No caso espec√≠fico do Visual Studio Code, ser√° necess√°ria tamb√©m a instala√ß√£o de uma extens√£o que viabilizar√° o desenvolvimento de solu√ß√µes baseadas em Azure Fuctions:
Na imagem a seguir temos um exemplo envolvendo o debugging de uma Function App no Visual Studio Code em Windows:
Para saber mais sobre o desenvolvimento serverless com Azure Functions e o Visual Studio Code acesse os seguintes links:
.NET Core + Serverless: implementando jobs com Azure Functions e o VS Code
.NET Core + Serverless: publicando uma Azure Function via VS Code
J√° na pr√≥xima imagem podemos observar a execu√ß√£o e debugging de uma Function App a partir do Visual Studio 2019:
Suporte a m√∫ltiplas linguagens
No momento da publica√ß√£o deste artigo (Dezembro/2019) temos como op√ß√µes para a implementa√ß√£o de Azure Functions as seguintes linguagens/plataformas:
Trata-se de uma ampla gama de alternativas para a cria√ß√£o de solu√ß√µes serverless no Microsoft Azure, al√©m de permitir at√© mesmo que SysAdmins habituados ao uso de PowerShell possam se beneficiar das vantagens oferecidas pelas Azure Functions.
Conforme demonstrado em prints nas se√ß√µes anteriores, conseguimos facilmente trabalhar na implementa√ß√£o de Azure Functions em ambientes como Windows e Linux. O desenvolvimento a partir de m√°quinas com macOs tamb√©m √© uma possibilidade, com isto acontecendo atrav√©s do Visual Studio for Mac ou do Visual Studio Code.
Na pr√≥xima imagem √© poss√≠vel observar o debugging de uma Azure Function no Visual Studio Code, com isto acontecendo em uma m√°quina com o Ubuntu Desktop 18.04 instalado:
No v√≠deo a seguir do canal Coding Night temos um exemplo de desenvolvimento com Azure Functions no macOS, utilizando para isto o Visual Studio for Mac:
Uma vez instaladas, as Azure Function Core Tools permitir√£o que executemos Function Apps via linha de comando (Bash ou PowerShell). Isso √© poss√≠vel atrav√©s da instru√ß√£o:
Podemos assim efetuar testes sem a necessidade de abrir uma aplica√ß√£o numa IDE ou no VS Code. √â o que demonstram as pr√≥ximas imagens, com a inicializa√ß√£o de um projeto de testes a partir do PowerShell no Windows 10 Professional:
A seguir temos um exemplo similar em Linux (Ubuntu Desktop 18.04), com a execu√ß√£o via Terminal de uma Function App:
Azure Functions documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
29 
29¬†claps
29 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/aplica%C3%A7%C3%B5es-web-escal%C3%A1veis-no-azure-app-service-docker-e-kubernetes-global-azure-bootcamp-2019-ead08d12e278?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 17, 2019¬∑3 min read
No dia 27/04/2019 (s√°bado) aconteceu a edi√ß√£o local do Global Azure Bootcamp no audit√≥rio da Venturus em Campinas-SP. Esta foi uma iniciativa promovida pelas comunidades Campinas .NET, .NET SP, Azure Talks, DevOps Professionals e Open Source SP (das quais tamb√©m sou um dos Coordenadores).
Este post √© um registro da apresenta√ß√£o que realizei sobre o uso de tecnologias como App Service, Docker e Kubernetes para a implementa√ß√£o de aplica√ß√µes Web escal√°veis no Microsoft Azure. Esta palestra foi realizada para um p√∫blico estimado em 70 pessoas.
Os slides que utilizei foram disponibilizados no SlideShare:
As demonstra√ß√µes realizadas tomaram como base o seguinte projeto:
ASP.NET Core + Docker + Alpine Linux + Docker Hub + Kubernetes + AKS (Azure Kubernetes Service)
A implementa√ß√£o deste projeto, bem como a cria√ß√£o de objetos para deployment em um cluster Kubernetes foram abordadas por mim nos seguintes artigos:
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem ‚Äî parte 1
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem ‚Äî parte 2
O post a seguir traz estes materiais e ainda outras refer√™ncias sobre a utiliza√ß√£o do Azure Kubernetes Service (AKS):
Azure Kubernetes Services - AKS: refer√™ncias gratuitas e dicas para solu√ß√£o de problemas comuns
O uso do Azure App Service com projetos ASP.NET Core foi tamb√©m abordado nos seguintes artigos:
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
ASP.NET Core: dicas √∫teis para o dia a dia de um Desenvolvedor - Parte 4
Para conhecer mais sobre o Azure Web App for Containers acesse o post a seguir:
ASP.NET Core + Azure Web App for Containers: escalando uma API REST com containers Docker
J√° demonstrei tamb√©m o deployment de aplica√ß√µes empregando containers Docker na nuvem no seguinte v√≠deo do Canal .NET, em que cobri a utiliza√ß√£o do servi√ßo conhecido como Azure Web App for Containers:
O uso do Kubernetes foi tema ainda de 2 eventos online gratuitos do Canal .NET, com a grava√ß√£o dos mesmos estando dispon√≠vel no YouTube e abordando desde os primeiros passos ao deployment automatizado via Azure DevOps (antigo VSTS):
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
115 
115¬†
115 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/ey-ireland/introduction-to-azure-machine-learning-service-87135cfbf78b?source=search_post---------294,"There are currently no responses for this story.
Be the first to respond.
In September of 2018, Microsoft launched Azure Machine Learning Service that helps data scientists and machine learning engineers build end-to-end machine learning pipelines in Azure without worrying too much about dev-ops behind training, testing and deploying your model. Azure has a number of offerings in this space such as Azure Notebooks, Azure Machine Learning Studio and Azure Batch AI. Comparison of the offerings is available here.
This article will focus on the newest offering by Azure, and it will cover the basic concepts including an example of training your own machine learning model. Following is the breakdown of the article:
In order to work with AML, you need to be aware of the following concepts:
Workspace: It is a centralised place for all your artefacts (experiments, runs, deployments, images). It helps keep a history of all your work including registering machine learning models that can be used for prediction. To create a workspace:
When you create a workspace, Azure will create container registry, storage, application insights and key vault for you. This enables you to store docker images in the container registry, store data in storage, monitor model performance in application insights and store sensitive information in key vault including compute target keys.
Experiment: Within the workspace, you can define experiments that contain individual training runs. Each training run you perform will associate itself with an experiment and a workspace. Defining logical high level experiments will help you monitor various training runs and their outputs.
Model: This is the heart of any machine learning process. AML provides the ability to register (version) models produced during each training run. Each registered model is physically stored in the storage provided while creating the workspace. This enables machine learning practitioners to test and deploy variety of versioned models without having to store them locally. Models produced from any of the machine learning libraries (scikit-learn, TensorFlow, PyTorch, etc) can be registered.
Image: Docker images containing the model and the prediction (scoring) script can be created once the model is tested. AML provides the ability to create these images and version them, similar to model versioning. This enables multiple Docker images to be created and deployed with different versions of the model.
Deployment: The Docker images created can be deployed using Azure Container Instances. This is where the true power of AML lies. Automatically creating a load-balanced HTTP endpoint without having to worry about underlying infrastructure or deployment configuration helps machine learning practitioners focus on training and evaluating their model. AML helps collect application insights to monitor the performance of your deployment.
In the above section, we went through the basic concepts of AML. It is important to understand these concepts before one can start working with this service. In this section, we will focus on setting up our AML environment.
Setup VS-Code: My preferred IDE for working with AML is VS-Code (referred to VSC from now on). This is motivated from the fact that there is an Azure extension available in VSC that enables seamless connection to the AML workspace helping us get a visual view of our workspaces, experiments, models, images, and deployments.
Follow the steps below to setup VSC:
Fork and clone AML repo: In order to get quickly started with AML, I have created a GitHub repo with an example that will help you quickly train and deploy a simple sklearn regression model. Please fork and clone the repository before continuing.
Setup conda environment: In the repo, there is a conda environment file available (environment.yml). As a good practice, I suggest creating a new virtual environment using conda. This will isolate your dependancies for the AML pipeline without breaking dependencies for your existing projects. To create a new conda environment with all the required packages:
This will create a new conda environment called myenv. Note: You can change the name of the environment by editing environment.yml file. Now you are ready to train your first model using the AML pipeline.
To configure the pipeline, a configuration file is provided in the repo. Let us go over each section in the configuration file and its corresponding module in the pipeline.
Part1 ‚Äî Workspace Configuration (ws_config):
In order to configure your workspace, a separate configuration file is produced under aml_config/config.json. This can be done by entering the above parameters in the ws_config section of the ml-config.ini file and running, (Note: you can run this via VSC but make sure to activate the conda environment first)
Note: This step needs to be done once for each project you create in the pipeline unless you change your resource group and/or workspace.
Part2 ‚Äî Training (train):
As an example, my configuration file looks like this:
train.py will contain your training code and an example of a simple sklearn regression model training is provided in the repository. main_train.py is the driver for training. The code is well commented and self explanatory, but I will go through the key parts of the code.
Now you can run your training by and it should produce the following output:
Part3 ‚Äî Create Docker Image (docker):
As an example, my configuration file looks like this:
The above downloaded model can be evaluated and it can be used to create a Docker image. score.py script is used for prediction and an example is provided in the repository. create_docker.py is the driver for creating the docker image and is well commented. I will go through the key parts of the code.
Now you can create the docker image by running,
You can now see the Docker image being created under Images section of your workspace in the VSC IDE.
Part4‚Äî Create Deployment (deploy):
As an example, my configuration file looks like this:
To deploy the above created Docker image, run:
I will go through the important parts of the create_deployment.py script,
The output will look something like this,
After deploying the service, you can access the HTTP endpoint from the properties.json file of the deployment.
Part5‚Äî Test Deployment:
I am using Postman to test the service with the following input for the above created sklearn regression service.
To recap,
I encourage you use this pipeline for your machine learning use-cases including training and deploying deep learning models.
Disclaimer: The views represented in this article our those of the author and not of EY Ireland.
For more information: https://www.ey.com/gl/en/issues/business-environment/ey-global-innovation
This publication is contributed to by EY Ireland community‚Ä¶
99 
99¬†claps
99 
This publication is contributed to by EY Ireland community of analytics and AI professionals. In this publication we will bring you coverage of latest developments in the field of AI, Analytics and RPA.
Written by
Data Science Manager @EY and Chief Data Scientist @IdeaChain; A hub for ideas, discussion and collaboration -http://ideacha.in
This publication is contributed to by EY Ireland community of analytics and AI professionals. In this publication we will bring you coverage of latest developments in the field of AI, Analytics and RPA.
"
https://medium.com/@olafusimichael/easy-steps-to-creating-and-deploying-a-predictive-model-using-azure-machine-learning-studio-d35adb018543?source=search_post---------295,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Olafusi
Apr 22, 2018¬∑7 min read
Yesterday, I demoed how in 10 mins you can create a predictive model, deploy it as a web service and test it without having to install anything or pay any money. That is the awesomeness made possible by Microsoft with the super easy to use Azure Machine Learning Studio. The yesterday event was the Lagos edition of the Global Azure Bootcamp held at Microsoft office, Lagos.  Participants were able to follow along, created and deployed their own predictive models too. In today‚Äôs post I will be guiding you with easy steps to follow on how you too can in a few minutes create and deploy a predictive model cost-free with Azure Machine Learning Studio.  Step 1 Download the sample data we would use: Bank Marketing data from UCI Machine Learning Repository. If you download from UCI Machine Learning Repository directly, then it is the bank-additional-full.csv file in the zip file you end up with. Then you have to make sure that you break the data into separate columns rather than leave them comma separated, using Excel‚Äôs Text to Columns. For you ease, I have shared a cleaned version you can directly use without any extra work by you: Bank Marketing data download  The sample data is a marketing campaign data of a Portuguese bank from May 2008 to November 2010 recording the details of prospects reached via phone calls and whether they eventually took up the service the bank was trying to sell them.
Below is the explanation of the different fields in the data records.  Input variables: # bank client data: 1 ‚Äî age (numeric) 2 ‚Äî job : type of job (categorical: ‚Äòadmin.‚Äô,‚Äôblue-collar‚Äô,‚Äôentrepreneur‚Äô,‚Äôhousemaid‚Äô,‚Äômanagement‚Äô,‚Äôretired‚Äô,‚Äôself-employed‚Äô,‚Äôservices‚Äô,‚Äôstudent‚Äô,‚Äôtechnician‚Äô,‚Äôunemployed‚Äô,‚Äôunknown‚Äô) 3 ‚Äî marital : marital status (categorical: ‚Äòdivorced‚Äô,‚Äômarried‚Äô,‚Äôsingle‚Äô,‚Äôunknown‚Äô; note: ‚Äòdivorced‚Äô means divorced or widowed) 4 ‚Äî education (categorical: ‚Äòbasic.4y‚Äô,‚Äôbasic.6y‚Äô,‚Äôbasic.9y‚Äô,‚Äôhigh.school‚Äô,‚Äôilliterate‚Äô,‚Äôprofessional.course‚Äô,‚Äôuniversity.degree‚Äô,‚Äôunknown‚Äô) 5 ‚Äî default: has credit in default? (categorical: ‚Äòno‚Äô,‚Äôyes‚Äô,‚Äôunknown‚Äô) 6 ‚Äî housing: has housing loan? (categorical: ‚Äòno‚Äô,‚Äôyes‚Äô,‚Äôunknown‚Äô) 7 ‚Äî loan: has personal loan? (categorical: ‚Äòno‚Äô,‚Äôyes‚Äô,‚Äôunknown‚Äô) # related with the last contact of the current campaign: 8 ‚Äî contact: contact communication type (categorical: ‚Äòcellular‚Äô,‚Äôtelephone‚Äô)  9 ‚Äî month: last contact month of year (categorical: ‚Äòjan‚Äô, ‚Äòfeb‚Äô, ‚Äòmar‚Äô, ‚Ä¶, ‚Äònov‚Äô, ‚Äòdec‚Äô) 10 ‚Äî day_of_week: last contact day of the week (categorical: ‚Äòmon‚Äô,‚Äôtue‚Äô,‚Äôwed‚Äô,‚Äôthu‚Äô,‚Äôfri‚Äô) 11 ‚Äî duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=‚Äôno‚Äô). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. # other attributes: 12 ‚Äî campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) 13 ‚Äî pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) 14 ‚Äî previous: number of contacts performed before this campaign and for this client (numeric) 15 ‚Äî poutcome: outcome of the previous marketing campaign (categorical: ‚Äòfailure‚Äô,‚Äônonexistent‚Äô,‚Äôsuccess‚Äô) # social and economic context attributes 16 ‚Äî emp.var.rate: employment variation rate ‚Äî quarterly indicator (numeric) 17 ‚Äî cons.price.idx: consumer price index ‚Äî monthly indicator (numeric)  18 ‚Äî cons.conf.idx: consumer confidence index ‚Äî monthly indicator (numeric)  19 ‚Äî euribor3m: euribor 3 month rate ‚Äî daily indicator (numeric) 20 ‚Äî nr.employed: number of employees ‚Äî quarterly indicator (numeric)  Output variable (desired target): 21 ‚Äî y ‚Äî has the client subscribed a term deposit? (binary: ‚Äòyes‚Äô,‚Äôno‚Äô)  Step 2 Sign up for Azure ML studio. It is easy and free: https://studio.azureml.net
Step 3 Upload the Bank Marketing dataset. From Datasets section on the left menu pane, click New at the bottom left.
Step 4 Create a new experiment. From Experiments section on the left menu pane, click New at the bottom left. Choose a blank experiment, as we are creating ours from scratch.
Step 5 Now we start dragging the tasks we want to carry out into the Experiment workspace, after renaming the Experiment.
Drag in the dataset we uploaded, it is in the Saved Dataset section on the left.
Next, we need to isolate the fields that would be useful for our predictive model. If you look at the description of all the fields in the dataset, it is obvious that some are not practically useful in creating a prediction of whether a prospect will take up the marketed service or not. Example is the length of the call, there is no way you would know that until the end of the call ‚Äî so not useful for profiling who to call (targeted marketing). By my thinking, the fields I that would be of real world use in creating an actionable predictive model are ‚Äî age, job, marital, education, default, housing, loan.  Drag Select Columns in Dataset in the Manipulation subsection of Data Transformation section. Connect the dataset previously dragged in to the select columns task. Then click on Launch column selector, and select the columns needed (including the outcome we want to predict, so as to be able to train the model).
Next, split the data into training set and testing set for building our predictive model. Drag Split Data, connect to the select columns task and on the settings pane on the right, set the training set to 0.75 (75%) of the entire dataset.
Drag in the model algorithm to use. It‚Äôs under the Initialize Model. I chose to use the Two-Class Decision Forest. In the end, you would evaluate the model to see if it fits well or you should try another algorithm.
Drag in Train Model. Connect to both the already dragged in algorithm and the left side of the Split Data (training set). Select the outcome to predict.
Drag in Score Model. Connect to the Train Model and the testing set of the Split Data.
Lastly, drag in Evaluate Model. Connect to Score Model.
Now run the entire experiment.
Wait for it to finish running.
Right click on Evaluate Model and visualize the evaluation result to see the fitness/accuracy of the algorithm.
If you are okay with the fit, then what‚Äôs left is to publish. Otherwise, you can change the algorithm, re-run and re-evaluate the fit.
Step 6 Now you set up the model as a web service that can be deployed online.
Change the input connector to point to the Score Model.
Also, remove the predicted column from the Selected Column as it was only needed for training the model.
Now re-run and deploy as web service.
You are presented with the web service details to use for integrating with any app or online tool. You can even test the API directly.
And that‚Äôs how you create and deploy a predictive model in Azure Machine Learning Studio without installing anything on your computer and without paying a cent/kobo.  Enjoy!
Originally published at www.olafusimichael.com.
Founder of www.urbizedge.com www.msexcelclub.com, www.nigeriamarketdata.com and www.investmentng.com I also blog daily at www.olafusimichael.com
114 
114¬†
114 
Founder of www.urbizedge.com www.msexcelclub.com, www.nigeriamarketdata.com and www.investmentng.com I also blog daily at www.olafusimichael.com
"
https://medium.com/@jthake/lets-build-the-office-365-and-azure-community-on-medium-b38c0c5ca9b5?source=search_post---------296,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeremy Thake
Nov 22, 2016¬∑1 min read
So I‚Äôm pretty amped about medium right now after exploring it this evening. I‚Äôve followed it for a while and the latest improvements to the platform are incredible. @ev has done a great job of explaining it all himself.
Looking at the current tag followers of Microsoft (7757), Office 365 (381) and Azure (448)‚Ä¶there is certainly work to do.
I suspect there are a few reasons bloggers haven‚Äôt engaged here.
From reading authors already on medium, the benefit here is the network it provides. It‚Äôs simply incredible how engaged people are on here and how quickly posts spread. Once the tags for the Office 365 and Azure community grow, the value will be enormous and it won‚Äôt take long.
I built both SharePoint dev wiki.com and NothingButSharePoint.com (still gets 60K views a month on legacy content) communities with amazing friends before!
I‚Äôll be rounding up the troops to see what we can start here‚Ä¶
Microsoft Graph Team, Senior Program Manager at Microsoft.
21 
5
21¬†
21 
5
Microsoft Graph Team, Senior Program Manager at Microsoft.
"
https://medium.com/@tsuyoshiushio/how-to-pass-variables-with-pipeline-trigger-in-azure-pipeline-5771c5f18f91?source=search_post---------297,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jan 11, 2020¬∑2 min read
Pipeline triggers are introduced. It enables one pipeline is completed then subsequent pipeline works. Then, how to pass the variables between two?
If you want to execute subsequent pipeline automatically, all you need is to add this section on your pipeline yaml. It shows that when the Parent.CI completed, this pipeline start working. The point is trigger: none Azure Pipeline seems trigger: master by default. So if you didn‚Äôt add trigger: none and you commit something to master branch, it automatically start this pipeline. Also, pipeline triggers also triggers this pipeline after the Parent.CI complete.
docs.microsoft.com
Then how to pass the variables from Parent to Child? We have no way to directly pass the variables. However, we can pass it through artifact.
Look at this example. It is simply save environment as file. We can choose the format, however, I save it as logging command.
Child.CI
The child pipeline echo the file. Then the variables are restored. You need to fill ` <YOUR_PROJECT_ID_HERE>` section. However, if you use editor on the Azure Pipeline, you can choose a Project and a Pipeline as a drop down list.
Senior Software Engineer ‚Äî Microsoft
90 
1
90¬†
90 
1
Senior Software Engineer ‚Äî Microsoft
"
https://medium.com/@gmusumeci/how-to-import-an-existing-azure-resource-in-terraform-6d585f93ea02?source=search_post---------298,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 19, 2020¬∑3 min read
In this story, we will learn how to import an existing Azure Resource in Terraform.
This allows us to use resources that we have created by some other means and bring it under Terraform management.
Update February 27, 2020: This procedure is valid for both AzureRM v1.x and AzureRM v2.x.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-deploy-an-azure-app-service-using-terraform-33f69b72e099?source=search_post---------299,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Jun 26, 2020¬∑3 min read
Azure App Service Web Apps is a PaaS (Platform as a Service) platform service that lets us quickly build, deploy, and scale enterprise-grade web, mobile, and API apps.
We can focus on the application development and Azure App Service will take care of the infrastructure required, and automatically scale our apps.
"
https://medium.com/@aallan/the-azure-sphere-starter-kit-fa2dc546eac7?source=search_post---------300,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alasdair Allan
Jul 17, 2019¬∑3 min read
Last year, Microsoft unveiled Azure Sphere ‚Äî an end-to-end solution for securing microcontroller based smart things. Alongside the announcement was hardware support for boards built around the MT3620 microcontroller.
Introduced earlier in the year at CES in Las Vegas, the Azure Sphere Starter Kit is based around the Avnet Azure Sphere MT3620 Module, and if you‚Äôve been thinking about taking a look, it might be the right time. That‚Äôs because Avnet and Microsoft just launched a contest that not only might bag you free hardware, but a share of $35,000 in prizes.
The starter kit is built upon the Azure Sphere module, which makes use of the MT3620 processor, a single-core Arm Cortex-A7 processor running at 500 MHz with 4MB RAM, as well as a dual-core Arm Cortex-M4F real-time core running at 200 MHz with 64KB RAM, and support for dual-band 802.11 a/b/g/n wireless. The 33 √ó 22 mm module is production-ready, both FCC and CE certified, and comes in two versions either with an on-board chip antenna or one with an external U.FL connector. The starter kit is based around the on-board chip version of the module.
The module has 3√ó ISU interfaces pre-configured for UART, SPI, I2C, along with 3√ó 12-bit ADC inputs (or 3 GPIOs) and 9√ó PWM outputs (or up to an additional 24 GPIOs). There is also support for an RTC.
The carrier board supports two MikroE Click board expansion sockets, and a Grove System expansion connector, and has a variety of on-board sensors including a 3-axis accelerometer and gyro, an ambient light sensor, as well as temperature and pressure sensors. There are also two user programmable buttons, and footprints to support a 128√ó64 pixel OLED display, and both +5V and VBAT supplies. Power and data are connection to the board are normally provided via a micro USB connector.
The Azure Sphere Starter Kit costs $75 plus $8 for ground shipping, or $12 for two-day air delivery. However, if you register to take part in the Secure Everything with Azure challenge, you can pick up one of 20,000 available starter kits for free.
Your project for the contest should integrate a new or existing Internet of Things edge device with the Azure Sphere, or secure a consumer electronics project. It should also show innovation around smart retail, factory solutions, buildings or home automation, or around renewables and energy solutions.
The focus of the Azure Sphere is to securely connect edge devices to the cloud, so to be eligible for the top prizes your project should be able to consistently stay online and connected for at least 15 consecutive days.
The top three competitors will win a Microsoft Hololens 2, while the next seven competitors will pick up a Surface Laptop 2. There are even brand new Raspberry Pi 4 boards for the next 500 entrants. Judging criteria for projects will focus on documentation, details of the Bill of Materials, the availability of schematics and code, and creativity. While your project doesn‚Äôt have to be original, it should be a creative take on the idea.
Applications for free hardware run all month, with recipients announced on the August 8th. Submissions close on September 29th, with the winners revealed on October 17th.
Scientist, Author, Hacker, Maker, and Journalist.
90 
2
90¬†claps
90 
2
Scientist, Author, Hacker, Maker, and Journalist.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-write-serverless-python-rest-api-with-azure-functions-504c0113c1c8?source=search_post---------301,"There are currently no responses for this story.
Be the first to respond.
An Azure Function is a simple way of running small pieces of code in the cloud. You don‚Äôt have to worry about the infrastructure required to host that code. You can write the Function in C#, Java, JavaScript, PowerShell, Python, or any of the languages that are listed in the Supported‚Ä¶
"
https://koukia.ca/deploying-docker-containers-to-azure-app-services-672d587c5c39?source=search_post---------302,"When you containerize your application and want to use Azure to host it, there are 3 main options out there right now that you can use. And each one comes with its own features and level of management and administration.
Different options to host Containers in Azure are:
"
https://towardsdatascience.com/how-to-connect-azure-data-factory-to-an-azure-sql-database-using-a-private-endpoint-3e46984bec5e?source=search_post---------303,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adam Prescott
Jan 21, 2021¬∑4 min read
Azure Data Factory (ADF) is great for extracting data from multiple sources, the most obvious of which may be Azure SQL. However, Azure SQL has a security option to deny public network access, which, if enabled, will prevent ADF from connecting without extra steps.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-difference-between-azure-service-bus-queues-and-topics-comparison-azure-servicebus-queue-vs-topic-4cc97770b65?source=search_post---------304,"There are currently no responses for this story.
Be the first to respond.
Comparison ‚Äî Azure Service Bus Queue vs Topic.
Queues and Topics are similar when a sender sends messages, but messages are processed differently by a receiver. A queue can have only one consumer, whereas a topic can have multiple subscribers.
Difference between Azure Storage Queue and Service Bus Queue
"
https://medium.com/bb-tutorials-and-thoughts/how-to-host-an-angular-static-website-on-azure-1257eed9d47e?source=search_post---------305,"There are currently no responses for this story.
Be the first to respond.
There are a number of ways you can build a website with Angular such as Java with Angular, NodeJS with Angular, NGINX serving Angular, etc. For the single-page applications, all you need to do is to load the initial index.html. Once you load the index.html the Angular framework kicks in and do the rest of the‚Ä¶
"
https://medium.com/bb-tutorials-and-thoughts/how-to-build-ci-cd-for-nodejs-azure-functions-using-azure-devops-4f32fb55a1a0?source=search_post---------306,"There are currently no responses for this story.
Be the first to respond.
An Azure Function is a simple way of running small pieces of code in the cloud. You don‚Äôt have to worry about the infrastructure required to host that code. You can write the Function in C#, Java, JavaScript, PowerShell, Python, or any of the languages that are listed in the‚Ä¶
"
https://medium.com/azure-na-pratica/10-servi%C3%A7os-do-azure-que-voc%C3%AA-precisa-conhecer-na-pr%C3%A1tica-conte%C3%BAdos-gratuitos-f6c18c3f3e68?source=search_post---------307,"There are currently no responses for this story.
Be the first to respond.
No dia 15/07/2020 (quarta-feira) aconteceu a primeira edi√ß√£o online do Azure Nights, um evento com foco em cloud computing e cobrindo a utiliza√ß√£o na pr√°tica de 10 servi√ßos que integram o Microsoft Azure.
Esta iniciativa foi uma parceria entre as comunidades TOTVS Developers, .NET SP, Azure na Pr√°tica, Azure Talks, DevOps Professionals e SampaDevs, contando com um excelente p√∫blico: pico de 174 pessoas online durante a transmiss√£o! Deixamos aqui nosso muito obrigado ao John Calistro (TOTVS Developers) e ao Jackson Feij√≥ (Microsoft) pelo apoio para que realiz√°ssemos a live. E tamb√©m agradecemos ao Fabricio Veronez (Coders in Rio, CNCF Speaker), que disponibilizou seu tempo com uma das apresenta√ß√µes.
Foram organizadores deste evento:
Tivemos ao longo da live as seguintes apresenta√ß√µes:
A grava√ß√£o est√° dispon√≠vel no YouTube e pode ser assistida gratuitamente:
Os slides j√° se encontram tamb√©m no SlideShare:
Caso queira conhecer mais sobre o Microsoft Azure de forma gratuita acesse os posts a seguir, no qual est√£o links com as grava√ß√µes de diversas palestras realizadas durante a edi√ß√£o 2020 do Azure Tech Nights (evento online promovido pelo Canal .NET):
Azure Tech Nights 2020: saiba como foi ‚Äî V√≠deos Gratuitos
E as grava√ß√µes dos 3 primeiros minicursos gratuitos com foco em nuvem realizados pelo canal Azure na Pr√°tica:
Azure na Pr√°tica Gratuito #1 - Desenvolvimento Web: saiba como foi + conte√∫dos gratuitos
Azure na Pr√°tica Gratuito #2 - Docker: saiba como foi + conte√∫dos gratuitos
Azure na Pr√°tica Gratuito #3 - Azure DevOps: saiba como foi + conte√∫dos gratuitos
A seguir est√£o tamb√©m diversos artigos e projetos de exemplo abordando diferentes servi√ßos do Azure (h√° v√≠deos sendo referenciados em alguns destes posts):
Docker ‚Äî Guia de Refer√™ncia Gratuito
Kubernetes ‚Äî Guia de Refer√™ncia Gratuito
GitHub Actions ‚Äî Guia de Refer√™ncia Gratuito
Azure DevOps ‚Äî Guia de Refer√™ncia Gratuito
Sobrevoando os servi√ßos do Azure
ASP.NET Core: identificando a cidade e o pa√≠s de origem de requisi√ß√µes com Application Insights
ASP.NET Core + Application Insights: monitorando o uso de Dapper, Entity Framework e NHibernate
.NET Core + Serverless: melhorando a experi√™ncia de Desenvolvimento com Azure Functions 3.x | pt 1
.NET Core + Serverless: melhorando a experi√™ncia de Desenvolvimento com Azure Functions 3.x | pt 2
Mensageria + .NET Core 3.1: exemplos com RabbitMQ, Kafka, Azure Service Bus e Azure Queue Storage
Serverless + Azure Functions: Guia de Refer√™ncia
Serverless √© muito mais que apenas APIs REST!
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
Microservices: alternativas para a implementa√ß√£o no Microsoft Azure
ASP.NET Core + Azure App Configuration: manipulando configura√ß√µes de forma mais inteligente
Como o Microsoft Azure pode simplificar a publica√ß√£o de suas Web Apps?- Dica R√°pida
GitHub + Azure App Service: deployment automatizado e sem complica√ß√µes de Web Apps na nuvem
Application Insights + Logic Apps + Aplica√ß√µes Web: enviando alertas de monitoramento via Slack
ASP.NET Core + Entity Framework Core: monitoramento descomplicado via Application Insights
ASP.NET Core + Application Insights: monitorando a disponibilidade dos BDs de uma aplica√ß√£o
Blog do Azure na Pr√°tica
163 
163¬†claps
163 
Blog do Azure na Pr√°tica
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Pr√°tica
"
https://medium.com/@attores/creating-a-free-kovan-testnet-node-on-azure-step-by-step-guide-8f10127985e4?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Attores Pte Ltd
May 15, 2017¬∑6 min read
Kovan is a new testnet for Ethereum using Parity‚Äôs Proof of Authority (PoA) consensus engine, with benefits over Ropsten:
Kovan, a public Proof-of-Authority testnet, uses parity to provide a stable, secure testnet environment for Ethereum developers, due to the instability of the existing Ropsten testnet.
PoA is a replacement for PoW (Proof of Work), which can be used for both public and private chain setups. There is no mining involved to secure the network with PoA, and relies on trusted ‚ÄòValidators‚Äô to ensure that valid transactions are added to blocks, processed and executed by the EVM faithfully.
Because mining does not occur on Kovan, malicious actors are prevented from acquiring testnet Ether, solving the spam attack that Ropsten faces.
You can use the Azure automated installation of Parity, however, that will cost almost $200 a month in total- above the free monthly resource level. So to get it for free, you will need to set it up manually.
Let‚Äôs create a Kovan testnet node on Azure.
Jump to:
A. Create a Ubuntu VM on Azure
B. Configure DNS
C. Setting up Kovan Testnet Node
First, let‚Äôs create and set up new Ubuntu VM on Azure portal. If you don‚Äôt have a free Microsoft azure account, let‚Äôs create it. If you have an azure account, you can skip this section.
The application below will require a credit card and may be chargeable after the first month. Therefore, it‚Äôs recommended that you try to get a higher tier of usage using the Bizspark program or other developer signup pages such as:
https://azure.microsoft.com/en-us/offers/ms-azr-0064p/
Open Account on Azure:
2. Continue with account setup and provide the required info
3. Get your identity verified
4. Once it‚Äôs done, agree to the terms and continue. Once the account is created, go to Portal in the menu.
You‚Äôll be directed to Azure portal and dashboard.
2. In search bar under New, search for ubuntu. You should see this:
Choose Ubuntu Server 16.04 LTS
3. In ‚ÄôSelect a deployment model‚Äô ‚Üí Choose ‚ÄòResource Manager‚Äô and then Create.
4. Fill in the info:
Type in your password and make a note of it. We‚Äôll need it later. Rest of the info can be the same as above.
5. Choose the size of the machine: Choose F1S machine with 1 core and 2 GB of memory which works well.
6. Configure optional settings: No need to change anything here. Azure configures it.
7. Done. The summary now is shown and our VM is created.
8. Our Ubuntu VM is created. Now Azure will deploy it for you.
We need to change the firewall rules for the setup.
We need to add 3 rules:
Click on Add.
2. Search the marketplace for ‚ÄòNetwork Security Group‚Äô. Choose Network security group from the options.
3. Once it‚Äôs created, then go back to all resources again. Choose KovanTestnetSecurity. Under Settings ‚Üí Inbound security rules.
4. There are no rules as of now. Click on ‚ÄòAdd‚Äô.
Create first rule for SSL.
5. Create the similar rules for TCP listening port 30303 and UDP port 30301 (in advanced). Set the priorities as 100, 101, 102. Finally, add SSH to allow Any (default setting).
After adding all 4 rules, the Network Security Group ‚Äî Inbound security rules should look like this:
6. We now need apply these rules to our Kovan network. In the same window of Network Security Group ‚Üí go to Subnets.
Click on ‚ÄòAssociate‚Äô.
Choose virtual network.
Choose the subnet.
Okay, good so far? Now let‚Äôs configure the DNS.
Use A name, pointing the subdomain to the IP address of the VM.
Go to your DNS manager on dashboard (The domain name you‚Äôve bought from e.g. GoDaddy or Namecheap)
Create ‚ÄòA‚Äô record. Point it to the IP address of the VM. Write the subdoamin you want in ‚ÄòHost‚Äô.
2. Install and start Parity:
a. It‚Äôll ask to install parity ‚Üí say ‚ÄòYes‚Äô
b. It‚Äôll ask to download and install netstats client ‚Üí say ‚Äòno‚Äô
3. Exit tmux with [ctrl + b], then d.
(not needed at the moment, just in case you want to watch the node running)
You are now running Parity on your Azure node!
‚Äî ‚Äî -
For RPC Usage
‚Äî ‚Äî -
Use the default /etc/nginx/nginx.conf
Edit /etc/nginx/sites-enabled/default
Start nginx
Get Kovan Testnet Ether (KETH)
That‚Äôs all. RPC node for Kovan is now set up on Azure Ubuntu VM.
Smart Contracts as a Service (SCaaS) platorm
81 
3
81¬†
81 
3
Smart Contracts as a Service (SCaaS) platorm
"
https://medium.com/hashmapinc/make-the-most-of-your-azure-data-factory-pipelines-68496da5257?source=search_post---------309,"There are currently no responses for this story.
Be the first to respond.
by Shekhar Parnerkar
Azure Data Factory (ADF) is one of the most powerful tools for building cloud data pipelines today. As with everything else, you need a well-thought-out approach in order to get the most from it. While approaches vary from project to project, some patterns remain consistent. This blog post provides some insights into best practices and potential loopholes to watch out for when planning to use ADF.
Best practices are generally common across all platforms. I will cover some of these practices that were implemented in our recent project using ADF:
ADF runs on a Spark cluster behind the scenes, and the integration is robust. You are unlikely to face any major issues unless your volumes or velocity are exceptional. However, ADF does have some limitations that must be accounted for in your design:
ADF has proven to be a robust framework for most of our data pipelining needs. It is easy to use and equally easy to deploy, which led to increased developer productivity.
Hopefully, what I‚Äôve shared through my experience gives you some insights into best practices and potential loopholes to watch out for when planning to use ADF.
Be sure and checkout Hashmap‚Äôs Azure focus page at hashmapinc.com and reach out if you‚Äôd like additional assistance in this area. Hashmap offers a range of enablement workshops and consulting service packages as part of our consulting service offerings, and would be glad to work through your specifics in this area.
Hashmap‚Äôs Data Integration Workshop is an interactive, two hour experience for you and your team where we will provide you with a high value, vendor-neutral sounding board to help you accelerate your data integration decision-making process, and selection. Based on our experience, we‚Äôll talk through best fit options for both on premise and cloud-based data sources and approaches to address a wide range of requirements. Sign up today for our complimentary workshop.
www.hashmapinc.com
To listen in on a casual conversation about all things data engineering and the cloud, check out Hashmap‚Äôs podcast Hashmap on Tap as well on Spotify, Apple, Google, and other popular streaming apps.
A rotating cast of Hashmap hosts and special guests explore technologies from diverse perspectives while enjoying a drink of choice. www.hashmapinc.com
www.hashmapinc.com
www.hashmapinc.com
medium.com
medium.com
www.hashmapinc.com
Shekhar Parnerkar is a Solution Architect and Delivery Manager at Hashmap, Pune. He specializes in building modern cloud data warehouses for Hashmap‚Äôs global clients.
Innovative technologists and domain experts helping‚Ä¶
23 
23¬†claps
23 
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/selenium-webdriver-azure-devops-automatizando-o-teste-de-aplica%C3%A7%C3%B5es-web-tdc-2019-porto-2b2e198845d0?source=search_post---------310,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Dec 7, 2019¬∑2 min read
No dia 27/11/2019 (quarta) eu e meu amigo Milton Camara Gomes (Microsoft MVP) participamos como palestrantes na Trilha Testes no TDC Porto Alegre, realizando uma apresenta√ß√£o focada no teste automatizado de aplica√ß√µes Web utilizando Selenium WebDriver e o Azure DevOps.
Essa edi√ß√£o do The Developers‚Äôs Conference aconteceu na UniRitter em Porto Alegre-RS ao longo dos dias 27 a 30/11 (quarta a s√°bado), contando com diversas trilhas simult√¢neas e participantes do Brasil inteiro.
Gostaria de deixar neste post meu muito obrigado √† Ariane Izac, √† Fabrice Nunes e √† Joyce Bastos que coordenaram a Trilha Testes pela oportunidade em participar como palestrante nesta edi√ß√£o TDC.
Os slides que utilizamos j√° est√£o no SlideShare:
A seguir temos um exemplo de implementa√ß√£o de testes com Selenium WebDriver utilizando .NET Core 3.0, Chrome Driver e xUnit:
.NET Core 3.0 + xUnit + Selenium WebDriver + .NET Standard + Chrome Driver
Diversos conte√∫dos sobre a implementa√ß√£o de testes (incluindo o uso de Selenium WebDriver) empregando Azure DevOps, .NET Core e o Visual Studio 2019 podem ser encontrados no seguinte artigo:
Testes de Software com .NET Core e o Visual Studio 2019
A implementa√ß√£o de testes com .NET Core e o Visual Studio 2019 foi tamb√©m assunto de uma live recente do Canal .NET, em que apresentei exemplos de utiliza√ß√£o dos frameworks xUnit, NUnit, MS Test, Moq, NSubstitute, Fluent Assertions e Selenium Web Driver. Nesta mesma live meu amigo Milton C√¢mara (Microsoft MVP) demonstrou a execu√ß√£o automatizada de testes a partir do Azure DevOps.
A grava√ß√£o da apresenta√ß√£o pode ser assistida gratuitamente no YouTube:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
28 
28¬†
28 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://itnext.io/tutorial-using-azure-event-hubs-with-the-dapr-framework-81c749b66dcf?source=search_post---------311,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This blog uses an example to walk you through how to use Azure Event Hubs integration with Dapr, a set of distributed system building blocks for microservices development.
Azure Event Hubs will be used as a ‚Äúbinding‚Äù within the Dapr runtime. This will allow services to communicate with Azure Event Hubs without actually knowing about it or being coupled to it directly (via SDK, library etc.), using a simple model defined by the Dapr runtime.
Sounds too good to be true? üòâ Read on‚Ä¶
You will:
Azure Event Hubs is a fully managed Platform-as-a-Service (PaaS) for streaming and event ingestion. It also provides Apache Kafka support enabling clients and applications to talk to Event Hubs without need to set up, configure, and manage your own Kafka clusters!
Dapr stands for Distributed Application Runtime. You can get all the scoop in this announcement blog, but here is a (buzzword compliant!) gist to get you started.
It is an open source, portable runtime to help developers build resilient, microservice stateless and stateful applications. It does so by codifying the best practices for building such applications into independent components. The capabilities exposed by the runtime components can be accessed over HTTP or gRPC, making it completely agnostic and allows it to work with any language and/or framework.
Dapr adopts a sidecar architecture and runs either as a separate container or as a process. This means that the application itself does not need to include Dapr runtime as a dependency. This allows for easy integration as well as providing separation of concerns.
Dapr also includes language specific SDKs for Go, Java, JavaScript, .NET and Python. These SDKs expose the functionality in the Dapr building blocks, such as saving state, publishing an event or creating an actor, through a typed, language API rather than calling the http/gRPCAPI.
Dapr is platform agnostic. You can run your applications locally, on any Kubernetes cluster and other hosting environments that Dapr integrates with.
At the time of writing, Dapr is in alpha state and supports the following distributed systems building blocks which you can plug into your applications - Service invocation, State Management, Pub/Sub messaging, Resource Bindings, Distributed tracing and Actor pattern
For a comprehensive overview, please refer to the Dapr documentation
As mentioned earlier, the example application in this post makes use of the Dapr bindings for Azure Event Hubs. Here is a peek at what ‚Äúbindings‚Äù are.
Bindings provide a common way to trigger an application with events from external systems or invoke an external system with optional data payloads. These ‚Äúexternal systems‚Äù could be anything: a queue, messaging pipeline, cloud-service, filesystem, etc.
Currently supported bindings include Kafka, Rabbit MQ, Azure Event Hubs etc.
In a nutshell, Dapr bindings allow you to focus on business logic rather than integrating with individual services such as databases, pub/sub systems, blob storage etc.
Alright, enough theory!
You can run Dapr anywhere, including Kubernetes, thanks to its first class Operator based support. Since this is an introductory blog post, let's focus on the end to end flow, keep things simple and run Dapr locally as a standalone component.
If you‚Äôre itching to run Dapr on Kubernetes, check out this getting started guide!
For the sample app, you will need:
Start by installing the Dapr CLI which allows you to setup Dapr on your local dev machine or on a Kubernetes cluster, provides debugging support, launches and manages Dapr instances.
For e.g. on your Mac, you can simply use this to install Dapr to /usr/local/bin
Refer to the docs for details
You can use the CLI to install Dapr in standalone mode. All you need is a single command
.. and that‚Äôs it!
If you don‚Äôt already have a Microsoft Azure account, go ahead and sign up for a free one! Once you‚Äôre done you can quickly set up Azure Event Hubs using either of the following quickstarts:
You should now have an Event Hub instance with a namespace and associated Event Hub (topic). As a final step you need to get the connection string in order to authenticate to Event Hubs ‚Äî use this guide to finish this step.
An Input Binding in Dapr represents an event resource that Dapr uses to read events from and push to your application. Let's run an app that uses this to receive data from Azure Event Hubs.
Start by cloning the repo and change into the correct directory
Update components/eventhubs_binding.yaml to include Azure Event Hubs connection string in the spec.metadata.value section. Please note that you will have to append the name of the Event Hub to end the connection string i.e. ;EntityPath=<EVENT_HUBS_NAME>. This is what the value for connectionString attribute should look like:
Start the Go app which uses the Azure Event Hubs Input Bindings
You should see the logs
This app uses the Azure Event Hubs native Go client to send messages.
Set the required environment variables:
Please ensure that the name of the Event Hub is the same as what you configured for the connection string in the input binding configuration
Run the producer app ‚Äî it will send five messages to Event Hubs and exit
Check Dapr application logs, you should see the messages received from Event Hubs.
Here is a summary of how it works:
Input Binding
The eventhub_binding.yaml config file captures the connection string for Azure Event Hubs.
The key attributes are:
Notice that the connection string contains the information for the broker URL (<EVENT_HUBS_NAMESPACE>.servicebus.windows.net), primary key (for authentication) and also the name of the topic or Event Hub to which your app will be bound and receive events from.
Using the binding in the app
The Go app exposes a REST endpoint at /eventhubs-input - this is the same as the name of the Input Binding component (not a coincidence!)
Dapr runtime does the heavy lifting of consuming from Event Hubs and making sure that it invokes the Go application with a POST request at the /eventhubs-input endpoint with the event payload. The app logic is then executed, which in this case is simply logging to standard output.
An output binding represents a resource that Dapr will use invoke and send messages to. Let's use an output binding to send data to Event Hubs.
Change to the correct directory
Update components/eventhubs_binding.yaml to include Azure Event Hubs connection string in the spec.metadata.value section. Please note that you will have to append the name of the Event Hub to end the connection string i.e. ;EntityPath=<EVENT_HUBS_NAME>. This is what the value for connectionString attribute should look like:
Start the Go app which uses the Azure Event Hubs Output Bindings. It will send five messages to the Dapr binding HTTP endpoint in quick succession and exit.
In the app, you will see logs:
This means that the messages were sent to Azure Event Hub via the Dapr output binding
In the receiver (input bindings) app, you should see
To keep things simple, we used the same Event Hub topic for input and output binding example. In reality these can be used independently i.e. an application which needs to be triggered by Event Hubs in an event-driven manner can subscribe to a topic (via Dapr) and another app which needs to push data to Event Hubs (to a different topic) can use the Dapr binding HTTP endpoint to POSTevent data.
The output binding configuration remains the same as Input binding.
The Go app sends a message to Event Hubs via the output binding. It does so by sending a POST request to the Dapr HTTP endpoint http://localhost:<dapr_port>/v1.0/bindings/<output_binding_name>.
In this example, the name of the output binding is eventhubs-output and the Dapr HTTP port is 3500, the endpoint URL is:
As was the case with Input Binding, the Dapr runtime takes care of sending the event to Event Hubs. Since we have the Input binding app running, it receives the payload and it shows up in the logs.
In this blog post, you saw how to use Dapr ""bindings"" to connect integrate your applications via Azure Event Hubs without having to know about it! Instead, you connected through the Dapr runtime (just a sidecar) using the Dapr HTTP API.
It is also possible to do it using gRPC or language specific SDKs (as previously mentioned)
As the time of writing, Dapr is in alpha state (v0.1.0) and gladly accepting community contributions üòÉ Vist https://github.com/dapr/dapr to dive in!
If you found this article helpful, please like and follow üôå Happy to get feedback via Twitter or just drop a comment.
ITNEXT is a platform for IT developers & software engineers‚Ä¶
140 
140¬†claps
140 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/participate-in-applied-f-challenge-2019-6100545b5f09?source=search_post---------312,"There are currently no responses for this story.
Be the first to respond.
F# is an open-source, functional, general-purpose programming language that originates from Microsoft Research and is used by many engineers across the globe. F# is actively developed by the open-source community and Microsoft‚Äôs developer division.
One of the great examples showing F# strengths is Azure Storage Type Provider. It helps ensure strongly-typed access to Azure storage blobs, tables, and queues with automatic generation of the schema, and makes programming more efficient and less error-prone.
F# can be used with many Azure services, including App Service, Cosmos DB, Azure Functions, Azure Storage, and more. For example, there is a great developer reference for that can help you get more insight into using F# for Azure Functions. The F# on Azure guide has many references and examples to help you get started. If you‚Äôd like to try F# on Linux, MacOs, or Windows, take a look at the useful quickstart.
F# Software Foundation has recently announced their new initiative ‚Äî Applied F# Challenge! We encourage you to participate and send your submissions about F# on Azure through the participation form.
Applied F# Challenge is a new initiative to encourage in-depth educational submissions to reveal more of the interesting, unique, and advanced applications of F#.
The motivation for the challenge is uncovering more of advanced and innovative scenarios and applications of F# we hear about less often:
We primarily hear about people using F# for web development, analytical programming, and scripting. While those are perfect use cases for F#, there are many more brilliant and less covered scenarios where F# has demonstrated its strength. For example, F# is used in quantum computing, cancer research, bioinformatics, IoT, and other domains that are not typically mentioned as often.
You have some time to think about the topic for your submission because the challenge is open from February 1 to May 20 this year.
Publish a new article or an example code project that covers a use case of a scenario where you feel the use of F# to be essential or unique. The full eligibility criteria and frequently asked questions are listed in the official announcement.
There are multiple challenge categories you can choose to write about:
F# for machine learning and data science.F# for distributed systems.F# in the cloud: web, serverless, containers, etc.F# for desktop and mobile development.F# in your organization or domain: healthcare, finance, games, retail, etc.F# and open-source development.F# for IoT or hardware programming.F# in research: quantum, bioinformatics, security, etc.Out of the box F# topics, scenarios, applications, or examples.
All submissions will receive F# stickers as a participation reward for contributing to the efforts of improving the F# ecosystem and raising awareness of F# strengths in advanced or uncovered use cases.
Participants with winning submissions in each category will also receive the title of a Recognized F# Expert by F# Software Foundation and a special non-monetary prize.
Each challenge category will be judged by the committees that include many notable F# experts and community leaders, including Don Syme, Rachel Blasucci, Evelina Gabasova, Henrik Feldt, Tomas Perticek, and many more.
As the participation form suggests, you will also have an opportunity to be included in a recommended speaker list by F# Software Foundation.
Help us spread the word about the Applied F# Challenge by encouraging others to participate with #AppliedFSharpChallenge hashtag on Twitter!
Any language.
119 
119¬†claps
119 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Solution Architecture. Distributed systems, big data, data analysis, resilient and operationally excellent systems.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/azure-na-pratica/net-core-serverless-melhorando-a-experi%C3%AAncia-de-desenvolvimento-com-azure-functions-3-x-pt-1-caf4090180a3?source=search_post---------313,"There are currently no responses for this story.
Be the first to respond.
Recentemente participei de 2 lives no Canal .NET com meu amigo Milton C√¢mara e que focaram na implementa√ß√£o de solu√ß√µes serverless com .NET Core + Azure Functions 3.x. Nosso objetivo com estes eventos foi demonstrar que √© poss√≠vel aproximar a experi√™ncia de desenvolvimento com Azure Functions daquilo a que muitos Desenvolvedores est√£o habituados em ASP.NET Core. Para isto abordamos:
Neste novo artigo (primeiro post de uma s√©rie de 3 artigos) trago as grava√ß√µes de cada uma destas lives (que podem ser assistidas gratuitamente), alguns dos exemplos utilizados, bem como dicas/orienta√ß√µes sobre mensageria e inje√ß√£o de depend√™ncias com Azure Functions.
E aproveito este espa√ßo para um convite‚Ä¶ Que tal participar do pr√≥ximo treinamento online promovido pelo Azure na Pr√°tica e que acontecer√° durante o dia 29/08/2020 (s√°bado), tendo como foco Serverless + Azure Functions e que engloba ainda o uso de tecnologias como Azure Logic Apps, RabbitMQ, Apache Kafka, SQL Server, MongoDB, Redis, Application Insights, Azure Cosmos DB e GitHub Actions? Acesse ent√£o o link a seguir para efetuar sua inscri√ß√£o com o desconto especial de pr√©-venda (apenas R$ 200,00):
https://bit.ly/anp-serverless3-blog-groffe-pre
Atualmente podemos implementar Functions para consumir mensagens utilizando:
Al√©m do pre√ßo reduzido para se hospedar uma Function App no Microsoft Azure, a utiliza√ß√£o de Azure Functions simplifica em muito a implementa√ß√£o de c√≥digo consumindo as mensagens de uma fila. A seguir temos um exemplo disto com Azure Queue Storage:
J√° o pr√≥ximo exemplo apresenta o uso de uma fila baseada no Azure Service Bus:
Temos ainda a possibilidade de utilizar RabbitMQ com Azure Functions (suporte ainda em Preview), como indicado no exemplo a seguir:
J√° abordei inclusive em detalhes o uso de RabbitMQ + Azure Functions no seguinte artigo:
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
T√≥picos tornam poss√≠vel o envio de mensagens a m√∫ltiplas aplica√ß√µes. No caso de uma Function vinculada ao Azure Service Bus e outra aplica√ß√£o qualquer, as mesmas apontar√£o para um mesmo t√≥pico (topic-servicebus no exemplo) e estar√£o registradas com diferentes Subscriptions cada uma (groffe0 para esta Function espec√≠fica):
O suporte a t√≥picos do Apache Kafka ainda est√° em Preview para as Azure Functions no momento da publica√ß√£o deste artigo (final de Maio/2020). O exemplo a seguir vincula uma Function a um t√≥pico chamado topic-kafka; j√° o topic-kafka-group0 indica o grupo ao qual a aplica√ß√£o correspondente est√° associada (funcionando de maneira similiar √†s Subscriptions do Azure Service Bus):
Maiores informa√ß√µes sobre o uso de Kafka com .NET Core podem ser encontradas no link a seguir:
.NET + Apache Kafka: Guia de Refer√™ncia
Os diferentes exemplos apresentados nesta se√ß√£o foram agrupados em 2 reposit√≥rios do GitHub:
.NET Core + Azure Functions 3.x + Queue Storage + RabbitMQ + Azure Service Bus (Queue)
.NET Core + Azure Functions 3.x + Azure Service Bus + Apache Kafka + Topics
Por fim, deixo aqui um projeto que criei para testes simples de envio de mensagens envolvendo as tecnologias aqui mencionadas:
.NET Core 3.1 + Console Application + Envio de Mensagens + RabbitMQ + Azure Service Bus (Queues e Topics) + Apache Kafka
Essencial para o uso de ORMs (Entity Framework Core, FluentNHibernate) ou visando um melhor tratamento de falhas por meio da biblioteca Polly, j√° abordei como o mecanismo de inje√ß√£o de depend√™ncias do ASP.NET Core tamb√©m pode ser ativado em um projeto baseado em Azure Functions no artigo:
.NET Core + Serverless: utilizando inje√ß√£o de depend√™ncias com Azure Functions
Tais ajustes passam pela inclus√£o de uma extens√£o pr√≥pria para isto em uma Function App:
E pela implementa√ß√£o de uma classe Startup baseada no tipo FunctionsStartup (namespace Microsoft.Azure.Functions.Extensions.DependencyInjection):
No caso espec√≠fico de uma Function, a implementa√ß√£o dessa estrutura dever√° ser alterada para o uso de uma classe contendo um construtor e um m√©todo n√£o-est√°ticos:
O c√≥digo-fonte do projeto detalhado nesta se√ß√£o j√° est√° dispon√≠vel no GitHub:
.NET Core + Azure Functions 3.x + Inje√ß√£o de Depend√™ncias
Serverless + Azure Functions: Guia de Refer√™ncia
.NET Core 3.x + Serverless: configura√ß√£o, dicas e exemplos com Azure Functions 3.x
Blog do Azure na Pr√°tica
27 
27¬†claps
27 
Blog do Azure na Pr√°tica
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Pr√°tica
"
https://burkeknowswords.com/new-react-azure-cosmos-db-videos-are-here-bb75f29c28af?source=search_post---------314,NA
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-6bb8a0d04a4e?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 5, 2020¬∑1 min read
2-Parte: Configurando o ambiente de banco de dados com Docker
Dando continuidade a libera√ß√£o dos m√≥dulos do meu curso: Criando API‚Äôs RESTful utilizando TypeScript, Node e mongoDB, hoje eu irei demonstrar como subir um ambiente Docker com o MongoDB e como acessar essa base de dados.
Caso voc√™ seja iniciante em Docker, eu recomendo a leitura do seguinte artigo: Comando b√°sicos docker.
Artigo contendo um passo a passo de como configurar um ambiente MongoDB com Docker: Docker: Criando servidor MongoDB
Configurando ambiente de banco de dados MongoDB com Docker
Acessando a base de dados mongoDB
Link para download do Robo 3T: Download
Espero que gostem e qualquer d√∫vida podem postar aqui ou no v√≠deo do Youtube.
Enjoy your life
69 
69¬†
69 
Aqui voc√™ vai encontrar os principais conte√∫dos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/xp-inc/azure-devops-angular-github-pages-6fe2ae45b39b?source=search_post---------316,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Oct 27, 2019¬∑3 min read
Veja nesse artigo como criar uma pipeline no Azure DevOps para publicar o seu projeto Angular no GitHub Pages
Dando continuidade ao meu artigo anterior: Angular 8.3: publicando projeto com o Angular CLI em 5 passos, hoje eu irei demonstrar como criar uma pipeline no Azure DevOps para automatizar o processo de deploy demonstrado no artigo anterior.
Para os pr√≥ximos passos sera necess√°rio ter uma conta no Azure DevOps Service. Caso voc√™ ainda n√£o tenha se cadastrado la, acesse o seguinte link para acessar o portal e criar uma conta: Azure DevOps.
Com o passo da conta OK, vamos a cria√ß√£o de uma nova pipeline. Para isso, siga os passos abaixo:
1- Clique em pipelines:
2 -Em seguida no canto superior direito, clique no bot√£o New pipeline:
3- Selecione o reposit√≥rio do seu c√≥digo.
Para esse artigo, eu irei utilizar um reposit√≥rio do meu GitHub.
4- Selecione a pipeline do Node.js:
Esse passo deve criar uma pipeline b√°sica para projetos Node.js.
Caso seja necess√°rio adicionar um novo step na sua pipeline, voc√™ pode utilizar o assistente no canto direito demonstrado na imagem abaixo:
Agora para ajustar pipeline para publicar o nosso projeto, altere o comando npm run build para npm run deploy.
Obs.: Esse comando e com base no meu arquivo package.json, nele eu adicionei o comando ng deploy da nova vers√£o do Angular 8.3
Clique em Save and Run, de um nome para o seu commit e clique em Save and Run novamente.
Assim que o Job finalizar, acesse a sua p√°gina no GitHub pages e verifique as altera√ß√µes que voc√™ subiu nessa nova release do seu projeto.
Link do meu projeto publicado em uma pagina do GitHub: Angular 8.3 DevOps.
Bom, a ideia desse artigo era demonstrar como automatizar o processo de deploy de um projeto angular, utilizando um arquivo .yml no Azure DevOps Service.
Espero que tenham gostado e at√© um pr√≥ximo artigo pessoal ;)
Enjoy your life
See all (155)
74 
74¬†claps
74 
Aqui voc√™ vai encontrar os principais conte√∫dos de tecnologia, design, dados e produto da XP Inc.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/azure-tech-nights-2018-saiba-como-foi-cada-uma-das-apresenta%C3%A7%C3%B5es-5fa5ecd750f8?source=search_post---------317,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 3, 2018¬∑3 min read
Entre os dias 20 e 28 de Agosto de 2018 aconteceu a terceira edi√ß√£o do Azure Tech Nights, um evento online, gratuito e noturno do Canal .NET focado no uso de tecnologias e servi√ßos que integram o Microsoft Azure.
Seguem alguns dados sobre o Azure Tech Nights 2018:
Pensando naqueles que n√£o puderam acompanhar ou, at√© mesmo, gostariam de rever alguma apresenta√ß√£o, foram agrupados neste post os links (indicados abaixo) de todas as palestras realizadas ao longo dos 5 dias de evento.
Dia 1 - 20/08/2018
Palestra 1 - Conhe√ßa o AKS, o servi√ßo de Kubernetes do Azure - Giovanni Bassi (Microsoft MVP)
Palestra 2 - Conhecendo as APIs do Azure Cosmos DB - Dani Monteiro (Microsoft MVP)
Palestra 3 - Implementando APIs seguras na nuvem - Renato Groffe (Microsoft MVP, MTAC)
Dia 2 - 21/08/2018
Palestra 1 - Cognitive Search nos seus Aplicativos - Thiago Cust√≥dio (Microsoft MVP)
Palestra 2 - Automatizando a entrega das suas Aplica√ß√µes ASP.NET Core com Docker, VSTS e Azure - Milton C√¢mara Gomes
Palestra 3 - Azure SignalR + Functions + Logic Apps: um exemplo pr√°tico - Ericson da Fonseca (Microsoft MVP) e Robson Ara√∫jo (Campinas .NET)
Dia 3 - 22/08/2018
Palestra 1 - Monitorando uma Aplica√ß√£o ASP.NET com Application Insights e Power BI ‚Äî Rafael Cruz (Microsoft MVP)
Palestra 2 - Boas Pr√°ticas com o Microsoft Azure ‚Äî Jaqueline Ramos (Microsoft MVP)
Palestra 3 - Intelig√™ncia artificial para desenvolvedores .NET ‚Äî Angelo Belchior (Microsoft MVP)
Dia 4 - 27/08/2018
Palestra 1 - Crie, implemente e escale aplica√ß√µes com o Azure App Service - Lucas Rom√£o (Microsoft MVP)
Palestra 2 - Criando seu primeiro experimento no Azure Machine Learning Studio - Luis Felipe
Palestra 3 - Conhecendo o Azure CDN - Mateus Rodrigues
Dia 5 - 28/08/2018
Palestra 1 - Estrat√©gias de Disaster Recovery para APIs - Rafael dos Santos (Microsoft MVP)
Palestra 2 - Introdu√ß√£o ao Azure IoT Edge - Andr√© Secco (Microsoft MVP)
Palestra 3 - Introdu√ß√£o ao Azure Maps - Joel Rodrigues (Microsoft MVP)
E por falar em cloud computing, ao longo de todo o dia 20/09/2018 (um s√°bado) acontecer√° o Azure Conference nos audit√≥rios da Microsoft em S√£o Paulo Capital.
Maior evento sobre nuvem Microsoft da Am√©rica Latina, o Azure Conference contar√° com 3 trilhas: Dev, Data & AI e Infra. Os ingressos j√° est√£o a venda (por um pre√ßo bem acess√≠vel), garanta sua participa√ß√£o acessando o site oficial do evento:
http://www.azureconference.com.br/
E para finalizar, ainda n√£o segue o Canal .NET nas redes sociais? Fa√ßa sua inscri√ß√£o ent√£o, para ficar por dentro de novidades sobre eventos, tecnologias Microsoft e outros conte√∫dos gratuitos:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
73 
73¬†
73 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/minha-participa%C3%A7%C3%A3o-no-mvpconf-2018-azure-cosmos-db-tecnologias-relacionais-e-nosql-no-azure-34bfa0c1f9ac?source=search_post---------318,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 30, 2018¬∑3 min read
Nos dias 6 e 7 de Abril/2018 (sexta e s√°bado) aconteceu a primeira edi√ß√£o do MVPConf, o maior evento t√©cnico brasileiro de experts Microsoft.
Criado e organizado pela comunidade de MVPs Microsoft no Brasil, o MVPConf teve car√°ter beneficente (todo o valor arrecadado foi repassado a institui√ß√µes como a APAE-SP) e contou com mais de mil pessoas inscritas de todo o pa√≠s.
Tive a honra de poder atuar como palestrante nos 2 dias de evento, realizando as seguintes apresenta√ß√µes:
Gostaria de deixar meu agradecimento neste post √† comiss√£o organizadora do evento (em especial ao Heber Lopes, com quem tive mais contato durante a prepara√ß√£o para o MVPConf), al√©m de todos aqueles que participaram e acompanharam as minhas apresenta√ß√µes.
Deixo registrado aqui tamb√©m meu muito obrigado ao Rodrigo Wanderley de Melo Cardoso (iMasters) e √† Ticiani Aguiar (Grupo Bandeirantes) pelo apoio na divulga√ß√£o do evento atrav√©s das seguintes mat√©rias:
Evento re√∫ne maiores experts Microsoft em S√£o Paulo - Band.com.br
Microsoft MVP Conference acontece nos dias 06 e 07 de abril, em S√£o Paulo - iMasters
A seguir est√£o os materiais utilizados em cada uma das palestras que realizei, al√©m de fotos tiradas durante estas apresenta√ß√µes.
Slides:
Fotos (deixo aqui meus agradecimentos ao MVP Andr√© Secco e ao Ericson Fonseca pela colabora√ß√£o durante esta palestra):
Slides:
Exemplos utilizados:
Fotos (deixo aqui meus agradecimentos ao Edson Vieira, ao MVP Erick Wendel, e ao MTAC Alexandre Malavasi pela colabora√ß√£o durante esta palestra):
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
16 
1
16¬†claps
16 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-free-tier-comparison-19b68578e7f?source=search_post---------319,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 20, 2020¬∑6 min read
Whether you‚Äôre new to public cloud altogether or already use one provider and are interested in trying another, you may be interested in a comparison of the AWS vs Azure vs Google free tier. The big three cloud providers ‚Äî AWS, Azure and Google Cloud ‚Äî each have a free tier available that‚Äôs designed to give users the cloud experience without all the costs. They include free trial versions of numerous services so users can test out different products and learn how they work before they make a huge commitment. While they may only cover a small environment, it‚Äôs a good way to learn more about each cloud provider. For all of the cloud providers, the 12-month free trials are available to only new users.
AWS free tier includes more than 60 products. There are two different types of free options that are available depending on the product used: always free and 12 months free. To help customers get started on AWS, the services that fall under the free 12-months are for new trial customers and give customers the ability to use the products for free (up to a specific level of usage) for one year from the date the account was created. Keep in mind that once the free 12 months are up, your services will start to be charged at the normal rate. Be prepared and review this checklist of things to do when you outgrow the AWS free tier.
The Azure equivalent of a free tier is referred to as a free account. As a new user in Azure, you‚Äôre given a $200 credit that has to be used in the first 30 days after activating your account. When you‚Äôve used up the credit or 30 days have expired, you‚Äôll have to upgrade to a paid account if you wish to continue using certain products. Ensure that you have a plan to reduce Azure costs in place. If you don‚Äôt need the paid products, there‚Äôs also the always free option.
Some of the ways people choose to use their free account are to gain insights from their data, test and deploy enterprise apps, create custom mobile experiences and more.
The Google Cloud Free Tier is essentially an extended free trial that gives you access to free cloud resources so you can learn about Google Cloud services by trying them on your own.
The Google Cloud Free Tier has two parts ‚Äî a 12-month free trial with a $300 credit to use with any Google Cloud services and always free, which provides limited access to many common Google Cloud resources, free of charge. Google Cloud gives you a little more time with your credit than Azure, you get the full 12 months of the free trial to use your credit. Unlike free trials from the other cloud providers, Google does not automatically charge you once the trial ends ‚Äî this way you‚Äôre guaranteed that the free tier is actually 100% free. Keep in mind that your trial ends after 12 months or once you‚Äôve exhausted the $300 credit. Any usage beyond the free monthly usage limits are covered by the $300 free credit ‚Äî you must upgrade to a paid account to continue using Google Cloud.
It‚Äôs important to note that the always-free services vary widely between the cloud providers and there are usage limitations. Keep in mind the cloud providers‚Äô motivations: they want you to get attached to the services so you start paying for them. So, be aware of the limits before you spin up any resources, and don‚Äôt be surprised by any charges.
In AWS, when your free tier expires or if your application use exceeds the free tier limits, you pay standard, pay-as-you-go service rates. Azure and Google both offer credits for new users that start a free trial, which are a handy way to set a spending limit. However, costs can get a little tricky if you aren‚Äôt paying attention. Once the credits have been used you‚Äôll have to upgrade your account if you wish to continue using the products. Essentially, the credit that was acting as a spending limit is automatically removed so whatever you use beyond the free amounts, you will now have to pay for. In Google Cloud, there is a cap on the number of virtual CPUs you can use at once ‚Äî and you can‚Äôt add GPUs or use Windows Server instances.
For 12 months after you upgrade your account, certain amounts of popular products are free. After 12 months, unless decommissioned, any products you may be using will continue to run, and you‚Äôll be billed at the standard pay-as-you-go rates.
Another limitation is that commercial software and operating system licenses typically aren‚Äôt available under the free tiers.
These offerings are ‚Äúuse it or lose it‚Äù ‚Äî if you don‚Äôt use all your credits or utilize all your usage, there will be no rollover into future months.
AWS has 33 products that fall under the one-year free tier ‚Äî here are some of the most popular:
For the always-free option, you‚Äôll find a number of products as well, some of these include:
Azure has 19 products that are free each month for 12 months ‚Äî here are some of the most popular:
For their always free offerings, you‚Äôll find even more popular products ‚Äî here are a few:
Unlike AWS and Azure, Google Cloud does not have a 12 months free offerings. However, Google Cloud does still have a free tier with a wide range of always free services ‚Äî some of the most popular ones include:
Check out these blog posts on free credits for each cloud provider to see how you can start saving:
Originally published at www.parkmycloud.com on July 15, 2020
CEO of ParkMyCloud
41 
41¬†
41 
CEO of ParkMyCloud
"
https://medium.com/velotio-perspectives/continuous-deployment-with-azure-kubernetes-service-azure-container-registry-jenkins-ca337940151b?source=search_post---------320,"There are currently no responses for this story.
Be the first to respond.
Containerization has taken the application development world by storm. Kubernetes has become the standard way of deploying new containerized distributed applications used by the largest enterprises in a wide range of industries for mission-critical tasks, it has become one of the biggest open-source success stories.
Although Google Cloud has been providing Kubernetes as a service since November 2014 (Note it started with a beta project), Microsoft with AKS (Azure Kubernetes Service) and Amazon with EKS (Elastic Kubernetes Service) have jumped on to the scene in the second half of 2017.
AWS had KOPS https://kubernetes.io/docs/getting-started-guides/kops/
Azure had Azure Container Service https://docs.microsoft.com/en-us/azure/container-service/kubernetes/container-service-kubernetes-walkthrough
However, they were wrapper tools available prior to these services which would help a user create a Kubernetes cluster, but the management and the maintenance (like monitoring and upgrades) needed efforts.
With container demand growing, there is always a need in the market for storing and protecting the container images. Microsoft provides a Geo Replica featured private repository as a service named Azure Container Registry.
Azure Container Registry is a private registry for hosting container images. It integrates well with orchestrators like Azure Container Service, including Docker Swarm, DC/OS, and the new Azure Kubernetes service. Moreover, ACR provides capabilities such as Azure Active Directory-based authentication, webhook support, and delete operations.
The coolest feature provided is Geo-Replication. This will create multiple copies of your image and distribute it across the globe and the container when spawned will have access to the image which is nearest.
Although Microsoft has good documentation on how to set up ACR in your Azure Subscription, we did encounter some issues and hence decided to write a blog on the precautions and steps required to configure the Registry in the correct manner.
Note: We tried this using a free trial account. You can setup it up by referring the following link: https://azure.microsoft.com/en-in/offers/ms-azr-0044p/
Note: If version 2.0.24 is used it has the following bugs:- https://stackoverflow.com/questions/48201850/login-to-the-azure-container-ser vice-fails-with-error-bool-object-has-no-attri/48204384#48204384- https://github.com/Azure/azure-cli/issues/5300
Steps to install Azure CLI 2.0.23 or 2.0.25 (ubuntu 16.04 workstation):
Steps for Container Registry Setup:
Login to your Azure Account:
Note: SKU defines the storage available for the registry for type Basic the storage available is 10GB, 1 WebHook and the billing amount is 11 Rs/day.
For detailed information on the different SKU available, click here.
Push the image to the Azure Container Registry:
Example:
Microsoft does provide a GUI option to create the ACR.
Example :
Microsoft released the public preview of Managed Kubernetes for Azure Container Service (AKS) on October 24, 2017. This service simplifies the deployment, management, and operations of Kubernetes. It features an Azure-hosted control plane, automated upgrades, self-healing, easy scaling.
Similarly to Google AKE and Amazon EKS, this new service will allow access to the nodes only and the master will be managed by Cloud Provider. For more information, you can read this.
Let‚Äôs now get our hands dirty and deploy an AKS infrastructure to play with:
Make sure you create a resource group under the following regions.
Example with different arguments :
Create a Kubernetes cluster with a specific version.
Create a Kubernetes cluster with a larger node pool.
To connect to the Kubernetes cluster from the client computer Kubectl command line client is required.
Note: If you‚Äôre using Azure CloudShell, kubectl is already installed. If you want to install it locally, run the above command:
Example :
We had encountered a few issues while setting up the AKS cluster at the time of writing this blog. Listing them along with the workaround/fix:
Error: Operation failed with status: ‚ÄòBad Request‚Äô.
Details: Required resource provider registrations Microsoft.Compute, Microsoft.Storage, Microsoft.Network are missing.
Fix: If you are using the trial account, click on subscriptions and check whether the following providers are registered or not :
Error: We had encountered the following mentioned open issues at the time of writing this blog.
https://github.com/Azure/AKS/issues/144
https://github.com/Azure/AKS/issues/141
https://github.com/Azure/AKS/issues/145
Microsoft provides a solution template which will install the latest stable Jenkins version on a Linux (Ubuntu 14.04 LTS) VM along with tools and plugins configured to work with Azure. This includes:
Refer the below link to bring up the Instance: https://docs.microsoft.com/en-us/azure/jenkins/install-jenkins-solution-template
What the pipeline accomplishes :
Stage 1:
The code gets pushed in the Github. The Jenkins job gets triggered automatically. The Dockerfile is checked out from Github.
Stage 2:
Docker builds an image from the Dockerfile and then the image is tagged with the build number. Additionally, the latest tag is also attached to the image for the containers to use.
Stage 3:
We have default deployment and service YAML files stored on the Jenkins server. Jenkins makes a copy of the default YAML files, make the necessary changes according to the build and put them in a separate folder.
Stage 4:
kubectl was initially configured at the time of setting up AKS on the Jenkins server. The YAML files are fed to the kubectl util which in turn creates pods and services.
Sample Jenkins pipeline code :
*****************************************************************
This post was originally published on Velotio Blog.
Velotio Technologies is an outsourced software product development partner for technology startups and enterprises. We specialize in enterprise B2B and SaaS product development with a focus on artificial intelligence and machine learning, DevOps, and test engineering. We combine innovative ideas with business expertise and cutting-edge technology to drive business success for our customers.
Interested in learning more about us? We would love to connect with you on ourWebsite, LinkedIn or Twitter.
*****************************************************************
Thoughts and ideas on startups, enterprise software &‚Ä¶
20 
20¬†claps
20 
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
Written by
Velotio Technologies is an outsourced software and product development partner for technology startups & enterprises. #Cloud #DevOps #ML #UI #DataEngineering
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
"
https://medium.com/serverlessguru/companies-moving-from-aws-and-azure-to-google-cloud-platform-55fe74f54bd2?source=search_post---------321,"There are currently no responses for this story.
Be the first to respond.
Amazon Web Services (AWS) is the oldest and most mature provider of cloud computing, offering the most services. However, Google Cloud Platform (GCP) and Microsoft Azure are innovating at a quick pace, and all three of these industry leaders have their pros and cons.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-increase-azure-databricks-cluster-vcpu-cores-limits-37beb07f3905?source=search_post---------322,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Mar 2, 2020¬∑5 min read
It is quite common to be asked by my customers:
Why my Azure Databricks cluster is limited?
About
Write
Help
Legal
Get the Medium app
"
https://blog.jeremylikness.com/create-a-serverless-angular-app-with-azure-functions-and-blob-storage-20164c083c88?source=search_post---------323,
https://medium.com/javarevisited/7-best-az-204-online-courses-for-microsoft-azure-developer-associate-certification-exam-in-2021-1bee42a03e7c?source=search_post---------324,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are aiming to become a Microsoft Certified: Azure Developer Associate in 2021 and preparing for AZ -204 Developing Solutions for Microsoft Azure exam then you have come to the right place. Earlier, I have shared the best AZ 900 courses and today, I am going to share the best AZ 204 online courses and practice tests for the AZure Developer associate exam.
Disclosure ‚Äî Btw, some of these links are affiliate links and I may get paid if you join these courses using my links.
Microsoft Azure is a cloud-based service that serves as a framework for businesses to manage their services and applications, amongst other things that enable them to run a successful business.
Microsoft Azure certifications are currently becoming some of the highest demanded certifications in the IT industry.
Businesses that have a reason to influence cloud servers and have a high demand for various remote smart services from different locations are the ones using Microsoft Azure the most.
Now, developers proficient in Microsoft Azure are those that know how to design and create cloud solutions in Microsoft Azure like services and applications. This covers the A-Z of project development from design, development, deployment, or launching and continuous maintenance. The AZ 204, Developing Solutions for Microsoft Azure exam tests for a developer‚Äôs abilities to Develop Azure Infrastructure as a Service compute solution, Azure storage, monitor, troubleshoot, and optimize Azure solutions and Implement Azure security. It also checks the ability to connect to and consume Azure and third-party services. If you are a Software developer looking to become Microsoft Certified as an Azure Developer, then you can register for it at the Microsoft page. The exam costs an amount of 165 USD in the United States and is offered in English. When you register for it, Microsoft offers you two options to prepare for the exam. One is free, and the other is for a stipulated price. The free course covers most aspects of Microsoft Azure while the paid course is led by an instructor and is specifically for Microsoft Certified: Azure Developer ‚Äî Associate certification Exam AZ-204. If you can‚Äôt afford the paid one then don‚Äôt disappoint because there are many more affordable options available. I have shared the best Microsoft Azure Developer associate courses in this article which you can easily buy for just $10 on Udemy.
Some of them are also free. You can use these free and paid online courses to prepare for the Microsoft Azure Developer Associate certification in 2021.
medium.com
Without wasting any more of your time, here is a list of the best online training courses from Udemy, Pluralsight, and edX that teach how to pass the Exam AZ-204: Developing Solutions for Microsoft Azure Exam to become a Microsoft Certified: Azure Developer ‚Äî Associate in 2021.
These online courses not only covers exam topics and syllabus but also provide some real-world experience of using Core Azure services which makes them great for learning as well.
Apart from being the highest-rated Microsoft Certified: Azure Developer ‚Äî Associate Exam AZ-203 course on Udemy; it is also available in German, Italian, Polish, Portuguese, Spanish, French, and English. It has almost 48,000 students enrolled and was created by Scott Duffy. In this course, you will be offered a timed practice exam to check how well you are doing and how ready you will be for the actual exam. Before you consider taking this course, you must have a free or paid subscription to Microsoft Azure with zeal to learn about the Microsoft cloud platform. A background in programming languages like .NET, NodeJS, and PHP will prove very valuable. This course is once in a lifetime. Once you buy it, you will always have access to it. In this online course, you will learn how to become a Microsoft Specialist: Developing Azure Solutions certified, pass the Microsoft AZ-203 Developing Microsoft Azure Solutions test, and master the main concepts of Azure, beyond the ones you normally use.
Here is the link to join this AZ 204 course ‚Äî AZ-204 Developing Solutions for Microsoft Azure Exam Prep
Just in case you‚Äôre still not convinced, here is a sneak peek of the course content. It covers the new AZ-204 syllabus updated last year and includes Virtual Machines, Azure Batch Services, Containerized Solutions, Azure App Service, Mobile Apps, API App, Function App, Azure Storage Accounts.
It also covers Cosmos DB, SQL Database, Blob Containers, Azure Authentication, Azure Access Control, Scaling Apps, and Services, Caching and Content Delivery Networks, Monitoring, and Logging, and Consuming Azure Services.
This is another well-designed course that prepares students for the Microsoft AZ-203 certification exam with almost 200 mock questions at the end. It has a rating of 4.4, with 3700 students enrolled. It was created by Alan Rodrigues and is taught in English. Before you consider taking this course, you have to be well versed in at least one programming language (most not be object-oriented) with a year or more experience as a programmer. You also need to have little experience with cloud computing. This course does not only teach the concepts of Microsoft Azure but also the important aspects of what is required from an exam perspective and makes you better equipped to write the AZ-203 certification exam. A look at the curriculum reveals the following topics covered. Starting with Azure (Optional), Develop Azure Infrastructure as a Service compute solutions, Develop Azure Platform as a Service compute solutions, Develop for Azure Storage, Implement Azure Security, Monitor, troubleshoot, and optimize solutions, Connect to and consume Azure and third-party services with a mock Exam.
Here is the link to join this Azure course ‚Äî Exam AZ-204 ‚Äî Developing Solutions for Microsoft Azure
This is an introductory course on AZ 204 exam to get you familiar with Exam objectives and how to prepare for this exam. In this course, you‚Äôll learn everything you need to know about beginning the process of studying for the AZ-204 Exam.
First, you‚Äôll explore the purpose and importance of the AZ-204 Exam. Next, you‚Äôll discover all of the different topic areas that will be covered throughout the entire AZ-204 Exam.
Finally, you‚Äôll learn about the tools and techniques that you will be able to take advantage of in follow-up courses.
When you‚Äôre finished with this course, you‚Äôll have the skills and knowledge of what is covered in the AZ-204 Exam and what tools and techniques will be available to help you along the way. Cloud computing is a big bonus, but not compulsory as all the sections on cloud computing are explained in detail.
Here is the link to join this AZ 204 path ‚Äî Microsoft Azure Developer: Introduction to the AZ-204 Exam
By the way, you would need a Pluralsight membership to access this course which costs around $29 per month or $199 per year (33% discount now).
While I highly recommend this kind of membership because it gives you access to more than 7000+ online courses on the latest technology, you can also try some courses using their 10-day free trial.
pluralsight.pxf.io
This course teaches students more about the capabilities and features of Microsoft Azure and how to explore those to create a more robust and highly available cloud-based solution application. You need a little background knowledge in programming to make the best of this course. The course outline covers how to configure and deploy web applications, create Azure Web Apps from the gallery, deploy and monitor Azure Web Apps, create and configure Azure Virtual Machines, create and manage a storage account, manage blobs and containers in a storage account. You will also learn to create, configure and connect to a SQL Databases instance, identify the implications of importing a SQL standalone database, and handle both user and group subscriptions in an Azure Active Directory instance, create a virtual network and implement a point-to-site network.
Here is the link to join this AZ 204 course ‚Äî Developing Microsoft Azure Solutions
This is a collection of 12 best online courses to prepare for AZ 204 ‚Äî Developing Solutions in Microsoft Azure exam from LinkedIn learning.
Cloud developers need to understand the process from design to deployment. The AZ-204 exam covers the skills a developer needs to plan, implement, monitor, and optimize optimal Azure solutions.
This learning path provides the background and support developers need to prepare for the exam.
Here is the link to join this AZ 204 path ‚Äî Prepare for the Developing Solutions in Microsoft Azure Exam
By the way, you would need a LinkedIn Learning membership to watch this course which costs around $29.99 per month but you can also watch this course for FREE by taking their 1-month-free-trail which is a great way to explore their 16000+ online courses on the latest technology.
linkedin-learning.pxf.io
This course provides 5 complete practice tests & 4 case studies for Microsoft AZ-204 Certification Exam based on the latest syllabus. Before you buy this course, a little background reading on Azure Technology is recommended. This udemy course contains 5 complete timed practice tests. Each test contains 50+ questions, that‚Äôs 250+ unique questions to test how well prepared you are for the real exam. These tests also have case studies.
This practice test course is designed to cover every topic, with a difficulty level like a real exam. They explain how to pick the right cloud services for particular applications and create databases that are secured.
Here is the link to join this test ‚Äî AZ-204 Developing Solutions for MS Azure Practice Tests
This is a brand new course on Udemy to prepare for Microsoft AZ-204 Cert Course: Developing Azure Solutions certification exam in 2021. Created by Skylines Academy and Joe Fecht, this 5.5 hours course will help you prepare for Microsoft AZ-204: Developing Azure Solutions Exam.
This course is based on the AZ-204 certification curriculum from Microsoft and is set up to align with the skills measured documentation. During this course, Skylines Academy Author and Instructor, Joe Fecht, will lead you through:
Using the Skylines Academy approach, lectures will educate you on the terms and principles of Azure solutions, and demos will enable you with a hands-on experience using scenarios to empower you in the real world.
Here is the link to join this AZ 204 course ‚Äî Microsoft AZ-204 Cert Course: Developing Azure Solutions
That‚Äôs all about the best courses to crack the Microsoft Azure Certified Developer Certification exam with code AZ ‚Äî 203. It‚Äôs a good time to become a certified Cloud developer and Microsoft Azure is one of the most promising cloud platforms.
More and more companies are adopting Microsoft Azure for their migration and demand for Azure certified programmers and professionals are increasing. You can join any of these online training courses to start your preparation.   Other Microsoft Azure and Cloud certifications you may like to explore
Thanks for reading this article so far. If you find these Microsoft Azure Administrator courses, both AZ-103 and AZ-104 useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. ‚Äî If you are new to Microsoft Azure and looking for a free beginner course to start learning key concepts of the Microsoft Azure platform then you can also check out Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It‚Äôs completely free and all you need is a free Udemy account to join this course online.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
101 
1
101¬†claps
101 
1
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/t-t-software-solution/%E0%B8%A7%E0%B8%B5%E0%B8%98%E0%B8%B5%E0%B8%A5%E0%B8%87%E0%B8%97%E0%B8%B0%E0%B9%80%E0%B8%9A%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%AA%E0%B8%AD%E0%B8%9A-az-900-online-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-azure-exam-voucher-c0d027253d34?source=search_post---------325,"There are currently no responses for this story.
Be the first to respond.
‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ Voucher ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£ Virtual Academy for Azure Fundamentals ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏Å‡πá‡∏°‡∏≤‡∏î‡∏π‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ô‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÅ‡∏£‡∏á‡∏ö‡∏±‡∏ô‡∏î‡∏≤‡∏•‡πÉ‡∏à‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì Rachanee Saengkrajai ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
medium.com
2. ‡∏Å‡∏î‡∏ó‡∏µ‡πà Link Microsoft Learn Dashboard
3. ‡∏Å‡∏î Schedule Exam
4. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ az-900
5. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Thailand ‡πÅ‡∏•‡∏∞‡∏Å‡∏î‡∏ó‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏° Schedule with Pearson VUE
6. ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢
7. ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ Exam Discounts ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡∏î‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏°‡∏µ Voucher ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏ö‡∏ü‡∏£‡∏µ ‡∏≠‡∏¥‡∏≠‡∏¥ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏•‡πà‡∏≠)
8. ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏û‡∏≤‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á Pearson VUE ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡∏î Run pre-check ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÅ‡∏•‡∏∞ internet ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô‡πÑ‡∏´‡∏°‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
9. ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á Microphone, Internet Speed ‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏•‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞ Microphone ‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏Ñ‡∏∏‡∏°‡∏™‡∏≠‡∏ö‡πÄ‡∏´‡πá‡∏ô
10. ‡∏ñ‡πâ‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡∏à‡∏∞‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö‡∏Å‡πá‡∏õ‡∏¥‡∏î Tab ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏ö‡∏¢
11. ‡∏Å‡∏î‡∏ï‡∏Å‡∏•‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
12. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∞‡πÑ‡∏£ ‡∏ú‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å English
13. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏Å‡∏î Next
14. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏Ñ‡∏∏‡∏°‡∏™‡∏≠‡∏ö ‡∏ú‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å English
15. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏° ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏∏‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Location Thailand ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á Timezone ‡πÄ‡∏õ‡πá‡∏ô Asia/Jakarta-WIB ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏•‡∏≠‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÅ‡∏Å‡πâ Location ‡πÉ‡∏´‡∏°‡πà‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
16. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏≠‡∏ö, ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ö ‡πÅ‡∏•‡∏∞ ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Proceed to Checkout
17. ‡∏Å‡∏î Accept ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏ï‡∏Å‡∏•‡∏á‡∏ï‡∏≤‡∏° Policies
18. ‡∏Å‡∏î‡∏ó‡∏µ‡πà Add Voucher or Promo Code, Copy Voucher ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏≤‡∏Å Email ‡∏°‡∏≤‡πÅ‡∏õ‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Apply
19. ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏•‡∏î‡πÅ‡∏ß‡πâ‡∏ß‡∏ß‡∏ß ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0 USD ‡πÄ‡∏•‡∏¢ ‡∏´‡∏∂‡∏´‡∏∂‡∏´‡∏∂‡∏´‡∏∂‡∏´‡∏∂‡∏´‡∏∂!
20. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ö ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏Å‡∏î Submit Order ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏™‡∏≠‡∏ö
21. ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö Email 2 ‡∏â‡∏ö‡∏±‡∏ö‡∏ö‡πÉ‡∏ô Inbox ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏Ñ‡∏£‡∏±‡∏ö
22. Email ‡∏â‡∏ö‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á Invoice ‡∏Ñ‡∏£‡∏±‡∏ö 0 USD ‡∏™‡∏ö‡∏≤‡∏¢‡πÉ‡∏à ‡∏≠‡∏¥‡∏≠‡∏¥
23. Email ‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏à‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏™‡∏≠‡∏ö‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Certification Dashboard ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≠‡∏ö
24. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Microsoft Learn Dashboard ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Appointments ‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏Ç‡∏ß‡∏≤‡∏°‡∏°‡∏∑‡∏≠‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡∏î‡∏ó‡∏µ‡πà‡∏•‡∏¥‡πâ‡∏á Start a previous scheduled online proctored exam ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ö
25. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ó‡∏µ‡πà Pearson VUE ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡πÉ‡∏ô‡∏Å‡∏•‡πà‡∏≠‡∏á Purchased Online Exams ‡∏Ñ‡∏£‡∏±‡∏ö
‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏î‡∏ó‡∏µ‡πà AZ-900: Microsoft azure Fundamentals
26. ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏Å‡∏î‡∏ó‡∏µ‡πà‡∏•‡∏¥‡πâ‡∏á run the system test ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Program ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ö
27. ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏ï‡∏¥‡πâ‡∏Å‡∏ñ‡∏π‡∏Å, ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Copy Access Code ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥ Code ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡πÅ‡∏•‡∏∞‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° Download Application ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏Å‡∏î‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
28. ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏à‡∏∞‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡πÜ Meeting Tool ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡∏£‡∏≠‡∏Å Access Code ‡πÅ‡∏•‡∏∞‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏Å‡πá‡∏Å‡∏£‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏Ñ‡∏£‡∏±‡∏ö
29. ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏Å‡∏•‡πâ‡∏≠‡∏á, Microphone, ‡πÅ‡∏•‡∏∞ Internet Speed ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö
30. ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏Å‡∏î Launch sample exam ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á!
‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏ú‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ú‡∏°‡∏°‡∏µ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ä‡∏∑‡πà‡∏≠ Bandicam ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏≠‡∏±‡∏î Video ‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏¥‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° Bandicam ‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö 1 ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏≥ ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ Capture ‡∏†‡∏≤‡∏û‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
31. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡πá‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Å‡∏î Close ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÄ‡∏•‡∏¢
‡∏à‡∏ö‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏∏‡∏Å‡∏ó‡πà‡∏≤‡∏ô‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ‡∏°‡∏µ‡∏ä‡∏±‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡πâ‡∏≤‡∏ö‡∏ö‡∏ö
‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô‡πÅ‡∏ß‡πâ‡∏ß ‡πÄ‡∏¢‡πâ‡πÜ‡πÜ‡πÜ AZ-900
www.youracclaim.com
Medium ‚Äî AZ-900 ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÅ‡∏ô‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡∏•‡∏á‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏™‡∏≠‡∏ö
Medium ‚Äî AZ-900 ‡∏™‡∏£‡∏∏‡∏õ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏∏‡∏î‡πÜ
‡∏ô‡∏≤‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô ; )
https://www.tt-ss.net/
47 

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
47¬†claps
47 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/five-things-about-azure-devops-with-damian-brady-894c95995259?source=search_post---------326,"There are currently no responses for this story.
Be the first to respond.
Five Things is back! Season 2 is here and our first episode has everything that you want in a low-budget video series about tech ‚Äî awkward interviews, marginal insight, AND miniature ponies.
In our season opener, I sit down with Damian Brady to discuss the new Azure DevOps. DevOps was formally known as Visual Studio Team Services. The new release brings not just a name change, but a new feature called ‚ÄúAzure Pipelines‚Äù.
azure.microsoft.com
Azure Pipelines is a new product which allows you to deploy your code from any source control repository to any cloud. You can pull, do builds, run tests and deploy your code. Furthermore, it‚Äôs free for open-source projects. If you‚Äôre using GitHub, you can get Azure Pipelines as an extension from the Github extension gallery and setup a pipeline whenever you create a new repo.
Don‚Äôt want to watch the video? That‚Äôs fine, I‚Äôm not offended. Here‚Äôs a summary.
Check out the interview with Damian, and be sure to visit the link below to get started with Azure DevOps.
dev.azure.com
Any language.
28 
3
28¬†claps
28 
3
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@tsuyoshiushio/azure-functions-v-2-0-httptrigger-with-cosmosdb-client-tips-15d313cb1cbe?source=search_post---------327,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Apr 1, 2018¬∑3 min read
I spent several hours to setup .NetCore 2.0 based Azure Functions Environment with CosmosDB. I‚Äôd like to share my work around to record what I encountered. These tips are not special. You can find these in the GitHub issues. However, it might be help to reduce the time for some search.
I create an Azure Functions app with .Net Core 2.0 with CosmosDB. Currently I don‚Äôt use Cosmos Trigger. However, I guess you can do it the same thing to it.
If you want to use CosmosDB extensions you can use Microsoft.Azure.WebJobs.Extensions.CosmosDB v3.0.0-beta7 It is the same as the DocumentDB.Core 1.5.1
However, I encounter a lot of issues. You can find the version match on this announcement issue.
github.com
I encounter a lot of these.
When you start Visual Studio debugging, it might happen. On the debugging feature, the Azure Functions runtime of your Visual Studio might old. Currently, we have no way to upgrade it. Instead, we can use the Azure Functions CLI instead. For installing Azure Functions CLI, you can refer this.
Visual Studio 2017 > Project right click > Properties > Debug . Then configure like this. This is just configure to use the Azure Functions CLI.
For more details discussion
github.com
You might get this missing library, You might write HttpTrigger with Old Style. Unfortunately, we have no way to solve until now. However, if you switch it to the new style, You might not encounter this error.
This is the old style. (This is the V1 Style)
The new Style V2
github.com
(Update! 03/05/2018) Rather than use this strategy, you can install Microsoft.Azure.WebJobs.Extensions.CosmosDB 3.0.0-beta7 works now much better!
(Duplicated!)To solve this issue, you can Install Microsoft.Azure.DocumentDB v1.19.1 nuget package. I‚Äôm not sure why it solve the problem. The nuget package is for .NetFramework! However, it install two DLLs and win7-x64 runtime DLL for these. Maybe I need to read code and understand .Net Core 2.0 nuget architecture. If you know a good resource, and why it happens. please let me know.
github.comConso
Now you can play with Azure Functions V2 with CosmosDB. :) In the future, the same thing might happen, however, you can check these issues.
Senior Software Engineer ‚Äî Microsoft
53 
1
53¬†
53 
1
Senior Software Engineer ‚Äî Microsoft
"
https://medium.com/@renatogroffe/hospedando-projetos-web-no-azure-de-um-site-est%C3%A1tico-a-um-cluster-kubernetes-msp-tech-days-7c02cf3ae62a?source=search_post---------328,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 8, 2019¬∑3 min read
No s√°bado dia 01/06/2019 participei como palestrante do MSP Tech Days, em uma apresenta√ß√£o sobre as diferentes op√ß√µes para hospedagem de aplica√ß√µes Web no Azure. Este evento em S√£o Paulo-SP, mais precisamente na Unidade Paulista da Universidade S√£o Judas (situada na Avenida Ang√©lica).
Realizei durante a palestra apresenta√ß√µes sobre os seguintes servi√ßos: Storage Account (para hospedagem de Web Sites est√°ticos), Azure App Service, Azure Container Registry, Azure Container Instances, Azure Web App for Containers e Azure Kubernetes Service (AKS).
O MSP Tech Days est√° sendo promovido por MSPs (Microsoft Student Partners) da Am√©rica Latina em cidades de diversos pa√≠ses, al√©m de contar com eventos online gratuitos cobrindo diferentes tecnologias Microsoft e previstos para este m√™s de Junho/2019. Sugiro a todos que acompanhem esta iniciativa, fazendo sua inscri√ß√£o no meetup mantido pelos MSPs LATAM:
www.meetup.com
Gostaria de deixar neste post meu muito obrigado ao Orlando Gomes (Microsoft MVP, MSP) e ao Rog√©rio da Rocha Rodrigues (Microsoft MVP, MSP, MTAC) pelo convite e por todo o suporte para que eu pudesse participar como palestrante. Deixo aqui tamb√©m meus agradecimentos ao Orlando Gomes, ao Gabriel Flor√™ncio e ao Andre Moreira pelas fotos tiradas durante a palestra.
Os feedbacks que tive sobre a apresenta√ß√£o foram tamb√©m √≥timos. Aproveito para agradecer mais uma vez ao Andre Gomes por inclusive mencionar isto no LinkedIn:
Os slides da apresenta√ß√£o j√° est√£o no SlideShare:
Para os interessados em conhecer mais sobre o conte√∫do abordado durante a apresenta√ß√£o deixo ainda como refer√™ncia os seguintes artigos que produzi sobre a hospedagem de projetos Web no Microsoft Azure:
Hospedando um website est√°tico de forma r√°pida e barata no Azure Storage
Publicando um web site est√°tico na nuvem com Docker, Nginx e Azure Container Instances
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
ASP.NET Core + Azure Web App for Containers: escalando uma API REST com containers Docker
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
ASP.NET Core + Azure + Kubernetes: Guia de Refer√™ncia
E tamb√©m a grava√ß√£o de uma live que fizemos no Canal .NET, com dicas de utiliza√ß√£o do Microsoft Azure para Desenvolvedores Web:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
27 
27¬†claps
27 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/the-pythic-coders-recommended-content-for-getting-started-with-machine-learning-on-azure-fcd1c5a8dbb4?source=search_post---------329,"There are currently no responses for this story.
Be the first to respond.
Tldr; Since the post on DevOps resources was well received, and it can be difficult tracking down documentation, I was asked to provide a list of recommended resources for Machine Learning on Azure. I will make an effort to keep this list updated as new developments emerge.
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
docs.microsoft.com
docs.microsoft.com
Our team‚Äôs charter is to help every technologist on the planet succeed, be they students or those working in enterprises or startups. We engage in outreach to developers and others in the software ecosystem, all designed to further technical education and proficiency with the Microsoft Cloud + AI platform. If your stuck feel free to send us a tweet.
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
github.com
github.com
github.com
github.com
github.com
medium.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
docs.microsoft.com
github.com
github.com
github.com
Any language.
36 
36¬†claps
36 
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/visual-studio-2017-2019-fails-when-i-create-an-azure-functions-project-89e993ef31f?source=search_post---------330,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
May 23, 2019¬∑1 min read
I encounter the issue that I can‚Äôt create an Azure Functions project.
We can create other project. However not for Azure Functions. If I try to create an Azure Functions project on VS2017, it says, One or more errors occurred.
For VS2019, it says, Illegal characters in path.
I remove/re-install the VS however, it is the same.
Remove %localappdata%\AzureFunctionsTools . It solves both cases. I encountered this solution before, at that time it was different error. Next time, I should try this work around. If we remove the directory, maybe the tools folder was broken.
developercommunity.visualstudio.com
Senior Software Engineer ‚Äî Microsoft
See all (200)
37 
4
37¬†claps
37 
4
Senior Software Engineer ‚Äî Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/streaming-at-scale-in-azure/apache-drill-azure-blobs-and-azure-stream-analytics-ef34a1360d2b?source=search_post---------331,"There are currently no responses for this story.
Be the first to respond.
Apache Drill is a very interesting project that, if you haven‚Äôt heard of it yet, allows you to use the ubiquitous SQL language to query‚Ä¶almost everything. Here‚Äôs the description on the website:
Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.
drill.apache.org
So I though it would have been perfect also to query Azure Stream Analytics results. As you know, Azure Stream Analytics doesn‚Äôt support (yet?) a notebook-style environment (√†-la Databricks Spark Structured Streaming for example), which makes development and testing a bit trickier. I already described a possible solution here:
medium.com
As said in the mentioned article, an alternative ‚Äî and actually better ‚Äî solution, than using Azure SQL, especially if you‚Äôre not really comfortable with it, is to use Apache Drill to read the output of Azure Stream Analytics and so check if the query produces the expected results.
Quite easy but with a big caveat if you already have installed Apache Spark. But let‚Äôs start from the beginning. Download and install Apache Drill as described here:
drill.apache.org
I used the embedded mode since I didn‚Äôt want to install an entire Hadoop cluster to run Drill. I just needed something easy, quick and cheap. The embedded mode, that can run on a single node, is just perfect for this use case. If your machine doesn‚Äôt have any Apache Spark or Apache Hadoop installed, you‚Äôll be able to see something like this:
But if you have Apache Spark installed, for example, Drill will notice the HADOOP_HOME environment variable existence and will try to use it. The result is that it won‚Äôt be able to start Apache Zookeeper in local mode and you‚Äôll get some errors:
To solve the problem you can just unset the HADOOP_HOME variable just for the Apache Drill session, creating a .bat file like the following:
To allow Apache Drill connect to Azure Blob Store, a specific Data Source needs to be configured.
https://drill.apache.org/docs/connect-a-data-source-introduction
Now, since explicit support to Azure Blob Store is mentioned even in the Apache Drill homepage, you would expect a nice documentation page, just like it exists for Amazon S3, that tells you how to configure everything, right? Wrong, of course.
The first step to make Azure Blob Store working with Apache Drill is getting the correct version of JARS. The one I found working are the following
download and copy them into jars/3rdparty folder. Easy right? The problem was just finding the correct combination of libraries versions that works‚Ä¶but you‚Äôre lucky since I‚Äôve already done it for you, so you can just enjoy one more time at the pub, instead of spending hours just testing libraries.
The libraries you just copied will be automatically picked up by Drill when needed‚Ä¶so it‚Äôs now time to tell it when it should do so. First of all you need to get the key value of the Azure Blob Store you what to access to. It can be done via the Azure Portal, the AZ CLI or via the nice Azure Storage Explorer:
and then you change (or create if it doesn‚Äôt exists) the core-site.xml file in conf folder so that the XML will look like the following:
replacing, of course, ACCOUNT_NAME and ACCOUNT_KEY with the Azure Blob Store account name you want to use and its own secret key.
You then have to create a new Storage Plugin. You can easily do that from the web interface (http://localhost:8047, just keep in mind the Drill must be running in order to access it), using the ‚ÄúStorage‚Äù section.
Create a new Storage Plugin, name it az for example, and then copy and paste the following configuration.
https://gist.github.com/yorek/35e2b693fb749f0388db22c2d814ddaf
The configuration has been taken from the dsl Storage Plugin already available in Apache Drill, and the connection has been modified to point to the Azure Blob Store. Of course, just like before, replace CONTAINER and ACCOUNT_NAME with your own values.
You should be able now to run Apache Drill and query the configured Azure Blob Store:
The most complex part is done now. All is needed now is to create an Apache Avro output for the Azure Stream Analytics job you want to monitor
and then just use Apache Drill to query it. Of course you can also use other output format like JSON or CSV, but I suggest to use Avro since it comes with a schema and so you don‚Äôt have to do any cast to correct data types. Plus it is fast and compact which make it perfect for streaming scenarios.
Here‚Äôs Drill in action:
If you don‚Äôt want to mess up your machine with Java, Apache Drill and other stuff that maybe you‚Äôre not familiar with, but you still like the idea of using Drill to query streaming results, you can just go for the Docker way. In order to have Apache Drill up and running with the Storage Plugin correctly configured, a trick I used is to inject the file storage-plugin-override.conf and core-site.xml to have my most accessed Azure Blob Stores already configured and ready to be used.
You can grab the Docker solution here:
github.com
Make the changes you need to the aforementioned files, build the image with the provided drill-build.bat script and then run it with drill-run.bat. You‚Äôll have Apache Drill running in a sec.
Notes on creating streaming at scale solution in the Azure‚Ä¶
49 

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
49¬†claps
49 
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-best-courses-to-learn-spring-boot-with-aws-and-azure-cloud-platform-9f953d12bb93?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn how to deploy Spring Boot apps and Microservices on public cloud platforms like AWS, Azure, and Google Cloud Platform and looking for the best resources like online courses then you have come to the right place.
Disclosure ‚Äî Btw, some of these links are affiliate links and I may get paid if you join these courses using my links.
Earlier, I have shared my favorite Spring Boot Course and best Microservices courses and in this course, I will share some advanced level courses to learn about containerization and deployment of Spring Boot Microservices on AWS, Azure, Google Cloud Platform, and even on legacy Cound Foundry.
If you have been reading technical blogs and articles then you might be hearing about widespread cloud adoption among all sizes of companies. In the last few years, many companies, both big and small have shifted their infrastructure to the cloud or in the process of doing it.
I have no doubt that the next generation of Java applications will be written for and run in the Cloud and that‚Äôs why it‚Äôs important for Java developers to learn about Cloud platforms like AWS, Azure, GCP, Cloud Foundry, and others.
Thankfully Java frameworks like Spring Framework is taking this cloud move seriously and new frameworks like Spring cloud is getting popular which makes developing cloud-based application easy.
While there will be some challenges to shift the focus from writing in premises to cloud-native applications, adopting Microservices architecture and cloud-native Java can help Java developers stay ahead of the curve.
The microservices architecture perfectly suits the public cloud, with its focus on elastic scaling with on-demand resources. Since most of the web application and Microservice development is happening on Spring Boot, the main thing you can learn as of now is how to deploy your Spring Boot application on different cloud platforms like AWS and Microsoft‚Äôs Azure Cloud Platform While I have shared about cloud-computing resources like AWS, Azure, and GCP in past my readers asked me something which is focused on Java and Spring Boot and that‚Äôs why I am going to share the 5 best cloud courses that are focused on Java and Spring Boot. These are the practical and hands-on courses that will teach you things like how to deploy your Spring Boot application to AWS with Elastic Beanstalk, ECS, and Fargat, or deploy Java Microservices to AWS and other cloud platforms. These courses are equally useful for both beginner and experienced Java developers how are working with Spring Boot and cloud platforms as well as DevOps Engineers who are responsible for managing Java applications on Cloud.
Without wasting any more of your time, here is a list of the best hands-on cloud courses for Java and Spring Boot developers. Currently, these online training course covers cloud platforms like Amazon Web Service, Microsoft Azure, Google Cloud Platform and Pivotal‚Äôs Cloud Foundry environment but I will keep adding new training courses which are focused on Java and Spring boot but teach you how to deploy a Spring Boot application and Microservices in Google Cloud Platform and others.
This is one of the first courses you should take if you want to deploy your Spring Boot applications to AWS. This course will teach you step by step to deploy a Java Spring Boot REST APIs and Full Stack application to AWS using Elastic Beanstalk service. Created by Ranga Rao Karnam, a fellow Java developer and best selling Udemy instructor this course will not only teach you to core AWS services like EC2, S3, AWS CodePipeLine, AWS CodeBuild, SQS, IAM, CloudWatch but also teach you things like how to deploy a RESTful web service into the cloud. You will learn how to containerize your Java and Spring Boot application using Docker and then deploy it into Cloud. You will also learn how to automatically scale your Java applications based on load as well as deploy multiple instances behind a load balancer using Elastic Beanstalk service in AWS.
You will also learn how to create a continuous delivery pipeline with AWS Code Pipeline which is quite important from a DevOps perspective.
Here is the link to join this course ‚Äî Deploy Java Spring Boot Apps to AWS with Elastic Beanstalk
Overall, a very practical and useful course for experienced Java developer who wants to learn how to deploy, scale, and manager Java and Spring boot application on AWS.
This is another great course for Java developers who wants to learn how to deploy Spring Boot Applications to the Cloud on AWS and how to implement Continuous Integration and Continuous Delivery in AWS (CI/CD) for DevOps.  Created by John Thompson from Spring Framework Guru, one of my favorite Java instructors on Udemy, this course is focused on DevOps for Spring application on the AWS cloud platform.
In this course, you will learn how to deploy Spring Applications to multiple environments including AWS. You will start with basics like creating a server in AWS using the Amazon EC2 service. This is a very hands-on course and to get the most out of this course, you will need an AWS account. Don‚Äôt worry, you don‚Äôt need to spend any additional money as you should be able to use the AWS free tier to complete the course assignments.
In this course, you will learn how to install Jenkins on a Linux server. A server that you will provide in the AWS cloud. You will also learn how to use Docker and MySQL databases in the AWS environment.
Here is the link to join this course ‚Äî Spring Framework DevOps on AWS
The course also teaches you best practices used in enterprise software development like using a continuous integration server for continuous delivery.
This is another advanced Spring Boot Microservice course for Java developers who wants to deploy into public cloud computing platforms like AWS and Azure.
One of the key steps before deploying into the cloud is containerizing or Dockerizing your Spring Boot applications and this course will teach you how to extend, refine, harden, test, and ‚Äúdockerize‚Äù your Spring Boot microservices, and turn them into production-ready applications.
You will also learn about how to link to external databases, build secure APIs, use unit and integration testing to uncover application flaws during development and configure scalable deployment options with Docker containers.
Overall an advanced course to extend, refine, harden, test, and ‚Äúdockerize‚Äù your Spring Boot microservices, and turn them into production-ready applications.
Here is the link to join this course ‚Äî Extending, Securing, and Dockerizing Spring Boot Microservices
By the way, you would need a LinkedIn Learning membership to watch this course which costs around $19.99 per month but you can also watch this course for FREE by taking their 1-month-free-trail which is a great way to explore their 16000+ online courses on the latest technology.
This is a free Spring Boto Microservice course from Coursera where you will learn how to develop Java Microservice with Spring Boot and Spring Cloud Microservices on Google Cloud Platform
This course is created by Google Cloud Training, so you will be learning from the source. In his course, you will use Cloud Runtime Configuration and Spring Cloud Config to manage your application‚Äôs configuration.
You‚Äôll send and receive messages with Cloud Pub/Sub and Spring Integration. You‚Äôll also use Cloud SQL as a managed relational database for your Java applications, and learn how to migrate to Cloud Spanner, which is Google Cloud‚Äôs globally distributed strongly consistent database service.
It will also teach you about tracing and debugging your Spring applications with Stackdriver.
Here is the link to join this course ‚Äî Building Scalable Java Microservices with Spring Boot and Spring Cloud
By the way, if you find Coursera courses useful, which they are because they are created by reputed companies and universities around the world, I suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
coursera.com
Apart from major cloud platforms like AWS, Azure, and GCP, there also exist specialized cloud platforms like Pivotal‚Äôs Cloud Foundry, also known as PFC. If you remember, Pivotal is the company behind Spring Framework and they are also pioneering cloud-native Java development.
By the way, PCF is now called Tanzi. PWS is no longer available. You would need to install PCF Dev on Your Local Machine to play with PCF. If you are looking for a course to learn how to deploy a Java or Spring Boot application, a RESTful API, Full Stack Applications, and Microservices to Pivotal Cloud Foundry then this is the perfect course for you. In this course, you will not only learn Pivotal Cloud Foundry ( PCF ) fundamentals but also things like how to deploy Spring Boot REST API to the Pivotal Cloud Foundry environment.
This course covers a number of PCF Services like Databases, Spring Cloud Services including Service Registry and Config Server which is important for Java developers.
You will not only learn to deploy REST APIS and Microservices but also Full Stack Applications are written in Java and Spring Boot.
Here is the link to join this Spring Boot occurs ‚ÄîMaster Pivotal Cloud Foundry (PCF) with Spring Microservices
You will also learn how to Auto Scale applications based on load as well as deploy multiple instances behind a load balancer using Pivotal Cloud Foundry. In short, a good, hands-on course to learn about the Pivotal Cloud Foundry platform from Java and Spring boot developer‚Äôs perspective.
This is another advanced course on AWS for Java and Spring Boot developers. It contains over 8-hours of online training material that will teach you everything you need to know about AWS from a Java developer‚Äôs perspective. Created by In28Minutes, this course starts with explaining AWS fundamentals and then covers a number of AWS Services like ECS ‚Äî Elastic Container Services, AWS Fargate, EC2 ‚Äî Elastic Compute Cloud, S3, AWS CodePipeLine, AWS CodeBuild, IAM, CloudWatch, ELB, Target Groups, X-Ray, AWS Parameter Store, AWS App Mesh and Route 53. You will not only learn how to build Docker images for your Java Spring Boot Microservice Projects but also the basics of implementing Container Orchestration with ECS (Elastic Container Service) ‚Äî Cluster, Task Definitions, Tasks, Containers, and Services.
You will also learn practical stuff like creating a continuous delivery pipeline with AWS Code Pipeline and how to debug problems with deploying containers using Service events and AWS CloudWatch logs etc. It also covers implementing Centralized Configuration Management for your Java Spring Boot Microservices with AWS Parameter Store.
Here is the link to join this course ‚Äî Deploy Spring Boot Microservices to AWS
I highly recommend this course to experienced Java developers and DevOps engineers who are responsible for managing Java-based Microservices and Spring boot applications. Overall, an advanced AWS course for Java and Spring Boot developers. You will learn a lot of practical stuff for deployment, scaling, monitoring, troubleshooting, and tracing Java and Spring boot application on AWS.
This is another Spring Boot course by Ranga Karnam and in this course, he will teach you how to deploy Java Spring Boot REST API, Full Stack, Docker, and Web Apps with Azure App Service and Azure Web Apps into the Microsoft Azure platform. It‚Äôs not very different from the first course which talks about deploying Spring Boot application on AWS and if you have gone through that course then deploying on Azure will be much easier as both AWS and Azure.
Even though both AWS and Azure have different services for computing, storage, and network but concepts and processes remain the same. Things like deploying a containerized version are applicable for both AWS and Azure. The good thing is that you will learn how to deploy your Java Spring Boot application online for live Internet access, which is what many Java developers always ask. It gives you a lot of satisfaction to see your app live on the web and you can also share the links with your friends and colleagues.
Here is the link to join this course ‚Äî Take Java Spring Boot Apps to Azure
If you have a startup idea then you can also use the techniques learned in this course to deploy a proof of concept app and share it with your clients and beta tester. Overall a practical and hands-on course to deploy Java and Spring Boot applications on the Microsoft Azure platform. That‚Äôs all about some of the best courses to learn how to deploy Spring Boot applications on various cloud platforms like AWS, Microsoft Azure, and Pivotal‚Äôs CloudFoundary.
The list not just include basic courses that teach you AWS and Azure basics along with Java deployment but also some advanced courses which will teach you how to deploy your spring boot on the internet and access it via the web and automatically scale up and down based upon load by using sophisticated services provided by AWS.   Other Java and Spring articles you may like to explore
Thanks for reading this article so far. If you like these best Spring Boot and Cloud Computing courses then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note. P. S. ‚Äî If you are looking for a free course to learn Spring Boot and Cloud then you can also check out this Spring Boot and AWS S3 free course on Udemy. This course is created by Nelson Djaolo and it will teach you how to upload images and files to Amazon S3. The course is completely free and all you need is to create a free Udemy account to enroll in this course.
udemy.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
142 
Collection of best Java articles, tutorials, courses, books, and resources from Javarevisite and its authors, Java Experts and many more. ¬†Take a look.

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
142¬†claps
142 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/t-t-software-solution/%E0%B8%AA%E0%B8%B2%E0%B8%98%E0%B8%B4%E0%B8%95%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%95%E0%B8%B4%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87-azure-paas-%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A-%E0%B8%81%E0%B8%B2%E0%B8%A3-migrate-asp-net-core-2-%E0%B9%81%E0%B8%A5%E0%B8%B0-entity-framework-core-6196eaeaefec?source=search_post---------333,"There are currently no responses for this story.
Be the first to respond.
‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ ‡∏ú‡∏°‡∏à‡∏∞‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ ‡πÉ‡∏ä‡πâ PaaS (Platform as a Service) ‡∏Ç‡∏≠‡∏á Azure ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏á Web ‡πÅ‡∏•‡∏∞ Database ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢ ‡∏î‡πâ‡∏ß‡∏¢ Technology Stack ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏õ‡∏•.‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÄ‡∏•‡∏¢‡∏à‡∏∞‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö PaaS ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Server ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏ô‡πâ‡∏ô‡∏î‡∏π‡πÅ‡∏Ñ‡πà ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ ‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏ó‡∏≤‡∏á Azure ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏î‡∏π‡πÅ‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô Windows ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≠‡∏¢ update ‡∏´‡∏£‡∏∑‡∏≠ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á
‡∏ú‡∏°‡∏Ç‡∏≠‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
Subscription ‡∏à‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏á‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∂‡∏á‡∏Ñ‡∏ß‡∏£‡πÅ‡∏¢‡∏Å subscription ‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏ï‡∏≤‡∏° ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ project ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
Resource Group ‡∏à‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡∏≠‡∏á Azure Resource ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏£‡πâ‡∏≤‡∏á Group Dev ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Resource ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Development ‡∏ã‡∏∂‡πà‡∏á ‡πÄ‡∏ß‡∏•‡∏≤‡∏•‡∏ö‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á Group ‡πÄ‡∏•‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ Storage Account ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö Database Backup File ‡∏à‡∏≤‡∏Å Server ‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤ Deploy ‡∏ï‡πà‡∏≠‡πÉ‡∏ô Azure Cloud Database ‡∏Ñ‡∏£‡∏±‡∏ö
‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏î‡πâ Storage Account ‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Container ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ô‡∏±‡πâ‡∏ô (‡∏°‡∏≠‡∏á‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô Storage Account ‡πÄ‡∏õ‡πá‡∏ô Drive, Container ‡πÄ‡∏õ‡πá‡∏ô Folder)
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ migrate database ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡πÑ‡∏ß‡πâ ‡∏¢‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà Azure Cloud Database ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô File ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Data-tier Applications (.bacpac) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ query ‡∏Ç‡πâ‡∏≤‡∏° database
‡πÉ‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏ú‡∏°‡∏ó‡∏≥ ‡∏ú‡πà‡∏≤‡∏ô MSSQL Express 2016 + Microsoft SQL Server Management Studio 13 (SSMS)‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Storage Account ‡∏ú‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ download Microsoft Azure Storage Explorer ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
Azure SQL Server ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡πÉ‡∏ô‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° logical database links ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏Ñ‡∏∑‡∏≠ database ‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Server ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏¢‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ query ‡∏Ç‡πâ‡∏≤‡∏° database ‡πÑ‡∏î‡πâ
Azure Database ‡∏à‡∏∞‡∏°‡∏µ Firewall ‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏á‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏≠‡∏á ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ Firewall ‡∏î‡πâ‡∏ß‡∏¢
‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ migrate database ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤ import ‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô storage account ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏™‡πà‡πÑ‡∏ß‡πâ‡πÉ‡∏ô database ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏∂‡πà‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á
‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á Azure database ‡∏à‡∏∞‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ß‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô DTU ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ú‡∏π‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö Database ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏ã‡∏∂‡πà‡∏á ‡∏ñ‡πâ‡∏≤ DTU ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡πá‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏ß‡πà‡∏≤ Database ‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
App Service Plan ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏°‡∏≠‡∏á‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Web Server ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ host web ‡∏¢‡πà‡∏≠‡∏¢‡πÜ‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö (App Service) ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å package ‡πÅ‡∏ö‡∏ö standard ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ Backup web ‡πÅ‡∏•‡∏∞ database ‡πÑ‡∏î‡πâ (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
‡∏Ñ‡∏∑‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÜ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Application ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏ó‡∏µ‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Web ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö (‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏≠‡∏µ‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô mobile app, function app)
‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ deploy web ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ 2 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Ñ‡∏∑‡∏≠
‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£ ‡∏Å‡∏î ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å App Service Menu ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ download public profile
‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ configure App Service ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ backup ‡∏ó‡∏±‡πâ‡∏á web ‡πÅ‡∏•‡∏∞ database ‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏Ñ‡∏£‡∏±‡∏ö ‡πÅ‡∏ï‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏ä‡πâ App Service Plan ‡πÉ‡∏ô package standard ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ
‡∏ó‡∏î‡∏•‡∏≠‡∏á Backup files ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Web API ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô ‡∏ö‡∏ô Azure
‡∏ú‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡∏ß‡πà‡∏≤ ‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏≠‡∏µ‡∏Å 2 ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Azure ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡πÜ‡∏Ñ‡∏£‡∏±‡∏ö
https://www.tt-ss.net/
15 
1

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
15¬†claps
15 
1
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/azure-kubernetes-services-aks-refer%C3%AAncias-gratuitas-e-dicas-para-solu%C3%A7%C3%A3o-de-problemas-comuns-975d6d0b19e8?source=search_post---------334,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 17, 2019¬∑3 min read
Neste post trago diversas refer√™ncias gratuitas (artigos, v√≠deos, slides, projetos de exemplo) que produzi sobre o Azure Kubernetes Service (AKS), alternativa que permite a utiliza√ß√£o do Kubernetes a partir do Microsoft Azure. Al√©m disso, inclu√≠ aqui alguns problemas comuns que podem acontecer quando da utiliza√ß√£o deste servi√ßo.
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem - parte 1
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem - parte 2
O uso do Kubernetes j√° foi tema de 2 eventos online gratuitos do Canal .NET, com a grava√ß√£o dos mesmos estando dispon√≠vel no YouTube e abordando desde os primeiros passos ao deployment automatizado via Azure DevOps (antigo VSTS):
Ao tentar conceder acesso via PowerShell ou bash para utiliza√ß√£o de um cluster criado via Azure Kubernetes Service atrav√©s do comando (considerando um grupo de recursos chamado TesteKubernetes, assim como um recurso do AKS denominado ContagemService):
az aks get-credentials --resource-group TesteKubernetes --name ContagemService
Poder√° ser exibido o erro ‚ÄúA different object named <NOME DO RECURSO> already exists in clusters‚Äù, como indicado na pr√≥xima imagem (provavelmente um recurso com o mesmo nome foi criado antes e posteriormente exclu√≠do):
Para resolver este problema basta apenas acrescentar o par√¢metro --overwrite-existing ao comando mencionado anteriormente:
az aks get-credentials --resource-group TesteKubernetes --name ContagemService --overwrite-existing
√â o que demonstra a imagem a seguir:
Com o comando a seguir (az aks browse) temos acesso ao Dashboard do Kubernetes:
az aks browse -g TesteKubernetes -n ContagemService
Recentemente, no entanto, o acesso a esta fun√ß√£o foi desativado no Azure de forma a restringir o acesso a cluster. Assim, √© comum que sejam apresentados erros como os descritos nas pr√≥ximas imagens:
A instru√ß√£o a seguir permitir√° que se libere o acesso ao Dashboard do Kubernetes:
kubectl create clusterrolebinding kubernetes-dashboard -n kube-system --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
Finalmente teremos sucesso ao acessar o Dashboard do Kubernetes:
AKS troubleshooting | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
22 
22¬†claps
22 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-69f502cf7b44?source=search_post---------335,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 14, 2020¬∑1 min read
3-Parte: Criando a base da projeto
Dando continuidade a libera√ß√£o dos m√≥dulos do meu curso: Criando API‚Äôs RESTful utilizando TypeScript, Node e mongoDB, hoje eu irei demonstrar como criar a base do nosso projeto.
Caso tenha interesse em ver os primeiros passos dessa serie, segue link de cada um deles abaixo:
Cria√ß√£o da base do projeto
Importando os pacotes NPM
Importando os @types
Abaixo voc√™ tem o comando para importar os pacotes NPM no seu projeto e os @types
Configurando o compilador do TypeScript
Link para documenta√ß√£o oficial do arquivo tsconfig.json: Documenta√ß√£o.
configura√ß√£o default que eu utilizo nos meus projetos:
Bom √© isso pessoal, no pr√≥ximo post n√≥s veremos como criar a nossa primeira rota.
Espero que tenham gostado e at√© a pr√≥xima pessoal :)
Enjoy your life
22 
22¬†
22 
Aqui voc√™ vai encontrar os principais conte√∫dos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/vamp-io/canary-releasing-on-kubernetes-from-0-to-100-with-vamp-and-azure-container-service-7260167197d6?source=search_post---------336,"There are currently no responses for this story.
Be the first to respond.
Releasing containerised application workloads on Kubernetes is almost too easy, and Kubernetes comes with some powerful release patterns out of the box. There are also already some great resources out there describing interesting blue/green and canary release deployment scenarios with Kubernetes ‚Äî the official docs have a section that proposes a canary release strategy and this write up also talks about a very similar scenario ‚Äî
In essence, the proposed strategy in most cases is to spin up as much replicas as you need to reflect the user distribution you want, i.e. if you want to have 1:3 of your users hit the ‚Äúcanary‚Äù version of your app and 2:3 hit the ‚Äústable‚Äù version, you spin up two stable versions and one canary version. You then map these replicas to the same (ingress) service and you‚Äôre done.
This completely works and totally valid in small scale, stateless and very general situations. It has its weaknesses however:
So, if you want to take it up a notch and gain more flexibility in programmatic routing and workflow you‚Äôll probably want to checkout Vamp. Luckily, getting up and running with Vamp and Kubernetes is incredibly easy and quick, especially when running on Azure Container Service as this enables some neat and out-of-the-box load balancer and endpoint integration.
In this post we‚Äôll walk you through all the initial steps to get up and running.
Full disclosure: Our dev team is still finalising the full Kubernetes integration. 95% of Vamp‚Äôs feature work extremely nice with Kubernetes, but we have some open bugs that we still need to squash.
Vamp is an open source, self-hosted platform for managing (micro)service oriented architectures that rely on container technology. Vamp provides a DSL to describe services, their dependencies and required runtime environments in blueprints.
Vamp takes care of route updates, metrics collection and service discovery, so you can easily orchestrate complex deployment patterns, such as A/B testing and canary releases.
If you have your Azure account setup and credentials in place, use the following script to bootstrap a Kubernetes cluster and install Vamp. The steps in this script are described below.
Microsoft has done an excellent job in providing a very easy and quick Kubernetes setup with their Azure Container Services. It takes just a handful of commands to get going. You need an active Azure subscription and the Azure command line interface installed to run the below commands
Integrating Vamp into Kubernetes is made delightfully simple using our install script. It talks directly to kubectl and sets up Vamp and its dependencies. Read the full source of the install script here.
The script should finish with the following output and an SSH tunnel on port 8001, connecting you to your Kubernetes host on ACS.
Open a browser, navigate to http://localhost:8001/ui/ and go to the workloads tab. You will a see all Vamp components installed and running.
Vamp‚Äôs UI and API endpoints are available on the external IP and port defined in the Vamp service. Either get it using the following kubectl command or get in from the Kubernetes UI on the ‚ÄúServices‚Äù tab
In our example this means:
Vamp UI: http://13.93.81.196:8080/#/vamp/Vamp API: http://13.93.81.196:8080/api/v1/
We‚Äôre going to perform a simple canary release using the Vamp CLI. First install it and set the VAMP_HOST environment variable to Vamp‚Äôs address.
Use the following script to insert two Vamp blueprints.
Then deploy version 1.0.0 of our simple service‚Ä¶
‚Ä¶and check if our deployment is done.
Vamp integrates directly with Kubernetes LoadBalancer service types by setting up a service and external endpoint. We provide the selector io.vamp.gateway=simple_dep_9050 to filter for the right data where simpleDep is the name we gave to our deployment and 9050 is what we defined as our gateway in the blueprint for simpleservice version 1.0.0. If the EXTERNAL-IP shows <pending> please be patient as the environment bootstraps the necessary infrastructure. Luckily this only needs to happen once as the 9050 is our stable endpoint.
But it doesn‚Äôt end there. The Kubernetes LoadBalancer service is in turn now also integrated into Azure‚Äôs Loadbalancer, creating a load balancing rule and health probe, exposing our service to the internet in a reliable fashion.
Having said all that, our service is now reachable on 13.73.165.183:9050 , as reported by kubectl . Open a browser and you should get:
As you might have noticed, that next to the LoadBalancer service this deployment will also show up as a Kubernetes Deployment, Pod and ReplicaSet. This demonstrates how Vamp uses the native scheduling and resource management of Kubernetes.
The deployment of 1.0.0 is done. We now merge version 1.1.0 into our existing deployment and expose it to 10% of traffic.
We can now have a look at the internal gateway that Vamp has setup and that allows us to migrate traffic to our new version. This internal gateway is completely separate from the external one described above, neatly separating the stable ingress endpoints and the internal dynamic routing.
Now let‚Äôs update the routing and assign 70% to version 1.0.0 and 30% to version 1.1.0.
Now, hitting our endpoint a couple of times should yield the following screen.
You can of course take much smaller steps then 70/30, as long as the number add up to 100. Also, there is no hard limit on the amount of services you split up.
Setting weights on gateways is just one way of doing canary releases. Using Vamp‚Äôs conditions you can influence traffic based on HTTP headers like Cookies, User-Agents etc. As this is not Kubernetes specific we won‚Äôt dive into that in this write up, but here are some links for further reading:
Vamp.io
58 
58¬†claps
58 
Vamp.io - Smart & stress free application releasing for modern cloud platforms.
Written by
Code and Product. Writing about solopreneurship, Javascript and containers. Founder at checklyhq.com
Vamp.io - Smart & stress free application releasing for modern cloud platforms.
"
https://medium.com/wso2-learning/wso2-deployment-reference-architecture-on-azure-33fe8f218cbf?source=search_post---------337,"There are currently no responses for this story.
Be the first to respond.
A reference deployment of WSO2 APIM and EI on Microsoft Azure cloud platform
WSO2 is an open-source, enterprise middleware platform that offers software products for API Management, Integration and Identity, and Access Management. WSO2 products can be deployed on infrastructure platforms such as
‚Äî Azure
‚Äî AWS
‚Äî GCP
In addition to these customer-managed infrastructure platforms, WSO2 also has a fully managed cloud version of the products as Software as a Service (SaaS) offering. Having said that, Azure is one of the upcoming cloud infrastructure platforms for deploying applications on the cloud. Hence we will be discussing how to deploy the WSO2 platform on top of the Azure.
Before moving on to the actual deployment details, let‚Äôs discuss some of the concepts of Azure that we will be using for the deployment architecture. Azure provides a plethora of services for application development and deployment. For this article, we will be using the below-mentioned services.
Azure Virtual Machines provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for applications. It provides a comprehensive computing environment with a choice of processor, storage, networking, operating system, and purchasing model. It offers computing resources available from a variety of geographical locations across the globe using its regions and zones. It also provides automatic scalability of computing resources within minutes.
Microsoft Azure services are available globally to run business applications at an optimal level. Users can choose the best region based on their needs based on
Regions and AZs allow users to utilize the cloud platform in the best possible way for their applications.
A region is a set of datacenters deployed within a latency-defined perimeter and connected through a dedicated regional low-latency network. Azure gives you the flexibility to deploy applications where you need to, including across multiple regions to deliver cross-region resiliency. Azure maintains multiple geographic regions, including
Azure opens new regions rapidly based on user demand.
An Availability Zone is a high-availability offering that protects your applications and data from datacenter failures. Availability Zones are unique physical locations within an Azure region. Each zone is made up of one or more datacenters equipped with independent power, cooling, and networking. To ensure resiliency, there‚Äôs a minimum of three separate zones in all enabled regions. The physical separation of Availability Zones within a region protects applications and data from datacenter failures. Zone-redundant services replicate your applications and data across Availability Zones to protect from single-points-of-failure. With Availability Zones, Azure offers industry best 99.99% VM uptime SLA.
An availability set is a logical grouping of VMs that allows Azure to understand how your application is built to provide for redundancy and availability. We recommended that two or more VMs are created within an availability set to provide for a highly available application and to meet the 99.95% Azure SLA. There is no cost for the Availability Set itself, you only pay for each VM instance that you create.
Azure Virtual Network (VNet) is the fundamental building block for your private network in Azure. VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks. VNet is similar to a traditional network that you‚Äôd operate in your own data center, but brings with it additional benefits of Azure‚Äôs infrastructure such as scale, availability, and isolation. Key use cases of VNets include
A subnet is a range of IP addresses in the VNet. You can divide a VNet into multiple subnets for organization and security. Each network interface(NIC) in a VM is connected to one subnet in one VNet. NICs connected to subnets (same or different) within a VNet can communicate with each other without any extra configuration. By default, there is no security boundary between subnets, so VMs in each of these subnets can talk to one another. However, you can set up Network Security Groups (NSGs), which allow you to control the traffic flow to and from subnets and to and from VMs.
A network security group (NSG) contains a list of Access Control List (ACL) rules that allow or deny network traffic to subnets, NICs, or both. NSGs can be associated with either subnets or individual NICs connected to a subnet. When an NSG is associated with a subnet, the ACL rules apply to all the VMs in that subnet. In addition, traffic to an individual NIC can be restricted by associating an NSG directly to a NIC.
NSGs contain two sets of rules: inbound and outbound. The priority for a rule must be unique within each set. Each rule has properties of protocol, source and destination port ranges, address prefixes, direction of traffic, priority, and access type. All NSGs contain a set of default rules. The default rules cannot be deleted, but because they are assigned the lowest priority, they can be overridden by the rules that you create.
Azure offers a choice of fully managed relational, NoSQL, and in-memory databases, spanning proprietary and open-source engines, to fit the needs of modern app developers. Infrastructure management ‚Äî including scalability, availability, and security ‚Äî is automated, saving you time and money. Focus on building applications while Azure managed databases make your job simpler by surfacing performance insights through embedded intelligence, scaling without limits, and managing security threats. The available database services include
Out of the above database types, Azure SQL database is a fully-managed database that provides the SQL server capabilities to the user.
Take advantage of fully managed file shares in the cloud that are accessible via the industry-standard SMB and NFS protocols. Azure file shares can be mounted concurrently by cloud or on-premises deployments of Windows, Linux, and macOS. Azure file shares can also be cached on Windows Servers with Azure File Sync for fast access near where the data is being used.
The term load balancing refers to the distribution of workloads across multiple computing resources. Load balancing aims to optimize resource use, maximize throughput, minimize response time, and avoid overloading any single resource. It can also improve availability by sharing a workload across redundant computing resources. Azure provides various load balancing services that you can use to distribute your workloads across multiple computing resources ‚Äî Application Gateway, Front Door, Load Balancer, and Traffic Manager.
Azure Load Balancer is a high-performance, ultra low-latency Layer 4 load-balancing service (inbound and outbound) for all UDP and TCP protocols. It is built to handle millions of requests per second while ensuring your solution is highly available. Azure Load Balancer is zone-redundant, ensuring high availability across Availability Zones. It‚Äôs the single point of contact for clients. Load balancer distributes inbound flows that arrive at the load balancer‚Äôs front end to backend pool instances. These flows are according to configured load-balancing rules and health probes. The backend pool instances can be Azure Virtual Machines or instances in a virtual machine scale set.
Azure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. Traditional load balancers operate at the transport layer (OSI layer 4 ‚Äî TCP and UDP) and route traffic based on source IP address and port, to a destination IP address and port.
Application Gateway can make routing decisions based on additional attributes of an HTTP request, for example URI path or host headers. For example, you can route traffic based on the incoming URL. So if /images is in the incoming URL, you can route traffic to a specific set of servers (known as a pool) configured for images. If /video is in the URL, that traffic is routed to another pool that‚Äôs optimized for videos.
As per this article, we will be deploying WSO2 API Manager and WSO2 Enterprise Integrator along with the respective Analytics components on Azure infrastructure. Each product and component is deployed with the minimum high availability.
‚Äî 2 worker nodes (active/passive)
‚Äî 1 dashboard nodes (active)
In addition to the WSO2 products, we will be utilizing the following Azure services
Let‚Äôs get on with the deployment architecture.
The preceding figure depicts the deployment architecture of WSO2 APIM, EI, and Analytics on the Azure platform. The deployment is designed so that it is deployed within a single Azure region. The entire deployment resides within a VNet inside the Azure cloud. There are private subnets created for WSO2 product deployment as well as for databases and deployment management tools (puppet).
A public subnet is created to host the publicly accessible components. This public subnet is considered as the DMZ and the following components are deployed within this subnet.
WSO2 API Manager deployment
Two instances of WSO2 APIM are deployed for high availability within an availability set. Azure availability set provides the required redundancy in case there is a failure of one node. For the deployment, the VM instance of type B2MS is used. This can be different based on the capacity requirements of the use case. The API gateway is exposed through the Azure classic LB which is deployed in the public subnet (DMZ). In addition to that, other administrative interfaces such as publisher, carbon, and admin portals are exposed through the Azure application gateway which is deployed in the public subnet and protected via firewall rules. This is accessed by the internal organization staff members who manage the deployment. For sharing the API definitions and throttling policies across APIM instances, Azure files is utilized by the APIM deployment. This deployment is connected to the Azure SQL database service running within the same VNet.
WSO2 Enterprise Integrator deployment
Two instances of WSO2 EI are deployed for high availability within an availability set. For the deployment, the EC2 instance of type B2MS is used. This can be different based on the capacity requirements of the use case. The carbon portal is exposed through the application gateway. The services that are running on WSO2 EI will be exposed to the API Manager and other systems via Azure classic load balancer. If needed, a separate classic LB can be setup within the private subnet for communicating to the EI nodes from APIM and other services. If there is a need to connect with on-premise systems such as databases, ERP, CRM from WSO2 EI, a VPN connection can be utilized as depicted in the above diagram.
WSO2 API Manager/EI Analytics deployment
The analytics component comes with 2 profiles called worker and dashboard. Worker profile receives the analytics events from gateways (APIM) and do the real-time processing of data and store the results in the analytics database. Then the dashboard profile reads the processed data from the database and visualizes it in dashboards. The worker requires 2 nodes of active/passive deployment for high availability and those are deployed in an availability set. The dashboard profile is okay to deploy as a single instance given the lower importance of the tool. All 3 nodes are connected to the database that is running on the same VNet but in a different private subnet. The analytics dashboard is exposed through the application gateway which is running on the public subnet. Azure VM type of F4s v2 is used for workers since it does heavy data processing as and when events are received from the gateways. A VM of type F2s v2 is used for the dashboard profile since it does not have heavy processing requirements.
Learn about WSO2 technology best practices from experts
25 
25¬†claps
25 
This publication contains best practices, examples and guidelines for WSO2 users.
Written by
Engineer | Author | Speaker | Associate Director @ WSO2
This publication contains best practices, examples and guidelines for WSO2 users.
"
https://itnext.io/easily-deploy-containers-to-azure-directly-from-your-desktop-16efebc87b21?source=search_post---------338,"Containers are now a mature solution providing an additional level of infrastructure abstraction. In many cases, containers can replace workloads traditionally powered by virtual machines.
In this blog, we are going to look at Azure Container Instances and showcase how fast and easy it is to deploy containers directly from your docker CLI to Azure.
If you would like to follow along, you will need to have Azure subscription, Azure CLI and Docker Desktop instance.
Azure Container Instances is a compute offering that bridges the gap between lightweight Azure Functions and more complex, but fully fledged Azure Kubernetes Service.
ACI is best suited for containerized workloads that can operate in isolation, simple apps, batch jobs including data science models, all kinds of tasks automation and integration scenarios.
We are going to deploy a sample web page. The idea is that with docker CLI and ACI we can rapidly prototype, test and deploy directly from docker command line!
Important node: this flow is only for testing purposes, in real code scenario you would have CI/CD pipeline deploying your app for you.
We are going to use bash, but the same is of course possible with powershell.
Docker CLI contains now build-in integration with Azure Container Instances through a context command. When using Azure CLI, you cat activate Azure Interactive by typing az interactive. This is an experimental feature of Azure CLI which gives you parameters completion and more!
First let‚Äôs setup variables and authenticate with Azure using docker CLI
This command is interactive and will prompt you to select subscription, resource group (create or select existing one) and location. Make sure to note resource group name if you create a new one, so later it‚Äôs easy to cleanup resources.
Now let‚Äôs deploy a test container!
We‚Äôve see how easy it is to deploy a container group directly to Azure Container Instances. This could be very useful for testing purposes and quick inner development loop.
This blog barely scratches the surface of what Azure Container Instances can do and how to integrate developer workflow. In my opinion Azure Container Instances is one of the most flexible and powerful serverless offerings in Azure.
There are a lot of great blogs and tutorials to check if you are interested to learn more.
ITNEXT is a platform for IT developers & software engineers‚Ä¶
62 
62¬†claps
62 
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/microsoftazure/fast-loading-data-into-azure-sql-a-lesson-learned-72ff0bc6d0c8?source=search_post---------339,"There are currently no responses for this story.
Be the first to respond.
I‚Äôm preparing a series of post and samples on how to properly load data into Azure SQL using Azure Databricks / Apache Spark that I will start to publish very soon, but I realized today that there is a pre-requisite that in many cases, especially by developers new to the data space, is overlooked: good table design.
Wait! If you‚Äôre not a Apache Spark user you might think this post is not for you. Please read on, it will be just a couple of minutes, and you will find something help also for you, I promise.
By good table design, I don‚Äôt mean, in this case, normalization, research of the best data type or any other well-known technique‚Ä¶no, nothing like that. They are still absolutely useful and encouraged, but let‚Äôs leave them aside for now, and let‚Äôs focus on something much simpler.
Simpler but that, in the case I used to build the aforementioned samples, had an impact of 300%. Right, 300%. By changing a very simple thing I could improve (or worsen, depending on where you are starting from) performance by 3 times.
The focal point to understand is that if we overlook something, maybe very small, when data has grown that little mistake or less-than-good decision may have its overhead applied to all the rows in your table. As a result of this it may happen that something negligible in terms of performance impact will instead become very bad once the data amount start to increase. This is the main challenge with database. Data grows and changes.
In the specific sample I want to discuss now, it turns out that using a correct data type for storing string data matters a lot. If you are a .NET developer you probably are already used to take advantage of StringBuilder instead of simpler String objects if you need to create or manipulate a string for several thousand or millions of times. StringBuilder is much more optimized to do this, even if you could obtain the same result just by using String objects.
In Azure SQL you can choose to use varchar(max) or varchar(n). With varchar(max) you can store up to 2GB of data. With varchar(n) you can store up to n bytes and anyway no more than 8000. The same logic applies to nvarchar (with limit now set to max 4000 chars as they use 2 bytes per char), but in this case strings will use UTF-16 encoding.
If you are a developer approaching the data world, you might have the impression that specifying the size of a string is something that only in ancient times made sense. More or less like a nibble, if you ever heard of this curious half-byte thing.
Now that we have gigabytes almost for free, specifying the string size should not matter. With this mindset, a table like the following would look absolutely adequate (for simplicity I‚Äôm using a table from TPC-H benchmark, so pardon the weird columns names):
All the strings in the table are treated like strings in any other language (C#, Python and similar) where you assume strings can have an almost infinite length.
In the database space things are quite different. To reduce I/O to a minimum (as accessing data outside RAM is still the slowest operation you can think of), several optimizations are done, so that with just one I/O you can read many rows at once. That‚Äôs why, in fact, the concept of data page exists.
If a string is known not to be potentially infinite, some other optimizations can be done. So, if we know what could be the potential maximum ‚Äî or even the exact size ‚Äî of a string, we could specify it, helping the database to optimize things a bit. For example, like in the following table, which is exactly the table shown before but with more precise string type definition:
This last table is much better from a physical design point of view. And you can see this yourself by loading data using Azure Databricks, for example.
Loading data into the first table will require something like 7 minutes for 9 GB of data, while on the second, exactly the same data will require only 2.5 minutes.
Loading data into a table that has been better designed (from a physical modeling point of view) is 3 time faster then loading data in a table not so well optimized.
Given that we only had to correctly set maximum string length, I‚Äôd say the optimization is totally worth the effort!
I did my experiments on Azure SQL Hyperscale Gen5 8vCore and with Azure Databricks 6.6 (Spark 2.4.5, Scala 2.11), 4 Workers each with 4 nodes, for a total of 16 workers that were loading data in parallel into Azure SQL.
So, the first step to make sure you can load your data as fast as possible is to create the table using the most suitable data type, especially when you are dealing with strings. In the past we used nibbles to spare memory as much as possible, as it was scarce.
Now we live in the time of plenty, but we generate and operate on huge amounts of data, so every byte wasted means wasting CPU cycles, network bandwidth, I/O bandwidth, cache and memory. Wasting something tiny ‚Äî in this specific case the bad design impacted only for 0.000005 seconds on a single row ‚Äî for 59,692,838 times (that‚Äôs the number of rows in my sample) still result in a huge impact, not matter what. So, let‚Äôs start by not doing that: you‚Äôll never know when your table will reach the size where even a single byte will be critical, so better be prepared.
There is also another nice aspect of this little optimization we just have done. As a result of a better physical design, reads will also be much faster too, exactly for the same reasons explained before. So spending just a few minutes to think about our model instead of just throwing some code here and there is a win-win.
Keep this in mind for your next table!
‚Äî
Photo by panumas nikhomkhai from Pexels
Any language.
49 
49¬†claps
49 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@michaelhenderson/how-to-download-your-source-code-from-azure-app-service-59c848752b0f?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Henderson
Oct 22, 2018¬∑3 min read
If you are like me you most likely have all of your eggs in one basket. For example, I have one laptop that is my daily driver. That laptop has everything I need on it including all the repositories that I contribute to. Well, the hard drive in my laptop decided to die the other day and I was not able to get my source code to one particular project that I have spend a lot of time on. The project I am speaking of is an API that I am running in Azure.
This API is one that I did not have under source control. I would make changes/update‚Ä¶
"
https://medium.com/@tsuyoshiushio/terragrunt-quick-start-with-azure-4a9a09ab2e21?source=search_post---------341,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jan 11, 2020¬∑5 min read
Terragrunt is a great tool to help our terraform files keep DRY. However, the tutorial is written for AWS. For my learning, I tried to do it for Azure.
If you want to see the original, please see the Quick start.
terragrunt.gruntwork.io
Download two binaries and put it to your path environment variables.
Download/Install AzureCLI then login it to your subscription. This command will open your browser. For Linux, use --use-device option.
Terragrunt is a thin wrapper that provides extra tools for keeping your configurations DRY, working with multiple Terraform modules, and managing remote state.
A project that I‚Äôm joining has several modules and environment. I can see the duplication between the modules and environment. terragrunt might help.
Where is the sample repo?
You can find a whole sample code terragrunt with Azure.
github.com
The repo has two modules. resource-group and virtual-network . You want to share something between two modules for making these D.R.Y. You might notice thatterragrunt.hcl These are the config files for terragrunt.
In this example, I‚Äôm sharing two things between two modules.
If you want to share the backend configration, , you can edit terragrunt.hcl under stage directory. That has two modules underneath.
In this example, I use storage account for the backend. You need to create a storage account by your self on Azure Portal. Please edit this file to fit your storage account name and resource group name.
backend is a feature to store terraform state in somewhere. We use storage account for Azure.
stage/terragrunt.hcl
stage/resource-group/terragrunt.hcl
As an example, let‚Äôs see the resource-group module configuration. This config is simply inherit the parent folder‚Äôs terragrunt.hcl
stage/resource-group/main.tf
The point is terraform part. You can see there is no config at all. terragrunt inject the info from terragrunt.hcl configuration.
Terragrunt is a wrapper of terraform. So all of the command is just by-passed with terraform with the configuration change.
if you want to do terrafrom plan you can simply do terragrunt plan apply is the same as well.
Then aresource_group is might created.
Sometimes, you might want to share some input variables among the modules.
Currently, resource-group module has these two variables.
Go to the parent configration.
stage/terragrunt.hcl
You can find these lines for share the input valuables.
Modules should be small, even if the whole infrastructure is big. However, small modules introduce duplication. This is the structure that is introduced at the original tutorial.
Terragrunt help you to remove the duplication with immutable, versioned, artifacts.
Create a re-usable module. It also might need the config of backend. However, we don‚Äôt need specify the specific values. I create a sample module repo.
github.com
The repo‚Äôs structure is
app/main.tf
Sample module definition. As you can see backend has no configration, and requires variables for project specific parameters.
Push your module somewhere github. I push my https://github.com/TsuyoshiUshio/infrastructure-module repo with Tag.
I made some mistakes, so, now it is version v0.0.3
For the project side, create the same structure as the module repo. I add the app directory with terragrunt.hcl
terragrunt.hcl
You can re-use the module with version.
Go to the app directory and plan/apply it. You can re-use the module with version!
You will see it is success fully works. and You will find .terragrunt-cache directory is created under app . Don‚Äôt forget to add it to .gitignore
I just want to share a sample with Azure.
github.com
For more details, please refer terragrunt .
terragrunt.gruntwork.io
Enjoy deployment.
Senior Software Engineer ‚Äî Microsoft
22 
2
22¬†
22 
2
Senior Software Engineer ‚Äî Microsoft
"
https://itnext.io/introduction-to-azure-functions-using-terraform-eca009ddf437?source=search_post---------342,"In this article I will provide a quick introduction to Azure Functions and Terraform with a hands-on example which is quite easy to follow.
There are many articles out there about Azure Functions, but most fails to explain how to automate their deployment for a real world scenario using CI/CD pipelines.
"
https://medium.com/bb-tutorials-and-thoughts/how-to-build-ci-cd-for-python-azure-functions-using-azure-devops-7087a76e535b?source=search_post---------343,"There are currently no responses for this story.
Be the first to respond.
An Azure Function is a simple way of running small pieces of code in the cloud. You don‚Äôt have to worry about the infrastructure required to host that code. You can write the Function in C#, Java, JavaScript, PowerShell, Python, or any of the languages that are listed in the‚Ä¶
"
https://blog.helium.com/from-zero-to-azure-iot-in-five-minutes-f32d74c82b9b?source=search_post---------344,"Can we go from unboxing new Helium hardware to sending hardware-encrypted data to an Azure IoT Hub in under five minutes?
Securing your devices and sensors should be top of mind when considering any IoT wireless and cloud platform. Sound, proven hardware-based encryption and authentication practices specifically are what should set an IoT service apart. X-509 certificates (and their sister mechanism JSON Web Tokens) are the standard when it comes to device-level authentication for big-ticket cloud providers. Both Azure IoT and AWS IoT both use X-509 certificates as their gold standard when authenticating devices. And Google Cloud IoT does the same with JSON Web Tokens.
But building full X-509 cert-based encryption and authentication into a low-power embedded device and securely managing the lifecycle of these certificates can be cumbersome for even the most capable developers. For starters, here‚Äôs how Azure IoT recommends you implement security at the device level. To really drive home how hard this is to do on your own with a wireless device like the Atom on a LPWAN like Helium, your end device needs a full TLS stack, a client certificate, and a MQTT client. And you need to keep the MQTT connection alive between the device and the cloud. The radio usage alone will crush your battery and this assumes your sensor is driven by a sufficient MCU. The processing and networking requirements are enough that the official Azure IoT Arduino Library doesn‚Äôt support the Arduino UNO. And users are having issues with it (to say the least).
So, to abstract this type of complexity, we built Helium Channels. Channels are prebuilt connections to cloud services like Azure IoT, AWS IoT, and Google Cloud IoT. (You can see a full list of Helium Channels available here.) They handle all the heavy lifting: from hardware secured certificate generation and management to integration with cloud device management APIs and everything in between.
To demonstrate the power of Channels, we thought we would see just how quickly a developer could go from unboxing their Helium Starter Kit to sending fully-encrypted and authenticated data to Azure IoT using an Helium Atom riding on an Arduino UNO. Can we do it in five minutes? Let‚Äôs give it a shot. And jump to the bottom if you want to get your hands on the hardware to replicate this yourself.
Want to replicate this yourself? Here‚Äôs what you need:
Building the world‚Äôs first decentralized wireless network
162 
2
162¬†claps
162 
2
Written by
VP of Business Development at @helium; Basho OG. - mark@helium.com
Building the world‚Äôs first decentralized wireless network
Written by
VP of Business Development at @helium; Basho OG. - mark@helium.com
Building the world‚Äôs first decentralized wireless network
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/aspnetrun/deploying-net-microservices-to-azure-kubernetes-services-aks-and-automating-with-azure-devops-c50bdd51b702?source=search_post---------345,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mehmet √ñzkaya
Jan 13, 2021¬∑6 min read
In this article, we‚Äôre going to learn how to Deploying .Net Microservices into Kubernetes, and moving deployments to the cloud Azure Kubernetes Services (AKS) with using Azure Container Registry (ACR) and last section is we will learn how to Automating Deployments with Azure DevOps and GitHub.
The image above, you can find the steps of our article structure.We‚Äôre going to containerize our microservices on docker environment, and push these images to the DockerHub and deploy microservices on Kubernetes. As the same setup, we are going to shifting to the cloud for deploying Azure Kubernetes Services (AKS) with pushing images to Azure Container Registry (ACR).
Also we will cover additional topics that;
I have just published a new course ‚Äî Deploying .Net Microservices with K8s, AKS and Azure DevOps.
In this course, we‚Äôre going to learn how to Deploying .Net Microservices into Kubernetes, and moving deployments to the cloud Azure kubernetes services (AKS) with using Azure Container Registry(ACR) and last section is we will learn how to Automating Deployments with CI/CD pipeline of Azure DevOps and GitHub.
See the overall picture. You can see that we will have 3 microservices which we are going to develop and deploy together.
First of all, we are going to develop Shopping MVC Client Application For Consuming Api Resource which will be the Shopping.Client Asp.Net MVC Web Project. But we will start with developing this project as a standalone Web application which includes own data inside it. And we will add container support with DockerFile, push docker images to Docker hub and see the deployment options like ‚ÄúAzure Web App for Container‚Äù resources for 1 web application.
After that we are going to develop Shopping.API Microservice with MongoDb and Compose All Docker Containers. This API project will have Products data and performs CRUD operations with exposing api methods for consuming from Shopping Client project.We will containerize API application with creating dockerfile and push images to Azure Container Registry.
Our API project will manage product records stored in a no-sql mongodb database as described in the picture. we will pull mongodb docker image from docker hub and create connection with our API project.
At the end of the section, we will have 3 microservices whichs are Shopping.Client ‚Äî Shopping.API ‚Äî MongoDb microservices.
As you can see that, we have
And the last step, we are focusing on automation deployments with creating CI/CD pipelines on Azure Devops tool. We will develop separate microservices deployment pipeline yamls with using Azure Pipelines.When we push code to Github, microservices pipeline triggers, build docker images and push the ACR, deploy to Azure Kubernetes services with zero-downtime deployments.
By the end of this articles, you‚Äôll learn how to deploy your multi-container microservices applications with automating all deployment process seperately.
This is the introduction of the series. This will be the series of articles. You can follow the series with below links.
Get the Source Code from AspnetRun Microservices Github ‚Äî Clone or fork this repository, if you like don‚Äôt forget the star :) If you find or ask anything you can directly open issue on repository.
Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly.
Advantages of Docker‚Äôs methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. Docker provides for automating the deployment of applications as portable, self-sufficient containers that can run on the cloud or on-premises. Docker containers can run anywhere, in your local computer to the cloud. Docker image containers can run natively on Linux and Windows.
A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application.
When using Docker, a developer develops an application and packages it with its dependencies into a container image. An image is a static representation of the application with its configuration and dependencies.
In order to run the application, the application‚Äôs image is instantiated to create a container, which will be running on the Docker host. Containers can be tested in a development local machines.
As you can see the images above, how docker components related each other.Developer creates container in local and push the images the Docker Registry. Or its possible that developer download existing image from registry and create container from image in local environment.
Developers should store images in a registry, which is a library of images and is needed when deploying to production orchestrators. Docker images are stores a public registry via Docker Hub; other vendors provide registries for different collections of images, including Azure Container Registry. Alternatively, enterprises can have a private registry on-premises for their own Docker images.
If we look at the more specific example of Application Containerization with Docker;
We will use all steps with orchestrating whole microservices application with docker and Kubernetes for the next articles ->
https://docs.docker.com/get-started/overview/https://docs.docker.com/get-started/https://medium.com/batech/docker-nedir-docker-kavramlar%C4%B1-avantajlar%C4%B1-901b37742ee0https://www.mediaclick.com.tr/tr/blog/docker-nedir-docker-ne-ise-yararhttps://www.docker.com/resources/what-containe
I‚Äôm currently working as a Software Architect. Focus on microservices architectures on .Net https://github.com/mehmetozkaya
See all (319)
30 
30¬†
30 
The best path to leverage your aspnet skills. Onboarding to¬†.Net Software Architect jobs. Developing production-ready enterprise¬†.Net applications with applying latest architectures and best practices.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/build-a-serverless-app-using-go-and-azure-functions-c4475398f4ab?source=search_post---------346,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Webhook backend is a popular use case for FaaS (Functions-as-a-service) platforms. They could be used for many use cases such as sending customer notifications to responding with funny GIFs! Using a Serverless function, it‚Äôs quite convenient to encapsulate the webhook functionality and expose it in the form of an HTTP endpoint. In this tutorial you will learn how to implement a Slack app as a Serverless backend using Azure Functions and Go. You can extend the Slack platform and integrate services by implementing custom apps or workflows that have access to the full scope of the platform allowing you to build powerful experiences in Slack.
This is a simpler version of the Giphy for Slack. The original Giphy Slack app works by responding with multiple GIFs in response to a search request. For the sake of simplicity, the function app demonstrated in this post just returns a single (random) image corresponding to a search keyword using the Giphy Random API. This post provides a step-by-step guide to getting the application deployed to Azure Functions and integrating it with your Slack workspace.
In this post, you will:
The backend function logic is written in Go (the code is available on GitHub. Those who have worked with Azure Functions might recall that Go is not one of the language handlers that is supported by default. That‚Äôs where Custom Handlers come to the rescue!
In a nutshell, a Custom Handler is a lightweight web server that receive events from the Functions host. The only thing you need to implement a Custom Handler in your favorite runtime/language is: HTTP support! This does not mean that Custom handlers are restricted to HTTP triggers only ‚Äî you are free to use other triggers along with input and output bindings via extension bundles.
Here is a summary of how Custom Handlers work at a high level (the diagram below has been picked from the documentation)
An event trigger (via HTTP, Storage, Event Hubs etc.) invokes the Functions host. The way Custom Handlers differ from traditional functions is that the Functions host acts as a middle man: it issues a request payload to the web server of the Custom Handler (the function) along with a payload that contains trigger, input binding data and other metadata for the function. The function returns a response back to the Functions host which passes data from the response to the function‚Äôs output bindings for processing.
Before we dive into the other areas, it might help to understand the nitty gritty by exploring the code (which is relatively simple by the way)
Let‚Äôs look at the how the app is setup. this is as defined in the doc
cmd/main.go sets up and starts the HTTP server. Notice that the /api/funcy endpoint is the one which the Function host sends the request to the custom handler HTTP server.
All the heavy lifting is done in function/function.go.
The first part is to read the request body (from Slack) and ensure its integrity via a signature validation process based on this recipe defined by Slack.
Once we‚Äôve confirmed that the function has indeed being invoked via Slack, the next part is to extract the search term entered by the (Slack) user
Look up for GIFs with the search term by invoking the GIPHY REST API
Un-marshal the response sent back by the GIPHY API, convert it into a form which Slack can make sense of and return it. That‚Äôs it !
Check the matchSignature function if you're interested in checking the signature validation process and look at slack.go, giphy.go (in the function directory) to see the Go structs used represent information (JSON) being exchanged between various components. These have not been included here to keep this post concise.
Alright! So far, we have covered lots of theory and background info. It‚Äôs time to get things done! Before you proceed, ensure that you take care of the below mentioned pre-requisites.
Please note down your GIPHY API key as you will be using it later
The upcoming sections will guide you through the process of deploying the Azure Function and configuring the Slack for the Slash command.
Start by creating a Resource Group to host all the components of the solution.
Start by searching for Function App in the Azure Portal and click Add
Enter the required details: you should select Custom Handler as the Runtime stack
In the Hosting section, choose Linux and Consumption (Serverless) for Operating system and Plan type respectively.
Enable Application Insights (if you need to)
Review the final settings and click Create to proceed
Once the process is complete, the following resource will also be created along with the Function App:
Clone the GitHub repo and build the function
GOOS=linux is used to build a Linux executable since we chose a Linux OS for our Function App
To deploy, use the Azure Functions core tools CLI
Once you‚Äôve deployed, copy the function URL that‚Äôs returned by the command ‚Äî you will use it in subsequent steps
This section will cover the steps you need to execute to setup the Slack application (Slash command) in your workspace:
Sign into your Slack Workspace and start by creating a new Slack App
Click on Create New Command to define your new Slash Command with the required information. Please note that the Request URL field is the one where you will enter the HTTP endpoint of function which is nothing but the URL you obtained after deploying the function in the previous section. Once you‚Äôre done, hit Save to finish.
Once you‚Äôre done creating the Slash Command, head to your app‚Äôs settings page, click the Basic Information feature in the navigation menu, choose Install your app to your workspace and click Install App to Workspace ‚Äî this will install the app to your Slack workspace to test your app and generate the tokens you need to interact with the Slack API. As soon as you finish installing the app, the App Credentials will show up on the same page.
Make a note of your app Signing Secret as you‚Äôll be using it later
‚Ä¶ make sure to update the Function App configuration to add the Slack Signing Secret (SLACK_SIGNING_SECRET) and Giphy API key (GIPHY_API_KEY) - they will be available as environment variables inside the function.
From your Slack workspace, invoke the command /funcy <search term>. For e.g. try /funcy dog. You should get back a random GIF in return!
Just a recap of what‚Äôs going on: When you invoke the /funcy command in Slack, it calls the function, which then interacts Giphy API and finally returning the GIF to the user (if all goes well!)
You may see timeout error from Slack after the first invocation. This is most likely due to the cold start where the function takes a few seconds to bootstrap when you invoke it for the very first time. This is combined with the fact that Slack expects a response in 3 seconds - hence the error message.
There is nothing to worry about. All you need is to retry again and things should be fine!
Clean up: Once you‚Äôre done, don‚Äôt forget to delete the resource group which in turn will delete all the resources created before (Function app, App Service Plan etc.)
Now, there is nothing stopping you from using Go for your Serverless functions on Azure! I hope this turns out to be a fun way to try out Custom Handlers. Let us know what you think.
ITNEXT is a platform for IT developers & software engineers‚Ä¶
57 
57¬†claps
57 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/protecting-against-malicious-payloads-over-dns-using-azure-sentinel-b16b41de52fd?source=search_post---------347,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 26, 2019¬∑7 min read
No matter how tightly you control your network, you probably allow DNS queries and UDP/53 traffic on your network.
Bad actors can abuse this to establish a stealthy command & control (C2) channel and/or exfiltrate data using DNS tunneling.
Azure Sentinel can help detect these types of attacks, and provide insights in the various stages of the kill chain of this attacker.
Delivering a payload over DNS
Samy Baiwir recently published a project on GitHub called DNSlivery that aims to deliver payloads over DNS. It is a lightweight solution built on Python and the Scapy library.
No need for a full-fledged DNS server, DNSlivery will listen on UDP/53 and serve the payload through TXT records. Here‚Äôs the GitHub repository:
Use DNSlivery to bootstrap DNS tunneling
How is DNSlivery different from for instance PowerDNS? With DNSlivery there is no need for a client on the target, you can just use native PowerShell in the operating system.
However, this does mean that it is one-way communication. But because it will not touch on disk, it can help bootstrap the next phase of your attack, DNS tunneling:
‚ÄúEven though more complete DNS tunneling tools already exist (fi: dnscat2 and iodine), they all require to run a dedicated client on the target. The problem is that there is probably no other way then DNS to deliver the client in such restricted environments. In other words, building a DNS communication channel with these tools require to already have a DNS communication channel.
In comparison, DNSlivery only provides one-way communication from your server to the target but does not require any dedicated client to do so. Thus, if you need to build a reliable two-way communication channel over DNS, use DNSlivery to deliver the client of a more advanced DNS tunneling tool to your target.‚Äù
Setting up the DNS server
You will of course need a domain name and be able to change and administer the NS records. Point the NS record(s) to the external IP of the machine you‚Äôll be using for this.
To set up the rogue DNS server itself you need a Linux operating system instance. You will also need Python3 and the right version of the Scapy library (v2.4.0).
DNSlivery makes things easy for you, just run these commands:
Create a directory on disk that has the file that contains the payload you want to serve over DNS. In this sample we‚Äôll be serving ‚Äúatp-cat.txt‚Äù with an ASCII picture of ATP cat.
Run the following command:
PRO TIP: If you‚Äôre getting errors that Python can‚Äôt load the Scapy module, make sure that sudo has the same environment variables as your current user. If not, use this command to solve that, before your run DNSlivery: alias sudo=‚Äôsudo env PATH=$PATH‚Äô
Consuming the payload on the target
As mentioned earlier, DNSlivery is different from other solutions in that it does not require a dedicated client. The content can be consumed by Powershell out of the box.
On the target, start by retrieving the launcher of the desired file by requesting its dedicated TXT record. The following three launchers are supported:
Copy and paste the response into the PowerShell console again, to retrieve the payload over DNS:
How can I defend against these types of attacks?
While most companies will have some form of network security solution in place that might already trigger on these types of attacks, Azure Sentinel can also play a role in detecting this malicious intent.
Looking at the Security eventlog on this specific server we find an event with ID 4688 that shows us the execution of the initial ‚Äòdnslookup‚Äô:
If we execute a Kusto (KQL) query on the Log Analytics workspace that this agent is connected to, we quickly find the event:
Capturing the malicious transfer itself
When we dig further, there is no logged event in the Windows security eventlog on the next stage of the attack that receives the multi-part base64 encoded payload. Because it uses Invoke-Expression (IEX) it does not spawn a new (child) process and therefore no extra event is created in the security eventlog.
However, we can use the DNS client logging in Windows. That is not enabled by default, but enterprises could (and should?) enable this:
You will see the following events appear in the DNS client log:
PRO TIP: Type 16 is the TXT record in DNS. There is a Wikipedia page that outlines all the various types and Type ID‚Äôs: https://en.wikipedia.org/wiki/List_of_DNS_record_types
Collecting the Powershell log
Going into Workspace Settings in Azure Sentinel, you will find Advanced Settings of the Log Analytics workspace associated and it will allow you to collect the DNS client log:
It will take a couple of minutes to start gathering and processing this new log, but directly afterwards you can query the data in Kusto format, returning the malicious command:
Azure Sentinel
The next step is to create a Case trigger in Azure Sentinel. This can be done through the ‚ÄòAnalytics‚Äô section:
Here are the properties you can use:
PRO TIP: make sure you map the entities as part of the rule creation ‚Äî this is a very important step since this will enable the investigation experience that will be released soon as part of Azure Sentinel!
The wizard has an Alert simulation feature that shows you if the specified query and scheduling properties will return events and if they will ‚Äúmeet the bar‚Äù (red line):
Azure Sentinel cases
Now that we created an advanced alert rule in Azure Sentinel, it will generate cases that you can assign and use to deeply investigate:
A case can include multiple alerts. It‚Äôs an aggregation of all the relevant evidence for a specific investigation:
PRO TIP: clicking the number under ‚ÄúHits‚Äù (in this example: 110) will bring you to the actual query to see the events that triggered the detection.
In the Entities tab, you can see all the entities that are involved:
Conclusion
A potentially better approach for enterprises is to block DNS / port 53 traffic at the edge by default. This ensures that all port 53 traffic has to go out via the corporate DNS servers, providing a central point for logging. At that point it is sufficient to do centralized logging on the DNS server itself.
Without central logging, or if you do allow DNS traffic at the edge, client-level logging is valuable for detecting DNS tunneling attacks. Client-level logging also means you directly get the name of the impacted host in the logs, which is an added value for Threat Hunting.
While DNS client logging wasn‚Äôt there out-of-the-box, Azure Sentinel makes it easy to start detecting the DNS attack vector.
Happy hunting!
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
13 
13¬†claps
13 
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/configuring-microsoft-graph-bindings-for-azure-functions-with-b2c-60fe0b63e81a?source=search_post---------348,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Feb 17, 2018¬∑4 min read
I had a hackfest with a customer last week. I really enjoyed. We use Microsoft Graph bindings with Durable Functions. We successfully fetch a token from the bindings. I‚Äôd like to share how to do it.
On this hackfest, we wanted to access Graph API for Azure Active Directory B2C. The code is very easy. However, the configuration was a little bit confusing for me.
If you want to use Graph bindings for fetching a token for B2C Graph API, you need to create an App Registration. But it should be created by someone who is in the Azure AD B2C directory.
For example, I usually login Azure by ushio@mydomain.com. I create an Azure AD B2C with a tenant named ‚Äúsomeorganization.onmicrosoft.com‚Äù. I can switch directory between MyDomain and SomeOrganization. However, you can‚Äôt create App Registration. You are the Azure AD user. Not Azure AD B2C user. You need to create a new user for the Azure AD B2C on the ‚Äúsomeorgranizaiton.onmicrosoft.com‚Äù tenant. Then logout the Azure and login with the new Azure B2C user (in this example, chirs@someorganization.onmicrosoft.com) then you need to create the App Registration by the chirs account . If you create it by ushio@mydomain.com, the App Registration doesn‚Äôt work for this use case.
Create a new admin account. Go to Azure Active Directory > Users > All users > New user. You need to create a user as admin. Then Login Azure using the new user.
Login as a B2C user (on the example, chirs account) on Azure. Go to Azure Active Directory > App registrations > New application registration
Coin it and Choose ‚ÄúWeb app / API‚Äù for application type. Sign-on URL is not used for this scenario. You can specify any url.
Choose the App Registration then go Settings > Required permissions > Windows Azure Active Directory
I want to read Graph API of B2C. Enable Read directory data then Save, then Grant Permission. It give the permission to the App Registration.
You need to create a key on the App Registration page. Go to Settings > Keys and create a key (password). Then get the Application ID
Then configure the Function App to fetch the token for the Graph API of B2C. Re-login at the original user to Azure and create a function app. Then Go to Authentication / Authorization on the Platform features
Then Click Azure Active Directory
Then set the App Registration values. Client ID = Application ID. Client Secret = Key. Issuer Url is https://sts.windows.net/{YOUR_TENANT_ID_OF_B2C}
You can also get the Issuer Url from this url.
https://login.microsoftonline.com/{YOUR_B2C_ORGANIZATION}/.well-known/openid-configuration
for example, my demo organization is like this.
https://login.microsoftonline.com/someorganication.onmicrosoft.com/.well-known/openid-configuration
Then you can find the ‚Äúissuer‚Äù attribute.
Done the settings.
Binding code is quite easy, all you need to do is define the Token. This is the binding part to fetch the token.
Then call the Graph API using the token.
This is the whole code.
Thank you for the hackfest team mate. Especially, Naohiro Fujie for teaching me about the B2C settings and Chris Gillum for the writing bindings.
I enjoy hacking with you guys. Let‚Äôs have another one.
Senior Software Engineer ‚Äî Microsoft
See all (200)
30 
4
30¬†claps
30 
4
Senior Software Engineer ‚Äî Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-build-step-by-step-a-multi-cloud-website-running-on-aws-azure-and-google-cloud-with-39dc98436891?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Dec 4, 2019¬∑14 min read
Running a web site in a single cloud provider is not complex.
We can deploy a web site with a load balancer in a popular cloud provider such as Amazon Web Services (AWS), Microsoft Azure or Google Cloud Platform (GCP) in just a few minutes, using‚Ä¶
"
https://blog.mapbox.com/bringing-the-mapbox-vision-sdk-to-microsoft-azure-iot-platform-b4751c62d851?source=search_post---------350,"By: Eric Gundersen
New Mapbox blog posts are now published on www.mapbox.com/blog. Leave your email to receive email updates when new posts are published or subscribe to the new RSS feed.
Our newly announced Vision SDK integrates with the Microsoft Azure IoT platform. This partnership improves the driving experience inside the vehicle and generates road data on the backend to power analytic solutions for smart cities, insurance companies, and more.
‚ÄúThe intelligent cloud and intelligent edge bring a wide range of possibilities for the future of smart cities, transportation, public safety and more. By integrating Mapbox‚Äôs Vision SDK with Azure IoT Hub, developers will have the power of Microsoft‚Äôs global-scale cloud platform and advanced AI services to ingest data in real-time.‚Äù ‚Äî Tara Prakriya, Group Program Manager, Microsoft Azure at Microsoft Corp.
The future of location will be building live maps in real time from distributed sensor networks embedded in vehicles and mobile devices at scale. The Vision SDK runs neural networks directly on a user‚Äôs mobile device or embedded hardware within a vehicle to segment the environment and detect discrete features like other vehicles, pedestrians, speed limits, construction signs, crosswalks, vegetation, and more.
We‚Äôre integrating the open sourced Azure IoT Edge Runtime, which provides custom logic, management, and communications functions for edge devices. Events detected from the Vision SDK integrated with Azure IoT Edge help developers build responsive applications that provide immediate feedback to the driver and stream semantic event data into Azure Cognitive Services for analysis on the back end.
Developers across a range of industries can securely send collision incidents to an insurance platform, for example; or deliver heavy traffic or blocked roadway alerts to a dispatch network. If businesses want to get granular, developers can send regular reports of activity at a crossing intersection to a business intelligence platform to optimize route paths.
For now, the Vision SDK is only available in private beta for select partners. We will be making it publicly available to everyone in September. Sign up now to get access as soon as it‚Äôs available .
www.mapbox.com
no passengers on the battleship
32 
32¬†claps
32 
Written by
mapping tools for developers + precise location data to change the way we explore the world
the official Mapbox blog
Written by
mapping tools for developers + precise location data to change the way we explore the world
the official Mapbox blog
"
https://medium.com/hackernoon/using-ngrok-with-azure-functions-7e209e96538c?source=search_post---------351,"There are currently no responses for this story.
Be the first to respond.
With things like the Azure Functions Cli and Azure Functions tools for Visual Studio you get the full development and debugging story locally on your machine. This is great as you can iterate and test quickly without the need to push the code to the cloud first, the drawback of this is that you can‚Äôt do incoming webhooks from. 3:rd party services, i.e. GitHub can‚Äôt access your locally running function.
But what if I said there‚Äôs a way you have your cake and eat it, wouldn‚Äôt that be great?
Introducing ngrok, ngrok is a tool and a service that will let you securely inspect and tunnel traffic to your local computer. It‚Äôs a free service with paid plans that will give you extra features like custom and reserved domains, IP address whitelisting etc.
ngrok is available cross plattform for MacOS, Windows, Linux and FreeBSD and it‚Äôs just a single binary you can download and unzip from ngrok.com/download. If you‚Äôre running Chocolatey on Windows like me, then it‚Äôs just a simple command way to get it installed:choco install -y ngrok.portable
Using ngrok is very straightforward, in general you launch the tool with which protocol and port your local service is listening on. ngrok http 8080
ngrok will launch and the forwarding urls is what you use to access your local service from the Internet.
You can find out which local port your Azure Functions by looking at the output of when the host starts, you can also specify the port using the port switch when launching the functions hostfunc host -p 8080
By default, ngrok will forward it‚Äôs temporary domain as host header to the locally running service, but as The Azure Functions host only listens to the hostname ‚Äúlocalhost‚Äù we‚Äôll need to override the default behavior using the host header switchngrok http 8080 --host-header localhost
To use your locally running function externally you just replace the base url provided by the function host to the temporary url provided by ngrok i.e.http://localhost:8080 becomes https://tempsubdomain.ngrok.io.
So say you have a GitHub webhook called GithubWebhookCSharp its local url will be http://localhost:8080/api/GithubWebhookCSharp and it‚Äôs external url will be https://tempsubdomain.ngrok.io/api/GithubWebhookCSharp.
Which you then could set up as i.e. a GitHub webhook
now when GitHub webhook triggers it‚Äôll tunnel through ngrok and its payload will be delivered to your locally otherwise externally inaccessible function
On off the real killer features of ngrok is that it provides a local web interface, you‚Äôll find it‚Äôs url in the tool‚Äôs output
This interface provides deep insight of all traffic that travels through the ngrok tunnel, you can see the response/request body and headers, it also lets you replay a request as many times as you like without needing to trigger an event on the external service, which is really useful when debugging, iterating over an implementation and fixing bugs!
Using a service like ngrok is really powerful and can ultimately speed up the development process.
#BlackLivesMatter
20 
1
20¬†claps
20 
1
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Partner & Technical fellow at WCOM AB. Microsoft Azure & Developer Technologies MVP. Been coding since I 80‚Äôs (C128 & Amiga). Father of 2, husband of 1.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/hackernoon/kubernetes-adventures-on-azure-part-2-windows-cluster-and-trick-for-scaling-pods-27e769edde15?source=search_post---------352,"There are currently no responses for this story.
Be the first to respond.
Part 3 available here: Kubernetes Adventures on Azure ‚Äî Part 3 (ACS Engine & Hybrid Cluster)
In Part 1 of this series, we have seen how to create a Linux Kubernetes cluster on Azure Container Services.
Today I will try creation of a Kubernetes cluster but with Windows as nodes instead of Linux. Obviously Master will always be Linux.
This time I will follow Deploy Kubernetes cluster for Windows containers step by step and then play with the newly created cluster.
Let‚Äôs start with the usual creation of a resource group for this test so that we can easily group all cloud artifacts in it and delete everything on the fly at the end. ARM has been a great addition to Microsoft Azure.
Cluster up and running! Awesome!
Before proceeding, open Kubernetes Dashboard to check what‚Äôs going on in the cluster using a browser.
Let‚Äôs start with the easy sample of IIS using windowsservercore instead of nanocore as described in Microsoft article. I will follow same steps to introduce concepts like pods and way to expose it once manually deployed. Note: I consider this bad because you end up with a pod, a service, but no controller between them as a Replica Set or a Deployment managing it. Later we will see how to ‚Äúfix‚Äù this issue without downtime.
Create iis.json with following content:
We can now apply configuration to a new Pod named iis: kubectl apply -f iis.json
When you have IP you can try to browse there and you should see:
Now let‚Äôs try to scale out our iis pod. Wait‚Ä¶ We can‚Äôt scale a pod directly. We need a ReplicaSet or a Deployment for this. How would you handle this?
Here I use a trick I found during my research that let you scale your pods without any downtime for final users. Probably there is a better way to do it, if so please add it to the comment.
If we knew what it was we were doing, it would not be called research, would it?Albert Einstein
Brief Explanation needed
The idea here is to create a new deployment that will create 2 new replica of our iis pod using the same label.
Note: during these steps, you can check your service using a browser and see that is up and running without problems.
What if we have a problem with a pod iis-1894189856‚Äì9fwsp and we want to isolate it from the rest while keeping it running for debugging purposes?
to the following content and save file:
In Part 3 I will try to create an hybrid cluster with Windows ad Linux nodes!
#BlackLivesMatter
71 
71¬†claps
71 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@ankitsharmablog/how-to-create-a-multi-language-translator-using-angular-and-azure-cognitive-services-5b0fcd9f592d?source=search_post---------353,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ankit Sharma
Feb 16, 2020¬∑12 min read
In this article, we are going to create a multi-language translator using Angular and the Translate Text Azure Cognitive Service. This translator will be able to translate between more than 60 languages which are supported by the Translate Text API. We will supply the text to translate and the target language to our application and it will return the translated text and the detected language of the input text.
You can get the source code from GitHub.
We will use an ASP.NET Core backend for this application. The ASP.NET Core backend provides a straight forward authentication process to access Azure cognitive services. This will also ensure that the end user won‚Äôt have the direct access to the cognitive services.
Log in to the Azure portal and search for the cognitive services in the search bar and click on the result. Refer to the image shown below.
On the next screen, click on the Add button. It will open the cognitive services marketplace page. Search for the Translator Text in the search bar and click on the search result. It will open the Translator Text API page. Click on the Create button to create a new Translator Text resource. Refer to the image shown below.
On the Create page, fill in the details as indicated below.
Click on the Create button. Refer to the image shown below.
After your resource is successfully deployed, click on the ‚ÄúGo to resource‚Äù button. You can see the Key and the endpoint for the newly created Translator Text resource. Refer to the image shown below.
Make a note of the key, we will be using this in the latter part of this article to request the translations from the Translator Text API. The values are masked here for privacy.
Open Visual Studio 2019 and click on ‚ÄúCreate a new Project‚Äù. A ‚ÄúCreate a new Project‚Äù dialog will open. Select ‚ÄúASP.NET Core Web Application‚Äù and click on Next. Now you will be at ‚ÄúConfigure your new project‚Äù screen, provide the name for your application as ngTranslator and click on create. Refer to the image shown below.
You will be navigated to ‚ÄúCreate a new ASP.NET Core web application‚Äù screen. Select ‚Äú.NET Core‚Äù and ‚ÄúASP.NET Core 3.0‚Äù from the dropdowns on the top. Then, select ‚ÄúAngular‚Äù project template and click on Create. Refer to the image shown below.
This will create our project. The folder structure of the application is shown below.
The ClientApp folder contains the Angular code for our application. The Controllers folders will contain our API controllers. The angular components are present inside the ClientApp\src\app folder. The default template contains few Angular components. These components won‚Äôt affect our application, but for the sake of simplicity, we will delete fetchdata and counter folders from ClientApp/app/components folder. Also, remove the reference for these two components from the app.module.ts file.
Right-click on the ngTranslator project and select Add >> New Folder. Name the folder as Models. Now right-click on the Models folder and select Add >> class. Name the class file as LanguageDetails.cs and click Add. This file will contain the User model.
Open LanguageDetails.cs and put the following code inside it.
Similarly, add a new class file TextResult.cs and put the following code inside it.
Add a new class file Translation.cs and put the following code inside it.
Create a class file DetectedLanguage.cs and put the following code inside it.
Create a class file TranslationResult.cs and put the following code inside it.
Create the class file AvailableLanguage.cs and put the following code inside it.
We will also add two classes as DTO (Data Transfer Object) for sending data back to the client.
Create a new folder and name it DTOModels. Add the new class file AvailableLanguageDTO.cs in the DTOModels Folder and put the following code inside it.
Add TranslationResultDTO.cs file and put the following code inside it.
We will add a new controller to our application. Right-click on the Controllers folder and select Add >> New Item. An ‚ÄúAdd New Item‚Äù dialog box will open. Select ‚ÄúVisual C#‚Äù from the left panel, then select ‚ÄúAPI Controller Class‚Äù from templates panel and put the name as TranslationController.cs. Click on Add. Refer to the image below.
The TranslationController will handle the translation request from the client. This controller will also return the list of all the language supported by the Translate Text API.
Open TranslationController.cs file and put the following code inside it.
The Post method will accept two parameters ‚Äî textToTranslate and targetLanguage. We will then invoke the TranslateText method which will return an array of TranslationResult. We will then prepare our data transfer object of type TranslationResultDTO and return it to the client.
The TranslateText method will accept two parameters ‚Äî the text to translate and the target language. We will set the subscription key for the Azure Translator Text cognitive service and define a variable for the global endpoint for Translator Text. The request URL contains the API endpoint along with the target language. We will then create a new HttpRequestMessage. This HTTP request is a Post request. We will pass the subscription key in the header of the request. The Translator Text API returns a JSON object, which will be deserialized to an array of type TranslationResult. This JSON object contain the translated text as well as the language detected for the input text.
The GetAvailableLanguages method is of type HTTPGet. It returns the list of languages supported by the Translate Text API. We will set the request URI and create a HttpRequestMessage which will be a Get request. This request URL will return a JSON object which will be deserialized to an object of type AvailableLanguage. We will then prepare the data transfer object of type AvailableLanguageDTO and return it to the client.
The code for the client-side is available in the ClientApp folder. We will use Angular CLI to work with the client code.
Using Angular CLI is not mandatory. I am using Angular CLI here as it is user-friendly and easy to use. If you don‚Äôt want to use CLI then you can create the files for components and services manually.
Navigate to ngTranslator\ClientApp folder in your machine and open a command window. We will execute all our Angular CLI commands in this window.
Create a folder called models inside the ClientApp\src\app folder. Now we will create a file availablelanguage.ts in the models folder. Put the following code in it.
Similarly, create another file inside the models folder called translationresult.ts. Put the following code in it.
You can observe that both these classes have same definition as the DTO classes we created on the server side. This will allow us to bind the data returned from the server directly to our models.
We will create an Angular service which will invoke the Web API endpoints, convert the Web API response to JSON and pass it to our component. Run the following command.
This command will create a folder name as services and then create the following two files inside it.
Open translator.service.ts file and put the following code inside it.
We have defined a variable baseURL which will hold the endpoint URL of our API. We will initialize the baseURL in the constructor and set it to the endpoint of the TranslationController.
The getAvailableLanguage method will send a Get request to the GetAvailableLanguages method of the TranslationController to fetch the list of supported languages for translation.
The getTransaltedText method will send a Post request to the TranslationController and supply textToTranslate and targetLanguage as the parameter. It will fetch the translated text and the language detected for the text to translate.
Run the following command in the command prompt to create the TextTranslatorComponent.
The--module flag will ensure that this component will get registered at app.module.ts. Open text-translator.component.html and put the following code in it.
We have defined two dropdown lists one each for input language and the target language. Both dropdowns contain the list of available languages for translation. The Azure Translate Text API will detect the language of the input text. We will use the detected language to set the value of the input language drop down. The dropdown list for target language will invoke the setTargetlanguage method on selection change.
We have also defined two text areas, one each for the input text and the translated text. The text area for the input text will bind the value to the sting variable inputText. Whereas the text area for the translated text will be populated using the translationOutput property of the TranslationResult class.
Open text-translator.component.ts and put the following code in it.
We are injecting the TranslatorService in the constructor of the TextTranslatorComponent. We will invoke the getAvailableLanguage method of our service in the ngOnInit and store the result in an array of type AvailableLanguage.
The GetTranslation method will invoke the getTransaltedText method of our service and bind the result to an object of type TranslationResult.
The setTargetlanguage method will set the outputLanguage to the value selected in the target language drop down.
We will add the styling for the textarea in text-translator.component.css as shown below.
We will add the navigation links for our components in the nav menu. Open nav-menu.component.html and remove the links for Counter and Fetch data components. Add the following lines in the list of navigation links.
Press F5 to launch the application. Click on the Translator link on the nav menu at the top. You can perform the multilanguage translation as shown in the image below.
We created a Translator Text Cognitive Services resource on Azure. We have used the Translator Text API to create a multilanguage translator using Angular. This translator supports transaltion for over 60 languages. We fetched the list of supported languages for translation from the global API endpoint for Translator Text.
github.com
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
See all (41)
104 
3
104¬†claps
104 
3
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/how-to-prepare-for-azure-technologies-architect-certification-az-303-304-exam-5a2207c9058b?source=search_post---------354,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you want to become a Microsoft Azure certified Solution architect and wondering how to prepare for this prestigious exam then you have come to the right place.
Earlier, I have shared a few tips, courses, and practice tests to pass the AZ 104, AZ-900, or Microsoft Azure Fundamentals exam and today, I‚Äôll talk about AZ-303 or Azure Solution Architect certification exam.
Based on the new Azure role-based certification path, the Microsoft AZ-303 exam is the main step towards becoming a Microsoft Certified Azure Solutions Architect.
This exam requires experienced candidates with skills in roles including Azure development, Azure administration, and DevOps. In addition, they must have expert-level skills in at least one of these areas.
However, observing these requirements, some of you may think that it would be completely impossible to pass the Microsoft AZ-303 Architect Technologies exam without years of prepration.
This is not true. If you have previous experience in the areas discussed above, you can effortlessly pass the exam with the right strategies and reliable study materials from the AZ-303 Architect Technologies exam.
In the past, I have shared some online courses and practice tests to crack the Microsoft Azure Solutions Architect exam, and today, I will provide you a complete guide on how to prepare and pass this prestigious exam.e Here, my intention is to provide you with this exam with the AZ-303 exam preparation strategies and appropriate study materials.
The eligibility criteria for the AZ-303 exam are so clear that it is easy for people to understand simply by reading the exam instructions.
If you want to take this exam, you need to have the skills of an Azure Solution Architect who can advise interested parties and transform business requirements into scalable, reliable, and secure solutions. Applicants for this exam must have advanced experience and skills in identity, visualization, networking, business continuity, budget, data management, governance, and disaster recovery. As I have already discussed, he/she must have advanced knowledge in Azure development, Azure administration, and DevOps and requires expert-level skills in at least one of these areas. So before you start preparing for the AZ-303 exam, make sure you meet the above criteria in each case.
Azure Exam AZ-303 is part of the Microsoft certification in Azure Solutions Expert. In this exam, you can expect 40 to 60 questions. This exam will have different types of question formats.
The types of questions that can be asked in this exam include the active screen, the review screen, the best answer, the brand review, the build list, the short answer, the case studies, the options for the repeated answer, drag and drop, multiple choices and active area. There are no downsides to incorrect answers. You simply don‚Äôt get some or all of the possible points for a question if you give the wrong answer. A review of the answers is also planned before leaving the exam room.
If you live in the United States, it will cost you $ 165 to take this exam. However, this price may vary based on location and many other factors. You may be eligible for a discount if you are a Microsoft Certified Trainer, a member of the Microsoft Partner Network Program, or a member of the Microsoft Imagine Academy Program. In addition, students can benefit from this type of fee reduction if they can present their valid lessons.
You will get the exam results that show the passing/failing status a few minutes after finishing the exam. However, getting a complete dashboard may take a few days.
This printed scorecard contains various elements such as the state of passing/failing, the overall exam score, the performance in the key areas of competencies, and the ways to interpret the results.
A score of 700 is required to pass the exam. Any score greater than or equal to 700 will be marked as ‚Äúapproved‚Äù; otherwise ‚Äúfailed‚Äù.
If a candidate fails the exam for the first time, he must wait at least 24 hours to repeat the exam. If this happens a second time, the candidate must wait at least 14 days to repeat the exam.
Therefore, a maximum of 5 attempts is allowed for an exam in one year. If a candidate has passed an exam, he cannot resume it under normal conditions.
There are no additional costs if you cancel or reschedule the exam at least 6 days before the appointment. If cancellation/postponement is made within 5 working days, minimum rates will be applied.
If you have not been able to take the exam or if you have not canceled/rescheduled the exam at least 24 hours before the appointment, you will lose all the exam fees that were spent when registering for the AZ exam 300.
There are a total of 5 objectives or topics you should know before starting preparation for the AZ-303 exam. These objectives are designed to effectively analyze a candidate‚Äôs skills and experience required for the exam.
Having the right preparation material can increase the chances of success. So we‚Äôve put together the best resources here to help you prepare for and pass the AZ-303 exam. Let‚Äôs go on to find the best AZ-303 exam preparation material.
This portal is the perfect starting point for the Microsoft AZ-303 Architect Technologies exam preparation trip. While preparing for the AZ-303 exam, you may need to visit this portal multiple times as it contains most of the AZ-303 Architect Technologies exam study materials needed to take the exam. Here you will find basic exam requirements, links to schedule the exam, skills discussed on the exam, study groups that discuss users‚Äô concerns about the exam, policies, and updates in relation to the exam, links to many other preparation options.
Since exam registration links are only accessible from this page, you cannot even think of skipping this page at any stage of preparation for the AZ-303 exam. In addition to the facts described above, this portal can be considered the most reliable preparation guide for the Microsoft AZ-303 exam, as all the latest updates on exam dates, price changes, etc. will appear first on this portal.
Therefore, be sure to confirm the information on this portal whenever you learn that there is a change in the modules, prices, or exam schedule for this exam.
If you need instructor-led training to prepare for the Microsoft AZ-303 Architect Technologies exam, you can get it from the Microsoft Learning Portal for AZ-303.
Total training options are divided into 6 modules to facilitate preparation for the AZ-303 exam. The availability of this training can change from country to country.
So when you need instructor training for one of these 6 courses, find a Microsoft Learning partner in your area. If you cannot afford instructor-led training then online courses are great options as they are affordable and you can learn from anywhere.
Here are the recommended Online Courses for AZ 303 Exam
udemy.com
2. AZ-303 Azure Architect Technologies Certification 2021 [Udemy]
udemy.com
3. AZ-303 Practice Test 2021 for Azure Architect Technologies [My Course]
www.udemy.com
Since this is a new exam, you may not find enough AZ-303 books or reference resources to prepare you for the Azure certification exam. However, once you‚Äôve come to see a similar book in this area, be sure to verify its authenticity using customer reviews and seller information. This type of verification is very important because many self-proclaimed books often convince readers to follow the slums instead of doing an actual analysis of the model or test procedure. The following brainstorming in preparation for the AZ-303 exam may also result in a ban on completing the exam or trying other exams in Azure. Therefore, be sure not to follow any of these errors when preparing for the AZ-303 exam.
There are many Microsoft forums that can help users prepare for the Azure certification exam. Some of these forums are also applicable to AZ-303 exam candidates.
Since comments on the forum won‚Äôt cost you a cent, it‚Äôs one of the easiest ways to prepare for the AZ-303 exam. In the forums, you can ask your questions without worries. You will receive replies from many experienced people. Microsoft has organized an official study group on its AZ-303 learning portal. When you go to the AZ-303 page on the Microsoft portal, you can see the study group in the lower section of the page. On this page, you can sort between unanswered and unanswered questions. However, you can also see the exam dumps in this section. So be sure to check each answer carefully to avoid those answers.
www.certification-questions.com
Microsoft provides sample tests for mostly exams that candidates use for their Azure Certification exam. Unfortunately, it is not available for AZ-303 at the moment. So, it is very important to have a sample test and a good Simulator for better preparation. No worries, David Mayer provides Microsoft AZ-303 exam Simulator which includes both sample tests and full-length tests. The questions in this Simulator are prepared to give you the best idea of each type of question that may appear in the exam. You can practice with this simulator to ensure the best exam results.
You can also try with these free 10 questions and then purchase a premium version to get ready for the exam and check your preparation level.
Udemy also has some decent AZ-303 Practice tests which you can use along with this for better preparation, here is a list of the best AZ 303 practice tests from Udemy which also include one of my courses:
www.udemy.com
That‚Äôs all about how to crack Microsoft Azure Solution Architect (AZ-303) Cloud Certification in 2021. If you want to start working with Microsoft Azure or any other cloud platform then this is a really good certification to add to your resume.
More and more companies are looking for certified Azure solution architects and there are not many in the market. My friends literally have Recruiter swarming them as soon as they put their certification on Linkedin profile.   Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like this article then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. ‚Äî If you need more practice then you can also checkout Whizlab‚Äôs Microsoft Azure Exam AZ-303 Certification Prep Material which contains both full-length tests and section quizzes to assess your preparation.
www.whizlabs.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
142 
3
142¬†claps
142 
3
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@tsuyoshiushio/serverless-idempotency-with-azure-functions-23ed5da9d428?source=search_post---------355,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
May 2, 2018¬∑5 min read
Azure Functions and Durable Functions said that ‚Äúfunctions should be idempotent.‚Äù However, what does it mean for the Azure Functions context?
According to the Wikipedia,
Idempotence is the property of certain operations in mathematics and computer science that they can be applied multiple times without changing the result beyond the initial application.
I can imagine the idempotence for the Infrastructure as Code tools and functional programming. However, what is that with Azure Functions?
For example, We write database, send messages like queue, notification, e-mails. It looks challenging to keep idempotency.
My colleague answer the question. In short, it is important for the cloud environment for keeping the resilient to retries. Especially it is very important for the durable functions. Your activity could be called multiple times by the replay.
By meriting of running in the cloud, assuming the worst case scenario: the underlying hardware might have a power outage at any time. That could be in the middle of your function execution of even after your function gets to the last line of your user code (so it looks like it was successful). This failure will lead to a retry of your function. This operation happens naturally for triggers like QueueTrigger, and even an HTTP client may resend the message if it has a retry loop. This means your function could get executed multiple times and needs to be idempotent so that it‚Äôs resilient to retries.
This blog post with the movie is a useful resource for the idempotence for REST-API.
www.restapitutorial.com
Read is idempotent. Upsert or Update is also idempotent. Delete is not idempotent, however, if there is no resource, just return an error. You can compromise it. Create is not idempotent. However, if the database support Upsert it is idempotent. Sometimes, Database state could be changed during the call. However, it is ok. For the update, you can use optimistic lock strategy to conquer that.
In case of the e-mail, you can compromise. If you receive two e-mails, it might not be a big deal. However, if you want to implement idempotency, you can save the state if the e-mail has been sent or not. This strategy is similar to the Queue strategy. Please see the next section.
We use queue frequently with Azure Functions. Imagine that if you use HTTP trigger with Queue output bindings, how we can make it idempotent? If you call HTTP trigger multiple times with the same parameter, obviously, it resends a queue.
We can‚Äôt achieve the idempotency for a single function. However, we can accomplish that in the whole system. If the queue trigger receives the queue with the same id, do nothing.
I create a sample code for this. These are the node functions with HTTP and Queue trigger. I use Storage Table bindings to make sure the execution has occurred in the past.
Using postman, send POST request to this body. You can change the id.
The HTTP trigger function sends this message to the Queue trigger function. The second function executes something if the id is new to them.
If you send the same request, you will see
It seems that the queue has been consumed, but the execution has not done yet. Don‚Äôt worry about that. Azure Functions Queue trigger looks after that for you. Once the error happens, Azure Functions leave the queue. The Queue is consumed only after the successful execution of the Queue trigger functions. Then it retries to consume the queue. If it fails five times, it is going to the dead queue.
See this article
docs.microsoft.com
I also include some logic for proving that.
When I repeatedly send, sometimes, you can see the error. Just after that, you can see the retry happens.
You can see the Queue Listener which is a part of the Queue Trigger bindings. It make the queue invisible for a while, then if it is completed the function, it will consume the queue.
github.com
To achieve the idempotency with Azure Functions, you don‚Äôt need to meet it in a single function. You can do it with the whole system. Sometimes, you can compromise it, however, always think the possibility of the retry. It will gain resiliency of your app.
Since this code is very simple, however, I struggled for a couple of hours. I can‚Äôt find the correct way to configure the Storage Table input bindings.
The answer is like this. ‚ÄúrowKey‚Äù was very difficult.
I try to find the answer by this document, however I can‚Äôt clearly understand the spec.
docs.microsoft.com
Eventually, I dump the context object, and realized that the parameter. The specification says that directly specify the data. however, RowKey didn‚Äôt work. :)
It help me to find out the solution. The output is like this
I should have dumped from the start.
Senior Software Engineer ‚Äî Microsoft
41 
41¬†
41 
Senior Software Engineer ‚Äî Microsoft
"
https://medium.com/t-t-software-solution/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-azure-app-service-%E0%B8%9F%E0%B8%A3%E0%B8%B5-%E0%B9%83%E0%B8%99-10-%E0%B8%99%E0%B8%B2%E0%B8%97%E0%B8%B5-e3a5c25ef749?source=search_post---------356,"There are currently no responses for this story.
Be the first to respond.
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Azure Cloud Service ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö Paas ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡∏ú‡∏°‡∏à‡∏∞‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á web application ‡πÅ‡∏•‡∏∞ web api ‡πÅ‡∏ö‡∏ö ‡∏á‡πà‡∏≤‡∏¢‡πÜ‡∏ú‡πà‡∏≤‡∏ô Azure ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
2. ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ô‡∏≥‡πÄ‡∏£‡∏≤‡πÑ‡∏õ‡∏ó‡∏µ‡πà my.visualstudio.com
3. ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠3.1 ‡∏ü‡∏£‡∏µ Azure 300$ 1 ‡∏õ‡∏µ (25$ ‡∏ï‡πà‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)3.2 ‡∏ü‡∏£‡∏µ Pluralsight (‡πÄ‡∏ß‡πá‡∏õ Learning Centre) 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (‡∏°‡∏µ‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡∏µ‡πÜ‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å‡πÜ‡∏Ñ‡∏£‡∏±‡∏ö)3.3 ‡∏ü‡∏£‡∏µ Azure App Service (‡∏Å‡∏î‡πÄ‡∏•‡πà‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡πÅ‡∏ï‡πà‡πÄ‡∏•‡πà‡∏ô‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏•‡πà‡∏∞ 1‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)
4. ‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Tool >> Azure App Service >> Use it free
5. ‡∏à‡∏∞‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å App ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó, ‡∏•‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Web App
6. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å MVC Template
7. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Account ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏û‡∏±‡∏Å‡πÉ‡∏´‡πâ Azure ‡∏™‡∏£‡πâ‡∏≤‡∏á Web ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤ ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡πâ‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏û
8. ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡πâ‡∏Ñ‡∏∑‡∏≠
8.1 Link ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Web ‡∏ó‡∏µ‡πà‡∏û‡∏∂‡πà‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
8.2 Link ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Azure
8.3 Edit Code ‡∏ú‡πà‡∏≤‡∏ô Visual Studio Code ‡πÑ‡∏î‡πâ online ‡πÄ‡∏•‡∏¢
8.4 Download sample code ‡∏•‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ
8.5 Download Azure Publish Profile ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ deploy web ‡∏ú‡πà‡∏≤‡∏ô Visual Studio ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤
8.6 Git
9. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏õ
10. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Azure Portal
11. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Edit code online
12. ‡∏ú‡∏° ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Web API ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡∏Å‡∏î Previous ‡πÅ‡∏•‡πâ‡∏ß ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏•‡∏ö web app ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
13. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å API APP
14. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å API To Do List
15. ‡∏´‡∏ô‡πâ‡∏≤ Web API
16. ‡∏Å‡∏î using the Swagger UI, ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏Ç‡∏≠‡∏á Swagger UI ‡∏û‡∏£‡πâ‡∏° list ‡∏Ç‡∏≠‡∏á Web API ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ^ ^
17. ‡∏ó‡∏î‡∏•‡∏≠‡∏á ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å service GET /api/ToDoList
18. ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡πÑ‡∏î‡πâ web api ‡πÅ‡∏ö‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ‡πÑ‡∏ß‡πâ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏ú‡∏° ‡πÉ‡∏ô Medium ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏°‡∏≤ ‡∏ì ‡∏ó‡∏µ‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ô‡∏≤‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô
https://www.tt-ss.net/
26 

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
26¬†claps
26 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/6-best-az-900-practice-tests-dumps-and-mock-exams-to-crack-azure-fundamentals-exam-990041818584?source=search_post---------357,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for Microsoft Azure Fundamentals or AZ-900 certification exam in 2021 and want to pass this in-demand certification in the first attempt then you should practice with mock exams and practice tests, those are great tools to assess your preparation and increase your speed and accuracy required to pass this exam in the first attempt.
Earlier, I have shared the best AZ-900 Online Courses and In this article, I am going to list down some of the best practice tests you can take to pass the Microsoft Azure Fundamentals (AZ-900) exam.
This exam focuses on essential cloud concepts like IaaS, PaaS, SaaS, and Azure Core services like geo availability, storage, network, compute services, security, pricing, and billing.
This is actually one of the easiest Microsoft Azure cloud certifications, much like Amazon‚Äôs AWS Cloud Practitioner certification and you can easily pass this exam even if you have never worked in any cloud platforms like AWS, Azure, or GCP. This exam is a good stepping stone for both technical people who want to learn how exactly cloud works and how to use different Azure services to run their application in Cloud, as well as for non-technical people like BA, Project Managers, the Sales guy who are involved in purchasing Cloud-based solution and services. Prior IT experience is not mandatory, but it certainly helps if you have some IT experience and understand common technical terms like server, storage, compute, memory, network, security, VPN, scalability, elasticity, etc. Now, coming back to Microsoft Azure Fundamentals certification, one question I often receive is how to clear AZ-900 certification in one week? Or is one week enough to pass the Azure Fundamentals certification?  The answer to this question is Yes, one week is enough provided you can spend 5 to 6 hours daily. And, the best way to prepare for this exam is by joining an Azure Fundamentals AZ-900 course, like  AZ-900: Microsoft Azure Fundamentals Exam Prep -2021 Edition, which is focused on the exam, much like an exam guide, do some hands-on lab on the Azure portal, and then go through some practice tests and mock exams. Mock exams and Practice tests play an important role in your certification journey. They are often the difference between success and failure but also between satisfactory and excellent results. You may pass your exam without going for Practice tests, but you will do a lot better if you include Practice tests in your preparation strategies. They will help you to find your strong and week areas and also helping with timing and speed, which is very important to complete all questions during real exams. When I gave certification exams like AWS Solution Architect and AWS Cloud Practitioner, I spent hours giving practice tests on the real exam like scenarios and then reviewing what I did good and where I needed to improve, and this single strategy has helped me to score outstanding results on my certification exams.
If you are preparing for Microsoft Azure Fundamentals or AZ-900 certification and looking for some excellent Practice tests to take your preparation and confidence to the next level, then you have come to the right place.  Here are some of the best AZ-990 practice and mock exams from Whizlabs, Udemy, Exam Pass, Certification-Questions.com you can take to clear the Azure Fundamentals exam on the first try.
This is a perfect online course + practice test to prepare for the Microsoft Azure Fundamental exam in one week. Scott Duffy, a cloud expert, has done a fabulous job in this course by teaching you everything you need to pass the AZ-900 exam. The course also comes with a lot of bonuses and supplementary resources like you will get access to a 24-page Microsoft Fundamentals Study guide to prepare offline. There are also quizzes to reinforce your learning through each chapter. As I said, the course also contains a 50 question mock test to find your strong and week areas. If you are in a serious hurry and want to pass AZ 900 exam in a couple of days, just go through this course and practice test. You may not get a high score, but you will definitely have enough knowledge to pass the exam.
Here is the link to join this test ‚Äî AZ-900: Microsoft Azure Fundamentals Exam Prep -2021
You can also combine this practice test with my topic-wise AZ-900 Udemy course for better preparation and practice, where I have shared 300+ questions covering each topic for Azure Fundamentals certification.
www.udemy.com
This mock simulator is best for passing the Microsoft Azure certification or AZ-900 exam, and you can access them via web and mobile. This practice test from Whizlabs contains 275 unique questions divided into 5 full-length exams, which check your fundamental knowledge of cloud services, and how Microsoft Azure provides the cloud services, irrespective of any specific role. There are also 7 section tests with 35 unique questions, and they provide an exhaustive explanation with every question, which not only helps you to learn why a correct option is correct but also why other options are not correct. When it comes to practice tests for Cloud certifications like Microsoft Azure Fundamentals, I trust Whizlabs. I have used Whizlabs in the past for passing several Java, and AWS certifications like OCAJP, OCPJP, AWS Solution Architect, AWS Cloud Practitioner, and it didn‚Äôt disappoint me on AZ-900 as well. Their reporting is also top-notch and really helps you to assess your strengths and weaknesses before the real exam. After a few tests, you will have enough ideas on which topics you need to prepare better.
Overall, one of the best exam simulators for the Azure AZ-900 certification exam, and I highly recommend it if you want to score in excess of 80%.
Here is the link to join Whizlabs ‚Äî Microsoft Azure Exam AZ-900 Certification Practice Test
By the way, if you are preparing for certification, consider taking Whizlabs subscription which provides full access to all of their online training courses and practice tests for different certifications like AWS and Google Cloud with just $99 per annum (50% discount now).
I highly recommend this subscription plan as Whizlabs has the best materials to prepare for IT certifications.
This is another online course that provides the latest practice questions for the Microsoft Azure Fundamentals exam with detailed explanations. It contains 6 practice papers with 55 questions on each, which means a total of 332 questions for practice. The best thing about these mock tests on Udemy is that they are closely aligned with the exam syllabus which means you have 15‚Äì20% questions from cloud concepts, 30‚Äì35% questions from core Azure Services, 20‚Äì25% questions on pricing, and support, and rest of them on security, privacy, compliance, and trust. Like Whizlabs, they also provide a good explanation for each question and link to official Microsoft documentation where required. In short, these practice tests are enough to pass the Microsoft Azure AZ-900 exam on the first attempt.
It is also very cost-effective, and I bought it for just $10, you can also buy it at a similar or a bit higher price depending upon the Udemy sale.
Here is the link to join this test ‚Äî Microsoft Azure Fundamentals (AZ-900) ‚Äî Practice Tests
This is an optional practice test, and if you have gone through Whizlab‚Äôs or the previous one, you may not need that. In case you need more practice, and you are not confident enough, or you really want to kill the exam by scoring 100 out of 100, then you can purchase this course on Udemy. This is also from Scott Duffy, author of the first course in this list. You can also use these practice tests along with that cours, but it‚Äôs not a pre-requisite. It contains three complete, timed practice tests for the AZ-900 Azure Fundamentals exam, 150 questions, and they are 100% original. This time-bound test is important to develop the speed and accuracy you need to solve all problems in the real exam. If you consistently score over 80% on these tests and able to complete all questions, then you are ready for the real exam. From my experience, I suggest you take these tests a couple of days before the real exam so that you have enough time to rectify weak areas.
Here is the link to join this course ‚Äî AZ-900: Microsoft Azure Fundamentals Original Practice Tests
This is slightly different than the practice test. These are actually exam dumps that are anonymously shared by candidates going through AZ-900 certification. The best thing about AZ-900 exam dumps is that they are the real questions from the exam, which means you can check the difficulty level and format. It‚Äôs not guaranteed that you will see those questions again, but they are still very useful to get yourself up to that level or above. Here is a brief detail about this Microsoft AZ-900 Practice Exam: Microsoft Azure Fundamentals: ‚Äî Number of Questions: 149 ‚Äî Exam Tests: 3 ‚Äî Last Update: 2021‚Äì03‚Äì15 So, it‚Äôs pretty up-t-date, and I saw many comments where candidates mentioned that the question they‚Äôve got during the exam was more than 80% the same as these tests. :-)
Here is the link to check these dumps ‚Äî AZ-900 Exams Dumps by David Mayer
If you get lucky, let us know, but I don‚Äôt recommend that approach. I prefer to over-prepare and score really high on the exam rather than mugging questions from exam dumps. Don‚Äôt get me wrong; they still have a lot of utility with respect to difficulty level and format, and that‚Äôs why I have included them on this list, but I don‚Äôt advise you to solely rely on them for your preparation.
This is the latest practice test designed for Microsoft Azure Fundamentals or AZ-900 exam. It contains AZ-900 Practice questions with detailed explanations which will help you to pass the Microsoft Azure Fundamental exam with confidence.
This test contains 260 questions divided into 6 practice tests, where 4 tests contain 50 questions each and two tests containing 30 questions each. This mock test is designed by Sunil Kumar, who has recently passed the Azure Fundamentals certification exam.
Every practice test includes-
If you are preparing for the AZ-900 Microsoft Azure Fundamentals exam in 2021 then you should take this practice test and solve the given questions to better prepare for the exam.
Here is the link to join this course ‚Äî AZ-900 Practice Tests | Microsoft Azure Fundamental
Here are some of the important things you should know while preparing for the AZ-900 or Microsoft Azure Fundamentals exam: 1. The cost of the exam is $99 USD, but it may vary depending upon which country you are in. Price is actually based on the country in which the exam is proctored. 2. The good thing about Azure Fundamental certification is that topics are clearly defined, and so is their weightage. Here are some important topics for the AZ-900 exam:
3. The AZ-900: Microsoft Azure Fundamental exam checks your knowledge about basic cloud concepts; core Azure services; security, privacy, compliance, and trust; and Azure pricing and support. 4. This certification is good for both technical people like Developers, Technical Lead, and architects who want to learn about Cloud and how Microsoft provides essential Cloud services as well as non-technical people like BA, Project Managers, Salespersons who are involved in buying and selling Cloud-based solutions and services 5. You can read the exam guide for AZ-900 or Azure Fundamental exam on the Microsoft Azure exam portal. It has some useful information for the exam.
docs.microsoft.com
That‚Äôs all about some of the best mock exams and practice tests to prepare for AZ-900 or Microsoft Azure Fundamentals certification. I highly recommend you to go through at least one of the practice tests listed here, if you need to choose one, then choose the first one from Udemy or the second one from Whizlabs, both are excellent, up-to-date, and very relevant to the real exam. Also, the AZ-900 exam is the perfect way to prove that you know about Cloud and Azure. You can also put the AZ-900 badge on your resume and LinkedIn to get recruiters‚Äô attention, who are always looking for cloud-certified professionals.  Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like these best AZ-900 certification questions and practice tests, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. ‚Äî If you are new to the world of Cloud and looking for some free courses to learn Cloud computing then you can also, check out this list of cloud computing courses with Amazon Web Services ‚Äî Free AWS Courses for Beginners on Medium. It contains some of the best and free online training courses to learn Amazon Web Services and in-demand skills for Clod engineers and architects.
medium.com
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
185 
2
185¬†claps
185 
2
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/xp-inc/azure-devops-docker-node-js-90cff720af22?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Nov 16, 2019¬∑5 min read
Veja nesse artigo como automatizar o processo de deploy do seu projeto utilizando Docker e o Azure DevOps
Dando continuidade ao meu artigo anterior: publicando imagem Docker no Azure Web App for Containers em 7 passos, hoje eu irei demonstrar como automatizar o processo de deploy demonstrando no artigo anterior, utilizando o Azure DevOps.
Para os pr√≥ximos passos ser√° necess√°rio ter uma conta no Azure. Caso voc√™ ainda n√£o tenha uma, eu recomendo a leitura do seguinte artigo: Azure DevOps + Angular + GitHub Pages, nele eu demonstro a cria√ß√£o de uma nova conta e a cria√ß√£o de uma pipeline no Azure DevOps.
Com o passo da conta OK, a sua imagem no Docker hub (passo demonstrado no artigo anterior) e o seu projeto j√° publicado no Web App for Containers. Vamos agora criar a nossa pipeline no Azure DevOps. Para isso, siga os passos abaixo:
Clique em pipelines:
Em seguida clique em Create Pipeline:
Agora clique em Use the classic editor:
Selecione o local do seu reposit√≥rio, para esse exemplo eu irei utilizar um projeto versionado no meu GitHub: node-azure-7-steps. Clique nos tr√™s pontos e selecione o seu projeto:
Em seguida selecione a sua branch:
Agora selecione o template Docker container.
Esse template deve criar dois steps: Build an Image para criar uma nova vers√£o da imagem do seu projeto e Push an image, para publicar a imagem no no seu reposit√≥rio de imagens, nesse artigo eu irei enviar para o Docker Hub.
Agora vamos dar permiss√£o para pipeline subir uma nova vers√£o da sua imagem no Docker Hub. Para isso, siga os passos abaixo:
Clique em Push an image, em Container Registry Type selecione Container Registry, em seguida selecione a sua conex√£o.
Caso n√£o tenha uma conta registrada ainda, clique em + New e preenche a modal com os seus dados de acesso no Docker Hub.
Em seguida clique em Include Latest Tag:
E no nome da imagem coloque o seu usu√°rio do dockerhub e o nome da sua imagem no passo de build e release:
Build
Release
Para verificar se tudo esta OK, clique em Save & queue e rode o processo:
Quando esse processo finalizar voc√™ deve receber o resultado abaixo:
Com o processo do build OK, vamos criar a nossa release. Para isso, clique em Releases -> New pipeline e selecione o template Azure App Service deployment.
Clique em Artifacts e preencha conforme os passos abaixo:
Agora clique em 1 job e forne√ßa os dados do seu projeto no Azure Web App for Containers conforme lista abaixo:
Obs.: Esse sufixo garante que iremos sempre pegar a ultima vers√£o da imagem criada.
Agora para verificar se todos passos anteriores est√£o OK, clique em Create release para gerar uma nova release do seu projeto:
Ao clicar em Create Release ir√° subir a seguinte mensagem:
Clique na sua Release para acompanhar o processo de deploy. Caso tudo esteja OK voc√™ deve receber o retorno abaixo:
Agora clique em Succeed:
Em seguida clique em Azure Wer App on Container Deploy:
E dentro do log copie a URL do seu projeto:
Agora para finalizar, cole a url no seu navegador e verifique se a ultima altera√ß√£o do seu projeto esta nessa vers√£o publicada:
Bom, a ideia desse artigo era demonstrar como automatizar o processo de deploy criado em um dos meus artigos anteriores.
Espero que tenham gostado e at√© um pr√≥xima artigo pessoal ;)
Enjoy your life
See all (155)
48 
1
48¬†claps
48 
1
Aqui voc√™ vai encontrar os principais conte√∫dos de tecnologia, design, dados e produto da XP Inc.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/asp-net-core-protegendo-segredos-de-uma-aplica%C3%A7%C3%A3o-com-o-azure-key-vault-b6e10cfe8a5?source=search_post---------359,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 22, 2019¬∑8 min read
O armazenamento de configura√ß√µes em que est√£o presentes informa√ß√µes sens√≠veis e utilizando arquivos como appsettings.json constitui uma pr√°tica bastante difundida. Haver√° ocasi√µes, entretanto, nas quais mecanismos externos precisar√£o ser empregados a fim de dificultar o acesso a tais defini√ß√µes por parte de um usu√°rio qualquer (que as obteria rapidamente pela simples leitura de um arquivo).
Considerando o desenvolvimento de projetos Web com ASP.NET Core, como proteger segredos de nossas aplica√ß√µes de maneira simples e eficiente?
Uma boa resposta para esse questionamento est√° no Azure Key Vault, uma solu√ß√£o de armazenamento de configura√ß√µes sens√≠veis oferecida pela plataforma de cloud computing da Microsoft. Possuindo integra√ß√£o com o Azure Active Directory, a manipula√ß√£o (leitura, escrita) de segredos definidos no Key Vault acontece mediante a concess√£o pr√©via de acesso a uma aplica√ß√£o.
Neste novo artigo demonstrarei o uso do Azure Key Vault em uma API REST criada com o ASP.NET Core 2.2. O exemplo em quest√£o foi disponibilizado tamb√©m no GitHub:
https://github.com/renatogroffe/ASPNETCore2.2_API-REST_AzureKeyVault
E aproveito este espa√ßo para deixar aqui ainda um convite.
Nesta ter√ßa 23/07/2019 partir das 21:30 - hor√°rio de Bras√≠lia - teremos mais uma live no Canal .NET . Desta vez o MVP Luiz Carlos Faria far√° uma apresenta√ß√£o sobre o uso de Docker com Envoy, abordando esta combina√ß√£o desde um simples proxy reverso a service mesh.
Para efetuar a sua inscri√ß√£o acesse a p√°gina do evento no Meetup. A transmiss√£o acontecer√° via YouTube, em um link a ser divulgado em breve.
Neste primeiro momento acessaremos o Portal do Microsoft Azure:
Na sequ√™ncia acionar na barra lateral √† esquerda o item Azure Active Directory:
Clicar agora na op√ß√£o App registrations:
Aparecer√£o ent√£o eventuais aplica√ß√µes j√° registradas no painel. Clicar na op√ß√£o New registration, a qual permitir√° que registremos a API REST empregada nos testes detalhados neste artigo:
Em Register an Application preencher:
Confirmar este procedimento acionando o bot√£o Register.
O registro da aplica√ß√£o ser√° exibido neste momento. O valor da configura√ß√£o Application (client) ID dever√° ser utilizado posteriormente, ao se configurar a comunica√ß√£o da aplica√ß√£o com o recurso do Key Vault:
Acionar agora a op√ß√£o Certificates & secrets; o bot√£o New client secret permitir√° a gera√ß√£o de uma chave, a ser empregada pela aplica√ß√£o para validar seu acesso ao Azure Key Vault:
Em Add a client secret preencher o campo Description, selecionando ainda em Expires o valor Never. Confirmar as a√ß√µes clicando no bot√£o Add:
O valor indicado no campo VALUE do segredo dever√° ser COPIADO IMEDIATAMENTE (um aviso no alto do painel alerta que tal configura√ß√£o n√£o estar√° mais dispon√≠vel quando o usu√°rio deixar esta funcionalidade no Portal do Azure):
E aproveito este espa√ßo e o grande interesse por Docker tamb√©m para um convite.
Tem interesse em conhecer mais sobre Docker? Que tal ent√£o fazer um curso completo, cobrindo desde fundamentos a diferentes possibilidades de uso de containers com tecnologias em alta no mercado? Adquira conhecimentos profundos sobre Docker, evolua e se diferencie no mercado, seja voc√™ um profissional DevOps, um Desenvolvedor ou um Arquiteto de Software!
Acompanhe o portal Docker Definitivo para ficar por dentro de novidades a serem anunciadas em breve!
Site: https://dockerdefinitivo.com/
Ao solicitar a cria√ß√£o de um novo recurso no Portal do Azure pesquisar por Azure Key Vault, selecionado a op√ß√£o destacada a seguir:
Clicar agora em Create:
Em Create key vault informar o nome do recurso (campo Name), um grupo de recursos (campo Resource Group) e o data center em que o mesmo ser√° criado (Location). Acionar em seguida a op√ß√£o Access policies:
Clicar em Add new, a fim de vincular ao recurso do Key Vault a aplica√ß√£o registrada no Azure Active Directory:
No painel Add access policy selecionar em Configure from template (optional) a op√ß√£o Secret Management, clicando na sequ√™ncia em Select principal:
Em Principal localizar e selecionar a op√ß√£o API REST Cota√ß√µes, confirmando isso atrav√©s do bot√£o Select:
J√° em Secret permissions habilitar apenas as op√ß√µes Get e List, de forma que a API REST vinculada a este recurso do Azure Key Vault possua somente acesso de leitura √†s configura√ß√µes cadastradas:
Concluir este procedimento clicando no bot√£o OK:
Aparecer√° agora em Access policies o item API REST Cota√ß√µes registrado como APPLICATION; confirmar este ajuste por meio do bot√£o OK:
Retornando ao painel Create key vault concluir o processo de cria√ß√£o clicando no bot√£o Create. Ap√≥s alguns segundos o recurso groffeartigo estar√° dispon√≠vel; o endere√ßo indicado em DNS NAME tamb√©m ser√° tamb√©m utilizado para configurar o acesso na API REST de testes:
Na se√ß√£o Secrets do recurso do Key Vault ser√£o cadastradas configura√ß√µes que n√£o dever√£o constar no arquivo appsettings.json. Acionar para isto a op√ß√£o Generate/Import (destacada em vermelho):
Em Create a secret:
Concluir esta a√ß√£o pressionando o bot√£o Create. O segredo ConnectionStrings--BaseCotacoes estar√° ent√£o dispon√≠vel para uso:
Acessando o item ConnectionStrings--BaseCotacoes ser√° exibida uma tela similar √†quela apresentada na imagem a seguir:
Um clique na defini√ß√£o que est√° em CURRENT VERSION trar√° a pr√≥xima tela:
Com um simples clique em Show Secret Value podemos consultar o valor da Connection String armazenada no recurso groffeartigo:
O package Microsoft.Extensions.Configuration.AzureKeyVault dever√° ser adicionado ao projeto que ter√° seus segredos armazenados no Key Vault:
Incluir agora a se√ß√£o AzureKeyVault no arquivo appsettings.json, preenchendo as seguintes configura√ß√µes:
Conforme √© poss√≠vel observar, n√£o h√° qualquer men√ß√£o a uma string de conex√£o neste arquivo de configura√ß√µes:
A √∫nica altera√ß√£o a ser realizada neste projeto acontecer√° na classe Program:
A seguir temos a implementa√ß√£o da classe Startup, com o uso normal do objeto Configuration. Isto inclui um acesso √† chave ConnectionStrings:BaseCotacoes (linha 35), a qual n√£o existe em appsettings.json e que numa situa√ß√£o normal implicaria na exist√™ncia de dois n√≠veis de propriedades no arquivo de configura√ß√£o:
Efetuando o debugging da aplica√ß√£o √© poss√≠vel constatar que o ajuste na classe Program vinculou a string de conex√£o que est√° no recurso do Azure Key Vault (o segredo ConnectionStrings--BaseCotacoes) ao objeto Configuration. Tal fato permite a diferentes partes da aplica√ß√£o acessar configura√ß√µes sem grandes complica√ß√µes, esteja uma determinada defini√ß√£o na nuvem ou no arquivo appsettings.json:
Um segundo teste envolver√° a classe CotacoesController. Uma chamada a config.GetConnectionString(‚ÄúBaseCotacoes‚Äù) - linha 18 - permitir√° acessar o segredo que est√° no Azure Key Vault, como se a defini√ß√£o correspondente fosse um item chamado BaseCotacoes e estivesse vinculada ao elemento ConnectionStrings em appsettings.json:
√â o que demonstra o exemplo de debugging na pr√≥xima imagem:
.NET Core 2.2 e ASP.NET Core 2.2: Guia de Refer√™ncia
Key Vault | Microsoft Azure
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
35 
2
35¬†
35 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/como-o-microsoft-azure-pode-simplificar-a-publica%C3%A7%C3%A3o-de-suas-web-apps-dica-r%C3%A1pida-6e934e775515?source=search_post---------360,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Aug 27, 2019¬∑2 min read
J√° precisou fazer o deployment de uma aplica√ß√£o em Windows utilizando IIS (Internet Information Services) e encontrou dificuldades de configura√ß√£o? E quanto a este mesmo tipo de procedimento em Linux, sofreu por n√£o possuir tanto conhecimento deste ambiente?
Que bom seria proceder com esta publica√ß√£o sem grandes complica√ß√µes, fosse uma aplica√ß√£o ASP.NET cl√°ssica (Web Forms, MVC, Web API), .NET Core, Java, Node, PHP, Python, Ruby ou at√© mesmo uma imagem Docker baseada em alguma destas plataformas! E que tal nem perder nosso precioso tempo instalando todo o runtime necess√°rio para essas stacks, processo este que muitas vezes est√° sujeito a erros?
E ainda se pudermos configurar o deployment automatizado de nossas aplica√ß√µes, bem como dispor de mecanismos que facilitam a constru√ß√£o de solu√ß√µes escal√°veis‚Ä¶ tudo isto com poucos cliques do mouse?
O Azure App Service √© uma alternativa para hospedagem de aplica√ß√µes que integra a plataforma de cloud computing da Microsoft e que nos oferece tudo isso! Em nossos eventos presencias e lives em comunidades como Canal .NET, .NET SP, DevOps Professionals, Coding Night, Campinas .NET e Azure Talks sempre abordamos o uso desta tecnologia.
E que tal aprender a trabalhar com o Azure App Service na pr√°tica, em um workshop que acontecer√° durante um s√°bado (dia 21/09) em S√£o Paulo Capital e implementando na pr√°tica um case que combina o uso deste servi√ßo com outras tecnologias? Acesse ent√£o o link a seguir para efetuar sua inscri√ß√£o com um desconto especial: http://bit.ly/promocao-azure-na-pratica
Para concluir este post deixo aqui algumas refer√™ncias que podem ser √∫teis para que voc√™ conhe√ßa um pouco mais sobre o Azure App Service:
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
Hospedando projetos Web no Azure: de um site est√°tico a um cluster Kubernetes
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
40 
1
40¬†
40 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://koukia.ca/azure-functions-and-how-they-work-6b515007001e?source=search_post---------361,"Azure functions are event or trigger driven functions that you can build on Microsoft Azure which you can implement using variety of languages and leverage a lot of Azure features like Continuous Deployment and Integration, Scaling up and Scaling out, Hybrid connections and etc.
The Azure Functions idea is that you need to implement a simple integration between your On-Premise or Cloud resources and you don‚Äôt want to build a complete service and just a simple function in the cloud will do the job.
"
https://medium.com/@renatogroffe/asp-net-core-azure-kubernetes-guia-de-refer%C3%AAncia-272a79248cd5?source=search_post---------362,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 23, 2018¬∑2 min read
Este post re√∫ne os diversos conte√∫dos que venho produzindo sobre a orquestra√ß√£o de containers Docker com Kubernetes, considerando o uso desta solu√ß√£o em conjunto com o ASP.NET Core e o Microsoft Azure. Voc√™s poder√£o encontrar aqui artigos, apresenta√ß√µes e projetos de exemplo envolvendo o uso conjunto destas tecnologias.
N√£o se trata de um material definitivo, j√° que pretendo manter este post sempre que poss√≠vel atualizado com novos artigos, apresenta√ß√µes e v√≠deos.
E aproveito este espa√ßo para deixar aqui um convite.
Dia 25/07/2018 (quarta-feira) √†s 21h30 - hor√°rio de Bras√≠lia - teremos mais um hangout no Canal .NET. Confira esta apresenta√ß√£o online com o MVP Elemar J√∫nior e aprenda mais sobre a modelagem de Microsservi√ßos com base em processos de neg√≥cio.
Para efetuar a sua inscri√ß√£o acesse a p√°gina do evento no Facebook ou ent√£o o Meetup. A transmiss√£o acontecer√° via YouTube, em um link a ser divulgado em breve.
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem - parte 1
ASP.NET Core + Azure + Kubernetes: orquestra√ß√£o de containers na nuvem - parte 2
ASP.NET Core 2.1 + Docker + Alpine Linux + Docker Hub + Kubernetes + AKS (Azure Kubernetes Service)
ASP.NET Core 2.0 + Docker + Kubernetes + Azure Container Registry + AKS (Azure Kubernetes Service)
E para fechar este post, caso tenha interesse em saber mais sobre o uso de Docker com tecnologias Microsoft acesse o seguinte guia de refer√™ncia:
Docker para Desenvolvedores .NET - Guia de Refer√™ncia
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
18 
18¬†
18 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://levelup.gitconnected.com/manage-azure-event-hubs-with-azure-service-operator-on-kubernetes-be61decb069?source=search_post---------363,"Azure Service Operator is an open source project to help you provision and manage Azure services using Kubernetes. Developers can use it to provision Azure services from any environment, be it Azure, any other cloud provider or on-premises ‚Äî Kubernetes is the only common denominator!
It can also be included as a part of CI/CD pipelines to create, use and tear down Azure resources on-demand. Behind the scenes, all the heavy lifting is taken care of by a combination of Custom Resource Definitions which define Azure resources and the corresponding Kubernetes Operator(s) which ensure that the state defined by the Custom Resource Definition is reflected in Azure as well.
Read more in the recent announcement here ‚Äî https://cloudblogs.microsoft.com/opensource/2020/06/25/announcing-azure-service-operator-kubernetes/
In this blog post:
The code is available in this GitHub repo https://github.com/abhirockzz/eventhubs-using-aso-on-k8s
Azure Service Operator supports many Azure services including databases (Azure Cosmos DB, PostgreSQL, MySQL, Azure SQLetc.), core infrastructure components (Virtual Machines, VM Scale sets, Virtual Networks etc.) and others as well.
It also supports Azure Event Hubs which is a fully managed data streaming platform and event ingestion service with support for Apache Kafka and other tools in the Kafka ecosystem. With Azure Service Operator you can provision and manage Azure Event Hubs namespaces, Event Hub and Consumer Groups.
So, let‚Äôs dive in without further ado! Before we do that, please note that you will need the following in order to try out this tutorial:
Start by getting an Azure account if you don‚Äôt have one already ‚Äî you can get for FREE! Please make sure you‚Äôve kubectl and Helm 3 installed as well.
Although the steps outlined in this blog should work with any Kubernetes cluster (including minikube etc.), I used Azure Kubernetes Service (AKS). You can setup a cluster using Azure CLI, Azure portal or even an ARM template. Once that's done, simply configure kubectl to point to it
Ok, you‚Äôre now ready to‚Ä¶
Nothing too fancy about it‚Ä¶ just following the steps to install it using Helm
Start by installing cert-manager
Since the operator will create resource on Azure, we need to authorize it to do so by providing the appropriate credentials. Currently, you can use Managed Identity or Service Principal
I will be using a Service Principal, so let‚Äôs start by creating one (with Azure CLI) using the az ad sp create-for-rbac command
Setup required environment variables:
Add the repo, create namespace
Use helm upgrade to initiate setup:
Before you proceed, wait for the Azure Service Operator Pod to startup
Start by cloning the repo:
Create an Azure Resource Group
I have used the southeastasia location. Please update eh-resource-group.yaml if you need to use a different one
Create Event Hubs namespace
I have used the southeastasia location. Please update eh-namespace.yaml if you need to use a different one
Once done, you should see this:
You can get details with kubectl describe eventhubnamespacesand also double-check using az eventhubs namespace show
The namespace is ready, we can now create an Event Hub
You can get details with kubectl describe eventhub and also double-check using az eventhubs eventhub show
As a final step, create the consumer group
This is addition to the default consumer group (appropriately named $Default)
You can get details with kubectl describe consumergroup and also double-check using eazventhubs eventhub consumer-group show
Let‚Äôs make use of what we just setup! We‚Äôll deploy a pair of producer and consumer apps to Kubernetes that will send and receive messages from Event Hubs respectively. Both these client apps are written in Go and use the Sarama library for Kafka. I am not going to dive into the details since they are relatively straightforward
Deploy the consumer app:
Keep a track of the logs for the consumer app:
You should see something similar to:
Using another terminal, deploy the producer app:
Once the producer app is up and running, the consumer should kick in, start consumer the messages and print them to the console. So you‚Äôll see logs similar to this:
In case you want to check producer logs as well: kubectl logs -f $(kubectl get pods -l=app=eh-producer --output=jsonpath={.items..metadata.name})
Alright, it worked!
‚Ä¶ how did the consumer and producer apps connect to Event Hubs without connection info, credentials etc.?
Notice this part of Event Hub manifest (eh-hub.yaml file):
secretName: eh-secret ensured that a Kubernetes Secret was created with the required connectivity details including connection strings (primary, secondary), keys (primary, secondary), along with the basic info such as Event Hubs namespace and hub name.
The producer and consumer Deployments were simply able to refer to this. Take a look at this snippet from the consumer app Deployment
The app uses env vars EVENTHUBS_CONNECTION_STRING, EVENTHUBS_NAMESPACE and EVENTHUBS_TOPIC whose values were sourced from the Secret (eh-secret). The value for EVENTHUBS_CONSUMER_GROUPID is hardcoded to eh-aso-cg which was the name of the consumer group specified in eh-consumer-group.yaml.
To remove all the resources including Event Hubs and the client apps, simply use kubectl delete -f deploy
Azure Service Operator provides a layer of abstraction on top Azure specific primitives. It allows you to manage Azure resources and also provide ways to connect to them using other applications deployed in the same Kubernetes cluster.
I covered Azure Event Hubs as an example, but as I mentioned earlier, Azure Service Operator also supports other services too. Head over to the GitHub repo and give them a try!
Coding tutorials and news.
101 
101¬†claps
101 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
"
https://medium.com/awesome-azure/azure-storage-replication-overview-azure-storage-data-redundancy-options-f8fc1c03042d?source=search_post---------364,"There are currently no responses for this story.
Be the first to respond.
Introduction to Azure Storage Data Redundancy Options.
Data availability is business-critical for most organizations. Data in Azure is replicated to ensure that it‚Äôs always available, even if a datacenter or region becomes inaccessible or a specific piece of hardware fails.
In Azure Storage, you have several options for replication. Each replication option provides a‚Ä¶
"
https://medium.com/swlh/crossing-the-streams-with-azure-event-hubs-and-stream-analytics-a-cloud-guru-d81a8020d4b4?source=search_post---------365,"There are currently no responses for this story.
Be the first to respond.
This blog provides a practical example of how to use Azure Stream Analytics to process streaming data from Azure Event Hubs. You should be able to go through this tutorial using the Azure Portal (or Azure CLI), without writing any code. There are also other resources for exploring stream processing with Azure Stream Analytics at the end of this blog post.
What‚Äôs covered?
Azure Stream Analytics is a real-time analytics and complex event-processing engine designed to analyze and process high volumes of fast-streaming data from multiple sources simultaneously. It supports the notion of a Job, each of which consists of an input, query, and an output. Azure Stream Analytics can ingest data from Azure Event Hubs (including Azure Event Hubs from Apache Kafka), Azure IoT Hub, or Azure Blob Storage. The query, which is based on SQL query language, can be used to easily filter, sort, aggregate, and join streaming data over a period of time.
Assume you have an application that accepts processed orders from customers and sends them to Azure Event Hubs. The requirement is to process the ‚Äúraw‚Äù orders data and enrich it with additional customer info such as name, email, location etc. To get this done, you can build a downstream service that will consume these orders from Event Hubs and process them. In this example, this service happens to be an Azure Stream Analytics job (which we‚Äôll explore later of course!)
In order to build this app, we would need to fetch this customer data from an external system (for example, a database) and for each customer ID in the order info, we would query this for the customer details. This will suffice for systems with low-velocity data or where end-to-end processing latency isn‚Äôt a concern. But it will pose a challenge for real-time processing on high-velocity streaming data.
Of course, this is not a novel problem! The purpose of this blog post is to showcase how you can use Azure Stream Analytics to implement a solution. Here are the individual components:
Azure Stream Analytics jobs connect to one or more data inputs. Each input defines a connection to an existing data source ‚Äî in this case, its Azure Event Hubs.
An individual order is a JSON payload that looks like this:
Customer information is provided as reference data. Although, the customer information is likely to change (e.g., if the customer changes her phone number), for the purposes of this example, we‚Äôll treat it is static reference data stored in Azure Blob Storage container.
This is the workhorse of our solution! It joins (a continuous stream of) orders data from Azure Event Hubs with the static reference customers data based on the matching customer ID (which is id in the customers data set and id in the orders stream)
Simply put, an Output lets you store and save the results of the Stream Analytics job. In this example, to keep things simple we continue to use Azure Event Hubs (a different topic) as the output.
Now that you have a conceptual overview, it‚Äôs time to dive in. All you need is an Azure account. If you don‚Äôt have it already, just grab one for free.
In this section, you‚Äôll:
You need to create an Event Hubs Namespace and Hub (topic). There are lots of options including Azure Portal, Azure CLI, ARM template or Azure PowerShell
Please note that you need to create two topics:
You‚Äôll need to create an Azure Storage account. This quickstart walks you through this process and provides guidance for Azure Portal, Azure CLI, etc. Once that‚Äôs done, go ahead and create a container using the Azure Portal or the Azure CLI if you prefer.
Save the JSON below to a file and upload it to the storage container you just created.
Start by creating an Azure Stream Analytics job. If you want to use the Azure Portal, just follow the steps outlined in this section or use the Azure CLI instead if you don‚Äôt prefer clicking on a UI.
To configure Azure Event Hubs Input
Open the Azure Stream Analytics job you just created and configure Azure Event Hubs as an Input. Here are some screenshots which should guide you through the steps:
Choose Inputs from the menu on the left
Select + Add stream Input > Event Hub
Enter Event Hubs details ‚Äî the portal provides you the convenience of choosing from existing Event Hub namespaces and respective Event Hub in your subscription, so all you need to do is choose the right one.
To configure Azure Blob Storage Input:
Choose Inputs from the menu on the left
Select Add reference input > Blob storage
Enter/choose Blob Storage details
Once you‚Äôre done, you should see the following Inputs:
Azure Stream Analytics allows you to test your streaming queries with sample data. In this section, we‚Äôll upload sample data for orders and customer information for the Event Hubs and Blob Storage inputs respectively.
Open the Azure Stream Analytics job, select Query and upload sample orders data for Event Hub input
Save the JSON below to a file and upload it.
Open the Azure Stream Analytics job, select Query and upload sample orders data for Blob storage input
You can upload the same JSON file that you uploaded to Blob Storage earlier.
Now, configure and run the below query:
Open the Azure Stream Analytics job, select Query and follow the steps as depicted in the screenshot below:
Select Query > enter the query > Test query and don‚Äôt forget to select Save query
The query JOINs orders data from Event Hubs it with the static reference customers data (from Blob storage) based on the matching customer ID (which is id in the customers data set and id in the orders stream.)
Explore or dig into the Stream Analytics query reference
It was nice to have the ability to use sample data for testing our streaming solution. Let‚Äôs go ahead and try this end to end with actual data (orders) flowing into Event Hubs.
An Output is required in order to run a Job. In order to configure the Output, select Output > + Add > Event Hub
Enter Event Hubs details: the portal provides you the convenience of choosing from existing Event Hub namespaces and respective Event Hub in your subscription, so all you need to do is choose the right one.
In the Azure Stream Analytics interface, select Overview, click Start and confirm
Wait for the Job to start, you should see the Status change to Running
Note: Although I have used kafkacat, you're free to choose any other mechanism (CLI or programmatic). This documentation provides lots of examples
Start a consumer to listen from Event Hubs output topic
To keep things simple, we can use the kafkacat CLI to produce orders and consume enriched events (instead of a program). Just install it and you should be good to go.
Create a kafkacat.conf file with Event Hubs info:
Let‚Äôs first start the consumer process that will connect to the output topic ( customer-orders) which will get the enriched order information from Azure Stream Analytics
In another terminal, start sending order info to the orders topic
In a terminal:
This will block, waiting for records from customer-orders.
You can send order data via stdout. Simply paste these one at a time and observe the output in the other terminal:
The output you see on the consumer terminal should be similar to this:
Notice how the order info is now enriched with customer data (name, location in this case). You can use the information in this topic anyway you want. For example, you can persist this enriched materialized view to Azure Cosmos DB, trigger an Azure Function, etc.
As expected, you won‚Äôt see a corresponding enriched event corresponding to orders placed by customers whose ID isn‚Äôt present in the reference customer data (in Blob Storage), since the JOIN criteria is based on the customer ID.
This brings us to the end of this tutorial! I hope it helps you get started with Azure Stream Analytics and test the waters before moving on to more involved use cases.
In addition to this, there‚Äôs plenty of material for you to dig in.
High-velocity, real-time data poses challenges that are hard to deal with using traditional architectures ‚Äî one such problem is joining these streams of data. Depending on the use case, a custom-built solution might serve you better, but this will take a lot of time and effort to get it right. If possible, you might want to think about extracting parts of your data processing architecture and offloading the heavy lifting to services which are tailor-made for such problems.
In this blog post, we explored a possible solution for implementing streaming joins using a combination of Azure Event Hubs for data ingestion and Azure Stream Analytics for data processing using SQL. These are powerful, off-the-shelf services that you are able to configure and use without setting up any infrastructure, and thanks to the cloud, the underlying complexity of the distributed systems involved in such solutions is completely abstracted from us.
Originally published at https://acloudguru.com on September 1, 2020.
Get smarter at building your thing. Join The Startup‚Äôs +750K followers.
114 
114¬†claps
114 
Get smarter at building your thing. Follow to join The Startup‚Äôs +8 million monthly readers & +750K followers.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
Get smarter at building your thing. Follow to join The Startup‚Äôs +8 million monthly readers & +750K followers.
"
https://medium.com/microsoftazure/azure-automated-machine-learning-a4d12eb95602?source=search_post---------366,"There are currently no responses for this story.
Be the first to respond.
Automated Machine Learning (AutoML) is a service available within the Azure Machine Learning (AML) service.
As the name suggests, the main goal of AutoML is that of automating the development and scoring of different types of ML models to eventually pick the one which is the most performing. Basically, AutoML offers the possibility to parallelize‚Ä¶
"
https://medium.datadriveninvestor.com/meet-azure-logic-apps-to-know-its-amazing-benefits-e2a622b06907?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
Wondering what are Azure Logic Apps? No worries I‚Äôve got your back covered!
Azure Logic Apps is a cloud service that helps you automate and orchestrate tasks, business processes, and workflows when you need to integrate apps, systems, data, and services across enterprises or organizations. Logic Apps simplifies how you design and build scalable solutions for app integration, data/system integration, enterprise application integration, and business-to-business (B2B) communication, whether in the cloud, on-premises, or both.
Here are some of the examples where you could use Azure logics apps
These are some of the benefits too, you will have more idea about it as you continue reading about it below.
Have a look at a quick video below
Enterprise integration solutions can be built with Azure logic Apps with the help of readily available connectors. Connectors play an integral part when you create automated workflows with Azure Logic Apps. By using connectors in your logic apps, you expand the capabilities for your on-premises and cloud apps to perform tasks with the data that you create and already have.
Each connector offers a set of operations classified as ‚ÄòActions‚Äô and ‚ÄòTriggers‚Äô. Once you connect to the underlying service, these operations can be easily leveraged within your apps and workflows.
Actions are changes directed by a user. For example, you would use an action to look up, write, update, or delete data in a SQL database. All actions directly map to operations defined in the Swagger.
Several connectors provide triggers that can notify your app when specific events occur. For example, the FTP connector has the OnUpdatedFile trigger. You can build either a Logic App or Flow that listens to this trigger and performs an action whenever the trigger fires
There are two types of trigger.
Polling Triggers: These triggers call your service at a specified frequency to check for new data. When new data is available, it causes a new run of your workflow instance with the data as input.
Push Triggers: These triggers listen for data on an endpoint, that is, they wait for an event to occur. The occurrence of this event causes a new run of your workflow instance.
Of course, it does come with a big box of benefits or rather why you should consider using Azure Logic Apps.
parameters, triggers, actions, outputs are the main parts of any logic apps.
Parameters are things that you want to reuse across workflows. Re-using values or even complex objects throughout the definition makes it easier to comprehend. Separate out configuration from the definition itself makes sharing easy as well as across different environments.
Triggers are what start the Logic App. Triggers can be evaluated at recurring intervals or have its state maintained across executions.
Actions are the things that happen in a Logic App. Actions can depend on other actions. The dependencies are what will determine the order of the Actions executing. The state is also maintained across executions.
Outputs are what gets sent from calls into the workflow.
I hope the basics of azure logic are clear and you are now ready to roll with the actual implementation. üòÑ
For more detailed understanding about azure logic apps here is the link
docs.microsoft.com
Happy Learning! üíª
empowerment through data, knowledge, and expertise.
106 
106¬†claps
106 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
I talk about JavaScript, Web Development, No-code to help you stay ultra-modern. See you on Twitter ‚Äî https://twitter.com/MakadiaHarsh
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@burkeholland/solving-the-azure-functions-challenge-with-javascript-65205b0c4920?source=search_post---------368,"Sign in
There are currently no responses for this story.
Be the first to respond.
Burke Holland
Jun 15, 2017¬∑8 min read
If you‚Äôve found this post, you‚Äôre probably looking for help on how to solve the Azure Functions Challenge using JavaScript. If that isn‚Äôt why you‚Äôre here, and you don‚Äôt know what the Azure Functions Challenge is or what Azure Functions are or even what Azure is, that‚Äôs ok too! You‚Äôll learn a lot just by going through this exercise. Click here to get started with the challenge.
Let‚Äôs get right to it. This is how I solved the Azure Functions Challenge with JavaScript.
The first puzzle is pretty simple as it just wants to show you how to create a function and have it return a value.
Create a new Generic HTTP Function with JavaScript. Now that you‚Äôve done that, you already have a function that returns a response. All you need to do is modify it to echo back out the value you received as the ping but with a key of ‚Äúpong‚Äù. It should look something like this.
Pretty easy. This is the most fundamental way Azure Functions work. It‚Äôs about to get more complicated pretty quickly.
Note that the HTTP Function and Generic Webhook function templates are not the same. The challenge will ask you to create a Webhook function, but you can use the HTTP Function template as well. The code samples here assume the HTTP Function so won‚Äôt work with Webhook.
This is exercise is a simple translation of a series of numbers into words. Kind of like a Little Orphan Annie Decoder Ring.
Create another JavaScript HTTP Trigger Function in the same project space.
This time the challenge is going to send you an object that contains a unique key, a msg string of numbers and a cipher which maps two digit numbers to characters. They want you to‚Ä¶
First, let‚Äôs look at splitting the string of numbers into 2 digit sets. There are a few ways to do this‚Ä¶
You could loop over the number and just count your way through it, storing the numbers in an array.
If the word ‚Äúregex‚Äù makes you queasy, you‚Äôre in good company. Everyone feels that way. There are only two types of people in the world: those that hate regex and those who won‚Äôt admit it.
You don‚Äôt need to master regex to use it. Just know what it can do and then Google. That‚Äôs how the rest of us are doing it.
In the case of this string, the regular expression to split the number into two character sets is dead simple‚Ä¶
Now that we‚Äôve got the array of numbers, we need to find their letter equivalents in the cipher object. Unfortunately, the cipher object is not at all in the format we need it. The letters are the keys and the numbers are the values. That‚Äôs going to force us to write a loop.
Or, we can just use Underscore to invert the keys and values in the object because Underscore is amazing. Come at me with your Lodash.
We can just install Underscore from npm because you can do that in the Azure Portal.
You can install npm modules via the web interface for Azure functions just like you would do on a terminal. If you are logged into your Azure Portal, just go to the Function project, then select ‚ÄúPlatform features‚Äù and open the console. Then it‚Äôs just npm install underscore.
If you‚Äôre using the free Try Azure sandbox, you don‚Äôt have access to the console so you can‚Äôt install npm modules. Instead you can use this handy function that I shamelessly stole from StackOverflow. Shout out to user tnanoba!
At this point, all we have to do is loop over the array of two digit numbers and compose the result.
On to the next challenge!
This one is going to send us a key and an array and it wants us to store the array by it‚Äôs key somewhere and then return it in a second function. This is great because we get to look at input and output bindings in Azure Functions as well as Azure Table Storage.
When it comes to data, you can think of bindings like this: Input bindings are for when you want to read data. Output bindings are when you want to write data. You can create bindings to all sorts of database stores, including CosmosDB and other fancy things, but for our purposes, simple Azure Table Storage will work just fine.
Create a new HTTP Trigger function. Select the ‚Äúintegrate‚Äù option beneath the function and then create a new output binding. You can select Azure Table Storage when it asks what kind of binding you want to create.
Once the binding is created, you‚Äôll need to configure it. The ‚ÄúTable parameter name‚Äù is the name that you will use to access the table storage in your function. It can be anything. The ‚ÄúTable name‚Äù is the name of the table that will be created in storage. Whatever name you give your table here is the same name you will use to access it in the next function. Lastly, you need to choose or select a ‚ÄúStorage account connection‚Äù.
Now that the binding is created, we can access the table by calling context.bindings.outputTable. If you recall, outputTable was our ‚ÄúTable parameter name‚Äù. We can write to the table storage by specifying an object with a PartitionKey, a RowKey and ArrayOfValues. The PartitionKey and RowKey are required fields. You can tack anything else you like onto the object but you have to at least have those two. In this case, we‚Äôre tacking on ArrayOfValues.
Notice that you never to call save on the outputTable. The save happens automatically. Simply setting the outputTable value is enough.
That‚Äôs all we have to do to store the key and ArrayOfValues in Azure Table Storage. On to step 2!
In step 2, they are simply going to pass the key that they passed to the first function and we are going to retrieve the ArrayOfValues from table storage.
To do that, create a new HTTP Function. Go to the ‚ÄúIntegrations‚Äù item and add a new input binding. To configure the input binding‚Ä¶
Your final configuration for the input binding should look something like this‚Ä¶
Now we can access the inputTable on the context.bindings object and read from it. It will only contain our table entry that was created in step one because we‚Äôve already passed the key in the binding.
Note that we have to parse the ArrayOfValues on the input table as it appears that Azure Table Storage stores our array as a string. Then we sort the array numerically per the challenge.
That‚Äôs it! And now you know how to use input and output bindings as well as Azure Table Storage. We call that a ‚Äútwo-fer‚Äù.
The last step just has you deploy from GitHub to Azure functions, because you can. It‚Äôs really just a matter of filling out a form and selecting a resource group.
I really liked the Functions Challenge as I‚Äôm new to this world of Serverless applications. I don‚Äôt fully understand all of it and we‚Äôre all learning every day. One thing is for sure, we are at the very beginning of the Serverless revolution, and Azure Functions is a great place to get onboard.
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
30 
4
Thanks to Faiz Shaikh.¬†
30¬†claps
30 
4
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mauridb/work-with-json-files-with-azure-sql-8946f066ddd4?source=search_post---------369,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
Aug 7, 2017¬∑4 min read
Dealing with CSV or JSON data today is more and more common. I do it on daily basis, since the our application send data to our microservice gateway backend is in a (compressed) JSON format.
Sometimes, especially when debugging or developing a new feature, I need to access that JSON data, before is sent to any further microservices for processing or, after that, being stored in the database.
So far I usually used CloudBerry Explorer to locate and download the JSON I was interested into and that a tool like Notepad++ with JSON-Viewer plugin or Visual Studio Code to load and analyze the content.
Being Azure SQL or main database, I spend a lot of time working with T-SQL, so I would really love to be able to query JSON directly from T-SQL, without even have the need to download the file from the Azure Blob Stored where it is stored. This will make my work more efficient and easier.
I would love to access JSON where it is, just like Hadoop or Azure Data Lake allows you to do
Well, you can. I just find out that with the latest additions (added since SQL Server 2017 CTP 1.1 and already available on Azure SQL v 12.0.2000.8) it is incredibly easy.
First of all the Shared Access Signature needs to be created to allow Azure SQL to access the Azure Blob Store where you have your JSON files. This can be done using the Azure Portal, from the Azure Storage Blade
or you can also do it via the Azure CLI 2.0 as described here:
buildazure.com
Once you have the signature a Database Scoped Credential that points to the created Shared Access Signature needs to be created too:
If you haven‚Äôt done it before you will be warned that you need to create a Database Master Key before being able to run the above command.
After that credentials are created, it‚Äôs time to point to the Azure Blob Account where your JSON files are stored by creating a External Data Source:
Once this is done, you can just start to play with JSON files using the OPENROWSET along with OPENJSON:
and voil√†, JSON content are here at your fingertips. For example, I can access to all activity data contained in our ‚Äúrunning session‚Äù json:
This is just amazing: now my work is much simpler, especially when I‚Äôm traveling and, maybe, I don‚Äôt have a good internet access. I can process and work on my JSON file without even have them leaving the cloud.
If you have a CSV file the technique is very similar, and it is already documented in the official Microsoft documentation:
Examples of Bulk Access to Data in Azure Blob Storage
The same approach is doable also via SQL Server 2017 (now in RC2). You can also access file not stored in the cloud, but on your on-premises storage. In such case, of course, you don‚Äôt specify the Shared Access Signature as an authentication methods, since SQL Server will just rely on Windows Authentication. Here Jovan showed a sample usage:
Sure, there is a Gist for that:
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
19 
1
19¬†
19 
1
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://towardsdatascience.com/access-azure-database-for-mysql-from-azure-functions-with-ssl-certification-verification-b6c6784f76fb?source=search_post---------370,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Nov 9, 2019¬∑6 min read
Recently I got a customer who has relatively small volume data to be managed. So, I suggested Azure Database for MySQL. Basically, the smallest instance cost about $560 AUD per month will be‚Ä¶
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@reefwing/espressif-esp32-tutorial-ir-remote-control-using-microsoft-azure-ed768a5cea4d?source=search_post---------371,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Such
Aug 9, 2018¬∑15 min read
This tutorial will outline how to create an IR Remote using the ESP32 and then control it from the IoT hub on Microsoft Azure. Driving an IR remote transmitter using an Arduino is simple, as there is a library, called IRremote.h which does all the hard work. You just need to connect your IR transmitter module signal pin to the appropriate Arduino pin, via a current limiting‚Ä¶
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/create-a-power-bi-dashboard-using-azure-application-insights-analytics-data-9f9e2435640c?source=search_post---------372,"In some previous posts I explained what is Azure Application Insight Analytics, and in this post I will show you how you can create Power BI dashboard using Azure Application Insights Analytics data.
So to start, you should login to the Azure Portal and navigate to your Application Insight resource there which will look like below:
"
https://medium.com/awesome-azure/azure-virtual-network-vnet-peering-overview-introduction-a795517bd83b?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
Introduction to Virtual Network (VNet) Peering in Azure ‚Äî What is VNet Peering?
VNet peering (or virtual network peering) enables you to connect virtual networks. A VNet peering connection between virtual networks enables you to route traffic between them privately through IPv4 addresses. Virtual machines in the peered VNets can communicate with‚Ä¶
"
https://medium.com/awesome-azure/azure-difference-between-azure-expressroute-and-azure-vpn-gateway-comparison-azure-hybrid-connectivity-5f7ce02044f3?source=search_post---------374,"There are currently no responses for this story.
Be the first to respond.
Comparison ‚Äî Azure ExpressRoute vs Azure VPN Gateway.
ExpressRoute provides direct connectivity to Azure cloud services and connecting Microsoft‚Äôs global network. All transferred data is not encrypted, and do not go over the public Internet.
"
https://medium.com/wortell/azure-sentinel-designing-access-and-authorizations-that-meet-the-enterprise-needs-501bfdafaa5f?source=search_post---------375,"There are currently no responses for this story.
Be the first to respond.
You‚Äôve successfully deployed Azure Sentinel and are collecting data and using it for monitoring and hunting purposes. Quickly after, your company‚Äôs privacy offer or auditor points out that both the law (for instance: GDPR & AVG) and the company‚Äôs requirements don‚Äôt allow all admins to have access all the time to all of that personal identifiable data.
You need to come up with a solution to design access to Azure Sentinel in a way that the SecOps people can work with the alerts, the SIEM admins can create/modify rules, and that hunters can sift through all the data to find what they are looking for. How should you do that? In this blog we‚Äôll share some design considerations.
Azure Log Analytics
Azure Sentinel uses a Log Analytics workspace to store its data. Recently, Microsoft introduced a more granular role-based access module for Log Analytics. Previously, we only had the workspace-context access mode, but now we also have a resource-context access mode. For each Log Analytics workspace you can chose the desired access mode.
Resource-context access mode allows you to set permissions all the way down to individual tables in the Log Analytics workspace, aka Table Level RBAC.
You can change the current workspace access control mode from the Properties page of the workspace, which can be found under the Workspace settings node in the Azure Sentinel UI. (Changing the setting will be disabled if you don‚Äôt have permissions to configure the workspace)
PRO TIP: Want to quickly know if your Log Analytics workspace is enabled for resource-context access mode? Run this Powershell command:
Get-AzResource -ResourceType Microsoft.OperationalInsights/workspaces -ExpandProperties | foreach {$_.Name + ‚Äú: ‚Äú + $_.Properties.features.enableLogAccessUsingOnlyResourcePermissions}
Want to enabled resource-context permissions for all your workspaces? Run this script:
Azure Active Directory
Because Azure Sentinel uses Log Analytics as the backend, part of the Azure platform, it therefore also uses Azure Active Directory for its identities. Azure has two built-in user roles for Log Analytics workspaces: Log Analytics Reader and Log Analytics Contributor. Members of the Log Analytics Reader role can:
Members of the Log Analytics Contributor role can:
These roles can be given to uses at different scopes:
Custom roles
If the built-in roles don‚Äôt meet the specific needs of your enterprise, you can create your own custom roles. Just like built-in roles, you can assign custom roles to users, groups, and service principals at subscription, resource group, and resource scopes.
PRO TIP: Custom roles are stored in an Azure Active Directory (Azure AD) directory and can be shared across subscriptions. Each directory can have up to 5000 custom roles. (For specialized clouds, such as Azure Government, Azure Germany, and Azure China 21Vianet, the limit is 2000 custom roles.)
You implement table access control with Azure custom roles to either grant or deny access to specific tables in the workspace. These roles are applied to workspaces with either workspace-context or resource-context access control modes regardless of the user‚Äôs access mode.
Create a custom role with the following actions to define access to table access control.
For example, to create a SecOps role for investigations with access only to the SecurityAlert and AzureActivity tables, create a custom role as follows:
Custom Logs
You can‚Äôt currently grant or deny access to individual custom logs, but you can grant or deny access to all custom logs. To create a role with access to all custom logs, create a custom role using the following actions:
PRO TIP: Tobias Zimmergren wrote a great blog how to log custom application security events in Azure Sentinel. You can read it here. These custom log tables end with _CL in their naming in the Log Analytics workspace.
Considerations
PRO TIP: Unsure who has what access? Use the ‚ÄòAccess control IAM‚Äô node from the Advanced Settings pane of your Log Analytics workspace to find out:
Privileged Identity Management
Azure AD Privileged Identity Management (PIM) enables you to set up IAM in a way that users and accounts don‚Äôt carry the required roles and permissions all the time.
Accounts are ahead of time enabled to request a certain role. Once they want to use that role, just in time they put in a request to use that role and for how long and, depending on the configuration in PIM, that request can then be evaluated and granted or denied. The great thing is that Azure AD PIM also works for custom roles.
And example would be where a Threat Hunter would use a regular Azure AD account and then go to the PIM interface to request the SecOps investigator role to access all the required information in Azure Sentinel for his or her investigation.
More information on Azure AD PIM and how to set it up, can be found here.
PRO TIP: Azure AD PIM is a premium feature in Azure. You need either a Azure AD P2 license for the user that needs PIM functionality, or license it through an EM+S E5 or Microsoft 365 E5 license.
Monitoring PIM usage
Most enterprises and auditors will also require you to monitor that role usage. This can be easily done using the Azure AD audit logs. Make sure you enable the Azure Active Directory connector in Azure Sentinel so that the data type ‚ÄòAuditLogs‚Äô is collected.
The PIM operations are stored in the AuditLogs table. You need to filter on the category for ‚ÄòResourceManagement‚Äô and operationname containing ‚ÄòPIM‚Äô. Here‚Äôs a base KQL query for hunting:
You could use a similar query to create a rule to alert the SOC when that role is being used:
Conclusion
Using a combination of the new resource-context access mode for your Log Analytics workspace, custom Azure AD roles, and Privileged Identity Management, you can achieve most if not all of the requirements that your enterprise and auditors have for your Azure Sentinel deployment.
Happy hunting!
‚Äî Maarten Goet, MVP & RD
UPDATE: Oleg Ananiev, group program manager at Microsoft for Azure Log Analytics, adds the following remark:
‚ÄúResource-centric RBAC and table-level RBAC are orthogonal. In fact, you can use table-level RBAC for workspace queries, it does not require access control mode change for the workspace. This is especially important, as most Azure Sentinel users will likely be using workspace-centric access. Why? The prerequisite for using resource-centric access is that data must be tagged with a valid Azure resource id. This is typically not true for many types of security data, for example: Office 365.‚Äù
Microsoft Cloud & Enterprise Security
31 
31¬†claps
31 
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/multi-node-distributed-training-with-pytorch-lightning-azure-ml-88ac59d43114?source=search_post---------376,"There are currently no responses for this story.
Be the first to respond.
TL;DR This post outlines how to distribute PyTorch Lightning training on Distributed Clusters with Azure ML
Full end to end implementations can be found on the official Azure Machine Learning GitHub repo.
github.com
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
In my last few posts on the subject, I outlined the benefits of both PyTorch Lightning and Azure ML to simplify training deep learning models and logging. Take a look you haven‚Äôt yet check it out!
medium.com
medium.com
Now that you are familiar with both the benefits of Azure ML and PyTorch lighting let‚Äôs talk about how to take PyTorch Lighting to the next level with multi node distributed model training.
Multi Node Distributed Training is typically the most advanced use case of the Azure Machine Learning service. If you want a sense of why it is traditionally so difficult, take a look at the Azure Docs.
docs.microsoft.com
PyTorch Lighting makes distributed training significantly easier by managing all the distributed data batching, hooks, gradient updates and process ranks for us. Take a look at the video by William Falcon here to see how this works.
We only need to make one minor modification to our train script for Azure ML to enable PyTorch lighting to do all the heavy lifting in the following section I will walk through the steps to needed to run a distributed training job on a low priority compute cluster enabling faster training at an order of magnitude cost savings.
docs.microsoft.com
Create Azure ML Workspace from the Portal or use the Azure CLI
Connect to the workspace with the Azure ML SDK as follows
docs.microsoft.com
To run PyTorch Lighting code on our cluster we need to configure our dependencies we can do that with simple yml file.
We can then use the AzureML SDK to create an environment from our dependencies file and configure it to run on any Docker base image we want.
Create a ScriptRunConfig to specify the training script & arguments, environment, and cluster to run on.
We can use any example train script from the PyTorch Lighting examples or our own experiments.
Once we have our training script we need to make one minor modification by adding the following function that sets all the required environmental variables for distributed communication between the Azure nodes.
Then after parsing the input arguments call the above function.
Hopefully in the future this step will be abstracted out for us.
For Multi Node GPU training , specify the number of GPUs to train on per a node (typically this will correspond to the number of GPUs in your cluster‚Äôs SKU), the number of nodes(typically this will correspond to the number of nodes in your cluster) and the accelerator mode and the distributed mode, in this case DistributedDataParallel (""ddp""), which PyTorch Lightning expects as arguments --gpus --num_nodesand --accelerator, respectively. See their Multi-GPU training documentation for more information.
Then set the distributed_job_config to a new MpiConfiguration with equal to one(since PyTorch lighting manages all the distributed training)and a node_count equal to the --num_nodes you provided as input to the train script.
We can view the run logs and details in realtime with the following SDK commands.
And there you have it with out needing to deal with managing the complexity of distributed batching, Cuda, MPI, logging callbacks, or process ranks, PyTorch lighting scale your training job to as many nodes as nodes as you‚Äôd like.
You shouldn‚Äôt but if you have any issues let me know in the comments.
I want to give a major shout out to Minna Xiao and Alex Deng from the Azure ML team for their support and commitment working towards a better developer experience with Open Source Frameworks such as PyTorch Lighting on Azure.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft‚Äôs Cloud Developer Advocacy team, he collaborates with the Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
79 
2
79¬†claps
79 
2
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@jeffhollan/build-serverless-alexa-and-google-assistant-skills-with-azure-f076512c2dfa?source=search_post---------377,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Mar 12, 2017¬∑3 min read
OK Google, how many new users did we add yesterday?
Alexa, swap back production and staging slots.
Hey Cortana, send my Office documents for approval.
Sounds like a not-so-distant future? It may be closer than you think.
Personal assistants like Cortana, Google Home, and Amazon Alexa are becoming integration hubs for consumers. They provide instant access to news, weather, shopping, traffic, and more. However there may be certain integrations that are more custom than what is out-of-the-box or readily available, or that leverage services that are not supported (or custom) themselves. Azure Logic Apps provides an easy way to build your own custom skills that can be used to extend these personal assistants into any number of our over 100 cloud and on-premises connectors. The best part? You could build the whole skill without any code.
All right, now to the nitty-gritty. For this blog I‚Äôm going to focus on how you can register a custom skill with Amazon Alexa, but the same pattern could be followed for Google Assistant and I assume Cortana.
To integrate with Alexa we will use the webhook pattern:
Logic Apps fits very naturally into steps 2‚Äì4. By using the Logic Apps HTTP Request Trigger a secure HTTP endpoint is automatically generated for us. We use connectors and control flow (potentially a switch statement to route intents) to process the response, and send back a response using the HTTP Response action.
After creating a new Logic App, add an HTTP Request trigger and define the request payload you expect to receive. In this case we expect an Alexa request so you can use this schema.
You can then begin to add actions to get the data you need for the response. For example, if I asked ‚ÄúHow many customers did we get today?‚Äù I may add a Dynamics 365 List Records action with a filter for customers added today and return the @length() of the response.
Once the data has been composed, add a response action to return the response to Alexa. You can use a number of different properties, but a basic speech response looks like this:
That‚Äôs it! Now simply go to the Alexa Skills Kit and register a new skill using the Request URL generated when you save. You can test the app in the Alexa console to make sure everything is working end-to-end before publishing.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
18 
1
18¬†
18 
1
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
"
https://medium.com/@maarten.goet/azure-sentinel-fusion-machine-learning-for-a-secops-world-64ccda3de5f8?source=search_post---------378,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 14, 2019¬∑7 min read
The annual RSA conference just wrapped up in San Francisco. With the introductions of Chronicle‚Äôs Backstory (Google) and Azure Sentinel, 2019 became the year of the ‚ÄòCloud SIEM‚Äô.
Why is this important? VisibleRisk summarizes it as: ‚Äúbecause these types of products can flip two decades of ‚Äúnormal‚Äù on their head and finally position those who defend our enterprises in a way that they can keep pace with the furious pace of change they face.‚Äù
Azure Sentinel leverages the immense compute power of the cloud and sophisticated machine learning models to help defenses in the enterprise. Microsoft calls is Azure Sentinel FUSION.
Azure Sentinel FUSION? Say what?
If you go to the Overview page in Azure Sentinel you‚Äôll see a reference in the bottom right corner a section called: Democratize ML for your SecOps. It says:
‚ÄúUnlock the power of AI for security professionals by leveraging MS cutting edge research and best practices in ML, regardless of your current investment level in ML.‚Äù
If you click on the Learn More link it brings you to this page.
Enabling Fusion
There is no UI to enable Fusion, however if you have an instance of Azure Sentinel running, you can use Azure Cloud Shell and the ‚Äòaz‚Äô command to enable Fusion for your Log Analytics workspace.
Start Azure Cloud Shell:
Run the following command:
You get back the result that you are now enabled for FUSION:
OK, now what?
Great question.
Because there is only one page of documentation online, I reached out to the Azure Sentinel product engineering team in Israel, and asked them what Fusion does and got this response:
‚ÄúFusion looks at alerts coming from different sources and tries to find out if there‚Äôs a connection between them in order to fuse them into one case with higher confidence.‚Äù
‚ÄúThink about having multiple low fidelity alerts that no one had the time to investigate, we tell you if you should investigate them by fusing them into one case.‚Äù
Microsoft‚Äôs documentation does give another couple of clues:
‚ÄúMachine Learning in Azure Sentinel is built-in right from the beginning. We have thoughtfully designed the system with ML innovations aimed to make security analysts, security data scientists and engineers productive. One such innovation is Azure Sentinel Fusion built especially to reduce alert fatigue.‚Äù
‚ÄúFusion uses graph powered machine learning algorithms to correlate between millions of lower fidelity anomalous activities from different products such as Azure AD Identity Protection, and Microsoft Cloud App Security, to combine them into a manageable number of interesting security cases.‚Äù
Unified SecOps
Not coincidently, Microsoft announced last week that they are integrating Cloud App Security, Azure ATP and Azure AD identity protection into an unified SecOps experience and portal:
Based on three pillars
So why are all security vendors adding machine learning and artificial intelligence to their solution? Well, first of all: sifting through tons of alerts in a SIEM is not something security analysts love doing. Their skill set can also be better put to work to hunt for bad actors, based on pre-filtered signals.
Secondly, it is well known that security analysts are drowning in those alerts and sometimes miss the critical piece to launch to the next step of investigation. In fact, Mark Russinovich laid out Microsoft‚Äôs strategy dealing with this three years ago.
Ram Shankar, who works on the Microsoft Azure team, wrote that the ML team behind Azure Sentinel FUSION asked three questions:
The ML team came up with these three ideas:
1: Probabilistic Kill Chain
Garden variety detections assume static kill chain. Not true ‚Äî real world attacks are complex and multistage. So, the ML Team modeled the probability of moving to the next step is conditioned not only on previous step but also factors like current asset.
2: Iterative attack simulation
A lot of noise looks like legit attacks because detections explore only one line of attack. For every alert, the ML team iteratively simulates multiple lines of attack using random walk style algorithms to evaluate if this attack is truly feasible.
3: Encode domain knowledge as priors!
Incorporating Bayesian methods to tap into expert‚Äôs domain knowledge is painfully obvious but the common hurdle inference style algorithms are slow. Not a problem because Azure Sentinel is a cloud based SIEM and the ML team can leverage the cloud‚Äôs scalable + compute.
These three ideas form the bedrock of Fusion, that Ram claims has shown to reduce alert fatigue by 90%.
MCAS & Azure ATP
Going back to the Data Collection page in Azure Sentinel and clicking on Azure Advanced Threat Protection (ATP) data source, we find another clue:
‚ÄúConnect Azure Advanced Threat Protection to Azure Sentinel: if your tenant is running the Azure ATP preview in Microsoft Cloud App Security, connect here to stream your Azure ATP alerts into Azure Sentinel.‚Äù
PRO TIP: Both Cloud App Security (MCAS) and Azure Active Directory data sources need to be connected for the current (preview) release of Azure Sentinel Fusion to work.
Azure Sentinel FUSION in action
The scenario we‚Äôll be demonstrating is where a user‚Äôs credentials are stolen, and the following actions happen afterwards:
Normally these two alerts are seen in different portals and it would take a security engineer to ‚Äòconnect the dots‚Äô.
However, when you connect these data sources (Azure AD, Azure ATP and Cloud App Security) to Azure Sentinel, the machine learning models behind Azure Sentinel FUSION kick in and generate a Case, showing that data is being exfiltrated:
In cybersecurity, it‚Äôs AI vs. AI
Paul Gillin of SiliconAngle wrote:
‚ÄúArtificial intelligence research group OpenAI last month made the unusual announcement: It had built an AI-powered content creation engine so sophisticated that it wouldn‚Äôt release the full model to developers.
Anyone who works in cybersecurity immediately knew why. Phishing emails, which try to trick recipients into clicking malicious links, originated 91 percent of all cyberattacks in 2016, according to a study by Cofense Inc. Combining software bots to scrape personal information from social networks and public databases with such a powerful content generation engine could produce much more persuasive phishing emails that might even mimic a certain person‚Äôs writing style, according to Nicolas Kseib, lead data scientist at TruSTAR Technology Inc.
The potential result: cybercriminals could launch phishing attacks much faster and on an unprecedented scale.‚Äù
AI is a new weapon that some people believe could finally give security professionals a leg up on their adversaries.
Conclusion
Microsoft is beating the other security vendors to the punch, having already added some real machine learning models and AI behind their just released Azure Sentinel cloud SIEM offering.
Azure Sentinel FUSION can help reduce alert fatigue, but more importantly ‚Äòconnect the dots‚Äô and provide security analysts with a clear picture of the (potential) threat.
‚Äî Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
16 
16¬†claps
16 
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/kubernetes-azure-devops-build-e-deployment-automatizado-de-aplica%C3%A7%C3%B5es-c216c35a5c64?source=search_post---------379,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 9, 2020¬∑8 min read
Em um artigo anterior demonstrei como configurar passo a passo o build e o deployment automatizados de projetos baseados em Docker no Azure DevOps, fazendo uso para isto de um projeto ASP.NET Core 3.1 e do Azure Web App for Containers para a hospedagem da aplica√ß√£o em quest√£o. Para o armazenamento das imagens geradas durante o build utilizei ainda o Azure Container Registry:
Docker + Azure DevOps: build e deployment automatizado de aplica√ß√µes
Neste novo tutorial abordarei o mesmo tipo de procedimento empregando o Azure DevOps e desta vez o Azure Kubernetes Service. A inten√ß√£o com isso √© demonstrar o suporte que o Azure DevOps nos oferece para o deployment de aplica√ß√µes em um cluster Kubernetes.
E aproveito este espa√ßo para deixar aqui um convite.
Que tal aprender mais sobre Docker, Kubernetes e a implementa√ß√£o de solu√ß√µes baseadas em containers utilizando o Microsoft Azure, em um workshop que acontecer√° durante um s√°bado (dia 04/04/2020) em S√£o Paulo Capital e implementando um case na pr√°tica?
Acesse ent√£o o link a seguir para efetuar sua inscri√ß√£o (inclui camiseta, emiss√£o de certificado e almo√ßo para todos os participantes) com desconto:http://bit.ly/anp-docker-blog-groffe
Utilizarei mais uma vez o seguinte projeto:
https://github.com/renatogroffe/ASPNETCore3.1-API-REST_Docker-Alpine
Optei na pr√°tica por duplicar este reposit√≥rio, uma vez que em tal c√≥pia ser√£o gravados arquivos com configura√ß√µes de build/deployment para um cluster Kubernetes do Azure Pipelines.
Na listagem a seguir temos o conte√∫do do arquivo Dockerfile, em que est√£o referenciadas as imagens Alpine do SDK do .NET Core 3.1 (para restaura√ß√£o de pacotes e build da aplica√ß√£o) e do runtime do ASP.NET Core 3.1 (com o ambiente necess√°rio para a execu√ß√£o da API REST a partir de um container):
Um recurso do Azure Container Registry chamado groffeazuredevops foi criado para este tutorial:
Maiores informa√ß√µes sobre como criar um recurso do Azure Container Registry podem ser encontradas no tutorial mencionado no in√≠cio deste artigo.
Para o exemplo aqui descrito foi necess√°rio ainda gerar um cluster Kubernetes, com isto acontecendo por meio da utiliza√ß√£o do Azure Kubernetes Service (AKS):
A cria√ß√£o de um novo recurso do AKS tamb√©m foi descrita em detalhes no seguinte artigo que produzi para o portal Microsoft Tech:
Orquestra√ß√£o de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
Projetos do Azure DevOps encontram-se agrupados em Organizations. Para o exemplo descrito nesse tutorial ser√° utilizada uma Organization chamada groffe-demos.
Acionar dentro da Organization a op√ß√£o + New project:
Preencher ent√£o os campos Project name e Description, selecionando ainda a op√ß√£o Private , Git em Version control, Basic em Worker item process e finalmente clicando na op√ß√£o Create:
Na pr√≥xima imagem aparecer√° o projeto APIContagem-TutorialKubernetes j√° criado:
Ser√° por meio do Azure Pipelines que definiremos o processo automatizado (pipeline) de build de imagens Docker e deployment em um cluster Kubernetes para o projeto APIContagem-TutorialKubernetes. Na se√ß√£o Summary deste projeto acessar na barra lateral Pipelines > Pipelines, como indicado na imagem a seguir:
Acessar agora a op√ß√£o Create Pipeline:
Em Where is your code? selecionar a op√ß√£o GitHub YAML:
J√° em Select a repository definir o reposit√≥rio do GitHub ao qual estar√° atrelado o pipeline de build:
Ser√° solicitada neste momento a autentica√ß√£o junto ao GitHub. Realizado este procedimento, aparecer√° agora uma tela com informa√ß√µes do Azure Pipelines:
Descer ent√£o com a barra de rolagem at√© o final da p√°gina, certificando-se de que o reposit√≥rio escolhido est√° selecionado em Repository access. Confirmar esta escolha acionando o bot√£o Approve and install:
Em Configure your pipeline selecionar a op√ß√£o Deploy to Azure Kubernetes Service, a fim de iniciar a montagem do pipeline com um m√≠nimo de configura√ß√µes:
Em Deploy to Azure Kubernetes Service selecionar a Azure subscription em que se encontram o cluster do AKS e o Container Registry criados anteriormente:
Ap√≥s a autentica√ß√£o no Microsoft Azure aparecer√° agora o painel Deploy to Azure Kubernetes Service com os campos para a sele√ß√£o do Azure Container Registry e do cluster Kubernetes:
Concluir este procedimento acionando o bot√£o Validate and configure.
A partir da tela Revise your pipeline YAML ser√° poss√≠vel editar as configura√ß√µes do pipeline:
Localizar o ponto no qual se encontra a task para build e push da imagem no Azure Container Registry:
Incluir logo ap√≥s $(tag) o valor latest, a fim de gerar uma imagem Docker tamb√©m com esta tag (uma conven√ß√£o adotada para a mais recente imagem criada em um Container Registry). J√° a express√£o $(tag) conter√° o BuildId do projeto do Azure DevOps (com a gera√ß√£o de uma imagem cuja tag ser√° identificada por este valor):
Ap√≥s esse ajuste o pipeline contar√° com o seguinte c√≥digo YAML:
Concluir a configura√ß√£o deste pipeline acionando a op√ß√£o Save and run:
Em Save and run:
Acionar finalmente o bot√£o Save and run.
Neste momento ter√° in√≠cio um Job para build e deployment da aplica√ß√£o de testes (est√°gios destacados em vermelho):
Clicando sobre os Stages podemos observar o andamento dos mesmos:
Ap√≥s algum tempo o status do Job indicar√° que os stages de Build e Deploy tiveram sucesso(√≠cones em verde):
Observando o reposit√≥rio no GitHub ser√° poss√≠vel notar a presen√ßa do arquivo azure-pipelines.yml e do diret√≥rio manifests, gerados via Azure DevOps e no qual constam as defini√ß√µes de build/deployment:
O diret√≥rio manifests conter√° os arquivos YAML com as defini√ß√µes para a cria√ß√£o dos objetos Deployment e Service, bem como do Pod no qual a aplica√ß√£o ser√° executada. Para escalar a aplica√ß√£o podemos alterar o arquivo deployment.yml indicando o n√∫mero de r√©plicas/inst√¢ncias da mesma:
Acessando o recurso do Azure Container Registry criado na se√ß√£o anterior ser√° poss√≠vel notar a presen√ßa da imagem apicontagem em Repositories:
Clicando sobre esta imagem ser√£o listadas as diferentes tags/vers√µes existentes para a mesma (81 e latest, que basicamente correspondem √† mesma imagem):
Ao acessar via PowerShell as estruturas do namespace tutorialartigo teremos o Deployment, o Pod e o Service criados para a aplica√ß√£o de testes:
Um teste de acesso √† API que est√° no IP p√∫blico 23.96.122.184 trar√° como resultado:
A grava√ß√£o de uma altera√ß√£o na branch master far√° com que o processo de build e deployment seja disparado automaticamente. A fim de simular isto farei uma altera√ß√£o no arquivo ContadorController.cs (a mudan√ßa realizada aqui foi no conte√∫do da propriedade Local):
Acessando o pipeline no Azure DevOps ser√° poss√≠vel constatar que um novo processo de build e deployment est√° em execu√ß√£o:
Na imagem a seguir visualizamos detalhes deste processo:
Conclu√≠da a execu√ß√£o do pipeline a execu√ß√£o do comando kubectl get pods mostrar√° que um novo Pod foi gerado para a aplica√ß√£o:
Um teste via browser trar√° o nome do novo Pod (propriedade machineName), bem como o conte√∫do alterado para a propriedade Local:
Docker - Guia de Refer√™ncia Gratuito
Kubernetes - Guia de Refer√™ncia Gratuito
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
14 
14¬†
14 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/deploy-webassembly-from-github-toazure-storage-static-websites-with-azure-pipelines-a15f05d26fb8?source=search_post---------380,"There are currently no responses for this story.
Be the first to respond.
WebAssembly is a new technology (just released in 2017) that enables a stack-based virtual machine to run byte code (called Wasm) in your browser without plugins. The latest stable version works in all modern browsers, including mobile. The byte code format, standard instruction set, and simple memory model enables Wasm to run at near-native speeds. It also serves as a viable compilation target for multiple languages. The key benefits of Wasm include better performance compared to JavaScript, a smaller footprint for client-side code, and the ability to reuse existing software written in the language of your choice.
Wasm is stored in a static file that can be loaded along with HTML, JavaScript, CSS, images, and other website assets. This makes Azure Storage Static Websites a perfect hosting platform for WebAssembly apps. Open source projects hosted on GitHub can take advantage of free Azure Pipelines instances to build and deploy your apps. This post explores how to set up continuous integration and deployment (CI/CD) for WebAssembly apps.
I wrote a series of articles to document my investigation of WebAssembly beginning with a unique framework called Blazor. Start with the .NET Framework ‚Äî including the Common Language Runtime (CLR) ‚Äî build it on top of WebAssembly, then add functionality to render and manage Single Page Applications using Razor templates and C#, and the result is Blazor (Browser + Razor templates). My first step in learning the framework was to port an existing Angular 2 app. I then created a Blazor presentation with several example applications:
blog.jeremylikness.com
After recording and publishing a video series about Azure DevOps for .NET Developers, I immediately realized that because the demo apps are Single Page Apps deployed as a set of static assets, I can host them on storage and automate the build and deployment.
To begin, open your GitHub project and navigate to the marketplace.
Search for and choose Azure Pipelines.
Follow the prompts to create your Azure DevOps account. After you‚Äôve created or selected a project, you are prompted to apply to all projects or pick specific ones. I recommend one Azure DevOps project per GitHub repository to start with. When prompted, choose public for your project. That doesn‚Äôt mean anyone will be able to see your secrets or deployment credentials; it gives them access to the build status and details. The deployment will be separate.
After you select the GitHub repository, a default azure-pipelines.yml file is generated based on preliminary inspection of your source code. For example, a .NET Core project will automatically generate a .NET Core pipeline. The initial pipeline is just a skeleton, you can choose ‚Äúsave and run‚Äù then throw away the results. The pipeline is checked into source control (without any secrets) so that users who fork your project can easily create their own automated builds. You can also use an existing pipeline as a starter template for new builds you create.
The first build process I created was for my Blazor examples. There are several examples in the same repository with separate solutions, so I created multiple build steps. You can inspect the build pipeline here.
The start of the configuration looks like this:
The build will automatically trigger based on a commit to the master branch. It uses a hosted Linux image (Ubuntu 16.04) and is configured for ‚Äúrelease‚Äù or production.
üìÉ Learn more about hosted agents, including how they are configured and what software is installed, here: Microsoft-hosted agents for Azure Pipelines.
The next section of the Yaml file details the build steps.
Blazor is currently in preview, so I use a .NET Core task to install the correct preview version. Next, each project is built and published into its own directory. Notice the use of the $(Build.ArtifactStagingDirectory) to reference where distributions should reside.
üìÉ You can read the full list of Azure Pipelines predefined variables.
Notice the last step publishes the artifacts. This places them in a compressed archive that you can inspect and download after the build. It also makes the artifacts available to the release pipeline that will deploy your assets. More on that later.
Someone wiser than me once said that to truly learn a technology, you should always go at least one layer beneath the surface. Underneath Blazor is WebAssembly, so I set out to learn as much as I could. There are several ways to build WebAsssembly, and the original tool chain that allows you to compile C and C++ projects to Wasm is called Emscripten.
I wrote about my experiments here:
blog.jeremylikness.com
Installing the tool chain is not straightforward and involves several steps. Fortunately, the entire development environment is configured and hosted in a Docker container. I chose to take advantage of that for my Emscripten build pipeline.
The first task installs the Docker client (the host is already available on the agent). I mount the source code onto the container and use the compiler to build the Wasm file and an associated JavaScript file that loads it. I copy the HTML and custom JavaScript for my app into the staging directory, followed by the loader and Wasm file. Finally, I publish the build artifacts.
Go is a powerful language that supported multiple platforms from the beginning. After WebAssembly was released, the Go team added a new target operating system (JavaScript) and platform (Wasm) for builds. In an exercise to learn more about Go and how it generates WebAssembly, I ported an old ‚Äúdemo scene‚Äù plasma effect I used in the 1980s to run in the browser.
I wrote about it here:
blog.jeremylikness.com
Go also provides a fully configured Docker container. Here is the build pipeline:
Notice there are two steps that use the same Docker container. Emscripten generated its own JavaScript loader for Wasm customized to the project. Go, on the other hand, ships a file named wasm_exec.js with each version that supports WebAssembly. This file is necessary for JavaScript to interact with the Go Wasm app, so I copy it from the container and into the staging artifacts.
Go is available as a build step/environment as well. I could have skipped the Docker image, configured Go on the build agent and built everything directly. However, I already was using a Docker-based build so it was faster for me to use the same steps for my build pipeline. Choices are good.
The Rust ü¶Ä language is ideally suited to WebAssembly development. Unlike Go and C# that both require runtimes to execute (the Go file is over two megabytes in size as a result), Rust generates low-level platform-ready code like C/C++ but with a mature syntax and many built-in features that provide security and thread safety. Rust also embraced WebAssembly very early and built a set of tools to support Wasm development. As a result, Rust makes it much easier to build and package WebAssembly apps while producing streamlined byte code (the Rust Wasm file is only 61 kilobytes or about 3% the size of what Go generates).
I wrote about the experience here:
blog.jeremylikness.com
The Rust build pipeline is significantly different that the previous ones.
The environment is setup to use Node.js. A command-line task installs the Rust toolchain (‚ÄúRustup‚Äù), followed by ‚Äúwasm-pack‚Äù that is used to build Wasm apps.
The need to install Rustup manually will go away soon. As of this writing, a pull request is in review to add Rust as a first-class pipeline task.
One folder contains the Rust project. That project is built using wasm-pack and generates the Wasm assets. Rust not only provides code to help connect structures in Rust to JavaScript, it also wraps the WebAssembly memory with code that makes it easier to pass buffers to and from Wasm. The project is linked to a Node project that contains the host HTML and JavaScript. A final npm build step packages everything together as a distinct set of assets to deploy, and the last task publishes artifacts directly from the distribution folder.
Now artifacts exist for multiple flavors of WebAssembly projects. The next step is to host them using inexpensive Azure Storage.
At this stage if you‚Äôre like me, you‚Äôre probably excited about getting automated builds up and running and want to share it with the world. Azure Pipelines makes this easy for you. Navigate from ‚ÄúAzure Pipelines‚Äù to ‚ÄúBuilds‚Äù and select the three dots in the upper then click on ‚Äústatus badge.‚Äù
The resulting dialog will allow you to specify some parameters and provides both a link to the status badge and markdown you can easily cut and paste into your README.md file.
The next few steps require an Azure subscription. If you don‚Äôt have one already, you can grab a free Azure account. Use the ‚ûï in the upper left to add a new resource and choose Storage account.
Give the storage account a name, pick your resource group, and be sure to select ‚ÄúStorage V2 (general purpose v2)‚Äù as the account kind. Pick a replication option that suits you (higher availability comes at a higher cost). Review, then create the storage account and wait for the notification that is ready and available.
Static websites are not enabled by default. To enable them, choose the ‚Äústatic websites‚Äù option, flip to ‚Äúenabled‚Äù and optionally enter a default document and error document (what will be served when files aren‚Äôt found).
After you click ‚Äúsave‚Äù a special blob storage container named $web is created. This is the ‚Äúroot‚Äù of your static website. You will also be presented with the URL you can use to access the site.
Navigate to ‚ÄúReleases‚Äù under ‚ÄúAzure Pipelines.‚Äù You will be presented with the option to create a new pipeline. Click ‚Äúnew pipeline‚Äù and choose ‚Äúempty job.‚Äù You can name the first stage of your deployment pipeline. I‚Äôm using mine for demo assets, so I only have one stage and name it ‚Äústatic websites.‚Äù
Next, add an artifact to feed into the release pipeline. See the ‚Äúadd‚Äù link.
A new dialog will appear. Select your project and source, and the default artifacts will populate. Click ‚ÄúAdd‚Äù to add this artifact.
To enable continuous deployment (that will be triggered after every successful build) click on the little ‚Äútrigger‚Äù icon in the upper right.
Finally, flip ‚Äúcontinues deployment trigger‚Äù to ‚Äúenabled.‚Äù
Next, click ‚Äúview stage tasks‚Äù to begin building the deployment.
I‚Äôm using a single static website to host multiple projects. The projects live under a folder path and not at the root of the website, so I need to specify the base URL using the <base href=""""> HTML tag. The easiest way to do this is to click the little ‚ûï next to the agent job to add a new task then go the marketplace and install the free RegExReplace Build Task. In the resulting dialog, I specify the path to the main HTML file that hosts the app. The regular expression varies from project to project. In the case of the Rust app, there is no existing tag to replace. Therefore, I search for the end of a style tag I know exists and append the base tag to the end of it. The regular expression and replacement look like this:
Notice that I put my Wasm apps under the wasm folder with each project in its own folder, in this case PlasmaWasmRust. I do this for every project that is deployed. The advantage of applying this transformation during the deployment phase is that the exact same build artifacts can be used for any environment. If my staging truly was ‚Äústaging‚Äù and required another step for production, I can use the same build artifact and apply a different transformation to target the production URL.
To copy the artifacts, use the ‚ÄúAzure File Copy Task.‚Äù
Give the task a name and click the ellipses after ‚Äúsource‚Äù to navigate to your artifact folder. Here I‚Äôve selected zmachine.
Pick ‚ÄúAzure Resource Manager‚Äù for the type and choose your subscription from the drop-down.
Note: if your subscription does not display automatically, you may need to use a service account to connect to Azure. To learn how, read: Connect to Microsoft Azure.
Pick ‚ÄúAzure Blob‚Äù as the destination type, then select the name of your storage account. The container name will be $web and the blob prefix is where you specify the folder path to place the artifacts in (it is the same ‚Äúpath‚Äù as the base URL but without the leading slash). Here is the task for the Rust project:
This step works for copying all of the main files into your static website, but one additional step is needed. The file copy task by default sets MIME types for files based on their extensions. This ensures they load correctly in your web browser. WebAssembly is a newer content type, so the current version of the task uses a default type that prevents it from loading correctly.
To adjust the default behavior, copy the .wasm files a second time and pass an argument that specifies the content type. Start by adding another Azure File Copy task. This time, set the source to a pattern that matches only your Wasm files:
$(System.DefaultWorkingDirectory)/_JeremyLikness.PlasmaWasmRust/plasmawasmrust/*.wasm
Next, add the optional arguments to set the file content type.
This last and final step should be all that is needed to successfully deploy your app. Save it and launch the stage (or commit to master and follow the entire pipeline from build through deploy) and see your app come out the other side!
You have a build badge, so why not a deployment one? To add your deployment badge, open the release pipeline and navigate to ‚Äúoptions‚Äù then ‚Äúintegrations.‚Äù Enable the status badge and you‚Äôll receive the appropriate link.
Azure Pipelines is free for open source projects hosted on GitHub. The goal of Azure Pipelines is to build any language, on any platform, and automate deployment to any destination. Do you have a mobile app? That‚Äôs no problem because Azure Pipelines will build your iOS code on a macOS agent (or Android code using a Java-based SDK) and automatically deploy it for review to the app store. Does your app have a comprehensive unit test suite? No problem! Azure Pipelines will run your tests and publish results, including code coverage, to your project dashboard.
Do you have an open source project that will benefit from automation?
‚ñ∂ Get started building your GitHub repository.
Any language.
125 
3
125¬†claps
125 
3
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ericsk/microsoft-ai-%E5%A4%A7%E8%97%8D%E5%9C%96-1-azure-machine-learning-services-%E7%AC%AC%E4%BA%8C%E4%BB%A3-ae11c2efb08c?source=search_post---------381,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eric ShangKuan
Nov 4, 2017¬∑11 min read
ÈÄôÊôÇ‰∏ÄÁØá‰ªãÁ¥πÁî± Microsoft Êèê‰æõÂêÑÁ®ÆËàáÊ©üÂô®Â≠∏Áøí (Machine Learning) Áõ∏ÈóúÊúçÂãô„ÄÅÂπ≥Âè∞„ÄÅÂ∑•ÂÖ∑ÁöÑÁ≥ªÂàóÊñáÁ´†Ôºå‰ª•‰∏ãÊòØÁ≥ªÂàóÁõÆÈåÑÔºö
Azure Machine Learning Services ‰∏çÊòØÊñ∞ÁöÑÊúçÂãôÔºå2014 Âπ¥Â∞±‰ª• Azure Machine Learning Studio ÁöÑÂΩ¢ÂºèÊé®Âá∫ÔºåÂÆÉÊèê‰æõÊÉ≥Ë¶ÅÂÅöÊ©üÂô®Â≠∏ÁøíÁöÑË≥áÊñôÁßëÂ≠∏ÂÆ∂ÊàñÁ®ãÂºèÈñãÁôº‰∫∫Âì°‰∏ÄÂÄãÂúñÂΩ¢ÂåñÁöÑ‰ªãÈù¢Ë®≠Ë®àÊ©üÂô®Â≠∏ÁøíÁöÑË≥áÊñôËàáÈÅãÁÆóÊµÅÁ®ãÔºå‰∏¶‰∏îÊèê‰æõ‰æùÁÖßÈúÄÊ±ÇÁöÑÈÅãÁÆóË≥áÊ∫êË®ìÁ∑¥Âá∫Â≠∏ÁøíÊ®°ÂûãÔºåÊúÄÂæå‰ª• Web Services ÁöÑÂΩ¢Âºè‰æÜÂèñÁî®Ë®ìÁ∑¥Â•ΩÁöÑÊ®°Âûã„ÄÇ
‰ªäÂπ¥ (2017 Âπ¥) 9 ÊúàÔºåÂæÆËªüÊé®Âá∫Êñ∞‰∏Ä‰ª£ÁöÑ Azure Machine Learning ServicesÔºåÊúâÂà•ÊñºÁ¨¨‰∏Ä‰ª£ÔºåÊñ∞‰∏Ä‰ª£ÁöÑ Azure Machine Learning Services Â∏åÊúõÁµ¶Ê©üÂô®Â≠∏ÁøíÁöÑÈñãÁôº‰∫∫Âì°Êúâ‰∏ÄÂ•óÂ•ΩÁî®ÁöÑÈñãÁôºÂ∑•ÂÖ∑Ôºå‰∏¶‰∏îËÉΩÂ§†Êõ¥ËºïÊòìÂú∞Â∞áÈñãÁôºÂ•ΩÁöÑË®ìÁ∑¥Á®ãÂºèÈÉ®ÁΩ≤Âú®ÂêÑÁ®ÆÁí∞Â¢É‰∏ãÂü∑Ë°åË®ìÁ∑¥ÔºåÂêåÊôÇÂèàËÉΩÁÆ°ÁêÜÂèäÈÉ®ÁΩ≤Ë®ìÁ∑¥Â•ΩÁöÑÊ®°Âûã„ÄÇ
ÂÆÉÈáçÊñ∞Ë®≠Ë®àÁöÑÂπ≥Âè∞ÂèäÊúçÂãôÂ¶Ç‰∏ãÔºö
‰ª•‰∏ãÂàÜÂà•‰ªãÁ¥πÊØè‰∏ÄÈ†ÖÊúçÂãôÁöÑÁ¥∞ÁØÄ„ÄÇ
Azure Machine Learning Workbench Êèê‰æõ‰∏ÄÂÄãÊ°åÈù¢Â∑•ÂÖ∑ÔºàÊîØÊè¥ Windows Âèä Mac OS XÔºâÔºåÂÆÉÂèØ‰ª•Âπ´‰Ω†ÁÆ°ÁêÜ‰∏ÄÂÄãÊ©üÂô®Â≠∏ÁøíÁöÑÂ∞àÊ°àË¶ÅÂú®Êú¨Ê©ü„ÄÅdocker ÊàñÊòØÈÅ†Á´ØÁí∞Â¢É‰∏≠Âü∑Ë°åÔºåÂêåÊôÇÂú®ÈÄôÂÄãÂ∑•ÂÖ∑Ë£°Èù¢‰πüÂåÖÂê´‰∫ÜÔºö
Á∞°ÂñÆÂú∞Ë™™ÔºåÈÄôÂÄãÂ∑•ÂÖ∑Â∞±ÊòØËÆì‰Ω†Âú®ÈñãÁôºÊ©üÂô®‰∏äÂØ´Á®ãÂºèÊôÇÁöÑÂ∞èÂπ´ÊâãÔºå‰∏çÂÉÖÂèØ‰ª•Âú®ÈñãÁôºÈÅéÁ®ã‰∏≠Ê∏¨Ë©¶Ôºå‰πüÂæàÊñπ‰æøÈÉ®ÁΩ≤Âà∞ÂÖ∂ÂÆÉÁí∞Â¢É‰æÜÂü∑Ë°åÈÅãÁÆó„ÄÇ
Ê©üÂô®Â≠∏ÁøíÁöÑÂ∞àÊ°àÈñãÁôºÂÆåÁï¢ÂæåÔºåÊúÄÈáçË¶ÅÁöÑÂ∞±ÊòØË∑ëË®ìÁ∑¥ÂèäÊ∏¨Ë©¶ÔºåAzure Machine Learning Experiment Service ÂÆöÁæ©‰∫Ü‰∏ÄÂ•óÊ©üÂô®Â≠∏ÁøíÂ∞àÊ°àÁöÑÂü∑Ë°åÁí∞Â¢ÉÔºåÂè™Ë¶ÅÊ©üÂô®Â≠∏ÁøíÂ∞àÊ°àÊòØÊåâÁÖßÈÄôÂÄãÂü∑Ë°åÁí∞Â¢ÉÈñãÁôºÔºåÈÄôÊ®£Â∞±ËÉΩÂú®ÊîØÊè¥ÈÄôÊ®£Âü∑Ë°åÁí∞Â¢ÉÁöÑÔºö
‰∏≠‰æÜÂü∑Ë°åÂ∞àÊ°à„ÄÇËÄå‰∏îÂÆÉ‰πüÊúâÂæàÂ•ΩÁöÑÁç®Á´ãÊÄßÔºåËÆì‰Ω†ÁöÑË®ìÁ∑¥ÈÅãÁÆó‰∏çÊúÉÂèóÂà∞Â§™Â§öÁí∞Â¢ÉÁöÑÂπ≤Êìæ„ÄÇ
Áï∂Ê©üÂô®Â≠∏ÁøíÂ∞àÊ°àË®ìÁ∑¥ÂÆåÁï¢ÂæåÔºåÁî¢ÁîüÁöÑÊ®°ÂûãÂ∞±ÂèØ‰ª•ÁÇ∫ÂæåÁ∫åÊâÄÂ•óÁî®ÔºåÈÄèÈÅé Azure Machine Learning Model Management Service Â∞±ÂèØ‰ª•ÊääÈÄô‰∫õË®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÈÉ®ÁΩ≤Âú®‰∏çÂêåÁöÑÁí∞Â¢É‰∏≠ÔºåÂÉèÊòØÔºö
Êúâ‰∫ÜÈÄôÊ®£ÁöÑÊ©üÂà∂Ôºå‰Ω†Â∞±ÂèØ‰ª•Âú®Èõ≤Á´ØÊàñÊòØÂº∑Â§ßÁöÑ‰º∫ÊúçÂô®‰∏äË∑ëÊ©üÂô®Â≠∏ÁøíÁöÑË®ìÁ∑¥ÁÆóÔºåËÄåË®ìÁ∑¥ÂÆåÁöÑÊ®°ÂûãÂâáÂèØ‰ª•ÈÉ®ÁΩ≤Âà∞ IoT Ë£ùÁΩÆ‰∏äÁõ¥Êé•Â•óÁî®„ÄÇ
ÈÄôÈÉ®‰ªΩÊòØÁî® docker container ÁöÑÂΩ¢Âºè‰æÜÂëàÁèæÔºåÊâÄ‰ª•Âè™Ë¶ÅËÉΩË∑ë container ÁöÑÁí∞Â¢ÉÂπæ‰πéÈÉΩËÉΩË∑ëÔºåÈúÄË¶Å GPU ÁöÑÂ∞±Ë¶ÅË£ùÂ•ΩÁõ∏ÈóúÁöÑÈ©ÖÂãïÁ®ãÂºèÂèäÂ∑•ÂÖ∑ÔºàÂ¶ÇÔºönvidia-dockerÔºâ
MMLSpark ÊòØ‰∏ÄÂ•óÈñãÊ∫êÁöÑ Apache Spark ÂáΩÂºèÂ∫´ÔºåÂÆÉËÆìÈñãÁôº‰∫∫Âì°Âú®‰ΩøÁî® Spark ÂÅöÊ©üÂô®Â≠∏ÁøíÂ∞àÊ°àÊôÇÊõ¥ÂÆπÊòìËàá Micorosft Cognitive Toolkit (CNTK) Ëàá OpenCV Êï¥ÂêàÔºåÊõ¥Â§öÁ¥∞ÁØÄÂèØ‰ª•ÂèÉËÄÉÈÄôÈ†ÅË™™Êòé„ÄÇ
ÈÄôÊòØ‰∏ÄÂÄã Visual Studio Code ÁöÑÂ§ñÊéõÁ®ãÂºèÔºåÂÆÉËÆì Visual Studio Code ÂèØ‰ª•Ëàá Azure Machine Learning Services ÁµêÂêàÔºåÈô§‰∫ÜÈñãÁôºÁ®ãÂºè‰πãÂ§ñÔºå‰πüÂèØ‰ª•Áõ¥Êé•Âú® Visual Studio Code ÁÆ°ÁêÜË®ìÁ∑¥ÁöÑÈÅãÁÆóÂ∑•‰Ωú‰ª•ÂèäÈÉ®ÁΩ≤Ê®°Âûã„ÄÇ
‰ª•‰∏ã‰ª•ÂÖ©ÂÄã‰æãÂ≠ê‰æÜÁû≠Ëß£‰ΩøÁî® Azure Machine Learning ÁöÑ end to end È´îÈ©ó„ÄÇÊìç‰Ωú‰πãÂâçÔºåÂª∫Ë≠∞ÂÖàÈñ±ËÆÄÈÄôÈ†ÅË™™ÊòéÂ∞á Azure Machine Learning Services ‰∏≠ÁöÑ Workspace Âª∫Á´ãÂ•Ω‰∏îÂÆâË£ù Azure Machine Learning Workbench„ÄÇ
Áï∂ÁÑ∂‰πüÂª∫Ë≠∞ÂÆâË£ù Visual Studio Code ‰∏¶‰∏îÂÆâË£ù Visual Studio Code Tools for AI ÁöÑÂ§ñÊéõÂ•ó‰ª∂„ÄÇ
MNIST ÊâãÂØ´Êï∏Â≠óËæ®Ë≠òÊòØÂæàÁ∂ìÂÖ∏ÁöÑÊ∑±Â∫¶Â≠∏ÁøíÂ∞àÊ°àÔºåÊàëÂÄëÂèØ‰ª•Âæû Azure Machine Learning Services ‰∏≠ÁöÑÂ∞àÊ°àÁØÑÊú¨Áõ¥Êé•Âª∫Á´ã‰∏ÄÂÄã‰ΩøÁî® CNTK ‰æÜÈñãÁôºÁöÑ MNIST ÊâãÂØ´Êï∏Â≠óËæ®Ë≠òÁöÑÂ≠∏ÁøíÂ∞àÊ°à„ÄÇ
Âª∫Á´ãÂ∞àÊ°àÊúâÂÖ©ÂÄãÊñπÂºèÔºö
‰∏çË´ñÁî®Âì™Á®ÆÊñπÂºèÂª∫Á´ãÂ∞àÊ°àÈÉΩÊòØÁõ∏ÂêåÁöÑÔºåÂú® Azure Machine Learning Workbench ‰∏≠ÂèØ‰ª•Ë®≠ÂÆöÂ∞àÊ°àÁî® VSCode ÈñãÂïüÔºõVSCode Âú®Âª∫Â∞àÊ°àÊôÇ‰πüÊúÉË¶Å‰Ω†Â°´ÂÖ•Â∞çÊáâÁöÑ Azure Machine Learning Services ÁöÑ Workspace„ÄÇ
ÂÆåÊàêÂ∞àÊ°àÂª∫Á´ãÂæåÔºå‰Ω†ÊúÉÂæóÂà∞‰∏ÄÂÄãÂ∑≤Á∂ìÁî® CNTK ÂØ´ÊàêÁöÑÊ∑±Â∫¶Â≠∏ÁøíÂ∞àÊ°à (‰πüÂ∞±ÊòØ‰∏çÁî®ÂØ´Á®ãÂºèÂ∞±ËÉΩÂãï‰∫Ü)ÔºåÈÄôÂ∞±ÂèØ‰ª•‰æÜÈ´îÈ©ó‰∏Ä‰∏ãÊÄéÈ∫ºÊääÈÄôÂÄãÂ∞àÊ°àÊîæÂú® Azure Machine Learning Experiment Service ‰∏≠Âü∑Ë°å„ÄÇ
Âú® Azure Machine Learning Workbench ÈñãÂïüÈÄôÂÄãÂ∞àÊ°àÔºå‰Ω†ÊúÉÁúãÂà∞Â∞àÊ°àÈ¶ñÈ†ÅÂ∞±ÂèØ‰ª•Áõ¥Êé•Âü∑Ë°åÂ∞àÊ°àÔºåËÄå‰∏îÂèØ‰ª•ÈÅ∏Êìá local Êàñ docker ÔºàÁ≥ªÁµ±ÂøÖÈ†àÂÖàÂÆâË£ùÂ•Ω docker ÁöÑÁí∞Â¢ÉÔºâ‰æÜÂü∑Ë°å„ÄÇ
‰∏çÈÅé‰∏çË¶ÅÈ¶¨‰∏äÊÄ•ËëóÂü∑Ë°åÔºåÂõ†ÁÇ∫‰Ω†ÂæàÊúâÂèØËÉΩÈÇÑÊ≤íÊúâÂú® Workbench ÁöÑÁí∞Â¢É‰∏≠ÂÆâË£ù CNTK ÁöÑÂáΩÂºèÂ∫´ÔºåÊâÄ‰ª•Ë´ãÁÖßËëóÈ†ÅÈù¢‰∏ãÊñπË™™ÊòéÂÆâË£ùÂ•Ω CNTK ÁöÑÂ•ó‰ª∂ÂÜç‰æÜÂü∑Ë°å„ÄÇËã•ÊòØÈÅ∏ÊìáÂú® docker ‰∏≠Âü∑Ë°åÂâáÁÑ°Ê≠§ÂïèÈ°åÔºåÂõ†ÁÇ∫ÂÆÉÊúÉÊääË©≤ÂÆâË£ùÁöÑÁí∞Â¢ÉÂú® container ‰∏≠Ë®≠ÂÆöÂ•Ω„ÄÇ
ÊàëÊé®Ëñ¶Áõ°ÈáèÁî® docker Áí∞Â¢ÉË∑ëÔºåÂõ†ÁÇ∫ÂØ¶Âãô‰∏äÈñãÁôºÂÆåÊàêÊáâË©≤ÈÉΩÊúÉ‰∏üÂà∞ÈÅ†Á´ØÁöÑÊ©üÂô®‰∏äÂü∑Ë°åÔºåÈÄô‰πüÊúÉÊòØÁî® docker ‰æÜÂü∑Ë°å„ÄÇ
ÈÅãË°åÂæåÂ∞±ÂèØ‰ª•ËßÄÁúãÁµêÊûúÔºö
Ëã•ÊòØÂú® Visual Studio Code Êìç‰ΩúÔºåÂú® Workbench ÈñãÂïüÁöÑÁãÄÊÖã‰∏ãÔºå‰Ω†‰πüÂèØ‰ª•Áõ¥Êé•Âú® VSCode ‰∏≠Êèê‰∫§ÈÅãÁÆóÁöÑÂ∑•‰ΩúÔºö
Â¶ÇÊûúÂú®Êú¨Ê©ü‰∏äÂü∑Ë°åÊ≤íÊúâÂïèÈ°åÔºåÊâìÁÆóË™øÊï¥ÂèÉÊï∏ÊääÂÆÉÈÉ®ÁΩ≤Âà∞Èõ≤Á´ØÂü∑Ë°åÔºàÈÄèÈÅé docker container ÁöÑÂΩ¢ÂºèÔºâ‰πüÂæàÂÆπÊòìÔºåÂè™Ë¶ÅÊääË¶ÅÈÅãË°åÁöÑÁí∞Â¢ÉÔºà‰æãÂ¶ÇÔºöÂú® Azure ‰∏äÂê´Êúâ GPU Ë¶èÊ†ºÁöÑ N Á≥ªÂàóËôõÊì¨Ê©üÂô®ÔºâÔºåÈÇ£Â∞±ÊääÊ©üÂô®ÁöÑÁµÑÊÖãË®≠ÂÆö‰∏Ä‰∏ãÔºåËÆìÂÆÉËÆäÊàê Azure Machine Learning Experiment Service Â∞±ÂèØ‰ª•Êï¥ÂêàÂú® Workbench ÁöÑÊìç‰Ωú‰∏≠„ÄÇ
Â¶ÇÊûúË¶ÅË∑ëÂú® GPU ÁöÑÁí∞Â¢ÉÔºåÂà•Âøò‰∫Ü‰øÆÊîπ aml_config ÁõÆÈåÑ‰∏ãÁöÑÁµÑÊÖãÊ™îÔºå‰ΩøÁî® GPU ÁâàÁöÑ CNTK Âèä MMLSparkÔºåÈÇÑÊúâÁõÆÊ®ôÊ©üÂô®‰∏ä‰πüË¶ÅÂÖàË£ùÂ•Ω docker Ë∑ü nvidia-docker ÁöÑÁí∞Â¢É„ÄÇ
Âè¶Â§ñÔºåWorkbench ÊúÉÂú®ÁõÆÊ®ôÊ©üÂô®‰∏ä‰ΩøÁî® sudo ‰æÜÂü∑Ë°å docker/nvidia-dockerÔºå‰ΩÜÊòØÈúÄË¶ÅË¢´Ë®≠ÂÆöÁÑ°ÂØÜÁ¢º‰øùË≠∑ÔºåÊâÄ‰ª•‰πüË¶ÅÂú®ÁõÆÊ®ôÊ©üÂô®‰∏ä‰øÆÊîπ /etc/sudoers ÊääÁôªÂÖ•ÁöÑÂ∏≥ËôüÂä†ÂÖ•‰∏çÈúÄË¶ÅÂØÜÁ¢º„ÄÇ
ÊåâÁÖßË™™ÊòéÔºàÂú® Workbench ÁöÑÂ∞àÊ°àÈ¶ñÈ†ÅÂç≥ÂèØÁúãÂà∞ÔºâË®≠ÂÆö compute target ‰πãÂæåÔºå‰Ω†Â∞±ÂèØ‰ª•Âú® Workbench Êàñ VSCode ‰πã‰∏≠ÁúãÂà∞Êñ∞ÁöÑÁõÆÊ®ôÁí∞Â¢ÉÂèØ‰ª•Âü∑Ë°åÈÅãÁÆóÔºö
Áï∂Â∑•‰ΩúÂÆåÁï¢ÊôÇÔºå‰Ω†‰πüÂèØ‰ª•Âæû job detail view ‰∏≠ÊääË®ìÁ∑¥Â•ΩÁöÑÊ®°Âûã‰øùÂ≠ò‰∏ã‰æÜÔºàÊîæÂú® artifacts ÁõÆÈåÑ‰∏ãÔºâÔºö
Êúâ‰∫ÜÊñ∞‰∏Ä‰ª£ÁöÑ Azure Machine Learning ServicesÔºåÊúÄ‰∏ªË¶ÅÂú®ÈñãÁôº„ÄÅÂü∑Ë°åË®ìÁ∑¥Ê∏¨Ë©¶ÁöÑÁí∞Â¢ÉÊ∫ñÂÇô‰∏äÂπ´‰∫ÜÂæàÂ§ßÁöÑÂøôÔºå‰Ω†ÂèØ‰ª•Âú®‰∏ÄËà¨ÁöÑÊ©üÂô®‰∏äÈÄ≤Ë°åÊ©üÂô®Â≠∏ÁøíÂ∞àÊ°àÁöÑÈñãÁôºÊ∏¨Ë©¶ÔºåËÄåÁúüÁöÑÈúÄË¶ÅË∑ëÂ§ßÈáèË≥áÊñôÁöÑË®ìÁ∑¥‰ª•ÂèäÊ∏¨Ë©¶ÊôÇÔºåÂÜçÊääÈÄôÂÄãÂ∞àÊ°àÈÉ®ÁΩ≤Âà∞ÂêàÈÅ©ÁöÑÊ©üÂô®Ôºà‰æãÂ¶ÇÊúâÂæàÂº∑ÂæàÂ§öÈ°Ü GPU ÁöÑÁí∞Â¢ÉÔºâ‰∏äÈÄ≤Ë°åÈÅãÁÆó„ÄÇÊòØÂÄãÂ¢ûÂä†ÁîüÁî¢ÂäõÁöÑÂπ≥Âè∞„ÄÇ
‰∏çÂ¶ÇÁèæÂú®Â∞±Áé©Áé©ÂÆÉÁöÑ 5 ÂàÜÈêòÂÖ•ÈñÄÊâãÂÜäÈ´îÈ©óÁúãÁúãÂêßÔºÅ
DevRel | Developholic | Technical Evangelist
See all (367)
17 
17¬†claps
17 
DevRel | Developholic | Technical Evangelist
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/top-10-cloud-certification-to-aim-in-2022-aws-azure-and-google-cloud-platform-bd054fff0538?source=search_post---------382,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are aiming for cloud certifications in 2022 but are not sure which cloud certification should you go for then you have come to the right place. Earlier, I have shared a list of the best IT certifications for Java developers, and today, I am going to talk about the best cloud certification to aim for in 2022.
You can go through this list of cloud certifications and choose the best one depending upon your skills and experience. I have shared the best cloud certifications for beginners, developers, system admins, and solution architects from Amazon AWS, Microsoft Azure, and Google Cloud Platform.
Cloud computing services is growing exponentially in nowadays technology and become a priority among big-name organization such as Netflix which uses Amazon AWS to run their business from hosting to database and analytics.
For this reason, many cloud computing has come to the real world with different infrastructure and services such as Google Cloud and Microsoft Azure, and more.
All of those cloud services are complicated in their infrastructure and it requires the person who wants to deal with them to get some sort of certificate to deal with a specific service and companies nowadays are requiring people to have these certifications in order to validate their skills.
This article will discuss with you some of the best cloud certifications from companies like Microsoft Azure, Amazon AWS and Google cloud and having those certifications will make your resume stand out among the other competitor during the hiring process and you can get a higher salary than the others without those certifications.
Without wasting any more of your time, here is a list of the best Cloud certification of 2022. You can aim for this certification to boost your career and also start your career in Cloud Computing as Cloud Professional, Developer, and Solution architect.
This AWS certificate is designed for people who can do solution architecture such as deploying the web applications and securing them and also it targets individuals who have one year working with AWS services.
In other words, this is the best cloud certification for experienced developers who wants to become software architects or solution architects. If you have been working on the AWS cloud platform, personally or for our company then you should aim to pass this exam to get certified for your skills.
The exam is very vast and you need to know a lot of things about AWS services. If you are aiming for this prestigious certification then this AWS Certified Solutions Architect course on udemy can assist you to acquire those skills.
This is the best Cloud certification for beginners or anyone who wants to start with Cloud computing and the AWS cloud platform.
If you want to jump to Associate-level or specialty certification then make sure to get this certification that gives you an overall understanding of AWS cloud services such as security and account management and more.
This is a relatively easier exam and you can easily pass this with a couple of weeks of preparation.
If you need a recommendation, I highly recommend you check out Stephane Maarek‚Äôs AWS Certified Cloud Practitioner [NEW] course on Udemy. Stephane is an AWS Guru and this will help you learn and prepare for the exam.
This is the best Cloud certification for programmers and software developer who wants to create cloud-native applications. If you have more than one year of managing AWS services then this associate certification is right for you and it will teach you how to use the AWS core services as well as its architecture, develop, and deploy the application on AWS.
This is also one of the toughest AWS certifications, compared to the previous two certifications like cloud practitioner and solution architect. It‚Äôs not enough to just be familiar with different AWS services, you need to know them in-depth so that you can use correct configurations in a given scenario.
If you are a developer and software engineer then I highly recommend this cloud certification to you as it will significantly boost your profile and make you eligible for many more opportunities.
As I said, the exam is tough to crack and you need multiple resources to prepare well but to start with I highly recommend you to go through this course AWS Certified Developer from CloudGuru, which will teach you the skills needed to pass the exam.
This associate certification is for people having at least one year in deployment, management, and operations on AWS and teaches you to choose the appropriate service for your needs as well as control the data flow from AWS and more.
In other words, this is the best Cloud certification for system admins and IT professionals who work on the Infra side.
If you are working in IT support or working as a System admin, you can aim for this certification to further boost your career.
If you need a course to prepare for this certification, I recommend you to this course named AWS Certified SysOps Administrator ‚Äî Associate is a good resource to learn those skills.
If you notice, all top four cloud certification is from Amazon AWS, and it's because AWS is the most popular public cloud platform for both startups and big companies but Microsoft Azure is catching up quickly which has, in turn, boost the demand for certified Azure Cloud professionals.
The Azure fundamentals certification is better for individuals to know some foundation of the cloud services and this certificate will teach you the cloud concepts as well as how to use Microsoft Azure services, security, privacy, and pricing.
In short, the best cloud certification for beginners who wants to learn Azure. This is very similar to AWS cloud practitioner and you can pass this certification with a couple of weeks‚Äô preparation.
If you need recommendations then this course on udemy AZ-900 Azure Exam Prep will explain all of this in one course and prepare you for the exam.
This is the best cloud certification for experienced programmers, developers, and DevOps engineers who want to become Azure experts.
When you pass this certification exam you will have the skills to design and implement solutions in Microsoft Azure and that includes security, network computing, and storage.
If your companies are migrating into Microsoft Azure cloud then aiming for this Azure certification and boost your profile and also help you to get promoted.
When it comes to preparation, this is a vast certification and you need to cover a lot of topics but thankfully there are many courses to learn these skills but this course on udemy but the AZ-300/AZ-303 Azure Architecture Technologies is the best of them all and it will help you a lot on this journey.
This is another Azure certificate that made the list of top 10 Cloud certifications. This is the best Azure certification for system admins and people who are working in IT support.
This certificate will get you the experiences to implement, manage, and monitor cloud services such as storage, security, and virtual environment, and many more responsibilities.
For preparation, you need to know all the essential Azure services and how to use them, configure them, and troubleshoot in case of any issue.
If you need an online course, I recommend you to check out this Udemy course AZ-104 Microsoft Azure Administrator is a good resource to learn all of those skills.
A list of best cloud certifications cannot be completed without Google cloud certification, another big player in the public cloud market. Google cloud has some of the best capabilities when it comes to dealing with Big Data and Machine learning and that‚Äôs why many startups who are working in those fields are opting for Google cloud.
This is the best Google cloud certification for programmers, developers, and software engineers.
The holder of this certificate will have the responsibility of deploying web applications in the cloud as well as monitoring the operations and managing the whole enterprise solutions and configuring access and security.
Regarding preparing for this exam, this Ultimate Google Certified Associate Cloud Engineer course can help you learn and pass the exam.
This is another popular and in-demand Google cloud certification. This is similar to AWS solution architect and Azure Technology expert but on Google cloud.
This is the best Google Cloud certification for experienced IT professionals who wants to become solution architect on Google Cloud technologies like Big Table, Big Query, and other GCP platforms services.
The holder of a professional cloud architect certificate plays an important role inside the organization since he could design develop, deploy, and manage your web application as well as secure them and more responsibilities.
Regarding preparation for this prestigious cloud certification, this course on udemy Ultimate Google Certified Professional Cloud Architect is a good resource for this certificate.
This is the ultimate cloud certification you can aim for in 2022. It‚Äôs also regarded as the toughest AWS cloud certification and requires extensive experience and knowledge of the AWS cloud platform.
There is a huge demand for this certification as there is always a shortage of AWS experts and I highly recommend this to expert cloud professionals.
This advanced certification teaches you how to design and deploy scalable web applications on Amazon AWS servers as well as select the appropriate service and power to use for your application and more skills you will have.
Regarding preparation, you may need to consult a lot of resources and AWS papers, documentation, and courses but to start with this course on udemy Ultimate AWS Certified Solutions Architect will help you in this journey.
That‚Äôs all about the best cloud Certification you can acquire in 2022. Those certifications are almost the most useful in the field of cloud computing and are issued by the cloud provider itself such as Amazon, Google, and Microsoft, and receiving one of these certifications will open the door to a successful career in this growing industry.
Other Certification Resources for IT Professionals and Programmers
Thanks for reading this article so far. If you like these best Cloud Certifications then please share them with your friends and colleagues. If these courses have helped you to pass the exam, then please spread the word so that other people can also benefit.
P. S. ‚Äî If you are a complete beginner to Cloud Computing and looking for some free courses to learn Cloud Computing in general then you can also check out this Introduction to Cloud Computing (FREE Course) on Udemy. More than 210,000 people have joined this course and it‚Äôs completely free, you just need a Udemy account to join this course.
Medium‚Äôs largest Java publication, followed by 14630+ programmers. Follow to join our community.
194 
194¬†claps
194 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://levelup.gitconnected.com/build-a-custom-url-shortener-using-azure-functions-and-cosmos-db-c20e59261375?source=search_post---------383,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This article describes how to build a custom URL shortener service using Azure‚Äôs serverless platform with Azure Functions and Cosmos DB. I had this idea after I recently read Jussi Roine‚Äôs article, where he built a URL shortener service (such as bit.ly) using a serverless Azure approach, an approach he led with Azure Logic Apps and a‚Ä¶
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/t-t-software-solution/3-virtual-academy-for-azure-fundamentals-%E0%B9%82%E0%B8%94%E0%B8%A2-aipen-studio-aa898b2654dd?source=search_post---------384,"There are currently no responses for this story.
Be the first to respond.
‡πÉ‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏û‡∏≤‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏´‡∏•‡∏±‡∏Å‡πÜ‡∏ö‡∏ô Azure ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏ô‡πÄ‡∏Ñ‡∏¢‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏ô ‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏≠‡∏∞‡πÑ‡∏£ ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏£‡∏≤‡∏ö‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£
Azure Compute ‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á CPUs ‡∏ö‡∏ô VM, ‡∏£‡∏±‡∏ô Code ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Infrastructure ‡∏î‡πâ‡∏ß‡∏¢ Serverless Computing
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà Virtual Machines, Containers, Azure App Service, Serverless Computing
‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡∏ö IaaS ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö Azure ‡∏°‡∏≤‡∏ô‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á Virtual Machine ‡πÇ‡∏î‡∏¢‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô Hyper-V
‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á OS ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ó‡∏±‡πâ‡∏á Windows ‡πÅ‡∏•‡∏∞ Linux
‡∏ã‡∏∂‡πà‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏Ñ‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏•‡∏∞ License ‡∏Ç‡∏≠‡∏á Software ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ú‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å VM ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á MSSQL Standard Edition ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡πà‡∏≤ License ‡∏Ç‡∏≠‡∏á MSSQL Standard ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ú‡∏°‡∏°‡∏µ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏ß‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ VMs ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ó‡πà‡∏≤‡∏ô‡πÉ‡∏î‡∏™‡∏ô‡πÉ‡∏à‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
medium.com
medium.com
medium.com
medium.com
Availability Sets‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ High Availability ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô Data Center ‡πÄ‡∏ä‡πà‡∏ô Hardware ‡∏û‡∏±‡∏á ‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á VMs ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ï‡∏π‡πâ Rack ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Data Center ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ß‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö‡∏ñ‡πâ‡∏≤ VMs ‡∏ï‡∏±‡∏ß‡∏ô‡∏∂‡∏á‡∏û‡∏±‡∏á ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ SLA 99.95% ‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Harddisk ‡πÄ‡∏õ‡πá‡∏ô SSD ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ñ‡πâ‡∏≤ Harddisk ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ SLA)
Virtual Machine Scale Sets‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô VMs ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á VMs ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á VMs ‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡πÅ‡∏Ñ‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Disk ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£ Sync ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Disk ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡πà‡∏∞ VMs ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤
‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏Ñ‡∏∑‡∏≠ ‡∏ì ‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà 2 ‡∏ô‡∏±‡πâ‡∏ô ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡πá‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏á‡∏±‡πâ‡∏ô‡∏à‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ScaleOut Database ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ScaleOut Web App ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏£‡∏±‡∏ö (‡πÅ‡∏ö‡∏ö Stateless)
‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡∏ö PaaS ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥ Code ‡∏´‡∏£‡∏∑‡∏≠ Container ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏õ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô Web App, RESTful API, Background Jobs ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏á‡∏ß‡∏•‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Infrastructure ‡πÄ‡∏•‡∏¢ ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á High Availability ‡πÅ‡∏•‡∏∞ Auto ScaleOut ‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ CI/CD ‡∏Å‡πá‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏°‡∏≤‡∏Å‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
‡∏£‡∏ß‡∏°‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡∏¢‡∏±‡∏á‡∏°‡∏µ Runtime ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô Python, Node.js, PHP, ASP.Net/ASP.Net Core, Java ‡πÄ‡∏£‡∏≤‡πÅ‡∏Ñ‡πà‡∏ô‡∏≥ Code ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà App Service ‡∏Å‡πá‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ú‡∏°‡∏°‡∏µ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏ß‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ App Service ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏ß‡πâ ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ó‡πà‡∏≤‡∏ô‡πÉ‡∏î‡∏™‡∏ô‡πÉ‡∏à‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
medium.com
medium.com
medium.com
Azure App Service Plan‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Web Server ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡∏°‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• App Service ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Plan ‡∏ö‡∏ô OS ‡πÑ‡∏´‡∏ô (Windows, Linux) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∞‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á Web Server ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡πà‡∏ô Development/Test ‡πÄ‡∏£‡∏≤‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Plan ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏°‡πà‡πÅ‡∏£‡∏á‡∏°‡∏≤‡∏Å ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡πá‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Production ‡∏ï‡πà‡∏≠‡πÑ‡∏õ
‡∏ï‡∏±‡∏ß Container ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Technology ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Environment ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö
‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° Code, Runtime, System Tools/Libraries/Settings ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô (‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Container Image) ‡πÅ‡∏•‡∏∞‡∏ô‡∏≥ Container Image ‡πÑ‡∏õ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ö‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Container Technology ‡∏Ñ‡∏£‡∏±‡∏ö
‡πÄ‡∏°‡∏∑‡πà‡∏≠ Container Image ‡∏ñ‡∏π‡∏Å‡∏ô‡∏≥‡∏°‡∏≤‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô Container ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡πÇ‡∏î‡∏¢ Docker ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ
Azure Container Instance (ACI)‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥ Container Image ‡∏°‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤‡∏Å‡∏ô‡∏±‡∏Å
‡πÇ‡∏î‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏à‡∏∞‡∏Ñ‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏£‡∏≤‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏ß‡πâ 30 ‡∏ß‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ñ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Web ‡πÄ‡∏£‡∏≤‡πÅ‡∏Ñ‡πà 1 ‡∏ß‡∏±‡∏ô ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÅ‡∏Ñ‡πà 1 ‡∏ß‡∏±‡∏ô‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
Azure App Service for Containers‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö Azure Container Instance (ACI) ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏ï‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏°‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Web App Service ‡πÄ‡∏ä‡πà‡∏ô‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å Scaleup ‡∏Å‡πá‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õ‡πÄ‡∏û‡∏¥‡πà‡∏° Size ‡∏Ç‡∏≠‡∏á App Service Plan
‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏Å‡πá‡∏à‡∏∞‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Azure Container Instance (ACI) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤ ‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Azure App Service Plan ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏à‡πà‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ï‡∏≤‡∏° Plan ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ñ‡∏∂‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Web ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏∞‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡∏à‡πà‡∏≤‡∏¢‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
Azure Kubernetes Service (AKS)‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£ ScaleOut Container Images ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÜ Instances ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∞‡∏î‡∏ß‡∏Å (‡πÄ‡∏£‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Technology ‡∏ô‡∏µ‡πâ‡∏ß‡πà‡∏≤ Container Orchestration) ‡∏î‡πâ‡∏ß‡∏¢ Kubernetes (K8s) ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô Azure ‡∏Ñ‡∏£‡∏±‡∏ö
‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ Code ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡∏°‡∏≤‡∏£‡∏±‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Infrastructure ‡∏ô‡∏±‡πâ‡∏ô Azure ‡∏à‡∏∞‡∏î‡∏π‡πÅ‡∏•‡πÉ‡∏´‡πâ
‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å App Service ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ô‡∏±‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ó‡∏≤‡∏á Azure ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á Scaling
‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡∏à‡∏∞‡∏Ñ‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å App Service ‡∏ó‡∏µ‡πà‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡∏°‡∏≤‡∏à‡πà‡∏≤‡∏¢‡∏ï‡∏≤‡∏° Package ‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô (App Service Plan) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏á‡∏±‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏Å‡πá‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏•‡∏á‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö
Azure Function‡πÄ‡∏õ‡πá‡∏ô Serverless Computing ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Code-Based ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡∏†‡∏≤‡∏©‡∏≤‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏∞‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö Event ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ß‡πâ
Azure Logic App‡πÄ‡∏õ‡πá‡∏ô Serverless Computing ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Web-Based Designer ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Worflow ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Code ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á Email ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ Twitter ‡∏à‡∏≤‡∏Å Microsoft
‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô
Structured Data ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á (Schema) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏∞‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Fields, Properties ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏ö‡∏ö Relational Database ‡πÄ‡∏ä‡πà‡∏ô Microsoft SQL Server, MySql, PostgreSQL
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡∏Å‡πá‡πÅ‡∏•‡∏Å‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
Semi-structured Data‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô (Hierarchy) ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏ö‡∏ö Table, Row ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Structured Data
‡πÉ‡∏ä‡πâ tags ‡∏´‡∏£‡∏∑‡∏≠ keys ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô NoSQL ‡πÄ‡∏ä‡πà‡∏ô Redis, Cassandra, MongoDB, Elasticsearch
‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ñ‡∏∑‡∏≠‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏ó‡∏≥ Cluster ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏ï‡πà‡∏Å‡πá‡πÅ‡∏•‡∏Å‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
Unstructured Data‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô Text/PDF/Document/Image/Video File
‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡∏ö PaaS ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Structured Data ‡∏î‡πâ‡∏ß‡∏¢ Microsoft SQL Server Database (‡∏Ç‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏¢‡πà‡∏≠‡πÜ‡∏ß‡πà‡∏≤ MSSQL ‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö)
‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏î‡∏π‡πÅ‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡∏≠‡∏á Database Server, ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Failover Cluster, ‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Backup & Recovery)
‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏û‡∏°‡∏≤‡∏Å‡πÜ‡∏Ñ‡∏∑‡∏≠ Azure ‡∏°‡∏µ AI ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° Index ‡πÉ‡∏´‡πâ Field ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô
‡∏ñ‡πâ‡∏≤‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ MSSQL ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ô‡∏≥‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ö‡∏ô Azure SQL Database ‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏ï‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏≠‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Join ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏° Database, ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Link Server ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÅ‡∏ó‡∏ô‡πÄ‡∏ä‡πà‡∏ô MSSQL on VM, SQL Managed Instance
‡∏ú‡∏°‡∏°‡∏µ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô SQL Database ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
medium.com
medium.com
medium.com
‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Semi-structured Data ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô Distributed Database ‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÜ Database ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ High Availability ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Real-Time ‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Big Data ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û, ‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏µ‡∏î‡∏µ‡πÇ‡∏≠, SMB file share ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡∏Ñ‡∏∑‡∏≠ BLOB, File Share, Table, Queue
Azure Blob Storage‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö Unstructured Data ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô Text / PDF / Document / Image / Video File
‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ó‡∏û‡∏Ñ‡∏∑‡∏≠‡∏ó‡∏≥ Video Streaming ‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Video-On-Demand Solution
Azure File Storage‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡∏ö PaaS ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Share File ‡∏ö‡∏ô Protocal Server Message Block (SMB) ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á file ‡πÑ‡∏î‡πâ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Mount Share Drive ‡∏ö‡∏ô Winidows, Linux, MacOS ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Encryption ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö At Rest (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Storage) ‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ö In Transit (‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏•‡∏π) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö
Azure Table Storage‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Semi-Structured ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á NoSQL ‡πÅ‡∏ö‡∏ö Key-Value (‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Key ‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö)
Azure Queue Storage‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ Message Queue ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Asynchronous ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡πÄ‡∏≠‡∏≤‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á‡πÉ‡∏ô Queue ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡πÉ‡∏ô Queue ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡πà‡∏≠
‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ó‡∏±‡∏ô‡πÉ‡∏î ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Thumbnails ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà User Upload ‡∏†‡∏≤‡∏û
‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ‚ÄúAzure Managed Disks‚Äù ‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏õ‡πá‡∏ô Virtual Disk ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô VMs ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Disk ‡∏Ñ‡∏£‡∏±‡∏ö
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô OS Disk, Data Disk
Storage Tiers
‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö
Azure ‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Network ‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö On Premise ‡πÅ‡∏•‡∏∞ Cloud ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏£‡∏≤‡∏°‡∏≤‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö : )
‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ü‡∏£‡∏µ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Network ‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ö‡∏ô Azure ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö
‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á VMs ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Virtual Network ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏™‡∏°‡∏≠ ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥ VPN Site-to-Site ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Network ‡∏Ç‡∏≠‡∏á On-Premise ‡πÅ‡∏•‡∏∞ Azure ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
‡πÄ‡∏£‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î Subnets ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Virtual Network ‡∏î‡πâ‡∏ß‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≤‡∏á Azure ‡∏à‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö DHCP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏à‡∏Å IP ‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
Network Security Group (NSG)
‡πÄ‡∏õ‡πá‡∏ô Firewall ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö Virtual Machine ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏¥‡∏î‡∏õ‡∏¥‡∏î Port ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Windows ‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î RDP Port 3389, ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Linux ‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î SSH Port 22 ‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
Load Balancing ‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Request ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏¢‡∏±‡∏á Servers ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢ Request ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÜ Servers
‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà Servers ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Stateless ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ State ‡∏ñ‡∏π‡∏Å‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏ß‡πâ‡πÉ‡∏ô Server ‡πÉ‡∏î Server ‡∏ô‡∏∂‡∏á ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà User ‡∏Ñ‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡∏π‡∏Å‡πÇ‡∏¢‡∏ô‡πÑ‡∏õ‡∏¢‡∏±‡∏á Server ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏Å‡πá‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏Å‡∏ï‡∏¥
Azure ‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ Load Balancing ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ
Azure Load Balancer‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô OSI Model ‡∏£‡∏∞‡∏î‡∏±‡∏ö 4 (Transport Layer) ‡∏ö‡∏ô Protocal TCP, UDP ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 Endpoint ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö (Source IP address and port to destination IP address and port)
‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Azure Virtual Machine
‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ Public IP ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ User ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Load Balancer ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤ ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á Load Balancer ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô Private IP ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° Load Balancer ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Servers ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
Azure Application Gateway‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô OSI Model ‡∏£‡∏∞‡∏î‡∏±‡∏ö 7 (Application Layer) ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Transport Layer ‡∏°‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Application Gateway ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Azure Load Balancer ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Cookie-Based Session Affinity, URL Path-Based Routing, Multisite Hosting
‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Web Application (HTTP/HTTPS) ‡πÄ‡∏ä‡πà‡∏ô Azure App Service, ACI, AKS
Azure Traffic Manager‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Network ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ Traffic Manager ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å DNS Server ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ö‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Web App ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏¢‡∏∏‡πÇ‡∏£‡∏õ ‡∏ï‡∏±‡∏ß Traffic Manager ‡∏à‡∏∞‡∏û‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Web App ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏¢‡∏∏‡πÇ‡∏£‡∏õ ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢ ‡∏ï‡∏±‡∏ß Traffic Manager ‡∏à‡∏∞‡∏û‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Web App ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢
Azure Content Delivery Network (CDN)‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏ã‡∏∂‡πà‡∏á Azure ‡∏°‡∏µ CDN ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Content ‡πÄ‡∏ä‡πà‡∏ô Images Files, Document Files
Digital Skill ‚Äî Azure Fundamentals (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
ExamTopics ‚Äî AZ-900 Exam Actual Questions
Facebook ‚Äî Data TH.com ‚Äî Data Science ‡∏ä‡∏¥‡∏•‡∏ä‡∏¥‡∏• (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
Github ‚Äî Microsoft Certified Azure Fundamentals (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
Medium ‚Äî Azure AZ-900 Exam Preparation Guide: How to pass in 3 days
Medium ‚Äî ‡∏ß‡∏µ‡∏ò‡∏µ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏™‡∏≠‡∏ö AZ-900 Online ‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Azure Exam Voucher
Medium ‚Äî AZ-900 ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡πÅ‡∏ô‡∏ß‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡∏•‡∏á‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏™‡∏≠‡∏ö
Medium ‚Äî AZ-900 ‡∏™‡∏£‡∏∏‡∏õ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏∏‡∏î‡πÜ
Microsoft Learn-Azure Fundamentals
Udemy ‚Äî Microsoft Azure ‚Äî Beginner‚Äôs Guide + AZ-900 (‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢)
WhizLabs ‚Äî AZ-900 (‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢)
Workshop ‡πÄ‡∏•‡πá‡∏Å‡πÜ‡∏à‡∏≤‡∏Å Microsoft ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AZ-900 ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°
‡πÉ‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Core Services ‡∏´‡∏•‡∏±‡∏Å‡πÜ‡∏ó‡∏µ‡πà Azure ‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û,‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÅ‡∏•‡∏∞ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô
‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡∏û‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏ö‡∏ô Azure ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡πà‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡πÜ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ô‡∏≤‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô : )
https://www.tt-ss.net/
14 

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
14¬†claps
14 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/plumbersofdatascience/aws-azure-or-gcp-c23f6fe83f45?source=search_post---------385,"There are currently no responses for this story.
Be the first to respond.
I get often asked: What is the best or easiest cloud platform to start with for Data Engineering?
Here are my thoughts on this. A general overview.
Amazon Web Services (AWS) is globally the biggest cloud provider (approx. 50% of the market). This goes for the US and the EU and so on, except China, because they‚Äôre not in China. Microsoft Azure is number two on the market.
Google Cloud Platform (GCP) comes in third place regarding market share.
Generally, when you compare the overall tools and functionalities that are on these platforms, it‚Äôs most of the time the same. All three named cloud services are almost equivalent. Which one you choose is up to your preferences.
If you work in a company, you should pick the cloud platform they are using. Personally, I like AWS and have worked with AWS. It‚Äôs a bit different from working on premise.
When you look at the Azure interface and how you set up stuff it‚Äôs a bit more aimed towards companies who have already worked with on-premise. And have employees who are familiar with on-premise skills and how to set up traditional infrastructures. That‚Äôs where Azure I think shines a bit more.
If you are looking for a job, do some research on the job ads in your area. What are the requirements in the job descriptions? What are employers looking for? Evaluate and then learn exactly that.
There are high chances that you find a job where you can apply AWS skills ‚Äî Azure or GCP will be less..
Which cloud provider you should learn also depends on where you are. Look at the regional preferences and make your choice afterwards.
For example, if you‚Äôre in the US I would look at AWS as it‚Äôs the most used there. Students from Norway say that in Norway it‚Äôs just Azure. Almost nobody uses AWS in Norway for instance. Then it makes sense, if you want to stay in Norway and get a job there, that you‚Äôll learn Azure.
The industry in which you are working or aiming to work also influences the cloud service. E. g. Banking, eCommerce or Aeronautics all have special needs and preferences for cloud providers.
You see, it does not matter which is the easiest one to learn, or which one has the best functionality.
You always have to check carefully what cloud provider suits the best for your existing or future job. If you‚Äôre starting from scratch and you get some cloud experts I would maybe look into AWS more.
See you later.
Andreas
The Data Engineering Community
33 
33¬†claps
33 
Written by
Data Engineer and Plumber of Data Science. I write about platform architecture, tools and techniques that are used to build modern data science platforms
The Data Engineering Community, we publish your Data Engineering stories
Written by
Data Engineer and Plumber of Data Science. I write about platform architecture, tools and techniques that are used to build modern data science platforms
The Data Engineering Community, we publish your Data Engineering stories
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rpradeepmenon/architecting-modern-data-engineering-using-azure-databricks-18686ac1c5ff?source=search_post---------386,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pradeep Menon
Aug 18, 2020¬∑6 min read
There should no doubts in anyone‚Äôs mind about how Big Data and AI are fueling the next revolution. Data is the new oil, and AI refines the oil. The questions to ask is the following:
Data Engineering has become vital for any organization that is serious about harnessing data.
This blog illustrates how Azure Databricks strives to modernize and yet simplify data engineering to reduce the data to insight turnover time.
Let us start by understanding the market.
According to IDC, there are three major trends when it comes to Data and AI.
Cloud computing is the primary catalyst to make these changes happen. Cloud enables innovation.
Forbes predicts that, in 2030, the global business value derived from Advanced Analytics & AI will be $15 trillion. This bucket is distributed across smart products, virtual agents, and decision automation.
The potential is immense.
The path to innovation is never easy. It has its road-blocks. Three key challenges prevent organizations from realizing their objectives with Big Data projects are:
By mapping these challenges to the study conducted by TDWI, it is clear that most companies are not satisfied with their current solutions:
New problems strive for new solutions. Addressing these considerations requires a modern data engineering strategy.
Azure Databricks strives to hit the sweet spot. Azure Databricks is the jointly-developed Data and AI service from Databricks and Microsoft with a razor-sharp focus on data engineering and data science.
Let us deep-dive into the key five capabilities of Azure Databricks.
Azure Databricks provides five key capabilities:
Azure Databricks along with Azure‚Äôs ecosystem offers a solution that is innovative, scalable, and focuses on value at the right cost.
Now that there is a better understanding of Azure Databricks, let us deep-dive into the Architectural constructs of a modern data engineering platform.
These are the four critical pillars of modern data engineering. Ingest. Store. Prep and Train. Model and Serve. It will look traditional, but the devils are in the details.
Let us drill down into it:
The Architecture is scalable, on-demand, with built-in high availability (HA), and is capable of processing and serving petabytes of data at lightning speed.
The architecture illustrated in the previous section is tried and tested with multiple customers across the globe. Architecture is such that it gives the right levers for cost and functionality control.
There are a few fundamental principles to note in this Architecture:
This Architectural pattern enables data engineers to focus on doing data engineering with a single goal in mind: To reduce data to insight turnover time. Focus less on technicalities. Focus more on functionality.
#Data and #AI Strategist @ Microsoft. Impact driven. Executive-level interpersonal skills. Hands-On. #WorldTraveller. #Blogger
26 
26¬†claps
26 
#Data and #AI Strategist @ Microsoft. Impact driven. Executive-level interpersonal skills. Hands-On. #WorldTraveller. #Blogger
"
https://medium.com/devops-dudes/amazons-failed-implementation-of-azure-devops-4e68965c96d2?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
Cloud computing is all the rage these days. AWS was way ahead of the curve when they first launched EC2 in August of 2006 to the general public. If you‚Äôre reading this right now you‚Äôve probably also seen the #CloudMania at your organization.
"
https://medium.com/open-at-microsoft/the-azure-25-dollar-challenge-2fe77ed18234?source=search_post---------388,"There are currently no responses for this story.
Be the first to respond.
What can you do with $25 in Azure? Ian Philpot, Sr. Technical Evangelist at Microsoft, breaks down all the different things you can do!
Here are a couple of highlights from this list‚Ä¶
Hopefully this paints a pretty clear picture that $25 dollars a month can go a long way.
Original blog post can be found here.
All things open at Microsoft
12 
12¬†claps
12 
All things open at Microsoft
Written by
All things open at Microsoft. www.microsoft.com/opensource
All things open at Microsoft
"
https://medium.com/@renatogroffe/azure-kubernetes-kubernetes-minha-m%C3%A1quina-roda-xp-setembro-2019-bedd6e71bddf?source=search_post---------389,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 23, 2019¬∑4 min read
No dia 18/09/2019 (quarta-feira) participei como palestrante do Minha M√°quina Roda XP, um encontro t√©cnico com profissionais da √°rea de Tecnologia da XP Investimentos em S√£o Paulo-SP e no qual tive a oportunidade de realizar uma apresenta√ß√£o focada no uso do Azure Kubernetes Service na orquestra√ß√£o de containers Docker.
Deixo aqui meus agradecimentos ao Thiago Fonseca (XP Investimentos) e ao Thiago Adriano (XP Investimentos, Microsoft MVP) pelo convite e por todo o apoio para que este evento acontecesse, sendo que tivemos um excelente p√∫blico para um in√≠cio de noite: aproximadamente 40 pessoas presentes!
Tem interesse por conhecer mais sobre orquestra√ß√£o de containers Docker com Kubernetes? E combinando a utiliza√ß√£o de Kubernetes com tecnologias como Azure DevOps para o deployment automatizado de aplica√ß√µes? N√£o deixe ent√£o de acompanhar este evento PRESENCIAL e GRATUITO da comunidade DevOps Professionals e que ser√° realizada no audit√≥rio do Audit√≥rio da TOTVS, localizado na Avenida Braz Leme, 1000 ‚Äî Santana ‚Äî S√£o Paulo-SP.
* DevOps + Kubernetes: uma imers√£o √† orquestra√ß√£o e deployment automatizado de containers *
Quando: 08/10/2019 (ter√ßa) a partir das 18:45
Palestrantes:- Renato Groffe (Microsoft MVP, MTAC)- Milton Camara Gomes (Microsoft MVP)
Ficou interessado(a)? Fa√ßa sua inscri√ß√£o no link a seguir para garantir sua presen√ßa e n√£o deixe de indicar o evento para amigas, amigos e colegas de trabalho.
** Link para inscri√ß√µes: http://bit.ly/devops-totvs-10-2019
* Teremos sorteio de brindes e um happy hour ao final do evento! *
Os slides da apresenta√ß√£o j√° est√£o no SlideShare:
A demonstra√ß√£o pr√°tica fez uso da imagem p√∫blica renatogroffe/apicontagem-env, que est√° dispon√≠vel para utiliza√ß√£o a partir do Docker Hub.
Os fontes que serviram de base para a gera√ß√£o desta imagem (uma API REST criada com o ASP.NET Core 2.2) est√£o no GitHub:
ASP.NET Core 2.2 + .NET Core 2.2 + Docker + Docker Compose + Environment Variables
J√° os scripts com comandos para a cria√ß√£o/configura√ß√£o do cluster Kubernetes e as defini√ß√µes para gera√ß√£o de objetos como Pods, Deployment e Service se encontram no seguinte reposit√≥rio do GitHub:
Docker + Kubernetes + AKS (Azure Kubernetes Service) + Environment Variables
No webinar a seguir (produzido para a Microsoft) demonstrei a utiliza√ß√£o do Azure Kubernetes Service (AKS); a grava√ß√£o pode ser assistida gratuitamente a partir do seguinte link:
Kubernetes: do Pod ao Deployment Automatizado [V√≠deo]
E para concluir deixo ainda como refer√™ncias os seguintes conte√∫dos gratuitos (artigos) que produzi para a Microsoft e tamb√©m para o meu blog:
Orquestra√ß√£o de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
Azure Kubernetes Services ‚Äî AKS: refer√™ncias gratuitas e dicas para solu√ß√£o de problemas comuns
Docker: dicas e truques na utiliza√ß√£o de containers ‚Äî Parte 1
Docker: dicas e truques na utiliza√ß√£o de containers ‚Äî Parte 2
Docker para Desenvolvedores .NET ‚Äî Guia de Refer√™ncia
A seguir est√£o fotos da apresenta√ß√£o:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
14 
14¬†
14 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/visual-studio-2019-dica-r%C3%A1pida-gerenciando-recursos-do-azure-via-cloud-explorer-4b69b331bdfe?source=search_post---------390,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Aug 12, 2019¬∑4 min read
Desenvolvedores .NET t√™m no Visual Studio uma ferramenta fundamental na implementa√ß√£o de suas aplica√ß√µes, com esta IDE oferecendo produtividade e trazendo um grande de n√∫mero de alternativas que em muito simplificam o processo de constru√ß√£o de novos projetos.
Uma destas op√ß√µes pode ser extremamente √∫til para o desenvolvimento de solu√ß√µes baseadas no Microsoft Azure: trata-se do Cloud Explorer, um recurso presente nas vers√µes 2017 e 2019 do Visual Studio. Tal funcionalidade permite a visualiza√ß√£o dos diferentes recursos vinculados a uma conta do Azure, al√©m da possibilidade de executar algumas a√ß√µes de acordo com o tipo de elemento considerado.
E como o assunto deste post √© o Microsoft Azure, deixo aqui um convite para um treinamento‚Ä¶
O Azure na Pr√°tica surgiu de necessidades observadas por mim (Renato Groffe) e 2 amigos (os palestrantes e Microsoft MVPs Ericson da Fonseca e Milton Camara Gomes) ao longo de diversos eventos em que os mesmos participaram nos √∫ltimos anos.
Constatamos nestes encontros t√©cnicos d√∫vidas recorrentes sobre qual a melhor maneira de uma empresa migrar para a nuvem, quais servi√ßos utilizar, al√©m de como conduzir este processo de mudan√ßa sem custos abusivos e maximizando a produtividade das equipes de TI.
Diante de tudo isso, surgiu a ideia de compartilharmos nossas experi√™ncias atrav√©s de um curso totalmente diferente dos hoje existentes no mercado. Um treinamento com foco totalmente pr√°tico e enfatizando os principais recursos do Microsoft Azure, tornando o aprendizado muito mais f√°cil e din√¢mico.
A primeira turma ser√° ao longo do dia 21/09/2019 (um s√°bado) em S√£o Paulo capital, enfatizando o desenvolvimento de aplica√ß√µes Web com o Azure. Ficou interessado(a)?
Acesse o link a seguir para obter 20% de desconto e outras informa√ß√µes sobre o treinamento:
http://bit.ly/anp-blog-groffe
Para utilizar o Cloud Explorer ser√° necess√°rio no Visual Studio 2019 acessar o menu View > Cloud Explorer:
Na imagem a seguir √© poss√≠vel observar o Cloud Explorer:
Para exibir os diferentes recursos vinculados a uma conta do Microsoft Azure ser√° necess√°rio efetuar o login com a mesma no Visual Studio:
Com o login realizado aparecer√£o selecionadas as diferentes subscriptions vinculadas a uma conta. Concluir ent√£o este procedimento de configura√ß√£o acionando o bot√£o Apply:
Os recursos de uma subscription ser√£o exibidos seguindo agrupamentos por tipos de recurso:
Podemos ainda organizar tais elementos com base em Resource Groups do Azure:
Embora n√£o apresente todas as op√ß√µes disponibilizadas pelo Portal do Azure, ainda assim o Cloud Explorer conta com outras funcionalidades bastante √∫teis como:
Visual Studio IDE documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
119 
119¬†
119 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@dunith/production-grade-microservices-on-azure-aks-with-ballerina-part-1-the-basics-87b92bef6bdb?source=search_post---------391,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dunith Dhanushka
May 13, 2020¬∑10 min read
In this multipart tutorial series, you will learn how to write a basic Microservice with Ballerina programming language, deploy it to Azure Kubernetes Service (AKS) and make it production-ready with features available in the Azure platform.
The first installment of this series focuses on getting the simplest things done with the bare minimum effort. At the end of this post, you‚Äôll learn the following:
As we dive deep into the series, I‚Äôll show you how to make this microservice production-ready to have features like authentication, auto-scaling, load balancing, monitoring, and integrating with a CI/CD pipeline.
This tutorial assumes a basic understanding of Ballerina programming language. But we are not going to explore all the bells and whistles that come with Ballerina. I‚Äôll limit the scope to the implementation of a basic REST service. For a primer on Ballerina basics, see https://ballerina.io/learn/
To complete this tutorial, you need to install Ballerina on your local machine. Use https://ballerina.io/downloads/ to get it done.
In addition to that, you need a fair amount of knowledge on Microservices, Docker, and Kubernetes along with a local Docker development environment running Linux containers. Docker provides packages that configure Docker on a Mac, Windows, or Linux system.
Finally, you‚Äôll need an account on Azure. It gives you $200 credit to explore Azure for 30 days. If you don‚Äôt have an account already, I suggest you register for a one.
I must say that this tutorial is going to be pretty long and full of code snippets. You can try out the code snippets while reading. But for those who need to skip the post and get into the source code, here‚Äôs the GitHub repo URL.
github.com
Ever since the Microservices concept conceived, Java-based frameworks like Spring Boot had been the developer‚Äôs first choice for writing Microservices. But here I‚Äôm going to take a little detour from that and introduce you to Ballerina, an open-source programming language purpose-built for optimizing microservices development.
The major reason I fell in love with Ballerina is it comes with all batteries included for Microservices development ‚Äî you don‚Äôt need to borrow anything from the outside.
As a developer, my justification for Ballerina is you‚Äôll get more things done with less effort.
In this section, let‚Äôs create a Ballerina service called TodoService and run it locally.
Open up a text editor of your choice (Visual Studio Code in my case) and create a file called todo_service.bal file inside a directory of your choice. Then add the following code to it.
In the above code, TodoService represents an HTTP service that is bound to /todos context. It listens for incoming HTTP connections with TodosEP endpoint which is bound to port 9090. It has got only one resource method getAll() that returns an in-memory array of Todo objects formatted as a JSON array.
Now we have the basic Ballerina Microservice code in place. Let‚Äôs try to run it locally just to make sure that our code works.
Open up a terminal and navigate to the place where you save the todo_service.bal file. Then execute below command in the terminal.
ballerina run todo_service.bal
If you have installed Ballerina on your local machine successfully, the ballerina command will be added to your PATH variable automatically. When you run a Ballerina service, it will spring up a lightweight web server under the covers and bind it to a port specified with the listener configuration. In our case, the port is 9090.
You will see an output similar to below when everything is properly in place.
A GET request to http://localhost:9090/todos will give you the below response in Postman.
In this section, you‚Äôll configure your Azure environment using Azure CLI to deploy above the Todo service.
I‚Äôll be executing almost all the commands related to Azure configurations with Azure CLI. If you haven‚Äôt installed it already in your local machine, see this guide to get it done. Note that you need to have Azure CLI version 2.0.53 or later to complete this tutorial. You can verify the version by running az ‚Äî version command.
In order to point to the Kubernetes cluster in AKS and manage it from the local machine, you are going to need the kubectl command-line tool. If you haven‚Äôt installed it already, see this guide to get it done.
Once installed locally, the Azure CLI tool can be accessed with az command. Type the following command in a terminal to log in with your Azure account credentials.
az login
Once you are successfully logged in to your Azure account, let‚Äôs create a resource group.
An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored.
The following example creates a resource group named myResourceGroup in the eastus location.
az group create --name myResourceGroup --location eastus
You‚Äôll get an output like below when your resource group created successfully.
Azure Container Registry (ACR) is a private registry for container images. A private container registry lets you securely build and deploy your applications and custom code.
Let‚Äôs create a container registry now so that it‚Äôll be useful in the later parts of this tutorial. The following example creates a container registry named dunithd and attaches it to the resource group that I have created previously.
The registry name must be unique within Azure and contain 5‚Äì50 alphanumeric characters. The Basic SKU is a cost-optimized entry point for development purposes that provides a balance of storage and throughput.
Once created, you can refer to your container registry as dunithd.azurecr.io
Now we have a basic Ballerina service that works and we have configured our Azure environment accordingly. Then let‚Äôs see how to generate Kubernetes artifacts for the Todo service.
Ballerina offers a convenient way of generating corresponding Kubernetes artifacts automatically with annotations built into the language. The Kubernetes builder extension offers native support for running Ballerina programs on Kubernetes with the use of annotations that you can include as part of your service code. Also, it will take care of the creation of the Docker images, so you don‚Äôt need to explicitly create Docker images prior to deployment on Kubernetes.
For more information, see How to Deploy Ballerina Programs in the Cloud.
To add annotations, open the todo_service.bal file and overwrite its content with the below code.
Here, @kubernetes:Deployment is used to specify the Docker image name, which will be created as part of building this service. Notice that I have put the private Docker registry name created in Part 2 in the image name as below.
image : ‚Äúdunithd.azurecr.io/todo-service:v1‚Äù
The @kubernetes:Service {} annotation will create a Kubernetes service that will expose the Ballerina service running on a Pod. For simplicity, I have put LoadBalancer as the service type. Then Azure will automatically provision a load balancer resource for me with an external IP address. I will show you how to configure NGINX as an Ingress controller in a later tutorial.
Now let‚Äôs use the below command to build the Todo_service.bal file. This will also create the corresponding Docker image and the Kubernetes artifacts using the Kubernetes annotations that you have configured above.
You can use the docker images command to verify that the Docker image specified in the @kubernetes:Deployment was created.
The Kubernetes artifacts related to your service will be generated in addition to the .jar file. Notice the docker and kubernetes directories created in your working directory.
Now we have Todo service as a Docker image in our local repository. This needs to be pushed to the Azure container registry (ACR) so that our Kubernetes cluster can pull it from there.
In a previous step, we created a container registry in Azure. In order to use it, you must first login. In the Azure CLI, type the below command with the name you have given as the registry name before (dunithd in my case).
az acr login --name dunithd
The command returns a Login Succeeded message once completed.
Use the docker push command to push to the local image to your ACR as follows.
docker push dunithd.azurecr.io/todo-service:v1
This command may take a few minutes to complete.
The following command returns a list of images that have been pushed to your ACR instance
The following command returns the tags for the todo-service image we just pushed.
So far, we have tested the todo-service locally, generated a Docker image, and Kubernetes artifacts out of it and pushed the local image to the Azure container registry. What‚Äôs left is to create a Kubernetes cluster in AKS and deploy the generated artifacts into that.
The following command creates a Kubernetes cluster of two nodes named myAKSCluster in the resource group named myResourceGroup which we have created in a previous step.
Here, the AKS cluster needs to access Azure Container Registry (ACR) instance to pull the todo-service:v1 image you pushed earlier. For that, Azure automatically creates an Azure Active Directory service principal and grants the right to pull images from the ACR instance. If needed, there‚Äôs an option to use a managed identity instead of a service principal for easier management.
After a few minutes, the deployment completes and returns JSON-formatted information about the AKS deployment.
The following command configures the kubectl tool to connect to your AKS cluster. It fetches credentials for the AKS cluster named myAKSCluster in the myResourceGroup and creates an entry in your ~/.kube directory.
Verify the connectivity to your cluster by running kubectl get nodes command as follows.
Now we are at the final moment of our tutorial. The only thing left is to deploy the Kubernetes artifacts generated in a previous step.
From your working directory where todo_service.bal resides, execute the below command.
If everything goes fine, it should produce an output like below.
Behind the scenes, kubectl has created a deployment and a service for our Todo service. You can verify the service as follows.
Notice the external IP address assigned our todos service. That is because I have set the service type to LoadBalancer when configuring the service annotation.
Now our service is accessible from outside the cluster. Here‚Äôs the output I got when I called the todo service with the external IP using Postman.
See, how easy it is to spin up a Kubernetes cluster and deploy your Microservice into that!
As this tutorial is the last part of the series, you may want to delete the AKS cluster. As the Kubernetes nodes run on Azure virtual machines (VMs), they continue to incur charges even if you don‚Äôt use the cluster. The following command removes the resource group, container service, and all related resources.
az group delete --name myResourceGroup --yes --no-wait
Well, I guess the tutorial was a bit long. But it covers almost every step that is essential to write a Microservice with Ballerina and getting that deployed into AKS. For an absolute beginner on Ballerina and AKS, this tutorial would be an ideal place to start.
We merely scratched the surface here. When I said ‚Äúproduction-grade‚Äù in the title, there‚Äôs a lot more to come in this series.
Hence, expect the following posts as follow up items to this article in the future.
If there‚Äôs any additional topic you want me to discuss in this series, put them as a comment or tweet me.
Editor of Event-driven Utopia(eventdrivenutopia.com). Technologist, Writer, Developer Advocate at StarTree. Event-driven Architecture, DataInMotion
262 
1
262¬†
262 
1
Editor of Event-driven Utopia(eventdrivenutopia.com). Technologist, Writer, Developer Advocate at StarTree. Event-driven Architecture, DataInMotion
"
https://medium.com/streaming-at-scale-in-azure/serverless-streaming-at-scale-with-azure-sql-304cd2bfd5c2?source=search_post---------392,"There are currently no responses for this story.
Be the first to respond.
Just before Ignite, a very interesting case study done with RXR has been released, where they showcased their IoT solution to bring safety in building during COVID times. It uses Azure SQL to store warm data, allowing it to be served and consumed to all downstream users, from analytical application to mobile clients, dashboards, API and business users.
If you haven‚Äôt done yet, you definitely should watch the Ignite recording (the IoT part start at minute 22:59). Not only the architecture presented is super interesting, but also the guest presenting it ‚Äî Tara Walker ‚Äî is super entertaining and joyful to listen. Which is not something common in technical sessions. Definitely a bonus!
If you are interested in the details, beside the Ignite recording, take a look also at the related Mechanics video, where things are discussed a bit more deeply.
Implement a Kappa or Lambda architecture on Azure using Event Hubs, Stream Analytics and Azure SQL, to ingest at least 1 Billion message per day on a 16 vCores database
The video reminded me that in my long ‚Äúto-write‚Äù blog post list, I have one exactly on this subject. How to use Azure SQL to create a amazing IoT solution. Well, not only IoT. More correctly how to implement a Kappa or Lambda architecture on Azure using Event Hubs, Stream Analytics and Azure SQL. It‚Äôs a very generic architecture that can be easily turned to IoT just by using IoT Hub instead of Event Hubs and it can be used as is if you need, instead, to implement an ingestion and processing architecture for the Gaming industry, for example.
Goal is to create a solution that can ingest and process up to 10K message/secs, which is close to 1 Billion message per day, which is a value that will be more than enough for many use cases and scenario. And if someone needs more, you can just scale up the solution.
This article is quite long. So, if you‚Äôre in hurry, or you already know all the technical details on the aforementioned services, or you don‚Äôt really care too much about tech stuff right now, you can just go away with the following key points.
If you‚Äôre now ready for some tech stuff, let‚Äôs get started.
So, let‚Äôs see it in detail. As usual, I don‚Äôt like to discuss without also having a practical way to share knowledge, so you can find everything ready to be deployed in your Azure subscription here:
github.com
As that would not be enough, I also enjoyed recording a short video to go through the working solution, giving you a glimpse of what you‚Äôll get, without the need for you to spend any credit, if you are not yet ready to do that:
Creating a streaming solution usually means implementing one of two very well know architectures: Kappa or Lambda. They are very close to each other, and it‚Äôs safe to say that Kappa is a simplified version of Lambda. Both have a very similar data pipeline:
Event Hubs is probably the easiest way to ingest data at scale in Azure. It is also used behind the scenes by IoT Hub, so everything you learn on Event Hubs, will be applicable to IoT Hub too.
It is very easy to use, but at the beginning some of the concepts can be quite new and not immediate to grasp, so make sure to check out this page to understand all the details: Azure Event Hubs ‚Äî A big data streaming platform and event ingestion service
Long story short: you want to ingest a massive amount of data in the shortest time possible, and keep doing that for as much as you need. To achieve the scalability you need, a distributed system is required, and so data must be partitioned across several nodes.
In Event Hubs you have to decide how to partition ingested data when you create the service, and you cannot change it later. This is the tricky part. How do you know how many partition you will need? That‚Äôs a very complex answer, as it is completely dependent on how fast who will read the ingested data will be able to go.
If you have only one partition and one of the parallel applications that will consume the data is slow, you are creating a bottleneck.
If you have too many partitions, you will need to have a lot of clients reading the data, but if data is not coming in fast enough, you‚Äôll starve your consumers, meaning you are probably wasting your money in running processes that are doing nothing for a big percentage of their CPU time.
So let‚Äôs say that you have 10MB/sec of data coming in. If each of your consuming client can process data at 4MB/sec, you probably want 3 of them to work in parallel (with the hypothesis that your data can be perfectly and evenly spread across all partitions), so you will probably want to create at least 3 partitions.
That‚Äôs a good starting point, but 3 partitions is not the correct answer. Let‚Äôs understand why by making the example a bit more realistic and thus slightly more complex.
Event Hubs let you pick and choose the Partition Key, which is the property whose values will be used to decide in which partition an ingested message will land. All messages with same partition key value, will land in the same partition. Also, if you need to process messages in the order they are received, you must put them in the same partition. If fact, ordering is guaranteed only at partition level.
In our sample we‚Äôll be partitioning by DeviceId, meaning data coming from the same device will land in the same partition. Here‚Äôs how the sample data is generated
In Event Hubs the ‚Äúpower‚Äù you have available (and that you pay for) is measured in Throughput Units (TU). Each TU guarantees that it will support 1MB/sec or 1000 messages(or events)/sec , whichever came first. If we want to be able to process 10.000 events/sec we need at least 10 TU. Since it‚Äôs very unlikely that our workload will be perfectly stable, without any peak here and there, I would go for 12 TU, to have some margin to handle some expected workload spike.
TU can be changed on the fly, increasing on reducing them as you need.
It‚Äôs time to decide how many TU and Partitions we need inour sample. We want to be able to reach at least 10K messages/second. TU are not an issue as they can be change on the fly, but deciding how many partitions we need is more challenging. We‚Äôll be using Stream Analytics, and we don‚Äôt exactly know how fast it will be able to consume incoming data.
Of course one road is running test to figure out the correct numbers, but we still need to come up with some reasonable numbers also to just to start with such test. Well, a good rule of thumb is the following:
Rule of thumb: create an amount for partitions equal to the number of throughput units you have or you might expect to have in future
For what concern the ingestion part we‚Äôre good now. If you want to know more, please take a look at this article: Partitioning in Event Hubs and Kafka, that will go into detail of this topic. Super recommended!
Let‚Äôs now move to discuss how to process the data that will be thrown at us, doing it as fast as possible.
Azure Stream Analytics is an amazing serverless streaming processing engine. It is based on the open source Trill framework which source code is available on GitHub and is capable to process a trillion message per day. All without requiring you to manage and maintain the complexity of a extremely scalable distributed solution.
Stream Analytics support a powerful SQL-like declarative language: tell it what you want and it will figure out how to do it, fast.
It also supports a SQL-like language so all you have to do to define how to process your event is to write a SQL query (with the ability to extend it with C# or Javascript) and nothing more. Thanks to SQL simplicity and ability to express what you want opposed to what to do, development efficiency is very high. For example determining for how long an event lasted, for example, is as easy as doing this:
All the complexity of managing the stream of data used as the input, with all its temporal connotations, is done for you, and all you have to tell Stream Analytics is that it should calculate the difference between a start and end event on per user and feature basis. No need to write complex custom stateful aggregation functions or other complex stuff. Let‚Äôs keep everything simple and leverage the serverless power and flexibility.
As for any distributed system, the concept of partitioning is key, as it is the backbone of any scale-out approach. In Stream Analytics, since we are getting data from Event Hub or IoT Hub, we can try to use exactly the same partition configuration already defined in those services. If was use the same partition configuration also in Azure SQL, we can achieve what are defined as embarrassingly parallel jobs where there is no interaction between partitions and everything can be processed fully in parallel. Which means: at the fastest speed possible.
Streaming Units (SU) is the unit of scale that you use ‚Äî and pay for‚Äîin Azure Stream Analytics. There is no easy way to understand how many SU you need, as consumption will totally depend on how complex your query is. The recommendation is to start with 6 and then monitor the Resource Utilization to see how much percentage of available SU you are using. If your query partition data using PARTITON BY, SU usage will increase as your are distributing the workload across nodes. This is good, as it means you‚Äôll be able to process more data in the same amount of time . You also want to make sure SU utilization is below 80% as after that your events will be queued, which means you‚Äôll see higher latency. If everything works well, we‚Äôll be able to ingest our target of 10K events/sec (or 600K events/minute as pictured below)
Azure SQL is really a great database for storing hot and warm data of an IoT solution. I know this is quite the opposite of what many thinks. A relational database is rigid, it requires schema-on-write, and on IoT or Log Processing scenarios, the best approach is a schema-on-read instead. Well, Azure SQL actually supports both and more.
With Azure SQL you can do both schema-on-read and schema-on-write, via native JSON support
In fact, beside what just said, there are several reason for this, and I‚Äôm sure you are quite surprised to hear that, so, read on:
Describing each one of the listed features, even just at a very high level, would require an article on its own. And of course, such article is available here, if you are interested (and you should!): 10 Reasons why Azure SQL is the Best Database for Developers.
In order to accommodate a realistic scenario where you have some fields that are always present, while some other can vary by time or device, the sample is using the following table to store ingested data
As we really want to create something really close to a real production workload, indexes have been created too:
At the time of writing I‚Äôve been running this sample for weeks and my database is now close to 30TB:
Table is partitioned by PartitionId (which is in turn generated by Event Hubs based on DeviceId) and a query like the following
Takes less then 50 msec to be executed including also the time to send the result to the client. That‚Äôs pretty impressive. The result also shows something impressive too:
As you can see, there are two calculated columns QueueTime and ProcessTime that shows, in milliseconds, how much time an event has been waiting in Event Hubs to be picked up by Stream Analytics to be processed, and how much time the same event spent within Stream Analytics before land into Azure SQL. Each event (all the 10K per second) is processed in ‚Äî overall‚Äîless than 300 msec on average. 280msec more precisely.
That is very impressive.
End-to-End ingestion latency is around 300msec
You can also go lower than that using some more specific streaming tool like Apache Flink, if you really need to completely avoid any batching technique to decrease the latency to the minimum possible. But unless you have some very unique and specific requirements, processing events in less than a second is probably more than enough for you.
For Azure SQL, ingesting data at scale is not a particularly complex or demanding job, on the contrary of what can expect. If done well, using bulk load libraries, the process can be extremely efficient. In the sample I have used a small Azure SQL 16 vCore tier to sustain the ingestion of 10K event/secs, using on average 15% of CPU resources on a bit more of 20% of the IO resources.
This means that in theory I could also use an even smaller 8 vCore tier. While that is absolutely true, you have to think of at least three other factors when sizing Azure SQL:
Just as an example, I have stopped Stream Analytics for a few minutes, allowing messages to pile up a bit. As soon as I restarted it, it tried to process messages as fast as possible, in order to empty the queue and return to the ideal situation where latency is less then a second. In order allow Stream Analytics to process data at higher rate, Azure SQL must be able to handle the additional workload too, otherwise it will slow down all the other components in the pipeline.
As expected, Azure SQL handled the additional workload without breaking a sweat.
For all the needed time, Azure SQL was able to ingest almost twice the regular workload, processing more than 1 Million messages per minute. All of this with CPU usage staying well below 15%, and with a relative spike only to the Log IO ‚Äî something expected as Azure SQL uses a Write-Ahead Log pattern to guarantee ACID properties‚Äîwhich, still, never went over 45%.
Really, really, amazing.
With such configuration ‚Äî and remember we‚Äôre just using a 16vCore tier, but we can scale up to 80 and more ‚Äî our system can handle something like 1 billion messages a day, with an average processing latency of less then a second.
The deployed solution can handle 1 billion messages a day, with an average processing latency of less then a second.
Partitioning plays a key role also in Azure SQL: as said before, if need to operate on a lot of data concurrently, partitioning is really something you need to take into account.
Partitioning in this case is used to allow concurrent bulk insert into the target table, even if on such table several indexes exists and thus needs to be kept updated.
Table has been partitioned using the PartitionId column, in order to have the processing pipeline completely aligned. The PartitionId value is in fact generated by Event Hub, which partitions data by DeviceId, so that all data coming from the same device will land in the same partition.
Stream Analytics uses the same partitions provided by Event Hub and so it make sense to align Azure SQL partitions to this logic too, to avoid to cross the streams, which we all know is a bad thing to do. Data will move from source to destination in parallel streams providing the performances and the scalability we are looking for.
Table partitioning also allows Azure SQL to update the several indexes existing on the target table without ending in tangled locking, where transactions are waiting for each other with the result of huge negative impact on performances. As long as table and indexes are using the same partitioning strategy everything will move forward without any lock or deadlock problem.
Higher concurrency is not the only perk of a good partitioning strategy. Partitions allow extremely fast data movement between tables. We‚Äôll take advantage of this ability for creating highly compressed column-store indexes soon.
What if you need to run complex analytical queries on the data being ingested? That‚Äôs a very common requirement for Near-Real Time Analytics or HTAP (Hybrid Transaction/Analytical Processing) solutions.
As you have noticed, you still have enough resources free to run some complex queries, but what if you have to run many really complex queries, for example to compare average values of month-over-month, on the same table were data is being ingest? Or what if you need to allow many mobile client to access the ingested data, all running small but CPU intensive queries? Risk of resource contention ‚Äî and thus low performances ‚Äî becomes real.
That‚Äôs when a scale-out approach start to get interesting.
With Azure SQL Hyperscale you can create up to 4 readable-copies of the database, all with their own private set of resources (CPU, memory and local cache), that will give you access to exactly the same data sitting in the primary database, but without interfering with it at all. You can run the most complex query you can imagine on a secondary, and the primary will not even notice it. Ingestion will proceed as usual rate, completely unaffected by the fact that a huge analytical query or many concurrent small queries are hitting the secondary nodes.
Columnstore tables (or index in Azure SQL terms) are just perfect for HTAP and Near Real Time Analytics scenario, as already described times ago here: Get started with Columnstore for real-time operational analytics.
This article is already long enough, so I‚Äôll not get into details here, but I will focus on the fact that using columnstore index as a target of a Stream Analytics workload, may not be the best option, if you are also looking for low latency. To keep latency small, a small batch size needs to be used, but this is against the best practices for columnstore, as it will create a very fragmented index.
To address this issue, we can use a feature offered by partition table. Stream Analytics will land data into a regular partitioned rowstore table. On scheduled intervals a partition will be switched out into a staging table, so that it be loaded into a columnstore table using Azure Data Factory, for example, so that all best practices can be applied to have the highest compression and the minimum fragmentation.
What if everything just described is still not enough? What if you need a scale so extreme that you need to be able to ingest and process something like 400 Billions rows per day? Azure SQL allows you to do that, by using In-Memory, latch-free, tables, as described in this amazing article:
techcommunity.microsoft.com
I guess that, now, even if you have the most demanding workload, you should be covered. If you need even more power‚Ä¶let me know. I‚Äôll be extremely interested in understanding your scenario.
We‚Äôre at the end of this long article, where we learned how it is possible with a Kappa (or Lambda) architecture to ingest, process and serve 10K msg/sec using only PaaS services. As we haven‚Äôt maxed out any of the resource of our services, we know we can scale to a much higher level. At least twice that goal value, without changing anything and much more than that by increasing resources. With Azure SQL we are just using 16 vCores and it can be scale up to 128. Plenty of space to grow.
Azure SQL is a great database for IoT and HTAP workload
Notes on creating streaming at scale solution in the Azure‚Ä¶
60 
1

By signing up, you will create a Medium account if you don‚Äôt already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
60¬†claps
60 
1
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ashish_fagna/introduction-to-microsoft-azure-bot-service-luis-language-understanding-8826d29d013e?source=search_post---------393,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ashish
Apr 13, 2018¬∑3 min read
Introduction:
Microsoft Azure in 2017 launched the General Availability of the Azure Bot service and the language understanding service (LUIS). These are the two amazing cloud AI service for creating conversational AI experiences.
In this article we will see how easy it is to create intelligent and connected Bots. Azure makes creating bots really easy. Azure bot service provides a core component and hosting environment for creating bots including the bot builder SDK for developing bots and the bot connector service to connect the bots to channels. After bot is created, you can add intelligence to the bot with Microsoft Cognitive Services such as Language Understanding Service (LUIS), Vision, Speech and many other capabilities so the bot can see, hear, understand and interact in more natural ways.
With Azure bot service, developers can get started in a few minutes with out of the box templates and reach your audience across the multiple supported channels or provide custom experience in your app or website using web chat.
Few points worth noting about Azure Bot Service:
Now let us explore Azure Bot Service in detail. In order to use Azure Bot Service, sign up on portal.azure.com and subscribe to Azure.
Once you login, you need to click on ‚ÄúNew Dashboard‚Äù and then you will see ‚ÄúAI+Cognitive Services‚Äù. Once you click on this option, you will see the ‚ÄòWeb App Bot‚Äô listed as one of the featured services.
Once you click ‚Äúsee all‚Äù next to ‚ÄúFeatured‚Äù link, you will see all the AI and Cognitive Services. There you can see under the ‚ÄúBot Service‚Äù subcategory, there are three offerings of the bot service.:
If you already have a bot, it helps to register to the multiple channels (bot interfaces) Azure Bot Service supports. In the article, we will use ‚ÄúWeb App Bot‚Äù. Once you click on it, you will see a new dialog box displaying information and useful links for ‚ÄòWeb App Bot‚Äô.
Next, Click on ‚ÄúCreate‚Äù to start the creation process.
Getting Started NodeJS Code:
If you wish to create a Bot using Azure Bot Service,you can refer to this blog post https://docs.microsoft.com/en-us/azure/bot-service/nodejs/bot-builder-nodejs-quickstart
Hope you find this article useful to get started with using Azure bot Service and LUIS.
My Name is Ashish @ashish_fagna. I am a software consultant. LinkedIn profile. If you enjoyed this article, please recommend and share it! Thanks for your time.
You can also contact me on my email ashish [dot] fagna [at] gmail.com
Software Developer. Machine Learning, Artificial Intelligence Learner.
See all (1,892)
75 
4
75¬†claps
75 
4
Software Developer. Machine Learning, Artificial Intelligence Learner.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/introduction-to-azure-private-link-andprivate-endpoint-and-private-link-service-a61be184356e?source=search_post---------394,"There are currently no responses for this story.
Be the first to respond.
Introduction to Azure Private Link ‚Äî What is Azure Private Link?
Private Link enables access to hosted customer and partner services over a private endpoint in your virtual network. It enables a true private connectivity experience between services and virtual networks.
"
https://itnext.io/how-to-get-your-kubernetes-cluster-service-principal-and-use-it-to-access-other-azure-services-637f185a5112?source=search_post---------395,"So, you have a Kubernetes cluster on Azure (AKS) that needs to access other Azure services like Azure Container Registry (ACR)? You can use your AKS cluster service principal for this.
All you need to do is delegate access to the required Azure resources to the service principal. Simply create a role assignment using az role assignment createto do the following:
It looks something like this:
Notice that the --assignee here is nothing but the service principal and you're going to need it.
When you create an AKS cluster in the Azure portal or using the az aks create command from the Azure CLI, Azure can automatically generate a service principal. Alternatively, you can create one your self using az ad sp create-for-rbac --skip-assignment and then use the service principal appId in --service-principal and --client-secret (password) parameters in the az aks create command.
You can use a handy little query in the az aks showcommand to locate the service principal quickly!
This will the service principal appId! You can use it to grant permissions. For e.g. if you want to allow AKS to work with ACR, you can grant the acrpull role:
If you found this article helpful, please like and follow! Happy to get feedback via Twitter or just drop a comment :-)
twitter.com
ITNEXT is a platform for IT developers & software engineers‚Ä¶
94 
94¬†claps
94 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/wortell/detecting-cve-2020-0601-and-other-attempts-to-exploit-known-vulnerabilities-using-azure-sentinel-652fbcc0364c?source=search_post---------396,"There are currently no responses for this story.
Be the first to respond.
Yesterday, Microsoft has released a security update for Windows which includes a fix to a dangerous bug that would allow an attacker to spoof a certificate, making it look like it came from a trusted source.
The vulnerability (CVE-2020‚Äì0601) was reported to Microsoft by the NSA. The root cause of this vulnerability is a flawed implementation of the Elliptic Curve Cryptography (ECC) within Microsoft‚Äôs code.
Tal Be‚Äôery, Microsoft‚Äôs security research manager, wrote an article explaining the root cause of the vulnerability using a Load Bearing analogy. You can find that here.
Watching the logs
Windows has a function for publishing events when an attempted security vulnerability exploit is detected in your user-mode application called CveEventWrite. To my knowledge, the fix for CVE-2020‚Äì0601 is the first code to call this API.
After the Windows update is applied, the system will generate event ID 1 in the eventlog after each reboot under Windows Logs/Application when an attempt to exploit a known vulnerability is detected. This event is raised by a User mode process, and more information can be found in the MSRC guidance for this fix (scroll to the bottom of the article to find it).
Azure Sentinel
By default the Log Analytics workspace powering Azure Sentinel will not collect events from the Application eventlog. You can change this by going to the Advanced Settings for your workspace, and adding Application as a source under Data > Windows Event Logs:
There are two ways of going about creating the Rule. Either you set up detection for event ID 1 in general so that you get alerted for any (future) CVE abuse, or you set up specific rules per CVE.
There will be pro‚Äôs or con‚Äôs to both approach. In this sample we‚Äôll build a specific Rule to detect potential CVE-2020‚Äì0601 abuse so that we can map it to the right MITRE tactics and follow specifically.
Alert rule
Pete Bryan, who works at Microsoft‚Äôs Threat Intelligence Center (MSTIC), tweeted a sample KQL query that you can use:
I was debating with Olaf Hartong from FalconForce which MITRE TTP‚Äôs would be most applicable. These make most sense to us:
That means that we‚Äôll be tagging the rule for Defense Evasion and Initial Access tactics.
AzSentinel powershell
If you want to programmatically push this rule into your Azure Sentinel environment, go have a look at our AzSentinel powershell module. My colleague Pouyan Khabazi built this module to work with alert rules in Azure Sentinel through automation. More information on working with the module can be found here.
Here‚Äôs the rule definition you could use with AzSentinel:
Microsoft Defender ATP
If you have Microsoft Defender ATP deployed across your enterprise (which now also supports MacOS), you will get the detection out of the box. The logic was added to MDATP at the same time that Microsoft released the fix:
And because MDATP is now unified with other the other Microsoft ATP products, the detection will also show up in the new Microsoft Threat Protection (MTP):
PRO TIP: Microsoft Defender ATP can also be deployed on servers. For workloads living in Azure: enable Azure Security Center (standard) and the MDATP client will be deployed and configured automatically.
Analyzing your enterprise for CVE-2020‚Äì0601 attacks
Today, the Microsoft Defender team also added a dashboard in the Threat Analytics section of MDATP. The dashboard shows you more information about the CVE and whether or not endpoints in your organization were found to have been potentially abused:
However, Microsoft also released the hunting query that you could use to investigate. Here‚Äôs the KQL:
Best of suite
In the case that you have MDATP and/or MTP, there is no need to create the Azure Sentinel rule(s). Just enable the Microsoft Defender ATP connector to Azure Sentinel and the alert will be created automatically.
Happy hunting!
‚Äî Maarten Goet, MVP & RD
Microsoft Cloud & Enterprise Security
21 
21¬†claps
21 
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you‚Äôll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer ‚Äî welcome home. It‚Äôs easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://jeremythake.com/using-powershell-to-update-an-azure-active-directory-user-manager-field-5b02fac116d1?source=search_post---------397,"I‚Äôm setting up a lot of demo environments right now to highlight typical organizational directories. I wanted to automate this and be able to roll back after demonstrations.
I used Windows 10 for this and ran PowerShell with ‚ÄúRun as Administrator‚Äù privileges.
You have to install the PowerShell Module for Azure Active Directory using
Once installed I simply typed:
This then prompted me in a browser to log in with my Azure Active Directory administrator credentials. Basically the same administrator credentials for my Office 365 Tenant I‚Äôm using.
Then I tested it worked by using
To get a specific user object I used Get-AzureADUser
Then I could easily see more information about the user by using
You can update attributes for the user object using the PowerShell cmdlet Set-AzureADUser for things like DisplayName etc. But for Manager field there is a special cmdlet called Set-AzureADUserManager. If you wish to remove the manager, you have to use Remove-AzureADUserManager.
For some reason, in the portal.azure.com, you cannot empty the text field and click save on Manager. It always comes back, so I have to use PowerShell if I want to clear this.
Unfortunately, Delve does not reflect this change immediately and you have to wait for a full crawl of Active Directory by the SharePoint User Profiles for this to show up. Unlike on-premises, there is no way to force a full crawl due to the multi-tenant nature of Office 365.
My tech musings
26 
1
26¬†claps
26 
1
Written by
Microsoft Graph Team, Senior Program Manager at Microsoft.
My tech musings
Written by
Microsoft Graph Team, Senior Program Manager at Microsoft.
My tech musings
"
