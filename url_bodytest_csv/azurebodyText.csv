story_url,bodyText
https://medium.com/globallogic-cloud-and-devops-blogs/clouds-compared-aws-vs-azure-vs-gcp-c59519b9d5e4?source=search_post---------0,"There are currently no responses for this story.
Be the first to respond.
Public cloud has, in the last few years, broken through the mainstream enterprise consciousness. Even if these enterprises are not betting it all on cloud, cloud adoption is, in one form or the other, an integral part of most enterprises’ infrastructure strategy and roadmap. For enterprises who are about to embark on this journey, the questions being asked are, “Which cloud platform should I adopt?”, “Which cloud platform provides cost effective services that are a fit for me?”, and “How do I go about my cloud adoption journey?” This blog attempts to answer the first two questions.
To that goal, I have compared the core infrastructure services across the most popular cloud providers, which are Amazon Web Services (AWS), Microsoft Azure and Google Cloud Platform (GCP). In addition to the core infrastructure services, each cloud provider brings their unique proprietary offerings in the NoSQL, Big Data, Analytics, ML and other such areas.
This blog calls out each cloud’s unique aspects that may influence an enterprise’s choice of cloud based on their specific business and technical requirements.
AWS is the oldest public cloud provider with the widest range of products, compute and data storage options and managed services. AWS Marketplace is also the largest marketplace for third party applications and appliances. They also rapidly iterate to continuously add a substantial set of product features. Highly driven by customer feedback, their new services provide close integration with their core services like IAM, KMS etc. They have a strong focus on security and architecture best practices. Their enterprise frameworks such as the Well-Architected Framework and Cloud Adoption Framework have been developed from their experience with large enterprise customers. Besides their mainstream services, they are also known to release unconventional services as SnowMobile (data transfer appliance in a truck), RoboMaker (robotics framework) and Ground Station-as-a-Service (for managed satellite data download). This keeps customer interest piqued and has potential to open up entire industries. Their 51% market share is a proof of that. That said, they aren’t the cheapest cloud on the market. They also don’t seem to be worried about providing deeper container offerings. Their EKS (managed Kubernetes) service was relatively late to the market. Instead, they seem to be placing bets on MicroVMs (Firecracker) and managed functions.
They have lately seem to realign their focus towards hybrid cloud, and have announced offerings such as Outposts, (in partnership with VMware) that will enable customers to use well known AWS services and APIs on infra running in their private data centers.
AWS is a great choice for startups and enterprises alike. From web and analytical workloads, to large scale data center migrations, AWS provides a breath of services that customers can leverage. To help get customers of all shapes and sizes started on the platform, AWS has released supposedly niche services such as RoboMaker on one end, while going backwards to build services such as LightSail (virtual private server) to help even the smallest single server workloads be onboarded without much overhead.
When it comes to compute, AWS provides the widest range of VM types. AWS also currently has the highest compute and storage options available in the market. Their wide range of VM types (136 VM types over 26 VM families) enable customers to run everything from small web workloads to the biggest HPC and SAP workloads.
For machine learning and AI workloads, AWS also provides the highest configurations of GPU enabled VM types. For workloads that require single tenancy for compliance and regulatory reasons, AWS also now provides Bare-Metal-as-a-Service. For virtualized workloads that might need it, AWS provides features such as placement groups to ensure that the workloads run on a designated underlying hardware.
AWS hopes that the various t-shirt VM sizes will match your workload requirements. It therefore doesn’t support creation of custom VM sizes (vCPU, RAM). Unlike other cloud providers (CSPs), it only provides a specific set of VM families which come with GPUs. It does not allow attaching GPUs to any or all VM types in its portfolio.
Block storage comes with a variety of options, such as dynamic resizing, different disk types (magnetic and SSD). Unlike other CSPs, AWS does not restrict IOPS by volume size. You can provision IOPS at extra charge to even small disks.
On the managed relational database front, AWS supports managed databases for MySQL, PostgreSQL, MariaDB, Oracle (both SE and EE) and MS SQL (Web and Enterprise editions) under their RDS offering. In addition, they have their own MySQL and PostgreSQL compatible database offering that boasts Oracle like performance at fraction of the cost. They are investing in this heavily and have also announced Multi-Master and Serverless versions.
For NoSQL databases, AWS has had their DynamoDB product available for over half a decade. This evolved from their SimpleDB offering. AWS is a proponent of and provides a range of purpose-built NoSQL databases. These include, DynamoDB (key value and document), Neptune (Graph), and Elasticache (key value caching).
AWS has improved its networking services portfolio over the last decade. It started with VPC and related network features such as security groups, network ACLs and Internet Gateways. At the time, users still had to configure their own NAT servers, bastion hosts etc. AWS has listened to customer feedback and gradually added these as managed network services to its portfolio. AWS now provides a managed NAT gateway, VPN Gateway, Transit Gateway, Direct Connect Gateway etc. They recently also announced a managed Client VPN service. This removes the need for customers to deploy OpenVPN servers to manage access to cloud VMs.
For network security, AWS has launched managed services for DDoS protection (AWS Shield) and Web Application Firewall (WAF), along with AWS Inspector, AWS Config and CloudTrail for inventory and policy management and auditing. GuardDuty provides threat detection.
For data security, AWS provides encryption at rest for most of its storage services. AWS also has KMS and CloudHSM services for key management. Macie provides an AI driven data loss prevention (DLP) service.
On the queueing, messaging and notification front, AWS provides managed AMQP compatible queue service (Amazon MQ) in addition to its SQS offering. For Pub/Sub, AWS has offered Kinesis and has recently added a managed Kafka offering. SNS provides a multi-channel integrated notification services that allows customer notifications over SMS, mobile, SMS and email notifications. Internally, it also connects with its other services to enable event driven loosely coupled architectures.
AWS serves US Government workloads in separate GovCloud regions in the US. Customers who need to provide services to customers in China can rely on AWS’s China region, which is provided via partnership with third party providers.
All in all AWS provides a breadth and depth of services and features that are suitable for a substantial number of enterprises.
Microsoft had lagged behind AWS in the public cloud game, but it focused first on SaaS and PaaS offerings as its strengths lie in both enterprise and consumer software. Microsoft initially focused on PaaS services for Azure. These were focused on their existing base of Microsoft developers. Over time, Microsoft expanded focus to both Linux and IaaS services. This also reflected in their re-branding Azure from Windows Azure to Microsoft Azure, and Microsoft loves Linux campaigns. Over time, Microsoft has also made Azure more startup friendly and built out API support for its various services. However, despite the breadth of its services, Microsoft lags substantially behind AWS in enterprise adoption. Large enterprises that already have existing Microsoft relationships remain a large part of the user base, though Azure is seeing robust growth in year-on-year revenue.
Azure is a mature cloud platform with a breadth of features which may be a preferred platform for customers that are already using Microsoft products in some way. While Azure supports a number of open source product based services, the Microsoft portfolio on cloud is what sets it apart for customers.
Azure has over 151 VM types over 26 VM families that support everything from small web workloads to the HPC, Oracle and SAP workloads. Azure has both Windows and multiple flavors of Linux (RHEL, CentOS, SUSE, Ubuntu). Azure has a separate family of instances for ML/AI workloads.
If you need to run high-end workloads that require up to 128 vCPU and 3.5 TB memory, Azure is a good bet. If you have existing licenses for windows OS, MS-SQL and bring them to cloud (BYOL) via Microsoft License Mobility Program, Azure is the cloud to choose. License costs form a substantial part of infrastructure expenses, and will be a major consideration for customers who run large deployments of MS-SQL etc.
Azure was also the first cloud player to recognize the trend towards hybrid cloud, and had one of the first hybrid cloud and Cloud-in-your-Datacenter offering (Azure Stack). Customers who wanted the interface of Azure but wanted to run services in their own data centers could use Azure Stack. Other cloud players are only catching up with Azure in this space. Azure also provided support for hybrid storage appliances like StorSimple, which was unique in the public cloud space.
If you have a data center with predominantly Microsoft workloads and need to do a large scale data center migration to cloud, while taking advantage of familiar tools, Azure provides tooling and services, such as Azure Site Recovery.
When it comes to SQL and NoSQL databases, Azure has a fairly well rounded set of services. It provides managed MS SQL Server and SQL Datawarehouse. Azure also provides managed databases for MySQL, PostgreSQL, and MariaDB. Azure Table is a managed key value store, whereas CosmosDB provides multi-model, globally distributed NoSQL database with multiple consistency models. It provides an API compatible with MongoDB, Cassandra, Gremlin (Graph) and Azure Table Storage. If you need to run multiple managed data models, including document, graph, key-value, table, and column-family data models in a single cloud, Cosmos may be the way to go. Azure cache for Redis rounds off the offerings with a managed cache.
In addition to PAYG billing model with credit card and invoicing modes, customers with existing enterprise accounts may pre-purchase Azure subscriptions as part of their annual renewals. This is useful for customers who want to be able to budget the annual cloud spend upfront. This prevents the uncertainty and additional mid-year budget approvals that are usually associated with PAYG models. When doing this, enterprises should size their projected workloads with some accuracy so that no pre-paid credits are wasted at the end of the year.
License mobility to cloud for Microsoft products is also relatively easy for customers with multiple Microsoft products running on-prem.
Google Cloud Platform (GCP), while late to the game and having the smallest market share of the public cloud providers compared here (current market share at about 4%), is showing a robust percentage growth. It boasts of several features that put it ahead of its competitors in certain areas. GCP is also riding the wave not only new customers who are already part of the ecosystem, but also early cloud adopters who are looking to expand their landscape to Google as part of a multi-cloud strategy. Google also started with PaaS services but has been steadily expanding its product portfolio.
Along with innovative features, Google boasts the lowest list price on infrastructure compared to all the other cloud providers. Of course, total expenditure for any enterprise depends on services used and cost governance measures in place.
From a compute perspective, Google has the smallest number of t-shirt VM sizes (28 instance types over 4 categories). However, it has one feature which makes these numbers slightly irrelevant. Google allows users to create their own custom sizes (CPU, memory) so that customers can match their cloud workloads sizing to their on-prem sizing. They also bill customers based on the total CPU and memory used, rather than individual VMs. This reduces wastage of unused capacity.
Another unique feature is that GCP allows almost all instance types to attach GPUs. This can turn any standard or custom instance into a ML ready VM. Google was also a leader in per-second billing, which forced other CSPs to follow suit. Compared to the pervious norm of per hour billing, per-second billing greatly reduces any capacity wastage. This results in up to a 40% savings overall, compared to relying on standard VM t-shirt sizes and per hour billing.
VM startup times in GCP are phenomenally fast, and leave other CSPs in the dust. This makes scaling out especially responsive. GCP also allows dynamic resizing of disks, so that you don’t have to do sysops acrobatics when your disks fill up. IOPS are assigned based on disk sizes and cannot be provisioned separately. This might be problematic for customers who want high IOPS on a small data set, and result on wasted dollars for unwanted storage.
Google has also tied up with or purchased third party cloud migration tools. These tools, such as CloudEndure, Velostrata and CloudPhysics, help customers assess, plan and live-migrate their VMs to GCP essentially for free. On other cloud providers, some of these tools cost several hundred dollars per VM. Google is clearly making migration to GCP as easy as possible.
Networking is where GCP shines. They have a global low latency network. Even from customer perspective, a VPC network spans all their regions. Other CSPs limit VPC networks to a region. This makes it easy for GCP customers to build applications that serve customers globally, without building complex cross region infrastructure design and data replication mechanisms.
Object storage also supports a multi-regional mode, where data is replicated across regions automatically. For customers considering a migration from AWS, or considering a multi-cloud strategy, GCP supports importing object storage from AWS.
For relational databases, GCP provides support for managed MySQL and PostgreSQL databases. For customers who want a globally distributed database that still supports immediate consistency and ACID properties, GCP has built Spanner. Spanner uses consensus algorithms and atomic clocks to synchronize transactions between nodes. This offering is unique to GCP and makes Spanner very attractive to large enterprise customers who have these requirements from their relational data store. In fact, another open source database, CockroachDB, is based on the Spanner paper that was published by Google.
From a NoSQL perspective, GCP has a product called BigTable. BigTable is a petabyte scale, managed wide-column NoSQL database that is used by Google in its own products such as Gmail.
From a billing perspective, Google provides automatic discounts such as sustained use discounts which reduce the on-demand price if a VM runs more than a certain number of hours in a month. If you want the most cost effective cloud provider in the market today, GCP is a good choice.
While it may not have the sheer depth of features of some of the other CSPs, it has some unique products in its portfolio, and is an attractive option being a price leader in the market.
As detailed above, each cloud has features and advantages that appeal to specific customer needs. While all the cloud providers will continue to provide certain common services (such as managed MySQL database), each CSP will continue to build out unique, differentiated services (e.g. Aurora, Cosmos, Spanner) that are purpose built to solve very specific customer needs. CSPs hope that this will increase customer stickiness and create a lock in.
From a customer perspective, these services will also become a driver to adopt a multi-cloud strategy. As an example, a customer might likely want to use GCP for one app that needs Spanner’s features, while they use AWS for their AI services, and Azure for specific Windows workloads.
Even for future looking services like computer vision and speech recognition, customer needs might drive them to mix and match services across cloud platforms, to meet their application’s requirements. Customer will likely use one cloud as their primary platform, while using services from others for specific applications.
This blog is part of our ongoing cloud series. To find out how GlobalLogic can help in your cloud adoption journey, please reach out to us at cloud@globallogic.com.
This blog contains articles, nuggets and pearls of wisdom…
1.3K 
10
1.3K claps
1.3K 
10
This blog contains articles, nuggets and pearls of wisdom from GlobalLogic Cloud and DevOps Practice
Written by
Cloud and DevOps Lead — APAC at GlobalLogic
This blog contains articles, nuggets and pearls of wisdom from GlobalLogic Cloud and DevOps Practice
"
https://servian.dev/azure-az-900-exam-preparation-guide-how-to-pass-in-3-days-dabf5534507a?source=search_post---------1,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
Top highlight
Perth Ngarmtrakulchol
Aug 28, 2019·10 min read
AZ-900 is the exam that test on foundational level knowledge of cloud concepts and Azure services.
Those who have experiences in other major cloud providers might find the most of AZ-900 materials to be very similar to they have learned.
I found the content of this exam to be very beneficial for anyone who would like to break in cloud computing on Azure.
This exam guide will give you all the information you need to pass the exam in 3 days or less.
Most of the exam information is on Microsoft website. However, before going into the exam, I was researching how long is this exam. The Microsoft website does not show how many questions and how long the exam is.
So I gathered these numbers based on my exam:
What I like about this exam is: Your result sheet is printed right after the exam, so you can see which area you are doing well and which area you need to work on.
This is the same study plan I used to pass AZ-900 exam:
One day means 6–8 hours of work. So it might take around a week to study 2–3 hours per day after work.
I studied from 3 resources and spent around $15 for the preparation (excluding $99 for AZ-900 exam itself).
The 3 resources are:
[Free] Microsoft Learn platform (11 modules)
Microsoft provides a free learning platform for everything you need to pass any Azure certificate.
Unlike other learning platforms such as Google Cloud’s Coursera or Udemy, Microsoft Learn platform is 90% text with few short videos here and there. However, the material is great quality and the learning platform is easy to use.
I found that by having learning material in text, I can take note much faster by copy-paste the text directly.
This course took me 9–10 hours including taking notes and research on random technical words e.g. N-tier architecture.
[$15] Udemy course “Microsoft Azure Beginners Guide”
This Udemy video course contains all you need to know to pass AZ-900 exam. The content is more hands-on than Microsoft Learn since you will see the instructor showing you inside Azure Portal, whereas Microsoft Learn only teach you in text.
The beginners in Azure will learn a lot more from seeing real Azure portal than reading.
For me, I had some experience playing around Azure portal before. So I skipped to the last section: AZ-900 Preparation. This section provides 70 questions x 2 practice tests.
I found the practice questions to be very similar to Whizlabs, the last resource I used for exam preparation. I have heard from my colleague who passed AZ-900 only using this course.
My recommendation is you could study from videos in this course or from Microsoft Learn. The content should be very similar.
At my company Servian, we provide Udemy Business subscription for staffs. So I can access this course for free.
[$15] Whizlabs AZ-900 Exam Practices Tests
Whizlabs is the popular website containing a lot of IT exam practice tests. Note that Microsoft also offers Official Practice Test but at the significantly higher cost and a similar number of practice questions. So I went for Whizlabs this time.
At the time of writing, Whizlabs has 3 practice tests with 55 questions each. The questions are surprisingly similar to the real exam, so you can get a feeling of what kind of questions you would get examined on.
This section is based on the list from Microsoft website which is quite accurate to the exam.
There are 4 main parts. For each part, I am giving summarised information here to speed up your learning or you can also use them as revision material.
Part 1) Cloud Concept
This part contains general knowledge about cloud computing e.g. what are the benefits of cloud, the differences types of cloud service offering, and the differences in cloud deployment models.
Technological benefits of cloud:
Business benefits of cloud:
Types of cloud service offering:
Differences in cloud deployment model:
Part 2) Core Azure Services
This part contains the introduction to different services offering in Azure for each service category:
Part 3) Security, Privacy, Compliance, and Trust in Azure
This part contains the security Azure provided for their services as well as Azure’s commitment in privacy and regulatory compliance.
Azure services for security in different areas:
Part 4) Azure Pricing and Support
This part contains the information about different types of Azure subscription, cost factors of Azure services, tools for cost calculation, and the support plans in Azure.
Azure subscription types:
Cost Factors of Azure services:
Tools for cost calculation for Azure
Service Level Agreement (SLA)
Public Preview vs Private Preview features
What you will learn from the recommended courses listed above as well as the summary in this article will make you pass the exam with flying colors.
For the next step, the knowledge in AZ-900 exam is crucial for any Azure role-based certification in the future. Whether you would like to become Azure Administrator or Azure Data Engineer or any other roles, AZ-900 will be the great starting point for your career.
Wish everyone all the best for your exam!
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help our customers sustain competitive advantage. If you need any help from us in these areas, feel free to ask!
Data Consultant at Servian | Monash Data Science Alumni | Front-end Developer based in Melbourne, Australia | LinkedIn: http://bit.ly/lkdn-perth
See all (779)
1.2K 
27
1.2K claps
1.2K 
27
At Servian, we design, deliver and manage innovative data & analytics, digital, customer engagement and cloud solutions that help you sustain competitive advantage.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-i-won-the-azure-machine-learning-award-418ff35c6e4d?source=search_post---------2,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Every year, Microsoft hosts the Imagine Cup. Young developers often call it the “Olympics of Technology” and consider it one of the top competitions related to software design. As a result, it attracts lots of young participants from around the world, who collaborate to solve some of the world’s toughest challenges.
In 2016, they hosted the Hello Cloud Machine Learning Award, where winners were selected from all entries based on quality, creativity and the effectiveness of their use of Azure Machine Learning Studio.
I jumped into the competition to learn more about machine learning and ended up as one of the winners of the challenge. What got my interested in the first place was the focus of the competition: to build creative and inventive systems using machine learning.
When a user stops using a good or service (in this case, a game), we call that “churn.” Based on past player history, or data for similar players over time, we can create a machine learning model to predict when a player is most likely to quit.
In the first part of the competition we had to build, train, score, and evaluate a model to do just that in the Azure ML Studio. We then had take a basic game provided by them, connect it to the Azure ML service, and publish it to the web.
One of the key factors that makes a player abandon a game is its difficulty level. If it's too easy the game gets boring, and if it's too hard it demotivates the user to keep playing.
I decided to use the rock-paper-scissors dynamics in the game. To earn new superpowers in the game, the players (math students) would have to solve some math equations (these were used like attacks in a fighting game).
Based on the data for each player, we could adjust the difficulty of the math equations to keep them motivated to play the game. We could also identify what kind of equations were causing kids the more trouble (subtraction? multiplication?). This is an amazing opportunity to help teachers and everyone involved in the learning process.
One thing I know for sure in competitions is this: focus on being different instead of just focusing on being better. We don't know the number of competitors for sure, but based on what we heard there in Seattle, the contest had almost 1,000 entries. That's a lot of games for the judges to evaluate. You have to do everything you can to stand out from a crowd that size.
I bet that when you read the word “different” your first thought was “Great, now I have to come up with something big and strange out of nowhere.” Don’t worry — that isn’t the case. Because here’s another thing I know for sure: to be different, you can just focus on being yourself. I know it sounds cheesy, but let's elaborate on that.
“Be yourself; everyone else is already taken.” ― Oscar Wilde
You are you, right? No one else in the world have had the experience you had, did everything you did or felt exactly everything you felt. That's it, you just have to use this to be different (and original). Now let's go back to what I did in the contest.
I bought my first Wacom tablet in time for the competition, and honestly I was just looking for excuses to use it. I like to get adventurous in other areas, and I know that this is something that differentiates me. So I decided to work and change the assets of the game.
In the competition we first had to follow a tutorial. Only after that could we begin to create our own version of the game. This is a great way to design the workflow of our projects (and side projects): always find a way to make the start phase easy.
I first heard this advice in the book Think Like a Programmer. It’s true for programming, but it’s also true for a bunch of other aspects of our lives.
Once you have divided the problem up into pieces, for example, go ahead and complete any pieces you already know how to code. Having a working partial solution may speak ideas about the rest of the problem. Also, as you may have noticed, a common theme in problem solving is making useful progress to build confidence that you will ultimately complete the task. By starting with what you know, you build confidence and momentum toward the goal. ― V. Anton Spraul, Think Like a Programmer
Let's be honest: programming is hard. During the competition I had some moments of frustration, but things like seeing my first predictive model running and seeing the pieces of the game start to work together got me going. Make sure you can start to see progress in a project, right from the beginning.
This was the key factor for winning the competition, because without it, my entry wouldn't even have been submitted. Time is a limited source. Everybody knows this, but it’s something that we always have to keep reminding ourselves of — especially us programmers.
If you take a look at my Game Design Document above, you can see that my first idea for the game had a lot of features. For example, we had levels for the players, items they would be able to collect, healing effects, and so on. As the deadline approached, I realized that I didn't have time to execute all of those ideas. So I had to think: what is the one thing that I should have in the game to make sure that it would accomplish my goal? The answer was math equations and the rock-paper-scissors dynamics, and that was what I implemented.
It's not easy to give instructions about how to adapt, because every situation is a different. But you should know that you will have to make choices along the way. Your main focus should be to finish the thing on time, so you can indeed compete in the contest.
Well, as you may know by now, my project was one of the two winners of the challenge (yay!). I won a trip to the Imagine Cup World Finals and had mentoring sessions with members of the Microsoft Data Platform product.
With this project I finally found my main career goal: to design Machine Learning systems that let humans do things they care about.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
459 
6

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
459 claps
459 
6
Written by
Award-winning Data Scientist 👩🏾‍💻 Loves to write and explain things in different ways✨ - http://deborahmesquita.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Award-winning Data Scientist 👩🏾‍💻 Loves to write and explain things in different ways✨ - http://deborahmesquita.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/deep-ai/study-guide-for-microsoft-azure-data-scientist-associate-certification-dp-100-c2e4611cb071?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
Update 22/4/2020:
Updated 17/5/2019:
Microsoft recently released a certification named DP-100: Designing and Implementing a Data Science Solution on Azure for the title of Microsoft Certified: Azure Data Scientist Associate. This certification is for a data scientist to check his knowledge and solutioning skills on Azure. This certification checks for both the width and depth of data science knowledge. Despite being the first one in the series this certification is a tough one to crack, buts that's where I can help.
Read about the certification on Artificial Intelligence here: Study Guide for Microsoft Azure AI-100: Designing and Implementing an Azure AI Solution (Beta)
Total time 220 minutes, with actually 180 minutes for 60 questions.
Below pointers are based totally on my certification experience.
Below are the type of questions:
Only 30–40 % questions were specific to Machine learning on Azure
Here is a comprehensive list of study material covering DP-100 scope & questions, you can thank me later.
If you need further help or have a question then write in the comments below or find me on LinkedIn. Also, do let me know about any changes in questions as the examination is still in beta, so questions or pattern might change. Thanks.
If you have any comment or question, then do write them below.
To see a similar post, follow me on Medium & LinkedIn.
If you enjoyed then Clap it! Share it!! Thanks!!!
Towards AI: Read, Learn, Apply !!
825 
11
825 claps
825 
11
Written by
MCT | MCSE: Azure | MCSA: Machine Learning | Blockchain| R, Architect/Consultant/Trainer. I love working with cutting-edge technologies.
Towards AI: Read, Learn, Apply !!
Written by
MCT | MCSE: Azure | MCSA: Machine Learning | Blockchain| R, Architect/Consultant/Trainer. I love working with cutting-edge technologies.
Towards AI: Read, Learn, Apply !!
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/azure-functions-choosing-between-queues-and-event-hubs-dac4157eee1c?source=search_post---------4,"There are currently no responses for this story.
Be the first to respond.
Top highlight
I have this conversation about twice a week. Someone has decided they want to take advantage of the benefits of serverless Azure Functions for an upcoming project, but when starting to lay out the architecture a question pops up:
“Should we be using Azure Event Hubs, Queues, or Event Grid?”
It’s honestly a great question — and it’s a question with consequences. Each of these messaging technologies comes with its own set of behaviors that can impact your solution. In previous blogs, I’ve spent some time explaining Event Grid, Event Hubs ordering guarantees, how to have ordering guarantees in queues / topics, and how to keep Event Hub stream processes resilient. In this blog, I specifically want to tackle one other important aspect: throughput for variable workloads. Here was the scenario I was faced with this last week:
We have a solution where we drop in some thousands of records to get processed. Some records only take a few milliseconds to process, and others may take a few minutes. Right now we are pushing the messages to functions via Event Hubs and noticed some significant delays in getting all messages processed.
The problem here is subtle, but simple if you understand the behavior of each trigger. If you just have a pile of tasks that need to be completed, you likely want a queue. It’s not a perfect rule, but more often than not queues may have the behavior you are looking for. In fact, this blog will show that choosing the wrong messaging pipeline can result in an order of magnitude difference in processing time. Let’s take a look at a few reasons why.
If you look at the problem posed by the customer above, one key element let me know queues could be best here.
“…Some records only take a few milliseconds to process, and others may take a few minutes.”
If I have a queue of images that need to be resized, a 100kb image will resize a lot quicker than a 100mb panoramic image. The big question is: “How does my processing pipeline respond to a long task?” It’s a bit like driving on a freeway. An incident on the road is much more impactful for a one lane road than a five lane road — and the same is true for your serverless pipelines.
Azure Event Hubs cares about two things, and it does a great job with those two things: ordering and throughput. It can receive and send a huge heap of messages very quickly and will preserve order for those messages. Our team has posted previously on how Azure Event Hubs allowed us to process 100k messages a second in Azure Functions. The fact that I can consume a batch of up to 200 messages in every single function execution is a big reason for that. But those two core concerns of ordering and throughput can also get in each others way.
> If you’re not already familiar with how Event Hubs preserves order, it may be worth reading this blog on ordering first and coming back.
Imagine I have a series of events that all land in the same Event Hub partition. Let’s say my Azure Function pulls in a batch of 50 events from that partition into a function. Messages 1–9 process in a few milliseconds, but message 10 is taking a few minutes to process. What happens to events 11–50? They are stuck. Event Hubs cares about ordering, so message 10 needs to complete before message 11 starts. Imagine a scenario where the events you are receiving are stock market events — you very much do care that a “buy” happened before a “sell” event, so you’d be glad it’s holding the line. But what if your scenario doesn’t care about ordering? What if events 11–50, or even 51–1,000 could be running while we wait for message 10 to complete? In this case Event Hubs behavior is going to slow down your performance dramatically. Event Hubs will not process the next batch in a partition until the current batch, and every event in that batch, has completed. So one bad event can hold up the entire partition.
What about queues? Standard queues don’t care as much about ordering¹ (session-enabled queues do). Queues care more about making sure that every message gets processed and gets processed completely. Head into any government building and you will usually witness distributed queue processing in action. You have a single line of people that need something done, and a few desks of employees to help (hopefully more than one 😄). While some employees may have requests that take a number of minutes, the other employees will keep grabbing the next person in line as soon as they are available. One long request isn’t going to stop all work from continuing. Queue triggers will be the same. Whichever instance of your app is available to take on more work will grab the next task in line. No task depends on the completion of another task in that queue².
As if deciding between Event Hubs, Event Grid, and queues wasn’t hard enough, there’s a sub-decision on storage queues vs service bus queues. I’m not going to go very deep into this here. There’s a detailed doc that will lay out the big differences. In short, Service Bus is an enterprise-grade message broker with some powerful features like sessions, managed dead-letter queues, and custom policies. Storage queues are super simple, super lightweight queues as part of your storage account. I often stick with storage queues because my Azure Function already has a storage account, but if you need anything more transactional and enterprise grade (or topics), Service Bus is absolutely what you want.
To illustrate the difference in behavior for variable workloads I ran a few tests. Here’s the experiment.
I put 1,000 identical messages in an Azure Event Hub, Azure Service Bus queue, and an Azure Storage queue. 90% of these messages would only take one second to process, but 10% (sprinkled throughout) would take ten seconds. I had three mostly identical JavaScript functions that I would start processing on those messages. The question is: which one would process fastest?
Shouldn’t be much of a surprise given the explanation above, but Event Hubs took roughly 8x longer than queues to process all 1,000 messages. So what happened? It actually processed about 90% of the messages by the time the queue function apps finished, but that last 10% had a very long tail. Turns out one instance got unlucky with its assigned partitions and had about 40 of those ten-second tasks. It was stuck waiting for long tasks to complete before moving on to the next set of events, which likely contained another ten-second task. The forced sequential processing for the final 10% was significant.
Storage queues and Service Bus queues were extremely close in terms of overall time to process for this experiment (within a few seconds). There is one subtle difference I want to call out here though. Behind-the-scenes in serverless there are instances, or worker nodes (you could even call them… servers), processing your work. While we handle the scale out for you, Azure Functions gives you the ability to process multiple messages on a single node at one time. This is actually super helpful in a number of cases as you can have much better connection pooling, cache sharing, and resource utilization. For both service bus queues and storage queues, the Functions runtime will pull down a batch of messages and process them in parallel on each running instance of your app. The default concurrency for both kinds of queue triggers is 16 messages. In my case my functions scaled out to many instances during the short test, so my total concurrency was higher than 16 messages, but each instance was processing sets of 16.
Why this matters is storage and service bus queues handle the batch slightly different. The big distinction point on: “How many messages have to be processed before the next message, or batch, will be retrieved.”
For Azure Storage queues there is a “new batch threshold” that has to be crossed before the next batch is retrieved. By default, this is half of the batch size, or 8. What that means is the host will grab 16 messages and start processing them all concurrently. Once there are <= 8 messages in that batch left, the host will go grab the next set of 16 messages and start processing (until the to-be-completed count gets to<= 8 messages again). In my case, since only 10% of messages were slow, this threshold could generally be trusted to keep instance concurrency high. But you can still see the little bursts of batch thresholds in Application Insights analytics. The sharp jumps correlate to the processing batch size and when new batches are retrieved.
Azure Service Bus queues rely on the MessageHandler provided by the Service Bus SDK to pull new messages. The message handler has an option for max concurrency (by default set to 16 for functions), and can automatically fetch more messages as needed. In my experiment you can see a much smoother rate of messages being concurrently processed. If 1 slow message was holding up one of the 16 concurrent executions, the handler could still keep cycling through messages on the other 15.
So as expected, queues of both flavors performed much better when your problem set is “I have a bunch of variable tasks that need to be executed in any order.”
Why wasn’t Event Grid included in this? There is a big difference in Event Grid’s throughput behavior than these other options. Event Grid is push based (via HTTPS) and these are pull. If I get 1000 Event Grid events, my functions get poked by 1000 concurrent HTTPS requests. They’ll have to scale and queue and handle them as quickly as possible. I would expect since ordering isn’t guaranteed for Event Grid it would perform closer to queues. That said, for this customer specifically they had some throughput considerations of downstream systems, so they wanted the flexibility to pull these messages at a more consistent click which Event Grid cannot provide (they’ll poke the function whether it wants more or not).
The only other note I’ll make in regards to Event Grid is it is meant for processing events, not messages. For details on that distinction check out this doc.
Hopefully, this experiment helps clarify that messaging options are not one-size-fits-all. Each comes with its own set of priorities and strengths, and it’s important to understand each when creating serverless architectures.
#BlackLivesMatter
1K 
7
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1K claps
1K 
7
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/aws-vs-azure-vs-firebase-vs-heroku-vs-netlify-how-to-choose-the-best-platform-for-web-projects-482d017de254?source=search_post---------5,"Sign in
You have 1 free member-only story left this month. Sign up for Medium and get an extra one
Ali Kamalizade
Aug 2, 2019·6 min read
Web and mobile development has come a long way in recent years. Modern web applications are often built based on powerful JavaScript features like Angular, React, and Vue.js. While you could host those web applications anywhere, you may need more than only hosting. Different big cloud companies like Google, Amazon, and Microsoft are offering practically anything you can ask for, while upcoming competitors like Netlify want to provide an impressive UX for building modern websites.
In this article, I want to focus on web-based projects. We will look at the following platforms:
Keep in mind that this is just a short look into this topic. There are many factors to consider depending on your project size and needs, such as:
Let’s get started and have a look at what these platforms can do for us.
Amazon Web Services has been in the cloud computing market for quite some time. No matter what you might need: AWS probably offers it. AWS offers many products with cool names like:
Netflix, Unilever, and Samsung use AWS. You can get one free year, which should be plenty of time to get a grasp. However, since AWS has grown a lot over the years, the usage is not always intuitive.
AWS fulfils ISO 27001 and SOC2
Microsoft Azure is one of the top cloud computing platforms. Similar to AWS, Azure has everything you will need. Microsoft offers special programs for startups which provide free limited access to Azure services. Azure offers services like:
Leading companies like Adobe, BMW, and HP rely on Azure. Microsoft has a very good standing in B2B, which is why many large corporations prefer Microsoft Azure. Similar to Amazon, the Azure UI is rather complex, which can be challenging for new users.
Firebase is a development platform owned by Google. Technically, Firebase is using the Google Cloud Platform. PayPal, Twitter, and Target are customers of Google Cloud. Thanks to the experience of Google, Firebase offers many useful services like:
Many developers love Firebase because of its strong tooling and its powerful Google infrastructure. However, especially due to GDPR and increasing strict data protection policies, some companies want to avoid using Google infrastructure.
Being a Google product has its upsides and downsides which applies to Firebase as well. Even though Firebase seems to perform quite well, we all know what happens to Google products which don’t live up to expectations.
Heroku is a Platform as a Service owned by Salesforce, an American cloud-based software company which is mostly known because of their CRM solutions. The free version of Heroku is fine for experimenting, but the server will sleep after some time of inactivity. There are lots of free and paid add-ons which provide additional functionality like:
While Heroku used to be rather unique when it started, it has slowed down when it comes to innovation and competition has caught up.
Heroku supports most popular languages like Java, Python, and JavaScript. Also, Heroku provides a CLI which you can use to deploy with one command. Citrix, Toyota, and Unsplash are known to use Heroku.
Fun fact: Heroku’s physical infrastructure and managed within Amazon’s secure data centers and utilize the AWS technology.
Netlify is a rather new contender. The free version is already quite generous and there is no sleeping, unlike with Heroku’s free version. The UX and the features Netlify provides make working with it seamless and intuitive. Some of the powerful add-ons Netlify provides are:
The downside is that cloud providers like Microsoft and Amazon offer way more functionality beyond web projects. Besides, you also cannot use other programming languages like Java or C# since Netlify promotes the usage of JAMstack. However, you can use Functions as an alternative to server-side languages like Java or C#.
Companies like WeWork, Verizon, and Nike are users of Netlify. Also, some popular open-source projects like Vue.js and Kubernetes have decided to use Netlify.
I have previously written an article about how to deploy an Angular app on Netlify:
itnext.io
As you can see, there are lots of options to choose from.
As I said, there are lots of important questions you need to ask yourself when choosing a platform for web projects. Some aspects might be more important than others depending on your situation: if you’re a startup you’re more looking for a easy and cheap solution while bigger projects and companies often require more sophisticated features and security standards.
Which platform are you using? Let me know in the comments about your experiences.
Senior Software Engineer @LeanIX. Co-founder of Sedeo. Passion for software engineering and startups. Looking forward to build great things. 有難うございます。🚀
1K 
6
1K claps
1K 
6
Advice for programmers. Here’s why you should subscribe: https://bit.ly/bp-subscribe
"
https://medium.com/hackernoon/making-sense-of-azure-durable-functions-645ecb3c1d58?source=search_post---------6,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Stateful Workflows on top of Stateless Serverless Cloud Functions — this is the essence of the Azure Durable Functions library. That’s a lot of fancy words in one sentence, and they might be hard for the majority of readers to understand.
Please join me on the journey where I’ll try to explain how those buzzwords fit together. I will do this in 3 steps:
Traditionally, server-side applications were built in a style which is now referred to as Monolith. If multiple people and teams were developing parts of the same application, they mostly contributed to the same code base. If the code base were structured well, it would have some distinct modules or components, and a single team would typically own each module:
Usually, the modules would be packaged together at build time and then deployed as a single unit, so a lot of communication between modules would stay inside the OS process.
Although the modules could stay loosely coupled over time, the coupling almost always occurred on the level of the data store because all teams would use a single centralized database.
This model works great for small- to medium-size applications, but it turns out that teams start getting in each other’s way as the application grows since synchronization of contributions takes more and more effort.
As a complex but viable alternative, the industry came up with a revised service-oriented approach commonly called Microservices. The teams split the big application into “vertical slices” structured around the distinct business capabilities:
Each team then owns a whole vertical — from public communication contracts, or even UIs, down to the data storage. Explicitly shared databases are strongly discouraged. Services talk to each other via documented and versioned public contracts.
If the borders for the split were selected well — and that’s the most tricky part — the contracts stay stable over time, and thin enough to avoid too much chattiness. This gives each team enough autonomy to innovate at their best pace and to make independent technical decisions.
One of the drawbacks of microservices is the change in deployment model. The services are now deployed to separate servers connected via a network:
Networks are fundamentally unreliable: they work just fine most of the time, but when they fail, they fail in all kinds of unpredictable and least desirable manners. There are books written on the topic of distributed systems architecture. TL;DR: it’s hard.
A lot of the new adopters of microservices tend to ignore such complications. REST over HTTP(S) is the dominant style of connecting microservices. Like any other synchronous communication protocol, it makes the system brittle.
Consider what happens when one service becomes temporary unhealthy: maybe its database goes offline, or it’s struggling to keep up with the request load, or a new version of the service is being deployed. All the requests to the problematic service start failing — or worse — become very slow. The dependent service waits for the response, and thus blocks all incoming requests of its own. The error propagates upstream very quickly causing cascading failures all over the place:
The application is down. Everybody screams and starts the blame war.
While cascading failures of HTTP communication can be mitigated with patterns like a circuit breaker and graceful degradation, a better solution is to switch to the asynchronous style of communication as the default. Some kind of persistent queueing service is used as an intermediary.
The style of application architecture which is based on sending events between services is known as Event-Driven. When a service does something useful, it publishes an event — a record about the fact which happened to its business domain. Another service listens to the published events and executes its own duty in response to those facts:
The service that produces events might not know about the consumers. New event subscribers can be introduced over time. This works better in theory than in practice, but the services tend to get coupled less.
More importantly, if one service is down, other services don’t catch fire immediately. The upstream services keep publishing the events, which build up in the queue but can be stored safely for hours or days. The downstream services might not be doing anything useful for this particular flow, but it can stay healthy otherwise.
However, another potential issue comes hand-in-hand with loose coupling: low cohesion. As Martin Fowler notices in his essay What do you mean by “Event-Driven”:
It’s very easy to make nicely decoupled systems with event notification, without realizing that you’re losing sight of the larger-scale flow.
Given many components that publish and subscribe to a large number of event types, it’s easy to stop seeing the forest for the trees. Combinations of events usually constitute gradual workflows executed in time. A workflow is more than the sum of its parts, and understanding of the high-level flow is paramount to controlling the system behavior.
Hold this thought for a minute; we’ll get back to it later. Now it’s time to talk cloud.
The birth of public cloud changed the way we architect applications. It made many things much more straightforward: provisioning of new resources in minutes instead of months, scaling elastically based on demand, and resiliency and disaster recovery at the global scale.
It made other things more complicated. Here is the picture of the global Azure network:
There are good reasons to deploy applications to more than one geographical location: among others, to reduce network latency by staying close to the customer, and to achieve resilience through geographical redundancy. Public Cloud is the ultimate distributed system. As you remember, distributed systems are hard.
There’s more to that. Each cloud provider has dozens and dozens of managed services, which is the curse and the blessing. Specialized services are great to provide off-the-shelf solutions to common complex problems. On the flip side, each service has distinct properties regarding consistency, resiliency and fault tolerance.
In my opinion, at this point developers have to embrace the public cloud and apply the distributed system design on top of it. If you agree, there is an excellent way to approach it.
The slightly provocative term serverless is used to describe cloud services that do not require provisioning of VMs, instances, workers, or any other fixed capacity to run custom applications on top of them. Resources are allocated dynamically and transparently, and the cost is based on their actual consumption, rather than on pre-purchased capacity.
Serverless is more about operational and economical properties of the system than about the technology per se. Servers do exist, but they are someone else’s concern. You don’t manage the uptime of serverless applications: the cloud provider does.
On top of that, you pay for what you use, similar to the consumption of other commodity resources like electricity. Instead of buying a generator to power up your house, you just purchase energy from the power company. You lose some control (e.g., no way to select the voltage), but this is fine in most cases. The great benefit is no need to buy and maintain the hardware.
Serverless compute does the same: it supplies standard services on a pay-per-use basis.
If we talk more specifically about Function-as-a-Service offerings like Azure Functions, they provide a standard model to run small pieces of code in the cloud. You zip up the code or binaries and send it to Azure; Microsoft takes care of all the hardware and software required to run it. The infrastructure automatically scales up or down based on demand, and you pay per request, CPU time and memory that the application consumed. No usage — no bill.
However, there’s always a “but”. FaaS services come with an opinionated development model that applications have to follow:
Frankly speaking, the majority of existing applications don’t really fit into this model. If you are lucky to work on a new application (or a new module of it), you are in better shape.
A lot of the serverless applications may be designed to look somewhat similar to this example from the Serverless360 blog:
There are 9 managed Azure services working together in this app. Most of them have a unique purpose, but the services are all glued together with Azure Functions. An image is uploaded to Blob Storage, an Azure Function calls Vision API to recognize the license plate and send the result to Event Grid, another Azure Function puts that event to Cosmos DB, and so on.
This style of cloud applications is sometimes referred to as Serviceful to emphasize the heavy usage of managed services “glued” together by serverless functions.
Creating a comparable application without any managed services would be a much harder task, even more so, if the application has to run at scale. Moreover, there’s no way to keep the pay-as-you-go pricing model in the self-service world.
The application pictured above is still pretty straightforward. The processes in enterprise applications are often much more sophisticated.
Remember the quote from Martin Fowler about losing sight of the large-scale flow. That was true for microservices, but it’s even more true for the “nanoservices” of cloud functions.
I want to dive deeper and give you several examples of related problems.
For the rest of the article, I’ll define an imaginary business application for booking trips to software conferences. In order to go to a conference, I need to buy tickets to the conference itself, purchase the flights, and book a room at a hotel.
In this scenario, it makes sense to create three Azure Functions, each one responsible for one step of the booking process. As we prefer message passing, each Function emits an event which the next function can listen for:
This approach works, however, problems do exist.
As we need to execute the whole booking process in sequence, the Azure Functions are wired one after another by configuring the output of one function to match with the event source of the downstream function.
In the picture above, the functions’ sequence is hard-defined. If we were to swap the order of booking the flights and reserving the hotel, that would require a code change — at least of the input/output wiring definitions, but probably also the functions’ parameter types.
In this case, are the functions really decoupled?
What happens if the Book Flight function becomes unhealthy, perhaps due to the outage of the third-party flight-booking service? Well, that’s why we use asynchronous messaging: after the function execution fails, the message returns to the queue and is picked up again by another execution.
However, such retries happen almost immediately for most event sources. This might not be what we want: an exponential back-off policy could be a smarter idea. At this point, the retry logic becomes stateful: the next attempt should “know” the history of previous attempts to make a decision about retry timing.
There are more advanced error-handling patterns too. If executions failures are not intermittent, we may decide to cancel the whole process and run compensating actions against the already completed steps.
An example of this is a fallback action: if the flight is not possible (e.g., no routes for this origin-destination combination), the flow could choose to book a train instead:
This scenario is not trivial to implement with stateless functions. We could wait until a message goes to the dead-letter queue and then route it from there, but this is brittle and not expressive enough.
Sometimes the business process doesn’t have to be sequential. In our reservation scenario, there might be no difference whether we book a flight before a hotel or vice versa. It could be desirable to run those actions in parallel.
Parallel execution of actions is easy with the pub-sub capabilities of an event bus: both functions should subscribe to the same event and act on it independently.
The problem comes when we need to reconcile the outcomes of parallel actions, e.g., calculate the final price for expense reporting purposes:
There is no way to implement the Report Expenses block as a single Azure Function: functions can’t be triggered by two events, let alone correlate two related events.
The solution would probably include two functions, one per event, and the shared storage between them to pass information about the first completed booking to the one who completes last. All this wiring has to be implemented in custom code. The complexity grows if more than two functions need to run in parallel.
Also, don’t forget the edge cases. What if one of the function fails? How do you make sure there is no race condition when writing and reading to/from the shared storage?
All these examples give us a hint that we need an additional tool to organize low-level single-purpose independent functions into high-level workflows.
Such a tool can be called an Orchestrator because its sole mission is to delegate work to stateless actions while maintaining the big picture and history of the flow.
Azure Durable Functions aims to provide such a tool.
Azure Functions is the serverless compute service from Microsoft. Functions are event-driven: each function defines a trigger — the exact definition of the event source, for instance, the name of a storage queue.
Azure Functions can be programmed in several languages. A basic Function with a Storage Queue trigger implemented in C# would look like this:
The FunctionName attribute exposes the C# static method as an Azure Function named MyFirstFunction. The QueueTrigger attribute defines the name of the storage queue to listen to. The function body logs the information about the incoming message.
Durable Functions is a library that brings workflow orchestration abstractions to Azure Functions. It introduces a number of idioms and tools to define stateful, potentially long-running operations, and manages a lot of mechanics of reliable communication and state management behind the scenes.
The library records the history of all actions in Azure Storage services, enabling durability and resilience to failures.
Durable Functions is open source, Microsoft accepts external contributions, and the community is quite active.
Currently, you can write Durable Functions in 3 programming languages: C#, F#, and Javascript (Node.js). All my examples are going to be in C#. For Javascript, check this quickstart and these samples. For F# see the samples, the F#-specific library and my article A Fairy Tale of F# and Durable Functions.
Workflow building functionality is achieved by the introduction of two additional types of triggers: Activity Functions and Orchestrator Functions.
Activity Functions are simple stateless single-purpose building blocks that do just one task and have no awareness of the bigger workflow. A new trigger type, ActivityTrigger, was introduced to expose functions as workflow steps, as I explain below.
Here is a simple Activity Function implemented in C#:
It has a common FunctionName attribute to expose the C# static method as an Azure Function named BookConference. The name is important because it is used to invoke the activity from orchestrators.
The ActivityTrigger attribute defines the trigger type and points to the input parameter conference which the activity expects to get for each invocation.
The function can return a result of any serializable type; my sample function returns a simple property bag called ConfTicket.
Activity Functions can do pretty much anything: call other services, load and save data from/to databases, and use any .NET libraries.
The Orchestrator Function is a unique concept introduced by Durable Functions. Its sole purpose is to manage the flow of execution and data among several activity functions.
Its most basic form chains multiple independent activities into a single sequential workflow.
Let’s start with an example which books a conference ticket, a flight itinerary, and a hotel room one-by-one:
The implementation of this workflow is defined by another C# Azure Function, this time with OrchestrationTrigger:
Again, attributes are used to describe the function for the Azure runtime.
The only input parameter has type DurableOrchestrationContext. This context is the tool that enables the orchestration operations.
In particular, the CallActivityAsync method is used three times to invoke three activities one after the other. The method body looks very typical for any C# code working with a Task-based API. However, the behavior is entirely different. Let's have a look at the implementation details.
Let’s walk through the lifecycle of one execution of the sequential workflow above.
When the orchestrator starts running, the first CallActivityAsync invocation is made to book the conference ticket. What actually happens here is that a queue message is sent from the orchestrator to the activity function.
The corresponding activity function gets triggered by the queue message. It does its job (books the ticket) and returns the result. The activity function serializes the result and sends it as a queue message back to the orchestrator:
When the message arrives, the orchestrator gets triggered again and can proceed to the second activity. The cycle repeats — a message gets sent to Book Flight activity, it gets triggered, does its job, and sends a message back to the orchestrator. The same message flow happens for the third call.
As discussed earlier, message passing is intended to decouple the sender and receiver in time. For every message in the scenario above, no immediate response is expected.
On the C# level, when the await operator is executed, the code doesn't block the execution of the whole orchestrator. Instead, it just quits: the orchestrator stops being active and its current step completes.
Whenever a return message arrives from an activity, the orchestrator code restarts. It always starts with the first line. Yes, this means that the same line is executed multiple times: up to the number of messages to the orchestrator.
However, the orchestrator stores the history of its past executions in Azure Storage, so the effect of the second pass of the first line is different: instead of sending a message to the activity it already knows the result of that activity, so await returns this result back and assigns it to the conference variable.
Because of these “replays”, the orchestrator’s implementation has to be deterministic: don’t use DateTime.Now, random numbers or multi-thread operations; more details here.
Azure Functions are stateless, while workflows require a state to keep track of their progress. Every time a new action towards the workflow’s execution happens, the framework automatically records an event in table storage.
Whenever an orchestrator restarts the execution because a new message arrives from its activity, it loads the complete history of this particular execution from storage. Durable Context uses this history to make decisions whether to call the activity or return the previously stored result.
The pattern of storing the complete history of state changes as an append-only event store is known as Event Sourcing. Event store provides several benefits:
Here is an illustration of the notable events that get recorded during our sequential workflow:
Azure Functions on the serverless consumption-based plan are billed per execution + per duration of execution.
The stop-replay behavior of durable orchestrators causes the single workflow “instance” to execute the same orchestrator function multiple times. This also means paying for several short executions.
However, the total bill usually ends up being much lower compared to the potential cost of blocking synchronous calls to activities. The price of 5 executions of 100 ms each is significantly lower than the cost of 1 execution of 30 seconds.
By the way, the first million executions per month are at no charge, so many scenarios incur no cost at all from Azure Functions service.
Another cost component to keep in mind is Azure Storage. Queues and Tables that are used behind the scenes are charged to the end customer. In my experience, this charge remains close to zero for low- to medium-load applications.
Beware of unintentional eternal loops or indefinite recursive fan-outs in your orchestrators. Those can get expensive if you leave them out of control.
What happens when an error occurs somewhere in the middle of the workflow? For instance, a third-party flight booking service might not be able to process the request:
This situation is expected by Durable Functions. Instead of silently failing, the activity function sends a message containing the information about the error back to the orchestrator.
The orchestrator deserializes the error details and, at the time of replay, throws a .NET exception from the corresponding call. The developer is free to put a try .. catch block around the call and handle the exception:
The code above falls back to a “backup plan” of booking another itinerary. Another typical pattern would be to run a compensating activity to cancel the effects of any previous actions (un-book the conference in our case) and leave the system in a clean state.
Quite often, the error might be transient, so it might make sense to retry the failed operation after a pause. It’s a such a common scenario that Durable Functions provides a dedicated API:
The above code instructs the library to
The significant point is that, once again, the orchestrator does not block while awaiting retries. After a failed call, a message is scheduled for the moment in the future to re-run the orchestrator and retry the call.
Business processes may consist of numerous steps. To keep the code of orchestrators manageable, Durable Functions allows nested orchestrators. A “parent” orchestrator can call out to child orchestrators via the context.CallSubOrchestratorAsync method:
The code above books two conferences, one after the other.
What if we want to run multiple activities in parallel?
For instance, in the example above, we could wish to book two conferences, but the booking order might not matter. Still, when both bookings are completed, we want to combine the results to produce an expense report for the finance department:
In this scenario, the BookTrip orchestrator accepts an input parameter with the name of the conference and returns the expense information. ReportExpenses needs to receive both expenses combined.
This goal can be easily achieved by scheduling two tasks (i.e., sending two messages) without awaiting them separately. We use the familiar Task.WhenAll method to await both and combine the results:
Remember that awaiting the WhenAll method doesn't synchronously block the orchestrator. It quits the first time and then restarts two times on reply messages received from activities. The first restart quits again, and only the second restart makes it past the await.
Task.WhenAll returns an array of results (one result per each input task), which is then passed to the reporting activity.
Another example of parallelization could be a workflow sending e-mails to hundreds of recipients. Such fan-out wouldn’t be hard with normal queue-triggered functions: simply send hundreds of messages. However, combining the results, if required for the next step of the workflow, is quite challenging.
It’s straightforward with a durable orchestrator:
Making hundreds of roundtrips to activities and back could cause numerous replays of the orchestrator. As an optimization, if multiple activity functions complete around the same time, the orchestrator may internally process several messages as a batch and restart the orchestrator function only once per batch.
There are many more patterns enabled by Durable Functions. Here is a quick list to give you some perspective:
Further explanation and code samples are in the docs.
I firmly believe that serverless applications utilizing a broad range of managed cloud services are highly beneficial to many companies, due to both rapid development process and the properly aligned billing model.
Serverless tech is still young; more high-level architectural patterns need to emerge to enable expressive and composable implementations of large business systems.
Azure Durable Functions suggests some of the possible answers. It combines the clarity and readability of sequential RPC-style code with the power and resilience of event-driven architecture.
The documentation for Durable Functions is excellent, with plenty of examples and how-to guides. Learn it, try it for your real-life scenarios, and let me know your opinion — I’m excited about the serverless future!
Many thanks to Katy Shimizu, Chris Gillum, Eric Fleming, KJ Jones, William Liebenberg, Andrea Tosato for reviewing the draft of this article and their valuable contributions and suggestions. The community around Azure Functions and Durable Functions is superb!
Originally published at mikhail.io.
#BlackLivesMatter
1K 
6
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
1K claps
1K 
6
Written by
Cloud, Azure Functions, Serverless, F#, Functional Programming. Microsoft Azure MVP.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Cloud, Azure Functions, Serverless, F#, Functional Programming. Microsoft Azure MVP.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@kamran-bilgrami/ethical-hacking-lessons-building-free-active-directory-lab-in-azure-6c67a7eddd7f?source=search_post---------7,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kamran Bilgrami
Jan 6, 2020·21 min read
The majority of IT experts concur that Active Directory is the dominant approach for managing the Windows domain networks. This is why adversaries get attracted to discover and exploit vulnerabilities within the Active Directory echo system. In order to defend against those types of attacks, there is a need for practice grounds…
"
https://medium.com/@vivekraja98/building-end-to-end-covid-19-forecast-model-using-azure-ml-16da338864b3?source=search_post---------8,"Sign in
There are currently no responses for this story.
Be the first to respond.
Vivek Raja
Jun 9, 2020·7 min read
A few months before, we would have never thought how the entire would change due to the pandemic caused by a virus so-called COVID19 a.k.a Coronavirus. It has changed how the entire world works right now and we must salute to the people working in the frontline to prevent, cure us of this deadly virus.
As a Data & Cloud Enthusiast, I wanted to contribute my purpose in these times of dire situation and most importantly to empower people with AI/ML knowledge to contribute. I hope this tutorial article would serve one of its purposes. I have tried my best to simplify the process that you can build your own model at the end of this article. All you need to know is the basics of Python Programming to get started!
The website I had created is https://newcovid.herokuapp.com to visualize and get real-time information about COVID-19 status worldwide, India (State and District wise) and most importantly, the forecast of COVID-19 cases in India for next 7 days.
On you mark….
First thing first, we need to get an Azure Account to get started. If you have one, that’s well and good. If not, don’t worry, Microsoft is giving a free trial to access Azure Cloud. Register and sign up here: https://azure.microsoft.com/free
We need to create a workspace for our project. Workspaces are Azure resources, and as such, they are defined within a resource group in an Azure subscription, along with other related Azure resources that are required to support the workspace.
In your Azure portal homepage, click “Create a Resource”. Since we are going to do many ML tasks, now select “AI + Machine Learning” under Azure Marketplace column. Select the first option “Machine learning”. Now create a workspace by filling up the following details. Enter the details as shown the image. You are free to use any name for the Resource Group, Workspace. Select the appropriate subscription (Free trial if you are using one). Click “Review and Create once it is done”.
Since we need a compute instance to run the entire ML tasks, let’s create one. Go to ml.azure.com. On the left side, click “Compute”. Under Computer Instances, click New. Give a name for your Compute Instance and select Virtual Machine type as “STANDARD_DS3_V2”. Wait a minute or two for your VM to spin up.
Click “Jupyter” and that will take to Jupyter Notebooks.
Create a new folder by clicking “New” on the right side. Navigate to the folder and then create new “Python 3.6 Azure ML”. Open the .ipynb notebook file you had just created.
We need to connect our load our workspace and set up a new experiment, say “covid_exp”. This can be done as shown below
We are going to create a script file which will be the input to the ML Pipelines. ( Don’t worry, we will come to back to Pipelines in the later section of the article)
The following lines of codes under this step must be executed in single notebook cell.
The script is going to perform the following actions under the run context of our “covid_exp” experiment.
First, we will create a new python script file called prediction.py and import all necessary libraries. You can run these lines of code in our .ipynb Python Notebook we are working on.
We need to define the run context for the experiment, followed by importing the COVID-19 Indian data from https://api.covid19india.org/data.json
Load the Time series data in Pandas data frame. We will be working on the daily number of cases of confirmed, recovered and deaths. Split the data as Train and test dataset with 0.75 ratios.
We can infer that this is a typical Time Series Forecast Problem BUT it is not. To model the spread(infection) and control(recovered) of infectious diseases, a mathematical model such as SIR (Susceptible-Infected- Recovered) Model. This section of the article is the playground of Data Scientists, you can come up with the models for this data.
To know more about SIR Model and its implementation, refer: https://www.kaggle.com/lisphilar/covid-19-data-with-sir-model
Write the forecast data to a local file called covid_pred.txt
The forecast data is available in covid_pred.txt must be uploaded to Azure Storage as a blob. This blob can we be accessed by Anonymous request to use the forecast data. The following steps need to be done to create a storage account and upload the blob.
4. Once the container is created, navigate inside the container. The Container is going to hold the uploaded forecast data as a blob. Click “Upload” and select an empty text file. Give a name for the blob .
5. Click “…” of the created blob and select “Generate SAS Token”. Leave the settings as follows. Copy the Blob URL (you need it later).
6. Coming back to our pipelinescript.ipynb. As our last step of our Python script file prediction.py is to upload the content of covid_pred.txt to the blob we had just created using SAS token. Complete the run of the experiment by run.complete()
A Pipeline object contains an ordered sequence of one or more PipelineStep objects. The pipeline is going to execute the script prediction.py
Allocate the compute cluster for the pipeline to be executed. You can use the same compute for this.
Create a Conda Environment for the pipeline by including the necessary libraries for execution. Name the environment, say “covid_env”
Now we need to define the pipeline Object which consists of the Pipeline step which is going to execute prediction.py on compute cluster.
Now the pipeline is ready and we need to execute the pipeline and publish it as “covid-pipeline”.
We can schedule the pipeline to be run at the time specified by us. For our problem statement, the SIR model will predict the new forecast for next 7 days every day. So the pipeline needs to be scheduled to run once a day using the pipeline id generated in above code.
Now execute all the cells in pipelinescript.ipynb. You can view the execution details in the output cell. Once the execution is over, we can notice two results
The blob forecast data may vary according to your model.
Now we need to make the blob to be accessible by applications. This can be done by an HTTP request. To make sure the data access safe, we need to modify CORS policy such that our application can access the blob.
Go to Storage Account in your Azure Portal → Click the account → Under Settings click CORS → Update the URL and necessary information as below.
As my website wants to access the blob, I had allowed the ‘GET’ method.
The blob now can be accessed with the SAS Blob URL we had generated in Step 4.
The plot of trends of daily confirmed cases + forecast and Daily recovered + forecast can be seen in the below two graphs hosted in the website
newcovid.herokuapp.com
Voila! Congratulations on creating an end to end ML model for COVID-19 forecast system using Azure ML. If you loved the article/tutorial, give claps, share to other devs as well.
Feel free to reach out to me regarding queries, suggestions, critics, appreciation, feedback through Email: vivekraja98@gmail.com, LinkedIn.
Microsoft Certified Azure Data Scientist, AI Engineer, Data Engineer Associate | Tech Speaker, mentor & Researcher| 15x Hackathon Winner|
See all (64)
5.3K 
6
5.3K claps
5.3K 
6
Microsoft Certified Azure Data Scientist, AI Engineer, Data Engineer Associate | Tech Speaker, mentor & Researcher| 15x Hackathon Winner|
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@to_pe/deploying-create-react-app-on-microsoft-azure-c0f6686a4321?source=search_post---------9,"Sign in
There are currently no responses for this story.
Be the first to respond.
Toni Petrina
Jan 2, 2017·3 min read
Creating React apps has never been easier with the advent of tools like create-react-app or next but deploying them is both easy and hard at the same time.
Since I usually deploy apps on Microsoft Azure, it was natural to deploy a shiny new frontend on an Azure Website. The CLI tool can build your React app that can be served as a simple static site since it consists only of a single html file, a single js file and a bunch of static images and style sheets.
Our first step is ensuring we have create-react-app installed on our system. To do that, run the following command:
To create a new project, simply invoke the tool with a project name:
This will create a new folder named AzureTest that will contain all the files used for developing and building your website. Let’s build the project using the following command:
After it is finished, a new folder named build is created which contains everything you need to run the website in production.
Let’s create a new website that will host our React app. Go into Azure Portal and create a new Web App. Let’s assume it is named AzureReactTest1 and before continuing download the publish profile which contains FTP credentials.
Once connected via the FTP client, copy the entire content of the build folder created earlier into the /site/wwwroot/ folder on your Azure Website.
Refresh the site in a browser and it should display the default page.
Once routing is added into the mix, problems appear. If routing is done on the client side, e.g. if you are using create-react-app and react-routerto build a single page app, the url site.com/section won’t work out of the box.
This means if we try to open the link above manually in a browser, we would get a 404 error page because the Azure is trying to find an index.html (or some other index.* named) file inside a section folder. To fix this, Azure needs to know how to pass the intended route to the one and only index.html placed at the site’s root. Create a new file in the /site/wwwroot folder named web.config with the following content:
This will tell Azure to rewrite all urls as if they are pointing to our root file and our SPA application can handle the links correctly.
Deploying React apps is as simple as deploying static sites — provided one is aware of the routing problems. In the next post we’ll setup a CI integration on our Azure website.
Apathy on the rise, no one cares! Will code for food!
See all (197)
1.1K 
26
No rights reserved
 by the author.
1.1K claps
1.1K 
26
Apathy on the rise, no one cares! Will code for food!
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@matthewleak/deploying-a-static-website-to-azure-storage-using-azure-devops-fa0bed457d07?source=search_post---------10,"Sign in
There are currently no responses for this story.
Be the first to respond.
Matthew Leak
Mar 17, 2019·5 min read
I had a recent requirement to host a static website for a Government project in MS Azure. There was also an additional requirement that all testing, building and deployment should be automated using CI/CD in Azure DevOps. I’m fairly new to the world of Azure after having used AWS for almost everything over the past 5+ years and so I wanted to use this as an outlet to document the process.
I decided to use Azure Storage over Azure App Service (S3 and Elastic Beanstalk being the respective AWS equivalents) as there were only a few static assets to host, such as an index.html file, some css and a couple of images. The infrastructure that comes out of the box with any PaaS (Azure App Service / Beanstalk) seemed pretty unnecessary here as I like to reduce my cost footprint wherever possible. (even when cost isn’t necessarily an issue …)
As with any new project in Azure we want to create a Resource Group. This will allow us to logically group resources for this project -
The next step is to create a storage account where our static assets will be stored and used for our static website -
While in the Storage Account, navigate to “Access Keys” and take a note of “Key #1” as we’ll need this later.
Here we begin to build our pipelines. Navigate to https://devops.azure.com and if required click “Create Project”. Give it a name, set your preferred visibility option and then click “Create”.
With our DevOps project created, it’s time to start building the Build pipeline which will produce a .zip artefact that will be used by the Release pipeline that will be created in the next section.
For this article, we’ll use the visual designer to build but I’ll provide the YAML config at the end of this article.
With the build pipeline complete and a zip archive of our build directory being produced, we need to release it to Azure Storage. To accomplish the upload, we’ll use the Azure CLI in the release pipeline as that’s what I’ve had the best results with.
You’ll notice that in the CLI script above, our destination is set to $web which is the storage blob that Azure automatically creates when you enable the static website feature in a storage account. Any object inside of this blob will be publicly accessible via the static website URL.
We would normally take advantage of pipeline variables and reference those in our CLI scripts instead of adding sensitive keys to the tasks directly. I felt they were out of scope of this article and are a self-explanatory topic if you’ve ever used any other CI/CD tool.
A full reference to the Azure DevOps YAML schema can be found here along with a catalog of tasks that also provide YAML snippets.
Build Pipeline:
If this article has been useful to you, be sure to leave lots of claps! (you can leave up to 50!)
UK-based Technology Consultant working in London and Manchester.
1.8K 
13
1.8K 
1.8K 
13
UK-based Technology Consultant working in London and Manchester.
"
https://medium.com/young-coder/could-microsoft-azure-actually-win-the-cloud-18c78b8780fe?source=search_post---------11,"There are currently no responses for this story.
Be the first to respond.
Being the first mover in a new field is a calculated risk. If you get it right (think Apple’s iPhone), you can own the market for years. But if you misjudge the technical challenges or your customers’ needs, you’ll end up with an expensive flop (think Apple Newton).
"
https://medium.com/@kyleake/exam-az-900-microsoft-azure-fundamentals-most-complete-preparation-guide-ever-76614d31a59c?source=search_post---------12,"Sign in
There are currently no responses for this story.
Be the first to respond.
Korkrid Akepanidtaworn (Kyle)
Aug 1, 2019·60 min read
“This exam is designed for candidates looking to demonstrate foundational level knowledge of cloud services and how those services are provided with Microsoft Azure. The exam is intended for candidates with non-technical backgrounds, such as those involved in selling or purchasing cloud based solutions and services or…
"
https://medium.com/swlh/10-great-courses-for-aws-google-cloud-and-azure-ec89bef8a078?source=search_post---------13,"There are currently no responses for this story.
Be the first to respond.
As 2019 comes to an end, it’s that time of year when we look to setting new goals, and focusing on what we want to learn next year.
As engineers many of those goals revolve around keeping up with every new technology and framework. In particular, technologies such as AWS, Azure and Google Cloud all…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/statuscode/getting-key-vault-secrets-in-azure-functions-37620fd20a0b?source=search_post---------14,"There are currently no responses for this story.
Be the first to respond.
NOTE: Updated on 11/28 to reflect new key vault and function capabilities
One of the common questions around building Azure Functions is how to deal with secrets that a function needs. We know we shouldn’t be putting secrets in the code. Another sometimes satisfactory alternative is place secrets as an app setting. However, since these app settings are retrievable via clear-text through the API/Portal, and would need to be replicated to each function app that needs the secret, some prefer to leverage Azure Key Vault.
Here are the steps to build an Azure Function which can retrieve secrets at runtime. The below uses C# compiled class libraries, but the same underlying pieces can be used with any Azure Function language or binding.
I followed the instructions here to create a key vault in my Azure Subscription. After the key vault was created I ran this command to add the secrets to the vault.
Now authorized identities can retrieve this secret as needed in a central, secure location. One way to access is create my own custom application identity, or “service principal”, which I could use to access the secret. The rub with that approach though is in order to retrieve a valid token for a service principal I need to provide an application secret — which itself is… a secret. With the announcement of Azure managed service identities, I can now give my resource (in this case, a function) an identity and give it permissions just as it were another user.
After creating the Azure Function, I enabled a managed service identity for the function app. This will now add an identity in my Azure active directory I can give permissions to any resource I may need.
After enabling the managed service identity, I went into my key vault and added an access policy so my Azure Function app had permissions to read secrets.
The last step is to fetch the secret when I run the app.
Starting in the fall of 2018 I don’t have to make any code changes in order to access secrets in key vault.
Here’s a sample Azure Event Hub triggered function that makes a simple log message. In this case this app has two secrets: the Event Hubs connection string, and the private variable superSecret.
When running locally I can use local.settings.json to have local stored values to develop and test off of. This can connect to a development Event Hub and use some development secret.
However, once I publish, I just have to modify the Application Settings to resolve the right values as secrets.
In the Azure Portal (or via Visual Studio / VS Code / CLI / whatever tool you want to use to manage app settings), I need to associate some app settings to correspond to the secrets in key vault. To do that I will create two new application settings for SuperSecret and EventHubConnectionString that mirror the local.settings.json — but instead of having the secrets in the app settings, I will point to where the secrets are in my key vault. It should be noted that the key vault secret ID is itself not a secret, it’s just an address of where the secret can be found.
I browsed to my key vault and found the secrets I needed and copied their address or Secret Uri (it looks something like https://{name}.vault.azure.net/secrets/{secret}/{id}) and then added special app settings that will fetch the secret from that source when the app instantiates on an instance. This means the secret only gets fetched once per instance, and not every execution (so I don’t have to worry about key vault throttles).
The app setting key is EventHubConnectionString and the value is something like @Microsoft.KeyVault(SecretUri={theSecretUri}).
When done configuring, here’s what my app settings looked like to get my secrets:
If I wanted to, I could make any of these app settings into key vault references, including things like the AzureWebJobsStorage account.
Once I click save, I’m done. The function instance will restart, authenticate to key vault using the managed service identity I configured, resolve the key vault secrets and store them in the appropriate environment variables, and my code can now trigger on the Event Hub and log the message as written.
Keeping developers informed.
531 
23
531 claps
531 
23
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Keeping developers informed.
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Keeping developers informed.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ngconf/angular-on-azure-part-i-d842e8f76462?source=search_post---------15,"There are currently no responses for this story.
Be the first to respond.
Using Azure Pipeline
Developers test software in a local environment using servers such as LAMP, WAMP, node, IIS, and Apache. Local deployment is good in terms of fast development and debugging, but we can’t ship our physical machines to the client in order to access to the application 😂. We have to deploy an application to a web server/cloud in order to make it accessible to the end user on their preferred platform (mobile, desktop, etc).
A variety of cloud providers exist in the current market, the most popularbeing Microsoft Azure, Google Cloud Platform, AWS. These providers offer an unbeatable combination of ease, speed, and automation, so if you have not deployed using such a platform, then this is the article for you! This article focuses on deploying an Angular app to Azure.
The action of bringing resources into effective action
In web development, deployment is concerned with making our static/dynamic resources available over the wire so the end user can access them in a desired device, using a browser.
Web hosting is a service that allows organizations and individuals to post a website or web page onto the Internet.
The deployment process is incomplete without hosting.
Deployment exposes your web application using a philosophy that has beenfollowed for year. The diagram below outlines typical deployment steps that could be applied to any type of software.
Azure is a cloud platform service which provides cloud services, including those for compute, analytics, storage, serverless, AI and ML, IoT, containers, DevOps, mobile, and networking. It is widely considered as both a PaaS and IaaS product. This article covers the development, deployment, and DevOps aspects of the platform.
If you are new to Azure, a free subscription is available for those wishing to try the platform without any commitment.
The Azure App Service is part of the PaaS section of the platform. It easily builds and deploys a highly available web app to the cloud. Multiple features are available right out of the box, as illustrated below.
The important steps are numbered in the above screenshot. If a resource group is not created, then do so in step 3. Also, if you do not have a service plan, create one at this time. Make sure you select ‘ASP .NET 4.7’ in the ‘Runtime Stack’ option in step 5. For more information, follow the guide for the detailed creation of Azure Service Plan and Azure App Service.
Once you’re done with the fill in details, click on “Review and create” button and then on the next screen press “Create” button. To see the newly created resource you can click on “All Resources” option in the sidebar.
The following url can be loaded to check if the recently deployed application is available in the cloud, https://<app-name>.azurewebsites.net/
In my case I used app name as “angular-deployment” so URL would become https://angular-deployment.azurewebsites.net/
But, before moving forward, we should minimize the final bundle size of theapplication. No worries; that process is discussed in a later section.
Angular CLI tooling is incredible; simply executing ng serve compiles angular code to Javascript and generates bundle files. For a simple hello-world app, however, the total file size is far short of desirable.
Angular currently offers two compilation modes
In short, JIT ships the Angular compiler over the wire and component templates are compiled inside the browser. AOT mode precompiles all templates and makes the resulting JS available for further optimization before shipping the bundled application over the wire. Smaller application sizes and quicker response makes for better UX!
For those new to the Angular CLI, AOT mode is enabled with the command
ng build --prod
This command compiles all templates, then applies tree-shaking, optimization, minification, and bundling to create a final, highly-optimized package. All distribution files are automatically placed in the dist folder of your project, which can be directly hosted to a cloud provider.
In this article, Azure DevOps (formerly known as VSTS) is used to deploy an application to the cloud.
If you have not created an organization, then do so before clicking the ‘CreateProject’ button, as shown in the above diagram. This displays the ‘Create NewProject’ dialog.
In “Create new project” screen, fill project name, description and select visibility (I selected private). Also, version control was set to ‘Git’, and ‘Work item process’ defaulted to ‘Agile.’ Then, click the ‘Create’ button.
The dashboard page is displayed after a project is created. Several actions maybe performed from the dashboard sidebar.
The most important feature in the above list for purposes of this article is theAzure Pipelines setup.
Select the ‘Pipelines’ option from the left sidebar, which displays the ‘New Pipeline’ button in the middle of the screen. The following dialog is displayed after clicking the ‘New Pipeline’ button.
Pipelines are created with yaml files. A new yaml file may be created with avisual tool or by using the ‘Use the classic editor’ link at the bottom of the dialog.
The next step is selecting a repository resource, which can be a new repository(above) or using an existing repo as shown below. I’m using my existing Github repo, so I selected ‘Github’ at this stage. To select a Github repo, click on the ‘…’ Button to browse repositories.
Select the desired repository for deployment. In this example, I selected the‘weather-cast’ repo. Then, click the ‘Select’ button.
At this point, you are very close to creating a brand new pipeline! By default, the ‘master’ branch is selected. Click on the ‘Continue’ button.
Now, you’ve made it to the final page of pipeline creation! Next, we create a ‘Job’, or the steps involved in the actual deployment. For now, just select ‘Empty Job’ to create a blank Job with no content. Don’t worry, we will add steps for it in the next section.
After the pipeline is created, you will see a screen where an Agent pool isassigned to run a job when any tasks are to be deployed. For this tutorial, we are going to setup deployment tasks under the ‘Agent Job 1.’ Simply click on the ‘+’button in the dialog.
Cool! We’ve finally made it to the stage where we can add tasks for thedeployment job! Refer to the following screen shot.
After clicking the ‘+’ icon beside ‘Agent Job 1,’ you can search by ‘node’ in the list (item 1 in the screen shot) then select ‘Node Tool Installer.’ When that dialog displays (item 2), click the ‘Add’ button (item 3 in the above screenshot).
This displays the first task in the ‘Agent job 1’ list. Next, fill in the details for this task. Enter display and version spec, as shown above. This configures NodeJS on our VM.
As before, search for ‘npm’ in the task list and then click the ‘Add’ button. Fill in the details as shown above to install the Angular CLI as the next step in the task list.
Continue the same process as above to create a task that installs all npmdependencies.
Again add npm ask and fill in the details shown above. This time select command as in “custom”, and “command and arguments” would be run build. Basically, it calls ng build --prod command written as scripts in . This task helps to create a production ready angular package.
This is the task that creates the production-ready Angular package.Continue as before using the details shown in the above screenshot. ‘Command’ is ‘custom’ and the ‘Command and arguments’ input is ‘ng build — prod’. This causes the ng build --prod command to be written in the ‘scripts’ section of the package.json file.
Next, search for ‘Azure App Service Deploy’ and add it to the task list. Fill in the details as shown below. This task hosts and deploys code to the server.
After you have finished entering details, click on the ‘Save and queue’ button.This saves and subsequently runs the pipeline. You will see a message with ahyperlink containing a build number. Or, you may navigate to ‘Pipelines > Builds’ to see the following screen.
After the job is finished, we can check it as shown below.
This article outlined the steps to deploy an Angular application to Azure directly from Github or another repository. An Azure pipeline is a powerful feature to setup and visualize a deployment job with minimal effort.
If you like this article press 👏 clap button 50 times or as many times you want. Feel free to ask a question if you have any. Thanks a lot for reading!
Soon, I will release part II of this article which covers CI and CD with AzurePipeline. Stay tuned!
For more Angular goodness, be sure to check out the latest episode of The Angular Show podcast.
Come hear top community speakers, experts, leaders, and the Angular team present for 2 stacked days on everything you need to make the most of Angular in your enterprise applications.Topics will be focused on the following four areas: • Monorepos • Micro frontends • Performance & Scalability • Maintainability & QualityLearn more here >> https://enterprise.ng-conf.org/
The World’s Best Angular Conference
2.1K 
6
Get up-to-date info, news, special offers and more from ng-conf! Join us for ng-conf 2022 March 16-18.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
2.1K claps
2.1K 
6
Written by
👨‍💻 Virtusa | MVP | GDE | Tech Savvy
The World’s Best Angular Conference
Written by
👨‍💻 Virtusa | MVP | GDE | Tech Savvy
The World’s Best Angular Conference
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@brentrobinson5/containerised-ci-cd-pipelines-with-azure-devops-74064c679f20?source=search_post---------16,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brent Robinson
May 23, 2019·15 min read
It starts simple — one application, one technology, one build server — and CI/CD is working flawlessly. Time and technology move on, complexity grows, and your build server, while stable, is loaded with numerous versions of different tools. One day, a new tool is required, which cannot co-exist with others, and before long, build servers begin to exist for specific projects which match their precise needs. Perhaps you’re not as fortunate, and new projects are held back, forced to use legacy tools because the effort to upgrade all other projects is too much.
"
https://koukia.ca/keep-calm-and-use-azure-application-insights-ff38e04a43fd?source=search_post---------17,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
If you have been following me here, I started writing about Azure Application insights 4 or 5 years ago on my blog when it first came out and I was really impressed by its power even then, but now after several years, I think it has made a lot more improvements so it’s time for another closer look and some of the new features it has.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://serifandsemaphore.io/azure-cloud-functions-vs-aws-lambda-caf8a90605dd?source=search_post---------18,"Microsoft announced their answer to AWS Lambda at their Build 2016 conference. Brace yourself for the next cloud hosting change; because, it now seems like everyone is going “serverless.” Amazon Web Service’s Lambda, Google’s Cloud Functions, IBM’s OpenWhisk, and now Azure’s Functions.
There’s too much to compare in one article, so I’ll write about the differences in a series of articles. I’ve had the opportunity to play with some. Google’s Cloud Functions are in private alpha, so I can’t share much about them, but Microsoft’s are in public preview.
I’ve been using Lambda a lot lately and it’s been a game changer.
While Azure’s cloud function usage is billed the same way, the service is radically different than Lambda under the hood.
Before you make any major decisions, keep in mind that these services are evolving.
This will change over time for every cloud function service.
Azure Functions supports Node.js, C#, F#, Python and PHP. They also list Java, bash, and batch but it’s not clear how to use these (again, it’s in preview).
AWS Lambda supports Node.js, Python, and Java for now.
I think PHP support is a very strategic choice given its popularity, but I wish one of these services would better support Go.
Why Go? Because it runs fast and its code is easy to read. It is a language of brevity and therefore fits well for the cloud and (micro)services. Add Rust and Swift to the list too.
It’s important to note that you can execute binaries in both services through one of these languages. This is the method in which Go works in Lambda. Both Apex and also Sparta are serverless frameworks that let you use Go in Lambda as well.
It would be great for a cloud function service to support Go natively. Swift for mobile developers who often need simple back-end services would also be a smart move.
No amount of fancy devops tricks or tooling will save you from having to use the web dashboard at least every now and then.
Azure’s Function usage charts are still being developed, but they will be there and at least on par with Lambda’s.
Azure’s new dashboard is beautiful.
Once you understand the organizational structure of Azure Function it becomes a little easier. The same can be said for AWS Lambda too.
One important difference is the editor. Both are on par when it comes to the function’s settings screen. However, Azure has the more robust Visual Studio Online which can also be used.
Visual Studio Online is found under the Function’s App Service’s tools section. It’s worth noting that there are some other cool tools here too, including a console.
Most of your development time should be outside of dashboards anyway. I don’t consider them a huge factor in my decision making process but they can offer convenience.
Let’s dive into the architecture of Azure Functions compared to Lambda.
An “App Service” is a container, or environment, for a set of Azure Functions. This is much different than Lambda. In fact, the two services couldn’t be more different.
Lambdas are organizationally independent, where Azure Functions are grouped logically into an “application.”
This App Service can either by “dynamic” or “classic.” The dynamic option is where you pay only for the time and memory your functions use. This is the biggest similarity between Lambda and Azure Functions.
It’s important to note that the memory you allocate is per app service (with potentially many functions), not per function like Lambda.
I think of the “classic” App Service more like Amazon ECS where you have EC2 instances running to handle the tasks or functions. The pricing model for this is then like EC2.
Azure is actually more like a blend between ECS Tasks and Lambda. For example, you can set environment variables on App Services which are then available for your Azure Functions. AWS Lambda cannot persist environment variables, but ECS Tasks can (correction/update: Lambda now can as pointed out by Jeremy Axmacher, thank you).
The entire container architecture is different. Lambdas provision a brand new one and deploys your code (from a zip file) on a cold request. Subsequent requests can be subject to container re-use and be handled much faster. However, you need to understand there is no persistence and with Node.js Lambdas you need to watch your variable scope because the container can be re-used.
However, Azure Functions are less subject to the cold/warm request effects. Azure still provisions resources as needed, but your files aren’t “frozen” somewhere in a zip file. They run on top of Azure’s WebJobs.
Azure Functions are also supposed to be accessible via FTP by connecting to the App Service’s FTP. Though I couldn’t get it to work so far during my review (no matter how many times I reset my credentials).
Azure Functions of course is a Windows system (32bit for dynamic services) while Lambda is a Linux system. Though I do hope that changes since Ubuntu can now run on Windows. In fact, that would put Azure in a good position with cloud hosting.
Speaking of working with the files, you can deploy your Azure Functions in a variety of ways:
You have many more inherent options than AWS Lambda, but I didn’t explore any sort of CLI deployment. Lambda has some great 3rd party tools for deploying Lambdas and both cloud services have SDKs to make just about anything possible.
I do have to hand it to Microsoft here though. You can hook up GitHub, even with your favorite CI tool, and easily deploy your cloud functions to Azure. Release managers rejoice.
Though it’s important to note that Lambdas can be versioned whereas Azure Functions can not (at least not yet). So if you wanted to rollback, you’ll need to rely on Git or some other version control system.
One challenge with Lambda is development workflow. People have created tools to test your Lambda locally so you can write code, run, and then even deploy if all looks good.
There are also several “serverless frameworks” that help with this process too.
Regardless, Lambda is closed source. However, Azure Function’s runtime is open-source. I would expect some clever CI integrations in the future.
I also have to concede that Azure Functions make things a bit easier in this department too. Every function automatically maps to an HTTP endpoint if enabled. Whereas with Lambda, you must configure API Gateway separately.
API Gateway is fine, but complex and time consuming. Again, some serverless frameworks ease this pain point by automatically setting up an API for Lambdas.
Microsoft gets points for UX because you have a lot less to configure. I’m not sure you can run Azure Functions from multiple triggers though. It doesn’t appear that way, but maybe by editing the function.json directly you can.
I would say that AWS Lambda has a slight edge in flexibility here, but both services are constantly changing.
The Azure app and function names are reflected in the URLs to trigger the cloud functions. This helps avoid confusion, but might also be limiting.
Apparently all HTTP methods are supported (GET, POST, etc.) through the endpoint assigned to each cloud function. With API Gateway, you need to configure each method manually.
One thing I don’t see is path variables. It seems that everything must be passed as a querystring or in the request body.
To be fair, it’s called an “HTTP trigger” so when thinking about it in those terms, it makes sense. If you need more robust settings, then you’re talking about “API configuration and management.”
Azure Functions also have super easy authentication. You can protect the cloud function endpoints with 3rd party authentication; this includes Facebook, Twitter, Google, and of course Microsoft’s auth.
I’ve never setup Facebook auth for an HTTP endpoint so quickly and easily before in my life.
You can configure CORS and you can also manage your own domain name for the endpoints. Though I couldn’t see if there was a way to limit the HTTP request methods. I imagine with Azure’s API management service more will be possible, but then you’re going through as much work as API Gateway.
I don’t think there’s a clear “winner” here. Both Microsoft and Amazon have powerful features for triggering cloud functions with HTTP requests. Though I definitely think Amazon could learn a thing or two about UX from Microsoft.
I was thoroughly impressed by Azure and I think it’s in part due to its new dashboard. The new dashboard and the process of setting up cloud functions in a logical group called an “app” makes complete sense.
To illustrate how obvious this pain point is, look no further than Serverless (formerly JAWS) and Apex. These are two serverless frameworks that address this specific organizational concern. They help a developer logically group functions.
Another pain point with Lambda is the maximum execution time limit. It wasn’t always clear and many people never thought their functions would run that long (at least in my experience).
The time limit used to be 1 minute for Lambda, but increased to 5 to help with “ETL” operations according to Amazon. I’m happy for the time limit increase, but even 5 minutes may not be sufficient for all operations.
Iron.io pointed out that their workers had no time limits in a comparison with Lambda. Iron workers are another option, though I stayed the course with AWS Lambda. I even found a way to re-purpose your Lambda code as ECS Tasks to cope with the 5 minute limit.
While that solved my own problems with the time limit, I understand that it’s not a solution for everyone. It also requires EC2 instances which are billed in a different fashion. So it’s not a perfect solution if “pay as you go” is a top priority.
Azure’s Functions really are the perfect blend of Lambdas and ECS Tasks. You can write and execute the same exact code regardless of the environment and billing model. There are apparently no time limits either(so long as your function is not idle).
On the other hand, I imagine there could be some surprises in billing too. Lambda’s 5 minute execution time limit does serve to protect you from expense. Will Microsoft keep billing you for some accidental loop in a cloud function? Assuming the same amount of resources were used, will the cost work out to be about the same regardless of billing models? It’s perhaps too early to say since it’s still in preview and there is no pricing information available yet.
There is some chatter about this concern though if you follow the GitHub issue.
As I explored Azure Functions and it’s underlying architecture (as best I could), I realized a few other things of note.
First, since there is persistence and no execution time limits, working with large files and ETL is easier. Just watch out for how cost effective this ends up being or not being.
With Lambda + API Gateway, the content-type of the response is set via API Gateway. With Azure Functions, the content-type is set by the code. This can be very useful, but could also lead to some bloated functions.
Remember, your goal is not to build a complete application within a cloud function. It’s not really possible to do with Lambda’s limitations, but it is with Azure. So be careful not to fall into any bad practices.
Watch out for Azure Function’s limitation — only 10 concurrent executions per function.
Again, Azure Functions run on a server with a persistent filesystem and webroot. It’s just not accessible to the outside world. However, it is accessible to you while logged in.
All of your code and assets can be seen via a URL like this: https://yourAppName.scm.azurewebsites.net/dev/api/files/wwwroot/HttpTriggerNodeJS1/index.js
This maps this location on disk: /home/site/wwwroot/HttpTriggerNodeJS1
Is there a cost to request these files this way? Can you somehow open this to the public? I don’t know. It definitely might raise an eye-brow or two though.
You have more insight, access to and control over the environment running your cloud function than you do with AWS Lambda…For better or worse.
Speaking of insight, when listing the directory of D:\home\site\wwwroot where your files are, it indicates that there is over 5 terabytes of disk space available!
This is much more than Lambda’s default ephemeral disk space of 500MB. There isn’t as much info about Azure’s Cloud Functions limitations just yet, but it is important to note that the instance limit is 10 while using “dynamic” service apps (up from 4). The concurrent execution limit per instance is a bit unknown. Lambda’s limit is 100 concurrent executions total (which is a soft limit).
Is everyone’s code on the same storage volume? Or does each Azure account gets its own, very large, storage volume? I’m not familiar enough with what’s going on to know, but exploring is fun.
I would encourage everyone to explore and try each of these services. Understand why and when they they can help you. The end result can be a game changer for you too.
The cloud has powerful magic, but not all clouds are the same. Get to know the environment your code lives in as well as the capabilities of the services available to you. Read the fine print. Understand the limitations and tradeoffs.
Only with this information can you make an informed decision. Remember, your choice will be the path less taken here.
My choice? The jury is still out. I use a variety of services because I believe there is strength and opportunity in diversity.
I primarily use Lambda and ECS because it’s well established. I love AWS and will continue to use it happily, but I definitely think these companies can learn a thing or two from each other.
There’s a lot of opportunity in this fast changing landscape. There aren’t many books. Few best practices. Barely any frameworks. Less than ideal monitoring and debugging. There will be trial and error. You are subject to the possibility of having to re-build code and move your operations. If it gives you any solace, you will be in good company. Some major companies and smart developers are on board with cloud services and the (micro)service movement. This is the new frontier on the web. Buy the ticket, take the ride.
The best-laid plans of unicorns and men…
357 
8
357 claps
357 
8
Written by
Product Person
The best-laid plans of unicorns and men…
Written by
Product Person
The best-laid plans of unicorns and men…
"
https://medium.com/@dancurrotto/creating-a-devops-pipeline-to-deploy-docker-containers-using-azure-kubernetes-service-and-89a18365aedb?source=search_post---------19,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dan Currotto
Jan 7, 2019·8 min read
Of course you know Bill Gates is no longer the richest man in the world. It is now his nemesis over at Amazon. Bill Gates is not running Microsoft but has a full time job giving his money away faster than anyone can spend it.
Meanwhile the Cloud War blazes on…
And I was sure the book seller had it in the bag until I began looking at Microsoft’s offering for Kubernetes.
And again, they’ve greatly simplified something that is very complex in a way that… well, only Microsoft can.
Kubernetes is a container-orchestration system that automates deployment, scaling and management of containerized applications.
It was originally designed and used at Google and is now maintained by the Cloud Native Computing Foundation.
It is taking the country by storm.
Kubernetes orchestrates the deployment of containers onto clusters. And it does it in a way that is pretty magnificent.
Some of the tasks it performs:
· Horizontal scaling
· Service discovery and load balancing
· Automated rollouts and rollbacks
· Secret and configuration management
But it contains a lot of complexity and is not easy to do.
After a spending a lot of time working with Kubernetes on Aws, I decided to also take a look at what Microsoft offered.
I was not disappointed.
This 3 part article will explain and show how to get a .Net Core application up and running on Docker Containers in load balanced clusters using the Azure Kubernetes Service (AKS) and Infrastructure as Code.
It will be done inside a Continuous Deployment pipeline!!
And if you’re signed up for an Azure Free Tier, run your builds as public on Azure DevOps (formerly VSTS) and put your code on free Git repo, you can do this one and many like it for free!
The source code for this article is a public repo in my GitHub account.
You can just grab the repo and following along. If you like, fork it and use it.
Just realize that in this section with the resource group templates you’ll have to use the templates that you will create in this series of articles.
This is what we’ll be doing in 3 parts:
1. Create a Kubernetes cluster using the Azure Kubernetes Service, create an ARM template (Azure Resource Manager) from the service deployment and save it as Infrastructure as Code.
2. Create an automated build that will build the Docker container and push it out to Docker Hub.
3. Create an automated release that will create or update the clusters and apply the deployment to the cluster, which gets the containers with the .Net Core workload and runs them.
Part 1: Create a Kubernetes cluster using the Azure Kubernetes Service, create an ARM template (Azure Resource Manager) from the service deployment and save it as Infrastructure as Code.
The first thing we’ll do is create a Kubernetes cluster using the Azure Kubernetes Service. We’re creating this cluster only to create an ARM template from it. Then we’ll use the ARM template later in the DevOps pipeline to create and update the cluster.
Infrastructure as code (IaC) is the process of managing and provisioning infrastructure using resources definition files. This allows all the infrastructure, the VMs, networks, load balancers, etc. to be stored in files inside source control.
Create a Kubernetes cluster using the Azure Kubernetes Service
After creating your free tier in Azure or if you already have one, go to the Azure Portal.
The first thing we want to do is create a Resource Group. A Resource Group is an Azure resource that allows you to group other Azure resources inside of it. You can also delete the resource group to blow away all the resources inside of it at once.
Click on Create a resource.
Type in ‘resource group’ and hit enter.
Click on resource group when it comes up.
Click on the Create button. This begins the creation process.
Choose a unique name for your resource group and a Region. Here I am putting an –rg on the end of my resource group name to specify it is a resource group. This is to make it easier to read and identify.
Review and create.
Click on ‘Resource Groups’ in the left menu. You’ll see the resource group you just created. Click on the resource group to open it.
It will be empty.
Now let’s create our Kubernetes service. Go back to the Azure home page. Click on ‘Create a resource’.
Type in ‘Kubernetes’ and hit enter.
Choose Kubernetes Service when it comes up.
Click the Create button. This begins the creation process.
Choose the resource group you just created and a cluster name. I like to choose these names in a way they will make sense when I am looking at them together later on. Also put in a short DNS name prefix. It can be anything.
Change the Node Count from 3 to 2. They are going to be replicas anyway.
A word of warning here: With a node count of 2, this is going to create 2 VMs. I will show you how to delete them at the end, but don’t go off and forget about them and just let them run. You’ll burn through all your free tier minutes for the month or worse, if you have a card set up, it will start charging your card. Not to scare you away; it’s just something to be aware of.
Click Review + create.
After the validation passes, click on Create.
You’ll see a moving line to indicate your cluster is being created.
Click on Resource Groups in the Menu. You’ll see the resource group you created earlier but also there will be another one that is named similarly but has an MC_ prefix. There is where Azure will put the resources for the new cluster.
Click on it and you’ll begin to see the new cluster resources as they are created. It looks like a lot but there is little or no charge for most of them. The VMs incur the most charge on your free tier minutes.
After the progress indicator stops, click on the icon and you see the deployment is complete. Your cluster has been created.
You’ve got 2 resource groups with cluster resources in them. This includes everything that the Kubernetes service creates.
At this point you have all the resources for a cluster. You could break out a command line and do everything you need to do to get Docker containers running on your clusters.
But that’s not good enough for us. We want Continuous Integration/Continuous Delivery. We want the whole thing running in a pipeline.
So let’s get our infrastructure as code.
Open the primary resource group.
Click on Deployments in the side menu and click on the only deployment available.
Highlight the template and click on View template.
Here is our infrastructure as code. Click on Download.
Download this to your machine.
Unzip it. These are the ARM (Azure Resource Management) template files. We will use this in the Deployment section of your deployment pipeline to create and maintain our Infrastructure as Code.
Once you have created these template, you’ll want to go back and delete the resource groups containing your clusters so you won’t incur charges. We will be creating them in Part 3 in the Release Pipeline using IaC (Infrastructure as Code)!
Go back to the Azure Portal. Click on Resource Groups.
Locate the 2 Resource Groups that were created that contain the cluster and related infrastructure and delete both of them.
I am a DevOps and Cloud Architect/Developer. I love learning new technologies and helping companies get the most out of them.
290 
4
290 claps
290 
4
I am a DevOps and Cloud Architect/Developer. I love learning new technologies and helping companies get the most out of them.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/200-practice-questions-for-azure-ai-900-fundamentals-exam-e981d28ce91d?source=search_post---------20,"There are currently no responses for this story.
Be the first to respond.
The exam AI-900 is a fundamental AI exam from Azure. According to the study guide here, Candidates for this exam should have a foundational knowledge of machine learning (ML) and artificial intelligence (AI) concepts and related Microsoft Azure services. This exam is an opportunity…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iizotov/azure-functions-and-event-hubs-optimising-for-throughput-549c7acd2b75?source=search_post---------21,"Sign in
There are currently no responses for this story.
Be the first to respond.
Igor Izotov
Jan 14, 2019·9 min read
TL;DR. I take a fairly standard serverless event processing scenario — an Azure Function triggered by messages in an Event Hub — and talk about how how to optimise this system for throughput through: a) overall architecture b) EH partitioning, andc) tweaking the Event Hub trigger host.json settings: maxBatchSize, prefetchCount and batchCheckpointFrequency. I doubt anyone would be interested in reading a yet another theoretical opus, so for my reader’s benefit I ran a series of experiments which involved testing multiple combinations of Event Hub partition counts and host.json settings, and observing the impact on latency and throughput. Feel free to jump straight through to the approach and results published to Power BI.
In other words, I’m trying to help those individuals who are at present staring detachedly in the middle distance trying to understand:
Why is my Azure Function suddenly struggling to catch-up / lagging a few hours behind. I thought the stupid cloud would scale out my stuff automagically…
Near real-time event processing is powerful and also complex since you’re no longer dealing with a traditional once-a-day batch job (that can run late, be rerun if needed, etc.) but with a distributed event-driven system, triggered by individual events (or micro-/mini-batches).
In Azure, a textbook serverless streaming scenario would be an Azure Function App triggered by messages in an Event Hub. It’s a common pattern, especially when your data requires a transformation to make it palatable for the rest of your pipeline.
Imagine you’re dealing with a stream of gzipped xml messages (happens more often than you think!)
Azure Functions’s native Event Hub trigger will take care of firing your code in response to events in the stream. While the trigger works great out of the box, when hitting scale (>10K msgs/sec) you will be required to tweak a couple of settings in host.json and function.json. By the way, everything discussed here applies across all languages supported by the Azure Functions Runtime, v1 and v2 (the latter is recommended due to improved performance). Please mind the syntactic differences in host.json between the two versions though.
Azure Function’s Event Hubs trigger is not exclusive to Azure Functions. The trigger is built on the Event Processor Host that you can use when building your own Event Hub consumers running in VMs, Containers, Webjobs, etc.
When measuring latency, taking the average is almost never a good idea, it hides outliers; consider using a combination of percentiles and a maximum value (a good read if you want a deeper dive). Since we may not always have full control over event publishers and their network connection, decide whether you are going to measure end-to-end latency or pipeline latency (or both).
When measuring end-to-end latency you will require every message to contain a timestamp you can trust, also known as Application Time. When measuring pipeline latency, Event Hub’s enqueuedTime attribute reliably captures the time of arrival of each message. Please keep in mind that since the pipeline latency is based on messages’ time of arrival, it’s usually not a bad idea to implement a policy for late arrival and out-of-order events.
When measuring throughput, decide if you’re counting messages or bytes, both approaches are valid (which one do you think would be a better fit if your payload size fluctuates significantly?). Also, it’s recommended to measure sustained throughput, or a running average over a sufficiently long period of time.
Some throughput metrics can be used out-of-the box, for instance Event Hub’s Incoming Messages and Outgoing Messages. One way of measuring pipeline’s latency is via custom metrics in Azure Application Insights. All that’s needed is a couple of extra lines of code to calculate the difference between EventHub’s enqueuedTime and your code’s invocation timestamp and push this value out to AppInsights.
Here’s a half-decent sample SLA statement for a streaming scenario: 99% of all events arriving in the Event Hub must be processed within the hot path within 1 second. The maximum processing delay should never exceed 30 seconds. The pipeline should be capable of processing up to 15K messages per second 24/7, provided the message payload does not exceed 1KiB
Let’s empirically measure how the throughput and latency is impacted by the number of EH partitions, batch and prefetch sizes. Since it will require quite a few iterations, let’s automate the process via Terraform.
For every iteration let’s create a 20 TU Standard Event Hub and an EH-triggered Azure Function. Let’s also provision a load generator to flood the EH with messages saturating the ingress. Our goal is to measure the resulting Throughput (T) and Latency (L) for various combinations of:- P: the number number of Partitions in the Event Hub- B: The maximum Batch size (maxBatchSize setting in host.json) and- R: The pRefetch Limit (prefetchCount setting in host.json)
Each iteration should be independent with a single EH per EH Namespace, one consumer group per EH. 20 Throughput Units should allow us to achieve a theoretical 20MB per second on the ingress (event publisher) side and 40MB per second on the consumer (Azure Function) side. In addition to the throughput allowance, the Throughput Unit also limits the number of ingress/egress events, each TU granting you ~1K ingress and ~4K egress events per second. The relationship between an ingress event and an actual message can be subtle and probably warrants a separate post. In short, if you’re sending events in batches one ingress event does not equal one message.
The number of Event Hub partitions does not define its theoretical throughput. There is no specific throughput limit on an Event Hub partition (…anymore)
Azure Function is running in a Consumption Plan on Windows performing a null operation on every message (it could’ve been anything) and reporting the processing latency per batch of messages. You can report per individual message within each batch but you should assess how much more telemetry will be generated and adjust sampling accordingly.
If interested, look through the code and explore the results in Power BI or read on. To produce the Power BI report, I’m joining a couple of Event Hub metrics stored in Azure Log Analytics with the custom metrics in Application Insights via a cross-resource query in Kusto which in turn can be easily consumed in Power BI.
I’d call this ‘expected behaviour’: the pipeline’s latency (top chart) is in the hundreds of seconds in the beginning, as the Azure Function is gradually scaling itself out (bottom chart) until eventually it reaches 32 instances, which is precicely one consumer per partition per consumer group.
Remember, since we’re dealing with an Event Hub, it’s parallel consumers, not competing consumers
Once the Function is running at full throttle it eventually catches up with the backlog of messages in the Event Hub and the latency drops to around 220ms towards the end of the 15-minute run. The 5-min 95P and 99P latency values read 3.5 and 3.88 seconds respectively, if I had it running for longer these values would’ve dropped further.
The middle chart shows the average message batch that my Azure Function was being triggered with. Interestingly, it stays well below the maxBatchSize=64 which indicates some spare capacity in the processing pipeline.
What if we increase maxBatchSize to 512?
The throughput is very similar to the run above but the pipeline catches up much quicker (understandably, look at the batch sizes in the beginning!). The 5-minute P95 and P99 latency measurements are both 180ms which is pretty low if you ask me.
Okay, what if we keep reduce the number of Event Hub partitions to 4?
Ouch. The pipeline does not seem to ever catch up: look at the growing latency and the difference between IncomingMessages (that is, how many messages were enqueued in the Event Hub) and OutgoingMessages (how many messages are processed by the Azure Function).
Why? Not enough parallelism. There’s only 4 partitions, hence 4 consumers for the same 20 Throughput Units. The two options to consider here are:
There’s 57 more runs published to Power BI here for you to have a play.
There’s (roughly) four areas to look at to improve your pipeline’s throughput and reduce latency: Event Publishers, Event Hub, Event Hub trigger settings and Azure Function code.
Some additional reading here and here
Thanks for reading and until next time!
Enterprise Solutions Architect @ AWS. Opinions shared are my own.
425 
6
425 
425 
6
Enterprise Solutions Architect @ AWS. Opinions shared are my own.
"
https://medium.com/@jeffhollan/in-order-event-processing-with-azure-functions-bb661eb55428?source=search_post---------22,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Feb 9, 2018·6 min read
⚡️ UPDATE: Azure Service Bus now supports sessions, so you can do in order queue processing with service bus queues and topics in addition to Event Hubs listed below. Service Bus Sessions provide the added benefit of reprocessing failures individually instead of in batches. While Event Hubs can guarantee order as show below, if a partition lock is lost the in-order batch could resume in another instance causing duplicates. Consider using Service Bus Sessions if this is an issue. Both provide at-least-once delivery guarantees.⚡️
I met with a customer this week who was interested in using serverless but didn’t know if it would work for their business requirements. Here was the scenario:
Thousands of events are being sent from hundreds of different users at any given time. Each of these events needed to be processed and analyzed, and the processing must be done in order for each user.
They wanted the rapid development and abstracted infrastructure serverless brings, but their question to me was “Is there a way to guarantee processing events in a specific order when serverless can scale and be running on multiple parallel instances?” Below are details on how to do exactly that.
The first question to be answered is how to get the events to the Azure Function to begin with. HTTP or Event Grid wouldn’t work, as there is no way to guarantee that message 1 will arrive and be processed by your function before message 2 arrives and is processed. In order to maintain processing order, order needs to be persisted somewhere that my Azure Function can pull from. Queues work, but if using Azure remember that Azure storage queues do not guarantee ordering so you’d need to stick with Service Bus Queues or Topics. For this specific scenario though, we’ll focus on Event Hubs. For details on how to do in order processing with Service Bus queue/topics, see my other blog.
Azure Events Hubs can handle billions of events, and has guarantees around consistency and ordering per partition. For the scenario above, the partition was clear: each users events would need to go to the same partition so consistency for all that users events would be guaranteed. This would be the first step in enabling in-order processing.
Now that the events are persisted in order with Event Hubs, I still have the challenge of processing them in order. This may seem extra complex with serverless technology, as a serverless function can scale across many parallel instances which could all be processing events concurrently. However there are some behaviors that allow us to enforce order while processing.
First, Azure Function’s event hubs trigger uses event hubs processor hosts to pull messages. The processor host will automatically create leases on available partitions based on how many other hosts are provisioned. There is another important constraint that works in our benefit in this case: a single partition will only have a lease on one processor host at a time. That means multiple function instances can’t pull messages from the same partition. This also means your function will only ever scale to n+1 instances, where n is the number of partitions for the event hub. It’s n+1 because we keep one running and ready to immediately start pulling messages from a partition if an instance goes away for any reason (you aren’t charged for this overprovisioning though — because serverless is awesome).
So as long as I know the events that need to be kept in order are in the same partition, I can also guarantee that a single instance of Azure Functions will receive those messages at one time. But are those messages preserved throughout execution? Let’s run some tests.
For my first test I have a simple Azure Function that will trigger on a message from Event Hubs and add it to an ordered list in a Redis cache. I will publish 1000 messages — in order — for 100 users in parallel (with each user ID being a partition key). I have 20 partitions in my event hubs, but that’s ok. Multiple users events may land in the same partition, but I still know all events in a single partition have order preserved, so it all works out.
After my functions trigger and process all messages, I can then compare the order in Redis cache to the order of the messages published.
Here’s what the first Azure Function looks like — I’m just going to pull in a single message and immediately push it to the end of the Redis list.
After sending 1000 in-order messages for 100 users in parallel, here’s what one of the user lists looked like in Redis:
Notice anything? While the order is pretty close, it’s not perfect. You can see a few examples above where one message got processed before or after its order. So what’s happening here? The answer is that even though a single instance of a function app gets all messages in a partition, it may process them concurrently. Specifically, when the Azure Function’s event hubs trigger grabs messages, it pulls them off in a batch (as many as Event Hubs will give it, usually around 200 messages but can be more). So even though the messages are initially pulled in order, they are then processed concurrently causing some items to complete before others.
To verify this, I configured the “maxBatchSize” for my function to 1. Now my function will only ever pull a batch of 1 message. After running my second test with this setting, I did in fact get perfect order, but it came at a cost. The function took a long time to burn down all of the messages, because it was having to go fetch each message individually from event hubs. Fetching each of the 100,000 total messages individually took a while to process. Luckily, there is a better way.
I can take advantage of the throughput benefits from pulling batches from event hubs, and preserve order. The trick is simple: instead of passing a single event hub message into a function execution, I’ll pass in the entire ordered batch. I can then process each message in order, without the costly call to event hubs for each message. Here’s what the in-order batched function looks like:
Now when running the same test (sending 1000 messages in-order for 100 users in parallel), the results were just what I wanted. My function was able to process the 100,000 total messages in a few seconds, and when checking the results in Redis cache? Perfect order, for each of the 100 user’s lists. Event Hubs is preserving each user event order in partitions, functions pulls in ordered batches from available partitions (scaling out as much as possible), and maintains that order while processing the batch.
⚠ It should be noted that in the case of failure or event hub lease loss, it’s possible that messages are reprocessed from the same batch again. In general with distributed computing you should work under the assumption of “at-least-once” delivery. If, for example, a failure was hit on message 223 and the batch retried, I could get: “220,221,222,223,220,221,222,223,224.” While ordering is preserved, the batch behavior is important to note. One recommended alternative if this behavior matters is to use Service Bus Sessions with Azure Functions where failures can be handled individually rather than in batches. More strategies on reliable message handling can be found here. More info on how to handle resiliency can be found here — in general it’s best to assume at-least-once and duplication may occur.
Hopefully this example has been helpful. I have the entire working solution (including the console app that sends the 1000 events for 100 users) in GitHub here.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
See all (133)
511 
11
511 claps
511 
11
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/clean-architecture-with-partitioned-repository-pattern-using-azure-cosmos-db-62241854cbc5?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
"
https://medium.com/bb-tutorials-and-thoughts/200-practice-questions-for-azure-data-dp-900-fundamentals-exam-ea2446ee3a0?source=search_post---------24,"There are currently no responses for this story.
Be the first to respond.
The exam DP-900 is an Azure Data Fundamentals exam. According to the exam guide here, Candidates for this exam should have a foundational knowledge of core data concepts and how they are implemented using Microsoft Azure data services. Candidates should be familiar with the concepts of relational and…
"
https://medium.com/@hakant/api-versioning-with-swagger-azure-api-manager-and-asp-net-core-an-introduction-a403d498eb5e?source=search_post---------25,"Sign in
There are currently no responses for this story.
Be the first to respond.
Hakan Tuncer
Aug 5, 2018·5 min read
API versioning has no “right way” and I am fairly convinced that API evolution is usually a better option for both API authors and clients of those APIs. Evolving APIs smoothly without breaking any existing clients and making it easy for them to consume new features seems like the best of both worlds. GraphQL, for example, chooses API evolution over versioning.
At frequent and incremental release cycles it’s usually easy to evolve an API without introducing any breaking changes, until it isn’t. At one point or another evolving an existing endpoint can have costly underlying implications on the codebase. I’ve seen many businesses struggling to find a stable vision for their product which can lead to large shifts in business direction which means large changes in APIs — and remember business always wins. If some kind of versioning eventually becomes a necessary evil, how and when should one version ?
The answer is a range of possibilities. One that goes from the most conservative approach to more liberal ones.
Microsoft, for example, doesn’t discard API evolution but dictates that all APIs support explicit versioning:
All APIs compliant with the Microsoft REST API Guidelines MUST support explicit versioning. It’s critical that clients can count on services to be stable over time, and it’s critical that services can add features and make changes.
As an organization, if you’re following Microsoft REST API Guidelines, your APIs should support versioning already from the get go.
Next question is when to increment the version number. Microsoft says:
Services MUST increment their version number in response to any breaking API change.
This part sounds very trivial at first. But what is a breaking change ?
Teams MAY define backwards compatibility as their business needs require.
Clear examples of breaking changes:
* Removing or renaming APIs or API parameters
* Changes in behavior for an existing API
* Changes in Error Codes and Fault Contracts
* Anything that would violate the Principle of Least Astonishment
So backwards compatibility as a concept contains some clear objective areas as well as some subjective areas that are highly dependent on the nature of a business. It turns out that Azure follows a more conservative approach following Hyrum’s Law and even defines the addition of a new JSON field in a response to be not backwards compatible. Office365 on the other hand perfectly allows it.
So one interesting key takeaway here is that even if you didn’t change a single thing in your API interface (URL, parameters, payload etc.) but radically changed the behaviour of an existing endpoint, you have introduced a breaking change. So it’s not always about mechanical changes but also about what that change means in your business domain.
One of the best ways to version a .NET API is to go to aspnet-api-versioning repo and use a package that’s suitable to your Web API version. These packages make it very easy and elegant to introduce versioning semantics to an API.
For my experiments I’ve been using the sample project based on ASP.NET Core.
Azure API Management is an API Gateway Service in its essence but it can do a lot of things when properly configured. For organizations that are creating and managing large number of APIs, Azure API Manager can serve as a central point of discovery and governance.
Microsoft recently added versioning capabilities to API Manager and is supporting a few main versioning schemes such as url path, http header and url querystring. So now developers can add explicit versioning to their version-less APIs or publish their versioned APIs through API Manager for other benefits.
Such a service can be invaluable for an organization but for developers and operations it’s yet another moving part to keep an eye on and maintain. All of a sudden, deploying your APIs isn’t sufficient anymore and now you also have to properly publish and configure them through Azure API Manager at each deployment cycle. In the long run, integration with the API Manager can only succeed if all these steps can be automated as part of a continuous delivery pipeline.
One nice thing about API Manager is that it can automatically import APIs that have implemented the OpenAPI Specification. That makes a big part of the automation a piece of cake. One bad thing about API Manager is that it’s a fairly young service in Azure with some rough edges, insufficient documentation and inconsistencies around automation. 80% of what you need is low hanging fruit, remaining 20% makes you cry — and you need 100% to be successful with it.
The OpenAPI Specification, formerly known as the Swagger Specification, comes with a set of tools & libraries that help API developers to document their APIs.
Here is a complete definition, emphasis mine:
The OpenAPI Specification (OAS) defines a standard, programming language-agnostic interface description for REST APIs, which allows both humans and computers to discover and understand the capabilities of a service without requiring access to source code, additional documentation, or inspection of network traffic.
Yes I know OpenAPI Specification doesn’t support Hypermedia and thus it can’t be REST but let’s accept the fact that most of us are building “RPC over HTTP APIs” and move on to the benefits.
Swashbuckle.AspNetCore repo is the home of Swagger tools for documenting API’s built on ASP.NET Core. When you annotate your action methods using the constructs from this library you get a human readable interactive documentation as well as a machine readable JSON representation of you API surface. If you’re running multiple versions of your API side by side Swagger got you covered as well. You can represent each version of your API docs separately.
Here’s how a human readable Swagger API doc looks like, notice the dropdown that let’s the reader switch to different versions.
And here is the same API spec (v3) in computer readable format:
After this introduction, in the next follow up post I’ll talk about a frictionless developer experience for versioning .NET APIs and automatically publishing those versions to Azure API Manager. I’ll share a PowerShell script that can run as a VSTS release step, automatically detect all available versions of an API and publish them to Azure API Manager.
Thanks for reading and until then, take care!
Update: Second part is published. You can find it here.
Originally published at www.hakantuncer.com on August 5, 2018.
Full Stack Software Developer & Architect focusing on .NET, Javascript, Angular, Node.js and Azure | Amsterdammer
See all (135)
353 
353 claps
353 
Full Stack Software Developer & Architect focusing on .NET, Javascript, Angular, Node.js and Azure | Amsterdammer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/medialesson/best-way-to-host-a-single-page-application-spa-in-microsoft-azure-3e70cbd075c3?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
I frequently need to deploy static websites (e.g. our company website https://www.media-lesson.com) or Single Page Applications and I’m always looking for ways to improve cost and time-to-deploy.
Microsoft just recently announced public preview of static website hosting for Azure Storage adding yet another option to host a single page application (SPA) on Azure.
Just before Microsoft added back support for Proxies in the Azure Functions Runtime 2.0.11776-alpha on May 14th providing a way to host a static website in Azure Storage and forwarding traffic through a Azure Function proxy route.
Both new options add to the traditional way of hosting of (static) website in an Azure App Service. There are even more options of hosting websites in Azure e.g. using Containers, Docker, Kubernetes, Virtual Machines etc. but I will keep the focus on easy and cost efficient Deployments of single page applications.
And since we’re evaluating hosting options it’s also worth comparing manual and automatic deployment using Visual Studio Team Services (VSTS) for the mentioned services.
So let’s answer these questions to help decide which service to use when hosting a Single Page Application:
So lets start this experiment by creating a simple SPA based on Angular using the Angular CLI as a test app to deployment and test on the 3 different services: ng new testapp --routing Please note that I’m directly adding the routing feature to app using the --routing parameter. I find this an important aspect to test when looking at hosting options as routing can be a configuration challenge in some environments like App Service because we need to configure the web server to allow route handling by our client app instead of server side.
To fully test routing we also need to have some sample routes. Therefore lets add a home and a about component to our app using the CLI: ng generate component home and ng generate component about. Next we need to have two routes in our app-routing.module.ts to allow navigation between the two components:
Finally some navigation buttons to allow the user to navigate between the two components in our app.component.html:
Let’s build our app locally in preparation for manual deployment using ng build --prod which gives us this little folder of build artifacts:
Of course we also want to dry automatic deployment as part of a continuous deployment process. Therefore let’s push our app to a VSTS repository and setup a build definition with these 3 tasks:
npm install
npm build
Publish build artifact from path dist to app
Azure App Service is a PaaS (Platform as a Service) offering and the classic way of hosting web content on Azure. Using app service we need to take care of configuration and scaling of our service by hand. To test this let’s create a new app service in the Azure Portal. I choose the S1 tier for this test as it’s the entry level tier for production apps.
Because of it’s nature in being a PaaS offering, developers to take care of the configuration of the underlying web server (in case of Windows it’s IIS). This means to enable routing inside our Angular app we have to provide a web.config file with instructions for the web server how to handle routing or host the angular app inside a ASP.Net Core 2.1 app with the SpaServices middleware enabled. Here a simple sample of a web.config with SPA routing:
For manual deployment let’s just use FTP to upload the content of our dist folder and the web.config to the wwwroot folder of the app service. The FTP credentials can be downloaded in the Azure portal in the overview tab of the app service by clicking on “Get publish profile”.
Setting up a release definition in VSTS to deploy the app to app service is pretty straight forward as we just need an empty release definition with one task:
To configure an FTP service endpoint let’s click on “Manage” and add a new generic endpoint with the same credentials used for manual deployment.
In this variant we’re going to use Azure Storage as an inexpensive store for our static files and an Azure Function App with proxies to serve the SPA to the user. This allows us to scale automatically (when using the consumption plan), combine our SPA with other functions (e.g. API calls) under one domain and manage our app in a serverless fashion.
So we need to create a Function App and a Storage Account (automatically created when creating a Function App). Next we need to create a blob container named “web” in the storage account where we will deploy our files to later.
The routing magic now happens in the way we configure proxies to forward request from our function app to the storage account. Therefore we will need 2 proxies:
The first one to forward requests to the base url to our index.html
and a second one to forward requests for other static assets like javascript files or stylesheets to their location in the storage account.
To manually deploy click on “Containers” in the storage account in the Azure portal and select the “web” container that we just created. Now let’s upload the files from the local build.
To test this we need to create a second release definition in VSTS with the empty process template and add the AzureBlob File Copy task:
This is the newest option that just recently went into public preview. The idea is to use storage to host the SPA because a real web server is not needed to serve just static files qualifying this variant for the buzzword “serverless”. So let’s create a new storage account (general purpose v2) and then enable the static website feature:
This is all configuration we need. Storage scales automatically and routing also works out of the box. The primary endpoint is our SPA’s url. Nice!
To manually deploy click on “Containers” in the storage account in the Azure portal and select the “$web” container (which is created automatically when enabling static website). Now let’s upload the files from the local build:
And that’s it!
To test this we need to create a third release definition in VSTS with the empty process template and add the AzureBlob File Copy task:
Make sure to select version 2.* (preview) otherwise the deploy wail fail because the “$” character in the container name was not allowed previously.
To get an idea about the performance let’s run some artillery.io load tests on our 3 deployments. Here are my results when firing 1,000, 10,000 and 100,000 requests:
There’s a drastic performance benefit using Azure Storage and avoiding the web server component while Functions and App Service run at a comparable speed. This makes sense as both share the same underlying infrastructure. I’m a bit surprised that the Function app is running even slower that the App Service.
I find this even stranger when comparing the total runtimes to complete the 100,000 requests test. This took around 46 seconds to complete to Azure Storage and 14 minutes and 27 seconds for App Service and even 17 minutes and 2 seconds for the Function app. I would have expected the Function app to gain speed over time as I was expecting it to scale horizontally automatically. Which doesn’t seem to work in this scenario.
So we have a clear winner in this discipline: Storage is really fast!
Getting the cost right is tricky as all have different billing models. Here’s my example calculation for monthly costs for 100.000 requests/day in the West Europe region (which I’m not 100% sure it’s complete and accurate!):
1x S1 Instance in West Europe running Windows (1 core, 1.75 GB RAM, 50 GB Storage) = 61.56 €/month
Our SPA consists of 5 files * 100,000 requests/day * 31 = 15,500,000 requests per month total. The total size of our app is roughly 0,33 MB which accounts for 0,98 TB of outbound traffic per month total. The minimum execution time is 100ms (which would be enough for our proxy purpose) for we can handle 10 requests/execution second.
1x Function App with 1.550.000 executions with a execution time of 1s each per month (each less than 128 MB of memory) for handling the requests going through the proxies = 0.17 €/month
1x Storage General Purpose V2 Block Blob Storage account with LRS redundancy and hot access tier. Capacity 1 GB (we actually need just 300kb but that’s the smallest size available), 100 Write Operations, 100 List operations, 15,500,000 read operations and 0,98 TB data retrieval = 5.64 €/month
Total: 5.81 €/month.
For the storage we just take the same calculation than above:
1x Storage General Purpose V2 Block Blob Storage account with LRS redundancy and hot access tier. Capacity 1 GB (we actually need just 300kb but that’s the smallest size available), 100 Write Operations, 100 List operations, 15,500,000 read operations and 0,98 TB data retrieval = 5.64 €/month
Hosting an SPA in Storage should be a no-brainer for dev, test and staging situations as it’s fast to setup and in most cases even free for those scenarios. I didn’t find any disadvantages yet so will dive a bit deeper and see if we can also use it in production.
Please feel free to provide feedback.
We help our customers design, architect, develop and…
548 
18
548 claps
548 
18
We help our customers design, architect, develop and operate modern, intelligent, beautiful and usable apps on any platform powered by the Cloud, IoT and AI.
Written by
CEO @ medialesson. Microsoft Regional Director & MVP Windows Development. Father of identical twins. Passionate about great User Interfaces, NYC & Steaks
We help our customers design, architect, develop and operate modern, intelligent, beautiful and usable apps on any platform powered by the Cloud, IoT and AI.
"
https://towardsdatascience.com/how-to-decide-between-amazon-sagemaker-and-microsoft-azure-machine-learning-studio-157a08af839a?source=search_post---------27,"Sign in
Steve Dille
May 16, 2019·13 min read
I recently published a walk-thru of Microsoft Azure Machine Learning Studio (Studio) https://towardsdatascience.com/how-microsoft-azure-machine-learning-studio-clarifies-data-science-8e8d3e6ed64e and was favorably…
"
https://koukia.ca/build-your-first-web-api-with-f-giraffe-and-host-it-on-azure-cloud-1d9dc07dc248?source=search_post---------28,"Today I am going to talk to you about programming Web APIs using F# programming language.
F# has been around for a while now, but what I would like to talk about is F# in .Net Core because it has a lot of cool features specially for Web Programming and building Web APIs and with playing around with it building some Web API, so far I personally…
"
https://medium.com/caf%C3%A9-con-leche/azure-7c827dbe2084?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
Azure, it means bright blue
a beautiful azure sky, not a cloud in sight
Azure means the same but she sounds so different
she has more feel than blue
more life
more harmony
more character, the way she dances right off the tongue.
"
https://medium.com/cloudskills/build-your-first-ci-cd-pipeline-using-azure-devops-b9dc930cefe1?source=search_post---------30,"There are currently no responses for this story.
Be the first to respond.
Continuous integration and continuous delivery (CI/CD) are considered by most to be the backbone of DevOps. Things start to get really interesting when you combine these practices with programmable infrastructure and a suite of services that allow you to automate the entire lifecycle of an application.
The goal with this guide is to give you a practical example of what that all looks like when you’re building, testing, and deploying applications with Azure DevOps Services. I’ll walk you through the end-to-end process of building a fully automated build and release pipeline for a Node and Express application. We’ll use Azure DevOps Services to create the CI/CD pipeline and Azure App Service for deploying to development/staging and production.
To follow along, you’ll need a GitHub account and Azure Subscription. The demo application is open source, so the Azure DevOps pipeline infrastructure we build will be covered under the free tier.
The first step is to navigate to dev.azure.com and sign in to Azure DevOps. If you’ve never done this before, you’ll need to create a new organization.
You need to have at least one organization, which is used to store your projects, structure your repositories, set up your teams, and manage access to data. The guidance from Microsoft is to keep things simple and start with a single organization. For more advanced scenarios, take a look at plan your organization structure in Microsoft’s documentation.
After clicking on continue, you may end up with an organization name that was generated at random. You can change this as shown in Figure 2.
Simply navigate to Organization Settings > Overview and update the name.
I wanted to demonstrate an application that was somewhat realistic but not overly complex for this walkthrough. The Node and Express app is a simple website for a fictitious company. This app uses Express and Handlebars to serve up a few common pages you’d see on any company website. Also included are some unit tests that ensure those routes are working and serving up the right content.
You can head over to my GitHub account to fork this repository.
Next, we can move on to deploying the infrastructure to support both development and production deployment slots using Azure App Service.
We’re going to use an Azure Web App for Linux resource to power our Node and Express application. We’ll set things up so our CI/CD pipeline can build and deploy the app into a development/staging slot. Then we’ll set up up a manual approval into the production slot.
We’ll use an Azure Resource Manager (ARM) template to build the App Service infrastructure.
Navigate to the node-express-azure repository you forked in the previous step. You’ll see a “Deploy to Azure” button about halfway down the screen.
Clicking the “Deploy to Azure” button will redirect you to the Azure portal as shown in Figure 5.
Notice that you’ll need to set a globally unique hostname for your web application, along with a name for the new app service plan. I’d recommend deploying these resources into a new resource group. That way when you’re done with this walkthrough, you can clean up the Azure resources easily by deleting the resource group.
Click “Purchase” to launch the template to agree that you’ll have to pay for the App Service resources that this template deploys on your behalf.
After you launch the template you should see a successful deployment message, and you should have a new resource group similar to the one shown in Figure 6. Notice that there is an App Service Plan, a web app that represents the production deployment slot, and a slot for development called “dev”.
Quick side note about the ARM template: the Deploy to Azure button references the azuredeploy.json ARM template in my GitHub repository. If you want to update the template, update the version in your own repo, and don’t forget to change the target of the button in the source of your README.md file.
We’re ready to move on and set up a build pipeline in Azure DevOps. Head back to dev.azure.com and create a new project inside your organization. Use the settings shown in Figure 7.
After clicking on the “Create project” button, you’ll see a summary page for the project. Navigate to Pipelines and click on Builds as shown in Figure 8.
Next, click the button to create a new build pipeline. You’ll be prompted to choose a repository. Select GitHub. You’ll see a screen like the one in Figure 9 where you’ll need to authorize the Azure DevOps service to connect to your GitHub account on your behalf. Click Authorize.
After your connection to GitHub has been authorized select the node-express-azure repo that you forked in the first step. You should end up seeing a “New pipeline” screen like the one shown in Figure 10.
The new pipeline wizard should recognize that we already have an azure-pipelines.yml in the repository. This file contains all of the settings that the build service should use to build and test our application, as well as generate the output artifacts that will be used to deploy the app later in our release pipeline.
After you click “Run” to kick off your first build, you should see a screen like the one shown in Figure 11.
Notice that a lot went on with the build. The service used an Ubuntu 16.04 build agent to grab the code from GitHub, installed our development dependencies, and then ran our unit tests to validate the application. Finally, the code was bundled into an output artifact and published so we can use it as an input artifact for our upcoming release pipeline.
Click on the release button at the top of this screen to create a new release pipeline.
When you get into the release pipeline screen, you’ll need to select a template. For this scenario, we are going to choose “App Service deployment with slot”.
Click on the apply button to create the new deployment stage within the release pipeline. On the next screen, you’ll be able to configure this stage. Change the name to “development” as shown in Figure 13.
While on this screen, click on the link that says “2 tasks” inside your development stage. This will take you to a screen where you can configure the deployment task. Make sure you fill out all the fields as shown in Figure 14.
Next, highlight and remove the second deployment task for swapping the slots.
Finally, click Save.
Head back over to the “Pipeline” tab at the top left of the screen. Inspect the deployment triggers for the artifacts as shown in Figure 17.
Notice that continuous deployment is enabled by default. Going forward, each new build will trigger a deployment to our development slot in Azure App Service.
First, let’s trigger a manual release.
Click the “Release” button on the top right of the release pipeline screen and create a new release. Use the settings as shown in Figure 18.
Click on the “Create” button to deploy the application to the development deployment slot. You should see a successful status in the properties of the release.
Navigate to the public URL of the “dev” deployment slot in your web browser. The hostname will have “-dev” appended to it. For example, my web app is named “node-express-demo” and the “dev” deployment slot URL is https://node-express-demo-dev.azurewebsites.net.
You should see the sample web application when you visit the “dev” slot URL. The production slot will show the default Azure App Service splash page since it is virtually untouched at this point. Let’s change that in the next step.
Head back over to the Azure DevOps portal and go to Pipelines > Releases. Click on the “Edit” button to modify the pipeline. Highlight the Development stage and click the dropdown to clone the stage.
Rename the stage to “Production”.
Next, click the pre-deployment conditions button for the Production stage. Enable pre-deployment approvals and add yourself as an approver.
We’re doing this because we don’t want automated deployments going straight into production. We’re not building a continuous deployment pipeline for production. We’re building continuous delivery pipeline.
Continuous delivery is a process that ensures our application is production ready. When we are doing a scheduled deployment we can do so with confidence since we know the application has been through a pipeline of tests before-hand.
Next, click on the “task” link on the Production stage. We need to modify this task so that it does not deploy our code into the development slot.
Simply uncheck “slot” and this will infer that the production slot of the web app should be used during the deployment. Click save when complete.
Navigate to your GitHub account and into the views folder of the demo application. Edit the index.handlebars file to update the app to version 2.0.0.
Committing the change in this repo should automatically trigger a build, perform our tests, and publish a deployment package. We can confirm this by reviewing the build status.
After the build, you should see a new release. The development stage should be green indicating that the deployment succeeded. The production stage should be blue and show that it’s pending approval.
Click approve to kick-off the production deployment.
Go back to your pipeline view and you should see the deployment to production succeeded.
Finally, head over to the web app URL for the production slot to confirm the correct version is running.
You should see version 2.0.0 on the homepage.
Have you ever seen those build pass/fail badges when browsing projects on GitHub? They’re really cool because you can tell at a glance if the code is still working or if it’s old and busted.
Let’s set up a badge for this project.
Go back to your Builds section and click the status badge button.
Copy the markdown code for the status badge.
Now, go back to GitHub and modify the README.md file in your node-express-azure repo. Paste the markdown you copied from the status badge page.
Commit the change and view the README. You should see a build passing status icon.
If you’re still reading after all this time, respect! You now know who to build a CI/CD pipeline on Azure.
You can simply delete all the resources to clean things up. Delete the resource group you created for this project, delete the Node demo project in the Azure DevOps portal, and delete the GitHub repo that you forked from my account (unless you want to keep a copy).
Isn’t this awesome stuff? There’s so much more. For now, check out these resources to dive deeper.
Originally published at cloudskills.io.
The Official Publication of CloudSkills.io
441 
3
441 claps
441 
3
Written by
Twenty-year tech industry veteran, author, educator, and entrepreneur. Founder of https://cloudskills.io
The Official Publication of CloudSkills.io
Written by
Twenty-year tech industry veteran, author, educator, and entrepreneur. Founder of https://cloudskills.io
The Official Publication of CloudSkills.io
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/increasingly-strong-and-serious-competition-from-switzerland-for-amazon-aws-microsoft-azure-co-7924ef0e0662?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
Initiated by Netkom IT Services GmbH, a Swiss based IT and cloud provider with a proven track record of 17 years of successful operations in various countries and one of the cloud pioneers in Europe, n’cloud.swiss AG provides high quality services and products in the field of cloud computing. Founded in 2001 by André Matter, Netkom IT Services GmbH developed early through various consulting mandates and IT projects as a specialist for IT strategy and its implementation in the operational IT infrastructures and organization of the respective customers. The latter were well-known large companies from Switzerland and abroad such as Citibank or ABB Turbo Systems. In the following years, further core competences in the fields of transformation and digitization have been established.
Swiss made alternative to Amazon AWS, Microsoft Azure, Google Cloud and other major cloud providers
Since small and medium-sized enterprises are generally unable to afford one’s own datacentre infrastructures, Netkom built its own cloud platform for this segment already in 2009 under the name n’cloud. Today, the n’cloud.swiss platform is considered as a serious “Swiss made” alternative to the major cloud providers like Amazon AWS, Microsoft Azure and Google Cloud Platform. With the expertise and experience as a specialist for IT strategy and its implementation in the operational IT infrastructures and organization of the respective customers, n’cloud.swiss AG is expanding on the international level and aiming to provide their products and services to a worldwide market range. Nowadays, cloud computing is the promise of having a modern and state-of-the-art IT infrastructure without the need for substantial capital investments and personnel increases. The cloud market revenue is expected to double in the next three years reaching up to 162 billion USD. Here is where the huge potential of n’cloud.swiss AG of establishing as a Swiss/European alternative to the leading cloud providers becomes clear.
n’cloud.swiss heralds a new era in cloud computing
n’cloud.swiss stands for a new era in cloud computing. In contrast to many cloud providers, this innovative cloud platform is capable of addressing and serving all cloud deployment requirements such as private, public, hybrid and community cloud. In addition, all cloud service models from Infrastructure- over Platform- to Software-as-a-Service models are included within this same product. The advantage for customers range from agility, flexibility and adaptability to a maximum amount of freedom to build their cloud according to their needs.
In addition, all cloud service models from Infrastructure as a Service (IaaS), over Platform as a Service (PaaS) to Software as a Service (SaaS) are part of the platform. All this is surrounded by an innovative internal n’cloud.swiss application catalogue. Within the latter, n’cloud.swiss offers more than 142 applications from 30 different IT categories “free and ready to go” as well as the opportunity to upload also other development applications and tools easily. Along personal support and competitive pricing models, API connectivity for easy and fast transfers of existing developments from or to other major cloud platforms award n’cloud.swiss a unique selling point and a competitive advantage.
n’cloud.swiss features demo
The establishment of n’cloud.swiss across the globe with the launch of an ICO
The number of ICO launches nowadays has reached a level making difficult for investors to stay on top of things. More than ever, ICOs of established companies are in demand given the fact that the number of scams is high as well as the number of ICOs leading to projects that fail. The ICO of n’cloud.swiss is truly the ICO of a proven business case. The particularity here clearly is the fact that investing in this ICO signifies investing in a real company distinguished by a successful record of accomplishment and existing customers. However, the question arises why such a company plans to launch an ICO. The target for n’cloud.swiss is to achieve a substantial market share in the global cloud market for IaaS, PaaS, SaaS services in the next 5 years. Consequently, as a decisive step towards its establishment as the undisputed alternative to the major cloud providers, n’cloud.swiss ICO has been launched to successfully expand and rollout the existing business operations into 60 additional countries around the world. The n’cloud.swiss ICO is divided into two main stages. While the Pre-ICO will be held from April 14th to April 29th, 2018, the main ICO event will take place from April 30th to July 8th, 2018. During these two events of ICO, investors can buy tokens. As the first and only company in the world, n’cloud.swiss AG is offering to investors the unique service of managing their tokens. The idea behind is to open up the doors to unexperienced ICO investors and allow them to benefit from the company’s success.
US Cloud Act or how foreign laws do not matter anymore
While the U.S. Congress recently approved the CLOUD Act for data stored overseas and strictly speaking opened a back door for the FBI, CIA, and NSA to intercept virtually anyone without any court order, the need for an alternative to the major cloud providers becomes probably greater than ever. A market that is dominated by four American (Amazon AWS, Microsoft Azure, Google Cloud Platform and IBM Softlayer) and one Chinese (Alibabcloud), definitely presents numerous possibilities for n’cloud.swiss AG. “Swiss made” high quality standards in terms of security and reliability in setup and operation of fail-safe cluster-systems can present for the company a valuable competitive advantage. On the other hand, investors in this ICO will be able to join a company with a huge potential, contribute to its international expansion and establishment and benefit from a success story in the making.
#BlackLivesMatter
404 
1
404 claps
404 
1
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Dynamic and data-driven marketing leader. Author, Mentor & Speaker. Top 50 Global Thought Leader. See www.yahyamohamedmao.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/hackernoon/azure-container-instances-vs-aws-fargate-3216607f63f4?source=search_post---------32,"There are currently no responses for this story.
Be the first to respond.
Last week, during their re:Invent 2017 extravaganza, AWS announced their new container service Fargate. Fargate is AWS’s clusterless/serverless way of running containers. Fargate’s announcement was joined by the announcement for AWS managed Kubernetes clusters, dubbed AWS EKS and available only through sign up somewhere in 2018.
In a rare occurrence, it seems like AWS is playing catch-up with Microsoft’s Azure. Azure has had Azure Container Instances (ACI) in preview for the last months and has already rolled out managed Kubernetes (and DC/OS) as part of the Azure Container Services…ehm…service.
Fargate is brand new and currently only available in the us-east-1 region, but it’s still fun to compare it to Azure Container Instances as they seem to both target the same audience and provide the same value. But looks can be deceiving, as it turns out they are quite different beast! Shock, horror!
Let’s first see how easy it is to get from zero — where zero means you have an AWS and/or Azure account with appropriate rights — to a running container exposing a simple web page to the internet on both services.
ACI has no real visual setup wizard or GUI. Luckily the Azure CLI is pretty nice and getting a container running is really dead simple: two bash one-liners.
After this, a quick az container show informs me my service is available on address 52.191.116.216:3000. And yes, it also actually worked:
Let’s start with a minor niggle: according to AWS, Fargate “allows you to run containers without having to manage servers or clusters”. However, the first step in running a Fargate container is…creating a cluster! Ok, it is very quick and you don’t have to manage it, but still.
In general though, setting up a first running container on Fargate is a lot more involved than ACI. Using the AWS GUI, you go through the following steps, where each step can have a ton of options, drop downs and input fields.
What is immediately clear is that Fargate is completed embedded in the current Amazon Elastic Container (ECS) service. You literally need to select it as a “launch type”, in contrast to the EC2 version.
This means Fargate comes with all the configuration and setup flexibility/bagage attached. Not an issue an sich but to me at least AWS ECS was always cumbersome to set up and configure.
After finishing this setup procedure, the dashboard informs us the container is running. But where can we reach it? This had me stumped for a while: there was nothing in the UI and where using the applicable describe call in the AWS CLI there was nothing in the resulting JSON. Turns out out you have to navigate to the ENI (Elastic Network Interface) of the task in the service. This takes you to the standard EC2 console which lists the public IPv4 address!
An initial try didn’t result in a response. This was solved by adding port 3000 to the Security Group attached to the ENI.
Note: this complete setup can be done using the AWS CLI, see AWS’s own write up here.
Now the containers are up and running, what can we do with them?
ACI has no GUI really, only a read only screen showing the running containers, called “container groups” for some reason. This means you’ll be interacting with ACI using the Azure CLI, which is mercifully short on what you can actually do with an ACI container:
No stopping, no suspending, no scaling, no metrics, no updating, no volumes/disk mounting, no nothing. You can tail the logs and that is about it. This is immutable infrastructure brought to its logical conclusion! The fact that there is no scaling, as in starting multiple load-balanced versions of the same container is a big miss though, even in this MVP form of the service.
As such, ACI seems like an island in the normally pretty well integrated Azure landscape and I bet Microsoft is working on integrating their load balancing solutions and things like volume mounting.
update 9–12–2017: In the comments the ACI PM Sean jumped in and mentioned volume mounting DOES exist: https://docs.microsoft.com/en-us/azure/container-instances/container-instances-mounting-azure-files-volume These CLI parameters are however not listed in the CLI version I’m using.
As noticed earlier, the setup is a lot more complex than ACI. But after jumping through all the hoops you do have a pretty flexible and well supported container platform with basically all ECS functions:
As you probably have noticed by now, Fargate is not really a separate AWS service, although AWS does kind of present it as such: Fargate is a different container runtime for ECS. In your day-to-day usage, you use ECS not Fargate.
BIG CAVEAT: This is an anecdotal “performance test” at best. It is actually mostly a network throughput test…Still, I was curious if there where any obvious and huge differences in simple performance between both services.
My test service is a Node.JS Hapi application serving a simple page. Running an Apache Bench benchmark with 1000 requests and a concurrency of 5 over 10 runs gave the following results:
The results are really, really close. What was noticeable though was that Fargate/ECS seems to have less peaky behaviour, e.g. the 99 percentile was generally lower.
Both services use a fairly similar pricing model where the main cost is a function of:
duration * containers * memory * CPU
ACI adds a “create request” cost of $0.0025 per created container. This sort of muddles the pay-per-use model based on resources, but probably not a big issue for most. Fargate containers are charged at a 1 minute minimum.
Here’s an example: you create 5 container instances with a 1 core, 2 GB configuration once daily during a month (30 days). The duration of each instance is 10 minutes (600 seconds). Fargate does not offer the combination of 1 vCPU / 1GB. The minimum is 2GB for a 1 vCPU container.
The cost in $ / month works out as follows:Azure Container Instances: $3.75AWS Fargate: $1.90
You can see the calculation for the above example in the Google Sheet I whipped for this post. Feel free to use it to do a price comparison for your own use case.
docs.google.com
Note: This calculation looks only at the pricing for the given services and treats Azure cores the same as AWS vCPU’s. Also, AWS will prompt you to add other, paid-for, services like a load balancers to “complete the package”.
Diving into this comparison, I expected both services to be extremely similar. They are not. In their current incarnations both offerings have vastly different use cases.
Azure Container Instances almost felt as a service like Requestbin or JsFiddle. A quick and simple sandbox for running containers with almost Heroku-like behaviour: one bash command and your container is online. And I actually liked that a lot: I’ve never, ever launched a container to the internet in such a quick way. Probably even quicker than a local Docker instance with a service such as Ngrok. I can see myself using this during prototyping, when working remotely with clients or when I need to show a quick demo at a meetup or conference.
As AWS describes it, Fargate is a technology within ECS and later EKS. This means it sits in the DC/OS and Kubernetes space and comes with that feature set. As mentioned earlier, you don’t use Fargate day-to-day. It is there in the background taking care of managing your ECS cluster.
Azure Container Instances summary
AWS Fargate summary
Tim is a product advocate for https://vamp.io, the smart & stress free application releasing for modern cloud platforms.
#BlackLivesMatter
325 
9
325 claps
325 
9
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Code and Product. Writing about solopreneurship, Javascript and containers. Founder at checklyhq.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/microsoftazure/deploying-create-react-app-as-a-static-site-on-azure-dd1330b215a5?source=search_post---------33,"There are currently no responses for this story.
Be the first to respond.
Building apps is fun! Deploying them can be slightly frustrating; but it doesn’t have to be. In this guide, we’ll take a detailed look at all of the different ways to deploy React to Azure as a static application.
Static means that we aren’t deploying any server code; just the front end files. Azure will do all of the web work for us.
While this guide assume the use of create-react-app, this is of course not necessary to run a React app on Azure. If you don’t have an Azure account and would like one, you can get a fully functional trial by going to https://azure.microsoft.com/en-us/free/
First, you need to create a new website on Azure. You can do that by searching for “Web App” in the search field and then selecting “Web App”.
When asked for Windows / Linux (preview), leave the default at Windows.
You will need to configure Azure to handle your routes. Otherwise the application will break when you add in client-side routing.
To find your site’s FTP path, username and password, download the site’s publish profile.
That file (which is just XML despite having a “.publishSettings” extension) contains all of the necessary FTP information.
Just use your favorite FTP client (mine is Filezilla) and publish.
You can also have Azure pull directly from Github. This can be accomplished via the CLI or the portal. The CLI is well covered in the documentation, so we’ll use the portal here.
Select “Deployment Options” on your site.
Select “Choose Source”. You will see a bunch of different deployment options. Azure supports both local Git and Github. Local git is just you pushing code to a Github repo on Azure. Github is Azure pulling from your Github repo.
If you select Local Git, you will get a new setting on the Overview page which shows you the git clone url and your local git username. That username is coming from the “Deployment Credentials” tab where you can change it or the password at any time.
Remember that we don’t want to push the entire project to production — with the source code, node_modules and the whole bit. We only want the “build” folder. If you aren’t currently using another git, this is easy. Just move into the “build” folder and do…
You can see from your terminal output that the deployment succeeded. Note that I trimmed out some items to make it smaller so yours will look more verbose.
It’s likely that your project is already under source control, in which case you may already have it up on Github.
Go to “Deployment Options” and select Github. Connect to your Github account and then select the repo that you want to deploy.
Azure then pulls in all of the code from your repo. By default, create-react-app adds a .gitignore entry for the “build” folder. You are going to need to commit your build for this to work, so go ahead and remove that entry.
Now tell Azure where your “index.html” file lives by adding a new “Default Documents” entry.
Usually people do not want to commit their “build” folder, hence its automatic inclusion in .gitignore. What we really want to be able to do is have Azure pull our code from Github, run npm build and then deploy just the build folder. We can do that with Visual Studio Team Services, which is completely free to use for 5 users or less. Perfect for setting up CI with create-react-app.
Start a new “Build And Release” project.
Make sure you select the correct “Version control” for your project. If you are using Github, then “Git” is the correct answer.
Now click on the “Build And Release” tab.
Select “New Definition”
Search for “Empty” template and select the Empty Template.
Select Process, and then set the Default Agent Queue to “Hosted”.
Now select the Get Sources tab. Select “Github” and authorize VSTS to your account.
It takes some time to populate the drop downs if you belong to several organizations and have several repos.
Now add a task to run “npm build” on the code that gets pulled from the Github repo.
To do that, search for “npm” and select the npm task. Add it to the process. By default the npm task just runs npm install, which is precisely what we need to do, so there is no more work to be done here.
Add a second npm task, but this time set it to custom and “run build”.
Search for the “App Service Deploy” task.
Select the correct Azure subscription and click “Authorize”.
Now select the correct App Service Name. You may have to click the little refresh arrow next to the drop down to get the list to populate.
Make sure that you point the “Package or folder” to
Now click “Save & Queue”. This will queue your job up for build and deployment.
I mentioned this earlier in the article, but please refer to this article for how to create and deploy a web.config that will do routing correctly. The route will work from inside your app, but there will be no deep linking because Azure will try to route https://yourapp.com/customers/add to “index.html” from the path “customers/add” instead of just loading the root “index.html” file and letting React handle the rest.
Happy deployment!
Any language.
367 
8
367 claps
367 
8
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nikovrdoljak/deploy-your-flask-app-on-azure-in-3-easy-steps-b2fe388a589e?source=search_post---------34,"Sign in
There are currently no responses for this story.
Be the first to respond.
Niko Vrdoljak
Feb 6, 2019·5 min read
In this article, I will show you how to deploy and publish your Flask web app on Azure. We will use an Azure App Service on Linux, which provides a highly scalable web hosting service using the Linux operating system.
I will assume that you are familiar with Flask framework, but in any case, I will create minimal Flask application for demonstration purposes and emphasize some aspects of app environment needed for successful publishing on Azure (or any other hosting service).
First in “hello.py” file create “Hello world” app:
Next, create and activate virtual environment, set startup file, initialize local git and start the app from your terminal:
Navigate to http://127.0.0.1:5000/ to check that your app is running.
We will use git later to push our app to Azure. Also, create a .gitignore file to specify files and folders you don’t want Git to check in:
(if you don’t have an Azure subscription, create a free account before you begin)
We will configure service via Azure portal. There is also an alternative to do this via Azure Cloud Shell, but we will skip it this time. So, log in to Azure portal. We will configure several elements to complete this step:
A resource group is a logical container into which Azure resources like web apps, databases, and storage accounts are deployed and managed.
On Azure portal left navigation bar click “Resource groups” and then “Add”. In displayed form, select your Azure subscription, location for your resources and type the name for your group.
Click “Create and Review” button and wait for notification:
Now we can create web app or App Service. On portal left navigation bar click “App Services” and then “Add”. Select “Web App” and click “Create”:
In Create Web App form, enter app name, select your subscription and existing resource group, Linux as OS, Code as publishing mode and Python 3.x as runtime stack:
Also, you have to create an App Service Plan for your app. An App Service Plan defines a set of compute resources for a web app to run. These compute resources are analogous to the server farm in conventional web hosting. One or more apps can be configured to run on the same computing resources (or in the same App Service plan). So click the App Service plan button and then Create new and fill the form:
Select appropriate pricing tier, but for demo purposes, Basic tier is the most suitable option.
Click “Create app” and wait for notification message:
Go to app service resource and examine your app info:
Notice URL for your app. If you click on it, it will open default page for your app which at this moment looks like:
Remember that URL of your app is:
http://<app_name>.azurewebsites.net
Our goal is to deploy our app from local git to Azure App Service that we created. To enable that, we first have to configure some deployment settings. Click “Deployment center”, and select “Local Git”:
On the next step, select “Kudu” as the build server:
Click “Finish”, wait for notification and you will get Git Clone Uri like this:
https://flask-hello.scm.azurewebsites.net:443/flask-hello.git
Also, click “Deployment Credentials” to see your app credentials:
Here you can see your app username and password, which you can change or create another user credentials for your deployment purposes.
Now we are ready for our deployment. Go to your local terminal and add Azure remote to your local Git repository. Replace <deploymentLocalGitUrl-from-create-step> with the URL of the Git remote that you get in the previous step:
git remote add azure-hello https://flask-hello.scm.azurewebsites.net:443/flask-hello.git
Now commit any changes to local git:
git commit -a -m “first commit”
And push to the “azure-hello” remote to deploy your app with the following command:
git push azure-hello master
When prompted for credentials by Git Credential Manager, make sure that you enter the credentials you created in Configure a deployment user, not the credentials you use to sign in to the Azure portal.
This command may take a few minutes to run. While running, it displays something like:
You can see similar log information in the Deployment Center:
If you refresh your app, you will see that nothing changed yet. The reason is App Service uses Gunicorn WSGI HTTP Server to run an app, which looks for a file named application.py or app.py. Since our main module is in hello.py file, we have to customize startup command. Go to “Application settings” and enter the following line in “Startup File” field:
Click “Save”, go to “Overview”, and click “Restart” to start the app with the new configuration. Now refresh your app. If everything is OK, out sample app should run in App Service on Linux:
That’s it. If you want to learn more on how you can customize the behavior of App Service with Python visit:
docs.microsoft.com
Also, if you want to get more information about your web site, it’s configuration, or access the diagnostic console from Bash or SSH, go to associated SCM service site:
https://<app_name>.scm.azurewebsites.net/
Father, husband, basketball fan, developer, project manager...
484 
7
484 
484 
7
Father, husband, basketball fan, developer, project manager...
"
https://medium.com/@sibeeshvenu/create-your-own-cryptocurrency-in-private-consortium-network-ethereum-azure-blockchain-2e4385018777?source=search_post---------35,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sibeesh Venu
Jan 15, 2019·10 min read
The words Blockchain and Cryptocurrency have been successful in disrupting the market for such a long time now, the effects they have made in the industry is massive. It looks like it has its own future, so thought to write something about it. In this article, we will create our own cryptocurrency…
"
https://medium.com/@fiqriismail/how-to-secure-your-reactjs-frontend-with-azure-ad-b2c-8fd165f602e8?source=search_post---------36,"Sign in
Fiqri Ismail
Jan 31, 2019·8 min read
A couple of months back I was introduced into the world of ReactJS. A client requirement to build a web frontend. All my developer days were spent on developing backend systems using Microsoft ASP.NET Web API and C#. And you can’t say no to your clients, right? if you have to do it you have to do it. So step into a new realm of building frontend applications.
My client already had a WEB API and a web frontend. This requirement was to create another separate sub-module using ReactJS to interact with that Web API. As usual security concerns put into the table and yep they already have Azure AD B2C setup for user management. The challenge was to use ReactJS to interact with Azure AD B2C for authentication and authorization.
Trust me, there were few tutorials on how to connect Azure AD and Azure AD B2C with frontend technologies. But all were bits and pieces here and there. I couldn’t find a full step by step tutorial that guides you through. So thought why not write one.
I have made a few assumptions here, that you already:
VS Code has this nice little feature called terminal window. This will open a terminal inside the code editor. It’s a very handy feature. Click on Terminal > New Terminalin the menu bar.
This will execute the ReactJS project. And it should look like this.
All good to go, let’s prepare our Azure AD B2C environment now.
In this section, we will be preparing our Azure AD B2C environment for authentication and authorization.
Using this URI, you will allow the permission to your application to access certain features in your directory. As an example, this could be reading user profile information.
I have specially marked this because if you didn’t give an identifier in this location, you won’t see any scopes under “Published scopes”. I am not sure its a bug or not but without it, you won’t get default scopes here neither can create new.
This is all you need at Azure AD B2C end. Let’s do a checklist.
Excellent, now the setup is done. Let’s go back to our react application and do some coding.
Now, go back to your ReactJS application. In the terminal type the following command to install the library. Remember we were using VSCode terminal window.
react-azure-adb2c is a library that will help you to get the functionality or Azure AD B2C to your ReactJS application. By clicking here you will get brief documentation of how to use it in your ReactJS application.
Now you have successfully installed the library. In your ReactJS application click on the index.js file, at the top of the file add the following line of code.
Add this line of code after the import to initialize.
Now you need to replace the items marked in “<>” from the values at your Azure AD B2C Application.
Now go back to the Azure portal and grab the following information.
To grab the value for the tenant, go back to your Azure AD B2C directory. Under overview, copy the value in “Domain name” field.
Now, to grab the applicationId, click on the Applications label, and copy the id from the newly created application, in this case, “ReactJS AADB2C” and replace the value at applicatoinId field.
Now click on the User flows (polices) label and copy the name of the policy and replace the value at signInPolicy field.
Now the scopes array field. This array will give the necessary permissions to your application. These permissions will allow your ReactJS application to access functionality at Azure AD B2C.
To grab this information:
Visit this link to get a full detailed documentation on scopes.
Excellent, we are almost done. Now, your initialize code should look like this.
One more thing to add. Let's replace the default ReactDOM.render() code with this.
After all these changes, your index.js file should look like this.
Almost there. Let's do a test run. In your terminal window type and execute the following command.
You should see this screen.
Now use your login details for the Azure portal or you can create a new account by clicking on “Sign up now”. Remember? we have created a user flow for both sign-in and sign-up. Cool isn’t it.
After creating a new account or using an existing account, you can log in to the application. But, you might not see the default ReactJS page. This might happen due to insufficient application permissions.
To fix this,
Lets’ go back to our ReactJS application and refresh or rerun it.
Congratulations !!! You are done.
Let’s grab some information from Azure AD B2C and display it under the react logo.
Go back to the terminal and install the following package.
This package will allow you to decode the JWT token from Azure AD B2C and grab information inside it.
Now you need to visit back to Azure portal and let Azure AD B2C send you this information. To do this,
Go back to your ReactJS application and click src directory. Add a new file. Name it as Auth.js. Copy and paste the following code inside the file.
Now open the App.js and replace with this code.
We are all done. Lets rerun our ReactJS application.
You will be prompted with the Microsoft login screen, after a successfull login you should see this screen.
And grab the code from here.
github.com
Have a nice day.
Architect| Microsoft MVP | Community Leader | Speaker | Blogger | Founder of PeachIT (www.peachit.digital)
586 
20
586 claps
586 
20
Architect| Microsoft MVP | Community Leader | Speaker | Blogger | Founder of PeachIT (www.peachit.digital)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-you-can-do-continuous-delivery-with-vue-docker-and-azure-2f1e31fff832?source=search_post---------37,"There are currently no responses for this story.
Be the first to respond.
A few weeks ago at ng-conf, I announced the launch of vscodecandothat.com — a project I worked on with Sarah Drasner to centralize all of my favorite VS Code tips into a collection of short, silent video clips. It’s like a site full of GIFs, except without the 600 megabyte payload and crashed browser tab.
Sarah designed and built the site using Vue. I put together the video clips with excessive pug references.
Sarah and I both work on the Azure team, so it was a good chance for us to use our own tools here at Microsoft to work with a real application. In this article, I’m going to break down how we do continuous delivery with vscodecandothat.com, and how you can do it yourself using the same tools we use.
Before we talk about the setup, I want to define exactly what I mean by “continuous delivery.”
The term Continuous Delivery refers to making releases easy, fast, and streamlined. We can argue about the exact definition of the term, but just remember that I am a front-end developer so my eyes may glaze over. I may snore. But go on. I swear I’m listening.
For our purposes, “Continuous Delivery” means that the process of building and deploying the site is completely automated. Here’s how that looks in real life:
Got all that? Basically, we’re going to automate everything that you would normally do as a developer so that checking code into Github is all you have to worry about. And lord knows that’s hard enough as it is.
OK, let’s begin at the beginning. The first thing we need to do is look at the application to see how it runs. And how it runs is “In a Docker, y’all.”
vscodecandothat.com is entirely front-end driven. It’s all HTML, JavaScript, and CSS in your browser. That being the case, all we want to do is serve up the index.html page from the dist folder. We use an nginx web server.
When you are just serving up static assets, the Dockerfile is very simple…
Sarah created an nginx configuration file that we just copy in when the container gets built. Because you do not want to be in the business of configuring nginx (OMG you don’t), Sarah has posted her config file to a gist.
I use the Docker extension for VS Code so that I can see and manage all of my images and containers. I’m not afraid of the terminal, but my brain can only remember so many flags.
Now we need a registry to push the container to. We’re going to configure Azure Container Services (ACR) for that.
You can create an ACR repository from the web portal, but to prove I’m not afraid of the terminal, we’ll do it with the Azure CLI.
First, we need a group for resources. I called mine “vsCodeCanDoThat”.
Now create the ACR repository. I called mine “hollandcr”.
Now we can push our image to that by tagging it with the path to the Azure Container Registry.
In the video you can watch me login to the Azure Container Registry from the terminal. This is important because your push will fail if you are not logged in.
OK — now we need a site to host our container. For that we use Azure App Service.
First create a Linux service plan. For that, you need your app name and your resource group.
So
Becomes
Now create the web app and point it at the container that was pushed to the AKS registry. This takes 4 parameters.
And that’s it. You’ll get back a URL, and you should be able to open it and see your site running.
Now what we want to do is automate everything that we just did. We never ever want to have to go through any of these steps again.
The first thing we will do is to set up our site for “Continuous Deployment” from our container registry.
If you are using the App Service extension for VS Code, all of your Azure sites will show up right in the editor. You can just right-click and say “Open in Portal.”
Select the “Docker Container” menu option…
On this page you will see the container you configured from the terminal. There is an option at the bottom to turn on “Continuous Deployment.”
When you toggle this on and click “save,” a webhook will get created in your Azure Container Registry for this specific container. Now, anytime the image with tag “latest” is updated, the webhook will fire and notify App Service which automatically pulls in your image.
So we’ve automated some of this already. Once we push the image, it will be deployed. There is nothing we have to do besides push it. But we don’t want to push it. We want someone else to that.
And who will do it? The robots, that’s who. Or whom? OR WHOMST. Fortunately I’m not in high school English anymore. I failed it once and that was enough.
This is the point at which I tell you that we are going to use Visual Studio Team Services (VSTS). Then you say, “Visual Studio? I’m not using .NET”. And I say, “I know, it’s confusing.”
We need a system specifically designed to automate builds and deployment. This is exactly what VSTS is/does. Also, it’s free for 5 users or less (in a project space) and “free” is the only word in my love language. The only word besides “beer.”
Create a VSTS account if you don’t have one. Once you do, you land on the dashboard screen.
From here, you want to create a new team project.
Give your project a name and a description that nobody will find helpful. Leave the version control at Git.
The next screen gives you a Git URL to check your code into. But we already have Github, so just ignore that and select the “or build code from an external repository” option.
Authorize VSTS to Github and select the repo…
The next screen is offering to help you start with a template. In this case we are going to roll from an empty process. Because we are hard core like that.
Now we are going to start adding steps for VSTS to perform to do the build and deployment. The pull from source control is already happening, so the first thing we need to do is to run npm install on our code. To do that, add a task to “phase 1”. There is only 1 phase in our build / deployment.
Search for “npm” and add the npm task.
By default, you get the npm install task, which is exactly what we want. You don’t need to add any options to this task.
Next, we’ll be running the npm run build command, which will build a production instance of our Vue app with all of its Webpacking magic. For that, add another npm task. This time, change the name to “npm run build.” Set the “command” to “custom” and the “command and arguments” to “run build.”
Great! We’ve got the build, now we’re ready to Dockerize it. Add a new task and find the “Docker” one.
This is a big screen, so here’s the image and then we’ll walkthrough the highlights.
Lastly, we want to push the image. Add another Docker task. This time, set the “Action” to “Push an image”. Set the “Image Name” to $(Build.Repository.Name) — just like before.
DO NOT SELECT THE “PUSH IMAGES” ACTION. If you do, your build will fail and you will blame god and all humanity before you figure out that you selected the wrong action. Don’t ask me how I know that.
And that’s it for defining the Build definition. You can now click “save and queue” at the top. Make sure that you select a “Hosted Linux Preview” agent. The Docker tasks needs the Linux agent.
Now sit back and wait for a build to kick off. If you’ve done everything right, you have now setup a completely automated build and deployment system for a Vue app that utilizes Docker and Azure. That’s the most buzzwords I’ve ever squeezed into one sentence.
This seems like a lot to setup, but once you have it just like you want it, all you have to do is check in code to your Github repo and all of this manual deployment 💩 happens automatically. Your customers will love you. Your developers will love you. Heck — even YOU might love you.
I hope you find this helpful. I’m off to update my résumé with all of these buzzwords.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
633 
4
633 claps
633 
4
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/@coderonfleek/hosting-a-laravel-application-on-azure-web-app-b55e12514c46?source=search_post---------38,"Sign in
There are currently no responses for this story.
Be the first to respond.
Fikayo Adepoju
Jun 11, 2017·6 min read
This post was born out of the pain I went through while trying to host a Laravel application on Azure for a client who insisted we host all his web apps there, after reading loads of blog posts and stack overflow suggestions i was able to get it running smoothly so i decided to share this knowing it will help someone out there.
I would like to give credit to Peter Katelaan on this blog post which really helped in finding some good answers to some disturbing points in my deployment.
Now lets get into it starting with some prerequisites. Before you proceed you should have and be comfortable with the following:
You good? Great! Lets get it on.
This can be done easily by running laravel new <app-name> from your terminal
Run git init within your project root. Then create a remote git repository with your favorite provider (I will be using Bitbucket) and add it as a remote branch to your project by running git remote add <endpoint name> <your git url> .
Now commit some code and push to your remote master branch (or any appropriate branch you have setup for the purpose of the deployment)
Got to the Azure Portal and navigate to New >> Web + Mobile >> Web App
Enter an appropriate name for your web app (Azure will verify in real-time if the name you have chosen is available) and select the Pricing tier (i am using the free tier for this practise)
When, you’re done filling all the appropriate fields, click “OK” at the bottom of the panel, Azure will begin setting up your web app and you will be notified immediately the process is complete
Click on your web app and go to the SETTINGS section and click on Application Settings, here you verify that the PHP version on your web app meets the minimum required by the version of Laravel you are running.
If not, change the PHP version and click the Save button to persist your changes.
Then, you need to install composer which is the dependency manager for laravel. To do this, go to the DEVELOPMENT TOOLS section and click on Extensions. From there you click the Add button at the top left-hand corner.
This opens up a blade section from which you can then select Composer, agree to the Terms and Conditions, then click OK to install it.
There is a range of differences between how Azure servers work compared to your typical Apache on Linux servers. One of which is that Azure serves out your web app from its wwwroot unlike public_html on Apache servers. However, Laravel is not a fan of this style so in order to get Laravel to work we need to tweak some settings.
Now lets setup the Deployment option. Go to the DEPLOYMENT section and click on Deployment Options. Select your Remote Git Provider (Note, you can also deploy directly from your local git repo), so i select Bitbucket and authorize it by entering my username and password.
Then, you select the repo and the branch you have setup as your deployment branch (I am selecting the master branch) and click Ok.
(Almost) Immediately, you will see your first commit being deployed.
As .htaccess is to Apache so is web.config to IIS which is the server Azure is using. Thus the .htaccess file that comes with your laravel app (in the public directory) will not work. So go into your public folder, create a web.config file and paste in the content below:
Commit and push this change to your remote branch and Azure automatically deploys it.
One important thing to note (and boy did this skip my mind) is that the .gitignore file that comes with your Laravel app ignores your .env file by default (for very good reasons one of which is to have different environment settings for your local and production environments).
Thus you need to add one in your production environment. Now don`t make make the mistake of removing it from one of the ignored files in .gitignore . Azure provides an app management console known as the Kudu Console which provides you loads of awesome tools to manage your web app. Simply navigate to it by going to https://<app-name>.scm.azurewebsites.net , for me that will be https://laravel-azure-app.scm.azurewebsites.net
From the top menu, go to Debug Console >> CMD , there you will see an FTP-like interface for your folder structure and below it is command-line interface (yeah, i know right) which is just awesome.
From the folder structure interface, click on the site folder. Then from the command line, simply create the .env file and copy the contents from .env.example by running the following command:
copy .env.example .env
This then creates your .env file, but wait, we ain’t done yet. Just a lil more edit to go.
Open your .env file by clicking the edit icon beside it
Then from the local version of your .env file, copy your APP_KEY and paste it in your production version. Also, change the APP_URL to that of your azure web app.
Phewww! Now we are done. We can now unveil our app. Simply go to the address of your app (for me that will be: http://laravel-azure-app.azurewebsites.net) and you will see your app Live and Robust. Wonderful!!!
@LinkedIn Author | Technical Writer | Software Developer
See all (71)
743 
24
743 claps
743 
24
@LinkedIn Author | Technical Writer | Software Developer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/microsoft-azure-sentinel-not-your-daddys-splunk-3775bda28f39?source=search_post---------39,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 5, 2019·7 min read
OK, I must admit; this title is misleading. I am not going to do a side by side comparison of Splunk and Azure Sentinel. Although that seems to be the thing that people on social media are talking about these days: how does Azure Sentinel compare to other SIEM solutions such as Splunk, etc.
Instead, I’ll be focusing on what role Azure Sentinel plays in securing your enterprise. And while Azure Sentinel does provide the advanced SIEM capabilities and dashboarding that many companies need, I really want you to understand the broader picture as Azure Sentinel, as a cloud security solution, is set to disrupt the SOC.
And with Microsoft owning and operating a big part of the technology you use every day in your workplace, along with making security a strategic investment and bet, I argue that they are becoming the biggest security company in the world.
Biggest security company in the world
Microsoft is investing heavily in security in recent years. Not only have they upped their game in finding and fixing product defects, they for instance also have a big organizational unit around threat intelligence (Microsoft Threat Intelligence Center). They are investing tens if not hundreds of millions in developing security products and solutions for their platforms.
And while one could argue that the early days of their AV solution were not watertight, they certainly turned around that “ship”, and Microsoft should not be underestimated if they are taking security seriously. If you look at their evolved EDR solution today, Windows Defender is not only achieving high scores, it also detects bad actors in ways and speed other vendors do not and cannot.
Because Microsoft’s owns both one of the two biggest cloud platforms in the world, as well as sell the most used cloud endpoint (Windows), they are poised to become the biggest security player in the world. On top of this, it can leverage its immense computing power to use machines learning and artificial intelligence to really make a difference in how security is approached.
You see this coming to life when you connect Windows Defender to their Azure cloud; you start to receive threat intelligence feeds, and new malware is detected and remediated through machine learning in under 14 minutes. This is why Defender ATP is growing very strong in adoption at enterprises in recent months.
Traditional SIEM’s and the cloud: a sour-sweet combination
As mentioned, Microsoft has an EDR solution called Windows Defender. But it has many more offerings. For instance, they also have specific solutions for protection your valuable data such as Cloud App Security and Office 365 ATP. They can protect your identity with Azure AD, and Azure ATP. Microsoft also has Azure Security Center to protect the assets that run on Microsoft Azure, and there are many more security solutions in their portfolio.
One thing that seemed to be lacking was a central orchestrator. A coordinator for all your security efforts. Something that ties this all together.
In the past years, enterprises would hook up the alerts that Microsoft security solutions were generating and forward them back to their on-premise SIEM solution as part of their cloud security strategy. But they are struggling to keep pace with the increasing volume and variety of data they process. Unhappy users complained about the inability of their SIEMs to scale and the volume of alerts they must investigate.
Enterprises struggling with the cost of data analysis and log storage often turn to open source tools like Elasticsearch, Logstash, and Kibana (ELK) or Hadoop to build their own on-premise data lakes. However, to gain useful insight from the data they collect, they realiz the expense of building and administering these “free” tools is just as great as the cost of commercial tools.
Sentinel, orchestrating your security efforts
This is where Azure Sentinel comes in; a central place to analyze your security data, across all parts of your environment. Cloud security solutions like Azure Sentinel are set to disrupt the SOC, Forrester concludes:
“This week, as thousands of security pros gather in San Francisco for RSA, tech titans Microsoft and Google (Alphabet) launch cyber security tools that promise to disrupt the traditional way of taking in and analyzing security telemetry. Chronicle Backstory (an Alphabet company) and Microsoft Sentinel are cloud-based security analytics tools that are addressing the challenges faced by SOC teams such as:
Chronicle and Microsoft are making these challenges cloud native with virtually unlimited compute, scale, and storage. These vendors have a unique advantage over legacy on-premise tools since they also own their cloud infrastructures and aren’t dependent on buying cloud at list price from would-be competitors.”
Connecting any and all clouds
One could lead to think that this will be an all-Microsoft centered approach. But nothing is truer. While Microsoft has not confirmed this publicly, they are indeed working with other cloud vendors to get their security data programmatically.
If you take a look at the Data Connections section of the Azure Sentinel preview, you already see a placeholder section for connecting the AWS CloudTrail data soon. I’ll write more about this in an upcoming blog.
The Intelligent Security Graph is at the center of this all
I’ve written about what Microsoft’s Intelligent Security Graph is before:
“Microsoft describes ISG as a way to ‘build solutions that correlate alerts, get context for investigation, and automate security operations in a unified manner.”
But with the release of Azure Sentinel, it really amplifies that strategy and makes it come to life. The intelligent security graph is a core piece of Sentinel’s backend to grab the relevant information from other Microsoft services such as Azure ATP, Defender ATP, Azure Security Center, etcetera.
But not only for Microsoft services. Exactly a year ago at RSA 2018, many vendors such as Palo Alto Networks, F5, Symantec, Fortinet and Check Point integrated their solutions into the intelligent security graph. Azure Sentinel leverages those technical integrations to get events from the network.
But not only network vendors integrate with Microsoft’s intelligent security graph. Well-known names such as Anomali, Sailpoint, Ziften and many others have joined the party recently.
Using the dashboards technology already available in Azure, Sentinel is able to provide you with a single pane of glass on the security of your environment. And because of the graph, it provides detailed out of the box drill-down dashboards for those network vendors, as part of your investigation.
Azure Firewall is the perfect example
But it doesn’t stop at getting even data from the network. Microsoft just announced new capabilities in its own Azure Firewall, most notably a feature called Threat intelligence-based filtering.
“Azure firewall can now be configured to alert and deny traffic to and from known malicious IP addresses and domains in near real-time. The IP addresses and domains are sourced from the Microsoft Threat Intelligence feed powered by The Microsoft Intelligent Security Graph.”
Threat intelligence-based filtering is default-enabled in alert mode for all Azure Firewall deployments, providing logging of all matching indicators. Customers can adjust behavior to alert and deny.
Democratizing AI: meet Azure Sentinel FUSION
Azure Sentinel features something Microsoft calls FUSION. As Microsoft is looking to democratize Artificial Intelligence, they are making it easy to use machine learning as part of your triage.
Instead of sifting through a sea of alerts, and correlate alerts from different products manually, ML technologies will help you quickly get value from large amounts of security data you are ingesting and connect the dots for you.
For example, you can quickly see a compromised account that was used to deploy ransomware in a cloud application. This helps reduce noise drastically.
Conclusion
I agree totally with Joseph Blankenship:
“For security pros that have been around awhile, don’t let your cynicism block the potential advantages your organization could experience by making use of Azure Sentinel. Take off the tinfoil hat and realize that Microsoft is a security company now. What Google and Microsoft have introduced will make the entire industry better, and that’s something to applaud.
The future of cybersecurity, just like the IT resources it protects, is in the cloud. The Tech Titans are staking out a claim and changing the way security solutions are purchased, delivered, and consumed… and it couldn’t come at a better time for the industry.”
Over the course of the next couple of weeks I’ll share my real-world experiences on Azure Sentinel with you in a multi-part blog series at http://www.maartengoet.org.
Part one will be about ‘design considerations for Azure Sentinel’. Stay tuned!
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
178 
1
178 claps
178 
1
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/reliable-event-processing-in-azure-functions-37054dc2d0fc?source=search_post---------40,"There are currently no responses for this story.
Be the first to respond.
Event processing is one of the most common scenarios in serverless and Azure Functions. A few weeks ago I wrote about how you can process events in order with functions, and for this blog I wanted to outline how you can create a reliable message processor so you avoid losing any messages along the way. I’ll be honest — this blog could have easily broken into two or three parts, but I’ve decided to keep it all here in a single post. It’s lengthy, but goes from basics all the way to advanced patterns like circuit breaker and exception filters. While these samples are in C#, all patterns work across any language (unless explicitly stated otherwise).
Imagine a system sending events at a constant rate — lets say 100 events per second. Consuming these events from Azure Functions is easy enough to setup, and within minutes you could have multiple parallel instances processing these 100 events every second. However, what if the event publisher sends a corrupt event? Or your instance has a hiccup and crashes mid-execution? Or a downstream system goes offline? How do you handle these while preserving the overall integrity and throughput of your application?
With queues, reliable messaging comes a bit more naturally. In Azure Functions when you trigger on a queue message, the function can create a “lock” on the queue message, attempt to process, and if failing “release” the lock so another instance can pick it up and retry. This back-and-forth continues until either success is reached, or after a number of attempts (4 by default) the message is added to a poison queue. While a single queue message may be in this retry cycle, it doesn’t prevent other parallel executions from continuing to dequeue the remaining messages — so the overall throughput remains largely unaffected by one bad message. However, storage queues don’t guarantee ordering, and aren’t optimized for the same high throughput of services like Event Hubs.
With event streams like Azure Event Hubs, there is no lock concept. To allow for high throughput, multiple consumer groups, and replayability, services like Event Hubs read more like a tape drive when consuming events. There is a single “offset” pointer in the stream per partition, and you can read forwards or backwards. While reading the event stream, if you hit a failure and decide to keep the pointer in the same spot, it prevents further processing on that partition until the pointer progresses. In other words, if 100 events per second are still coming in, and Azure Functions stops moving the pointer to new events while trying to deal with a single bad one, those events will start to pile up. Before long you have a massive backlog of events that are constantly growing.
Given this offset and consumer behavior, functions will continue to progress the pointer on the stream regardless if the execution succeeded or failed. This means your system and functions need to be aware and structured to deal with those behaviors.
The behavior for the Azure Functions Event Hubs trigger is as follows:
There’s a few important things to note in this. The first is that if you have unhandled exceptions you may lose messages — because even an execution that results in an exception will progress the pointer. The second is that, as is the norm with distributed systems, Functions guarantees at-least-once delivery. Your code and dependent systems may need to account for the fact that the same message could be received twice. I have examples showing both of these behaviors, and how to code around it, below:
For these tests I did the following — I publish 100,000 messages to be processed in order (per partition key). I’ll log each message to a Redis cache as it’s processed to validate and visualize order and reliability. For the first test, I wrote it so every 100th message throws an exception without any exception handling.
When I push 100,000 messages at this sample, here’s what I see in Redis:
You’ll notice I missed a whole chunk of messages between 100–112. So what happened here? At some point one of my function instances got a batch of messages for this partition key. That specific batch ended with 112, but at message 100 my exception was thrown. This halted that execution, but the function host continued to progress and read the next batch. Technically these messages are still persisted in Event Hubs, but I’d need to manually go and re-fetch 100–112 to reprocess.
The simplest resolution to this is adding a simple “try/catch” block in my code. Now if an exception is thrown, I can catch it inside the same execution and handle it before the pointer progresses. When I added a catch in the code sample above and re-run the test, I see all 100,000 messages in order.
Best Practice: All Event Hubs functions need to have a catch block
In this sample I used the catch to attempt an additional insert into Redis, but you can imagine other viable options like sending a notification, or outputting the event to a “poison” queue or event hub for later processing.
Some exceptions that arise may be transient in nature. That is, some “hiccup” or other issue may just go away if an operation is attempted again a few moments later. In the previous section I did a single retry in the catch block — but I only retry 1 time, and if that retry failed or threw its own exception I’d be out of luck and still lose events 100–112. There are a number of tools out there to help define more robust retry-policies, and these still allow you to preserve processing order.
For my tests, I used a C# fault-handling library called Polly. This allowed me to define both simple and advanced retry policies like “try to insert this message 3 times (potentially with a delay between retries). If the eventual outcome of all retries was a failure, add a message to a queue so I can continue processing the stream and handle the corrupt or un-processed message later.”
And the resulting Redis:
When working with more advanced exception catching and retry policies, it is worth noting that for precompiled C# class libraries, there is a preview feature to write “Exception Filters” in your function. This enables you to write a method that will execute whenever an unhandled exception is thrown during a function execution. Details and samples can be found in this post.
We’ve addressed the kind of exceptions that may occur if your code hits an exception, but what about the case where the function instance has a hiccup or failure in the middle of an execution?
As stated earlier — if a function doesn’t complete execution the offset pointer is never progressed, so the same messages will process again when a new instance begins to pull messages. To simulate this, during the 100,000 message processing I manually stopped, started, and restarted my function app. Here are some of the results (left). You’ll notice while I processed everything and everything is in order, some messages were processed more than once (after 700 I reprocess 601+). That overall is a good thing as it gives me at-least-once guarantees, but does mean my code may require some level of idempotency.
The above patterns and behaviors are helpful to retry and make a best-effort at processing any event. While a few failures here and there may be acceptable, what if a significant number of failures are happening and I want to stop triggering on new events until the system reaches a healthy state? This is often achieved with a “circuit breaker” pattern— where you can break the circuit of the event process and resume at a later time.
Polly (the library I used for retries) has support for some circuit-breaker functionality. However these patterns don’t translate as well when working across distributed ephemeral functions where the circuit spans multiple stateless instances. There are some interesting discussions on how this could be solved in Polly, but in the meantime I implemented it manually. There are two pieces that are needed for a circuit breaker in an event process:
For my purpose I used my Redis cache for #1, and Azure Logic Apps for #2. There are multiple other services that could fill both of these, but I found these two worked well.
Because I may have multiple instances processing events at a single time, I needed to have shared external state to monitor the health of the circuit. The rule I wanted was “If there are more than 100 eventual failures within 30 seconds across all instances, break the circuit and stop triggering on new messages.”
Without going too deep into specifics (all of these samples are in GitHub) I used the TTL and sorted set features in Redis to have a rolling window of the number of failures within the last 30 seconds. Whenever I add a new failure, I check the rolling window to see if the threshold has been crossed (more than 100 in last 30 seconds), and if so, I emit an event to Azure Event Grid. The relevant Redis code is here if interested. This allows me to detect and send an event and break the circuit.
I used Azure Logic Apps to manage the circuit state as the connectors and stateful orchestration were a natural fit. After detecting I needed to break the circuit, I trigger a workflow (Event Grid trigger). The first step is to stop the Azure Function (with the Azure Resource connector), and send a notification email that includes some response options. I can then investigate the health of the circuit, and when things appear to be healthy I can respond to “Start” the circuit. This resumes the workflow which will then start the function, and messages will begin to be processed from the last Event Hub checkpoint.
About 15 minutes ago I sent 100,000 messages and set each 100th message to fail. About ~5,000 messages in I hit the failure threshold, so an event was emitted to Event Grid. My Azure Logic App instantly fired, stopped the function, and sent me an email (above). If I now look at the current state of things in Redis I see a lot of partitions that are partially processed like this:
After clicking the email to restart the circuit, running the same Redis query I can see the function picked up and continued on from the last Event Hub checkpoint. No messages were lost, everything was processed in order, and I was able to break the circuit for as long as I needed with my logic app managing the state.
Hopefully this blog has been helpful in outlining some of the patterns and best practices available for reliably processing message streams with Azure Functions. With this understanding you should be able to take advantage of the dynamic scale and consumption pricing of functions without having to compromise on reliability.
I’ve included a link to the GitHub repo that has pointers to each of the branches for the different pivots of this sample: https://github.com/jeffhollan/functions-csharp-eventhub-ordered-processing. Feel free to reach out to me on Twitter @jeffhollan for any questions.
#BlackLivesMatter
569 
7
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
569 claps
569 
7
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/net-core-mensageria-exemplos-utilizando-rabbitmq-e-azure-service-bus-66a81d02a731?source=search_post---------41,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
May 28, 2018·3 min read
Message Brokers são soluções voltadas à troca de mensagens entre diferentes aplicações. O uso de deste tipo de tecnologia está comumente associado à integração entre projetos e emprega um pattern de mensageria conhecido como publish-subscribe, o qual prevê uma aplicação (publisher) da qual se originam as mensagens, geralmente uma fila (queue) controlada pelo Message Broker e um ou mais consumidores (subscribers) desta queue.
A imagem a seguir traz uma representação esquemática deste padrão:
Dentre as vantagens oferecidas pela adoção de mecanismos de mensageria é possível destacar:
Este post traz 2 exemplos empregando soluções de mensageria em aplicações baseadas no .NET Core 2.0 e no ASP.NET Core 2.0:
Open source e multiplataforma (contando ainda com suporte a containers Docker), o RabbitMQ é sem sombra de dúvidas uma das alternativas mais populares em se tratando de Message Brokers. Aplicações construídas em tecnologias como .NET, Node, Java, Ruby e Python podem se beneficiar dos recursos oferecidos por esta solução em projetos que dependam de mecanismos de mensageria.
No repositório a seguir temos exemplos de uso do RabbitMQ com o .NET Core 2.0 e o ASP.NET Core 2.0:
https://github.com/renatogroffe/RabbitMQ-DotnetCore2-Selenium
Aplicações .NET podem interagir com um Broker baseado no RabbitMQ empregando para isto o package RabbitMQ.Client (este último já compatível com o .NET Standard):
O RabbitMQ foi também tema de um hangout do Canal .NET neste ano de 2018. A gravação se encontra disponível no YouTube e pode ser assistida gratuitamente (inclusive os projetos aqui mencionados foram descritos em detalhes neste vídeo):
Alternativa ao RabbitMQ, o Service Bus é um serviço de mensageria que integra o Microsoft Azure. A seguir está uma nova versão do mesmo exemplo apresentado na seção anterior já adaptado para uso desta solução (inclui o uso do Application Insights, serviço de monitoramento que integra a plataforma de cloud computing da Microsoft):
https://github.com/renatogroffe/ASPNETCore2_API_AppInsights-ServiceBus-SQL
Em aplicações .NET o package Microsoft.Azure.ServiceBus (também compatível com o .NET Standard) permitirá a integração com o Azure Service Bus:
Conteúdos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0/7.1/7.2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
371 
3
371 claps
371 
3
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://levelup.gitconnected.com/aws-vs-azure-vs-google-detailed-cloud-comparison-b075a35fc8b8?source=search_post---------42,NA
https://medium.com/free-code-camp/how-to-build-a-serverless-report-server-with-azure-functions-and-sendgrid-3c063a51f963?source=search_post---------43,"There are currently no responses for this story.
Be the first to respond.
It’s 2018 and I just wrote a title that contains the words “Serverless server”. Life has no meaning.
Despite that utterly contradictory headline, in this article we’re going to explore a pretty nifty way to exploit SendGrid’s template functionality using Timer Triggers in Azure Functions to send out scheduled tabular reports. We are doing this because that’s what everyone wants in their inbox. A report. With numbers in it. And preferably some acronyms.
First, let’s straw-man this project with a contrived application that looks sufficiently boring enough to warrant a report. I have just the thing. A site where we can adjust inventory levels. The word “inventory” is just begging for a report.
This application allows you to adjust the inventory quantity (last column). Let’s say that an executive somewhere has requested that we email them a report every night that contains a list of every SKU altered in the last 24 hours. Because of course, they would ask for that. In fact, I could swear I’ve built this report in real life in a past job. Or there’s a glitch in the matrix. Either way, we’re doing this.
Here is what we’re going to be building…
Normally the way you would build this is with some sort of report server. Something like SQL Server Reporting Services or Business Objects or whatever other report servers are out there. Honestly, I don’t want to know. But if you don’t have a report server, this gets kind of tedious.
Let’s go over what you have to do to make this happen…
This is the kind of thing that nobody wants to do. But I think this project can be a lot of fun, and we can use some interesting technology to pull it off. Starting with Serverless.
Serverless is a really good use case for one-off requests like this. In this case, we can use Azure Functions to create a Timer Trigger function.
To do that, I’m going to use the Azure Functions extension for VS Code. I’m going to use it for everything in fact. Why? Because I don’t know you, but I do know it’s highly likely that you are using VS Code. VS Code is great because it’s like a movie that all developer’s can universally agree is completely awesome. Sort of the opposite of “Children of Men”. That movie was terrible and you know it.
Make sure you install the Azure Functions extension.
marketplace.visualstudio.com
Now create a new Function App from within VS Code.
Then create a new Timer Trigger function. Timer Trigger functions are scheduled using standard Cron Expressions. You have likely not ever seen before because I had not seen one until a few months ago. And I’ve been in this industry for a LONG time. I am old, father William.
Cron expressions look kind of scary cause they have asterisks in them. In the case below, I’m saying that when minutes is 0 and seconds is 0 and hours is evenly divisible by 24, fire the function. This would be midnight.
Now we can run this locally (F5). We’ll see in the embedded terminal the schedule on which our Function will be called; the next 5 occurrences.
It feels good, man.
OK, now we need to get some data. I’m not going to drag you into the specifics of me querying SQL Server from this function because that’s not what this article is about, but here’s the code anyway.
I’m connecting to the database, doing a simple query and….wait a minute…did not I say I wasn’t going to get into specifics? You had me there for a minute, but I’m onto your game!
So this pulls in data and we get it in a JavaScript object that we can pass as JSON. If we were to JSON.stringify this, we will see the data set that we need to send in the report.
OK! We’ve got data, now we just need to make it pretty and email it to someone we don’t like. How are we going to do that? With SendGrid!
SendGrid is a nifty service with a really nice dashboard. You will like it. Or you won’t. Either way, you have to use it to get through this blog post.
You can create a free account if you don’t already have one. That’s plenty for what we’re doing here today.
Once you create a report, SendGrid is going to drop you into your “dashboard”. From this dashboard, you need to create a new API Application and get the key.
Make sure you copy your API key when it gives it to you. You can’t ever get back to it and you’ll have to do this all over again. Let’s face it: it was kinda boring the first time around.
Copy that key into your Azure Functions project. Put it in the local.settings.json file so you can access it as a Node.js environment variable later.
Now we are going to create a template in SendGrid. That’s what we will use to design our report. SendGrid has something called “Transactional Templates”. I have no idea why they are called that, but we are going to be needing one.
Once you create a new one, you have to create a new “version”. I had a hilariously hard time figuring this out. But then again, my brain is tad on the smallish side of little.
Choose to design your template with the Code Editor. You don’t need no freakin’ Designer Editor!
SendGrid support handlebars, which is a template syntax that’s so easy, even I can do it. In the Code Editor, you can paste the JSON data into the “Test Data” tab…
Now iterate over the data using its key name from the JSON…
It’s BEAUTIFUL! I’m crying. Ship it.
ALRIGHT. Fine. We’ll make it a little nicer on the old eyeballs. Here is a style that I shamelessly ripped off of the gorgeous Bulma CSS framework.
It’s ok at this point for you to be audibly impressed.
Now you might have noticed that the Subject of the email is missing. How do we fill that in? Well, after another embarrassing period of failure followed by introspection, I figured out that it’s behind the “Settings” icon on the left. You just have to pass a value in your JSON for “Subject”.
Now we need to get the template ID and add it to our Azure Functions project. Save this template and select the ID from the main template screen.
Drop it in the trusty local.settings.json file right underneath your SendGrid API key.
Now we are ready to pass our data from our Azure Function to SendGrid and send out this incredible work of business art.
Azure Functions provides a binding for SendGrid. If you create a function through the Azure Portal, it will create this binding for you when you select the “SendGrid” template. If you are doing it locally like I am, you have to add it yourself.
First you need to open the function.json file for the CreateReport function and add in the SendGrid binding.
The SendGrid binding comes as an extension for Azure Functions. Run the following command in the terminal to install it.
When you run this command, VS Code will ask you to restore some dependencies. You can click restore. Nothing bad will happen…OR WILL IT?!
One other thing you need to do is tweak your extensions.csproj file to reference the latest SendGrid library. This is required to use dynamic templates.
When you add that, VS Code will prompt you to restore again and yes, you definitely need to do it this time. VS Code needs to build these binaries and the restore does that.
OK! Now we’re ready to send an email via our SendGrid template. Here is the code to do it. It’s depressingly simple. I know after all this you were hoping for enough code to choke a cat (what? you’ve never heard that metaphor before?), but this is all it takes.
The items of note are me passing in a Subject as part of the JSON. As well as the fact that you can override to/from addresses specified in the function.json file here.
Now you can run your function and wait 24 hours to test it!
No but seriously — how do you manually test a Timer Trigger without constantly modifying the damn Cron Job?
I’ll show you how I do it and then you can figure out a better way.
I create an Http Trigger in the same project and call it “RunCreateReport”. In that function, I just import and call the timer function.
The only drawback to this is that you have to repeat your SendGrid binding settings from function.json in the “CreateReport” over in the “RunCreateReport” function.json. But other than that, this works just fine. Now you can run this thing, fire up a browser and hit the URL which will call the timer function immediately. You can test without having to touch that icky old Cron expression.
Now go check your email and bask in the glory of the report. Note that you don’t have to own an email address to send from SendGrid. You can literally send from any address. Seriously. Go ahead and try. JUST THINK OF WHAT YOU CAN DO WITH THIS POWER.
Here’s what my inbox looks like. Heads up, it does go to junk. Probably because I don’t own the sender email address.
WHAT? There’s a “Business Resilience Conference”? OMG so much business. I bet those people get a LOT of reports.
You can get this project from Github.
github.com
Here are a few other Azure Functions resources to keep you busy.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
301 
3
301 claps
301 
3
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://medium.com/@maarten.goet/securing-kubernetes-on-microsoft-azure-are-your-container-doors-wide-open-bb6e879cec5d?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Jan 15, 2019·8 min read
Matt Asay from InfoWorld wrote the following a couple of weeks ago: “The operating system no longer really matters. And for developers and the cloud, that means that Linux no longer really matters. We now live in a Kubernetes world.”
Kubernetes is quickly becoming the new standard for deploying and managing software in the cloud. Microsoft last year launched their managed Kubernetes offering in Azure called Azure Kubernetes Service (AKS) and IBM’s main driver behind the $34 billion acquisition of Red Hat in 2018 was its Kubernetes-based OpenShift offering.
Few people have extensive experience with Kubernetes. And if they did have training or have gotten their hands dirty with deployments, how many only focus on general engineering and administration and overlook the security aspect?
A quick search on Shodan, the leading search engine for finding vulnerable internet-connected systems, reveals that in early 2018 over 15K+ Kubernetes dashboards were directly facing the internet, and few of them had security measures in place.
Have you secured your Kubernetes environment, or are your container doors wide open?
Securing Linux to secure Kubernetes
Jose Alvarez summarizes it nicely: you need to make sure you tackle the basics first — secure Linux to secure Kubernetes. There are a couple of things to look at:
By no means is this a complete list, but a justa few things to get you thinking on this topic. Do you have other things you secure on Linux as part of hardening your Kubernetes environment? Please share them in the comments below.
Whilst it’s possible to run Microsoft Windows Server as the OS for Kubernetes worker nodes, more often than not, the control plane and worker nodes will run a variant of the Linux operating system. There might be many factors that govern the choice of Linux distribution to use (commercials, in-house skills, OS maturity), but if it’s possible, use a minimal distribution that has been designed just for the purpose of running containers.
Examples include CoreOS Container Linux, Ubuntu Core, and the Atomic Host variants. These operating systems have been stripped down to the bare minimum to facilitate running containers at scale, and therefore, have a significantly reduced attack surface.
PRO TIP: Aqua Security is also available directly from your Kubernetes cluster overview page in Azure!
Kubernetes is a framework, not a platform
So why is Kubernetes not secure by default? Jussi Nummelin summarizes it nicely: from a security standpoint, Kubernetes is more like a framework intended to be utilized in building more higher-level solutions. What that means in practice is that plain Kubernetes does not lock down everything properly by default, instead it is expected to be configured by the operators setting it up.
Not sure how Kubernetes works? Microsoft together with the Cloud Native Computing Foundation (CNCF) released a cartoon called “Phippy Goes To The Zoo” telling about the basics. But also Daniel Sanche has written a great overview of the core concepts in Kubernetes you could look at.
Tesla found out, the hard way
I wrote about the rise of the cryptocurrency miners in a previous blog. Well, add Tesla to the legion of organizations that have been infected.
In a report published early 2018 researchers at security firm RedLock said hackers accessed one of Tesla’s Amazon cloud accounts and used it to run currency-mining software. The initial point of entry for the Tesla cloud breach, the report said, was an unsecured administrative console for Kubernetes.
“The hackers had infiltrated Tesla’s Kubernetes console which was not password protected,” RedLock researchers wrote. “Within one Kubernetes pod, access credentials were exposed to Tesla’s AWS environment which contained an Amazon S3 bucket that had sensitive data such as telemetry.”
ArsTechnica has a great in-depth article on the breach which you can read here.
Netscylla was intrigued on how easy this could happen to any customer cloud estate. So, they decided to do some OSINT (Open Source reconnaissance) and their own research. Want to understand what ports are involved and how to find vulnerabilities? Have a look at their writeup.
Kubernetes on Microsoft Azure
One of the questions I have been hearing a lot from customers, is about how RedHat’s OpenShift (which is based on Kubernetes) compares to the managed Kubernetes services provided by most of the major public cloud providers.
Google introduced the world to Kubernetes mid-2014, and their Google Cloud Platform has a managed service called “Google Kubernetes Engine” (GKE). Amazon has a service called “Amazon Elastic Container Service for Kubernetes” (EKS) and Microsoft has a managed Kubernetes based offering called “Azure Kubernetes Service” (AKS).
If you want more detail about choosing OpenShift or not, Chris Saunders, who works at RedHat, wrote a lengthy article on what OpenShift provides additionally to a plain-vanilla Kubernetes environment. For now, I’ll focus on Microsoft’s Azure Kubernetes Service.
Amplifying that Microsoft is serious about containers, Kubernetes and open source in general is the hire of Brendan Burns. Brendan helped create the Kubernetes open source container orchestration technology and joined Microsoft to work on the Azure in 2016.
Want to get your own Kubernetes cluster running on Microsoft Azure? Microsoft has great documentation on this, and how to get your first application running on that cluster.
Securing your Kubernetes environment
Here are some general recommendations on improving security:
Etienne Dilocker has written a lengthy article on comparing authentication methods for Kubernetes.
Microsoft provides an own network policy module to implement Kubernetes network policies with the Azure CNI plugin for acs-engine and AKS called Azure NPM. My good friend Daniel Neumann wrote a blog on how to use this, and you can find the relevant Microsoft documentation here.
John Kindervag’s Zero Trust Network Architecture is a great concept to embrace, and optimizes the security architectures for future flexibility. It works great in a data-centric world with shifting threats and perimeters and provides you with new network designs that integrate connectivity, transport, and security around potentially toxic data. John calls this “designing from the inside out.”
PRO TIP: Docker is often viewed as a simple and fast replacement for virtualization. While this is true to certain extent it should be noted that containers, compared to virtual machines, do not provide the same level of isolation from either the host or other containers running within the same host. Please always remember that the host’s kernel is shared among all the running containers.
The Cloud Native Computing Foundation (CNCF) published a couple of helpful pointers: 9 Kubernetes Security Best Practices Everyone Must Follow and also Nikita Mazur shared some great insights into the things you should be doing to secure your Kubernetes environment.
Beware: the Kubernetes network stack!
The most contentious point for building a Kubernetes cluster is the network stack. There are a ton of providers available and if you have never deployed K8s before, it can be daunting!
Kubernetes handles networking using a different approach to the normal ‘Docker way’ of doing things. Docker uses port-forwarding from the host to the underlying container using virtual bridge network. This means that coordinating ports across multiple devices is a difficult thing to do at scale and exposes users to cluster-level issues outside of their control.
Kubernetes on the other hand imposes the following requirements on its network implementation:
Rory Chatterton does a great job in explaining, and providing you with an approach to use.
DevSecOps is the new DevOps
While DevOps approach to software delivery has positively revolutionized the IT industry, the security requirements have largely been left unattended. This is what DevSecOps aims to fix.
DevSecOps is one of the most important DevOps trends. It is an approach to IT operations security, allowing to utilize the principles and best practices of DevOps to ensure better, faster and more secure software delivery. This essentially means, that all the security requirements are codified and weaved into the automated unit tests from the start, instead of having to deal with them before the product release.
“DevSecOps is the trend to automate all security checks by codifying them in the unit tests and using them from the very beginning of the software development, not at the end of the cycle.”
Arseny Chernov has written a lengthy article on securing DevOps and what approach to take. A recommended reading if you want to go from DevOps to DevSecOps.
Summary
Kubernetes is a very powerful platform that makes application deployment much easier compared to a traditional on-premises server or a VM. That’s probably one of the key success factors that made Kubernetes adoption boom over the last couple of years. The counterpart is that this way of doing things is relatively new, which unfortunately sometimes leads to poor implementation of good practices in terms of security.
The need to secure your Kubernetes environment is real, as exploits are now being found in the wild. Learning about Kubernetes and understanding how the framework works is essential to understand what to secure and I would certainly recommend Nigel Poulton’s course for this.
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
272 
272 
272 
Microsoft MVP and Microsoft Regional Director.
"
https://medium.com/@mauridb/calling-azure-rest-api-via-curl-eb10a06127?source=search_post---------45,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
May 4, 2018·4 min read
In these days I needed to call Azure REST API directly, without having the possibility to use some nice wrapper like AZ CLI or .NET SDK or Python SDK or whatever, since the REST API I needed to call was not included in any of the mentioned tools.
Make sure you check out the latest updates at the bottom of the article!
I decided to use curl since it is one of the easiest way to issue HTTP requests. But it turned out to be a little more complex that I what I could have expected at the beginning, especially while dealing with the authentication phase. The entire process is pretty simple as you’ll see, documentation is just a bit scattered all around...so it may be difficult to quickly understand the path you must follow to get everything working nicely.
Azure API security, and thus authentication (which is based on OAuth2) is a pretty broad topic as you can see from the long documentation available here:
docs.microsoft.com
I read throughout all the documentation, hyperlinks included and at the end I was still confused. There are so many options and each one have quite a number of prerequisites that requires even more reading. So, for my future reference and for all those who just need a straightforward way to solve the problem, here’s the list of all steps required.
In order to access resources a Service Principal needs to be created in your Tenant. It is really convenient to do it via AZ CLI:
for much more details and options see the documentation:
docs.microsoft.com
What is happening here is that you’re registering your application in order to be able to be recognized by Azure (more precisely: from the AD tenant that is taking care of your subscription). Exactly like when you register your application to access Twitter or Facebook in order to be able to read and write posts/tweets/user data and so on.
As said before authentication used the OAuth2 protocol, and this means that we have to obtain a token in order to authenticate all subsequent request. We need to use the client_credential flow:
all the three required information:
can be obtained from the previous step. You already have the PASSWORD since you used it to create the Service Principal. The TENANT_ID and the APP_ID will be returned by the az ad sp create-for-rbac command you executed before. Otherwise you can execute the following az command to find it the tenant id:
And the following to get the APP_ID:
The result of the curl call will be an Authorization Token that looks like the following:
The obtained token that needs to be used in the Authorization HTTP header as the Bearer Token to make sure your HTTP call will be authorized:
And that’s it. Is really easy at the end. And once you have the token it is also easy to use it in your preferred REST client tool, be it Postman or Insomnia.
If you want learn more on how to use the OAuth2 authentication protocol to access Azure, just go here:
docs.microsoft.com
If you need a token just to run some test and you don’t want to go through Service Principal creation I just discovered that now you can just do
and you’re good to go, you’ll get your access token with a maximum validity of 1 hour, which is more than enough to do tests. Using curl is really easy now:
In the latest version of AZ (2.0.67, at moment of writing) there is new command rest that allows to call any azure REST API with just one command:
the {subscriptionId} will be automatically replaced with your active subscription Id. Great! Can’t be easier that this now :)
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
407 
14
407 
407 
14
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://medium.com/@gercheq/how-500-000-microsoft-azure-sponsorship-might-kill-your-startup-42912f9b22a1?source=search_post---------46,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gercek Karakus
Aug 6, 2018·6 min read
At Raklet, we’re building automated messaging and payments solutions for communities. We got accepted into Microsoft Ventures Accelerator in April 2016 and we got offered $500,000 Azure sponsorship for the 3 years. Yay!
We were really happy to receive the credit as server costs were one of the major costs at that time. We started migrating our infrastructure from a monolithic architecture to a more scalable modular one. We’ve setup local, dev, test, prod resource groups; scaled up our app services and databases. Everything was running much faster, finally!
Then, fast-forward 24 months, I realized that our monthly server costs ramped up above $20,000/month. Unbelievable.
At that point, we only had 12 months left on our sponsorship and we’d run out of credits even before the expiration deadline. So it was time for us to focus on infrastructure optimization instead of growth.
Billing for Microsoft Azure Sponsorship is not on the portal so you have to visit an external website for the usage.
Visiting the https://www.microsoftazuresponsorships.com website, you can only generate an excel sheet similar to the one below:
ResourceIDs and names in this sheet do not match to the ones on Azure portal. So we decided to reach out to the Professional Direct Services. After spending 2 months back and forth with them, they simply gave up and we were left with this problem. There’s no way for Microsoft Azure Sponsorship usage to be tied to actual resources on Azure Portal.
I hope this email finds you well. Please note that my team, the Startups Business Desk, has been closely following the congruent support request #118041818024304. As stated in that support request, it is a technical limitation of Azure Sponsorship reporting that we are unable to provide usage by resource group. We understand that this is an important feature and heavily weighs on your continued use of the platform and apologize that we cannot provide a workaround at this time. Please note that engineering is working to implement reporting by usage group in the future. However, we do not have an ETA. — Microsoft Azure Team
We were left with the excel sheet to figure out what was going on with our billing. So, we built pivot tables to figure out what was going on with our account.
We downloaded the excel file and became a pivot table master. With some tricks, we were able to identify break down of each service such as name, type and resource.
Sounds like the ultimate solution but again, ID or names in this sheet do not map to your resources on Azure.
We were still not able to identify which resources were being used for different resource groups (local, dev, test, prod) out of our 250+ resources. So we had to drill down into each resource to find our way to optimization.
We were able to drop down to $7.5k/month in May and $4k/month in July. Below you can see the breakdown for July 2018.
Let me summarize what we have done for each service type and how much it helped:
With a little bit educated guess work, we can categorize resources to different groups (again in a sheet) and see that our daily costs are around ~$120/day as of August 1st, 2018.
We’re lucky to identify this issue with Azure billing with sufficient time in our hands so that we can prepare accordingly.
We’ve always postponed optimizing our infrastructure in order to focus on growth which is pretty much understandable startups searching for product-market fit. However, years pass by with the blink of an eye and credits expire much faster than anticipated.
Our goal is to drop our costs to $1,000 per month and we’ll see how far we can go…
Thanks for reading this far and please spread the word 🙏
This story is published in Noteworthy, where thousands come every day to learn about the people & ideas shaping the products we love.
Follow our publication to see more product & design stories featured by the Journal team.
Product Architect @Raklet
524 
8
524 
524 
8
Product Architect @Raklet
"
https://faun.pub/deploy-to-azure-kubernetes-service-aks-using-azure-devops-and-helm-77f3fa804ee7?source=search_post---------47,"Lately I was seeing lots of innovation and excitement on deployment automation using DevOps and Kubernetes. This have nothing to do with software development (I mean writing code). This is the business of the DevOps guys. Especially for someone like me who spent the last five years focusing on software development, and so much far away from anything Ops.
I was curious, but really curious so that I decided I want to learn more about these stuff. In this blog I want to share what I have learned.
I started with Azure DevOps, created the CI/CD pipelines for ASP.NET Core, Angular, Xamarin and Dockerized apps. I used Azure and App Center to deploy to.
Then I started thinking about the Database ! I found we can add it to the CI/CD pipelines. During the Build, we create the .dacpac file. Then we publish it during the release. I also used ARM templates (IaC) to automate the deployment of the infrastructure.
After that, I moved to Kubernetes, created a Docker container and deployed it to k8s. At first, I used kubectl to deploy manually. Then I automated the process of deployment through Azure DevOps.
This blog will serve as a continuation to these workshops. Should we start ? I hope you got a clear idea about k8s and you are ready to learn Helm Charts. So what is Helm ?
Helm is the package manager for Kubernetes.
It is npm for javascript, nuget for .NET and maven for Java. Helm makes it easier to deploy apps to Kubernetes. It can deploy multiple apps with multiple yaml files as one single package. It provides a way to solve dependencies between different resources. The package is called Chart. Charts could be shared and reused by different teams. If you want to deploy Cassandra, you’ll have a Chart for that. If you want to install SonarQube or Jenkins, then that becomes easy as running one single command. This means you don’t need to figure out how to install the web app, the database, mount a disk volume, connect web app to database, run a script that waits until the database starts...
The community creates the Charts and they are verified before they get published as official/stable. They are published here: github.com/helm/charts/tree/master/stable
The traditional way to deploy an application to k8s is through yaml files. These files contains k8s objects like Deployment, Service, ConfigMap, Secret… They describe the required configuration like which docker containers to use, the networking, the persisted volume, secret keys… The ones we use are here: github.com/HoussemDellai/ProductsStoreOnKubernetes
Once we have the required yaml files, we can deploy them to Kubernetes using the following command:
kubectl apply -f deployment.yaml
Helm needs to be installed into both the machine (client) and Kubernetes cluster (server). For the client and depending on the OS, it could be installed by running one of the following commands:
For the server side (Helm is called Tiller in this land), it could be installed by:
Check the documentation here for more details on the install. docs.helm.sh/using_helm/#installing-helm
To generate a sample Chart, we can use helm create command. This will create sample files for deployment, ingress and service. All they have values defined in values yaml file. The Chart file defines the version of the template. helmignore is the equivalent of gitignore, used to not include specific files into the package. Then, lint command to validate the template.
With Helm, we will reuse the same k8s yaml files, but we’ll make them configurable and reusable. We’ll extract all the values that might change from one deployment to another. And we’ll put them inside a specific file that contains all these values defined as key-value pairs. This file is values.yaml.
Kubernetes configuration files (deployment, service, pvc, configmap, secret, pod…) can use these values to set its properties. The advantage is now more clear: we have one single file to configure k8s objects.
Note: Another advantage of using Helm is to resolve dependencies between objects.
Now these values should be substituted using Helm. For this reason, we need to install Helm on both the client machine and the cluster (server). Helm on the server is called Tiller. Here’s the official guide to install Helm (helm.sh/).
Note: Helm on the server called Tiller will be removed by the v 3.0 of Helm.
Now we can deploy the the k8s objects using Helm cli instead of kubectl cli. The first step is to create the package:
This will create a tgz compressed file that contains all the files with a specific version 0.1.0.
Then, to deploy the generated package, we run:
This will deploy the v 0.1.0 of the the package. To update the app, we make the changes to the files, generate a new package (v 0.1.1), and run:
At this point you can check if the app was successfully deployed by running either viewing the kubernetes dashboard,
$ kubectl get {deployments/services/pvc/configmap} or using $ helm
We want to automate the deployment. The goal is that in each time we push new commit to the app’s source code, a new package will be created during the CI pipeline. And that package will be deployed during the CD pipeline.
The CI/CD pipelines are at the end a sort of sequence of command lines. This means that the task that will create the Helm package should run the command $ helm package productsstore.
But before that, we need to make sure Helm is installed on the client side. For that, we’ll use the task “Package and deploy Helm charts” with the command:
Now, we are ready to create the Helm package. We’ll use again the same previous task to generate the package, as following:
Note: The first two tasks, will build and push a Docker container into Docker Hub. And the last task will copy and publish the Helm package to an artifact called drop.
We are done with the CI/Build pipeline. We can run the pipeline using the Queue button and check from the console view that Helm was installed and the package was generated sucessfully.
Let’s move to the CD/Release pipeline. In this stage, we want to get the package published during the Build and deploy it to Kubernetes. Let’s start creating the pipeline by choosing the template “Deploy an application to a Kubernetes cluster by using its Helm chart.”
After configuring the connection to AKS cluster, we need to go through 3 steps. First, we need to install helm in both client and server (Tiller). That’s the role of the first task which will install 2.11.0. Then, we need to run helm init, to configure helm on client side. Finally, we run helm upgrade with the specified package.
Note: Despite we already installed Helm on client side during the Build pipeline, we need to install it again during the Release. That is because sometimes it is not the same machine that runs both Build and release.
Now we are ready to deploy our app by running the Release pipeline.
Join our community Slack and read our weekly Faun topics ⬇
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
237 
1
237 claps
237 
1
Written by
Premier Field Engineer at Microsoft, ex MVP, Cloud & DevOps enthusiast. I share what I learn in my professional experience through articles and videos.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Premier Field Engineer at Microsoft, ex MVP, Cloud & DevOps enthusiast. I share what I learn in my professional experience through articles and videos.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
"
https://medium.com/data-hackers/aws-vs-google-cloud-vs-azure-o-que-cada-um-tem-de-melhor-52107174f7b7?source=search_post---------48,"There are currently no responses for this story.
Be the first to respond.
Pessoal de tecnologia já sabe: depois do “minha linguagem é mais cabulosa”, tem discussão mais comum no buteco da TI que “qual Cloud é melhor?”
Inspirado por uma discussão no Slack do Data Hackers, resolvi escrever esse texto breve sobre as MINHAS opiniões sobre o que cada cloud tem de vantagem sobre a outra. Minha avaliação, obviamente, é enviesada. Se você tem outra opinião, comenta aí pra gente trocar aqueles 5 minutos saudáveis de porrada, sem perder a amizade HAHA!
A cultura da Amazon sempre foi de fazer o possível pelos consumidores. Jeff Bezos não tem só a cara de louco, ele realmente é louco pelo mercado.
Muita das vezes, a AWS lança um produto sem ele estar nem mesmo pronto! Isso é horrível? Sim! Porém abre uma vantagem absurda: a de ouvir os primeiros clientes que aplicam para o teste da ferramenta— que são os que querem aquele produto badly!
Mas uma coisa que eu vejo que é essencial, e muita gente que tá fora do ramo de cloud as vezes se esquece, é a força da Rede de Parceiros. Um grande fornecedor, não consegue vender, nem suportar as vendas, sem uma forte rede de suporte. E a AWS valoriza, capacita e apoia muito seus parceiros.
Pensa só, um Arquiteto de Soluções da AWS (SA), por mais foda que seja, nunca vai alcançar a expertise de um bom consultor com anos de experiência numa sub-area da cloud (containeres, data, ML…). Com 2 anos, é bem provável que o SA saia do país. Fora que ele fica longe do dia a dia dos projetos nos clientes, que é onde o pau quebra de verdade. Essa é indiscutivelmente uma vantagem competitiva da AWS. A Oracle dominou (e ainda domina) o mercado de bancos de dados on-premises justamente com essa estratégia. Andy Jassy, que não é bobo, copia o que deu certo.
Todos os grandes produtos da Google Cloud são fruto de anos de pesquisa dentro da própria empresa. Podemos listar alguns que tem liderança quase que inquestionável:
GCP preza pela inovação dos seus serviços. Tais produtos são o estado da arte nas áreas de domínio, e por isso dominam o mercado com folga. GCP, por sua vez, tem uma rede de parceiros fraca e, francamente, não está nem aí pra eles. Se tivesse uma boa rede e um customer services decente, estaria despontando nessa briga, sem sombra de dúvidas.
Desde que Satya Nadella assumiu como CEO da Microsoft, a estratégia da empresa mudou muuuuito. Você deve ter percebido isso após a compra do Github, pela bagatela de U$ 7.5 bilhões. Mas o que talvez você não saiba, é que grande parte dos novos serviços da Azure são sobre plataformas open-source amplamente adotadas. Posso listar algumas:
O suporte da Azure e o apoio à rede de parceiros é bom, não é tão forte quanto da AWS, mas beeem melhor que o da Google. Com essa política, a Microsoft diminui a fricção de adoção de cloud.
Isso sem contar que Cloud Híbrida já existe na Azure desde sua fundação, principalmente para virtualização e bancos de dados.
Digital Ocean: se destaca pela facilidade de uso. É muito fácil começar um projeto na Digital Ocean. O problema vem quando se exige escala. É comum ver empresas migrando para uma das 3 anteriores, visando diminuir os custos e ter acesso a serviços mais complexos que simplesmente VMs, Redes e Storage.
Heroku: é a mais dev-friendly de todas. Claro que é mais um PaaS do que um IaaS, mas vale citar dado o tempo de mercado e grande penetração no mundo dev. Lembro ainda que a Heroku foi a primeira a disponibilizar Kafka-as-Service de todos provedores.
IBM/Red Hat: nem iria citar a IBM aqui, já que seu projeto de Cloud, o Bluemix, vinha sofrendo várias baixas no mercado. Mas um shift bem importante nessa batalha foi a compra da Red Hat, que trouxe todo o know-how da plataforma OpenShift, o melhor provedor de serviços de Containeres na minha opinião de noob.
Resumindo, as principais características que levam a adoção de cada cloud, na minha opinião são:
Claro que existem diversos casos e nem sempre isso é verdade. A melhor cloud é aquela que resolve seu problema, mais rápido e no menor custo. E essa decisão tem muito mais variáveis que essas que apontei nesse texto.
[Edit 1] — Eu não entrei em detalhes de custo. Por dois motivos: qualquer um consegue avaliar isso no primeiro contato com qualquer cloud. Não faz sentido expor isso, já que escrevo a partir da minha razoável experiência com clouds. Segundo: custo é relativo e particular de cada produto e à produtividade do time que faz o sustain/evolution da plataforma. Quanto mais um produto me entrega, melhor o meu serviço/produto. Minha organização ganha sobre isso em diversos aspectos. Logo, avaliar somente custo, não é diferencial… é tratar tecnologia como commodities.
E aí? Qual sua opinião sobre essa guerra de clouds? Você é fanboy de alguma?Comenta aí enquanto eu vou buscar minha luva de boxe 😁 Valeu e até a próxima!
Blog oficial da comunidade Data Hackers
285 
2
285 claps
285 
2
Written by
CTO | Lead Data Engineer | Co-Founder of Data Hackers and Dadosfera. Loves science, code and cats ^*^
Blog oficial da comunidade Data Hackers
Written by
CTO | Lead Data Engineer | Co-Founder of Data Hackers and Dadosfera. Loves science, code and cats ^*^
Blog oficial da comunidade Data Hackers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/deploying-terraform-infrastructure-using-azure-devops-pipelines-step-by-step-d58b68fc666d?source=search_post---------49,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 13, 2020·13 min read
Azure DevOps is a hosted service to deploy CI/CD pipelines and today we are going to create a pipeline to deploy a Terraform configuration using an Azure DevOps pipeline.
In this story, we will take a look at a step by step procedure to have our Azure DevOps Pipelines ready in few minutes.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/using-aad-pod-identity-in-your-azure-kubernetes-clusters-what-to-watch-out-for-73d5d73960f?source=search_post---------50,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Azure has been improving its products to support contextual authentication. This is powered by a feature called Managed Identities, which, in simple terms allows you to assign an identity to a Azure Compute (i.e. AKS, Virtual Machine, App Service, etc). Applications inside that compute can then request access tokens to access other Azure Services, such as a Sql Azure database or KeyVault instance.
Without going into too much detail, this is based on the Azure Instance Metadata API. A HTTP end-point which returns a new access token when called inside an Azure compute with this feature enabled. The URL for calling it looks like this:
In this case specifically, instead of having a connection string in your application with a “fixed” user name and password for your database, you would instead use an access token generated at run-time. An example from the official documentation on setting this for an App Service and Sql Azure can be found here.
The idea is quite interesting and provides a few security benefits for your applications, such as:
Last year, Microsoft started an open source project to bring this concept to Kubernetes clusters, allowing you to bind an Azure Managed Identity to a running Pod, its name is aad-pod-identity.
To make the security concerns raised here clearer, we will consider a multi-workload cluster, shared among at least two different teams. Each one of them being responsible for their own pipelines, namespaces (within the cluster) and resource groups (within Azure), all properly configured and with least privileges in mind.
The green box represents the cluster control plane, whilst the blue box represents Azure control plane. The red dotted rectangle represents what Team A should have access to, whilst the blue is the same for the Team B.
The idea behind the approach above is to implement security in depth. Ensuring that if any point gets compromised, it stays contained within its vicinity and doesn’t take over the entire cluster or Azure resources. There are a lot of things that need to be put in place in order to attain that, which goes beyond this post. The goal here is to highlight where AAD Pod Identity may make it harder to attain that.
Before going any further, it’s important to first understand how it all fits together.
Once deployed, aad-pod-identity will add a few new things into your cluster:
Node Managed Identity is a daemonset, that hijacks (yes, this is pretty much the same as a man-in-the-middle attack) all calls to Azure’s Instance Metadata API from each node, processing them by calling MIC instead.
Managed Identity Controller is a pod that invokes Azure’s Instance Metadata API, caching locally tokens and the mapping between identities and pods.
A new Customer Resource type that represents an Azure Identity inside Kubernetes.
A new Customer Resource type that links Azure Identities to Pods inside the cluster, using labels.
A sample application would look like this:
There are a few things worth noting;
Here are the scenarios that could lead to lateral movement and privilege escalation by abusing the default settings of this project.
Some of these considerations may or may not be relevant to you. This will largely depend on your security posture, and your use case. For example, if you only have a single application within your cluster, a single Azure Identity shared across all applications, or if in general “least privilege” is not high in your priority list. :)
By re-using the same value for the aadpodidbinding label in the customer-pod, a rogue pod would be linked to the customer identity. Now if it requests for an access token from the Instance Metadata API, it will be granted one that allows it to do any actions that Customer Identity is allowed in Azure:
In this scenario, a compromised account with read access to the default namespace and create pod access anywhere else within the cluster, would be able to acquire access tokens on behalf of the Customer Manager Identity.
What makes this more likely to happen, is the fact that both AzureIdentity and its binding are in a shared namespace. Which can mean that multiple teams/applications will have access to it, even if just as read-only.
Similarly to the scenario above, a new AzureIdentityBinding can be created, linking to the existing customer AzureIdentity.
At this point in time, the AzureIdentityBinding object must be created in the default namespace. Once namespaces are supported, it could also be deployed in other namespaces, if the annotation forceNamespaced is not set to true.
In this case, the compromised account trying to move laterally would need pod read and AzureIdentity/Binding write accesses to the default namespace and also create pod access anywhere else within the cluster.
Another version of the scenario above. This time creating both an AzureIdentity and AzureIndentityBinding, however, pointing to the Managed Identity in the prod-customer-rg resource group.
Access permissions and threat vectors would be pretty similar to the previous scenario.
The NMI pod “intercepts” all the requests to the Instance Metadata API, and handles it internally. However, none of this traffic is sent using a secure channel (TLS). The access tokens in always transported in plain-text.
Privileged pods with NET_ADMIN capabilities can eavesdrop network traffic on anything scheduled on the same node, which could benefit from plain-text sensitive information being sent around. It is actually worse on this case, that the end-point address is well-known.
Note that if the cluster is configured to have Managed Identity enabled, the NMI pod will talk with the Azure Instance Metadata. If it is not, it will talk with Azure AD instead, in which case, part of that communication would be under TLS (from nmi to Azure AD) and part of it won’t (from requesting pod to nmi).
This is a bit more complicated than the previous scenarios, although it only requires the compromised account to have create pod access. There must be nothing blocking the creation of privileged pods — which is the default behaviour.
Note that it does not matter what namespace the rogue pod is deployed. A privileged pod goes beyond that level of isolation, and will have access to all the traffic within the node it was scheduled in.
Currently AKS stores in plain-text the SPN (Service Principal Name) credential used for the cluster to talk with the Azure API. This file is physically available within each and every node. This is a known issue and I blogged about it a while ago.
The MIC pod mounts that file into itself, therefore, any user (or service account) with execute access on MIC has access to it with a single line:
Note that the existing template provided on the aad-pod-identity places all its components in the default namespace.
The threat vector here is any user, or service account, which has pod execute access in the default namespace would be able to privilege escalate to at least have access to all Azure resources that the cluster has. For example, it could manipulate the node VMs, load balancer, Network Security Groups (NSGs), etc.
This can also be a problem on AKS-Engine, if it was setup with useManagedIdentity = false.
The overall concern here is “how easy” lateral movement between the cluster Control Plane and the Azure Control Plane can be achieved, for a non-admin user, after all, none of the scenarios above requires clusterAdmin rights.
The recommendations below covers the scenarios mentioned above for the implementation of AAD Pod Identity. But it also focuses on improving the isolation within the cluster. Here it goes:
Almost two decades ago it was quite common to find references to SD3+C in Microsoft’s security literature. That concept was a shorthand for: Secure by Design, Secure by Default, Secure in Deployment, and Communications. Basic concepts which meant a more holistic view to security in software development.
That concept was as valid then, as it is now, especially when security is high on your priority list. However, it is clear that Secure by Default is not “top of mind” in quite a few open source projects (including Kubernetes), which then requires a lot more know-how from end-users to implement them securely.
ITNEXT is a platform for IT developers & software engineers…
205 
1
205 claps
205 
1
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/how-to-pass-variables-in-azure-pipelines-yaml-tasks-5c81c5d31763?source=search_post---------51,"There are currently no responses for this story.
Be the first to respond.
This is a quick reference on passing variables between multiple tasks in Azure Pipelines, a popular CI/CD platform. They have recently enabled support for multi-stage pipelines defined in YAML documents, allowing the creation of both build and release (CI and CD) pipelines, in a single azure-pipelines.yaml file. This is very powerful, as it lets developers define their pipelines to continuously build and deploy apps, using a declarative syntax, and storing the YAML document in the same repo as their code, versioned.
One recurrent question is: how do you pass variables around tasks? While passing variables from a step to another within the same job is relatively easy, sharing state and variables with tasks in other jobs or even stages isn’t immediate.
The examples below are about using multi-stage pipelines within YAML documents. I’ll focus on pipelines running on Linux, and all examples show bash scripts. The same concepts would apply to developers working with PowerShell or Batch scripts, although the syntax of the commands will be slightly different. The work below is based on the official documentation, adding some examples and explaining how to pass variables between stages.
This is the easiest one. In a script task, you need to print a special value to STDOUT that will be captured by Azure Pipelines to set the variable.
For example, to pass the variable FOO between scripts:
Full pipeline example:
You can also use the $(FOO) syntax inside task definitions. For example, these steps copy files to a folder whose name is defined as variable:
Wondering why the vso label? That's a legacy identifier from when Azure Pipelines used to be part of Visual Studio Online, before being rebranded Visual Studio Team Services, and finally Azure DevOps!
Passing variables between jobs in the same stage is a bit more complex, as it requires working with output variables.
Similarly to the example above, to pass the FOO variable:
A full example:
At this time, it’s not possible to pass variables between different stages. There is, however, a workaround that involves writing the variable to disk and then passing it as a file, leveraging pipeline artifacts.
To pass the variable FOO from a job to another one in a different stage:
Example:
Here’s the pipeline running. Note in the second stage how line #14 shows some value in both bash scripts. However, take a look at the script being executed on line #11: in the first case, the variable was expanded inside Azure Pipelines (so the script became echo ""some value""), while in the second one bash is reading an environmental variable (the script remains echo ""$FOO"").
If you want to pass more than one variable, you can create multiple files within the $(Pipeline.Workspace)/variables (e.g. for a variable named MYVAR, write it inside $(Pipeline.Workspace)/variables/MYVAR), then read all the variables in the second stage.
Originally published at https://withblue.ink on August 5, 2019.
Any language.
326 
8
326 claps
326 
8
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Cooker of great risotto. Sometimes tech nerd. Driving dev tools, @code & open source @Microsoft @Azure ☁️ Opinions are mine 🇮🇹🇨🇦🇺🇸
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://koukia.ca/entity-framework-core-2-0-vs-dapper-net-performance-benchmark-querying-sql-azure-tables-7696e8e3ed28?source=search_post---------52,"While the .Net Core 2.0 is still being baked (Preview 3 is out now!), I thought I’d give the Entity Framework Core 2.0 a try and do a benchmark and compare it to Dapper.Net.
So without further rant, lets get to it:
"
https://medium.com/microsoftazure/azure-event-grid-the-whole-story-4b7b4ec4ad23?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
Azure Event Grid is a cloud service that provides infrastructure for event-driven computing. Event Grid focuses on events or messages that declare, “something happened.” It is not designed for commands that imply “something will happen.” The service allows you to send messages, route them to endpoints, and consume them with custom code. It enables near real-time delivery (typically less than one second) at scale (thousands of events per second).
Most Azure services automatically send messages through Event Grid and many can directly consume messages “out of the box.” Event Grid also supports posting to secure web API endpoints to deliver messages and uses the WebHook standard for delivering messages. Therefore, any language or platform that supports posting to a web endpoint and consuming an HTTP payload is capable of working with Event Grid.
Azure Event Grid supports a “push model.” Your application will never have to ask or “poll” for events. Instead, events are delivered immediately to an endpoint that you specify. This means your code can respond to events as they happen. In many cases it can also lead to cost savings because it removes the overhead of polling on a regular basis and instead triggers code only when it is needed to consume an event.
The infrastructure is fully managed through a serverless model: it automatically scales to meet your demands and only bills when you are actively using the service. To illustrate the billing model, consider that five million events are published during the month to two active subscriptions. One of the subscriptions has a filter for “advanced match” of messages coming in, resulting in one million messages passing the filter. One service handler goes down for a period, missing one million messages (so the delivery attempt fails). Event Grid is built to redeliver all of those messages after the service comes back up. The cost comes out to $10.14 USD for the month.
You can explore pricing further by using the 💰Azure Pricing Calculator. For the cost, Event Grid provides guaranteed delivery in a 24-hour window (with option to save undelivered messages) and 99.99% availability.
Now that we’ve introduced what Event Grid is, let’s dig into some examples of how you would use it in practice. After an application updates a status, it may raise a message “status was changed.” This then allows you to consume the message to integrate with other systems, kick off workflows and synchronize data.
The following diagram represents an example of a serverless application that uses Event Grid, taken from a session in Microsoft Ignite | The Tour. We will use this example to explore Event Grid concepts and terminology.
In this example application, the command line interface (CLI) tool is responsible for uploading images and updating a table of SKUs. A SKU is a stock-keeping unit and represents a unit of inventory. The CLI raises several events related to the lifecycle of a SKU:
In response to the events, three workflows are kicked off:
The important thing to note is that all these processes are built independently of the main application. After it sends the appropriate messages, the rest of the application can be built independently with multiple workflows kicked off by the same messages. This enables not only application scale, but the ability to scale development teams as the product grows.
🔗Access the GitHub repository for the Tailwind Traders example
There are several concepts that are useful to understand when working with Event Grid.
Native Azure services have predefined topics and already publish messages. These messages are available for you to subscribe to from the appropriate source. For custom topics, you simply create your Event Grid topic and then begin publishing and setting up subscriptions. The topic has an endpoint to publish messages and a set of keys for secure access. There are many other features and available options for configuring Event Grid to meet your specific needs.
The following section provides video to illustrate the various features available with Azure Event Grid.
Note: all the following videos have no audio.
In this video, an Azure Function is created with an Event Grid trigger (it is called by Event Grid for a subscription). The subscription is added to Azure Storage, so that whenever a file is uploaded to blob storage, the function is triggered.
As another example of how Azure services are built ready to consume Event Grid events, this video shows how to consume events using Azure Storage Queues.
This video shows how to create a custom Event Grid topic.
The Event Grid SDK is available for many popular languages and platforms. The library makes it easy to authenticate and publish messages. In this .NET Core example, a payload with information about a SKU event is wrapped in a message and published.
It is not necessary to use the SDK. Any language or platform that supports setting an authentication header and issuing an HTTP POST is capable of publishing to Event Grid. This is the .NET Core code to publish without using the SDK.
🔗 Access the GitHub repository for the “//build Event Grid” example
The code for the handler has two responsibilities. The first is to honor a validation handshake. To avoid spamming endpoints with messages, Event Grid requires a handler to “opt-in” to a subscription. It will post a special validation request with a unique token that must be echoed back. If this doesn’t happen, no further messages will be sent to the handler. The second responsibility is to simply parse messages as they come in.
The following code is part of an ASP.NET Core MVC app. The controller exposes an endpoint to receive the messages. It will echo back the validation token for subscriptions and write the contents of the payload to the application logs for incoming messages.
These examples in .NET Core can easily be implemented in Go, Node.js, Ruby, Python, or any other language.
Sometimes it may not be possible to modify the handler to echo a validation request back. When requesting validation, Event Grid sends an optional validation URL that can be used instead. It expires after several minutes. The URL can be accessed using the GET method, so it is suitable for either automatically validating in code (for example, by a third-party process that examines the application logs and issues the request) or manually by pasting it into the browser. This video demonstrates manual validation.
Event Grid guarantees delivery within a 24-hour window. If a handler goes down for any reason, Event Grid will continue to retry until the delivery window expires. This video shows how to configure the delivery time frame and retry count and verify that guaranteed delivery works by bringing a handler down, issuing a message, then bringing it back up to confirm delivery.
If a handler is not able to recover in a 24-hour window, Event Grid will stop trying to deliver those messages. It is possible to configure Event Grid to store “dead letters” (messages that could not be delivered) in storage. This way you can parse the missed messages and event replay them once the handler is back up and running. This video shows how to configure and confirm dead letter delivery.
Event Grid uses a proprietary schema to wrap messages. The Event Grid team worked with the Cloud Native Computing Foundation (CNCF) to create an open standard for cloud messaging called CloudEvents. The following video shows how to configure Event Grid to use the CloudEvents schema instead of the proprietary one.
The final video shows an alternate way to consume events via Azure Logic Apps to kick of integrations and workflows.
Tip: the example uses the Event Grid schema to parse information from the header. The Data portion (payload) can contain custom properties depending on the event, so it’s not available to parse automatically. Logic Apps provides a Parse JSON connector that allows you to specify the schema of the payload and parse its information in later steps.
In this article you learned about Azure Event Grid, one component of the Azure serverless platform that provides the infrastructure for event-based applications. You calculated how Event Grid bills per operation, explored terminology and concepts behind Event Grid, and studied an example serverless application that uses Event Grid. The walk through demonstrated how to use various features from publishing and subscribing to configuring delivery retries and capturing undelivered messages in storage.
A major benefit of Event Grid is the ability to manage all of your events in one place. It was also built to reliably handle massive scale. You get the benefits of publishing and subscribing to messages without the overhead of setting up the necessary infrastructure yourself.
👍🏻 You can learn more about Azure Event Grid and get started by visiting the comprehensive Event Grid documentation.
🔗Access the GitHub repository for the Tailwind Traders example
🔗 Access the GitHub repository for the “//build Event Grid” example
Are you using Event Grid in your own solutions? Do you have questions or feedback after reading this article? Please share your stories, insights, suggestions and feedback in the comments below!
Any language.
240 
2
Thanks to sigje. 
240 claps
240 
2
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/t-t-software-solution/%E0%B9%81%E0%B8%8A%E0%B8%A3%E0%B9%8C-mock-up-%E0%B9%83%E0%B8%AB%E0%B9%89%E0%B8%81%E0%B8%B1%E0%B8%9A%E0%B8%97%E0%B8%B5%E0%B8%A1%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%A1%E0%B8%B5%E0%B8%84%E0%B9%88%E0%B8%B2%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B8%88%E0%B9%88%E0%B8%B2%E0%B8%A2%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-pencil-azure-2142ee7d85f8?source=search_post---------54,"There are currently no responses for this story.
Be the first to respond.
ผมได้มีโอกาสหาวิธีในการ สรุป Requirements ที่ได้รับมาจากทางลูกค้า และเสนอไอเดียเบื้องต้นให้กับทางทีมงานของผม ทั้ง Designer และ Developer
เลยตั้งโจทย์กับตัวเองว่า ทำยังไงให้สามารถสรุปและส่งข้อมูลต่างๆเหล่านี้ให้กับทีมได้เร็วที่สุด ซึ่งผมสรุปกับตัวเองคร่าวๆได้ดังนี้
จากโจทย์ข้างต้น ผมจึงตัดสินใจใช้ Tools ซึ่งไม่มีค่าใช้จ่าย ดังต่อไปนี้
ซึ่งไอเดียผมก็คือ จะ ใช้ Pencil เพื่อ สรุปไอเดียและ Export เป็น HTML และนำ files ไปวางไว้ใน Azure ครับ
เรามาเริ่มต้นกันด้วย Pencil Tool กันก่อนน่ะครับ
Shapes
Clip-art
ตัวอย่าง Pencil Design ที่ผมสร้าง
ทำการ Export HTML File จะได้หน้าตาประมาณนี้
ตอนนี้เราจะได้ HTML Files สำหรับ Mock-up แล้วน่ะครับ ในขั้นถัดไปจะเป็นการติดตั้ง ลงใน Azure ซึ่งจะมีขั้นตอนดังนี้
ทำการกรอก
ขั้นตอนการ Deploy HTML files ขึ้นสู่ Azure
มีทั้งหมอ 2 ขั้นตอน คือ 1. FTP, 2. Local Azure Git
Deploy ผ่าน FTP
Deploy ผ่าน Local Azure Git
หวังว่าบทความนี้จะมีประโยชน์กับผู้อ่านที่ที่กำลังมองหา Tool และระบบ ในการสื่อสารภายในทีมน่ะครับผม
ขอบคุณมากๆครับ
นายป้องกัน
https://www.tt-ss.net/
150 
1

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
150 claps
150 
1
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/gochain/announcing-gochains-private-deployment-offering-on-microsoft-azure-dcfd28d6aa85?source=search_post---------55,"Sign in
There are currently no responses for this story.
Be the first to respond.
GoChainGo
Oct 4, 2018·3 min read
RENO, Nev. (October 4, 2018) — GoChain is excited to announce its private blockchain deployment offering is now available on Microsoft Azure. Enterprise clients and developers can easily deploy their own private GoChain network with a few clicks. As an Enterprise grade product offering, it can be used to build decentralized applications and smart contracts in a fast, secure, and reliable private environment. GoChain is now among the only public blockchain protocols with a private offering listed on the Azure Marketplace:
Official Microsoft Blog PostAzure Marketplace ListingDeploying GoChain on Azure
A private implementation of GoChain provides a number of advantages in five key areas compared to existing solutions:
Speed, Performance, and Reliability — all of the performance optimizations built into GoChain have been tested and proven in a private, controlled environment.
100% Ethereum Compatible — GoChain is frontwards and backwards compatible with Ethereum making it seamless and easy to transition from Ethereum to GoChain or vice versa with no code changes.
New Consensus Model — moving away from Proof-of-Work (PoW) increases the transaction speed, security, and scalability of the network via our Proof of Reputation consensus mechanism.
Upgradeable Smart Contracts — as every developer knows, the flexibility to update code to fix bugs or add functionality is critical. In blockchain, where a lot of value is on the line and hacking runs rampant, this is even more true. GoChain is developing upgradable smart contract functionality, where trust is retained between parties as well as moving critical loss prevention features, such as pausing a smart contract, to the protocol level.
Service and Support — GoChain takes pride in providing hands-on support to ensure our users have successful implementations. They have a client success and support team consisting of professionals and industry experts from IBM, Microsoft, and Oracle.
The GoChain team comes from Fortune 500 companies and Enterprise-level corporations. This has given them an in-depth understanding of what Enterprise clients need in a changing business ecosystem. On the technical side, clients want performance, reliability, and increased functionality. Our client success and support team allow our offering to go way beyond the deployment solution on its own. GoChain is proud to be the first and only blockchain company to provide a public and private blockchain solution designed with enterprise in mind.
“We are really looking forward to working with Azure clients and providing them with an Enterprise level of support not currently available in Blockchain today. This will also help drive mass Enterprise adoption across the GoChain ecosystem.”
— Jason Dekker, GoChain CEO
For press inquiries, contact GoChain’s Marketing Director, Adam Norris at anorris@gochain.io. Our press kit is available for download here.
About Microsoft AzureMicrosoft Azure is a cloud computing service created by Microsoft for building, testing, deploying and managing applications and services through a global network of Microsoft-managed data centers. It provides Software as a Service (SaaS), Platform as a Service (PaaS) and Infrastructure as a Service (IaaS) and supports many different programming languages, tools and frameworks, including both Microsoft-specific and third-party software and systems. Learn more at azure.microsoft.com
About GoChainGoChain, a public cryptocurrency, is a scalable, high-performance, low-cost, decentralized blockchain network protocol that supports smart contracts and distributed applications. It is the only network protocol that offers Enterprise level support, high transaction speeds, and upgradeable smart contracts. Learn more about GoChain at gochain.io.
More info at https://gochain.ioJoin our Telegram GroupPress kit here
Follow us on Social!TwitterMediumRedditCrunchbaseInstagramFacebookBitcoin Talk
The Medium Behind the Chain — https://gochain.io/
2.4K 
1
2.4K 
2.4K 
1
100% Ethereum Compatible, 100x Faster - https://gochain.io
"
https://medium.com/ontologynetwork/you-can-now-develop-ontology-smart-contracts-on-google-cloud-aws-and-azure-4c7425e0cfb7?source=search_post---------56,"There are currently no responses for this story.
Be the first to respond.
Today the Ontology Development Platform (ont_dev_platform) was released on Google Cloud Platform Marketplace, making Ontology one of the first public blockchains to have a development platform on the leading cloud provider marketplaces: Google Cloud, Amazon Web Services, and Microsoft Azure. Using the Ontology Development Platform on one of these cloud providers allows you to play around with and develop smart contracts without having to go through the fuss of configuring and setting up an environment locally.
Ontology has also joined the Google Cloud Technology Partner program, which gives Ontology the opportunity to collaborate with Google in marketing activities. With this new relationship and the development platform releases, Ontology hopes to grow the tech community and make developing dApps more accessible for all.
Are you a developer or want to start out? Please check out the:
Are you a developer? Make sure you have joined our tech community on Discord. Also, take a look at the Developer Center on our website, there you can find developer tools, documentation, and more.
Ontology website / Ontology GitHub / ONTO website / OWallet (GitHub)
Telegram (English) / Discord
Twitter / Reddit / Facebook / LinkedIn
A high performance, open-source blockchain specializing in digital identity and data.
973 
973 claps
973 
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
"
https://towardsdatascience.com/how-i-passed-the-microsoft-azure-fundamentals-certification-in-5-days-75b8e261d5d1?source=search_post---------58,"Sign in
There are currently no responses for this story.
Be the first to respond.
Arunn Thevapalan
Dec 27, 2020·9 min read
For the past 5 days, I’ve been preparing for an exam called Microsoft Azure Fundamentals AZ900. I sat for it today, and it turns out I passed. Yes, today. I’m writing this…
"
https://medium.com/@jeffhollan/serverless-doorbell-azure-functions-and-ring-com-f24b44e01645?source=search_post---------59,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Dec 20, 2017·3 min read
My parents sent me an early Christmas present this year — a Ring video doorbell. The doorbell notifies my phone whenever someone rings the doorbell (or if it detects motion) and can capture video and audio of my doorstep.
I’ve seen these doorbells in more than a few sales this year, and as a result they seem to be growing in popularity. Like any good IoT maker, I wanted to see how I could extend and connect it with my other things. In the space of an afternoon was able to build a simple serverless app to trigger whenever someone rings my doorbell, and push that data instantly wherever my hacker-heart desires (including face/audio detection). All of this functionality will add a grand total of about $0.40 to my Azure subscription each month. In the end, I used Azure Functions to create the trigger for ring.com events, and push the data to Azure Event Grid to notify any listeners of events.
I firmly believe any good IoT device should have an equally good API. Unfortunately, Ring doorbells have no public API. Luckily, it only took a quick GitHub search to find this project from @davglass who had reverse-engineered the API and published a super easy npm package to access it. The project lives here if interested.
Since there is no way to trigger directly to an event like a doorbell press, I need to poll the ring.com API for events. Azure Functions timer triggers are a great fit for these kind of custom polling triggers. I wrote this simple cron expression so my function will check for rings every 15 seconds: */15 * * * * * . I poll for new events (either a ‘ring’ or a ‘motion’ event). If the doorbell has sent an event, the function logic executes. If not, the function completes.
See polling logic in GitHub
Once I detect a motion or ring event has occurred, I want to take some action. Maybe I want to create a log in a Cosmos DB database, or make my WiFi connected lightbulb flash. I don’t know when I’m authoring the trigger function what interested entities may exist. This type of event routing and “publish/subscribe” was a nice fit for Event Grid. By forwarding my ring.com events to Event Grid via Azure Functions, I can now add and remove reactive components as needed. A logic app here, a function there — anything that exposes a secure URL. Even my raspberry pi inside my local network can participate by leveraging an Event Hub subscriber to these events.
I followed this tutorial to create an Event Grid and grabbed the endpoint URL and key.
Adding the integration to Event Grid to my function is as easy as adding an outgoing HTTPS request. I POST the event data in an “event envelope” to the event grid endpoint, and the event is instantly routed (seriously, like no latency). The top-level event envelope allows me to set the right context so my subscribers can listen only to the events or sources they care about — like “filter to only ‘ring’ events from the front door”:
Event Grid allows for decoupling of the event source and the event actions. This triggering function I wrote doesn’t need any knowledge of what may be listening, and listeners can be added or removed independently.
Here’s the javascript code I added to my function to push data to Event Grid:
Now that the events are pushed to the Event Grid, I can start building the subscribers. Some of the two easiest tools for this are Azure Logic Apps (connecting my doorbell to over 200 connectors) and Azure Functions. In part 2 of this post, I will go into details of creating serverless components to listen to events and react with elements like adding documents to Cosmos DB. If time permits this week, I’m hoping to add some nice Cognitive Service integration to bring my doorbell to the next level of awesomeness 😎.
Continue to part 2 — Integrating with Cosmos DB
https://github.com/jeffhollan/functions-node-ring-doorbell
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
213 
213 claps
213 
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://blog.bernd-ruecker.com/orchestrating-azure-functions-using-bpmn-and-camunda-a-case-study-ff71264cfad6?source=search_post---------60,"Serverless functions are all the hype at the moment whether it be AWS Lambda or Azure Functions. I am happy that I am allowed to share a case study from NexxBiz to show you how Azure functions and BPMN work together. For a customer in the insurance field they set up an architecture around the Microsoft Cloud (Azure). They have build a very interesting tool-chain to master this environment.
NexxBiz run multiple customers (tenants) on their platform. They need to implement dedicated business processes or customize standard ones for every tenant. Business functionality is available as dedicated serverless functions. As they are using the Microsoft .NET stack, Azure Functions are the way to go.
For most use cases just calling one simple function is not enough, you have to call multiple functions in the right order to implement a proper end-to-end use case or business process. This is referred to as orchestration. And it is pretty typical as functions intend to cut overall business logic into rather small and stateless pieces. I want to spare any discussion on orchestration vs. choreography in this blog post but can recommend Events, Flows and Long-Running Services: A Modern Approach to Workflow Automation if you are interested).
Azure does not offer complex orchestration capabilities, you can only do very simple flows within Azure Logic Apps (which is basically Microsoft Flow). Logic apps follow a low code approach which is typically not suitable for complex flows. That’s why they chose Camunda.
To give you a quick first impression, this is a simple but real-life orchestration flow from their system checking if a prospect is already known in the CRM, showing some runtime statistics as heatmap on top of the BPMN process model (using Camunda Cockpit).
There are multiple ways to run the Camunda Workflow Engine (see Architecture options to run a workflow engine). NexxBiz decided to minimize contact with Java and therefore,
See Use Camunda without touching Java and get an easy-to-use REST-based orchestration and workflow engine for more details on how to run Camunda in context of .NET.
In order to call the Azure Function they used the Azure Service Bus. Every Azure Function can be naturally triggered by a message on this bus.
To connect Camunda to the Azure Service Bus they decided to build small connectors consisting of three Azure Functions on their own:
The cool thing is, these functions are auto-generated and automatically provisioned during the deployment. No need to do anything manually.
Any configuration they need is done via configuration of the external tasks or input/output mappings in the BPMN process.
An interesting detail I want to highlight is that they wrote Java Unit tests for their BPMN processes. But wait, didn’t they want to avoid contact with Java? Indeed. But our consultant onsite talked them into writing the unit tests in Java anyway — as we have so much support for doing this (Camunda test support, assertions, scenario tests and visualizations of test runs). And this was a great idea as they told me:
Thank god Niall (Note: The guy with the hawk) talked us into sticking with Java Unit Tests, they are so powerful and easy to write. And as we can program in C# it is not hard to do that in Java.
To get this architecture off the ground they automated the build and deployment pipeline. And as they wanted to have much more control over these pipelines than they could get by Visual Studio Team Services out-of-the-box, they “drunk their own champagne” and used Camunda to control this. So their tool stack looks like this:
Another interesting aspect was that they use Cawemo to capture and discuss business processes and their requirements in the first stage. It saves them lots of paper for specification. They told me that they just conduct a one day workshop instead which is sufficient to get a solid basis for a first working increment.
Whenever a BPMN model is mature enough it is exported into the developers workspace and placed into normal version control. Now the Camunda Modeler is used to add properties like expressions or input/output mappings for external tasks. A unit test for the process is also written.
When everything is executable the workflow can be deployed onto Azure as described above with all glue code being generated automatically.
I asked the NexxBiz guys what they like about their approach and architecture. This is what they told me:
Thanks to NexxBiz for walking me through their architecture and allowing me to share this information. Please not that this blog posts reflects their architectural decisions and not expresses a recommendation from me or Camunda.
As always, I love getting your feedback. Comment below or send me an email.
Stay up to date with my newsletter.
My personal blog.
228 
1
Thanks to Darya Niknamian. 
228 claps
228 
1
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
Written by

My personal blog. Who is Bernd Ruecker? See http://berndruecker.io/
"
https://medium.com/@jeffhollan/serverless-doorbell-ring-com-and-azure-functions-part-3-7e865f28a1f?source=search_post---------61,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Dec 22, 2017·5 min read
This is part 3 in a series of how I have been using Azure Serverless to extend and connect the functionality of my ring.com video doorbell. Part 1 is here.
Ever since I got my ring.com video doorbell installed, I’ve been connecting and extending it with serverless components. Previously I hadn’t added much new functionality that I couldn’t already get from the ring app. Today is when that all changed. Below is how I used Azure Logic Apps, Functions, and Cognitive Services to detect the faces of anyone who comes to my doorstep, grab the audio, and create an AI-powered log inside of a Cosmos DB database.
I knew I wanted to have this type of facial recognition the moment I had access to the ring.com API. However it wasn’t as easy as just making a call to “grab video feed” for it to work. Here’s how the reverse-engineered ring.com API works:
So the challenge I was faced with was how can I detect and analyze the video as quickly as possible? If I did something like “when a ring event occurs, fetch the recording” in an Azure Function, I would get a 404, as the recording doesn’t exist yet. At the same time I never know exactly how long it will be until the event “completes” and the recording is published. I need some way to orchestrate and maintain state for the process — which isn’t always easy in the traditionally stateless world of serverless. Enter Azure Logic Apps.
Azure Logic Apps provided the perfect orchestration capabilities I needed to solve this rather simply. Not to mentioned, Logic Apps comes loaded with 200+ connectors to different APIs reducing the amount of code I needed to write. The only code I actually had to write for this entire process was these 23 lines of Azure Functions code to grab the video recording link. #serverlessFTW
Here’s what’s happening:
Now I know Logic Apps better than the average developer, but even then this entire logic app took about 45 minutes to build. 23 lines of code, 3 out-of-the-box connectors, and 1 workflow later, I get rich insights into the videos captured by my IoT doorbell. How much do I have to pay for this science fiction fantasy come to life? Assuming I get 3 visitors a day, it’d come to about ~$0.18 a month. All told my entire serverless project to this point is less than $2 a month.
Now that my videos are automatically being uploaded and analyzed into my Cosmos DB account and Video Indexer profile, what can I do more? Well first off, when the initial videos started coming in, all of the faces were “unknown.” Unfortunately no celebrities have been visiting, so the cognitive service doesn’t recognize any of the faces. No worries though — I can “train” my profile and teach it about the faces it is seeing. After opening a recording in the Video Indexer portal, I can replace the label “Unknown Person” with their name. Next time they show up in a video, Video Indexer recognizes them and will correctly assign a name. This same type of learning can be applied to the audio transcripts as well, so I can start to teach my profile about the words it may expect to hear on my doorstep to get more accurate results. While the results aren’t perfect — sometimes the quality of video or angle isn’t quite good enough to pick up a face — it works often enough that I’m very happy with the results.
You may be asking: “Does anyone really need an in-depth historic analysis of the patterns of faces that appear on their doorstep every month?” Maybe not. But for less than a cup of coffee a month, why not create my own doorstep version of HAL 9000 so my home may one day turn against me?
If interested in trying to build something similar yourself — I’ve got all the code here in my GitHub account. Enjoy! https://github.com/jeffhollan/functions-node-ring-doorbell
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
See all (133)
197 
2
197 claps
197 
2
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/asp-net-core-azure-kubernetes-orquestra%C3%A7%C3%A3o-de-containers-na-nuvem-parte-1-67f8d3568ba9?source=search_post---------62,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 23, 2018·5 min read
Inúmeras são as vantagens obtidas ao empregar containers Docker em projetos de software. Isolamento de aplicações, uma utilização mais racional de recursos computacionais, velocidade no deployment e uma menor dependência em relação aos ambientes operacionais estão entre os fatores que contribuem para o crescimento no uso desta tecnologia.
Por mais que as vantagens trazidas pelo Docker sejam inegáveis, existem também dificuldades decorrentes de sua adoção:
As respostas a tais questões estão em soluções voltadas ao gerenciamento e uso orquestrado de containers Docker. Dentre as alternativas mais populares neste segmento temos o Kubernetes, o Docker Swarm e o DC/OS da Mesosphere. O próprio Microsoft Azure não ficou alheio a este tipo de demanda, contando com um serviço conhecido como Azure Container Service e que suporta estas 3 tecnologias.
Mais recentemente (Outubro/2017) a Microsoft disponibilizou uma nova opção no Azure voltada ao uso de Kubernetes: trata-se do Azure Kubernetes Service (AKS), solução que visa simplificar o gerenciamento e operação de ambientes baseados em Kubernetes.
Este artigo é a primeira parte de um tutorial no qual abordarei a utilização combinada do ASP.NET Core, do Microsoft Azure e do Kubernetes (através do serviço AKS) para a implementação de projetos que dependam da orquestração de containers. Nesse post estão os principais conceitos envolvendo o Kubernetes, bem como descrita a aplicação ASP.NET Core que servirá de base para os testes com o AKS.
Também conhecido como K8s ou kube, o Kubernetes é um projeto open source escrito na linguagem Go e desenvolvido originalmente pela Google. Mantido atualmente Cloud Native Computing Foundation, o Kubernetes conta com recursos de gerenciamento que viabilizam a orquestração, auto recuperação, reinício, replicação e escalonamento de containers Docker.
Assim como acontece com outras tecnologias populares na atualidade, o Kubernetes possui também uma ferramenta de linha de comando: o kubectl, utilitário que permite a execução de instruções voltadas ao gerenciamento de clusters e containers.
As diferentes estruturas controladas via Kubernetes serão criadas a partir de arquivos no formato YAML e por meio da execução de comandos via kubectl. Um ambiente para testes pode ser disponibilizado através da instalação do Minikube (software que possibilita a criação de um cluster local para uso do Kubernetes).
Dentro da arquitetura adotada pelo Kubernetes merecem destaque os seguintes elementos:
Um cluster no Kubernetes está dividido em:
Já um Pod é uma estrutura que agrupa um ou mais containers. A implantação deste elemento (Pod) acontece em algum dos Nodes disponíveis, com os containers que compõem o mesmo compartilhando o mesmo endereço de IP, nome de host e outros recursos.
A figura a seguir traz a representação esquemática de um Pod:
Um objeto Deployment é uma abstração de um Pod, contando ainda com recursos adicionais:
Na próxima imagem temos a representação de um objeto Deployment:
Um objeto do tipo Service é uma estrutura que funciona como um Load Balancer, cuidando assim do acesso aos diferentes Pods de uma aplicação. Trata-se de um elemento mais estável, até porque Pods são criados ou removidos continuamente ao se escalar uma aplicação.
Dentro da arquitetura do Kubernetes o elemento conhecido como Replication Controller determinará quantas cópias idênticas de um Pod serão executadas e em quais locais (Nodes) do cluster. Já o Kubelet é o serviço que garante a inicialização e execução dos containers nos diferentes Nodes.
A imagem a seguir traz mais uma representação do Kubernetes, com Pods distribuídos entre os diferentes Nodes de um cluster:
Para os testes envolvendo o uso do Kubernetes a partir do Microsoft Azure será criada uma API REST baseada no ASP.NET Core 2.0. Esta aplicação produzirá como retorno a quantidade de acessos à API, além de exibir o nome do host/máquina e do sistema operacional utilizado pelo container Docker.
Na próxima listagem está a implementação da classe Contador, a qual armazenará a contagem de acessos à API REST de testes:
Já o tipo ContadorController receberá solicitações HTTP através da Action Get, controlando a utilização sincronizada de uma instância de Contador (via instrução lock) e devolvendo o número de acessos à API, o nome da máquina/host e a versão do sistema operacional em uso:
A seguir temos um exemplo de execução desta API por meio do Visual Studio 2017:
As demais etapas envolvendo a publicação da aplicação no Kubernetes serão detalhadas na segunda parte desta série. Os fontes deste projeto de testes já estão disponíveis no GitHub:
https://github.com/renatogroffe/ASPNETCore2_Kubernetes
Azure Container Service (AKS)
Docker para Desenvolvedores .NET - Guia de Referência
Kubernetes - Site Oficial
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
196 
196 claps
196 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mauridb/powerbi-and-azure-databricks-193e3dc567a?source=search_post---------63,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
Feb 8, 2018·6 min read
Please note that from middle of February 2018 connection to Azure Databricks is also possible via Spark connector as described here, which is now the recommended approach. Continue reading this article if you’re interested in setting up a connection via ODBC.
I’ve recently changed my role and as a result I’m now again 100% focused on my first love, data and databases.
Among all the cool stuff released in the last two years in the data space, for which I haven’t had enough time to play with, there is Databricks, the Spark distribution, created by the creator of Spark themselves. Spark has also reached version 2.x and I really wanted to give it a test run, to play with the new features by myself.
Since Databricks is available on Azure, I just created new cluster and to get confident with Azure Databricks I firstly did the “Getting started — A Gentle Introduction to Apache Spark on Databricks ” tutorial. It’s very introductory and allows you to get confident with terminology, concepts and usage of Notebooks. If you already used tools like Jupyter or Apache Zeppelin you’re already familiar with the Notebook idea and you can go through the tutorial real quick.
After the first tutorial, the second one is the “Apache Spark on Databricks for Data Scientists”.
Once I finished the tutorial I immediately thought that would have been great to connect to the available data using PowerBI. Doing data exploration and visualization on the notebook is great but I wanted to do some interactive data exploration. In addition I also wanted to create a nice dashboard that can be easily shared with non-tech users, and for these things PowerBI (or other tools like Tableau) is just a killer application.
So, how do you connect PowerBI to Spark on Azure Databricks? There is a complete documentation here:
docs.azuredatabricks.net
And following it is really not difficult, but it may be a bit tricky, if you’re not familiar with the PowerBI ecosystem (Service, Gateway and Desktop). If you just want to connect your PowerBI desktop client to Spark on Azure Databricks, just keep reading.
First of all make sure you have the latest PowerBI version installed on your machine:
powerbi.microsoft.com
and then download the Apache Spark JDBC/ODBC driver:
databricks.com
Make sure you select the Windows (ODBC) version and then proceed to install it. Since I’m using PowerBI x64, I just installed the 64-bit one.
Once installation is done, go and open the ODBC Data Source Administrator:
and create a new DSN. Choose a “User DSN” if it’s only for you, or “System DSN” if other people on the same machine will be using it.
To create a new DSN click on “Add” and the select the “Simba Spark ODBC Driver”. After that you’ll see a window like the following:
Since we’re connecting to a Spark 2.0 cluster, as you can verify in the “Configuration” page of you Databrick Spark cluster,
the ODBC “Spark Server Type” option needs to be set to the following:
The “Hosts(s)” and “Port” option must be set to the values you can see in the JDBC url (that is available in the “Configuration” page of the Spark cluster too):
Azure Databricks allows authentication via unique users tokens. The ODBC Authentication mechanism needs to be set to “User Name ad Password” and the user name must be set to “token”. Literally, just without quotes.
The password is the token that can be generated on the Databricks portal by clicking on the user icon on the top right and selecting “User Settings”:
This will bring up the “Access Token” page. Just click on Generate New Token, specify the lifetime of the token and then copy the generated token somewhere. The token is something like:
This is actually the password you have to specify in the ODBC Driver:
Authentication is now one. It’s now time to configure the Thrift Transport. It must be set to HTTP and then “HTTP Options” and “SSL Options” needs to be configured accordingly.
In the “HTTP Options” specify the HTTP Path you can see in the JDBC Url (or, again, in the Cluster Portal, Configuration Tab, HTTP Path section)
In the “SSL Options” just make sure you check “Enable SSL”.
That’s it! Now click on the “Test” button to the test connection and you should be able to see a window like this (Just make sure your cluster is running and you don’t have any firewall blocking connections)
So far, so good, ODBC DSN is created. It’s now time to open PowerBI Desktop and get data from the Spark cluster by using the newly created DSN:
Make sure you select the DSN you created before
use the token as a password again when asked and make sure the user name is set to “token” (again, without quotes)
and you’ll be good to go!
As you may notice I’ve also added the “farmer_markets” as a non-temporary table, so that I can do exactly what the tutorial tells you to do via Spark, but using PowerBI this time.
And now you can easily explore and visualize data, PowerBI way:
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
128 
3
128 
128 
3
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://medium.com/awesome-azure/azure-difference-between-azure-load-balancer-and-application-gateway-9a6019c23840?source=search_post---------64,"There are currently no responses for this story.
Be the first to respond.
Comparison: Azure Load Balancer vs Application Gateway in Azure.
Azure Load Balancer works with traffic at Layer 4.Application Gateway works with Layer 7 traffic, and specifically with HTTP/S (including WebSockets).
"
https://medium.com/@evonneyifangtsai/b2b%E5%B9%B3%E5%8F%B0%E7%AD%96%E7%95%A5-%E5%BE%AE%E8%BB%9F%E6%80%8E%E9%BA%BC%E6%8E%A8%E5%BB%A3%E4%BB%96%E7%9A%84azure-iot%E5%B9%B3%E5%8F%B0-9a3e34d9744f?source=search_post---------65,"Sign in
There are currently no responses for this story.
Be the first to respond.
Evonne Tsai
Sep 17, 2017·8 min read
參加Microsoft IoT Expo之後，驚訝於Microsoft Azure平台的完整性，也從它如何經營與行銷這樣一個B2B平台覺得獲益良多。
關於Azure IoT的介紹，以及台灣廠商的策略，請參考：大神都做完了，台灣廠商還怎麼搞？微軟IoT Expo有感，以下專注於分析微軟雲端平台的商業模式與推廣策略。
講到雲端平台，大家第一個還是會想到Amazon，其實目前雲端廠商四強中，除了Amazon之外，Microsoft已經擁有第二大市占率，接著才是Google以及IBM。在這些競爭者當中，微軟是怎麼定位自己的？
根據微軟「Azure之父」詹森‧桑德在數位時代的專訪中提到，微軟的優勢在於提供整合服務：
「我們的競爭對手，有些只做IaaS，有些只做SaaS，但是微軟做的是基礎建設。我們有PaaS、有SaaS，有Azure也有Office 365，所有的服務都搭建在一起。特別是在企業環境裡，很多公司會用Office，而他們也需要用到雲端，微軟則是唯一可以提供整合性服務的。」
可以看得出來，微軟希望能發揮其既有與企業的關係與優勢，使用IT部門熟悉的架構與語言，並從企業使用者熟悉的Office切入，提供整合性的服務。這的確是微軟相對於Amazon與Google的優勢。
我詢問微軟攤位上的業務，微軟和競爭者有什麼差別，他們也有類似的說法：「微軟專注於B2B，和企業關係好，也理解企業的需求，競爭者則在新創公司或個人較常被使用」。所以這樣的定位是有傳達到第一線的銷售人員的，且銷售人員也很清楚，並不是要與其他競爭者比功能、爭機房數量，而是從定位上差異化，用不同的語言與其選定的用戶做溝通。
我在IoT Expo上看到的demo是PaaS(Platform as a Service，平台即服務，例如分析平台)加上SaaS(Software as a Service，軟體即服務，例如各種能畫出漂亮報表的應用程式)，當然這一切是架在Azure的infrastructure上(IaaS，架構即服務)。
與硬體廠商的合作模式其實有點像「代工」，微軟提供平台與軟體，讓硬體廠商將資訊丟到平台上運算，並將介面客製化成該硬體提供的獨特服務。
這樣的平台有幾個特性：
例如要推智慧零售解決方案，除了平台以及應用程式之外，要有能提供零售相關感測器的硬體廠商加入，也要有熟悉零售客戶的系統整合商與安裝商加入。
但是一開始平台什麼都還沒有時，兩邊是不會有廠商願意加入的，而沒有廠商的情況下，是不會有成功案例吸引終端客戶的，而沒有專案與客戶，廠商未必願意投資生產，這形成了雞生蛋蛋生雞的問題。
因此平台一開始必須先提供非常多的誘因，吸引多邊加入。對微軟來說，也許就是非常完備的分析方法與應用程式、其響噹噹的名聲，還有與企業的關係，這也和微軟一開始定位在「B2B」市場可以互相呼應。
既然提到平台生態系統的搭建有雞生蛋蛋生雞的問題，但總是要從一方開始，那麼應該要從哪一個stakeholder開始經營呢？
關鍵在於什麼樣的角色加入，能創造這個平台的網絡效應，吸引更多人加入？且對什麼角色而言，這個平台有最大的黏著度，用了就不太會換成其他平台？
（註:網絡效應：越多人使用，效益越高，例如第一個使用電話的人其實沒什麼價值，要大家都使用電話，人們才能用電話互相溝通，電話才有價值。社交平台也是類似的概念。）
系統整合商在新產品剛開始推廣時，不會是這樣的角色，一般是觀望軟硬體解決方案與客戶需求，尋找適合的component來自己兜，哪邊有錢（終端客戶需求），就往哪邊提供服務，尤其在同樣具有技術能力的IT面前，SI的角色較被淡化，但有些產業SI反而能帶來終端客戶，要和具有客戶名單與技術能力的SI合作。以下討論硬體廠商與終端客戶。
對硬體提供者而言，越多硬體廠商加入其實是競爭者越多，其獨特性減少了，而且由於各雲端平台本來均會顧及「相容性」，硬體提供者轉移系統的成本其實不高。
因此我認為一開始吸引許多硬體廠商加入效益不高，應該找尋能提供「足夠多樣」硬體（包含感測器與顯示器）的廠商，或是具有業界領導地位的硬體廠商結盟就夠了。
對終端客戶而言，「許多的硬體廠商」也不是重點，只要微軟能確保既有的感測器相容，或是有廠商能提供完整解決方案就好。
我想這也是為什麼微軟一開始只跟幾家國內工業電腦大廠合作的原因。這些工業電腦大廠甚至原本就已經有關係良好的系統整合商與安裝商夥伴，可以解決尋找系統整合商與安裝商的問題。
在微軟以前的時代，Office是具有這樣的網絡效應的－－越多終端客戶使用Office，大家越離不開Office，因為Office變成了一種「溝通方式」。（這也是許多人不願意換成Mac電腦的原因，因為大家都用Office，轉檔太麻煩而且格式可能跑掉，不用office檔案難以流通。）
但是，雲端平台與分析數據似乎不具有「溝通方式」的特性，因為它頂多是個分析的後台而已，不會因為越多人使用微軟平台，導致終端客戶的使用效率／效益變高（頂多AI辨識比較準，但AI最強大的Google還在第三名呢！看起來AI辨識準度目前還沒成為做BI的痛點，可能目前還是「有總比沒有好」的階段吧？）。
但這個平台還是創造了「黏著度」。
對終端客戶而言，尤其是IT部門，只要選定了雲端平台，幾乎不太可能會換，因為從資料，到伺服器，全都建構在雲端平台上，要搬移是怎麼樣的大工程呀！
而且雲端平台架構邏輯與應用程式的使用，都是學習的成本，沒事不會給自己找麻煩換系統的。所以微軟用非常便宜的方式先推廣，因為終端客戶一旦使用，就緊緊黏在上面了。終端客戶，而且是IT部門，是微軟最適合開始推廣期解決方案的角色。
這些都仰賴微軟的市場教育以及生態圈服務提供者，這幾個也是微軟強調的賣點。
（圖片來源：微軟Azure IoT網站）
微軟的Azure架構實在太龐大，且由於其平台的本質，針對不同stakeholder均有一套賣點與解決方案，但是全部的語言一起混合在網站上，實在讓人難以理解，個別角色難以找到所需資訊。
光是類似app store的應用程式中心，就有Azure Marketplace跟AppSource 在同一個頁面，裡面均有數千個以上的應用程式，雖然其有提到前者是IT解決方案，後者是SaaS商務解決方案，但我還是非常迷惑阿！(可能我不是它的target audience我不用懂吧......)
曾經以為微軟的時代已經過去了，但其成功地轉型至雲端業務，加上善用其長久以來累積的企業客戶關係與語言，讓它用Azure急起直追，躋身雲端大廠四大巨頭，坐二望一（但對一是遠目，哈！），這是令我相當佩服的。我期待這樣的平台能與國內硬體廠商合作發揮綜效，創造價值，甚至將台灣的解決方案推廣到世界！
P.S. 我幾乎是從頭開始認識Azure這個服務，其架構又無比巨大，但從分析中學到很多東西，也希望大家給我一些鼓勵或回饋！
財金人，科技人，行銷人，商業思維學院產品課程主理人；十餘年產品行銷與管理經驗，商業思維學院社團：https://lihi1.cc/jC9rk； 官網：https://bizthinkers.com/hi ；Mail: evonnetsai417@gmail.com
310 
1
310 
310 
1
財金人，科技人，行銷人，商業思維學院產品課程主理人；十餘年產品行銷與管理經驗，商業思維學院社團：https://lihi1.cc/jC9rk； 官網：https://bizthinkers.com/hi ；Mail: evonnetsai417@gmail.com
"
https://medium.com/net-core/deploy-an-asp-net-core-app-with-ef-core-and-sql-server-to-azure-e11df41a4804?source=search_post---------66,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to deploy an ASP.NET Core web application with EF Core and SQL Server to App Service on Azure. Besides, I will demonstrate how to create an AzureSQL Database and use this database from the application.
The sections of the post will be as follows:
The application that we will deploy manages a database of TV Shows and its main page looks like below:
The app is an ASP.NET Core MVC web application and it uses EF Core to perform CRUD operations on a SQL Server database.
You can download the source code from this Github repository.
If you don’t have an Azure account, go to Microsoft Azure site and click Start Free button. Sign up with your Microsoft account and get $200 (for free) to explore any Azure service for 30 days.
In this section, we will create our SQL database on Azure.
Azure SQL Database is a general-purpose relational database, provided as a managed service. With it, you can create a highly available and high-performance data storage layer for the applications and solutions in Azure. It’s based on the latest stable version of the Microsoft SQL Server database engine.
SQL Database is a fully managed service that has built-in high availability, backups, and other common maintenance operations. Microsoft handles all patching and updating of the SQL and operating system code. You don’t have to manage the underlying infrastructure.
You can learn more about Azure SQL database here.
Now, we will create our SQL database on Azure.
Go to Azure Portal and select Create a Resource -> SQL Database as shown below:
Select your Subscription and click Create new in Resource Group field:
In the Database Details part, click Create New in the Server field:
After filling the necessary fields, click Ok. Please note your login and password as we will need this to connect to this database in the next section.
In the next screen, enter your Database name and click Configure Database to choose the plan that suits your needs:
Next, go the Networking tab and update the tab as shown in the following image:
Here we add the current client IP address because we will need this to connect to this database from our local machine.
We allow the Azure services to connect to this server as we will need this when we publish our web application to Azure. As mentioned in the highlighted section above, this gives access to all Azure services.
If you want to learn more about setting Azure SQL server/database firewall rules, please refer to this page.
Click Review + Create and then click Create. When the deployment is complete, click Go to resource button. Your resource will resemble the following:
If you click the All resources on the left pane, you can see the resources that you created:
In this section, we will connect to the database that we created on Azure from our local machine using Visual Studio.
In Visual Studio select View -> SQL Server Object Explorer and click Add SQL Server icon. Then fill the necessary fields with your database information as shown in the following image:
and click Connect.
After establishing the connection, you will see this database in the SQL Server Object Explorer. Right-click on the server name and select Properties and get the connection string from there:
Go to appsettings.json file in the project and add this connection string as follows:
Please note that you should update your password here as it is masked with stars.
In our scenario, we will use our local database for development and Azure SQL database for production. So, we need to add the following part to Startup.cs to provide the related connection string for each environment. Update the ConfigureServices method as follows:
The Database.Migrate() call helps you when it's run in Azure, because it automatically creates the databases that your .NET Core app needs, based on its migration configuration.
Now, we will test the application on our local computer.
Before that, we need to update the ASPNETCORE_ENVIRONMENT variable to use the Azure SQL database
Right-click on the project in Visual Studio and select Properties. Then change the variable as follows:
and then Save.
Now we can run our application.
In this test configuration, our application server is on localhost and database server is on Azure.
Run the application on the Visual Studio and click TVShowsApp on the web page and then Create New on the next page.
After creating our first record, it is shown below on the main page:
We can check the record created using SQL Server Object Explorer as well:
Now that we checked that we can use the Azure SQL database we created, we can revert the ASPNETCORE_ENVIRONMENTto Development.
In this section, we will deploy our application to App Service on Azure.
Azure App Service enables you to build and host web apps, mobile back ends, and RESTful APIs in the programming language of your choice without managing infrastructure. It offers auto-scaling and high availability, supports both Windows and Linux, and enables automated deployments from GitHub, Azure DevOps, or any Git repo.
We can deploy our application to App Service via automated deployment or manual deployment.
Azure supports automated deployment directly from several sources. The following options are available:
Below are options that you can use to manually push your code to Azure:
We will use the manual deployment with Visual Studio option.
In Visual Studio, right-click on the project and select Publish… Then click Create Profile in the next screen:
In the next dialog, enter your App service Name and select the Resource Group that you created when creating the Azure SQL database on Azure portal. Then, click New in the Hosting plan field and choose Free for Size and fill the Name and Location fields.
Then click OK and Create respectively.
Now, our publishing profile is created as seen in the following image:
If we click the Publish button, our application will be deployed to Azure.
Before that, we need to add ASPNETCORE_ENVIRONMENT variable to our app service in Azure. We can check and update the App Service that we created on Azure portal.
Go to Azure Portal and click the App Services and then click the app service that you created:
In the app service, click Configuration and click New application setting. In the next dialog, enter ASPNETCORE_ENVIRONMENT for Name and Production for Value. Then, a new application setting will be formed as seen below:
Click Save.
Now, we can publish our application to Azure. Go to Visual Studio and click the Publish button that we saw in the publishing profile above.
After the deployment operation is completed, the web page is launched automatically and the application’s URL is as follows:
I created two more records and the main page of the application running on Azure looks like below now:
We can see the newly added records from the database as well:
When you’re working in your own subscription, it’s a good idea at the end of a project to identify whether you still need the resources you created. Resources left running can cost you money. You can delete resources individually or delete the resource group to delete the entire set of resources.
Now we will delete the resource group to stop billing for all the resources used within the group.
Go to Azure portal and click Resource groups and then select the resource group that you created. You will get a screen like below:
As you see in the above image, we have four resources in the group. We created the SQL server and the database from the Azure portal. App service and the app service plan were created automatically from the Visual Studio during the creation of the publishing profile.
You can click the Delete Resource group link to delete all of the resources.
That’s the end of the post. I hope you found this post helpful and easy to follow. If you have any questions and/or comments, please share them in the responses section below.
And if you liked this post, please clap your hands 👏👏👏
Bye!
https://docs.microsoft.com/en-us/azure/app-service/app-service-web-tutorial-dotnetcore-sqldb
https://docs.microsoft.com/en-us/aspnet/core/tutorials/publish-to-azure-webapp-using-vs?view=aspnetcore-3.1
https://docs.microsoft.com/en-us/learn/modules/host-a-web-app-with-azure-app-service/5-deploying-code-to-app-service
https://docs.microsoft.com/en-us/azure/sql-database/sql-database-technical-overview
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
260 
5
260 claps
260 
5
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
"
https://medium.com/microsoftazure/setting-up-a-macbook-pro-mac-os-x-high-sierra-for-java-and-azure-cloud-development-ca5d60ed79ba?source=search_post---------67,"There are currently no responses for this story.
Be the first to respond.
A bit of background here before we start, because to me using Mac is a very recent event.
If you like this article, please clap for it! Click on the little hands icon to the left or bottom of this page.
I had been a Linux user for over 15 years. Not a specialist/hacker kind of thing — just a user of Linux on the desktop (Ubuntu mostly; but also used Slackware and Fedora in the early days).
Why am I using Mac now? Well… Let's go from the beginning.
In January of 2018 I left Oracle after about 5,5 years working in the Product Management organization doing Developer Relations for Java and Cloud — you can see me here at Oracle Code conference keynote in October 2017.
At Oracle, by the time I joined the company in California in 2015(though I had been working for them since 2012 in Brazil), I was given only one laptop option: a Lenovo. Either I'd have Windows, or Linux. I stood with the Tux. Only in late 2017, Oracle started giving MacBooks for the engineering group by default — which is pretty cool.
Left Oracle. Where was I heading to? Canada!
Because I recently joined Microsoft's Cloud Developer Advocacy group, to help the product engineering teams building and enhancing products and services to the taste and standards of the Java audience.
When I was transferred from Oracle Brazil to Oracle USA, I was given what is called the Intracompany Transferee Visa, aka L-1B type. This visa has a restriction: you are bond to that company until you get either a permantent resident card (green card), or you change status to an H1 type visa. Given my short stay in the US, neither processes completed in time.
So, in my conversations with the Microsoft Immigration team, we found that the best solution would be to move to Canada, where I'd be close to Redmond — and in the same timezone.
Just one minor detail: Canada also issues a Work Permit bond to the company you are going to work for, if that is the case during the visa application. My wife on the other hand received a Work Permit that allows her to work pretty much for any company and almost any area, but that thanks to the relationship with me. Visa approval arrived in 2 weeks. Passport stamps plus 2 weeks. Pretty fast in this case, but if you want you can apply without a ""sponsor"". It will likely take more time to be approved — if it gets approved (although in most cases, it does).
Done already with the background story… let's get to the business!
At Microsoft I was given two options: either I’d get a Mac, or a Windows laptop. And given the similarities (in the terminal) between Mac and Linux, I chose the former. And as I said before, I do know how to exit vim.
First of all, you will need a decent terminal. No developer should rely solely on mouse and windows, so a powerfull command-line interface is a must!
This is one article I found that helped me set up my iTerm2. Walkthrough it.
medium.com
Besides the terminal, by following the article above you also get Homebrew, the missing package manager for Mac.
Cask is a complementary tool for Homebrew that extends to other capabilities such as installing common desktop applications distributed as DMG files somewhere on the Internet, but that are not available on Homebrew directly.
The next step is to install some basic tools for Java development.
Although Java 9 is available, many of my projects are quite still on Java 8. Some of them need changes in Maven POM files, and I just want to get them up and running. So that is why I am sticking to java8 below, but feel free to just say java, and cask will pick the latest (9).
It is also a good idea to always check the package version on Cask and compare with the latest on the website of the tool. Use info for that.
Now… if you really want, you can also install Eclipse through Cask…And I will leave it to you :-)
These should give you enough to get started, but in the real world, you will need more. A lot more!
Quite frankly, if you know any other important tool for Javascript development, please let me know.
The reason I installed Wireshark twice, one with cask, was to get the GUI.
Now let's get into more sys admin, DevOps, runtime tools…
The Docker package available on Homebrew is just the CLI. If you install with Cask, you get the fully featured Docker for Mac. I strongly recommend you get this one. Lastly, Terraform migrated from Cask to Homebrew/Core, so that's why.
Docker also has an Edge version that includes Kubernetes out of the box. If you prefer that, don't install minikube (because you won't need it), and instead install the package docker-edge. Personally, I prefer Docker (stable version) and Minikube.
The Azure CLI is a key element for development with Azure services. Almost all Azure services are accessed through this single CLI, so keep it installed!
This one is tricky. In the past I used to have MySQL installed and that's it. Most of the time I'd start MySQL daemon, create a database and grant a new user entire permission, and then use that in my application.
These days, I think that for develompent purposes, all developers should be using databases inside Docker containers. But only for development/testing. I am still not so sure about RDBMS on Docker in production.
Now, what you really need to have are the Client CLIs for these databases. So let's install them!
Sadly, the new Oracle Database CLI — sqlcl — does not provide an easy install method. You will have to go to its download page and take it from there. Good luck.
The part about running databases locally for develoment purposes on Mac will come in a future post.
Other databases — none, because I'd rather stick with good old fashion SQL :-)
One thing I noticed initially was that the battery of this Mac wasn't lasting long enough. I was barely getting 2 hours straight.
So I dug the internet for tweaks. Here are they:
So that's it! If you know any other tool that you believe is extremely important for a Java developer to have on its Mac, please comment below!
Cheers!
Any language.
157 
8
157 claps
157 
8
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Brazilian, Product and Program Manager for Java at Microsoft. Promoting great developer technologies to the world. Previously at Oracle.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://towardsdatascience.com/code-free-data-science-with-microsoft-azure-machine-learning-studio-65f245b81ee0?source=search_post---------68,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gilbert Tanner
Jun 17, 2019·11 min read
In the last weeks, months and even years a lot of tools arose that promise to make the field of data science more accessible. This isn’t an easy task considering the complexity of most parts of the data science and machine learning pipeline. None the less many libraries and tools including Keras, FastAI, and Weka made it significantly easier to create a data science project…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/data-science-reporter/a-simple-hands-on-tutorial-of-azure-machine-learning-studio-b6f05595dd73?source=search_post---------69,"There are currently no responses for this story.
Be the first to respond.
I’ve recently stumbled upon a Microsoft Azure tool called Microsoft Azure Machine Learning Studio, which is a graphical, web interface to…
"
https://medium.com/asos-techblog/azure-service-bus-functions-urban-airship-at-work-together-for-an-e-commerce-website-94cc7e44e561?source=search_post---------70,"There are currently no responses for this story.
Be the first to respond.
A modern e-commerce website based on the microservices architecture is probably powered by a variety of different services and APIs: Promotions, Customers, Product Catalog, Search, Checkout, Recommendations, Order Processing and potentially many more pieces. These services can communicate between each other by direct API calls, if they need an immediate feedback, or they can use a messaging-based system implementing the Publisher-Subscriber pattern, where they basically send a message and then zero, one or multiple subscribers can be notified and react to the message in different ways. In the Microsoft Azure cloud, the messaging systems are Azure Service Bus, Event Hub and Event Grid (see here for a comparison), and listeners can be WebJobs, Console apps, Azure Functions or other types of applications.
The diagram below offers a simplified and high-level overview of how some of these pieces can work together to implement a feature that allows customers to receive push notifications when an out-of-stock product comes back in stock:
This article focuses on showing how to set up and implement the bits around sending messages to Service Bus and the Azure Function that is triggered by those messages, which then sends a push notification through Urban Airship.
I won’t cover this step by step, because this article on MSDN does a great job already. Suffice to say, I created a new Azure Service Bus resource, created a Topic named ‘productstockchanged’ and a subscription for it named ‘BackInStockNotifications’. (Since I wanted to test how things work with multiple subscriptions, I also created a second, named ‘UpdateCatalog’. This is an extra step and not needed for the scenario being implemented here). Here’s what the final state looks like:
Reminder: Topics are for when you might need multiple listeners to receive and process an event independently, while Queues are for when there will be only one processor. As shown by the diagram at the beginning, our ‘stock changed’ messages would be processed by at least a couple of services (one that sends out push notifications and one that updates the ‘in stock’ information on the Product Catalog database), so a Topic was required.
In reality, it would probably be more accurate to say that a Queue is for commands that someone sends to a recipient, and the recipient is the owner of the message. A Topic instead implements the actual Pub-Sub pattern, where the publisher is the owner that broadcasts a message without knowing who are the listeners that will pick it up.
In the real implementation it would be the Stock microservice that sends messages to the Service Bus Topic once a product goes out of stock or is back in stock. For this demo however, I created a simple Console app that sends messages according to comments entered on Terminal.
Microsoft.Azure.ServiceBus is the NuGet package that must be added as a new dependency:
The code is quite simple: a TopicClient object is instantiated with the Service Bus connection string and the Topic name in input, and its SendAsync method is used to send a new Message object, which is created with the json produced by serialising a ProductStockChanged object. ProductStockChanged contains the ID of product affected by the change, and a boolean indicating whether the product is now in stock or out of stock. An additional attribute that I could have added is the product title, but I decided to keep things simple.
New messages are sent by sending commands like ‘instock 123’ or ‘outofstock 123’ (where ‘123’ is the ProductID) from the command line. Here’s the complete code:
If you now run this, send a few sample commands, and then reload the Topic page on the Azure Portal, you’ll see that the ‘Message Count’ for the subscriptions created previously was increased because there are now messages there waiting to be picked up and processed.
There are plenty of good services to send push notifications for iOS, Android and Web, but I chose Urban Airship (UA) because it’s well documented, has a nice UI for creating messages (I used the UI during some initial testing, before switching to the API) and a good client-side SDKs and RESTful API. However, other services work just as well (Microsoft itself offers Notification Hubs, and I have also successfully used OneSignal previously), and the overall solution would stay exactly the same.
For this demo, I chose to send web push notifications rather than notifications to an Android or iOS app, because I integrated push notifications on many native mobile apps in the past but never tried it on web and I felt it was easier to set up for a demo project build from scratch. I was right in thinking this — while it takes a few steps for web, it takes longer to create a dummy native app and deal with push certificates. However, the beauty is that since we’re using UA we’re abstracted away from the platform-specific implementation — we just ask UA to send a push notification and it does it for all registered/configured platforms. This means that if we wanted to add support for iOS or Android later, there wouldn’t be code changes.
So, I created a free account, and set up a web-based project: you choose a title, default action url (this is where the user would be redirected to if they click the notification) and icon url, and you’re ready to go: Urban Airship will now let you download a zip file, which contains a JS file and a snipped to add to your front-end website. More on this later…
As we are going to interact with UA via its API, I accessed Settings / APIs & Integrations and grabbed the AppKey and App Master Secret, which are required to authenticate the calls as you’ll soon see in the Function implementation.
It’s time to create the ‘core’ of the solution, which is the Azure Function that actually sends the push notification when it gets triggered by new messages for the productstockchanged Topic on the BackInStockNotifications subscription. I created a new Azure Functions resource and created a new function from the ‘Service Bus Topic trigger’ template, as shown below:
Should you need to modify the trigger’s settings after the function has been created, visit the Integrate page:
When a new message is posted to Service Bus, the Azure Function is instantiated and its Run method is invoked. Run has a string input parameter, which is the json of the Message created previously, that contains the ProductId and IsInStock attributes. The json body is deserialised into a ProductStockChanged object, and, if its IsInStock property is true, the UA API is used to send a push notification to all clients that have expressed an interest in knowing when that particular product is back in stock.
How do we target only those customers/clients and not everyone though? A quick solution is using ‘tags’: a client registers for a tag named notify-instock-{product id here}, and the push request has a parameter indicating that we’re only targeting clients with that tag associated to them. A client could of course also unregister for that tag later on, and therefore stop receiving notifications for some products.
The actual UA API call is done by doing a POST request through a HttpClient object, which has an Authorization header with the UA credentials, and takes in input a json body with the text of the notification and the audience’s tag. (Targeting can be much more advanced, for example, you can combine multiple tags using AND or OR; can target only specific device types, and can schedule a notification for a specific time. Read more here.)
The following code should be self-explanatory:
The Urban Airship credentials needed to call the UA API should never be hardcoded in the script, but be in app settings and read through the environment variables, or in Azure Key Vault.
Note the #load statement in the first line, which loads an external file with the ProductStockChanged class definition. This was created from the ‘View files’ tab displayed below:
Please note: I initially created and tested the Function locally with Visual Studio, but then I manually copied/pasted the code into the online editor because the ‘Publish to Azure’ wizard was not available on my installation for some reasons. I’m using macOS, but I guess you’d have a better experience with Visual Studio on Windows though.
Refer to this page about Azure Functions Core Tools to find out more about the local development experience. Developing the Function locally and then deploying to Azure means that you’ll have a Function based on a precompiled library rather than on a C# script, and also means you’ll be able to more easily reference other libraries/namespaces as you would do normally. In order to use this option however you can’t create the Function from the portal.
Refer to this page to see how a new Function can be created through the Azure CLI rather than the portal.
Once the Function is complete, go back to your console app developed previously, and send a ‘instock 123’ command. The log window on the Azure Function should display some lines confirming the Function was invoked by the Service Bus Topic trigger and some custom logs. If you then switch to the Urban Airship’s Activity log, you should see a new push notification sent for the notify-instock-123 tag.
The above log is the only way to verify that the UA call is successful so far, because the client web page hasn’t been implemented yet.
This article by Zhongming Chen, published on the ASOS Tech Blog, has some very good notes about Azure Functions, so take a look if you’re planning to use them.
The last step involves creating the client app, which is a website or, rather, a single web page for this demo. In a real-world implementation there would be product listing pages and product details pages getting data from something like a Product Catalog API, and, as part of the product attributes returned by the API, there would be something like an ‘isInStock’ attribute. When that’s false, the customer might click a button to register their interest for the product, and the notify-instock-{productId} described before would be associated to them.
For the purpose of this article however, the simplest, and ugliest page — with a static list of three product names and a ‘Notify’ checkbox on their right-hand side will do just fine. The UA client-side SDK, as well as the initial configuration, is fully explained on this page, but here’s the gist:
Here’s the full HTML/js code:
When the page loads for the first time (or even subsequently, if you just cancel the prompt for the notifications) this is what you should get:
Click ‘Allow’, and then tick the ‘Notify’ checkbox for one or more products:
Finally, use the Console app to send a ‘instock 123’ command. If everything is set up correctly, you should get a notification like this on Chrome (even if you’re on another page of course):
On a Mac, the notification will also be shown on the system-level Notifications panel on the right-hand side of your desktop:
Please note: don’t despair if you don’t immediately get a notification. It sometimes takes a few minutes to show up, after you see the log on the Urban Airship console.
Urban Airship and the other third-party providers abstract a lot of details. If you want to better understand how the Web Push Protocol works behind the scenes, start from this page.
By using third-parties, it spares you the need to create all the infrastructure to record the IDs of the devices that register to get the notifications (ie: a DB with a table of device IDs, their tags etc.) and all the work to send the notifications to all targeted devices. After you make a single call to Urban Airship targeting a tag, it might mean 1M calls that UA does on your behalf, one per device, if there are 1M users/devices registered for that tag. You can easily see how not having to worry about that is good.
When you create a new Azure Service Bus Topic subscription through the Azure Portal, you won’t have the option to set up a ‘filter’, which means that all new messages for that Topic will be copied into the subscription. However, you might only want to process a sub-set of messages, not all. In the scenario described in this article, only messages with ‘IsInStock = true’ need to be processed, assuming that customers only want to be notified when something they liked is back in stock…and they don’t need to be notified about something going out of stock (that might be a reasonable option as well in reality, but not for the purpose of this demo).
The way it works so far is that there’s an if statement when the Function starts and the json data is decoded into a ProductStockChanged message. While this works, it’s not optimal, because it means the function is going to be executed plenty of times for messages it doesn’t need to process — this means wasted compute time, which in turns means additional instances being necessary and additional cost. A better solution is to create a subscription with a rule/filter, so that only messages matching certain conditions are copied into it for further processing. It’s not possible to create the filter directly from the Portal unfortunately, but luckily, it’s simple enough to do it programmatically (in a method that you could execute from a management console app or something similar). Here’s the method that modifies the existing subscription by adding a SqlFilter for ‘IsInStock = true’:
The SqlFilter can refer to attributes defined in the message’s UserProperties array, not on attributes defined directly in the json (because the message’s body could be plain text, json, xml or anything else)…so now line #67 of the Console app’s code displayed in Section 2 of the article should be clearer.
After executing this code, use the Console app to send some ‘outofstock {id}’ commands, and you’ll see that they’ll end up in the UpdateCatalog subscription (the second subscription I had created at the beginning, with no filter), but not in the BackInStockNotifications subscription. A much better result. (I still left the check in the Function itself as well though, just in case the filter doesn’t work or is removed by mistake.)
An even better solution that doesn’t require executing custom managed code as part of your deployment/configuration pipeline is using the Azure CLI as described here.
This was meant to be a POC, but in a real implementation you will very likely want to consider the following aspects when sending out push notifications:
That’s all folks. Albeit quick and simple to set up, the proposed implementation is quite powerful and flexible, as it’s based on loosely coupled components (the checkout and stock microservices know nothing about the Function that handles the stock-related messages to send push notifications) and it offers great scalability (the Azure Function will be scaled up automatically according to how many messages it has to process). Last but definitely not least, the use of Service Bus offers a better overall reliability of our system, because if the delivery of the notification fails for any reason (eg: Urban Airship is down), the Function fails, the message goes back into the queue and it will be processed again later. Hooray Azure!
Who am I / what do I do? I proudly work as a Solutions Architect in the Mobile Team @ ASOS.com (iOS app | Android app), and we’re always looking for strong, friendly and talented developers that want to have an impact on how customers shop online. ASOS is the biggest online-only retailer in the UK and, let’s be real, the best tech and fashion company in the world. Some of the technologies we use are Swift for iOS, Kotlin for Android, React and Node on the web front-end, .NET and Azure on the back-end. If that sounds interesting to you, and you happen to live in beautiful London (or are willing to move here — after all, it’s the best city in Europe, except for some in Italy!), do get in touch!
A collective effort from ASOS's Tech Team, driven and…
224 
1
Thanks to Giovanni Puntil. 
224 claps
224 
1
Written by
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
A collective effort from ASOS's Tech Team, driven and directed by our writers. Learn about our engineering, our culture, and anything else that's on our mind.
Written by
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
A collective effort from ASOS's Tech Team, driven and directed by our writers. Learn about our engineering, our culture, and anything else that's on our mind.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/free-code-camp/how-to-deploy-a-blazor-application-on-azure-cf6f3b1f03a0?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
In this article, we will learn how to deploy an ASP.NET Core hosted Blazor application on Azure. We will use Visual Studio 2017 to publish the app. We will create a SQL database server on Azure to handle DB operations.
Please refer to my previous article Cascading DropDownList in Blazor Using EF Core to create the application that we will be deploying in this tutorial.
We will create a resource group on Azure portal to contain all our resources on Azure.
Login to Azure portal and click on Resource groups on the left menu and then click Add. It will open a “Resource group” window as shown in the image below:
In this window we need to fill the following details:
We will create the SQL database and a database server on the Azure portal to handle our DB operations.
Click on SQL databases on the left menu of your Azure portal and then click Add. It will open a “SQL Database” window as shown in the image below:
Here you need to fill in the following details:
Before creating the database, we need to create a database server for the SQL database. Click on the “Server configure required settings” and then click Create a new server. It will open a “New server” window as shown in the image below:
Here we need to furnish the following details:
Check the “Allow Azure services to access server” check box and click on Select to create your DB Server.
Note: The word “admin” is restricted for the administrator user name of the database server. Use any other username than “admin”.
Once the DB server gets created, you will be redirected back to the “SQL Database” window. You need to click on the “Create” button to create your database.
Here is the whole process explained in a gif.
The database DDLDemodb do not contain the tables that we are using in our application. We will connect to Azure database using SQL Server Management Studio (SSMS) to create our DB objects.
Open SSMS in your machine and put the server name as ddldbserver.database.windows.net. Provide the admin user id and password that you have configured in the previous section. Then click on “Connect”.
You will get a pop up window for configuring the firewall rule to access the Azure DB. Login with your Azure account credentials and add your machine’s IP address under Firewall rule. Click on OK to connect to the Azure database server. Refer to the image below:
Once the connection is successful, you can see the DDLDemodb database on the server. Refer to my previous article Cascading DropDownList in Blazor Using EF Core. Run the SQL commands to create and insert sample data in the Country and Cities tables that we are using in our application.
After creating the database objects, we need to replace the connection string of local database in our application with the connection string of the Azure database.
Open Azure portal and click on SQL databases on the left menu. It will open a window displaying the list of all the databases that you created on the Azure portal. Click on DDLDemodb database and select Connection strings from the menu. Select the ADO.NET tab and copy the connection string. Refer to the image below:
You need to put the admin user id and password for the database server that you have configured earlier in this connection string.
Open the BlazorDDL application using Visual Studio, navigate to BlazorDDL.Shared/Models/myTestDBContext.cs and replace the local connection sting with this new connection string.
Launch your application from Visual Studio to verify if the new connection string is configured correctly and you are able to access the Azure database.
If the application is not working and you are unable to connect to the database, then check if your connection string is correct or not. Once the application is working as expected in your local machine then move to the next section to publish it on Azure.
To publish the Blazor app on Azure, Right-click on the Server project of your solution and click publish. In this case, it will be BlazorDDL.Server >> Publish.
It will open the Pick a publish target window. Select App Service from the left menu. Select the Create New radio button and click on the “Create profile” button. Refer to the image below:
The next window will ask you to login to your Azure account if you are not logged in. Once the login is successful, a Create App Service window will open. Refer to the image below:
The fields of this window have default values in them as per the configuration of your Azure account. However, you can change these values depending on your requirements.
You can fill the details as mentioned below:
Click on the “Create” button to start the application deployment on Azure. It will take few minutes to complete depending on your internet connection speed.
After the deployment is successful, click on the “Publish” button to publish the app to Azure. Once the application is published successfully, the website will be launched automatically in the default browser of your machine. You can also access the website using the URL BlazorDDLDemo.azurewebsites.net.
You can see the application in your browser as shown in the image below:
In this article, we learned how to deploy and publish a Blazor application on Azure. We created a SQL database and DB server on Azure and used them in our application to handle the DB operations.
Get my book Blazor Quick Start Guide to learn more about Blazor.
You can also read my other articles here.
Originally published at https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
276 
276 claps
276 
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
Written by
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
We’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. See you there.
"
https://media.consensys.net/truffle-is-now-available-on-microsoft-azure-marketplace-12d646caef59?source=search_post---------72,"Truffle is a development environment, testing framework and asset pipeline for the Ethereum Blockchain, aiming to make life as an Ethereum developer easier.
Earlier in the week, Microsoft Azure launched their Truffle integration in its Marketplace, offering a suite of developer tools that now makes it easier to build on Ethereum. With Truffle, you get:
To learn more about the pricing details, visit the Microsoft Azure Marketplace.
News, insights, and education on all things…
144 
1
144 claps
144 
1
Written by
ConsenSys is the leading Ethereum software company building MetaMask, Infura, Codefi, ConsenSys Quorum, Truffle, and Diligence. Visit consensys.net
News, insights, and education on all things decentralization from leaders in the blockchain industry
Written by
ConsenSys is the leading Ethereum software company building MetaMask, Infura, Codefi, ConsenSys Quorum, Truffle, and Diligence. Visit consensys.net
News, insights, and education on all things decentralization from leaders in the blockchain industry
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/deep-learning-turkiye/azure-ml-studio-kullanarak-bitcoin-fiyat-tahminlemesi-yapmak-f5179f395811?source=search_post---------73,"There are currently no responses for this story.
Be the first to respond.
Eğer elinizde büyük bir veri kümesi varsa bu veri içinde bazı örüntüleri (pattern) bularak bu veriyi anlamlandırabilirsiniz. Makine öğrenmesinden faydalanarak büyük veri içindeki örüntüleri tespit edebilir ve bu ilişkileri yeni veri kümeleri üzerinde kullanabilirsiniz.
Makine öğrenmesi için kendi bilgisayarınızı kullanabilirsiniz ama bu durumda çoğu zaman kaynak (GPU, CPU) yetersizliğinden dolayı modelinizi eğitmeniz uzun sürecektir. Bu gibi durumlarda bulut servislerini kullanmanız hem büyük veriyle uğraşmanızı kolaylaştıracak hem de size bazı ekstra kolaylıklar sunacaktır. Google Colab ve Azure Machine Learning Studio bu bulut servislerinden bazılarıdır. Bu yazıda Azure ML Studio’yu tanıtacağım.
Azure ML Studio, makine öğrenmesi modellerini kolaylıkla eğitmenizi sağlayan grafiksel öğelerle kolaylaştırılmış bir araçtır. Bu aracı kullanarak veri ön-işleme yapabilir, veri üzerinde makine öğrenmesi algoritmalarını çalıştırarak denemeler yapabilirsiniz. Modelinizi test ederek istenilen sonuçlara ulaştığınızda direkt olarak Microsoft Azure’da yayınlayabilirsiniz.
Azure ML Studio bu işlemleri kolaylıkla yapmanız için hazır veri ön işleme modülleri, makine öğrenmesi algoritmaları ve yayınladığınız modele erişim sunan bir API sağlıyor. Bu sayede bir geliştirici olarak eğittiğiniz makine öğrenmesi modelini hiç uğraşmadan yayınlayarak uygulamalara hizmet vermeye başlayabiliyorsunuz.
Azure ML Studio’ya buradan ulaşabilirsiniz. Herhangi bir kart bilgisi girmeden platformu ücretsiz olarak kullanabilir ve denemeler yapabilirsiniz. Ücretsiz versiyonda bir çalışma alanına en fazla 100 bileşen eklemenize izin veriliyor. Detaylar için burayı inceleyebilirsiniz. Giriş yaptıktan sonra karşınıza şöyle bir ekran gelecektir.
Burada oluşturduğunuz projeleri, denemeleri, web servisleri, Jupyter not defterlerini, Studio’ya yüklediğiniz veri kümelerini ve eğittiniz modelleri görebilirsiniz.
İlk denemenizi oluşturmak için Experiments bölümünü seçip aşağıdan New’e tıklayın. Daha sonra açılan ekranda paylaşılan hazır denemeleri görebilirsiniz. Biz şimdilik sıfırdan oluşturacağımız için “blank experiment”i seçerek devam edelim. Karşınıza aşağıdaki gibi boş bir stüdyo alanı gelecektir.
Bu ekranda soldaki bileşenleri boş alana sürükle bırak işlemiyle ekliyorsunuz. Hazır veri kümeleri, veri ön işleme yöntemleri, yeni veri kümesi içeri aktarma, makine öğrenmesi algoritmaları, Python ve R scriptleri ve yazı analiz algoritmaları gibi herşeyi bileşen olarak kullanabiliyorsunuz.
İlk denememiz için Data Input and Output bölümünden Import Data bileşenini sürükleyip bırakalım. Bu bileşen sayesinde kullanacağımız veriyi nereden elde edeceğimizi belirliyoruz.
Sürüklediğiniz Import Data bileşenini seçin solda açılan menüde “Launch Import Data Wizard” ı tıklayarak veriyi nereden-nasıl alacağınızı belirleyin. Veriyi direk dosya yükleyerek ekleyebileceğiniz gibi isterseniz herhangi bir online ortamdan da (RSS, Azure vb.) çekebiliyorsunuz.
İlk önce son bir yılın BTC fiyat verisini çekelim. Biz bu örnekte datayı bir siteden csv formatında alacağız bundan dolayı “Web URL via HTTP” seçerek ilerleyin. Sonraki ekranda “Data source URL” olarak “https://api.blockchain.info/charts/market-price?timespan=1year&format=csv” adresini girin. CSV veri kümemizde başlıklar olmadığı için “CSV or TSV has header row” bölümünü tiklemeyin.
Burada kullandığınız adresin yönlendirilmiş url olmaması gerekiyor aksi takdirde “Http redirection not allowed” gibi bir hata alıyorsunuz. Eğer elinizdeki adreste bazı yönlendirmeler varsa nihai adresi bulmak için redirectdetective sitesini kullanabilirsiniz.
Veri çekme işlemini başlatmak için stüdyonuzdaki “Import Data” bileşenini seçin sağ tıklayın ve “Run Selected”i seçin. İşlem tamamlandığında bileşenin sağında yeşil tik işareti belirecektir.
İşlem bittikten sonra veriyi görüntülemek için “Import Data” bileşenine sağ tıklayıp “Results dataset”->”Visualize”i seçin.
Çektiğiniz veri aşağıdaki gibi gösterilecektir. Burada kolon seçerek o kolona ait verinin detaylarını sağdaki bölümde görebilirsiniz. Bu bölümde kolona ait istatistiki bilgileri (ortalama, standart sapma vb.) kontrol edebilir, frekans dağılımını görebilir ve diğer kolonlarla karşılaştırmalar yapabilirsiniz.
Bu şekilde bir yıllık Bitcoin fiyat verisini çektik. Şimdi aynı işlemi günlük Bitcoin piyasa değeri (market cap) verileri için yapalım. Bu işlem için yeni bir “Import Data” bileşenini sürükleyip bırakın. “Data source URL” olarak şu adresi girin: “https://api.blockchain.info/charts/market-cap?timespan=1year&format=csv”. Bu bileşeni ekledikten sonra seçin ve sağ tıklayarak “Run Selected” ile çalıştırın. Daha sonra visualize butonuna tıklayarak veriyi görselleştirin.
Son bir yılın Bitcoin piyasa değeri verisini de çektik. Şimdi bu iki veri kümesini tarih (Col1) kolonunu baz alarak birleştirelim. Bu işlem için birleştirme (join) yapmamız gerekiyor. Sol taraftaki “search experiment items” kutucuğuna “join data” yazarak bu bileşeni bulun ve çalışma alanına sürükleyin. Daha sonra aşağıdaki gibi “Import Data” bileşenlerinin altındaki noktaları “Join Data” bileşenine bağlayın.
Birleştirme işleminde hangi kolonun bağlantı kolonu olarak kullanılacağını belirtmemiz gerekiyor. Bu işlem için “Join Data” bileşenini seçin ve sağ taraftan “launch column selector” e tıklayarak “Col1” kolonunu yani tarih kolonunu seçin. Join işlemi yapıldığında iki import bileşeninden de gelen 2 tane tarih kolonu olacak, 2. “import data”dan gelen tarih kolonunu tutmaya gerek yok bu yüzden “Keep right key columns” seçeneğindeki tiki kaldırın.
“Join Data” bileşenini seçerek “Run Selected” ile çalıştırın ve kontrol için “Visualize ” ile tabloyu açın. Bu işlem sonunda veri kümesi aşağıdaki gibi BTC fiyatı ve piyasa değeri aynı tabloda birleştirilmiş şekilde görünecektir.
Kolon isimlerini düzenlemek için “Join Data” bileşenini altına aşağıdaki gibi “Edit Metadata” bileşenini ekleyin. Kolonları seçin ve “new column names” alanına kolon adlandırmalarını virgül ile ayırarak yazın.
“Join Data” bileşeninin çıktısını “Edit Metadata” bileşenine bağlayın. “Edit Metadata” bileşenini çalıştırın ve tabloyu görüntüleyin, tablo aşağıdaki gibi görünecektir.
Bu veri kümesindeki tarih kolonunu kullanmayacağız. Tablodan kaldırmak için “Select Columns in Dataset” bileşeni ekleyin ve “Edit Metada” bileşenine bağlayın. Bu bileşeni seçerek sağ taraftan fiyat ve piyasaDeğeri kolonlarını seçin.
Birleştirdiğimiz veri kümesini eğitime sokmadan önce eğitim ve test verisi olarak ayıracağız. Bunun için “Split Data” bileşenini kullanacağız, bu bileşeni bularak sürükleyip bırakın. “Select Columns in Dataset” bileşeninden “Split Data” bileşenine bağlantı okunu aşağıdaki gibi bağlayın.
“Split Data” bileşenini seçerek eğitim/test verisi oranını belirleyin, verilerin ayrımı bu orana göre yapılacaktır. Bu oranı %80 eğitim, %20 test verisi olarak belirleyeceğiz bunun için “Fraction…” bölümüne yukarıdaki resimde gösterildiği gibi 0.8 girin.
“Split Data” bileşenini de çalıştırın ve altındaki noktalara sağ tıklayarak görüntüleme yaparak verileri kontrol edin.
Bu denemede doğrusal bağlanım (linear regression) kullanarak BTC piyasa değerine göre fiyatını tahminlemeye çalışacağız. Doğrusal bağlanım iki veya daha çok değişken arasındaki ilişkiyi anlamak için kullanılır. Mesela, elimizde insanların yaşlarını ve maaşlarını gösteren bir veri var. Bu veri kümesindeki her bir kişi için yaşı ve maaşı gösteren veri noktalarımız var: (50, 5000TL), (25, 3000TL) gibi. Bu veri noktalarını aşağıdaki gibi bir grafik üstünde gösterildiğini düşünelim. Doğrusal bağlanım bu veri noktalarını en iyi şekilde kesen aşağıdaki doğruyu bulmamızı sağlar. Daha sonra bu doğruyu kullanarak elimizdeki veri kümesinde bulunmayan bir veri için tahminleme yapabiliriz. Mesela 40 yaşındaki bir kişinini maaşını tahminleyebiliriz.
Arama kutucuğunu “linear regression” yazarak bileşeni bulun ve çalışma alanına sürükleyip bırakın. İsterseniz bileşeni seçip sağ taraftan ayarlarını değiştirebilirsiniz bu çalışma için default değerleri kullanacağız.
Bu algoritma ile model eğitim yapmak için “Train Model” bileşeni eklememiz gerekiyor. Aşağıdaki gibi ekleyerek bağlantıları yapın. “Split Data” bileşeninin altındaki sol nokta eğitim verisini temsil ediyor, bu noktayı “Train Model” bileşenine bağlayın.
“Train Model” . bileşenini seçerek sağ taraftan eğitim kolonunu belirleyin, bu işlem için fiyat kolonunu aşağıdaki gibi sağdaki menüden seçin.
Şimdi ilk eğitimimizi yapacağız, bağlantıları yaptıktan ve kolonu seçtikten sonra “Train Modeli”i seçerek çalıştırın ve adım adım tüm bileşenlerinizin çalışmasını izleyin. Bu aşamada hepsinde yeşil tik işaretini göreceksiniz.
Train Model bileşenine sağ tıklayarak model özelliklerine göz atın.
Eğitilen modeli test verisi üzerinde denemek için “Score Model” bileşeni ekleyin. Aşağıdaki gibi “Train Model” bileşenine ve “Split Data” bileşeninden çıkan test veri kümesine bağlayın.
“Score Model” bileşeni tahmin verilerini üretmek için kullanılmaktadır. Sınıflandırma modelleri için sınıf çıktısı verirken, bağlanım modelleri için tahmin edilen sayıyı üretir.
“Score Model” bileşenini çalıştıralım ve görüntüleyelim.
Görüldüğü gibi test veri kümesine yeni bir kolon (Scored Labels) daha eklendi. Burada modelin o gün için yaptığı fiyat tahmini bulunuyor. Bu veri kümesini inceleyerek ve 2. kolon ile karşılaştırarak modelin fiyat tahminlerini gözden geçirebilirsiniz.
Modelin ürettiği fiyatların ne kadar tutarlı olduğunu kontrol etmek için ise “Evaluate Model” bileşeni kullanılıyor. Bu bileşeni de çalışma alanına ekleyin. Bu bileşeni de “Score Model” bileşenine aşağıdaki gibi bağlayın.
“Evaluate Model” bileşenini çalıştırın, işlem bittikten sonra bu bileşene sağ tıklayıp “Evaluation Results”->”Visualize”a tıklayın ve modelin hata katsayılarını ve hata histogramını inceleyin.
Bu değerlerden hata (error) değerleri modelin ne kadar yanıldığını yani gerçek değerlerle tahminlenen değerler arasındaki farkları gösterir. Ortalama Mutlak Hata (Mean Absolute Error: MAE ) bu değerler arası farkların mutlak değerlerinin ortalamasını gösterirken, Karesel Ortalama Hata (Root Mean Squared Error: RMSE) ise bu farkların karelerinin ortalamasının kareköküdür. Bu hata değerlerinin sıfıra yakınlaşması modelin daha az hata yaptığını gösterir. Detaylar için şurayı inceleyebilirsiniz. Determinasyon katsayısı (Coefficient of Determination) ise tahmin edilen değerlerle gerçek değerler arasındaki korelasyonun karesidir. 0'a yaklaşması aradaki korelasyonun zayıf olduğunu 1'e yaklaşması ise güçlü olduğunu gösterir.
Modelinizi bir API olarak dışarı açmak için “Set up Web Service” bölümünden “Predictive Web Service”i seçin.
Azure ML Studio bu aşamada modelinizde bazı otomatik değişiklikler yapacak ve yeni bir sekmede “Predictive Experiment” bölümünde modelinizin son halini gösterecektir.
Burada normalde webservis iki girdiyle çalışıyordu yani hem fiyat hem piyasa değerini alıyordu. Bunu düzeltmek için predictive experiment tabında
“Select Columns..” altında bir “Select Columns” bileşeni daha ekledim ve sadece piyasadeğeri kolonunu seçtim daha sonra bunu “Score Model” bileşenine girdi olarak verdim. Bu sayede web servis sadece piyasaDegeri girdisiyle çalışır hale geldi.
Düzenlemeleri yaptıktan sonra bu bölümden modeli tekrar çalıştırın ve son kontrollerinizi yapın. Daha sonra aşağıdan “Deploy Web Service” i seçin. Açılan ekrandan API anahtarınızı görüntüleyebilir ve API test sayfasına erişebilirsiniz.
API’nizi test etmek için aşağıdaki Test butonunu kullanabilirsiniz.
Buraya servisinizi besleyecek olan değeri yani o günün BTC piyasa değerini girin ve butona basın.
Servis, modeli çalıştıracak ve aşağıdaki gibi BTC fiyat sonucu dönecektir. Bugün BTC 8070 USD olacakmış, yatırım tavsiyesi değildir :)
Tebrikler artık makine öğrenmesiyle BTC fiyatlarını tahmin etmeye çalışan bir web servisiniz var. Piyasa değeriyle BTC fiyatı tahminlemek pek sağlıklı bir yöntem olmasa da bu rehberin basit olması açısından böyle bir veri seçilmiştir. Daha sağlıklı bir model oluşturmak için indikatörleri (RSI, MACD), tweet sentiment analizlerini eğitime dahil edip farklı makine öğrenmesi algoritmaları kullanabilirsiniz.
Benim oluşturduğum stüdyo örneğine buradan ulaşıp kopyalayabilirsiniz. Örneklerin paylaşıldığı Azure yapay zeka galerisine buradan ulaşabilirsiniz.
Not: Eğitim sürecinde hata çıkarsa aşağıdaki gibi kırmızı çarpı işaretinin üzerine gelerek kontrol edebilirsiniz. Aşağıdaki hata, veri kümesinde kolon ismini değiştirilip “train model” bileşeninde kolon adı değiştirilmediğinde oluşmaktadır.
Azure ML galerideki örnekleri incelemenizi şiddetle tavsiye ederim. İlginç örnekler var ve bunları kendi stüdyonuza kopyalayıp denemeler yapabiliyorsunuz.
Mesela;
Amerika seçimlerinde yatırımların hangi adaya gideceğini tahminleyebilir,
gallery.azure.ai
bir ürünü alan müşterilerin tweetlerinde duygu analizi yapabilir,
gallery.azure.ai
veya kanser teşhislerini tahminleyebilirsiniz.
gallery.azure.ai
Kaynaklar
https://api.blockchain.info/charts/market-price?timespan=1year&format=csv
https://api.blockchain.info/charts/market-cap?timespan=1year&format=csv
gallery.azure.ai
gallery.azure.ai
peter.intheazuresky.com
Türkiye'nin En Etkili Yapay Zeka Topluluğu tarafından yapay…
610 
610 claps
610 
Türkiye'nin En Büyük Yapay Zeka Topluluğu tarafından yapay zekanın alt dalları olan makine öğrenmesi ve derin öğrenme alanlarındaki Türkiye'den üretilen içerikleri bulabilirsiniz.
Written by
Articles about Deep Learning, iOS App Development, Entrepreneurship and Psychology
Türkiye'nin En Büyük Yapay Zeka Topluluğu tarafından yapay zekanın alt dalları olan makine öğrenmesi ve derin öğrenme alanlarındaki Türkiye'den üretilen içerikleri bulabilirsiniz.
"
https://faun.pub/an-introduction-to-microsoft-azure-and-cloud-computing-54a7ebac0287?source=search_post---------74,"There are currently no responses for this story.
Be the first to respond.
You have heard lots of things about the cloud. Let’s understand each component of the cloud on this story. Also, let’s understand how the organizations are implementing various cloud models using Microsoft Azure. Before going…
"
https://medium.com/the-lark/azure-one-662dca9c9f7?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Floating on the waves of the ocean,the sun and the sky were above me.Burnt orange in the eyes of the shadow,pulsating into a flow of crimson red
The sky and the ocean, and the horizonin between, fading afar as I was drifting.For so long, lost in the flow of movement,wishing to…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/an-ambiverts-guide-to-azure-functions-95931976c565?source=search_post---------76,"There are currently no responses for this story.
Be the first to respond.
The following blog post will walk you through how to use Azure Functions, Twilio, and a Flic Button to create an app to trigger calls/texts to your phone. If you’re looking for a quick summary and overview on Azure Functions, I recommend starting here with our documentation, or taking 4 minutes to read in more detail how to create your first Azure Function, so you have some context on how to use functions within Azure.
If you’d like to skip straight to the code, scroll down to the Let’s get to the code!​ section below.
Happy learning!
-Chloe
Hello everyone, my name is Chloe, and I’m an ambivert 👋
As a former actress (and current developer advocate), people often assume I’m an extrovert. The big secret is… I’m not! I’m actually an introverted extrovert (also known as an ambivert). Day-to-day, that means I have no problem being on stage and giving a talk at a conference, socializing with attendees, doing a couple on-camera interviews, and sticking around for the schmoozing at a happy hour… in fact, I love it! But afterwards I will escape to my hotel room and watch TV under a blanket for the proceeding 3 hours because I have to “recharge.”​
So, here’s the definition of ambivert:​
​A person whose personality has a balance of extrovert and introvert features.​​
The best way (not so eloquent way) to describe my ambivert features would be that I equally love and hate being around other humans (only half kidding!).​ I love to socialize- conferences, meet-ups, and coffee chats are right up my alley (in fact, it’s a big part of my job!)…. but only half of the time. I have to recharge afterwards, or even schedule a couple nights at home to counter my social interaction. I’m likely very chatty at a dinner, but opt out of the evening karaoke (and trust me- ya girl LOVES karaoke).​​
So, what I’m trying to say is, if you ever see me sticking around at a conference happy hour, I’ve either had espresso later in the afternoon, I’m jetlagged, or I’m secretly dying on the inside.​
Remember… I have a theatre degree. I’m very good at convincing people I’m not internally screaming inside 😐. I see my extroverted-ness almost like a button I can turn on and off depending on the situation… which is a great segue into the device I’ll be using to showcase how I built this.
Learning and creating ways to navigate my ambivert-ness “in the wild” at conferences, meet-ups, and meetings has been an interesting process!​​ Today, in this post, I’d like to show you one of those hacks I’ve created for myself… and hopefully, if you’re an introvert (or ambivert, or a human/robot looking to build cool things) you can try this demo yourself and code your way out of awkward or undesirable social situations!​
When deciding to take on this project, I considered several different devices to build my project with.​​ At first, I pictured it as a wearable.​ I reached out to Sara Chipps and Jennifer Wadella to see if Jewelbots’ capabilities would work for my use case. Unfortunately, while very fashionable and fun, the range of the devices is limited (but if you have a young girl in your life- get them one of these. Very cool, fun way to learn programming!). I also considered FitBit as an option, but had concerns over folks thinking I was triggering my functions through it right in front of them. I needed something that was small, subtle and able to fit in my pocket.
So, the winner is… a Flic button! What’s a Flic button? Great question! It’s a small button (about the size of a quarter) that acts as a Bluetooth Remote for iOS and Android. You can program these buttons to do anything from turning on Smart lights in your home, controlling music, taking photos, sending texts, making calls, posting tweets, calling Ubers, etc.
Flic has many pre-built integrations and apps you can use within their app, ranging from sending a tweet to triggering a MP3 to play a laugh track (I may or may not have set up this specific example to bug my boyfriend in our apartment for when I tell bad jokes). Suz Hinton sent me a Flic button for our mentorship sessions a few months back for an Azure exercise, and once I learned how to connect it to an Azure Function, I knew I had to build something fun with it.​ While the Flic button does include a pre-built “fake call” feature in it’s app, it doesn’t actually create a call (you use a screenshot of a call, and it plays a ringtone sound). This is why using the Twilio API was necessary for this project, and how this blog post was born.
If you’re unfamiliar, Azure Functions is a serverless compute service that enables you to run code on-demand without having to explicitly provision or manage infrastructure. Not only can you can use Azure Functions to run a script or piece of code in response to a variety of events, but it also lets you execute your code in a serverless environment without having to first create a VM or publish a web application.​
You can trigger the execution of Azure Functions in a variety of ways. Here are 5 common ones​:
☝🏻HTTP (which is what I’ll be talking about today)​
✌🏻 Timers (example: every day at 11am, execute this function, that starts + checks the temperature of my sous vide)​
👌🏻 You can create a function triggered when data is added to or changed in Azure Cosmos DB​
🖖🏻 You can create a function triggered when files are uploaded to or updated in Azure Blob storage
🖐🏻 You can create a function that is triggered when messages are submitted to an Azure Storage queue
And many more! For a complete list, tutorials, documentation, and additional details of the capabilities of Azure Functions, start with the Azure Functions Documentation.
If you haven’t played much with serverless/Azure Functions, I recommend starting with reading this Azure Functions Overview and completing this Create Serverless Logic with Azure Functions module to get a better idea of how all of these pieces fit together before diving into programming your Flic button. The Microsoft docs are a great place to get free resources and lessons on how to get started!
Speaking of great documentation/getting started, I also used Twilio to create this. Twilio allows software developers to programmatically make and receive phone calls, send and receive text messages, and perform other communication functions using its web service APIs.​ Their walk-through/demo code has a special Rick Astley Easter Egg that I will show you in a bit since I kept it in my demo 🙃
Alright- let’s review our goals!
-Texting my friends an SOS message to save me​
-Triggering a call from my “boyfriend”*
​For the sake of easy to understand visuals/screenshots I used the Azure portal to create this. You can also use VS Code, the Azure CLI, etc.​ With Azure Functions you are given the the ability to code and test Functions locally on your machine without having to deploy to the cloud every single time you want to test (a huge time saver!).
To create an Azure function, you can just start from the Get Started menu and select (surprise!) Function App.
Then you’ll need to fill in some basic info about your function here. Including the app name, the Azure subscription you’d like to use, a resource group (I’m creating a new one in this case), the Operating System you’d like to use, the hosting plan (I’m using consumption), the location I’d like to use (I’m in California, so West US 2 is usually my default), the runtime stack I’d like to use (I’m using JavaScript in this case), and I have the option to create new storage or use existing. I created a new one in this case.​​
Once I have all these filled out, I can go ahead and deploy! Wait about a minute or two, then watch for the Deployment succeeded message.​
​
Woo! If you followed those steps, we have our resource! We’ll just select “Go to resource” to view your new Function App.​ Now we’ll add a new function
It typically takes about a minute to deploy and then we’ll have a fresh new Azure Function waiting to be called. The default code is a simple hello world app, where if you paste the function URL into your browser’s address bar. Add the query string value &name=<yourname> to the end of this URL and press the Enter key on your keyboard to execute the request. You should see the response returned by the function displayed in the browser.​
Cool! So, we see this works now. Let’s get to the fun part…
My boyfriend Ty Smith works full-time as an Android Developer at Uber, and is an Android GDG and GDE, and also travels for conferences as well. Needless to say, he’s a busy guy and I didn’t want my app to call him, because maybe he’d be in a meeting/at dinner/playing the new Resident Evil game, and I wouldn’t want to disturb him (also, testing this would have been a bit of a nightmare- example can be seen in this Twitter thread).
​So, everyone, please meet my new fake boyfriend Twilio Smith- he’s a Twilio # that I purchased (with a Texas area code 🌵🤠).​
After reviewing the Twilio API docs, I was able to get up and running pretty quickly with some sample code (shout-out to Twilio for the excellent documentation!).​
I have 2 Azure functions I needed to create and call. One for the call, and one for the texts. Please note: it’s okay to hardcode your Twilio credentials when getting started, but you should use environment variables to keep them secret before deploying to production. Check out Frank Boucher’s video on How to use Environment Variables in Azure Functions for a great 5 minute tutorial!
You’ll probably notice that this function sends a text to me vs to friends/coworkers at a conference. For the sake of this demo, I’ve made it so the code texts me so I can show this off in-person when I demo this on stage (plus, you’ll annoy less folks with test texts while debugging… again, you can learn more about that in this Twitter thread 😬🤦‍♀️). But obviously, you’d replace these numbers with the numbers of your friends you wish to alert.
The code for our phone call trigger is pretty similar except we’re making a call, not a text. You’ll also notice that I’m linking to something here.​.. let’s take a look at what that link is hosting.
As I mentioned earlier, one of the reasons I decided to use Twilio was to be able to have a real call come in on my phone. Twilio also gives us the capability to use TwiML to compose voice messages, as well as do things such as, oh, I don’t know… play an MP3 of Rick Astley perhaps? Obviously, you can record your own voice message MP3 (I’ve included several samples of my own voice as your cousin/partner/friend in the repo). You can take a look at Microsoft’s documentation on how to use Twilio for voice and SMS capabilities from Azure if you’d like to dive deeper into TwiML, or have more questions about configuring your Application to use Twilio libraries.
Now we can incorporate our Flic button. Here’s what the Flic app looks like (left). For the sake of time, I won’t walk through every step, but essentially you just add the URL of the Azure Function and click save. Flic’s app is very straightforward, and will require a simple copy/paste of the https link we created with our 2 Azure Functions.
​
​
Last, but certainly not least, I needed to add my fake boyfriend to my contacts (complete with an image) so it would look more legitimate when a call came through. Otherwise this would show up in my phone as an unknown #. So…. shall we go ahead and test it out?​​
As I mentioned before, I wanted to configure one of my Functions to text other people (for it’s actual use case), but I can’t really demonstrate/test that well on my own. So with this demo, my fake boyfriend is going to be texting me.
​​
So, that’s the app! As you can see, it’s pretty easy to get up and running with Azure Functions! If you’d like more instructions on how to deploy to Azure, check out the GitHub repo here. There are so many easy ways to deploy to Azure, and you can read about them in more detail in our docs.
Using simple Azure Functions just like this can open the door for a plethora of automation in your applications or even your personal life. Anything from a button for your children to press when they get home from school (to alert the bus dropped them off safely), even starting a tea kettle in the morning so your tea is ready to go while you’re groggily getting ready for work, or creating a function to check a database in your app on a timed schedule. This particular Twilio demo was created just for fun, but think about how using Azure Functions in your applications or everyday tasks could automate things for you!
So, what’s next for this project?​ Well, I’d love to add a couple more features​- please check out the repo on Github if you’re interested in contributing your own features and ideas! Here are a couple that folks have suggested on Twitter:
If you’ve read this far- congrats! You’ve successfully learned how to get yourself out of awkward social situations using technology. If you’d like to dive deeper into any of these topics, here are some great places to get started:
Azure Functions Documentation — a great starting point for beginners which includes 5 minute Quickstarts​ to create functions that execute based on events created via:
Have any questions? Comment below, or shoot me a message on Twitter!
​
Any language.
306 
1
Thanks to Suz Hinton and chanezon. 
306 claps
306 
1
Written by
Musical theatre actress turned developer evangelist.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Musical theatre actress turned developer evangelist.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@onmyway133/machine-learning-in-ios-azure-custom-vision-and-coreml-645e93f35eee?source=search_post---------77,"Sign in
There are currently no responses for this story.
Be the first to respond.
Khoa Pham
May 23, 2018·6 min read
This is a Part 2 of my Machine Learning in iOS tutorials, check Part 1 first.
You may have read my previous article about Machine Learning in iOS: IBM Watson and CoreML. So you know that Machine Learning can be intimidating with lots of concepts and frameworks to learn, not to mention that we need to understand algorithms in Python.
Talking about image labelling, you might have a bunch of images and you want to train the machine to understand and classify them. Training your own custom deep learning models can be challenging. The easiest approach is to start with the easy step and to use some cloud services, so that you don’t get discouraged at first. Imagine that the code to train the model is already written for you, all you have to do is to tag related images, and get the trained result.
In this tutorial we will take a look at Azure Custom Vision service by Microsoft, that allows us to build custom image classifier.
Custom Vision is a service that allows us to build custom image classifier, firstly announced at MSBuild 2017, you can watch the keynote here. It is one of many services in Cognitive Services in the AI + Machine Learning product in Azure. Other services include speech, language, search, bot, etc.
The Custom Vision Service is a Microsoft Cognitive Service that lets you build custom image classifiers. It makes it easy and fast to build, deploy, and improve an image classifier. The Custom Vision Service provides a REST API and a web interface to upload your images and train the classifier.
To begin your work, go to Custom Vision home page and click Get Started. You should have an Azure account and if not, just register here for free. The free tier is enough for us to get started using the service, with 2 projects and 5000 training images per project.
The usage is extremely easy. Here is the dashboard for Custom Vision, I can’t expect it to be simpler. Follow me with the next steps.
Create a new project called Avengers. For laziness and for easy compare with other cloud services, here we use the same dataset from the post Machine Learning in iOS: IBM Watson and CoreML. Just to recap: last time we made an app that recognised superheroes. And since last post, people request that they want to see more superheroes ❤️
Note that in the Domain section, you need to select General (compact). As this produces lightweight model that can be consumed in mobile, meaning that the trained model can be exported to .mlmodel, the format supported by CoreML.
Click Add images and select images for each superhero. Name a proper tag.
About data set, this What does Custom Vision Service do well? says that:
Few images are required to create a classifier or detector. 50 images per class are enough to start your prototype. The methods Custom Vision Service uses are robust to differences, which allows you to start prototyping with so little data. The means Custom Vision Service is not well suited to scenarios where you want to detect subtle differences. For example, minor cracks or dents in quality assurance scenarios.
So our images can be sufficient for this tutorial.
Click Train to start the training process. It shouldn’t take a long time as Custom Vision uses transfer learning.
Azure allows us to use Prediction API to perform prediction based on our trained model. But in this case, we just want to get the trained model to embed in our iOS apps to run offline. So click Export and select CoreML.
We use the same project as in the Machine Learning in iOS: IBM Watson and CoreML. The project is on GitHub, we use CoreML and Vision framework to perform prediction based our trained model.
We name the model to AzureCustomVision.mlmodel and add it to our project. Xcode 9 can autogenerate a class for it, so we get the class AzureCustomVision. Then we can construct Vision compatible model VNCoreMLModel and request VNCoreMLRequest, and finally send the request to VNImageRequestHandler. The code is pretty straightforward:
Build and run the app. Select your superheroes and let the app tell you who he/she is. Our dataset is not that big, but you can see the model predicts pretty well with very high confidence. For now, we have images for only 4 superheroes, but you can add many more depending on your need.
We have covered IBM Watson and Microsoft Azure Custom Vision. There are other cloud services that worth checking out. They can be as easy as uploading images and training, or more advanced with custom TensorFlow code execution or complex rules.
This post Comparing Machine Learning (ML) Services from Various Cloud ML Service Providers gives a lot of insight into some popular machine learning cloud services together with sample code, worth taking a look too.
Here are some links to get started with using cloud services, especially Azure Custom Vision:
Check out my apps https://onmyway133.com/apps
932 
932 
932 
Check out my apps https://onmyway133.com/apps
"
https://blog.aion.network/microsoft-azure-lists-aionnode-a563181b0fbd?source=search_post---------78,"© 2022 The Open Application Network. All rights reserved.
"
https://medium.com/@anthonypjshaw/azure-pipelines-with-python-by-example-aa65f4070634?source=search_post---------79,"Sign in
There are currently no responses for this story.
Be the first to respond.
Anthony Shaw
Jan 2, 2019·7 min read
In this tutorial, I’ll show you -by example- how to use Azure Pipelines to automate the testing, validation, and publishing of your Python projects.
Azure Pipelines is a cloud service that supports many environments, languages, and tools. It is configured via a master azure-pipelines.yml YAML file within your project.
Your project can exist in multiple source repositories, and automated integrations are available for GitHub.
If you want to see the project layouts and how the configuration looks, everything in this tutorial is on my GitHub repository. It’s licensed as MIT, so you can do with it as you wish!
github.com
A build can have multiple jobs. You might want to segment your jobs by tasks like:
In these examples, I’ve focused on testing. Because Python is an interpreted language, the build stage is typically replaced with dynamic and static checkers, known as ‘linters’.
Having automated tests for any Python project can give you fast notifications when a change to the code has broken an existing feature in your application.
Your azure pipelines build will consist of jobs, these jobs consist of a number of steps. Some steps are pre-defined and called “Tasks”, you can find the full list on the Microsoft Azure Pipelines website.
You can also define your own tasks consisting of scripts in either Bash (Linux and MacOS) or PowerShell (Windows).
Some of the pre-defined tasks you’ll need for Python are:
UsePythonVersion@0 — Use a specific Python version (like a virtual environment)
PublishTestResults@2 — Publish the test results to the dashboard. Test results should be in the Junit XML format.
When configuring a job, you have to pick which Virtual Machine image to use for the job agent to execute your steps. For Linux, there are many images to choose from, Ubuntu 16 has the latest versions of Python available.
To test on Windows, the best image to choose is the vs2017-win2016 image, which has Visual Studio 2017 on Windows Server 2016.
Some of the other images include:
Because steps are defined in a job, and a job can only have 1 operating system it can get very tedious to copy steps between jobs and keep the up to date.
If your build and test steps are the same across Linux and Windows, you can move the steps into a separate file (in this example, called templates/steps.yml) and include it in the job.
Once you’ve configured your testing jobs to output a Junit XML file and you’ve successfully configured the PublishTestResults task, you will see a detailed test dashboard. All test failures will be available here. In the examples, I’ve named the Test Results based on the operating system, job name and the version of Python so it’s easier to see what’s happening. You can add any variable or label to the test names.
To test a Django application, there are 4 basic steps you need to add to a build job:
This is important as Django can behave very differently between Python versions. This example tests Python 3.6 and 3.7. You can change these for other versions, like 3.5.
The installation of Django would be via. pip . This script task updates the pip binary and then installs any dependencies in django-basic/requirements.txt, which is the example Django Project I’ve set up for this tutorial.
To test a Django application you can use the python manage.py test command, or use another tool like Pytest.
Because Azure Pipelines test output can be viewed and sorted online, it’s beneficial to use the Junit XML output. Pytest is the easiest way to generate this. The pytest-django package is a plugin for Pytest that will work with your existing Django tests. You only need to make a small change. Create a pytest.ini file in your project directory with these settings:
The task itself brings together these tools to test an application:
Lastly, use the builtin task to copy the test output to the Azure Pipelines service.
Here is the azure-pipelines.yml for the example Django app which can be found on GitHub.
If you want to test your Django application against multiple versions of Django, or a particular plugin, you can add extra version numbers as variables in the build matrix.
In this example, I’m testing 2 versions of Django across 2 versions of Python. This generates multiple jobs automatically.
Because there is no built-in task for selecting the version of Django, we use the variable to control the version installed in pip.
This script task adds the django.version variable defined in the matrix to the pip install command.
Here is the final azure-pipelines.yml for the matrixed Django test
If you’re using the Flask web framework, the steps are very similar to Django, but it doesn’t require a Pytest plugin.
In the example repository, I’ve simply copied the Flask example web application which comes with tests. To use this on Azure Pipelines, you need to run the setup.py command using pip install -e . from the application directory. Adding the [test] suffix will install the testing tools.
If you wanted to calculate the test coverage of your application, you combine the testing task with the coverage package.
For the flask-basic example project, you install both Pytest and coverage, then run pytest through the coverage run command.
You will be able to view the coverage data in the Azure Pipelines portal.
If you’re building Python libraries for distribution via shared files, PyPi or another artifact system, this example illustrates how to build source and binary distributions on Azure Pipelines.
Aside from the test commands that have been shown throughout this tutorial, we need to add a couple of extra tasks.
Azure Pipelines comes with an artifact publishing, hosting and indexing API that you can use through the tasks. You can also see the artifacts from a build in the web interface.
In the example project, I’ve created a really simple Python package, with a setup.py and setuptools configured.
To install the dependencies, this time we’ll install the package using pip install -e . once we’ve changed to the source directory.
Now, to test the library we can use Pytest again to generate the Junit XML output:
Then, after the test results have been published, the sdist (source distribution, a tar.gz copy of the source files, and a binary wheel can be built and published in the artifact repository using the CopyFiles and PublishBuildArtifacts tasks:
Azure Pipelines has support for publishing the packages to PyPi, but I haven’t added that to my examples. It’s well documented on the website.
If you’re using pyproject.toml as your configuration file and using Flit to publish and build packages, there are a few changes to make to the previous example.
Finally, if you want to download the artifacts and install them as a final verification, you can add an additional job like this:
Some other pointers I learned whilst writing these examples:
Group Director of Talent at Dimension Data, father, Christian, Python Software Foundation Fellow, Apache Foundation Member.
183 
2
183 claps
183 
2
Group Director of Talent at Dimension Data, father, Christian, Python Software Foundation Fellow, Apache Foundation Member.
"
https://medium.com/swlh/lets-do-devops-bootstrap-aws-to-your-terraform-ci-cd-azure-devops-github-actions-etc-b3cc5a636dce?source=search_post---------80,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
Pairing Terraform with a CI/CD like Azure DevOps, Terraform Cloud, or GitHub Actions can be incredibly empowering. Your team can work on code simultaneously, check it into a central repo, and once code is approved it can be pushed out by your CI/CD and turned into resources in the cloud.
"
https://medium.com/flutter-community/nlp-chat-on-flutter-azure-676aa4768fbb?source=search_post---------81,"There are currently no responses for this story.
Be the first to respond.
At this point Flutter has alot of momentum and works really well to build cross-platform apps quickly and has web support as well. On the downside the community & 3rd party support is in its infancy and the platform is changing fast. Obvious choice to use the newest and coolest tech now that its past Beta.
I wanted realtime push of messages rather than polling. Unfortunately once you choose Flutter, it has a ripple effect on what push hubs you can use. Flutter aligns with Googles Firebase and has near native support for Flutter Cloud Messaging (FCM). Integration with Azure Events does not seem straightforward hence the quick path is to use Firebase for both Events and Storage.[11]
Given Flutter and Firebase use, the natural choice is to use Google Cloud Platform rather than Azure for all hosting. I chose Azure for personal reasons — being more experienced on it, and understanding the Azure Machine Learning environment better. I am going very “polyglot” and going with a Python Flask REST to host Python ML models — very cost effectiveness of way to host services on Azure (esp if you bundle onto an existing web app — plan B is to use Azure Functions).
There are various NLP Sentiment Analysis [2,3,4] models you can implement. The cutting edge at this point is Google’s BERT and its variants[6,7]. Microsoft has released Azure Compute compatible pre-trained models which we can fine tune [8]. It maybe worth building a few models, starting with simple ones so we can measure differences/improvements.
This was a very brief article to kickoff my new project and explain how we make some basic design decisions up front. Look for Part 2 code + walkthru as I build it soon!
References and Inspirations:
[1] Flutter Overview — https://medium.com/swlh/why-businesses-should-start-focusing-on-googles-flutter-and-fuchsia-48e16820f2a9
[2] Nice Overview of NLP w/ code— https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72
[3] Guide to word embeddings — https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec
[4] More on Sentiment Analysis — https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17
[5] Azure already has a Sentiment service — https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/?WT.mc_id=blog-medium-abornst
[6] BERT primer — https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
[7] More on BERT — https://medium.com/sciforce/googles-bert-changing-the-nlp-landscape-5f4a7bf65cc5
[8] Azure and BERT — https://azure.microsoft.com/en-us/blog/microsoft-makes-it-easier-to-build-popular-language-representation-model-bert-at-large-scale/
[9] Github details on pre-training and fine tuning BERT — https://github.com/google-research/bert
[10] Github for Azure BERT implementations — https://github.com/microsoft/AzureML-BERT
[11] Flutter and Firebase integration — https://medium.com/flutterpub/enabling-firebase-cloud-messaging-push-notifications-with-flutter-39b08f2ed723
Https://www.twitter.com/FlutterComm
Articles and Stories from the Flutter Community
181 
181 claps
181 
Articles and Stories from the Flutter Community
Written by
Tech Manager by Day, ML Hacker by Night — founder: foostack.ai
Articles and Stories from the Flutter Community
"
https://medium.com/@maarten.goet/azure-sentinel-design-considerations-492f87fae384?source=search_post---------82,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 18, 2019·11 min read
I’ve written about Azure Sentinel before and how cloud SIEM’s are changing the security landscape. Microsoft provides Azure Sentinel as-a-service, which you can enable with the click of a button, only paying for the storage you use.
However, Azure Sentinel, as with any cloud services and/or SIEM, still needs some design considerations if you are putting it into production. What are these considerations? And what are the options available to me and my company?
In this article I’ll show you a couple of things to consider when designing for Azure Sentinel. From foundational choices, to identity & access, to (data) connections and dashboarding; I’ll share some real-world experiences.
Let’s look at the foundation first
Before we start, let’s make sure we are on the same page first and understand the fundamentals. Azure Sentinel uses a Log Analytics workspace as its backend, storing events and other information. Log Analytics workspaces are the same technology as Azure Data Explorer uses for its storage. These backends are ultra-scalable, and you can get back results in seconds using the Kusto Query Language (KQL).
The first thing to plan for is the Log Analytics workspace we’ll be using. When setting up Azure Sentinel for the first time, it allows you to create a new Log Analytics workspace or to pick an existing one.
DESIGN CONSIDERATION: New or existing Log Analytics workspace?
Let’s look at why would you want to re-use an existing workspace. Of course, it would be the easy way; it is already there, you’ve set up the right access to it, data is already streaming in and you can just add Azure Sentinel to it. No problem, right?
Well, access control is particularly one of the bigger reasons to potentially create a new Log Analytics workspace. That allows you to tightly control who has access to that aggregated data in Azure Sentinel, which often is a CISO requirement as we’ll be discussing below.
Apart from access control reasons, you might also run into a technical challenge that forces you to create a new workspace; it is relatively hard to move an existing Log Analytics environment over to another subscription. You need to first offboard agents, remove current Solutions, before you can move it. And that might cause ‘downtime’ for the monitoring solution currently using that workspace.
And of course, the last reason would be that sometimes you’ve created a bit of history in your current environment; experimented with settings, have a name for your workspace that you’d like to change etcetera, so you might want to start with a ‘clean slate’ because of that.
DESIGN CONSIDERATION: How long do we need to store our data?
One other thing to consider is how long you will want to store the data. The default setting will be 31 days. However, you can change this workspace setting and extend to up to two years. As per the documentation:
“The retention period of collected data stored in the database depends on the selected pricing plan. By default, collected data is available for 31 days by default, but can be extended to 730 days. Data is stored encrypted at rest in Azure storage, to ensure data confidentiality, and the data is replicated within the local region using locally redundant storage (LRS). The last two weeks of data are also stored in SSD-based cache and this cache is encrypted.”
No, Azure Sentinel will NOT replace Azure Security Center
An often-heard remark is: “Oh, so Azure Sentinel will replace Azure Security Center.” No, no no. Azure Security Center has its own place in the security landscape. It acts as the primary ‘engine’ to perform detections on Microsoft Azure, in your VM’s, in containers and on other properties such as Azure Stack, your on-premises infrastructure, etcetera.
Want to detect crypto miners in your Linux VM on Azure? Enable Azure Security Center. Want to get best practices and insights on securing your network in Azure? Enable Azure Security Center.
However, if you want to coordinate your security operations centrally, and aggregate multiple security solutions, such as Azure Security Center, Microsoft’s Cloud App Security, Azure ATP and others, you will want to enable Azure Sentinel.
By connecting all these data sources, you can start building a single pane of glass, and have one point of entry for your responders when they need to go threat hunting.
DESIGN CONSIDERATION: Which other security solutions will I be enabling alongside Azure Sentinel?
The identity and access piece is important
As pointed out above, often the CISO office will require you to tightly control who has access to that aggregated data in Azure Sentinel, because it could contain personal identifiable (PII) data. Normally only appointed security officers will be granted ‘read access’.
DESIGN CONSIDERATION: Who needs access to the data in Azure Sentinel? Can we provide that access ‘just in time’ to these people & roles?
Microsoft is in the process of adding RBAC features to Log Analytics workspaces as Oleg Ananiev, the group program manager for both Azure Monitor and Log Analytics, points out. This will then implicitly work for Azure Sentinel. More information can be found here.
As people come and go in a company, security offers will also likely be changing over time. We don’t want to grant access to a specific person but to the role he or she is fulfilling. We also don’t want to grant access all the time, but only when needed; for instance, when hunting for threats, or when a specific case was raised, and an investigation is opened.
This is where Azure Active Directory (AAD) Privileged Identity Management (PIM) can help. You can find more information on Azure AD PIM here. You will need either Azure AD P2 licenses or EM+S E5 licenses for those users you would like to use with Azure AD PIM.
Plan for the data connections
Azure Sentinel has a lot of possible data sources. Each and everyone of those needs a data connection and potentially a configuration.
DESIGN CONSIDERATION: Which data sources will I be connecting? What configuration does that data connection need?
I won’t be writing up each configuration of each possible data source. But I will provide you with a few ones that you need to think about, because there are things to know on how you can connect them:
.
.
How should I connect other SIEM systems?
Some enterprises will already have some sort of SIEM solution, like for instance ArcSight. And while I am NOT advocating that this is the preferred way of setting up cloud governance, your CISO office might want to hook up Azure Sentinel to that current system.
DESIGN CONSIDERATION: Will I be connecting Azure Sentinel to another SIEM solution?
If that is a requirement, you will want to consider using Azure Monitor and Event Hubs to forward your alerts to this other system. By using Event Hubs, you can do this safely and reliably; even when the receiving end is offline or malfunctioning, events get stored in the queue and Azure will release them when the system is back online.
If your system is not supported by Azure Monitor or Event Hubs, there still a fair chance to get it integrated with Azure Sentinel. There is a growing list of third parties that have built their own integrations on top of the API, that you can use. You can find the list here.
How can we support the threat hunters?
Up until now, we’ve talked about getting data into Azure Sentinel. But after it gets processed and an alert gets raised, you will want to investigate. Your threat hunting colleagues need access to the data to understand what is going on.
DESIGN CONSIDERATION: What technology will the threat hunting colleagues be using? Do they prefer Jupyter? Will they require KQL access to the workspace?
One of the ways to do threat hunting is using the Kusto Query Language (KQL) and search through events quickly and easily in the workspace. They could use Azure Data Explorer, the ‘Logs’ function of the Log Analytics workspace, a third-party application (such as Grafana) or the native Azure Sentinel UI in the Microsoft Azure portal.
That last option, going threat hunting from the Azure Sentinel portal UI, is a neat option. Microsoft provides you out of the box with pre-fab hunting queries and maps them back to the right Tactics category (fi: Initial Access, Lateral Movement, etcetera). Either way, consider what you would to do to provide them with the right UI, and access rights.
Another popular option among threat hunters is Jupyter. Microsoft has a free service based on Jupyter notebooks called Azure Notebooks. Through the ‘Kqlmagic’ extension, you can use Python to directly query the workspace using KQL queries. I’ve wrote about that here. Consider if they will be using Jupyter locally (fi: in a docker container) or if they’ll use Azure Notebooks. Also consider where you will be storing the notebooks; GitHub is a great option for that. And remember, Microsoft already provides you with many sample notebooks to get you jumpstarted.
Dashboards: how will we visualize the Azure Sentinel data?
Azure Sentinel provides a lot of out-of-the-box dashboards. Some of them are solution focused (Office 365), some are technically focused (Insecure Protocols) and some are geared towards third parties (F5, Palo Alto, etcetera).
Technically, these are JSON files that work in the Azure Dashboards section of your portal. You import them into your Tenant, and they will be available for everyone who can access that Tenant. Of course, you can restrict this with the built-in Azure access controls as they are just resources like any other.
Microsoft regularly updates it (GitHub) repository with new versions of the dashboards as they receive feedback from the field. You can manually update the JSON file in your Tenant or use the built-in functions in the Azure Sentinel UI. Either way, you should plan for some change management around this.
DESIGN CONSIDERATION: What are my requirements for visualizing Azure Sentinel data? How do I provide access to those programs and/or operators?
Another popular choice to visualize data from Azure Sentinel is to use open source visualization tools. Grafana is a great option, because it has a large ‘store’ with visualization types (most of them free), and because Microsoft provides you with a native Log Analytics connector for Grafana.
With that connector, you can use Kusto (KQL) queries to get specific data from Azure Sentinel and map it onto one of Grafana’s visualizations. For instance, a world map with network connections, or a list of Alerts. Grafana has dashboarding features that most SOC’s will love, for instance the rotating dashboards. You will of course need to plan access from Grafana to Azure Sentinel’s data.
Escalation and notifications
All the above are technical design considerations. However, if Azure Sentinel will be powering your Security Operations Center (SOC), you will need to design your processes as well. How will your Alerts be followed up? Do we need a connection to our ticketing system? What if alerts & tickets stay open for too long? Are the right people, and potentially the management, informed in time (before breaching the SLA)?
DESIGN CONSIDERATION: What process do I need to run my Security Operations Center (SOC)? Which tools will support my Service Levels?
One of the options available to you out-of-the-box to automate the follow-up of alerts are Playbooks. In essence, these are Azure Logic Apps that can be triggered whenever a certain condition is met. For instance, an Alert with a high severity gets raised by Azure Sentinel, and you want to send this to a security engineer via text message. The logic app could contain code that connects to Twilio and sends the Alert description to a specified phone number.
But is this reliable enough? How do I know the security engineer has read it? What if he or she didn’t, and we need to escalate to the next engineer. Or worse yet, we’re approaching SLA times and we need to start informing management. This is where 3rd party solutions like SIGNL4 come in. There are a few out there, but SIGNL4 is great because it is a cloud service where you can set escalation paths, do two-way communication (to receive acknowledgement), use multiple channels (ersistent push, text and voice) and log the audit trail. They also support duty scheduling and have a 2-tier escalation model.
Key takeaways
The key takeaway from this article is that while Azure Sentinel is software-as-service, you should still plan for the implementation of the service. Gather your business / CISO requirements and consider for each subject what you should do. Also, don’t forget that it is not only a technical deployment, but you will need to plan for the process side as well.
Do you have other design considerations you are taking into account when deploying Azure Sentinel? Do you have real-world experience with Azure Sentinel? I would love to hear from you in the comments below.
Stay tuned for the next installment in my multi-part deep-dive series on Azure Sentinel!
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
79 
6
79 
79 
6
Microsoft MVP and Microsoft Regional Director.
"
https://medium.com/ontologynetwork/ontology-added-to-microsoft-azure-and-amazon-aws-marketplaces-92b3cb0593ec?source=search_post---------83,"There are currently no responses for this story.
Be the first to respond.
Ontology ONT_Dev_Platform has been added to the Microsoft Azure and Amazon AWS Marketplaces — Microsoft and Amazon’s online software stores — and is expected to enter Google’s in January 2019.
ONT_Dev_Platform is an Ontology blockchain dApp product built on a cloud service, which includes the Ontology stand-alone test environment, SmartX, and Block Explorer. It is available to users worldwide for free to deploy in just one click.
Got questions? Feel free to ask the Ontology development team and community in the #development channel in the Ontology Discord.
Are you a developer? Make sure you have joined our tech community on Discord. Also, take a look at the Developer Center on our website, there you can find developer tools, documentation, and more.
Ontology website / Ontology GitHub / ONTO website / OWallet (GitHub)
Telegram (English) / Discord
Twitter / Reddit / Facebook / LinkedIn
A high performance, open-source blockchain specializing in digital identity and data.
723 
723 claps
723 
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
"
https://medium.com/microsoftazure/moving-from-lambda-%C6%9B-to-azure-functions-b6d5ed5ca007?source=search_post---------84,"There are currently no responses for this story.
Be the first to respond.
“It’s time to migrate.”
Maybe the decision to switch cloud providers came “down from above.” Maybe the decision was yours. Perhaps your original solution was merely a spike to “kick the tires” of one cloud, and now it’s time to try another. Whatever your reasons, if you are considering moving your serverless functions from AWS Lambda to Azure Functions, you’re in the right place to learn how!
“Moving from Lambda to Azure Functions” is a six-part videos series that covers what you need to know to make the transition between cloud providers. You’ll learn how to migrate your app, explore how resources in Azure relate to Amazon AWS, build a function locally, deploy it manually and learn how to push it automatically as part of a CI/CD pipeline.
The sample app is simple but does more than just echo text or print, “Hello, world.” It computes whether or not a number is a prime and uses a cache to store the results to serve them quickly on subsequent calls. The AWS Lambda implementation uses JavaScript (Node.js) and Amazon DynamoDB.
You can view the source code for the sample app and deploy the migrated code directly to Azure with a single-click in the “AWSMigration” GitHub repository.
github.com
The first video provides and overview of the sample application and shows how to test and access it from the portal and the command line.
The next video walks through how to create an Azure Functions app: the solution for hosting serverless functions in Azure.
Learn how to migrate the code and move from using Amazon DynamoDB to Azure’s inexpensive and easy-to-use Azure Table Storage for the application cache.
After the app is migrated and deployed, review how Azure resources are organized and accessed compared to Amazon AWS.
Use the cross-platform Azure Functions Core Tools to create a local functions project and run it. Then, using free and cross-platform Visual Studio Code, build and debug a project in just a few short steps. After implementing the fully migrated function, deploy it to Azure directly from Visual Studio Code.
Friends don’t let friends right-click publish. That’s why in this final video we’ll make your DevOps team proud by implementing continuous deployment. The function will also get a security lift in two areas: first, it will require authentication for access. Second, it will get assigned a managed identity to securely access other resources and assets.
This is a short series designed to ease your understanding of how to migrate from AWS Lambda to Azure. As always, we welcome your feedback, comments, and suggestions. If you have experienced a similar migration, please share your thoughts and tips in the comments below!
What’s next? Check out an Overview of Azure Functions.
Any language.
157 
1
157 claps
157 
1
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/pytorch/efficient-serverless-deployment-of-pytorch-models-on-azure-dc9c2b6bfee7?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
Authored by: Gopi Kumar, Principal Program Manager at Microsoft. (@zenlytix)
Recent advances in deep learning and cloud-based infrastructure have led to innovations in models for various domains like natural language processing, computer vision, recommendations. Of course, developing the model is only half the story. Your models are mostly useful once they are served up for making predictions for consumption in in AI-driven scenarios from the end applications. It is important to do it in a cost-effective and reliable manner. However, managing infrastructure for hosting your models is challenging as it involves several aspects like maintaining your fleet, ensuring reliability, scaling, security and ongoing monitoring and management. Can we leverage serverless technologies for our model hosting?
Azure provides serverless infrastructure with the Azure Functions service that offloads many of these infrastructure management tasks and simplifies the rest. Azure Functions operates the hosting instances to run your models as small functions without the developer or operator being aware of the specific virtual machine or fleets. Depending on your application needs and cost budget, you can use a choice of several hosting plans within Azure Functions from basic instances with a consumption plan, to premium instances and dedicated hosting. We will use the “consumption plan” which is usually the most cost effective (often free upto 1 million monthly requests per subscription ) option for relatively low volume scenarios.
You are only charged for the duration that the function actually runs in a consumption plan. Also, as your needs change, you can easily upgrade to premium or dedicated hosting plans as the underlying technology and methodology is still the same. Details on pricing for various hosting options are here. For efficient model serving we will use the ONNX Runtime, a highly optimized, low memory footprint execution engine. With ONNX Runtime, the deployment package footprint can be upto 10x lower, allowing us to use the more cost effective plan. More details are in the section below titled “Optimizing the runtime footprint”.
We will walk through the steps to take a PyTorch model and deploy it into the Azure Functions serverless infrastructure, running the model prediction in the highly efficient ONNX Runtime execution environment. While the steps illustrated below are specific to a model that was built using the popular fast.ai (a convenience library built on PyTorch), the pattern itself is quite generic and can be applied to deploying any PyTorch model.
The main steps to get your models into production on Azure serverless infrastructure using the ONNX Runtime execution engine (after you have trained your model are):
The model illustrated as an example is the Bear Detector model which is one of the popular examples in fast.ai. We won’t go into the actual training process here as it is the same method you normally use. The end result of the training process is a PyTorch model object in your Python environment. PyTorch provides a built-in mechanism to export your model object in the format needed by ONNX Runtime with the following code:
The parameters for the dummy_input depends on the shape of the tensors in your model. The output will be the model written to a file called model.onnx.
You also need to create a label file (labels.json) since this is a classification model. In the Bear Detector example, the model is classifying across the three classes of bear, and hence the label file looks like this:
The classes names must match the vocabulary you used during the training.
One of the best practices for productive development is to be able to test your deployment on your development machine before you deploy it to the cloud. I use a Windows laptop with Windows Subsystems for Linux (WSL2) as my development-test environment. The instructions should also apply to development environments like a local Linux machine, Azure Cloud Shell or a Virtual machine on the cloud like the Data Science Virtual Machine or Azure Machine Learning Compute Instances.
Setting up the environment and tools
You need to have the following tools installed on your development machine.
Create an Azure Function Project
First, you need to create a project for your Azure Functions locally, which is just a directory on your machine.
Next, you must initialize the Function App and specify the runtime. We use Python runtime and the Azure Functions whose execution is triggered through a HTTP request. This means that to get a prediction from your model you send a HTTP request from the client with the desired parameter (to be described later).
Create your inferencing code
We have a convenience inference code template that is published on GitHub that you can use as boiler plate and update to your needs. Here are steps to clone the code template and adapt it for your Azure Function App project to deploy your model.
The main source files are __init__.py and predictonnx.py in start/classify directory. In the Bear detector example, it takes input from the HTTP GET request in the “img” parameter which is a URL to an image which will be run through the model for prediction of the type of bear. You can adapt the same easily for deploying other models.
The predictonnx.py which does the actual prediction function, expects the model file and labels file in the current directory. In this example, we also need to do the pre-processing of the input image by normalizing it and scaling it to the desired size before it can be passed onto ONNX Runtime to run the inference operation. This file contains both the pre-processing code and the code to get the model prediction with ONNX Runtime.
Copy the model.onnx and labels.json files (created in the earlier step) to the directory.
Install the dependent Python libraries locally in a virtual environment.
Deploy Azure Functions App locally and test
Now you are ready to test your Azure Functions App locally. The Azure Functions Core Tools makes this super simple. Literally you just run one command from the “start” directory:
This will start an environment very similar to what would be in the cloud-based Azure Functions on your local machine. It listens on port 7071 and is ready for your request. For testing the Azure functions all you need is to visit the following URL in a browser or use tools like curl or invoke a Web request from your client application where you want to consume the model.
http://localhost:7071/api/classify?img=[[URL of the image to classify]]
Effectively you are pass an URL to an image that the Azure functions in the “img” parameter of the web request to receive predictions from the model on the type of bear with the above example model.
After you have tested with a few sample images and are satisfied that your model works fine, you are now ready to deploy it to the cloud where a client or application from anywhere is able to consume predictions from the model.
Pro Tip: If the only consumer for the model is an app running on your development machine this can be an end state.
We will use the Azure CLI to create an Azure Function App and a Storage account and put all these in a resource group for easy management.
Note: If you have not logged into Azure CLI. you must first run “az login” and follow instructions to log into to Azure with your credentials. In the example above, we are deploying the resources in westus2. You can choose another Azure data center/ region if that is more convenient for you. Here in this example, we set a flag to disable Application Insights on this Azure Functions App. Application Insights is a service Azure provides to help you monitor your Azure Functions and other Azure services. We recommend enabling Application Insights for production deployment and refer you to the documentation on Functions Monitoring for more information on its usage.
Finally, you run the command to publish your Azure Function App project into Azure.
After a few minutes, your model is deployed to the cloud. The last command also output the URL base include a key that can be used to make HTTP request and get predictions from the model. In case you missed the output, you can go back and fetch the URL by running the command “func azure functionapp list-functions [[YOUR Function App name] — show-keys”. For this Bear detector app, you can append “&img=[Your Image URL]” to the InvokeUrl from above command to invoke the Azure functions and receive predictions from the model.
You can visit the Azure portal and search for your Azure Function App.
Azure Functions provides additional deployment modes. For simplicity I used the local zip deployment which essentially packages up the local project directory (including the dependent python libraries) into a zip file which is then deployed to the Azure Functions App in the cloud. Azure Functions also supports container deployment on premium and dedicated hosting.
One challenge with the consumption plan is that the instance sizes are relatively small with a maximum of 1.5GB of main memory per instance. A native PyTorch model has a bigger footprint both from an App on-disk size and the working memory size perspective. The default runtimes in popular deep learning frameworks are more optimized for model development experience as opposed to serving. Microsoft developed the ONNX Runtime, a highly optimized, low memory footprint and open source execution engine for inferencing.
Using ONNX Runtime as the execution runtime in Azure Functions helps lower the footprint of hosting your PyTorch model and enables you to deploy models on the cheaper consumption plan hosting mode of Azure Functions. The total deployment package for our example was about 75MB (including the model file, Python library dependencies). In contrast, if you are using standard PyTorch runtime, the deployment package is almost 10X bigger for the same model since the PyTorch library and dependencies has to be bundled with your model. This often requires deploying to a larger instance type for hosting. In our experience in deploying numerous models within applications in Microsoft, the ONNX Runtime is on an average 2X faster enabling to serve models at low latency and high throughput. So, ONNX Runtime is a great option to deploy your PyTorch models in most scenarios especially in low cost / low resource environments such as the Azure Functions Consumption plan instances. Hosting models in Azure Functions with HTTP interface enables you to consume the same from cross platform clients.
It should be noted that there are other technologies you can use to deploy models on Azure. Many customers use Kubernetes clusters to run their applications and host their models. Azure offers a managed Azure Kubernetes Service (AKS) that can be used to host your models. Azure Machine Learning service provides out of the box support to deploy your models to AKS.
Some of the other considerations in deploying models into production include having a streamlined development and deployment processes. This is where end-to-end machine learning services like Azure Machine Learning addresses these challenges by effectively bridging the experimentation world of data scientists who are iterating on new models and the operational world of machine learning (also known as ML Ops) where the models are served in a production environment with the appropriate SLAs, ensuring model reproducibility, versioning, monitoring and feedback loops to improve models over time. We don’t cover these here but provide pointers in the “Learn More” section below.
We have seen how it is quite easy to deploy PyTorch models cost-effectively to the Azure serverless infrastructure and get the benefits of offloading operational concerns like scaling, security, monitoring and infrastructure management. The tooling provided by Azure Functions enables a good local development, debugging and deployment experience. ONNX Runtime enhances PyTorch with optimized inferencing and a fast execution engine in a small footprint, making your PyTorch model inferencing highly performant. We would to love to hear your experience with serverless deployment of your models and how we can improve our tools and processes further.
An open source machine learning framework that accelerates…
109 
109 claps
109 
Written by
PyTorch is an open source machine learning platform that provides a seamless path from research prototyping to production deployment.
An open source machine learning framework that accelerates the path from research prototyping to production deployment
Written by
PyTorch is an open source machine learning platform that provides a seamless path from research prototyping to production deployment.
An open source machine learning framework that accelerates the path from research prototyping to production deployment
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-difference-between-traffic-manager-and-front-door-service-in-azure-4bd112ed812f?source=search_post---------86,"There are currently no responses for this story.
Be the first to respond.
Comparison: Traffic Manager vs Front Door in Azure.
When choosing a global load balancer between Traffic Manager and Azure Front Door for global routing, you should consider what’s similar and what’s different about the two services. Both services provide
"
https://posts.specterops.io/death-from-above-lateral-movement-from-azure-to-on-prem-ad-d18cb3959d4d?source=search_post---------87,NA
https://medium.com/@renatogroffe/hospedando-um-website-est%C3%A1tico-de-forma-r%C3%A1pida-e-barata-no-azure-storage-da65913f26df?source=search_post---------88,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 5, 2019·5 min read
Figurando entre as principais plataformas de cloud computing da atualidade, o Microsoft Azure conta com diversos serviços para a hospedagem de aplicações Web. Estão disponíveis desde soluções voltadas a projetos de médio e pequeno porte, quanto alternativas recomendadas a cenários de uso mais intensivo e que englobem até mesmo milhões de acessos/usuários simultâneos.
Diante disso como poderíamos então realizar o deployment de um simples web site estático sem grandes complicações e com um custo reduzido?
Uma boa alternativa neste sentido é o suporte à hospedagem de sites estáticos oferecidos pelo Azure Blob Storage, um dos principais serviços de armazenamento que integram a plataforma de cloud computing da Microsoft.
Na imagem a seguir (extraída do site de precificação do Azure Storage no início de Março/2019) é possível ter uma dimensão dos valores envolvidos:
Ao longo deste artigo será demonstrada a hospedagem de um web site estático empregando o Azure Blob Storage, com isto acontecendo a partir do Visual Studio Code em um ambiente Linux (Ubuntu Desktop 18.04).
E aproveito este post para deixar aqui um convite.
Dia 06/03/2019 (quarta-feira) às 21:30 — horário de Brasília — teremos mais um evento online no Canal .NET. Desta vez farei uma apresentação sobre o presente e o futuro do .NET Core e do ASP.NET Core, cobrindo as versões 2.2 e 3 (esta última ainda em Preview). Além disso mostrarei novidades envolvendo o Visual Studio 2019 e o C# 8.0.
Para efetuar a sua inscrição acesse a página do evento no Meetup. A transmissão acontecerá via YouTube, em um link a ser divulgado em breve.
O primeiro passo para se proceder com a publicação será a criação de uma nova Storage Account no Portal do Azure:
É fundamental que o tipo da nova conta seja StorageV2 (general purpose v2), como indicado na próxima imagem. Por se tratar de um site e se esperar a ocorrência de acessos frequentes ao mesmo foi selecionada a opção Hot em Access tier:
Após a criação da conta de armazenamento (chamada de groffestatic neste exemplo) o próximo passo consiste na configuração deste recurso, de forma que o mesmo seja empregado na hospedagem de um site estático. Acessar para isto o item Static website:
Ativando então a opção Enabled e informando o arquivo correspondente ao índice/página inicial do site em Index document name:
Ao acionar o botão Save neste mesmo painel e, após alguns segundos, aparecerão 2 endpoints para acesso ao site estático:
Testes de acesso a esses 2 endereços mostrarão que nenhum conteúdo foi publicado até este momento:
Muito utilizado no desenvolvimento Web, o Visual Studio Code conta com diversas extensões que simplificam a utilização e gerenciamento de recursos gerados a partir do Microsoft Azure.
Na imagem a seguir observamos o conteúdo do web site a ser publicado na conta de armazenamento criada na seção anterior (basicamente uma página HTML, que exibirá uma imagem/banner):
OBSERVAÇÃO: Os procedimentos descritos nesta seção foram executados em uma máquina na qual foi instalado o Ubuntu Desktop 18.04.
Para o deployment será necessário instalar a extensão Azure Storage:
Acessando o ícone do Azure será exibido o item STORAGE, com contas de armazenamento vinculadas a uma assinatura/subscription desta plataforma. Navegando até Blob Containers na conta groffestatic aparecerá um container chamado $web (não confundir essa estrutura de armazenamento com containers Docker!); será justamente este elemento no qual será hospedado o site:
Clicar com o botão direito do mouse sobre $web, acionando na sequência a opção Deploy to Static Website…:
Selecionar agora o diretório contendo o website:
O deployment terá finalmente início, com uma mensagem sendo exibida ao final em caso de sucesso:
Acessando neste momento o container $web aparecerão então os arquivos index.html e mvpconf-banner.jpg vinculados ao mesmo:
A seguir temos o resultado no browser para o endpoint primário:
E também um teste com o endpoint secundário:
Deploy a static website to Azure
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
68 
2
68 claps
68 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mbellinaso/microsoft-azure-services-overview-and-notes-cfb30b95d8e?source=search_post---------89,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marco Bellinaso
May 25, 2018·10 min read
This page will grow and change over time. It’s not meant to be “documentation”, but rather a quick list of important notes and high-level concepts that I wrote down for myself, in pretty much random order. Feel free to suggest additions and point out mistakes in the comments below or directly highlighting the incriminated content.
Azure Services
Containers
Service Fabric supports .NET Framework apps, Core apps, Linux and Windows Docker containers.
AKS supports Core apps and Linux-based Docker containers. Win-based Docker container will be added. .NET Framework apps are not supported.
Even Azure AI models can be exported to containers that can run in AKS.
Announced at Build 2018 (and currently in private beta), Dev Spaces allows to deploy and *debug* AKS-hosted containers from Visual Studio.
Data Storage
Azure SQL DB, PostgreSQL and MySql for relational data.
Cosmos DB for unstructured data + small and large data
Azure Storage
Security
Other services
Messaging
Monitoring
Visual Studio and Tools
Who am I / what do I do? I proudly work as a Solutions Architect in the Mobile Team @ ASOS.com (iOS app | Android app), and we’re always looking for strong, friendly and talented developers that want to have an impact on how tens of millions of customers shop online. ASOS is the biggest online-only retailer in UK and, let’s be real, the best tech+fashion company in the world. Some of the technologies we use are Swift for iOS, Kotlin for Android, React and Node on the web frontend, .NET and Azure on the backend. If that sounds interesting to you, and you happen to live in beautiful London (or are willing to move here…after all it’s the best city in Europe except for some in Italy!), do get in touch with me!
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
164 
2
164 
164 
2
Software Architect @ASOS.com (and iOS / full-stack dev for fun)
"
https://medium.com/@wtfmitchel/is-azure-stagnating-c91894e67292?source=search_post---------90,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mitchel Lewis
Jul 31, 2021·7 min read
It’s not a secret that Microsoft’s future depends on Azure not only being successful, but dominant. With Google Workspace dethroning Office 365 in the cloud productivity markets, Windows needing a complete re-write since a decade ago and their consequent monopoly on exploits and ransomware attacks in the PC and Server markets, much is riding on Azure’s ability to dominate the cloud infrastructure space as Microsoft has done with the OS, productivity, and server spaces before.
One consequence of Microsoft’s dependence on Azure is that Microsoft can post its best quarter ever and investors will get spooked if Azure’s revenue growth slips in the slightest. It also doesn’t help when their CFO Amy Hood admitted that Azure slowing revenue growth still performed better than she anticipated.
“Forty-five percent was both better than we expected and driven by consumption growth, which is very good,” Hood said in an interview. “Demand is healthy. The overall execution was better than I expected.” -Amy Hood
Unlike before though, Microsoft isn’t starting at the top of an emerging market as it did in the OS, Productivity, and Server spaces. Instead, Microsoft was 2 years late to the market and has to compete with the likes of AWS instead and claw market-share away from them. And this is bad news for Microsoft as competing with other tech monopolies in established markets is not something that they’re especially good at; they just aren’t the same company that mothballed IBM all those years ago.
The success of Microsoft’s business model relies mostly on them being among the first movers of infant markets, becoming the industry standard, and entrenching its products throughout said industry; lock-in if you will. In turn, their products no longer need to compete on quality, cease to evolve, and stagnate no differently than the human race as they have no ecological competition. Apparently, the law of natural selection even applies to markets just as it applies to us.
In doing this, Microsoft’s frustrating, insecure, and unstable architecture renders users change and technology averse, traumatized if you will, and consequently vying to keep everything the same. Further, they can artificially inflate the switching costs of moving to their competition, derail migration efforts to their competition even if it’s better technology, and maintain dominance. Put simply, Microsoft’s products and services create a moat of sorts that keeps users in and competition out while allowing them to compete with themselves. Mitigating their defenses is much easier said than done.
Being a first-mover that optimizes their solutions for lock-in is a double whammy for Microsoft and no one seems to care; hence why they do it. This happens to be why Windows, Active Directory, Server, and Exchange are still in play today despite being legacy, expensive, complex, frustrating, and unstable for users and admins alike. It’s simply too ingrained and users/admins are rendered apathetic to change.
While Microsoft can’t exactly take credit for this brilliant aspect of their business model, they can absolutely laugh all the way to the bank at anyone who is criticizing them about their quality woes without realizing that they don’t even have to compete on quality; at least until Azure became their last hope.
One immediate problem with their tactics though is that they don’t bode well in markets that are already well-established. Nor is it easy to re-structure a company to engineer for quality when it’s structured to maximize lock-in. Although absolute genius goes into engineering products for lock-in, especially when realizing that all of their engineers are trying to do their best/ethical job, this heroin-esque approach to engineering is systemic and cannot be turned off like a light switch; quite the contrary. Any manager at Microsoft can and will affirm that Microsoft is a big ship to steer and such a restructuring could take years to fully implement.
As such and much like their founder Bill Gates, Microsoft isn’t built for fair competition, hence why they lose their ass in markets that they’re late to. And as they have shown repeatedly with cloud, mobile, social, gaming, and laptop markets, Microsoft is consistently a fish out of water when entering saturated markets because they are not optimized to compete on quality which is the only card that a new entrant has to play against the status quo; exhibit Zoom, Slack, Twitch. All of which stacks the deck further against Microsoft’s ambitions with Azure given out competitive the cloud infrastructure market is.
To highlight this and although Microsoft is doing great things in the cloud space with Office 365, they were late to the market, ironically among the last to host their own services, and are in second place while losing further ground to Google Workspace. The same is true of Azure in that it was 2 years behind AWS to the cloud infrastructure market.
And although Microsoft and analysts claim Azure to be second in the cloud infrastructure space from a revenue perspective, Microsoft has yet to corroborate this with data and is refusing to post individual performance metrics of Azure after a decade of production. Based on what little we’ve seen though, AWS revenue is growing while Azure revenue growth is shrinking which is the opposite of what Azure needs to do. Meanwhile, AWS revenue grew 9% in the last year.
No matter where you look, you can find Microsoft consistently omitting all key performance indicators (KPIs) worthy of mention concerning Azure financials or usage; MAU, P&L, CPA, ARPA, RPE, etc; nada. Meanwhile, you’ll find a whole host of ambiguous metrics such as vague growth rates, total user counts instead of monthly use statistics, and containers like the Intelligent Cloud averaging various offerings together. All of which takes significantly more effort than simply reporting individual performance and is frankly hard to keep under wraps for 12 years. Meanwhile, AWS has no problem reporting on AWS’s performance; they have nothing to hide.
Oddly enough though and while it’s even their policy to never report on KPIs, they definitely track them and occasionally post them but only if they exude a dominant market presence. In doing this though, Microsoft has a tell so to speak. Put simply, when products are doing fantastically, Microsoft will break protocol from time to time and report KPIs. But when products are doing horribly, Microsoft seems to hide behind their bogus policy so as to keep KPIs under lock and key while sugar-coating poor performance with ambiguities instead.
In doing this though, this being not reporting common usage and financial metrics while further hiding individual performance in the Intelligent Cloud, Microsoft has made it impossible for analysts to evaluate where Microsoft stands in the fold compared to AWS or Google Cloud. Ironically, the assessments declaring Azure to be in second place among cloud providers are speculative at best.
“Muddy waters make it easy to catch fish.” — Chinese Proverb
With all of this in mind, it’s easy to see why Microsoft needs investors to believe that Azure’s position is strong and why Microsoft is working so hard to keep Azure’s performance under wraps; that dog don’t hunt. Although I can only speculate, it seems as if the KPIs surrounding Azure do not exhibit dominance or a route to dominance that Microsoft needs to project in order for share prices to keep rising while its stagnant revenue growth serves as further evidence of this.
If said KPIs did exhibit Azure’s dominance or even a route to dominance, then Microsoft would have no reason to be shy and release them in the face of increasing scrutiny of their persistent refusal to report on these metrics. And their refusal to post these metrics while muddying the waters with pointless statistics/rates and odd financial containers instead isn’t exactly a good omen so far as the health of Azure is concerned; if not symptomatic of the contrary. Put simply, if Azure truly had a big ol’ dong then Microsoft would have thrown it on the table by now rather than hiding it behind excuses and obscure metrics for over a decade.
To be fair though, Microsoft could indeed be shy about Azure’s performance for the past 12 years. Azure could be doing swimmingly for all I know and I’m willing to accept that reality if it bears truth. What I do know is that omission is the most common form of lying with statistics, followed by obfuscating matters with ambiguous metrics, and Microsoft just doesn’t have an incentive to resort to these squid and ink tactics if Azure is in great shape.
All stars go through an inflationary phase before they go supernova and I’m forced to wonder if this is true of Microsoft given their questionable reporting. Obviously, you’re welcome to believe otherwise though. It’s not like Microsoft has provided us with enough information to do anything else but speculate.
Further Reading:
hackernoon.com
Engineer, Farmer, and Hellion
175 
8
175 
175 
8
Engineer, Farmer, and Hellion
"
https://medium.com/@moments-with-bren/free-courses-on-devop-aws-azure-gcp-linux-windows-python-golang-scripting-helm-database-b146f2ca147?source=search_post---------91,"Sign in
There are currently no responses for this story.
Be the first to respond.
Momentswithbren
Dec 22, 2019·3 min read
Free Courses on DevOp, AWS, Azure, GCP, Linux, Windows, Python, Golang, Scripting, Helm, Database, ML.
START LEARNING!!
DevOps - FREE Udemy Courseshttps://www.udemy.com/course/devops-crash-course-cicd-with-jenkins-pipelines-groovy-dsl/
https://www.udemy.com/course/learn-devops-with-jenkins-all-in-one-guide/
https://www.udemy.com/course/jenkins-quick-start/
https://www.udemy.com/course/jenkins-intro/
https://www.udemy.com/course/ansible-essentials-simplicity-in-automation/
https://www.udemy.com/course/devops-series-server-automation-using-ansible/
https://www.udemy.com/course/devops-beginners-guide-to-automation-with-ansible/
https://www.udemy.com/course/learn-devops-kubernetes-deployment-by-kops-and-terraform/https://www.udemy.com/course/containers-101/
https://www.udemy.com/course/kubernetes-for-developers/
https://www.udemy.com/course/jenkins-beginner-tutorial-step-by-step/
https://www.udemy.com/course/deploying-containerized-applications-technical-overview/
https://www.udemy.com/course/docker-on-windows/
https://www.udemy.com/course/learn-devops/
https://www.udemy.com/course/docker-essentials/
https://www.udemy.com/course/kubernetes-by-example/
https://www.udemy.com/course/atlassian-jira-for-beginners/
https://www.udemy.com/course/ci-cd-pinepline-devops-automation-in-1-hr/
https://www.udemy.com/course/complete-junit-5-course-for-beginners/
https://www.udemy.com/course/master-virtualbox-in-one-day/
https://www.udemy.com/course/linux-academy-devops-essentials/
https://www.udemy.com/course/git-started-with-github/
https://www.udemy.com/course/kubernetes-getting-started/
https://www.udemy.com/course/learn-git-bash/
AWS - FREE Udemy Courseshttps://www.udemy.com/course/developing-cloud-native-applications-microservices-architectures/
https://www.udemy.com/course/namrata-h-shah-aws-tutorials-dynamodb-and-database-migration-service/
https://www.udemy.com/course/aws-developer-associate-training/
https://www.udemy.com/course/aws-certified-solutions-architect-associate-in-30-days/
https://www.udemy.com/course/linux-academy-aws-essentials-2019/
https://www.udemy.com/course/learn-amazon-web-services-the-complete-introduction/
https://www.udemy.com/course/amazon-web-services-aws-v/
https://www.udemy.com/course/amazon-web-services-aws-cloudformation/
https://www.udemy.com/course/aws-developer-associate-training/
https://www.udemy.com/course/aws-certified-solutions-architect-associate-in-30-days/
https://www.udemy.com/course/learn-amazon-web-services-the-complete-introduction/
https://www.udemy.com/course/amazon-web-services-aws-v/
https://www.udemy.com/course/amazon-web-services-aws-cloudformation/
https://www.udemy.com/course/aws-appstream-20-introduction/
https://www.udemy.com/course/ec2with10labs/
https://www.udemy.com/course/launch-a-lamp-stack-and-install-wordpress-on-aws/
https://www.udemy.com/course/aws-certified-solutions-architect-associate-in-30-days/
https://www.udemy.com/course/aws-certified-security-specialty-exam-introduction/
https://www.udemy.com/course/amazon-web-services-aws/
https://www.udemy.com/course/mastering-aws-featuring-iam/
https://www.udemy.com/course/learn-amazon-web-services-aws-easily-to-become-architect/
https://www.udemy.com/course/commvault/
https://www.udemy.com/course/aws-concepts/
https://www.udemy.com/course/complete-aws-course-learn-hands-on-practically/
https://www.udemy.com/course/terraform-fast-track/
https://www.udemy.com/course/amazon-web-services-monitoring-and-analysis/
Azure - FREE Udemy Courses
https://www.udemy.com/course/maruti-microsoft-azure-step-by-step-part-1/
https://www.udemy.com/course/azure-storage-security-guide/
https://www.udemy.com/course/learn-fundamentals-of-cloud-thru-microsoft-azure/
https://www.udemy.com/course/az-900-basics-of-cloud-computing/https://www.udemy.com/course/jumpstart-docker-platform-with-microsoft-azure/
https://www.udemy.com/course/introduction-to-cloud-computing/
GCP - FREE Udemy Courseshttps://www.udemy.com/course/google-cloud-platform-overview-for-aws-professionals/ Linux - FREE Udemy Courseshttps://www.udemy.com/course/linux-command-line-for-beginners-42/
https://www.udemy.com/course/vi-editor/
https://www.udemy.com/course/linux-basics-for-beginners/
https://www.udemy.com/course/command-line/
https://www.udemy.com/course/ubuntu-linux-on-virtualbox-quick-setup/
https://www.udemy.com/course/command-line-arguments-for-beginners/
Windows - FREE Udemy Courseshttps://www.udemy.com/course/iis-web-server-free-course/
Python - FREE Udemy Courseshttps://www.udemy.com/course/python-for-every1/
https://www.udemy.com/course/complete-python-masterclass-learn-from-scratch/
https://www.udemy.com/course/complete-python-course-go-from-zero-to-hero/https://www.udemy.com/course/complete-python-course-zero-to-hero/
https://www.udemy.com/course/complete-python-course-learn-from-scratch/
https://www.udemy.com/course/python-regular-expressions-with-examples/
https://www.udemy.com/course/python-for-machine-learning-t/
https://www.udemy.com/course/complete-python-course-go-from-beginner-to-advanced-x/
https://www.udemy.com/course/python-for-every1/
https://www.udemy.com/course/complete-python-masterclass-go-from-beginner-to-advanced/
https://www.udemy.com/course/the-ultimate-python-course-learn-from-scratch-r/
https://www.udemy.com/course/complete-python-course-zero-to-hero-x/
https://www.udemy.com/course/complete-python-course-learn-from-scratch-k/
https://www.udemy.com/course/complete-python-course-go-from-zero-to-hero-z/
https://www.udemy.com/course/ultimate-python-course-go-from-zero-to-hero/
https://www.udemy.com/course/the-ultimate-python-course-learn-from-scratch-i/
https://www.udemy.com/course/learn-python-from-scratch-w/
https://www.udemy.com/course/complete-python-masterclass-learn-from-scratch-b/
https://www.udemy.com/course/pythonbasics/
https://www.udemy.com/course/python-fundamental-basics/
https://www.udemy.com/course/learn-python-from-scratch-m/
https://www.udemy.com/course/lets-learn-python-s/
https://www.udemy.com/course/absolute-python-basics-for-anyone/
https://www.udemy.com/course/python-programming-basic/
https://www.udemy.com/course/the-complete-python-course-beginner-to-advance/
https://www.udemy.com/course/mongodb-and-python-quickstart-with-mongoengine/
https://www.udemy.com/course/comprehensive-python-course/
https://www.udemy.com/course/complete-python-course-learn-from-scratch-c/
https://www.udemy.com/course/complete-python-course-beginner-to-advance/
https://www.udemy.com/course/technobytes-python-for-beginners/
https://www.udemy.com/course/learn-python-from-zero/
https://www.udemy.com/course/pythonforbeginnersintro/
https://www.udemy.com/course/ready-for-python-within-an-hour/
https://www.udemy.com/course/exception-handling-in-python-3-try-except-else-finally-raise/
https://www.udemy.com/course/advanced-python-memory-management/
https://www.udemy.com/course/python-3-crash-course/
https://www.udemy.com/course/complete-python-course-zero-to-hero-e/
https://www.udemy.com/course/complete-python-course-learn-hands-on-practically-i/
https://www.udemy.com/course/complete-python-course-learn-hands-on-practically/
https://www.udemy.com/course/learn-python-build-a-virtual-assistant-in-python/
https://www.udemy.com/course/getting-started-with-python/
https://www.udemy.com/course/python-for-absolute-beginners-u/
https://www.udemy.com/course/pythonforbeginnersintro/
https://www.udemy.com/course/learn-and-practice-python-programming-python-from-scratch/
https://www.udemy.com/course/python-for-complete-beginners-u/
Golang - FREE Udemy Courseshttps://www.udemy.com/course/getgoing/
Scripting - FREE Udemy Courses
https://www.udemy.com/course/practical-bash-scripting/
https://www.udemy.com/course/groovy-step-by-step-for-beginners/
https://www.udemy.com/course/introduction-to-linux-shell-scripting/
https://www.udemy.com/course/powering-you-with-powershell/
https://www.udemy.com/course/bash-shell-scripting/
https://www.udemy.com/course/powershell-command/
https://www.udemy.com/course/learn-powershell/
Helm - FREE Udemy Courseshttps://www.udemy.com/course/helm-best-practices-2019/ Database - FREE Udemy Courses
https://www.udemy.com/course/oracle-database-lab-setup-at-home/
https://www.udemy.com/course/sql-crash-course-postgresql-for-beginners/
https://www.udemy.com/course/complete-machine-learning-masterclass-learn-from-scratch-c/
https://www.udemy.com/course/complete-machine-learning-course-learn-from-scratch/
https://www.udemy.com/course/python-for-machine-learning-t/
https://www.udemy.com/course/machine-learning-masterclass-a-z-beginner-to-advance/
https://www.udemy.com/course/making-computers-think/
Service Design | Power Skills Training | Positive Psychology | Education, Employment and Economic Advancement | Scholarships | Knowledge Mobilization
164 
164 
164 
Service Design | Power Skills Training | Positive Psychology | Education, Employment and Economic Advancement | Scholarships | Knowledge Mobilization
"
https://medium.com/@renatogroffe/publicando-um-web-site-est%C3%A1tico-na-nuvem-com-docker-nginx-e-azure-container-instances-d688823bbe1b?source=search_post---------92,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jan 25, 2019·5 min read
A utilização de Docker em aplicações Web costuma ter como ponto de partida imagens pré-existentes, nas quais já foram devidamente configurados um runtime para a execução de projetos em plataformas como .NET, Java ou Node, por exemplo.
E se precisarmos fazer o mesmo com um site estático, cujo conteúdo normalmente envolve apenas arquivos HTML, imagens, JavaScript e CSS? Empregar uma imagem contendo um SDK não seria a melhor opção neste caso, já que esta última traria uma série de componentes desnecessários para a execução do site em questão.
Uma ótima alternativa para este cenário seria o uso do Nginx a partir de um container. O Nginx é um web server open source bastante popular no que se refere à implementação de mecanismos de load balancing, proxy reverso, compressão de dados e segurança. Outro ponto que pesa a seu favor está no fato de que conta com imagens Docker relativamente pequenas, sobretudo se levarmos em conta sua distribuição baseada em Alpine Linux (na casa dos 17 MB).
E quanto à publicação de um site estático na nuvem a partir de um container Docker, este último baseado em uma imagem do Nginx? Uma solução relativamente barata e de fácil utilização seria o serviço Container Instances que integra o Microsoft Azure.
Neste novo artigo demonstrarei a geração de uma imagem contendo um web site estático e empregando o Nginx, com a sua posterior publicação no Azure Container Instances.
E aproveito este espaço para deixar aqui ainda um convite.
Dia 08/02/2019 (sexta-feira) às 21h30 — horário de Brasília — teremos mais um evento online gratuito no canal Coding Night. Desta vez acontecerá um bate-papo online descontraído (e com polêmicas) entre Desenvolvedores Front e Back-end sobre a implementação de soluções, frameworks JavaScript, performance e outros temas relacionados ao desenvolvimento Web.
Para efetuar a sua inscrição acesse a página do evento no Meetup. A transmissão acontecerá via YouTube, em um link a ser divulgado em breve.
O site utilizado como exemplo neste artigo será formado por um arquivo index.html, além de conter uma imagem .png de divulgação de um evento. Na imagem a seguir é possível observar a estrutura de arquivos (com os arquivos Dockerfile e docker-compose.yml) e diretórios (o conteúdo correspondente ao site se encontra na pasta site-meetup-59-dotnet-sp):
O arquivo Dockerfile referencia a imagem Alpine do Nginx, além de constar no mesmo uma instrução que copiará o conteúdo da pasta site-meetup-59-dotnet-sp para o diretório /usr/share/nginx/html da imagem correspondente ao site estático:
Na arquivo docker-compose.yml estão as configurações para a geração da imagem meetup-59-dotnet-sp-nginx, bem como a criação de um container para testes baseado na mesma e que estará acessível através da porta 30000:
A execução do comando docker-compose up -d no diretório em que se encontram os arquivos docker-compose.yml e Dockerfile terá como resultados:
Na próxima imagem estão os resultados indicando a imagem gerada (meetup-59-dotnet-sp-nginx com 17.9 MB), assim com o container gerado a partir da mesma:
Um teste de acesso via browser ao endereço http://localhost:30000 mostrará que o site hospedado no container está funcionando corretamente:
O comando docker login permitirá a autenticação junto a uma conta/repositório público no Docker Hub, exigindo para isto que sejam informados um usuário pré-existente e sua respectiva senha.
O deployment de imagem pública do site estático aqui apresentado envolverá:
Consultando o Docker Hub após este procedimento já aparecerá a imagem meetup-59-dotnet-sp-nginx:
No portal do Azure será criado um novo recurso baseado no serviço Container Instances:
Preencher no formulário de criação:
Na próxima tela indicar:
Em Summary acionar o botão OK, confirmando a criação de um novo container:
Na próxima imagem é possível observar o container já criado. O acesso ao mesmo se dará tanto via IP (indicado em IP address), quanto via endereço especificado em FQDN (Fully Qualified Domain Name):
E nas imagens a seguir é possível observar o site hospedado em execução (acessado via IP, assim como através do endereço associado ao recurso do Azure Containers Instances):
Azure Container Instances Documentation
Docker para Desenvolvedores .NET - Guia de Referência
Nginx - Docker Hub
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
79 
79 
79 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/daily-programming-tips/how-to-deploy-a-local-ml-model-as-a-web-service-on-azure-machine-learning-studio-5eb788a2884c?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@thisiszone/azure-durable-functions-before-and-after-b7266d51ed4d?source=search_post---------94,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zone
Jun 11, 2018·10 min read
Zone’s head of .NET development, Andy Butland, looks at the use of Azure Functions and Durable Functions to tackle common concerns…
I recently had the pleasure of attending and speaking at the Intelligent Cloud Conference in Copenhagen, where I talked on the subject of Azure Functions.
I went through some of introductory topics such as what the platform is and why we might consider using it, and then ran through a number of examples — illustrating solving real-world problems with Azure Functions via a combination of diagrams, demos and code. The latter half of the presentation was focused on a comparison between two implementations, for two patterns, using “classic” (my name — it’s too early for a “classic” designation I think!) Azure Functions and a newly released extension to the platform named Durable Functions.
I’m going to spin that discussion off into the following blog post — comparing the implementation of two long-running tasks, using chaining and fan out/in techniques. Some important aspects of the code will be shown within the article, but to view the full implementation of each pattern, please see the Github repository.
As a brief introduction though, Durable Functions allow us to abstract away intermediate queues and tables that we might otherwise need to support workflows using the standard platform. Rather than going via these storage components, functions can call other functions directly, passing input and retrieving output — which sounds obvious given the name “functions” of course, but it’s much more clearly apparent when using the durable functions approach. As we’ll see, we can’t completely ignore the underlying storage components that support this, but nonetheless, many scenarios can be implemented with less “plumbing” required.
We might look to chain together multiple Azure Functions for a couple of reasons. The first is that function execution time is limited — 5 minutes by default, extendable via configuration to 10 minutes. That’s still quite a long time and plenty for most processes but we could certainly have examples of a workflow that takes longer than that, and therefore couldn’t be completed within a single function invocation.
Even if that’s not the case, we would likely want to adhere to the single-responsibility principle (SRP) within our function applications — for the same reasons of clean code and ease of understanding and maintenance as we would in any software development. So we would likely want to break up more complex workflows into multiple functions, each responsible for a single, smaller piece.
To do that though, we need to find a way to have the output of one function be passed to the next one in the chain. Which we can do via an intermediate storage component, most typically, a queue.
The example I looked at was a (fictitious) e-commerce workflow, where we want to carry out a number of steps following the placement of an order by a customer. We need to write to a database, make a call to an external web-service to manage inventory updates and to send a confirmation email to the customer. In order to provide a responsive experience to the user, we don’t want to wait to complete all the necessary steps in the workflow before sending a response back to them, so instead we take the order details, save them to a queue message and continue the workflow as a background process.
The first step of our implementation using the standard Azure Functions platform is to create a queue triggered function to respond to this queue message. This function has one primary job — to update the database. Its secondary job is to create a new message, on a new queue, which in turn triggers the second function to carry out its job of calling the external web service. And so on, to the final function that sends the email to the customer.
We are able to exhibit some control and customisation implementing logic to simply not pass the message onto the next queue if part of the process fails and we want to cut short the full workflow.
The following diagram illustrates the components involved:
The implementation of the first function is as follows:
Key points to note include the incoming item parameter, which is bound via the QueueTrigger attribute and thus contains the content of our initial message. We also have a second parameter named outputQueue, which is of a collection type. Once the task of updating the database is complete, we manipulate that collection in code by adding a typed item to it — and because of the Queue attribute applied to the parameter, this change is reflected in the storage component itself as a new message in the queue.
This is one of the nice features of Azure Functions I find, these attributes provide a lot of power when it comes to integrations, meaning we can avoid dropping down into the lower-level APIs of the storage component or service libraries themselves and focus more on implementing the logic of our business process.
Taking the same scenario with Durable Functions, we can immediately see via a compare and contrast of the two component diagrams that we have on the face of it a much simpler solution:
On the left, we have our input as before, from queue storage. And on the right, our external systems and services. In the middle though, just functions calling functions, and no (apparent) intermediate storage components.
The first function is now implemented as follows:
We have our queue trigger as before, but this time a new parameter called starter that’s of a type DurableOrchestrationClient. Via it, we can call our main workflow function — known as an orchestration function within the framework — passing in the name of the function to call, and the initial input (the contents of our queue message).
The orchestration function looks like this:
As input we receive a parameter of type DurableOrchestrationContext which allows us to do various things like retrieve the input to the function and then make calls to other functions — called activity functions — to carry out their dedicated tasks. We can pass information to those functions, as well as receive their returned output back — and if need be, respond appropriately with standard C# control of flow code to amend the workflow. For example here, if the call to update the inventory fails, we opt not to send the confirmation email to the customer.
Finally an example of an activity function, which has a parameter of type DurableActivityContext, again allowing us to retrieve the function input (which, as is shown here, can be typed rather than simply a string):
Another pattern used for handling larger workloads is known as fan-in/out, or sharding, where we take one large input file and break it up into smaller ones that can be executed in parallel. When all are finished, we aggregate these intermediate results in order to produce an overall output.
The example I chose to illustrate this was to obtain from Kaggle a large data-set of summer Olympic data from 1896 to the present day, containing every single event and medal won. I then wanted to analyse that to determine the overall ranking of countries in terms of medal success in Olympic history.
The following diagram illustrates the components and functions that were involved in the implementation using the standard Azure Functions platform:
We start with the large dataset (a CSV file) uploaded to a blob storage container. A function, configured with a trigger for that container is then fired. It’s going to be responsible for the “fan-out” operation, and will break the file up into smaller files, one for each year, and write them to a second blob storage container.
First though, we need somewhere to store our intermediate results for each of the fanned out operations. We use table storage for that, so populate a table with one record for each year, with an empty field reserved for population when each operation on a given year completes.
A second function set up to be triggered on the second blob storage container will be fired. At this point the scaling feature of the Azure Functions platform will kick-in, and, as we have multiple files in this container, we’ll get a number of function instances triggered and run in parallel. As each completes it does two things. Firstly, it updates the record in table storage for the given year. And secondly, it writes a message to a queue — just a simple flag indicating “I’m done”, that will be used to trigger the last step in the process.
The final function, responsible for the “fan-in” operation, triggers from these queue messages. We have an issue here though, in that we can only calculate our final result once all years have been processed. So the first thing this function does is check in table storage and query to see if data for all years has been completed. If not, the function simply exits. When the final message on the queue is processed, we’ll have data for all years, so can continue, aggregate the results and produce an output, which is written as a CSV file to a final blob storage container.
Code-wise, the first function looks like this:
Perhaps the most interesting thing to note here is the use of the parameter of type Binder. We need this because we are going to create multiple files in blob storage, and we don’t know for sure at compile time what the amount and names of the files will be (there’ll be one for each year). So we can’t use an attribute as before. Well, in fact we can, but it’s one we have to create dynamically at run-time which can be seen in the WriteFilePerYear method. Here we dynamically create an attribute on an instance of a CloudBlockBlob object, and having done that, the changes we make to it in terms of setting properties and uploading bytes, are reflected in the blob storage container. Again, we avoid digging down into the lower-level storage API directly (though of course, if we ever need to, we can ).
The second function processes the data for each year and writes the results to table storage:
And the final function carries out the fan-in operation as described above:
We can turn to Durable Functions to implement the same pattern and, as we’ll see, we get a couple of key benefits. Here’s the updated component diagram:
Once again, note the absence of intermediate storage components; we’ve been able to do away with explicit use of message queues and table storage for tracking the intermediate results. We have our input and output blob storage containers, but the workflow itself is just functions calling other functions, passing input and receiving and acting on their return values.
The other valuable benefit is that our fan-in operation — which was tricky to implement previously — is much more straightforward, and efficient. We are no longer having to make unnecessary function invocations where we check to see if all intermediate operations are complete, and if not, exit. Each of those would have an, albeit very small, cost associated, which we can now avoid.
Our initial trigger function is now responsible for taking the initial input and transforming it into a typed structure to pass to the orchestration function:
There’s one little aside to note here, which is that although intermediate storage components are abstracted away, they still exist. Queues are used to pass data between functions, which have a 64KB limit, equating to 32KB of UTF-16 string data. If the serialised function input or output goes over this limit, currently the process will fail, often in quite odd and tricky to diagnose ways. As such currently I’m taking care to serialise and GZip compress the inputs and outputs, pass them as a string, and decompress and deserialise on the other side.
This is something the functions team are aware of — nicely, much of the development is out in the open on Github and can be followed via issues and commits — and hopefully in future this compression or use of an alternative storage mechanism will become something the platform takes care of and is also abstracted away from the developer. But for now, it’s something to keep an in mind.
Here’s the orchestration function:
The orchestration function, having received input, then creates multiple Tasks, each hooked up with an activity function instance being passed it’s appropriate input for a given year. Once all primed, they are triggered in parallel and the code waits for all to complete. When they all have, the final activity function is called to process the output. Note in particular here how we’ve replaced all the calls to table storage for handing intermediate output with the single await Task.WhenAll(parallelTasks) line.
The two activity functions looks like this:
Over the last few months we’ve been making use of Azure Functions and other server-less offerings for implementing a number of supporting, auxiliary application components for the web applications we are building. With a few provisos — not unexpected in a new and evolving platform — we’ve been able to make good use of them, partly replacing the more traditional console apps or Windows services we might have previously employed for similar tasks. The introduction of the durable functions extension just makes that all a bit more straightforward, particularly when it comes to the more complex workflows.
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
115 
115 
115 
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
"
https://medium.com/hackernoon/kubernetes-adventures-on-azure-part-1-e0f68b486679?source=search_post---------95,"There are currently no responses for this story.
Be the first to respond.
This is the first article of a series of 3:
In the last month I read 3 awesome books around Kubernetes:
Now it’s time to start adventuring in the magical world of Kubernetes for real! And I will do it using Microsoft Azure.
Microsoft Azure offers a ready to go Kubernetes solution: Azure Container Service (ACS). It seems easiest way to test a Kubernetes cluster on Azure, if we don’t consider the new Azure Container Instance. It hides Kubernetes behind the scenes leaving you with simple deployments of containers that will be charged by cou, by memory and moreover by seconds!
Let’s try ACS! But first I want to highlight its current limits immediately so that you are aware of them:
Easiest way to start our ACS journey is following “Deploy Kubernetes cluster for Linux containers” that shows a beautiful 4 min to read on top of the page.
Note: It will guide you in using Azure Cloud Shell to create a Kubernetes cluster with Linux only nodes. Personally I installed an used a local Azure CLI following this article from Microsoft. Another article: “Deploy Kubernetes cluster for Windows containers” will show how to create a Kubernetes cluster with Windows only nodes. This missing hybrid deployment is a limitation for me, because I want to use an hybrid cluster with worker roles with Linux and Windows. But I know for sure that this limitation can be overcome using ACS Engine directly to manually deploy a Kubernetes cluster on Azure (another chapter in my adventure).
Main Steps to install a Linux ACS Kubernets cluster are:
After few minutes your cluster should be up and running with 1 master and 2 nodes, but I had no luck with it at first try.
Failure on step 2 (solved with a second try): on first try of step 2 I received an error, that disappeared on second run of the command, probably due to newly created app credentials in AAD not yet ready to be used. Here detailed error:
Note on step 3 (solved deleting and creating cluster again in another way): this step failed with “Authentication failed” error. Maybe due to the fact that there was already an id_rsa file under my user .ssh folder?
Fast solution is deleting the cluster with following command:
and create it again, but this time we will first create an SSH key pair on our own.
From Linux/MacOS you can follow: How to create and use an SSH public and private key pair for Linux VMs in Azure to create an SSH Key pair to be stored on your machine. This is really important and needed to connect to your Kubernetes cluster.
To create SSH key pair run following command and be sure to specify a path to store your key, mine is ~/acs/sshkeys/acsivan:
Note: I changed group and cluster name to avoid conflict with pending deletion of previous groups, that has been perform asynchronously using — no-wait argument.
Let’s try again to create our Kubernetes cluster with following commands (replace ssh key pair path with your one):
If there are no errors in console you are ready to connect to your first Kubernetes cluster on Azure!!! Hurray!
Let’s run our first kubectl command to check nodes of our cluster:
Wait… v1.6.6? Latest Kubernets version on 24th August 2017 is 1.7.4. This is another limit of Azure ACS: it’s not updated on the fly to latest versions.
First of all we will deploy Azure Vote app as described in Microsoft article we are following and then we will run some commands on our cluster to play with it a bit before moving to a Windows cluster.
Now let’s play a bit on it to test some kubectl commands:
Again a super easy command will lead you to a Dashboard showing your cluster from your browser (I love Kubernetes!)
The best way to reach it is kubectl proxy that should give you an output like Starting to serve on 127.0.0.1:8001
Open a browser to http://127.0.0.1:8001/ui and you will see dashboard running.
Now we can easily delete (and stop paying) everything with this simple command: az group delete --name myAcsTest2 --yes --no-wait
I tested Azure Container Services with a Windows cluster before moving to a full hybrid cluster. You can find details in Part 2.
I will then try to scatter cluster across multiple cloud providers and onpremises location (dreaming…)
#BlackLivesMatter
195 
2
195 claps
195 
2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/hackernoon/all-your-containers-are-belong-to-us-deploying-to-microsoft-azure-2e9aa464a113?source=search_post---------96,"There are currently no responses for this story.
Be the first to respond.
Azure Container Instances enables deployment of Docker containers onto Azure infrastructure without provisioning any virtual machines or adopting a higher-level service.
Follow me on Twitter, happy to take your suggestions on topics or improvements /Chris
The year was 1989. The game Zero Ving had just been released in Japan. Little did the creators of the game, Toaplan, know that their game would be legendary in 2019 for the screen you as a user was faced with when losing the game. All your base are belong to us — that is, you’ve lost. That’s of course not the case with containers, you are very much winning using containers and even more so when you bring them to the Cloud.
It becomes more and more common today to develop as well as deliver your application in one or more containers. One of the most common containerization software’s out there is Docker. It’s a great tool making it very easy to create image as well as containers and also monitor the same. Wouldn’t it be great if we could continue using Docker and bring our app to the cloud
In this article we will do the following:
In case you missed the links we are mentioning in this article. Here they are:
Using container technology allows us to split up our application into many services. On top of that, it offers a secure and reliable option to deliver the application. Now comes the next question, where do we deliver it to, On-premise or maybe to the Cloud?
This article is about delivering your application to the Cloud so of course, we are a little bit biased. Let me explain in a few short points why we think the Cloud is a great place for your app:
You will need the following installed
We said initially we would focus more on how to deploy rather than write an application, so for that reason, we are going to use a pre-made application that you can pull down from here:
git clone https://github.com/Azure-Samples/aci-helloworld.git
Looking at it you can see that it is a very simple Node.js application running Express. There are two files of interest in the Repository for the sake of our demonstration:
Let's have a look at the app/index.js file:
Above we can see that it’s pretty standard Node.js + Express application, no magic here.
Lets now have a look at the Dockerfile:
It does the following:
All in all, this is a pretty standard looking Dockerfile.
Building an image is pre-step we need to do before our application can actually be started. The build step will pull in the OS image we ask for, download dependent library, copy our app code in its place and so on.
We can use the docker build command to build an image. The exact command we will need to use is:
docker build ./aci-helloworld -t aci-tutorial-app
The above command looks for the Dockerfile in the directory /aci-helloworld and creates an image called aci-tutorial-app. Running the command should yield an output looking like this:
Above it’s showing us all the steps that we set up in the Dockerfile like:
We can see our created image if we run the following command:
docker images
Ok, then, we have an image which means we are ready for our next step; testing it locally.
Now that we have an image, we can create a container from it, using docker run. The full command looks like the following:
docker run -d -p 8080:80 aci-tutorial-app
Let’s look at the arguments:
We can see that the external port is 8080, which means we can navigate to
http://localhost:8080 to ensure our application works.
This is the image we get, so I would say our container is working:
With the following command we can list all the running containers:
docker ps It should present the following result:
We don’t want a container running and using up resources so let’s shut it down. We want to run the command docker kill to shut down the container, however, that command needs an argument, it needs the container id. Remember when we run docker ps ? The first column was our container id. We don't need the full id though, it suffices with the 4 first characters. So let's kick off our command
docker kill [container id, 4 first characters] docker ps // it should be an empty list That’s it. Here is a screen dump of the commands we just ran:
Azure Container Registry is your private Docker registry in Azure.
We need Docker, Docker Engine and Azure CLI for this to work. We have already installed Docker at this point so let's see how we can install Azure CLI:
https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest
Before we can create said registry we will need a Resource Group. A Resource Group is a logical container in which we need to place all our resources like applications, databases and now Resource Group. Everything in the same group can easily and securely communicate.
so let’s create that first:
Once this Resource Group is created we can go back to creating our Container Registry.
The command looks like the following:
Let’s break it down a bit.
az acr create
Is the actual command to create our Container Registry. Then we need some arguments:
You should get an output looking like the following:
The important part is getting a provisionState back with value Succeeded.
We need to log in to our registry before we can push docker images to it. So let’s log in:
That should tell you Login Succeeded if all is well
Your output should look something like this:
Above you can see that I opted to call the registry chriscontainerregistry put you would have to replace that with your chosen name.
To push a container image to a private registry like Azure Container Registry, you must first tag the image with the full name of the registry's login server.
That’s something you can find out by looking at the JSON output when you created your registry. You are looking for a property called ""loginServer"". It has the format of [your registry name].azurecr.io. In my case, that would be chriscontainerregistry.azurecr.io.
So either you remember the name of loginServer, from when we created our container registry or you can always retrieve the loginServer later by calling this command:
This will give us the loginServer name printed in our terminal. Of course [container registry name] would in our case be the value chriscontainerregistry, so adjust accordingly depending on your chosen name.
Let’s now head back to Docker. We need to Tag the aci-tutorial-app image with the loginServer of your container registry.
We tag it with the following command:
docker tag aci-tutorial-app /aci-tutorial-app:v1
Let’s break it down.
So the correct command in our case, using the correct values would be:
docker tag aci-tutorial-app [container registry name].azurecr.io/aci-tutorial-app:v1
Run docker images command at this point, to verify it was correctly created. It should look something like this:
Now we can actually push the image to the repository. We do so by executing the following command:
docker push /aci-tutorial-app:v1
and with all the correct values in place, it would be:
docker push chriscontainerregistry.azurecr.io/aci-tutorial-app:v1
You may need to log in first, in which case you run the following command:
az acr login — name [container registry name]
Carrying out the docker push should render the following result:
Ok, so now we actually want to see what images we have in there, spoiler there should be the one we just uploaded ;)
We can run the following command:
Using the correct value for acrName it would look like this:
There it is, our only pushed image :)
Now that we have our image in the repository, we can tell the repository to create a container from our image and thereby deploy our application.
To run our deploy command we first need a little info, namely the following:
This will return the password
Ok, now we come to the deploy command, that might look a little bit intimidating:
There are a ton of ways to create a container, if you are interested in other ways, have a look at this link az container create
If it takes a while to deploy you can check status meanwhile, with this command:
After a very long JSON response back, look for provisioningState: Succeded, if you have that you are good.
Let’s have a look at our container with the following command:
We can see the logs from the app by running:
This will tell us running on port 80
Once it’s deployed we can visit the app on the --dns-name-label value, like so:
We set out to deploy an app. This time we wanted to deploy a docker container. For that, we first needed to create a docker image. So we created one using docker build.
Then we realized we needed an Container Registry, cause it was from there we would deploy our image, i.e instantiate a docker container and deploy it.
To make it end up in the Container Registry we first needed to tag it with the loginServer name, after that we pushed the tagged image.
Lastly, we told the Container Registry to create a container from our image and deploy it. Once deployment was done we could go to our browser and verify the app was there, success :))
It wasn’t that many steps really. I mean let’s say our app consisted of 3 other services. We would only need to build an image for each, tag it, push, and create a container.
#BlackLivesMatter
138 
138 claps
138 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Google Developer Expert. Cloud Developer Advocate at Microsoft, https://twitter.com/chris_noring
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/awesome-azure/azure-application-security-group-asg-1e5e2e5321c3?source=search_post---------97,"There are currently no responses for this story.
Be the first to respond.
Introduction to Application Security Group (ASG) in Azure — What is ASG?
Application Security Groups helps to manage the security of Virtual Machines by grouping them according the applications that runs on them. It is a feature that allows the application-centric use of Network Security Groups.
"
https://medium.com/awesome-azure/azure-vs-aws-difference-between-azure-virtual-network-vnet-and-aws-virtual-private-cloud-vpc-2e8debc3290e?source=search_post---------98,"There are currently no responses for this story.
Be the first to respond.
Comparison: Azure Virtual Network (VNet) vs AWS Virtual Private Cloud (VPC)
The journey to the cloud begins with choosing a cloud provider and provisioning private networks or extending their on-premise network. Customers looking to provision their resources in the cloud can choose from the…
"
https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429?source=search_post---------99,"Sign in
There are currently no responses for this story.
Be the first to respond.
Manikanta Yadunanda
Dec 16, 2017·6 min read
In this post, I will go through the happy path for setting up a deep learning machine in Microsoft Azure. This will be beneficial for people who want to set up a deep learning machine easily on Azure(Without going through all the graphics driver, CUDA, and deep learning libraries installation).The latter part of this post will be targeted for setting up fast.ai course content and libraries on Azure. I am part of the fastai deep learning 2017 course as an international fellow. This course content will be available to the general public early 2018 as a MOOC.
Virtual machine images contain all the required environment for various tasks. One can deploy them and get up and running straight away. Amazon AWS has AMI for this purpose. Similarly Azure offers virtual machine images.
The one we are interested in is Azure Deep Learning Virtual Machine(DLVM) part of the Azure Data Science Virtual Machines(DSVM). It was released mid-2017. Please go to the below page to get an idea of what it offers.
azuremarketplace.microsoft.com
In short, it has almost all the tools required to get you started in deep learning. We will refer it as VM for the rest of the article.
If you have a visual studio subscription then I have a good news for you. At the time of this writing, you can enable the benefit of monthly Azure credits. You can use those credits for your VM costs. You have to select this subscription while creating your virtual machine.
Please visit this link for more details and how to activate the benefit.
Note: Virtual machine pricing depends on region and instance types. You are advised to refer here for proper pricing information.
1. Azure Deep Learning Virtual Machines can be only deployed on Azure NC series virtual machines. By default, it’s not enabled( at least for me). So one needs to raise a support request to get them enabled on your ID.
2. Please note this is only needed if you observe the NC series options are greyed out during the process of our deployment. You might see a message like “Not available for the current subscription”
3. It may take some time depending on the availability of NC series machines.
4. Once NC series are enabled on your ID, you can easily create your VM.
You can manage your machines from the virtual machines tab.
You can then select the VM you want, to open a detailed view of that VM.
Using the above tab, you can control your VM. “Connect” will show the details required for SSH login to your instance. “Start” will help to boot up your VM whenever you want to use it. And make sure you use “Stop” when you are not using it. You will be billed for the time your VM is in running state.
You can connect to your instance using ssh with the below command.
ssh -L 8888:127.0.0.1:8888 username@ip
-L 8888:127.0.0.1:8888 is used for SSH tunneling which helps us to connect to our jupyter notebook server on our remote VM from our local browser.
username@ip replace this with your details from “Connect” tab which was shown in the previous image.
You will be prompted for the password. Upon completion, you will have the terminal of your VM. You can play and move around to see the examples and tutorials this VM is loaded with to get you started.
As jupyter is already installed in our VM. You can straight away run the jupyter server to serve you notebooks. To start the jupyter server, you can run the following command.Note that instead of running the server from notebooks you can run from any folder location from which you want your notebooks to be served.
You should see something like the below image on the screen.
Copy the URL+Token from the command line and enter in your local browser to view the jupyter notebooks. You can play with the sample notebooks which are already present.
That’s it for the setup and should be enough to get you going :)
Note: All the above information is what I observed at the time of writing this post. These things might change in future and some of them may be wrong. You are advised to always refer to the official documents.
pip install ipykernel
python -m ipykernel install — user — name fastai — display-name “Python (fastai)”
4. You can keep any name you like for the kernel instead of “Python (fastai)”.
5. The above steps would add fastai kernel to jupyter notebooks.
6. You can change the kernels of your notebook as shown in the following image.
7. With the above kernel, you will be able to run your fastai notebooks.
8. Let’s say for Deep Learning Part1 V2, you can change your directory to fastai/courses/dl1/ on your cloned repository.
9. Launch jupyter notebook from there as explained earlier and change your kernel to fastai kernel. You can then run all the lessons notebooks and experiment with them.
Fastai package
The fastai library is also available through pip. And you can install it using “pip install fastai” as mentioned in the official readme. This will help if you want use fastai as a library in any of your environments or projects.
Update for fast.ai 2018 course
All the above mentioned steps remain same except for fast.ai installation.The github readme has updated instructions.
Happy Deep Learning :)
iOS Developer, Machine Learning Enthusiast, https://www.linkedin.com/in/manikanta-sangu-88292386
175 
9
175 
175 
9
iOS Developer, Machine Learning Enthusiast, https://www.linkedin.com/in/manikanta-sangu-88292386
"
https://towardsdatascience.com/azure-synapse-analytics-as-a-cloud-lakehouse-a-new-data-management-paradigm-cdcbe2378209?source=search_post---------100,"Sign in
There are currently no responses for this story.
Be the first to respond.
Korkrid Akepanidtaworn (Kyle)
Jul 2, 2020·4 min read
In the early days of data repository, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a commonly known system used for reporting and data analysis, and is considered a core component of…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@sahityamaruvada/spinning-up-instances-with-terraform-in-azure-and-aws-38f251d462e8?source=search_post---------101,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sahitya Maruvada
Jul 18, 2019·5 min read
When I first started working on Terraform with a little knowledge of AWS and Azure there were several blog posts and of course the official documentation to aid me throughout the process. This article aims to be a beginner-friendly guide giving information on what resources are required to create a virtual machine in a crisp and easy manner. The goal of this post is to give a basic idea of…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/seed-digital/using-microsoft-azure-identity-with-firebase-in-a-react-native-app-c9eef0fd0af8?source=search_post---------102,"There are currently no responses for this story.
Be the first to respond.
[UPDATE] We’ve updated the library with an example using Expo, if you want to see it running check out the Github
Authenticating users into your application is quite a bread and butter requirement when developing an app.
Most app users are familiar with the idea of signing into an application using an existing service like Facebook or Google OAuth, and sometimes this can be the best option when a new application requires an account setup.
Leveraging existing user data from an OAuth service can have many advantages for both users and developers when integrating with an application:
Here at Sheda we tend to use Firebase and React Native for most of our mobile development projects.
Firebase comes with some handy features such as real time-database service, user authentication and file storage which means our development process can move at a super-fast pace.
Which in turn, means that we get products to market at a faster rate and get that all-important customer feedback quicker, resulting in faster iterations and a better overall product for users.
The Firebase user authentication service allows us to integrate sign-in services from major social media platforms, like Facebook and Twitter, into the application with a few lines of code.
With this capability, our developers can focus on its core functionality instead.
Firebase authentication supports many service providers, but unfortunately, Microsoft Account is an exception.
While this is a bummer, Firebase does still allow you to register new users with a custom token which means we can handle the login mechanism by ourselves and save Microsoft Identity user credentials to Firebase for later use.
We’ll explain how to get this working in your react native app.
Trying to access a Microsoft account is kind of bizarre.
Microsoft previously separated their user accounts into two different domains — one for their cloud platform Microsoft Azure and another for general users who are using their services like Hotmail, One Drive or Xbox.
This meant developers had to use different authentication endpoints in order to authenticate users from different services.
Thankfully, they recently converged their disparate authentication service into a single service called ‘v2.0 endpoint’ which allows you to use OAuth authentication for whichever Microsoft service account you have.
Authenticating a user via the v2 endpoint will give us access to a custom bearer token, this token allows us to consume REST APIs from the Microsoft Graph (a single endpoint into all Microsoft services) and allows your app to request simple user data, for example, first name, last name, email and get other information like email messages, contacts and notes associated with their accounts.
As I have stated earlier, Microsoft used OAuth 2.0 protocol for user authentication.
If you are familiar with OAuth 2.0 you may skip this part. Otherwise, let’s see how this thing works to have a clear understanding of what we are going to implement.
The authentication process gets started from a web browser (WebView in this case) requests a login page from the Microsoft authentication server (http://login.microsoftonline.com/common).
In this step, when requesting to the server, we need to equip our application identification as a query string to allow the Microsoft server to identify which application we are using.
After a user enters their credentials and goes through the general login process, the Microsoft server will redirect a user back to the URL that was provided when registering an application with a temporary code, which is a crucial piece of data that we need to use in order to obtain the access token for using to consume REST APIs.
The last step in this part would be requesting an access token. Once we have a temporary code from the last step, we have to use it as a part of the payload to gain an access token.
To allow the application to interact with the Microsoft server, it needs to be registered through Microsoft Application Registration Portal. (how to register an app in Azure)
Once the app is registered, an Application ID will be generated.
In order to get the authentication to work on React Native, without using other third-party libraries, we decided to utilize React Native WebView component.
Consequently, we are going to need a secret key for communicating with the Microsoft Authentication service through the web interface.
Moving down to the Application Secrets section, click on the ‘Generate New Password’ button to get the secret key generated.
It will take two to three seconds before the secrete key will be shown on screen. Keep in mind that, it will be shown just only once — so keep it in a safe place.
Everything seems to be pretty good, but one more thing we have to tell Microsoft is which platform we are going to integrate this app on.
Move down a little bit into a Platforms section and click on ‘Add Platform’.
The ‘Add Platform’ dialog will be prompted, in this case we will go with Web platform as we’ll be using JavaScript to access the service.
Then, define a callback URL. Make sure that it’s using HTTPS protocol if you are going to integrate this application with iOS to avoid problems that may occur.
Once everything is set, scroll down to the bottom, hit ‘Save’ and we are done with the Application Register on the Microsoft Application Portal.
React Native is a JavaScript library developed by Facebook. It allows you to develop native applications for both iOS and Android by using JavaScript.
React Native has its origins and shares the same syntax and other design considerations as ReactJS, which is the web application version of the library.
One of the main design paradigms for React apps is to split each component of the application into independent, reusable pieces and think about each piece in isolation.
This allows components to be reused across a whole application where appropriate.
Apart from creating all components ourselves, developers can also integrated the other components from around the world created by other developers into their application through a package manager like NPM or Yarn.
Since we are going to reuse the Microsoft authentication service with our future projects as well, we have decided to create a separate NPM module called ‘react-native-azure-ad-2’.
The module contains three main components where:
As shown in the diagram:
After implementing the component, using it is quite easy.
First, import the ‘react-native-azure-ad-2’ component.
Then create an AzureInstance by using Microsoft application credentials that we registered above.
Also, add application scope in order to ask users to consent when they login.
For more information about scope visit Microsoft blog.
After that, return AzureLoginView where you want the login WebView to be rendered and pass along with azureInstance that we have created from the last step.
With AzureLoginView, we can have a few prop options to render the view differently.
That should make it easy to integrate Microsoft Azure authentication into your React Native app. If you encounter any issues drop us a line in the comments below or on Github.
Want some help building your technology startup, growing your business and creating products/services that your customers will love? Contact Sheda and let’s see how we can help.
We use design and technology to inspire creation and…
176 
5
176 claps
176 
5
We use design and technology to inspire creation and purpose — creating a future we’d like to live in. These are our ideas and musings on design, technology, business and Industry 4.0 from the crew at Sheda.
Written by
A product development & design firm. We use design & technology to help purpose-driven companies & entrepreneurs to create the future we all want to live in
We use design and technology to inspire creation and purpose — creating a future we’d like to live in. These are our ideas and musings on design, technology, business and Industry 4.0 from the crew at Sheda.
"
https://medium.com/microsoftazure/5-microsoft-learn-modules-for-getting-started-with-azure-cf948c8c3caf?source=search_post---------103,"There are currently no responses for this story.
Be the first to respond.
Microsoft Learn is an free interactive sandbox for learning new skills. With over 62 Azure modules and counting, it can be hard for a new comer to know where to get started.
I was recently asked for my perspective as a Cloud Advocate, from some start ups, in my local tech ecosystem. Below is a list of 5 recommended Microsoft Learn Modules for getting up to speed with Azure Services.
I recommend you try them out in the following order:
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
If you have any questions, comments, or topics you would like me to discuss feel free to follow me on Twitter if there is a milestone you feel I missed please let me know.
Aaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
44 
44 claps
44 
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/blueteamlabs/using-sysmon-in-azure-sentinel-883eb6ffc431?source=search_post---------104,"There are currently no responses for this story.
Be the first to respond.
Over the last couple of nights I've been playing with Azure Sentinel to see how useful it will be as a SIEM/Hunting platform. One of the fist things I wanted to do is onboard Sysmon data. Unfortunately the documentation isn't up to par yet and it took me a LOT of time and some help from Kevin Beaumont, @ashwinpatil and Maarten Goet to get this working. Thanks guys!
For instance the ""Security and Audit"" Solution has a SysmonEvent schema, this one is broken however. In order to save you the same struggle I'll give a brief outline here.
First of all you'll need to connect machines, this is relatively straight forward. Then you need to start ingesting some data;
Now it is ready to start ingesting events, to configure which ones go to ""Workspace Settings"" and then to ""Advanced Settings"". Now start adding the Data sources you require.
By now the data should be flowing into your instance. You can check this by going to Logs and use the following KQL query;
Sadly these events are unparsed. There are two options now, parse them by hand, which I don't recommend ;). Or parse them by creating a function and use that in your future queries.
Fortunately the Azure team loves Sysmon like I do and they were so kind to provide a parsing KQL over here.
I have created a OSSEM mapped equivalent of that one, since I like the consistency in the field names, I also use it in my Splunk ThreatHunting app. This parsing KQL version is available here.
Copy the entire contents of the file and paste it in an empty query box, next click Save on the top right. Choose a name and alias name and save it as a function.
In some cases it can take a few minutes to become available, make sure to remember this :D It caused me some annoyance when testing this for the first time. So next test your new function in a new query window.
And there you go, properly parsed events! The only slight inconvenience is that the syntax autocomplete function is not working since these field names are not part of a Schema, there is no way known to me to address this at this moment. Given you know what you're looking for most of the time, you'll be fine without.
Blogging platform for the BlueTeamLabs GitHub repository
74 
1
74 claps
74 
1
Written by
FalconForce | DFIR | Threat hunter | Data Dweller | Splunk | Sysmon | Microsoft MVP
Blogging platform for the BlueTeamLabs GitHub repository
Written by
FalconForce | DFIR | Threat hunter | Data Dweller | Splunk | Sysmon | Microsoft MVP
Blogging platform for the BlueTeamLabs GitHub repository
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@auchenberg/introducing-remote-debugging-of-node-js-apps-on-azure-app-service-from-vs-code-in-public-preview-9b8d83a6e1f0?source=search_post---------105,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kenneth Auchenberg
May 17, 2018·5 min read
Finding and identifying issues with Node.js apps deployed to the cloud can be burdensome process that usually involves local debugging, the sprinkling of console.logson to your codebase, and many re-deployments to get the problem identified and solved.
Today we are changing that, as we are introducing a public preview of remote debugging for Node.js apps deployed on Azure App Service for Linux. Our remote debugging experience brings you the same great debugging experience that you already know from Visual Studio Code when debugging Node.js locally to the Azure Cloud.
Remote debugging for Azure App Service works by taking advantage of a new Azure mechanism that allows us to forward the remote debugging information from your Node process running in Azure to your local computer in a secure way. Once the connection is established we can take VS Code’s built-in Node.js debugger to attach and debug — just like if you were debugging your Node app locally!
You can read more about how it works, and other Azure App Service announcements here on the Azure blog.
To get started you first have to prepare your Node.js app running in Azure and setup Visual Studio Code. Let me show you how.
This section assumes that you already have your Node.js app deployed to Azure using Azure App Service for Linux. If you haven’t deployed your app you can get started here or using the Azure CLI here.
In order to enable remote debugging when your app is running in the cloud, you might need a make a slight modification to your Node app, depending on how you start your app.
Per our default logic for Node Apps on Azure, we’ll try to auto-detect how you start your Node app, if you use bin/www server.js app.js hosting.js or index.js as your main script file. If that’s the case, there’s some good news: You don’t have to change anything!
If you are using NPM scripts to start your Node app, you need to make a slight modification to how you start your app, as we won’t try to fiddle with your app. You know best how to launch your app, so for this public preview, we are asking you explicitly to create a new debugging specific NPM script that runs your Node app with --inspect=0.0.0.0:$APPSVC_TUNNEL_PORT.
This runtime flag tells your Node app to start in debug mode listening on the debugging port specified by Azure, which is exposed as an environment variable.Your scripts section in your package.json should look something like:
2. The next step is to go to Azure Portal, and find your deployed Node.js app on Azure App Service (Linux).
4. Go to Application settings and update your Startup File to your newly configured npm run start_azure_debug script.
It should look something like:
Your Node app is now figured to run with remote debugging enabled.
Yay! 🎉🔥
Next is to get your Visual Studio Code setup going, which is an easy process:
3. If you haven’t logged into Azure from VS Code, you should now click the Azure icon in the sidebar to the left, and login to see your App Service apps.
4. Since remote debugging is a preview feature, you now have to go to your VS Code settings to enable it. You do this by clicking File > Preferences > Settings. modify your appService.enableRemoteDebugging to be true.
5. Now that you have remote debugging enabled, the next step for you is to open the source code for your Node app as your workspace in VS Code.
6. Once you have the source code open next is to find your deployed Node app in the Azure App Service list within VS Code.
Right click and select the new “Start Remote Debugging” option.
Once clicking on “Start remote debugging”, Visual Studio Code will check if remote debugging has been enabled for your app, and if it hasn’t you will be asked to confirm before enabling remote debugging on your behalf.
Once the right configurations has been set, you should now see remote debugging for your Node.js app being started (you can follow the progress in the status bar) and once the debugger is connected VS Code will enter debug mode.
Notice: You might get a prompt from your firewall on Windows, please allow the connection
Bam. That’s it. 🎉🔥
The debugger is now connected, and you can remote debug your Node.js app running in Azure! Try set a breakpoint!
In the March 2018 release of VS Code, we introduced a new debugging concept called Logpoints. When combining Logpoints with remote debugging on Azure you have a powerful combination for seamless production debugging!
Try it out and read more about Logpoints here 👉https://code.visualstudio.com/updates/v1_22#_logpoints
We are excited to bring you real remote debugging for Node.js apps running on Azure App Service (for Linux) directly from Visual Studio Code. Giving you a simply integrated experience that allows you to easily debug and diagnose problems if they occur when running your Node.js apps in our Cloud.
We’d love to hear your feedback: Please find us in GitHub to report issues, provide suggestions, or tell us your success stories.
Happy coding!
/k
Developer Experience @stripe / Alum @code , @microsoft , @citrix , @podio , @vodafone , @zyb / @WEF Global Shaper / @coldfrontconf founder / @goog
263 
1
263 
263 
1
Developer Experience @stripe / Alum @code , @microsoft , @citrix , @podio , @vodafone , @zyb / @WEF Global Shaper / @coldfrontconf founder / @goog
"
https://medium.com/@renatogroffe/net-core-2-1-nosql-exemplos-utilizando-mongodb-redis-documentdb-ravendb-e-azure-cosmos-db-f9675b80df76?source=search_post---------106,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Aug 20, 2018·4 min read
Neste novo post trago todos os exemplos que implementei anteriormente em artigos e palestras cobrindo NoSQL soluções como MongoDB, Redis, DocumentDB, RavenDB e Azure Cosmos DB. As aplicações em questão já foram convertidas para o .NET Core 2.1 e o ASP.NET Core 2.1, estando disponíveis no GitHub (com os repositórios correspondentes sendo referenciados nas próximas seções).
Esta é uma atualização complementando outro artigo que publiquei anteriormente sobre o uso de tecnologias NoSQL com .NET Core 2.0 e ASP.NET Core 2.0:
.NET Core 2.0 + NoSQL: exemplos utilizando MongoDB, DocumentDB e Redis
E por falar em tecnologias Microsoft, não deixem também de acompanhar o Azure Tech Nights, que acontecerá entre os dias 20 e 28 de Agosto de 2018. Será um evento NOTURNO, ONLINE e GRATUITO promovido pelo Canal .NET, com apresentações focadas no Microsoft Azure e cobrindo temas como microsserviços, Inteligência Artificial, desenvolvimento Web, bancos de dados, NoSQL, Docker, Kubernetes e muito mais.
Entre os palestrantes teremos MVPs Microsoft, MTACs e Especialistas de Mercado.
Para efetuar a inscrição acessem este link.
A grade com as palestras e outras informações podem ser encontradas no site oficial do Azure Tech Nights.
O MongoDB foi tema de um Live Demo do Canal .NET durante o ano de 2017:
Os exemplos de uso do MongoDB em .NET Core 2.1 e ASP.NET Core 2.1 estão no seguinte endereço:
https://github.com/renatogroffe/MongoDB-DotNetCore2.1
Ao consultar o repositório MongoDB-DotNetCore2.1 você encontrará os seguintes projetos:
O uso de Redis com ASP.NET Core já foi abordado em um evento online do Canal .NET sobre performance em APIs, em que demonstrei a implementação de cache distribuído empregando esta solução NoSQL:
A demonstração no caso foi baseada no conteúdo apresentado no seguinte artigo:
ASP.NET Core 2.0: implementando cache em APIs REST
O próximo artigo abordou o uso de Redis em conjunto com Docker, Docker Compose e ASP.NET Core, através de um exemplo de implementação de uma solução multi-container e que emprega uma API REST gratuita da Marvel Comics para a apresentação de informações (armazenadas em cache em uma instância do Redis):
ASP.NET Core + Docker Compose: implementando soluções Web multi-containers
Uma demonstração envolvendo o projeto descrito no artigo foi também realizada em um evento online do Canal .NET sobre Docker Compose:
Este último exemplo já foi convertido também para o ASP.NET Core 2.1:
https://github.com/renatogroffe/ASPNETCore2.1_Docker-Compose
Um ponto importante deve ser destacado quanto ao uso de cache com Redis no ASP.NET Core 2.1. Devido à mudança para o metapackage Microsoft.AspNetCore.App o package Microsoft.Extensions.Caching.Redis deverá ser adicionado a um projeto a fim de tornar possível este tipo de implementação:
Solução NoSQL multi-model que integra o Azure, o Cosmos DB é um serviço de alta disponibilidade que permite a utilização de tecnologias como MongoDB, DocumentDB, Cassandra e Gremlin a partir da nuvem da Microsoft.
O Azure Cosmos DB foi inclusive tema de uma apresentação que fiz na edição 2018 do TDC São Paulo. Seguem os slides descrevendo as principais características deste serviço:
E foi também abordado durante a 2a. edição do Azure Tech Nights:
Alternativa NoSQL da Microsoft e orientada a documentos, o DocumentDB se destaca pela possibilidade de uso de SQL (por meio de um conjunto limitado de instruções) na consulta às informações de uma coleção.
O site a seguir traz exemplos e permite inclusive a realização de testes envolvendo o uso de SQL para a consulta a uma coleção do DocumentDB:
Azure Cosmos DB Query Playground
Para consultar os exemplos de uso do DocumentDB com .NET Core 2.1 e ASP.NET Core 2.1 acesse:
https://github.com/renatogroffe/DocumentDB-DotNetCore2.1
No repositório DocumentDB-DotNetCore2.1 se encontram projetos similares àqueles apresentados na seção sobre MongoDB:
Vale destacar ainda que o DocumentDB conta com um emulador local, cuja instalação pode ser obtida a partir do seguinte link (há uma alternativa a isto sob a forma de uma imagem no Docker Hub):
Use the Azure Cosmos DB Emulator for local development and testing
A seguir está um exemplo de consulta através da interface disponibilizada para gerenciamento do emulador local de DocumentDB:
Também orientado a documentos, o RavenDB é uma alternativa NoSQL implementada em .NET, conta com uma versão Community e possui a capacidade de suportar transações ACID (comportamento este típico de bancos relacionais). Além disso é multiplataforma, o que possibilita seu uso em ambientes como Windows, Linux, MacOS, containers Docker e Raspberry Pi.
Uma Solution envolvendo carga de um catálogo de produtos e serviços, bem como uma API REST de consulta a tais dados, foi disponibilizado no seguinte repositório do GitHub:
https://github.com/renatogroffe/RavenDB-DotNetCore2.1
O RavenDB juntamente com o MongoDB, o Redis, o DocumentDB e o Azure Cosmos DB foram também tema de uma apresentação online que realizei pelo canal Developers-BR e cuja gravação já está disponível no YouTube:
.NET Core 2.1 e ASP.NET Core 2.1: Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
93 
93 
93 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://javascript.plainenglish.io/scan-your-code-for-vulnerabilities-with-azure-devops-tools-7db80955d4fe?source=search_post---------107,"Loopholes are everywhere. Software systems built by software engineers are no exception and we all know them as vulnerabilities. Usually, the severity increases at an exponential rate. So, if one vulnerability is not fixed it might lead to other vulnerabilities.
Ideally, we, as software engineers, want to develop software systems that are free of vulnerabilities but that’s not always the case. We do have vulnerabilities in real-world software.
To identify them, we need to scan our code by using different tools. These tools might be manual or can be integrated into a DevOps pipeline to automatically scan the code before deployment.
Today, we are going to review some of the tools that can be used for scanning the code in an Azure pipeline. Here they are.
SonarCloud is another leading online tool for code security that can be integrated into the Azure DevOps pipelines. SonarCloud supports nearly all major programming languages like JavaScript, TypeScript, C/C++, C#, VB .Net, and is free for open-source projects too.
However, If you have some closed source code then SonarCloud has a subscription fee to analyze private repositories.
With SonarCloud, you will have the advantage of automated detection of vulnerabilities and bugs across all pull requests and branches. It can also provide build tasks that you may integrate into your build definition.
However, SonarCloud gives detailed logs of all issues related to code, giving you the chance to fix them even before merging and deployment. When you are working at the project level, you will also get detailed tracking of the overall health of your application by the virtue of a dedicated widget.
The AWS Toolkit for Azure DevOps enables you to add tasks to easily build and release pipelines in Azure DevOps to seamlessly work with the vast array of AWS offerings that include AWS CodeDeploy, AWS Elastic Beanstalk, Amazon S3, AWS Lambda, Amazon Simple Queue Service, Amazon Simple Notification Service, and AWS CloudFormation.
With AWS Toolkit, you can also run commands using both AWS CLI and Windows PowerShell AWS Tools. It is available for free in the Azure DevOps category at the Visual Studio Marketplace.
Since AWS Toolkit is an open-source project, you can contribute to it, improve its docs or make a feature request too.
Ado Security Scanner is another open-source tool for code scanning in Azure DevOps pipelines by Microsoft DevLabs. This tool is specifically designed to assist organizations to manage secure Azure DevOps pipelines with the help of built-in ADO dashboard widgets through continuous scans and visualization of security issues and problems.
It assists you in keeping your Azure DevOps artifacts such as project/org settings, build/release configurations, agent pools, service connections, etc. configured securely. Also, you can run it in an Azure DevOps pipeline via a marketplace extension or maybe separately in a PowerShell console.
It manages different components of Azure DevOps and scans them like Builds, Releases, Organization, Service Connections, User, etc.
Beagle Security is an intelligent, automated, and holistic tool to make your applications free of vulnerability-related problems and issues. It provides continuous penetration testing that requires some human supervision for organizations so that they can remain safe from cyber threats and exploits.
It finds the loopholes in your application before the real hackers do. The advantage of using Beagle Security is that it allows you to perform penetration testing of web applications with both SANS and OWASP standards.
You can also download the reports in PDF and other user-friendly formats. Alongside that, you get complete support from their team of experienced security researchers in the whole process.
In a nutshell, there are many tools that you can use for scanning your code for vulnerabilities. If you are looking for comprehensive tools that work well with both open source and closed source projects, then you can try any of the above tools mentioned in this list.
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
67 
67 claps
67 
Written by
I help developers to be better engineers! 💻 Software Engineer | 📈 Data Science | 💼 Entrepreneurship | 🧠 AI | 🖋️ Writer at DEV with 250K+ views.
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
Written by
I help developers to be better engineers! 💻 Software Engineer | 📈 Data Science | 💼 Entrepreneurship | 🧠 AI | 🖋️ Writer at DEV with 250K+ views.
New JavaScript and Web Development content every day. Follow to join our +2M monthly readers.
"
https://medium.com/@mauridb/powerbi-and-azure-databricks-2-d0ed16427d36?source=search_post---------108,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
Feb 15, 2018·3 min read
Just a couple of days after I published the article that describes how to connect to Azure Databricks with PowerBI via the ODBC connector, I received an email from friends (Yatharth and Arvind) in the Azure Databricks and AzureCAT team that told me that a better and easier way now available to connect PowerBI to Azure Databricks was possible.
Not only this new way is way simpler and lightweight, but it also enables usage of DirectQuery to offload processing to Spark, which is perfect when you have a real huge amount of data, that doesn’t make sense to be loaded into PowerBI, or when you want to have (near) real-time analysis.
The PowerBI connector to be used to go this way is, as you may have guessed, the Spark connector:
Configuring the connector, compared to all the setup needed with ODBC, is really a breeze. All you have to specify is the server and the protocol. Protocol must be set to HTTP:
Setting the server is just a bit tricky. Information can be found in the JDBC/ODBC pane available in the Configuration page of your Azure Databricks Spark cluster:
The url needs to be constructed following this procedure:
The final url would be:
https://eastus.azuredatabricks.net:443/sql/protocolv1/o/6132794369297039/0214-200557-viols339
Now just insert it into the Server textbox of PowerBI Spark connector configuration window, as mentioned before, and you’re almost done.
Credentials are issued and managed via the Personal Token, exactly as explained in the ODBC article. So just insert the token user name (remember that you really have to literally insert “token” as username) and then the generated token as password:
After that it will just works, now also with DirectQuery:
Here a glimpse of what’s happening behind the scenes, with the SQL queries sent to Spark when DirectQuery is used:
Official documentation will be updated soon, in the meantime, for all those like me who are eager to play with Azure Databricks and PowerBI, I hope this helps.
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
See all (156)
82 
5

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
82 claps
82 
5
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/azure-kubernetes-service-aks-observability-with-istio-service-mesh-4eb28da0f764?source=search_post---------109,"In the last two-part post, Kubernetes-based Microservice Observability with Istio Service Mesh, we deployed Istio, along with its observability tools, Prometheus, Grafana, Jaeger, and Kiali, to Google Kubernetes Engine (GKE). Following that post, I received several questions about using Istio’s observability tools with other…
"
https://medium.com/@renatogroffe/asp-net-core-azure-kubernetes-orquestra%C3%A7%C3%A3o-de-containers-na-nuvem-parte-2-6c922daeadab?source=search_post---------110,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 18, 2018·9 min read
Esta é a segunda parte da série em que abordo o uso combinado do ASP.NET Core, do Microsoft Azure e do Kubernetes (através do AKS - Azure Kubernetes Service) na implementação de soluções que empregam orquestração de containers. As próximas seções descrevem os passos necessários para a publicação de uma imagem Docker no Azure, bem como a criação de um cluster no Kubernetes e o deployment neste último da aplicação de testes.
Caso não tenha ainda acessado ou, até mesmo, deseje rever o primeiro artigo deste tutorial consulte então o link a seguir:
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem - parte 1
A imagem Docker utilizada a partir do Kubernetes será gerada através do Visual Studio 2017. Acionar para isto na IDE o menu de contexto para o projeto APIContagem e, em seguida, as opções Add e Docker Support (este processo também pode ser realizado durante a criação de um novo projeto):
Aparecerá neste momento a janela Docker Support Options. Selecionar em Target OS a opção Linux:
Com isto serão gerados o Dockerfile e os arquivos do Docker Compose:
A imagem Docker será gerada com base nas configurações de release da aplicação. Selecionar para isto a opção Release no Visual Studio 2017:
Efetuar na sequência a compilação do projeto (a partir do menu Build > Build Solution). Ao executar o comando docker images no PowerShell aparecerá então a imagem apicontagem:latest:
O Azure Containter Registry permite o armazenamento de imagens Docker de forma privada, representando assim uma alternativa dentro da nuvem da Microsoft ao Docker Hub. Este serviço pode ser empregado em conjunto com tecnologias como Azure Container Service (com seus diferentes orquestradores - Docker Swarm, DC/OS e Kubernetes), Azure Kubernetes Service (AKS) e Azure Web App for Containers.
No portal do Azure será criado um novo recurso baseado no serviço Container Registry:
Informar no formulário de criação:
Após alguns segundos o item groffecr (um Container Registry) aparecerá na lista de elementos que compõem o grupo de recursos indicado no passo anterior:
Na próxima imagem aparecem detalhes deste novo recurso, com o login para conexão ao serviço de registro/armazenamento de imagens em destaque:
Uma tag chamada groffecr.azurecr.io/apicontagem deverá ser criada para a imagem apicontagem:latest. Este novo elemento contém o nome que será gravado no Azure Container Registry (formado pela identificação do registro de containers + nome da aplicação/imagem; esses dois itens estão separados ainda por uma barra — “/”). Executar para isto o seguinte comando no PowerShell:
docker tag apicontagem:latest groffecr.azurecr.io/apicontagem
O próximo passo agora consiste em efetuar o login no recurso do Azure Container Registry criado na seção anterior. Executar para isto o seguinte comando no PowerShell (em que serão fornecidos o usuário e uma senha disponibilizados pelo Microsoft Azure):
docker login groffecr.azurecr.io -u USUÁRIO -p SENHA
As credenciais necessárias estão na seção Access keys do Container Registry (para o exemplo deste artigo foi empregada a senha indicada em password):
Para publicar a imagem groffecr.azurecr.io/apicontagem no Azure Container Registry será utilizado o comando:
docker push groffecr.azurecr.io/apicontagem
A imagem groffecr.azurecr.io/apicontagem aparecerá então no Azure Container Registry (seção Repositories), logo após a conclusão deste último procedimento:
Para a criação do cluster que servirá de base para testes com o Azure Kubernetes Service (AKS) será necessário instalar a versão 2.0 do Azure Command Line Interface (CLI).
O Azure CLI 2.0 é um utilitário de linha de comando para gerenciamento e administração de recursos do Microsoft Azure. Multiplataforma, há a possibilidade de uso desta ferramenta em ambientes Windows, Linux e macOS. Informações sobre como instalar o Azure CLI podem ser encontradas no seguinte link:
https://docs.microsoft.com/en-us/cli/azure/install-azure-cli
A instrução a seguir utiliza o Azure CLI para registrar o provider que permitirá a criação e gerenciamento de recursos do AKS via linha de comando:
az provider register -n Microsoft.ContainerService
Um grupo de recursos chamado TesteKubernetes deverá ser criado, com o mesmo estando vinculado à região Leste dos EUA (East US):
az group create --name TesteKubernetes --location eastus
Já o próximo comando irá gerar um novo recurso do AKS (ContagemService) com um cluster contando com 2 nodes, sendo que o mesmo estará vinculado ao grupo TesteKubernetes:
az aks create --resource-group TesteKubernetes --name ContagemService --node-count 2 --generate-ssh-keys
Na figura a seguir é possível observar a execução destas instruções a partir do PowerShell:
Concluída esta primeira sequência de tarefas será possível notar a existência de 2 novos grupos de recursos no portal do Azure: TesteKubernetes e MC_TesteKubernetes_ContagemService_eastus:
No caso do grupo MC_TesteKubernetes_ContagemService_eastus serão criados aqui todos os recursos relacionados à estrutura do Master e dos Nodes que formam o cluster do Kubernetes:
O recurso ContagemService aparecerá dentro do grupo TesteKubernetes:
Ao acessar este recurso (ContagemService) o status do mesmo constará como Succeeded (o que indica sucesso em sua criação) e na seção Properties estará especificado o número de Nodes (2, em NODE COUNT):
Para a criação de objetos no cluster será empregado o kubectl, utilitário de linha de comando para o gerenciamento de recursos do Kubernetes. O link a seguir contém instruções para a instalação desta ferramenta:
Install and Set Up kubectl
No próximo comando o Azure CLI será utilizado com o intuito de liberar o acesso do kubectl ao cluster do AKS:
az aks get-credentials --resource-group TesteKubernetes --name ContagemService
Será também configurado o acesso do kubectl ao recurso do Azure Container Registry criado anteriormente:
kubectl create secret docker-registry contagemregistrykey --docker-server=https://groffecr.azurecr.io --docker-username=<USUÁRIO> --docker-password==<SENHA> --docker-email=<E-MAIL>
A seguir está o resultado da execução destas 2 instruções:
As definições do objeto Deployment que será criado ficarão em um arquivo YAML (contagem.yaml) cujo conteúdo foi disponibilizado na listagem seguinte:
O comando kubectl create -f contagem.yaml procederá com a criação do objeto Deployment:
Já o objeto Service funcionará como um Load Balancer, disponibilizando um endereço de IP para acesso à aplicação e distribuindo o processamento entre os diferentes Pods que vierem a existir para a mesma. A listagem a seguir (arquivo service.yaml) contém as definições necessárias para a geração desta estrutura:
https://gist.github.com/renatogroffe/a31fca447004623f8114da1b17e1fde0
A instrução kubectl create -f service.yaml fará com que este novo recurso seja criado:
A instrução kubectl get deployment listará o objeto de deployment criado nesta seção:
O comando kubectl get pods exibirá os Pods disponíveis (até o momento apenas um):
Ao executar o comando kubectl get services serão listados os serviços disponíveis, com o IP para acesso à aplicação destacado em vermelho:
Um teste com a URL http://40.117.133.3/api/contador trará como resultado o valor do contador, a propriedade machineName com o nome do Pod que está executando o container e o sistema operacional do host (Unix/Linux):
O kubectl será empregado agora para escalar a aplicação via linha de comando, com a instrução a seguir definindo o uso de 2 Pods para processamento das solicitações enviadas à aplicação de testes:
kubectl scale deployment contagem-deployment --replicas=2
Com a execução do comando kubectl get pods serão exibidos agora 2 Pods (o já existente e um novo que está destacado em vermelho):
Novos testes mostrarão que o Load Balancer configurado através do objeto Service está em funcionamento, com o consequente direcionamento de requisições para os 2 Pods existentes:
O Kubernetes conta também com uma aplicação Web/dashboard para monitoramento e gerenciamento de um cluster. Para ativar o Kubernetes Dashboard executar a seguinte instrução (referenciando o recurso do AKS e seu grupo correspondente):
az aks browse -g TesteKubernetes -n ContagemService
Na próximas imagens temos detalhes da página inicial que aparecerá ao acessar o Kubernetes Dashboard, com resumos englobando as diferentes estruturas existentes no cluster:
É possível inclusive escalar uma aplicação a partir da seção Deployment:
Para efeito de testes a aplicação será configurada a fim de empregar 4 Pods:
Com isto teremos 2 novos Pods, destacados em vermelho na próxima imagem:
A remoção forçada de um destes Pods levará à criação automática de uma nova estrutura deste tipo (o que demonstra a capacidade do Kubernetes em garantir uma alta disponibilidade da aplicação, com a alocação de novos containers/instâncias/Pods caso sejam detectados problemas em algum destes elementos):
Nas próximas imagens temos exemplos de retornos produzidos pelos novos Pods:
Os fontes do projeto de testes e scripts para criação de objetos no Kubernetes estão disponíveis no GitHub:
https://github.com/renatogroffe/ASPNETCore2_Kubernetes
Para remover um recurso do Azure Kubernetes Service (AKS) e todas as estruturas associadas a este executar a seguinte linha de comando:
az aks delete --name ContagemService --resource-group TesteKubernetes --no-wait
Azure Container Service (AKS)
Docker para Desenvolvedores .NET — Guia de Referência
Kubernetes — Site Oficial
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
185 
2
185 
185 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/building-a-diy-adhd-medication-reminder-with-azure-functions-e84e4b0611c3?source=search_post---------111,"There are currently no responses for this story.
Be the first to respond.
Lately, I’ve been playing around with Azure Functions to automate different parts of my life. Just today, as I was building a demo using Functions for an upcoming work project, I looked up at the clock and discovered it was 4pm.
"
https://medium.com/@jeffhollan/ordered-queue-processing-in-azure-functions-with-sessions-c42ee21e689d?source=search_post---------112,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
May 30, 2019·6 min read
Let’s chat about ordering. It’s one of my favorite topics, and something I’ve blogged about extensively before. Previously ordered processing in Azure Functions was only possible with event streams like Azure Event Hubs, but today I want to show how you can preserve order for Service Bus queues and topics as well.
On the surface it seems pretty straight-forward: I want to be able to process messages from a queue in the exact order that I received them. For a simple service running on a machine, it’s pretty easy to achieve. However, how do I preserve the ordering of queue messages when I want to process at scale? With something like Azure Functions I may be processing messages across dozens of active instances, how can I preserve ordering?
Let’s use a simple example of a messaging system that deals with patients at a hospital. Imagine I have a few events for each patient:
I want to make sure I never process a message out of order and potentially discharge a patient before I’ve processed their treatment!
Let’s run some quick experiments to see what happens. For this I’m going to simulate 1000 patients each sending these 4 messages (in order) and processing them (ideally in order as well).
Let’s try this with a simple Azure Function that just triggers on a queue. I’m not going to do anything special, just trigger on the queue and push the operation it’s processing to a list on Redis Cache.
After sending 1000 patients worth of data (4 messages each) to this queue, what does the Redis Cache look like after processing? Well some of the patients look great. When I lookup Patient #4 I see:
Great! All 4 events were sent for Patient 4, and got processed in order. But if I look at patient 2:
In this case it didn’t finish processing the “patient arrives” message until after 2 other messages had already been processed. So what happened here? Azure Service Bus does guarantee ordering, so why are my messages out of order?
Well by default, the queue trigger will do a few things. First, for every instance that spins up, it will process a set of messages concurrently. By default an instance concurrently processes 32 messages. That means it may be processing all 4 messages for a patient at the same time and they finish in different order than they were sent. Well that seems easy enough to fix, let’s just limit the concurrency to 1.
Here’s maybe the most common solution to the above problem that I see. Let’s limit the concurrency to only process 1 message at a time instead of 32. For that I modify my host.json file and set the maxConcurrentCalls to 1. Now each instance will only process 1 message at a time. I run the same test again.
First off, it’s super slow. It takes me a long time to chew through the 4000 queue messages because each instance only processes 1 at a time. And worse yet? When I check the results afterwards, some of the patients are still out of order! What’s going on here? Even though I limited the instance concurrency to 1, Azure Functions has scaled me out to multiple instances. So if I have 20 function app instances that have scaled, I have 20 messages being processed concurrently (1 per instance). That means I still get into a spot where messages from the same patient could be processed at the same time — just on different instances. I’m still not guaranteed ordered processing.
The fix here? Many people want to limit the scale out of Azure Functions. While it’s technically possible, it would hurt my throughput even more. Now only one message globally could be processed at a time, meaning during high traffic I’m going to get a large backlog of patient events that my function may not be able to keep up with.
Wouldn’t this be such a sad blog post if I ended it here? There is a better way! Previously I would have said your best bet here may be to use Event Hubs which, because of partitions and batches, you can guarantee ordering. The challenge here though is that sometimes a queue is the right message broker for the job given its transactional qualities like retries and deadlettering. And now you can use queues and get ordering with Service Bus sessions 🎉.
So what are sessions? Sessions enable you to set an identifier for a group of messages. In order to process messages from a session, you first have to “lock” the session. You can then start to process each message from the session individually (using the same lock / complete semantics of a regular queue). The benefit of sessions is it enables you to preserve order even when processing at high scale across multiple instances. Think of before where we had something like 20 Azure Function app instances all competing for the same queue. Rather than not scaling to 20, now all 20 instances each will “lock” its own available session and only process events from that session. Sessions also ensure that messages from a session are processed in order.
Sessions can be dynamically created at any time. An instance of Azure Functions spins up and first asks “are there any messages that have a session ID that hasn’t been locked?” If so, it locks the session and starts processing in order. When a session no longer has any available messages, Azure Functions will release the lock and move on to the next available session. No message will be processed without first having to lock the session the message belongs to.
For our example above, I’m going to send the same 4000 messages (4 patient events for 1000 patients). In this case, I’m going to set the patient ID as the session ID. Each Azure Functions instance will acquire a lock on a session (patient), process any messages that are available, and then move on to another patient that has messages available.
Sessions are currently available in the Microsoft.Azure.WebJobs.Extensions.ServiceBus extension using version >= 3.1.0, and at the time of writing this is in preview. So first I'll pull in the extension.
And then make the tiniest code change to my function code to enable sessions ( isSessionsEnabled = true):
I also need to make sure I’m using a session-enabled queue or topic.
And when I push the messages to the queue, I’ll set the right sessionId for each patient message I send.
After publishing the function I push the 4000 messages. The queue gets drained pretty quickly, because I’m able to process multiple sessions concurrently across scaled-out instances. After running the test I check Redis Cache. As expected, I see all messages were processed, and for every single patient I see they were processed in order:
So with the new Azure Functions support for sessions, I can process messages from a Service Bus queue or topic in order without having to sacrifice on overall throughput. I can dynamically add messages to a new or existing session, and have confidence that messages in a session will be processed in the order they are received by service bus.
You can see the full sample I used for testing and loading messages in my GitHub repo. The master branch will be all in order, and the out-of-order branch is the default and out of order experiment.
Originally published at https://dev.to on May 30, 2019.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
See all (133)
234 
3
234 claps
234 
3
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@borakasmer/parse-website-with-go-for-processing-the-azure-net-core-web-service-result-in-microservice-a145b3b98c9c?source=search_post---------113,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bora Kaşmer
Sep 18, 2020·10 min read
This article will perform some operations on the data of products sold on an online shopping site and save them to the DB.
“Getting information off the internet is like taking a drink from a firehose.” — Mitch Kapor
We will get new product data with different currency prices by using .Net Core web API. But we will convert all product prices to ₺ currency and save on to the SqlDB. So we will parse a webpage and get Dolar, Euro, and British Pound currency value from the HTML and convert it to the ₺ Turkish Liras with Go lang. For all these processes, we will use Microservices. For improving the performance, we will use the Redis Cache. If there is no any Currency data in Redis, we will put all product data that we get from the .Net Core service to the RabbbitMQ. After all, we will get every product data one by one from the consumer and convert all product’s price currency to ₺ Turkish Liras with Go lang. We will save current currency data to Redis for one minute and insert all product data with the created date to the SQL DB.
Create a WebApi “GoService”:
Add RabbitMQ and Newtonsoft package.
Models/Exchange.cs: This is sold product data model.
With this Insert() Action, we will get Exchange data, connect to local RabbitMQ with 78.217. This is my modem IP. And we will put it on the one channel. We will use the “test” for the username and password from the 1881 port. We will connect to RabbitMQ like this because we will try to reach my local RabbitMQ from the remote Azure server. Because in the end, we will publish this .Net Core Web Service to the Azure.
We will put the data to the RabbitMQ because we will perform some operations on the data, and it takes so long. So this process must work behind the current operations. We will create a “product” channel on RabbitMQ and put this data on it to takes and process it by the consumer. “[SwaggerOperation(Summary = “ExchangeType is an Enum.”: We declare columns of Exchange parameter with this SwaggerOperation Attribute[].
Controller/ExchangeController.cs:
Let’s declare to config on Startup.cs:
Startup.cs: We will declare the swagger document for this service. So we add service.AddSwaggerGen() and we will add swagger declaration with c.EnableAnnotations() method. The swagger declaration is soo important. In this application, we declare Enum values of ExchangeType like above for the Insert() method.
webParserGo.go: This is our main page. Firstly we will get all the products list from the SQL. So we will create a “sql.go” file and import it to “webParser.go.” We will download the “mssqldb” library from GitHub as below.
“BIG problems are best solved in small pieces.”
— Frank Sonnenberg
The packaging is everything for the GoLang. As the clean code said, we must distribute the packages by its duty with folders for readability, test, and debugging.
We will take the product from the queue, convert the price to Turkish Liras ₺, and save it to the SQL server.
“db.Prepare” is a prepared statement that is bound to a single database connection. The typical flow is that the client sends a SQL statement with placeholders to the server for preparation; the server responds with a statement ID. In this example, we will prepare an SQL query with Products properties placeholder with “@p1,@p2..” And finally, we will return inserted @@Identity “SCOPE_IDENTITY()”
. “context.WithTimeout()”: We will set a one-minute timeout for SQL query..”defer cancel() & defer stmt.close()”: It is so important for memory management..”rows := stmt.QueryRowContext(ctx, product.Name, product.Price,..”: We will set the insert product parameter and get inserted ProductID(@@identity).
webParserGo.go(Sql):
For using Sql in Go, we will get belowe library.
We will open the SQL-DB as below. And with the “defer” keyword, we will close it when everything is finished about this DB.
This file is used for the global declaration.
“Your website is the window of your business. Keep it fresh, keep it exciting.” —Jay Conrad Levinson
We will parse “https://doviz.com” for getting exchanges value because there is no endpoint to calculate the Turkish lira equivalent of the incoming product price.
We will use “PuerkitBio/goquery” for the parsing a website.
We will not parse the web page for every product insert. It is insane and not safe for performance. So what will we do? We will keep the above exchangeList into the Redis for one minute.
We will use go-redis/redis library for using Redis in Go.
All main processes are calling from here. This is our microservice. We will use “streadway/amqp” for the consuming data from the RabbitMQ. And we will import all packages like “redis,sql,parser,shared.”
We will get the final all product data from the SQL.
We will publish the .Net Core WebService to the cloud, and we will call swagger from the Azure.
We will create a new App Service on Azure. We will go to http://portal.azure.com on the browser. We will set Subscription, Resource Group, which we created before, Web App unique name (rmqservices), Running platform(.Net Core3.1), and finally, Region.
The next step is downloading the publish profile. We will use this profile for publishing the .Net Core service to the Azure on Visual Studio. And that’s all.
We will right-click the project and click the Publish item. We will select publish Profile, and finally, we will Import Profile, which we downloaded from the Azure. After all, we will publish the .Net Core project to Azure “rmqservices.azurewebsites.net.”
This beloved’s code is Azure .Net Core RabbitMQ config. The hostName is my fake modem IP, and the external Port is 1881.
We have to make some arrangements for our modem for redirection from our modem to the Local RabbitMQ machine port. When we insert a product with the swagger on Azure .Net Core WebApi service, this package will come to my ModemIP and 1881 port. We will redirect to this request to 192.168.1.1, which is RabbitMQ Pc IP and, of course, port 5672, which is RabbitMQ port as bellow picture. After all, this inserted product data will subscribe to the product channel on RabbitMQ. And local Go service will consume it and start all the process. All Setup is done.
In this application, we post new product data from .Net Core WebService with swagger. All new products’ currency could be changed every request. And when we insert every product, we convert the product’s price to Turkish price currency. There is no service for the getting exchange rate while converting the Product’s Price to Turkish currency. So we parsed an exchange web site for getting rates. We used microservices technologies because of this long process. Microservice allows us to work together with different technologies. Go is a powerful programming language. The packaging is so important for clean code in Golang. So We distributed all packages according to their duty. Redis, SQL, Parser, Shared all the packages are used in the “consumer.go” file. We published the .Net Core Project to Azure and redirected from the external 1881 port to the internal local RabbitMQ machine with 5672 port on the local Modem Port Forwarding setup. So we could process locally consumed product data in Go, which is subscribed by remote Azure Service.
I hope this article has given you a different perspective on how different technologies work together.
“If you have read so far, first of all, thank you for your patience and support. I welcome all of you to my blog for more!”
Source: kb.objectrocket.com, https://github.com/masnun/gopher-and-rabbit, jenicaandpatrick.com, bsilverstrim.blogspot.com, github.com/PuerkitoBio/goquery
Source Code: https://github.com/borakasmer/GoWebParser
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
See all (22)
132 
2
132 claps
132 
2
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
About
Write
Help
Legal
Get the Medium app
"
https://blog.bankex.org/bankex-microsoft-azure-setup-overview-facbbadf4b18?source=search_post---------114,"What’s Microsoft Azure?
It’s a set of services which you can utilize to assemble your solution.
Why Blockchain as a Service from Azure?
As an open, flexible, and scalable platform, Azure supports a rapidly growing number of distributed ledger technologies that address specific business and technical requirements for security, performance, and operational processes.
The intelligence services, like Cortana Intelligence, provide unique data management and analysis capabilities that no other platform is able to offer. And the vast Microsoft partner ecosystem extends the capabilities of our platforms and services in a way to fit specific roles and industry needs.
Blockchain as a Service (BaaS) provides a rapid, low-cost, low-risk, and fail-fast platform for organizations to collaborate together by experimenting with new business processes — backed by a cloud platform with the largest compliance portfolio in the industry. (taken from: https://azure.microsoft.com/en-us/solutions/blockchain/)
How does BANKEX use Azure?
In the case of BANKEX, we use the 5 most powerful services provided by Microsoft Azure:
Azure Active Directory is a IaaS (identity-as-a-service) which currently provides and manages access of our team to services that are currently available on Microsoft Azure.
Storage Account is a durable, robust and scalable storage solution which allows us to store all the essences that are deployed within our solution.
App Service Plan is a powerful tool that allows us to host our publications. In fact, it’s a set of virtual machines which allows us to deploy and manage easily the web solutions that we build at BANKEX. It can also protect our platform from DDos attacks using the Web Application FireWall.
Application Insights allows us to monitor the way our solutions are currently performing and based on these parameters we can decide if developers need to intervene. Developers can also see if any error have occurred or exceptions have been thrown. This information is given to us before the customer can see the issue.
Visual Studio team services is a code management tool which allows to encapsulate all the artefacts and modifications that happen during the lifecycle of our solution.
We host our public solutions on Visual Studio Team Services. We utilize modern dev-op solutions in order to organize a continuous delivery and integration of our solutions. This way, we want to achieve the shortest time possible between development and deployment of them.
Our solutions can be broken down into parts: private and public.
The public part is primarily smart contracts on Ethereum that can be found on our BANKEX GitHub (https://github.com/BankEx).
Microsoft Azure allows us to firstly deploy our smart contracts on the TestNet and then once all the tests are successful it can be then deployed on the Ethereum production net.
Our private part is written on Node.js and Angular and it is also deployed on Azure with all the benefits of continuous integration and delivery.
Video explaining how BANKEX uses Microsoft Azure services:
BANKEX is available at:
Website — Telegram — Twitter — Facebook— Youtube — LinkedIn — Reddit — GitHub — Steemit
BANKEX is Bank-as-a-Service on blockchain, building the…
799 
799 claps
799 
Written by
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
BANKEX is Bank-as-a-Service on blockchain, building the Proof-of-asset-Protocol. www.bankex.com
Written by
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
BANKEX is Bank-as-a-Service on blockchain, building the Proof-of-asset-Protocol. www.bankex.com
"
https://medium.com/awesome-azure/azure-difference-between-azure-storage-queue-and-service-bus-queue-azure-queue-storage-vs-servicebus-3f7921b0159e?source=search_post---------115,"There are currently no responses for this story.
Be the first to respond.
Comparison — Azure Storage Queue vs Azure Service Bus Queue
Storage Queue is a simple message queuing service to store large numbers of messages.Service Bus Queue is part of a broader messaging service that supports queuing, publish/subscribe, and more advanced integration patterns.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-create-different-environments-on-azure-22331d11ea11?source=search_post---------116,"There are currently no responses for this story.
Be the first to respond.
Every application needs different environments for different purposes and each application needs at least 3 environments. For example, we need a Development environment for the developers to push the code and test it themselves, a QA environment for testers to test the app before we…
"
https://towardsdatascience.com/building-snowpipe-on-azure-blob-storage-using-azure-portal-web-ui-for-snowflake-data-warehouse-f0cdd7997250?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Dec 14, 2019·10 min read
Snowpipe is a built-in data ingestion mechanism of Snowflake Data Warehouse. It is able to monitor and automatically pick up flat files from cloud storage (e.g. Azure Blob Storage, Amazon S3) and use the “COPY INTO” SQL command to load the data into a Snowflake table.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/iotforall/does-microsofts-azure-iot-edge-live-up-to-the-hype-6086dd5f7d5b?source=search_post---------118,"There are currently no responses for this story.
Be the first to respond.
Microsoft’s Azure IoT Edge was announced at Build 2017 to much fanfare. The idea goes that the edges of a network are underutilized and devices are unnecessarily chatty with the cloud. Wouldn’t it be great if you could move some of the work being done in the cloud down to your device? That’s where Microsoft’s Azure IoT Edge comes in.
The example given at the Build highlighted work done with Sandvik Coromant, a manufacturing tools company that creates multi-million dollar machines. Before IoT Edge, Sandvik Coromant utilized the Azure’s Stream Analytics and Machine Learning platforms to predict disasters and shut down machines if one seemed pending.
There’s only one problem: communication to the cloud introduced a two-second latency and time is of the essence when it comes to an emergency shutdown. By utilizing IoT Edge, Sandvik Coromant was able to take the same logic used in the cloud and move it to the edge, reducing latency to 1/10th of a second.
There are many potential advantages to moving compute from the cloud to the edge; reducing latency is but one. Reducing bandwidth costs, simplifying development by using the same programming models in the cloud as on your devices, allowing the use of Azure’s advanced cloud platform locally, the list goes on. For some work, IoT Edge seems like a fantastic tool.
But does reality live up to the hype? Let’s delve in a little deeper.
After some investigation, I was surprised to find that Azure IoT Edge was not a completely new product. It’s a new name to reflect all of the new features that have been packed into an existing product, Azure IoT Gateway SDK.
Although I felt a little duped at first, I understand Microsoft’s reasoning. If anything else, Azure IoT Edge is a sexier name, so let’s just roll with it. Azure IoT Edge does everything that Azure IoT Gateway SDK did, and more. So other than moving compute from the cloud, what does this thing do?
Azure IoT Edge’s primary objective is connecting existing industrial devices to the cloud and each other; making previously dumb machines smarter and more connected.
The GitHub repository for Azure IoT Edge was my starting point. The sample apps that are linked to in the ReadMe seemed especially useful. Since Azure IoT Edge runs on both Linux and Windows and is able to run JavaScript, Java, and .NET modules, Microsoft is making an effort to reach out to developers of all stripes.
As a C#-loving .NET developer at heart, I decided to grab the latest and greatest .NET Standard module sample to get started. After following the instructions to download the sample app, I attempted to run it to no avail. “The debug executable…specified in the debug profile does not exist.”
Huh?
As a software developer, this type of curveball happens a lot. I’m not married to .NET, so I just ran the JavaScript sample. Lo and behold, it worked! Well, running on my local machine logging faked data anyway.
Hooray! I had a (fake) sensor throwing some stuff at IoT Edge. But really, what was that doing? There was no cloud involved, no models I had created in Azure and put on my local device. I had to dig deeper.
Taking a different tack to learn a bit more, rather than running the standalone samples I decided to get the source for IoT Edge. Interestingly, the source for IoT Edge itself contains additional samples. I would have liked to see the main GitHub repository contain the source of IoT Edge without any samples, or just lump everything together into one repo. Having multiple repos with samples confuses the issue, I think.
I found the Microsoft documentation on Azure IoT Edge helped clarify things and I was able to getting up and running with the “Hello World” sample.
I installed the prerequisites as instructed to build IoT Edge from source, which includes installing CMake and Python 2.7 and making sure the folders in which they reside were added to the PATH environment variable.
Then, I ran the command tools\build.cmd --disable-native-remote-modules to build the sample in the DeveloperCommand Prompt, opened through Visual Studio. Don’t be like me; make sure you have “Desktop environment with C++” installed through the Visual Studio Installer even if that’s not your development bread-and-butter. You’ll need that.
After digging into the documentation a bit, I understood a bit more about how this thing works. You can imagine it as a mini operating system, with “modules” standing in as apps.
Some modules are pre-written and do common gateway functions, but you can create your own custom modules using various languages (C#, Java, JavaScript, and C). These modules ingest data from your edge devices, make some logical decisions, and communicate with other modules and the cloud. This all runs on top of a software abstraction that can live in a Linux or Windows environment.
Following the instructions to create the “Hello World” sample, I found it utilizes a JSON configuration file. In this file, you indicate the modules you want to use and their entry point as well as any static arguments you will be needing.
It seems like a straightforward system of setting up configuration variables, especially useful for tying together a bunch of modules you don’t own. For the “Hello World” sample, it uses two modules: “Hello World” and “Logger.”
The idea is that the Hello World module just prints some messages to the Logger, whose only role is to output these messages into a file. Extrapolating out, it’s obvious the Logger module could be used by any other module you decide to create.
To run the sample, all one needs to do is enter samples\hello_world\Debug\hello_world_sample.exe ..\samples\hello_world\src\hello_world_win.json into any old command prompt. This starts the sample executable and points it to use the included default JSON configuration file. Then, Azure IoT Edge starts, runs the modules, and starts logging a “Hello World” message to the file specified in the JSON config file every five seconds.
Can we edit this module to do our bidding?
The out-of-the-box configuration writes to a iot-edge/build/log.txt file. I wanted to test that changing the configuration file (located at iot-edge/samples/hello_world/src/hello_world_win.json) would change the output. After updating it to write to a .json file, it was in JSON format after all, I ran the Hello World sample. Yep, it logs to a bonafide JSON file now!
Seeing the promise of Azure IoT Edge and getting a handle on how to start and run a sample application is a start, but I want to dive deeper. In Part Two I’ll be exploring how to connect IoT Edge to the cloud and run on an actual device.
Originally published at iotforall.com
Expert analysis, simple explanations, and the latest…
233 
2
233 claps
233 
2
Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
Written by

Expert analysis, simple explanations, and the latest advances in IoT, AR/VR/MR, AI & ML and beyond! To publish with us please email: contribute@iotforall.com
"
https://medium.com/@pjbgf/azure-kubernetes-service-aks-pulling-private-container-images-from-azure-container-registry-acr-9c3e0a0a13f2?source=search_post---------119,"Sign in
There are currently no responses for this story.
Be the first to respond.
Paulo Gomes
Nov 21, 2017·4 min read
— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — Updated on Jan/2018: Another way of achieving the same results is to ensure the service principal used by your AKS server is on the Reader role of your ACR, in which case you won’t need to create a cluster secret. More information on this can be found here. — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
Microsoft has just recently released a preview of AKS, its Managed Kubernetes offering which is a response to GKE (Google Kubernetes Engine). It is still early days but it does sound great to have that on Azure — that is, when you don’t bump into one of the known issues.
The only problem is that all examples I found so far only use public container images from Docker Hub, which is most probably not what you would use for your own projects in production. So here I will go through a more “real life” scenario, based on the Microsoft walkthrough example, but provisioning the cluster, the registry and deploying a container based on a private container image.
In order to proceed you will need to have Azure CLI 2 installed, so if you haven’t done that yet, here is how you can do it. Also make sure you have logged in on azure with the CLI command az login.
Let’s start by provisioning the container registry:
Open the Azure Portal in your browser and look for all your Container Repositories. Select the one you have just created, then go to its Settings and Access Keys. Enable Amin user and copy the password.
Download Microsoft’s sample container image and tag it with the ACR address we have just created:
When you do this for your own images, just make sure you prefix them with your full registry address. Once that is done, push it onto your private registry:
If you don’t have a cluster already, create one:
Make sure that the location specified above is one of the supported ones on the preview. This bit has been quite unstable since the launch, with some regions simply not accepting new clusters after a few days. So if you start getting to many issues with your cluster, do go to the AKS GitHub account and check whether is there any known issue or existing work around for your problem.
Install the AKS CLI:
Download the configuration settings and set the current context to connect to your kube cluster:
You are almost all set now. You just need to create a “docker-registry” secret in the cluster, which you can then use in your yml file:
Create a file named azure-vote.yml with the contents below. Make sure you use the correct image name and secret that we have previously defined.
Deploy the application into your cluster:
During the deployment process the cluster will use the secret to connect to the private registry. To confirm all worked properly, just fire the commandkubectl get pods. The result will show all the current pods in the cluster and their respective statuses.
That’s it! All done. Hopefully this also works for you and you have saved a few bangs of your head against the wall. :)
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
179 
3
179 
179 
3
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
"
https://towardsdatascience.com/how-to-fail-the-azure-fundamentals-certification-e37a650a251e?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Briggs
Sep 16, 2020·4 min read
AZ-900 is the most popular certification for the (almost) most popular Cloud service in the world. Azure Fundamentals acts as the starting point for many people working with the Cloud.
"
https://medium.com/devopsturkiye/azure-devops-%C3%BCzerinde-uygulama-yay%C4%B1nlama-2d37ba7720b5?source=search_post---------121,"Sign in
There are currently no responses for this story.
Be the first to respond.
Abdulkerim Karaman
Feb 11, 2019·3 min read
Bildiğiniz gibi Azure, Microsoft tarafında bulut teknolojilerinin çatısını oluşturmaktadır. Bu makalemizde Azure platformunun sunduğu DevOps yapısını inceleyeceğiz.
Hesabınız yoksa https://azure.microsoft.com/tr-tr/services/devops/ adresine giderek ücretsiz hesap oluşturabilirsiniz. Azure DevOps altyapısını daha iyi anlamak için bir uygulama oluşturarak deploy edeceğiz.
Örnek bir .Net Core uygulaması oluşturalım.
Yeni bir product controller oluşturalım ve içine data girelim.
ProductController:
Product:
Uygulamayı çalıştırarak test edelim.
http://localhost:5000/api/product url üzerinden uygulamaya erişebilirsiniz.
Github üzerinde bir proje oluşturarak kodlarımızı atalım.
Evet uygulama kodlarımız hazır. Şimdi https://azure.microsoft.com/tr-tr/services/devops/ adresine giderek yeni bir proje oluşturalım.
Projemizi oluşturduktan sonra bizi aşağıdaki gibi bir proje yönetim ekranı karşılayacak.
Evet sol tarafda ki Repos menüsü altından, isterseniz azure ‘un sunmuş olduğu repository kullanabilirsiniz. Fakat biz yine aynı menü altından bulunan “import repo” kullanarak kendi github adresimizi vereceğiz.
Soruce alanından git seçerek github proje adresimizi yazalım. Import dediğinizde aşağıdaki adres ile karşılaştıysanız tamamdır :)
Pipeline altında builds sekmesine girerek yeni bir build paketi tanımlıyoruz. Github repo seçerek proje kodlarımızı gösteriyoruz.
Ardından proje tipini belirtiyoruz.
Seçtikten sonra açılan ekrandan build aşamlarını oluşturuyoruz. Default olarak Restore, build, test, publish aşamaları geliyor. Sadece publish aşamasında yer alan “publish web project” checkbox’ını kaldırıyoruz.
İşlemi tamamladıktan sonra builds menüsü altına gelen paketi çalıştırdığımızda başarıyla build olduğunu gözlemliyoruz.
İlgili build paketi üzerinde edit’e tıklayarak triggers sekmesi altından her push işleminin ardından bu build paketinin otomatik çalışmasını sağlayabileceğiniz gibi, yine bu sekme altından zamanlayıcı tanımlayarak belli zamanlarda bu işlemi yapmasını sağlayabilirsiniz.
Evet sıra geldi Release paketi oluşturmaya bunun için sol menüden Release sekmesine girelim.
Karşıma gelen ekrandan Azure service deployment’ı seçelim.
Bu aşamada Azure Portal tarafında daha önce oluşturduğumuz bir web app uygulamamızı seçeceğiz. Bu aşamayı https://portal.azure.com üzerinden kolayca yapabilirsiniz.
Evet ilgili app’i de seçtikten sonra oluşturduğumuz Release paketi çalıştıralım.
Evet tüm agent ’larımız başarılı bir şekilde çalıştı. Oluşturduğumuz ve deploy ettiğimiz uygulamaya aşağıdaki url üzerinden ulaşabilirsiniz.
https://productapi.azurewebsites.net/api/product
GitHub Repo:
https://github.com/akaramanapp/ProductApi
Evet oluşturduğumuz bu pipeline sayesinde github üzerine atacağımız her kodun ardından, ilgili agent’lar çalışarak otomatik deploy işlemi yapılmış olacak.
Yararlı olması dileğiyle…
Full Stack Developer
81 
81 
81 
Medium independent DevOps publication. Join thousands of aspiring developers and DevOps enthusiasts
"
https://medium.com/@selcukusta/azure-functions-ile-slack-bot-yap%C4%B1m%C4%B1-8305cdcaacda?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Selçuk Usta
Jan 21, 2018·7 min read
IT dünyasının son dönemde merkezinde yer alan bir kavramın — Serverless Architecture — tasarım ve uygulamasında kullanılan Azure Functions ile (ki bir dönem WebJobs olarak adlandırılan bir yapı idi) küçük bir uygulama yaparak sistemin mantığını çözebilmek derdimiz.
Ne yazık ki Türkçe karşılığını tam olarak ifade edemediğim bir kavram Serverless Architecture. Sunucusuz mimari olarak düz bir çeviri yapsak içime sinmiyor çünkü en nihayetinde bulut ortamda olsa dahi bir sunucu ortamı mevcut. Tüm bu sunucu bakım ve zaman maliyetlerinin minimize edildiği desek farklı bulut çözümleri ile karışıyor. Tam bu noktada Functions as a Service (FaaS) kavramı kurtarıcı olarak hayatımıza giriyor. Birim işlemlerin, event-based uygulamalarla sunucu yönetimine gerek olmaksızın fonksiyon seviyesinde karşılanması ve işlenmesi yöntemi ile bu mimariyi canlandırabiliyoruz.
Yazının ana konusuna girmeden hemen önce yukarıda bahsettiğimiz kavramın teknik detaylarına dair çok değerli bulduğum üç referansı buraya bırakmak istiyorum:
Ekip olarak yemek yemeyi çok — fazlaca çok — seviyoruz. Öğlen yemekleri için yaptığımız gurme kaçamakları organizasyonlarımız da meşhurdur :)
Hızlıca organize olabilmek için bir mail başlatıyoruz ve katılanlar, katılmayanlar +-1 ile dönüş yapıyorlar. Organizatör gurmemiz ise son listeyi toplayıp ona göre planlama yapıyor.
Düşündüm ki “Slack” üzerinde bir kanalımız olsa; ismi “Gurmecikler”. Slack çalışma grubumuzun da bir bot’u olsa (Gurmetör) ve Gurmecikler kanalına üye tüm arkadaşlara özelden “Bu öğlen yemeğe gidiyoruz, geliyor musun?” diye mesaj atsa. Katılanları da bir yere yazsa. Sonrasında organizatör arkadaş hiç sayım zahmetine girmeden bu listeden planlamayı gerçekleştirse. Hayat daha kolay — ve lezzetli — olmaz mıydı?
Organizatör genelde ben olmuyorum ama olsaydım bu olay çok hoşuma giderdi diye düşündüm ve kolları sıvadım (yemek için değil, uygulama için :))
Yazının devamındaki uygulamaları çalıştırabilmek için bir takım ön gereksinimlerimiz mevcut. Şöyle ki;
Tüm bunlara sahip olduğumuzu düşünerek yola devam ediyoruz.
Slack üzerinde bir çalışma grubu oluşturduysak bu adrese giderek “Create App” butonu yardımıyla bir uygulama oluşturmamız gerekiyor. Ben uygulamama “Gurmetör” adını verdim.
Sol taraftaki menüden “Bot Users” seçeneğini seçiyor ve uygulamamıza bağlı bir bot oluşturuyoruz. Display Name ve Default Username özelliğini dilersek değiştirebiliriz. Bu isimler Slack ekranında görünecektir. Default Username özelliği Türkçe karakter barındıramayacağı için bunu gurmetor olarak değiştiriyoruz.
Sırada uygulamaya tanımlı yetkiler var. Bunun için yine sol menüden “OAuth & Permissions” seçeneğini seçiyoruz ve Scopes alt başlığından yetki tanımlarını gerçekleştiriyoruz. Bu botu bir kanala üye olmuş kullanıcıların listesini çekerken kullanacağımız için channels:read yetkisi ile ve bu kullanıcılara mesaj atabileceği için chat:write:bot yetkisi ile sarmalıyoruz.
Yetkilerle ilgili en önemli adım, programatik olarak tüm bu işlemleri yapacağımız için bir Token’a ihtiyaç duyuyoruz ve bunu üreteceğiz. Bunun için yine aynı ekrandaki en üst bölümde yer alan Tokens for Your Workspace bölümüne çıkıyoruz ve “Install App” butonu ile uygulamamızı çalışma grubumuza ekliyoruz. Sonrasında bu alanda iki token görülecek. Biz bot olarak devam edeceğimiz için Bot User OAuth Access Token altındaki değeri bir yere not alıyoruz.
Son yapmamız gereken işlem ise uygulamamızda kullanacağımız özelliği devreye almak. Bunun için Slack bir çok özellik sunuyor (interaktif menü, interaktif mesaj, slaş komutları, vs…). Bizim uygulamamız ise kullanıcıya bir soru sorup butonlar yardımıyla cevap alacağı için sol menüden “Interactive Components”i seçiyor ve aktif hale getiriyoruz:
Bizden iki özellik istiyor — biri zorunlu olmak üzere — :
Request URL: Uygulamamız, kullanıcıdan gelen cevabı bir webhook yardımıyla burada verdiğimiz adrese POST edecek.
Options Load URL (for Message Menus): Mesaj menüleri, buton içerikleri gibi alanları dinamik yönetmek istersek bu alana bir adres tanımlaması yapabilir ve uygulamanın, içerikleri bu alandan çekmesini sağlayabiliriz.
Tahmin ettiğiniz üzere bizim doldurmamız gereken alan Request URL. Buradan bir event fırlayacak ve biz bu event’i yakalayıp bir fonksiyon aracılığıyla yorumlayacağız, işlemler yapacağız ve kullanıcıya cevap döneceğiz. Bu adrese sahip olmak ve fonksiyonumuzu geliştirmek üzere Azure’a gidiyoruz.
Azure üzerinde iki uygulama kullanacağız.
Bunlardan ilki Azure Functions. Görevi; Slack’ten gelen isteği karşılamak, kullanıcı cevabını yorumlamak ve kullanıcıya uygun cevabı dönmek.
Diğeri ise Redis Cache. Görevi; Azure Functions üzerinden aldığımız kullanıcı cevabını saklamak.
Redis Cache kurulumu oldukça basit. Yapılması gereken Azure portal üzerinden New butonuna basarak açılan pencereye Redis Cache yazmak ve sunulan tek ürünün kurulumunu en maliyetsiz haliyle gerçekleştirmek.
Kurulum bittikten sonra Redis’e ulaşmak için gerekli olan bağlantı cümlesini temin etmek üzere ana ekran üzerindeki Show Access Keys linkine tıklıyoruz.
Açılan yeni penceredeki Primary connection string (StackExchange.Redis) başlığı altında yer alan bağlantı cümlesini bir yere saklıyoruz.
Sırada uygulamanın kalbi, Azure Functions kurulumu var. Bunu Azure portali üzerinden kurabileceğimiz gibi Visual Studio 2017 üzerinden de Publish Profile ile kolaylıkla gerçekleştirebiliriz. Ben de bu şekilde devam ediyorum. Öncelikle yeni bir proje oluşturuyorum:
Sonrasında ise açılan projeme sağ tıklayarak Add > New Azure Function diyor ve ismini MessagingFunction adını verdiğim fonksiyonu ekliyorum.
Uygulamanın tamamına aşağıdaki repodan ulaşmanız mümkün. Ben tıpkı diğer yazılarda olduğu gibi ana fonksiyona yoğunlaşacağım.
Kodu incelemeden hemen önce belirtmem gereken nokta şu ki; Slack payload body parametresi ile application/x-www-form-urlencoded tipinde aşağıdaki gibi bir istek yolluyor:
Ancak Azure Functions mevcut sürümünde application/x-www-form-urlencoded tipine özel bir kabul gerçekleştiremiyor. Dolayısıyla kodda yer alan 6. satırdaki ReadAsStringAsync ifadesini ReadAsFormDataAsync olarak değiştiremiyoruz. Bu sebeple gelen içeriği olduğu gibi metinsel bir şekilde okuyup payload= ifadesinden sonraki json ifadeyi alıyor ve Newtonsoft.Json kütüphanesi ile işliyoruz.
Kodun geri kalan kısmında ise gelen cevaptaki kullanıcı seçimini (actions[0].value) ve kullanıcı adını (user.name) alıyor, Redis’e yazma işlemini gerçekleştiriyor ve kullanıcıya; seçimine göre bir cevap dönüyoruz.
Azure Functions üzerindeki hassas verileri (appsettings ve connectionstrings olarak özelleştirebiliriz) kod üzerinde tutmak yerine oluşturduğumuz Function’a özel Application Settings alanında tutmamız mantıklı:
Tabii henüz yukarıdaki pencereyi görebilecek duruma gelemedik. Bunun için Azure üzerinde Azure Functions servisimizin kurulu olması gerekli. Yukarıda da bahsettiğim üzere bunun için Publish Profile kullanacağız. Yapmamız gereken projemize sağ tıklayarak “Publish” butonuna basmak.
Açılan penceredeki sağ üst köşede Azure hesabımızla ilişkilendirilmiş Microsoft hesabımızı görmemiz gerekli. Eğer göremiyorsak giriş yapmamız yeterli olacaktır. Sonrasında ise tüm alanlar otomatik olarak doluyor ve bize düşen tek şey “Create” butonuna basmak oluyor.
Tüm işlemler başarıyla gerçekleştikten sonra ise “Publish” butonu yardımıyla uygulamamızı Azure’a yüklüyoruz. Yükleme işlemi başarıyla gerçekleştikten sonra ise iki üst görselde yer alan ekrana ulaşabiliyor olmalıyız. Görselde de göreceğimiz üzere iki anahtarı ekliyoruz (REDIS_CONNECTION ve REDIS_DEFAULT_DB). Bu iki değer, uygulamadaki Redis’e bağlantı gerçekleştiren RedisCacheProvider sınıfı içerisinde kullanılacak.
Son adımda yapmamız gereken ise fonksiyona ulaşacağımız public adresi edinmek. Functions penceresinden ilgili function’ı buluyor ve sağ üst köşedeki “</> Get function URL” linki ile açılan pop-up’dan URL’i kopyalıyoruz.
Kopyaladığımız function URL’ini, tekrar Slack paneline dönerek Request URL olarak gördüğümüz alana yapıştırıyor ve aktif hale getiriyoruz.
Son durumda ihtiyacımız olan 3 sistem de (Slack, Redis, Azure Functions) hazır durumda. Yapmamız gereken son şey, bot kullanıcısı ile istediğimiz kanaldaki üyelerin listesini almak ve her birine mesaj atmak.
Mesaj atma işi için küçük bir Python scripti geliştirelim.
3. satır en önemli kısım. Yazının en başında bahsettiğim ve bir yere not aldığımız Bot User OAuth Access Token değerini buraya yapıştırıyoruz.
İlk fonksiyon (get_users_in_channel) parametre olarak kanal id’si alıyor ve bu kanaldeki kullanıcıların id’lerini döndürüyor.
İkinci fonksiyon ise (send_message) yine parametre olarak kanal id’si alıyor ve o kanala özelleştirilmiş bir mesaj gönderiyor. Bu kanal id’si public bir kanal olabilir ya da private bir kanal olabilir. Ayrıca her kullanıcı id’si, o kullanıcı ile yapılan özel görüşmelerdeki kanalın da id’si olarak ifade ediliyor.
Ana fonksiyonda ise kanaldaki kullanıcılarda dönerek her kullanıcıya mesaj gönderme işlemi gerçekleştiriliyor.
Gif’i kaydederken arka tarafta Python uygulamasını tetikledim, kısa bir süre sonra mesaj geldi ve cevabı verdim. Sonrasında ise Azure Functions tarafından nihai cevap bana ulaştı. Bir de Redis tarafını kontrol edelim dilerseniz:
Burada da gelmeyecek olarak kayıt altına alındığımı görebiliyoruz. Uygulama istenen kıvama gelmiş durumda.
Bu yazının sonuna gelirken karnımın acıktığını söylemem gerekli. Eğer aynı etkiyi yarattıysam özür diliyorum :) Bir sonraki yazıya dek, bot’lardan aldığınız cevaplarınız hep anlamlı olsun.
github.com
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
159 
159 
159 
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
"
https://medium.com/javarevisited/5-best-azure-fundamentals-courses-to-pass-az-900-certification-exam-in-2020-9e602aea035d?source=search_post---------123,"There are currently no responses for this story.
Be the first to respond.
Hello Guys, If you are preparing for AZ-900 or Microsoft Azure Fundamentals exam and looking for some good online courses to pass this exam, then you have come to the right place.
In the past, I have shared the best courses to learn Azure and certifications like Azure Administrator (AZ-103), Azure Developer (AZ-303), and Azure Architect (AZ-303) and In this article, I am going to list down some of the best courses to crack the AZ-900 certification exam.
You may know that Cloud computing is becoming more and more critical, and it’s almost mandatory for both technical and non-tech IT people to know about Cloud computing and different Cloud platforms like AWS, Azure, and GCP.
You just can’t hide with Cloud anymore, you need to learn it to understand and master it to stay relevant in technology jobs. Thankfully there is a lot of learning material available to learn about the benefits of the Cloud and why companies should move to the Cloud.
Still, certifications are probably the best way to develop knowledge, skill, and get the recognition required by many Cloud jobs. If you are just getting started with Cloud Computing and Cloud platforms like Microsoft Azure, then Microsoft Azure Fundamentals (AZ-900) is probably the best certification to build foundational Cloud skills and also get the recognition. This certification is very similar to the AWS Cloud Practitioner certification and primarily designed for candidates looking to demonstrate foundational level knowledge of cloud services and how those services are provided with Microsoft Azure platforms. People, who successfully pass this exam, known as Microsoft Azure Fundamentals or AZ-900, will earn the Microsoft Certified Azure Fundamentals? Certification, which is recognized in many cloud-based jobs. You can also put the certification badge on your LinkedIn profile and your resume to attract recruiters. This exam can be also be taken as an optional first step in learning about cloud services and how those concepts are exemplified by Microsoft Azure. If you happen to attend any of Microsoft training, then you may also get some free vouchers to participate in this certification exam for free.
Here is my list of best courses you can join in cracking the AZ-00 or Microsoft Azure Fundamentals exam and earning a Microsoft Certified Azure Fundamentals certification, but before that let’s take a look at this nice Microsoft Azure Certification RoadMap which is prepared by Whizlabs, one of the leading IT certification test provider.
They also have very good study material and practice tests for Azure certification including AZ-900 (Microsoft Azure Fundamentals exam).
This is one of the best courses on Udemy to prepare for the AZ-900 or Microsoft Azure Fundamentals exam.
This course is perfectly aligned well with the AZ-900 syllabus and covers all the topics well. Scott Duffy, the instructor of this course, is not clear and concise, which makes it easy to digest the concepts. The course also comes with a lot of bonuses like you will get access to a 24-page study guide, and you can even download audio of the course to learn on the go. The best thing is the course is well up-to-date, which is very challenging given constant updates from Azure. It also contains quizzes to reinforce learning.
Here is the link to join this course — AZ-900: Microsoft Azure Fundamentals Exam Prep -2021 Edition
It also contains a 50 questions practice test for final preparation. It’s not sufficient, but you can still take to find your strong and weak areas before the exam. If you are serious, I suggest you combine this course with Whizlab’s AZ-900 practice test for better preparation.
This is a great resource or online course to prepare for Microsoft Azure Fundamentals and AZ -900 Exam on Coursera as this is created by Microsoft itself, which means you will be learning from the source itself.
This Microsoft Azure Fundamentals AZ-900 Exam Prep Specialization consists of four courses that will provide all the fundamental knowledge you need to prepare you for the AZ-900 certification exam and for a career in the cloud computing field.
Here are the key things you will learn in this course :
The best thing about this course is that the content of this Coursera Specialization is tightly aligned to the AZ-900 exam objective domains. This is Ideal for IT personnel just beginning to work with Microsoft Azure or anyone wanting to learn about it.
Talking about social proof the program has an impressive 4.7 ratings from more than 10K people who have trusted this to start their career in Cloud. In short, one of the best Coursera courses for the AZ-900 exam and to learn Cloud computing in Microsoft Azure.
Here is the link to join this course — Microsoft Azure Fundamentals AZ-900 Exam Prep Specialization
By the way, instead of joining these courses and specialization individually, you can also join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
coursera.com
This is one of the best courses to prepare for the Microsoft Azure Fundamentals exam. It contains all you need to know to pass the AZ-900 exam. The content is very hands-on, and you will see the instructor showing you real stuff inside the Microsoft Azure portal, where Microsoft Learn only teach you in text. If you are a beginner in both cloud technologies and Azure, then you will learn a lot more from seeing a real Azure portal than reading on Microsoft Learn documentations. Apart from lectures that cover all the topics for the AZ-900 exam like Cloud Fundamentals, Core Azure Services, Pricing, and Billing, etc., this course also provides AZ-900 practice questions to get you ready for the exam.
Here is the link to join this course — Microsoft Azure — Beginner’s Guide + AZ-900 preparation
At the end of the course, you will get access to two AZ-900 mock tests, containing 74 questions. Since mock tests are crucial to building speed and accuracy, I strongly suggest you go through these tests to improve your speed and accuracy. It will also help you to find your strong and weak areas to focus on before the exam.
Microsoft provides several free learning materials to prepare for their Azure certification, and it’s beneficial to prepare all topics for Microsoft Azure Fundamentals AZ-900 certification. This is a text-based document with some videos, but very well written, accurate, and full of information. If you don’t like reading too much, then you can combine this learning resource with one of the online courses I have shared above. But, if you like reading more than watching online courses, then you will love this text-based training material, which is also completely free of cost. You can take notes easily, and reading is also faster than watching courses, and in most cases, they complement your learning from online courses.
Here is the link to join this course for FREE — Azure fundamentals by Microsoft
This is another comprehensive AZ-900 Course on Udemy to learn both Cloud Computing and Azure Fundamentals. You will not only learn about Cloud basics like differences between Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), and Software-as-a-Service (SaaS) but also core Azure services like Azure computer, storage, network, and security features. This course is taught by Thomas Mitchell, a 25-year IT industry veteran (and Microsoft Certified Trainer), and it is one the most comprehensive AZ-900 exam-prep course available on Udemy. It features 5 hours of video lectures, numerous hands-on demonstrations, slick downloadable infographics, and several quizzes. You will also get access to a 50-question practice test at the end of the course for practice.
Here is the link to join this course — AZ-900 Azure Exam Prep: Microsoft Azure Fundamentals
Talking about social proof, this course is trusted by more than 10,000 students, and it has got on average 4.4 ratings from close to 3000 ratings, which is incredible and speaks a lot about its quality.
Overall, an excellent course to prepare for AZ-900 or Microsoft Azure Fundamental certification exam.
I am a big fan of solving practice questions before appearing for the certification exam, and this single thing has helped me to achieve high scores in most of the exams I have given so far. There is an immense benefit of solving practice questions, and when it comes to choosing the exam simulator or practice test, I always go for Whizlabs, a leader in the IT certification test. They provide quality test questions that mimic real exams in terms of difficulty level and formats. The Microsoft Azure Fundamentals or AZ-900 practice test on Whizlabs is no exception. You will get access to top-quality and unique 275 questions for practice. They are spread out into 5 full-length mock exams. There are an additional 35 questions divided into 7 section tests.
This test costs just $15, and I strongly suggest you buy this if you want to pass the AZ-900 exam on the first attempt. Btw, if you are preparing for multiple certifications or plan to give them in near future, I suggest getting a Whizlabs subscription which provides full access to all of their online training courses and practice tests for different certifications like AWS, Java, Cloud, Docker, and Kubernetes with just $99 per annum (50% discount now).
Her is the link to join this test — Microsoft Azure Fundamentals (AZ-900) — Practice Tests
They also provide a detailed explanation of each and every question so that you know why a particular option is correct, and why others are not correct. This helps to consolidate your learning and remove any misconceptions you have.
Apart from Whizlabs’ practice test, there is also an official Microsoft test which you can buy while scheduling for the exam. They are a bit pricy though and cost around $100. I prefer Whizlabs, which is much cheaper and similar in quality. That’s all about some of the best courses to crack AZ-900: Microsoft Azure Fundamentals certification. This is the first cloud certification in several of Microsoft Azure certifications and provides a nice platform to prepare for more comprehensive and distinguished certifications like AZ-300 Microsoft Azure Solution Architect. It’s both easy to pass, and preparation will equip you with good knowledge of essential cloud technologies and Azure services like compute, storage, networking, security, and price. Since Cloud is essential in today’s world and every company is migrating on Cloud, I strongly recommend this course to all IT professionals like Programmers, BAs, Project Managers, Infra guys, and support people.  Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like these AZ-900 dumps or practice tests, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. — If you are new to the world of Azure Cloud and looking for some free courses to learn Cloud computing then you can also, check out this list of free cloud computing courses with Microsoft Azure in 2021.
medium.com
P. S. S. — And, if you are feeling gracious, you can also purchase my topic-wise AZ-900 Practice Test on Udemy. It contains more than 250+ high-quality questions, divided into 5 topics as per the AZ-900 curriculum, and provides you with comprehensive practice. you can use the tet to find your strong and weak areas and work on them before the actual exam.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
257 
257 claps
257 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@renatogroffe/desenvolvimento-serverless-com-net-core-implementando-sua-primeira-azure-function-5a3898c4cf51?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 7, 2019·5 min read
A adoção de arquiteturas serverless vem ganhando cada vez mais força em projetos de software nos mais variados cenários, com o baixo custo de alternativas como Azure Functions favorecendo em muito esta prática. A possibilidade de se preocupar menos com questões de infraestrutura e mais na entrega de soluções também constitui um fator importante e que merece ser destacado em tais casos.
Na imagem a seguir (print gerado no início de Outubro/2019) podemos ter uma dimensão do baixíssimo preço ao optar pelo uso de Azure Functions:
Desde a versão 2.x das Azure Functions é possível implementar aplicações serverless com .NET Core, o que abre caminho para o desenvolvimento multiplataforma de projetos combinando a codificação com estas tecnologias a partir de ambientes Windows, Linux ou Mac. Uma outra possibilidade seria a criação de Azure Functions através do próprio Portal do Microsoft Azure.
Neste artigo demonstrarei os primeiros passos na construção de soluções serverless fazendo uso para isto do .NET Core e do serviço Azure Functions, com a implementação de uma aplicação no Portal do Azure.
O primeiro passo para a implementação de Azure Functions na nuvem consiste na criação de uma Function App, recurso ao qual poderão estar vinculadas várias funções:
Ao gerar uma nova Function App preencher as seguintes configurações:
E:
Concluir a geração da Function App acionando o botão Create.
Consultando o grupo de recursos Serverless aparecerão a Function App e demais itens criados para esta aplicação:
Acessando a Function App groffefunctions podemos iniciar a implementação de uma nova Azure Function através da opção + New function:
O tipo da Azure Function a ser criada será Webhook + API, o que fará com que o disparo da mesma aconteça mediante o envio de requisições HTTP:
Ao final deste processo aparecerá a função HttpTrigger1, com sua implementação default:
Neste momento muitos se perguntarão: como posso modificar o nome de uma Azure Function criada a partir do Portal?
Acessando o item Console veremos que a Azure Function HttpTrigger1 e seu conteúdo estão organizados como um diretório vinculado à Function App:
Podemos voltar um nível de diretório acima com a instrução cd.., modificando em seguida o nome da função com a instrução:
rename HttpTrigger1 Saudacao
Atualizando a Function App groffefunctions será listada agora a função Saudacao:
Alterar agora o código de Saudacao para o conteúdo da listagem a seguir:
Concluir os ajustes acionando o botão Save. Será iniciada então a compilação da Azure Function, com o resultado deste procedimento aparecendo em Log (a próxima imagem mostra sucesso ao compilar a função):
Para testar a função Saudacao acessar a opção Get function URL:
Aparecerá agora um popup com uma URL que deverá ser copiada acionando o botão Copy:
Um teste via browser com a URL:
https://groffefunctions.azurewebsites.net/api/Saudacao?code=t6WIPHiER0RY0LnxeT2UKAW6WF7zUFBWGKfblNmMeyVoOnO5WahAEQ==&nome=Renato
Produzirá como retorno:
Já um teste via Postman com o envio de um nome no corpo da requisição irá gerar como retorno:
Recentemente aconteceu também uma live no Canal .NET sobre a implementação de soluções Serverless empregando Azure Functions. A gravação está disponível no YouTube e pode ser assistida gratuitamente por todos aqueles interessados em conhecer mais sobre as tecnologias mencionadas neste artigo:
Azure Functions documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
103 
103 claps
103 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-cloud-market-share-2019-what-the-latest-data-shows-dc21f137ff1c?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 12, 2019·5 min read
Q1 earnings are in for the ‘big three’ cloud providers and you know what that means — it’s time for an AWS vs Azure vs Google Cloud market share comparison. Let’s take a look at all three providers side-by-side to see where they stand.
Note: a version of this post was originally published in April 2018. It has been completely rewritten and updated for 2019.
To get a sense of the AWS vs Azure vs Google Cloud market share breakdown, let’s take a look at what each cloud provider’s reports shared.
Amazon reported Amazon Web Services (AWS) sales of $7.7 billion, compared to $5.44 billion at this time last year. AWS revenue grew 41% in the first quarter — at this time last year, that number was 49%.
Across the business, Amazon’s growth rates are slowing down — which perhaps is all that can be expected at their mammoth size. However, their profit margins are increasing, giving investors a boon of $7.09 earnings per share compared to the projected $4.72.
AWS has been a huge contributor to this growth. This quarter, AWS revenue makes up 13% of total Amazon sales, up from 10% in the fourth quarter. AWS only continues to grow, and bolster the retail giant time after time.
In media commentary, AWS’s numbers seem to speak for themselves:
While Amazon breaks out revenue from AWS separately, Microsoft has a more nebulous “commercial cloud business” — which includes not only Azure, but Office 365, Dynamics 365, and other segments of the Productivity and Business Processes Division. This fact frustrates many pundits as it simply can’t be compared directly to AWS, and inevitably raises eyebrows about how Azure is really doing. Microsoft reported that the commercial cloud business grew 41% in the first three months of 2019, to $9.6 billion.
What Microsoft reported for Azure specifically is the growth rate: 73%. However, Microsoft did not specify what that growth actually represents. This time last year, the Azure growth rate was reported at 93%. Supposedly, analysts say that Azure is growing at a faster rate than AWS was at a similar size, but without specific numbers, it’s hard to say what this actually means.
Here are a few headlines on Microsoft’s reporting that caught our attention:
Like Microsoft, Google avoided reporting specific revenue numbers for its cloud business yet again. Parent company Alphabet reported $36.34 billion in revenue for the quarter, up 17% from $31.15 billion for the same quarter last year. Google Cloud Platform revenue is included in Google’s “other” revenue category, alongside G Suite, Google Play, and hardware such as Nest. That category reported revenue of $5.45 billion for the quarter, up 25% from the same quarter last year when it was $4.25 billion.
According to Google and Alphabet CFO Ruth Porat, “Google Cloud Platform remains one of the fastest growing businesses in Alphabet with strong customer momentum reflected in particular in demand for our compute and data analytics products”. But without specifics, it’s hard to say what this means.
Further reading on Google’s quarterly reporting:
When we originally published this blog last year, we included a market share breakdown from analyst Canalys, which reported AWS in the lead owning about a third of the market, Microsoft in second with about 15 percent, and Google sitting around 5 percent.
This year they report an overall growth in the cloud infrastructure market of 42%. By provider, AWS had the biggest sales gain with a $2.3 billion YOY increase, but Canalys reports Azure and Google Cloud with bigger percentage increases.
Ultimately, it seems clear that in the case of AWS vs Azure vs Google Cloud market share — AWS still has the lead.
Bezos has said, “AWS had the unusual advantage of a seven-year head start before facing like-minded competition. As a result, the AWS services are by far the most evolved and most functionality-rich.”
Our anecdotal experience talking to cloud customers often finds that true, and it says something that Microsoft and Google aren’t breaking down their cloud numbers just yet.
Others have made their own estimates. In November, a Goldman Sachs report stated that AWS, Azure, Google Cloud, and Alibaba Cloud made up 56% of the total cloud market, with that projected to grow to 84% this year. The report shows AWS far, far in the lead with 47% of the market projected for this year, with Azure and Google trailing at 22% and 8% market share, respectively.
AWS remains far in the lead for now. With that said, it will be interesting to see how the actual numbers play out, especially as Google positions itself for multi-cloud and Azure continues rapid growth rates. Perhaps this time next year will report revenue numbers broken out and we’ll be able to say for sure.
Originally published at www.parkmycloud.com on April 30, 2019.
CEO of ParkMyCloud
41 
2
41 
41 
2
CEO of ParkMyCloud
"
https://medium.com/@mentormate/the-microsoft-azure-development-practices-you-should-know-ebf28d687e9?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
MentorMate
Feb 23, 2017·14 min read
[Original Post Here]
Cloud computing is the set of technologies and infrastructure capabilities offered in a utility-based consumption model. Microsoft Cloud Computing offers Microsoft Azure, previously known as Windows Azure, to help its customers realize the benefits of cloud computing.
Applying existing development ideologies to Microsoft Azure is not straightforward, as direct one-to-one conversion is rarely possible. How should you alter your methodology and what Microsoft Azure development best practices should you use?
Cloud computing that replaces the traditional on-premise (locally built and deployed) software is broadly divided into three categories:
The development lifecycle of software that uses the Azure platform mainly follows two processes:
During the application development stage the code for Azure applications is most commonly built locally on a developer’s machine. Microsoft has recently added additional services to Azure Apps named Azure Functions. They are a representation of ‘serverless’ computing and allow developers to build application code directly through the Azure portal using references to a number of different Azure services.
The application development process includes two phases: 1) Construct + Test and 2) Deploy + Monitor.
In the development and testing phase, a Windows Azure application is built in the Visual Studio IDE (2010 or above). Developers working on non-Microsoft applications who want to start using Azure services can certainly do so by using their existing development platform. Community-built libraries such as Eclipse Plugins, SDKs for Java, PHP or Ruby are available and make this possible.
Visual Studio Code is a tool that was created as a part of Microsoft efforts to better serve developers and recognize their needs for lighter and yet powerful/highly-configurable tools. This source code editor is available for Windows, Mac and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js. It also has a rich ecosystem of extensions and runtimes for other languages such as C++, C#, Python, PHP and Go.
That said, Visual Studio provides developers with the best development platform to build Windows Azure applications or consume Azure services.
Visual Studio and the Azure SDK provide the ability to create and deploy project infrastructure and code to Azure directly from the IDE. A developer can define the web host, website and database for an app and deploy them along with the code without ever leaving Visual Studio.
Microsoft also proposed a specialized Azure Resource Group deployment project template in Visual Studio that provides all the needed resources to make a deployment in a single, repeatable operation. Azure Resource Group projects work with preconfigured and customized JSON templates, which contain all the information needed for the resources to be deployed on Azure. In most scenarios, where multiple developers or development teams work simultaneously on the same Azure solution, configuration management is an essential part of the development lifecycle.
Microsoft Azure provides a number of different ways to build and deploy an application. They can be grouped in several categories by Azure cloud service type:
Azure App Service can be used for easy publishing of web-based projects in one of the following categories:
Azure Web Apps are built with continuous integration and delivery in mind. They support various tools like GitHub Webhooks, Jenkins, Visual Studio Team Services, TeamCity, etc.
Azure Virtual Machines are a well-known standard in VMs with a large collection of pre-configured Windows or Linux Servers. VMs allow full control over machine configuration along with responsibility for support, updates, software installations and administration. Basically, they offer the Azure IaaS layer (infrastructure-as-a-service).
Azure Functions are the ‘serverless’ style response to business’ needs and developers’ demands to be able to write code directly through the web portal. With Azure Functions, a team can consistently and securely connect each function to different Azure services.
Azure Service Fabric is used to package, deploy and manage scalable and reliable microservices.
Azure Cloud Services is the PaaS layer of Azure, lying between App Service and Virtual Machines. Cloud Services employ the App Service approach to deliver various capabilities that enable developers to build n-tier cloud apps with more control over the OS.
This Azure feature functions as a kind of OS virtualization allowing teams to deploy applications in a more efficient and predictable way. The containerized application approach works the same way in development, in test and in production systems. Azure Container Service supports standard Docker tools to manage containers.
Microsoft has dramatically extended their source control solutions in the cloud by adding Visual Studio Team Services — a wide range of tools and services to support DevOps work for continuous integration and delivery. With support for a range of tools including Jenkins, GitHub, Puppet, Chef, TeamCity, Ansible and VSTS for Azure services, a developer can work with existing tools enabling productivity and maximizing his/her experience.
After the application development is complete, development teams can move on to the quality assurance stage. Unit testing of the code is normally completed locally by the developer using different types of testing tools, such as automated unit tests and web tests. Azure Developer Tools contain emulators for some of the Azure environments, including cloud services and storage. These tools are provided for developers to unit test their application code before deploying it to Azure.
To ensure a smooth transition of the application to the cloud, I recommend using the following method:
Microsoft has implemented remote debugging for Visual Studio. The remote debugging process can be attached to any App Service (web or running in background) through the Visual Studio UI using Server or Cloud Explorer abilities. I recommend a hybrid approach to ensure our code will run on Azure — test locally using emulators and then move the application to cloud.
There are a few steps to take in order to do an Azure deploy. First the code is tested and then synchronized with the source code manager. Then the engine and versions are added to all assemblies. And finally after the code is compiled, a package is created so it can be uploaded to the Azure platform.
All applications are prepared for a release and published at some point in their lifecycle. Uploading to the environment though is different in our case. We need to validate our Azure hosting environment first. Then, we need to focus on the host, management and execution of our cloud application. After that, we purchase a subscription, and finally, we can manually upload our deployment package to our staging or production environments.
Microsoft provides several tools that can help optimize and automate the deployment process. Some of them are well-known: MSBuild scripts and the Azure Service Management API. The API can be used to interface with the management portal and create a package (one config file and one package file) directly from a VS project. That package can then be uploaded to the specific cloud environment (Azure Service). An alternative method to do that is using some of the DevOps tools I referred to above in order to deploy to Azure.
With your application up and running in Azure, you need to be able to monitor performance, watch for issues and see how customers are using your app.
Azure provides several monitoring options:
Finally, a developer can use the Azure Management portal to manage an already deployed application. Each Azure service provides access to a few managed environments — the so-called deployment slots. They can be used for Staging, Production or any other environment we want to deploy to and use in the Azure cloud. There is no rule dictating the environments we must use. We can deploy our application directly to the production environment or deploy it to staging.
Microsoft provides a SWAP procedure that will do a transition (switch) between one deployment slot and another one. It is important for the production environment IP address to stay the same. That is the Virtual IP address on Azure. Any change to the Public IP Address will lead to changes in all DNS records and name servers. Microsoft tries to set as standard practice using CNAME records when we configure our DNS servers. Azure provides a DNS service (DNS Zone) as well.
As a part of some teams’ Microsoft Azure development best practices, the SWAP process can be automated to run each time new code is pushed as part of the continuous delivery procedure. It can even be used in preview mode so we can be absolutely sure that it will work after the SWAP process has finished.
From a development perspective, I recommend deploying the package first to the staging environment, so the application can be tested in a separate QA-like environment. Then, promote it to production for release. This approach will ensure the tested release of the application is ready for public use.
According to Microsoft Azure development best practices, the Deployment & Release stage involves the following two key phases.
System Testing. In this phase, different application tests are carried out. The tests can start with a basic “Smoke Test” to ensure that all basic functionality of the deployed services is running as it should on the cloud. This can be followed by “Integration Testing” to ensure that all external touch points to the services are functioning as expected. Subsequently followed by a “User Acceptance Test”, the services would be tested by a sample set of the application users. These are a representative set of tests that can be carried out as a part of the development lifecycle on the Azure platform, which an enterprise can run as a part of their standard project delivery practices.
All of these tests are carried out on the Azure platform without any investments required for procuring or setting up separate test environments on-premise. These tests can be carried out in the default staging environment, as discussed previously, or separate environments, in the case of larger projects, to carry out each test.
Production Release. After all of the test cycles have been completed, the tested Azure Service’s code can be released into production by promoting the services from the Windows Azure portal. The promoted services will execute from the production regions of the Azure data center fabric. As a part of the promotion process, the hosted services will have a public facing URL like “http:// [project].cloudapp.net”, where [project] is the name of the project defined at the time of creating a new hosted instance. In the production stage, the services are configured, managed and monitored from the Windows Azure portal.
Some of the activities an administrator can control and manage are:
Unlike traditional on-premise systems, Azure does not provide full control of the computing environment and resources to administrators. Azure’s datacenter is fully managed and controlled by Microsoft. Users are only provided with private virtualized instances, packaged as services, as the unit of computation and storage for hosting their apps.
This will require a shift in the IT support operations. The Azure of today does not permit administrators to deploy some of the custom or third-party tools, utilities, agents, etc., that might be in use extensively with the existing operational processes to support and administer. Administrators use these tools to investigate production-related issues such as poor performance and crashes.
A cloud-based application can run into different problems related to environments and architecture. Microsoft works consistently to avoid most of the problems by creating and proposing specific patterns to be used during the application development process. They are grouped in following areas.
The time that the system is functional and working. It will be affected by system errors, infrastructure problems, malicious attacks and system load. It is usually measured as a percentage of uptime.
A key feature of most cloud applications is the ability to manage data under different conditions. Data is typically hosted in different locations and across multiple servers for reasons such as performance, scalability or availability, and this can present a range of challenges.
Microsoft Azure development best practices for design considerations should include consistency and coherence in component design and deployment, maintainability to simplify administration and development and the reusability of components and subsystems. Decisions made during the design and implementation phase have a huge impact on the quality and the total cost of ownership of cloud-hosted applications and services.
Cloud-based applications need a reliable communication channel that connects components and services ideally in a loosely coupled manner in order to maximize the scalability of distributed applications. Asynchronous messaging is widely used and provides many benefits, but also brings challenges such as the ordering of messages, poison message management and idempotency etc.
Most cloud applications use the PaaS layer of the cloud. It brings great flexibility, scalability and expense reduction in development and administration efforts but can make management and monitoring more difficult than an on-premises deployment. Applications must expose runtime information that administrators and operators can use to manage and monitor the system, as well as support changing business requirements and customization without requiring the application to be stopped or redeployed.
Performance is an indication of the responsiveness of a system to execute any action within a given time interval, while scalability is the ability of a system either to handle increases in load without an impact on performance or increase available resources. Cloud applications typically encounter variable workloads and peaks in activity. Predicting these, especially in a multi-tenant scenario, is almost impossible.
Resiliency is the ability of a system to gracefully handle and recover from failures. The nature of cloud hosting, where applications are often multi-tenant, 1) use shared platform services, 2) compete for resources and bandwidth, 3) communicate over the Internet and 4) run on commodity hardware. This means there is an increased likelihood that both transient and more permanent faults will arise. Detecting failures, and recovering quickly/efficiently, is necessary to maintain resiliency.
Security is the capability of a system to prevent malicious or accidental actions outside of the designed usage and to prevent disclosure or loss of information. Cloud applications are exposed on the Internet outside the trusted on-premise boundaries. They are often open to the public, and may serve untrusted users. Applications must be designed and deployed in a way that protects them from malicious attacks, restricts access to only approved users and protects sensitive data.
Rather than sharing final comments, I’ve added the most-used design patterns for the cloud.
This pattern can improve the stability and resiliency of an application — handling faults that may take a variable amount of time to rectify when connecting to a remote service or resource.
Enable multiple concurrent consumers to process messages received on the same messaging channel. This pattern enables a system to process multiple messages concurrently to optimize throughput, to improve scalability and availability and to balance the workload.
Delegate authentication to an external identity provider. This pattern can simplify development, minimize the requirement for user administration and improve the user experience of the application.
Indexes are a key feature of database management. Currently, they are presented in a new light for the cloud-based solutions.
The pre-populated view has returned to support the performance of the cloud data services. When the data is spread over couple of data tables it is better to combine it in a special view for the actual application needs.
Communication between the application in the cloud or an app in a hybrid environment is of high importance for the stability, the performance and the quality of the solution. To achieve better scalability and loose coupling, one can modify the project architecture by organizing the communication between the different layers of the solution through a message queue-like communication channel. Implementing messaging communication can minimize the impact on availability and responsiveness for both the task and the service in peak traffic situations.
To create a stable cloud application, the pattern must be applied on almost every layer of the application that communicates with external services. For example database or storage access, web services access, etc.
This pattern can reduce the requirement for potentially expensive computing instances. With the Static Host Couple Pattern, static content is deployed to a cloud-based storage service that can deliver the resources directly to the client.
Innovate with us! Click here to access all of our free resources. Authored by Alexander Dimitrov.
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
See all (1,229)
61 
2
61 claps
61 
2
Trusted guidance, global expertise, secure integration. We design and develop custom software solutions that deliver digital transformation at scale.
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/text-annotation-on-a-budget-with-azure-web-apps-doccano-b29f479c0c54?source=search_post---------127,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aaron (Ari) Bornstein
Jan 14, 2019·6 min read
TLDR: This post walks through how to deploy Doccano on Azure Web Apps in order to collaboratively annotate text data for natural language processing tasks.
All code for this post can be found here:
github.com
Doccano is an open source tool that provides annotation features for text classification, sequence labeling and sequence to sequence.
Recently I’ve been doing work on annotating a dataset for co-reference as part of this task I had time to evaluate a couple of different text annotation platforms.
Most free open source annotation tools such as Brat and Anafora, don’t abide by modern UX principles. Doccano is the only open source annotation tool I’ve seen with a modern UX experience. While other modern text annotations tools exist like Prodigy and LightTag exist but they have very expensive licenses.
However to collaborate Doccano we need to host the site somewhere to make this process easier this tutorial will show you how.
Azure App Service not only adds the power of Microsoft Azure to your application, such as security, load balancing, autoscaling, and automated management. You can also take advantage of its DevOps capabilities, such as continuous deployment from Azure DevOps, GitHub, Docker Hub, and other sources, package management, staging environments, custom domain, and SSL certificates.
If you have an existing Azure subscription can get started annotating you data just by clicking the button below to auto deploy .
Otherwise you can get a free Azure Account here and then click the deploy button above.
azure.microsoft.com
After the deployment navigate to the following url where {appname} is the appname you choose above.
https://{appname}.azurewebsites.net/login
For example in our deployment above the login url would be
https://doccana.azurewebsites.net/login
This will bring you in to the Doccano login page where you can login with the Admin_user and Admin_pass you configured in the deployment.
Now you are set to begin annotating your own data check out the instructions from the Doccano github . The following steps are taken verbatim from the tutorial.
Here we take an NER annotation task for science fictions to give you a brief tutorial on doccano.
Below is a JSON file containing lots of science fictions description with different languages. We need to annotate some entities like people name, book title, date and so on.
books.json
We need to create a new project for this task. Logging in with the superuser account.
To create your project, make sure you’re in the project list page and click Create Project button. As for this tutorial, we name the project as sequence labeling for books, write some description, choose sequence labeling project type and select the user we created.
After creating a project, we will see the “Import Data” page, or click Import Data button in the navigation bar. We should see the following screen:
We choose JSON file books.json to upload. After uploading the dataset file, we will see the Dataset page (or click Dataset button list in the left bar). This page displays all the documents we uploaded in one project.
Click Labels button in left bar to define our own labels. We should see the label editor page. In label editor page, you can create labels by specifying label text, shortcut key, background color and text color.
As for the tutorial, we created some entities related to science fictions.
Next, we are ready to annotate the texts. Just click the Annotate Data button in the navigation bar, we can start to annotate the documents.
After the annotation step, we can download the annotated data. Click the Edit data button in navigation bar, and then click Export Data. You should see below screen:
Here we choose JSON file to download the data by clicking the button. Below is the annotated result for our tutorial project.
sequence_labeling_for_books.json
Congratulation! You just mastered how to use doccano on Azure for a sequence labeling project.
If you have any questions, comments, or topics you would like me to discuss feel free to follow me on Twitter if there is a milestone you feel I missed please let me know. Thanks to Hironsan for the amazing work!
Aaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
See all (62)
263 
2
Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.
263 claps
263 
2
Your home for data science. A Medium publication sharing concepts, ideas and codes.
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/vsts-is-dead-long-live-azure-devops-6f184baa7e56?source=search_post---------128,"Starting today, if you use Azure you will get DevOps, whether you want it or not!
In the past few years everyone has been talking about DevOps and how important it is for every successful software team to have proper DevOps solutions.
But before today, people who used Azure, they needed to plan for their own DevOps pipeline for CI/CD, source code repository, work item…
"
https://read.acloud.guru/azure-functions-wants-to-make-it-easy-for-developers-to-get-started-with-serverless-896766af985f?source=search_post---------129,"When it comes to trendy buzzwords, “serverless” might be the most popular.
At the heart of the serverless paradigm is the Function as a Service (FaaS) model, a category of services that make it ridiculously easy to run code in the cloud without provisioning any compute infrastructure.
FaaS has truly been a game-changer: it considerably accelerated the ability to deploy complex backend services and democratized application development. Gone are the days when companies need to invest capital and resources to take their ideas to market. Now, all you need is an idea, good code, and an account with a major cloud provider. Within minutes, your application can be running on managed infrastructure, serving millions of requests, and scaling (virtually) infinitely.
This article will compare the FaaS services of Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) and offer some insight into one of the most transformative technologies of the modern cloud age! 
At the core of any application is the code that makes up its logic and functionality. Whether it is the latest mobile game, or your typical boring enterprise finance software, there are lines of code (sometimes thousands of lines) that need to run somewhere. This “somewhere” is typically a server, or groups of servers, where CPU cycles execute the logic that powers these applications.
But servers, even virtual cloud servers, are expensive and can be a pain to maintain, often requiring highly trained and experienced administrators to secure and manage them. Additionally, when no users are playing that game or using the finance software, these expensive servers will sit idly, virtually twiddling their thumbs, waiting for new “work” to come in. 
Traditional compute infrastructure can be very inefficient, and that is exactly what makes FaaS so appealing.
Instead of standing up complex infrastructure (servers, load balancers, etc), FaaS lets you run your code on a managed pool of compute resources, while paying only for the duration of execution. 
FaaS functions are event-driven, meaning they run in response to certain events. While not always the case, those functions are often short-lived, ephemeral, stateless and single-purpose.
The table below is a brief summary of the FaaS services offered by AWS, Azure, and GCP:
One of the main reasons for the popularity of FaaS is cost: it is dirt-cheap, and in many cases, practically free. With that being said, like everything else in the cloud, that price tag could dramatically change as the scale gets larger. 
The two main contributors to FaaS cost are:
Additionally, other charges like data transfer or storage costs might apply, but those depend on the specific use case.
All of the three major providers have a monthly free tier quota, and cost is incurred after that quota is reached. If you are experimenting or building a proof-of-concept, you will most likely be covered in the free tier.
The table below compares what each provider offers in terms of free tier quota, and how much additional usage costs.
Takeaway: The pricing structure is almost identical across all three providers.
AWS and Azure have identical pricing and free monthly quotas, while GCP offers an extra 1 million free requests per month, and has comparable pricing for additional requests. Both AWS and Azure round their duration to the nearest 1ms, while GCP rounds to the nearest 100ms increment, which could add to the overall cost at scale.
Get the Cloud Dictionary of PainSpeaking cloud doesn’t have to be hard. We analyzed millions of responses to ID the top concepts that trip people up. Grab this cloud guide for succinct definitions of some of the most painful cloud terms.
There are more programming languages out there than there are Star Wars sequels, prequels, and spin-offs. And for obvious reasons, every programming language has strengths and weaknesses, so one needs to pick the right tool for the right job. Cloud providers, for the most part, support most of the popular languages in their respective FaaS offering.
The table below shows the currently supported FaaS runtimes for AWS, Azure, and GCP:
Takeaway: Very comparable language support between the three major providers, with the only notable exceptions of GCP lacking support for PowerShell, and Azure lacking support for Go (a rather interesting observation considering the fact that Go was developed by Google and PowerShell by Microsoft!)
If your language of choice is not listed above, it is still possible to “bring your own runtime”, which allows you to implement a provider’s FaaS in any programming language. See the table below for current support for custom runtimes.
Early in the days of FaaS, the most common use case was to “kick the tires” or “mock something up” before moving to a more mature solution. Those days are gone, and today it is not uncommon to find global-scale production apps running fully on a FaaS backend.
To achieve this, it is important to understand how cloud providers scale FaaS workloads in response to increased demand, and how they handle concurrent requests.
As mentioned earlier, FaaS functions are event-driven, so when an event is received, an instance of the function is spun up and the request is processed. The instance is kept alive to process subsequent events, and if none are received within a certain time frame, it is recycled.
All three providers advertise virtually unlimited and automatic scaling, although there are other factors that might be at play here, which are discussed below.
When a request is being processed by a FaaS instance and another request is received, a second instance is spun up to process this additional request (instances process only one request at a time.) Concurrency is the number of simultaneous functions that can run at any given time.
AWS is the only provider to offer highly customized concurrency management options, while Azure and GCP are a little vague on how concurrent executions are handled.
Given the relatively young age of FaaS as a technology, it has many detractors and naysayers. Their favorite critique, by far, is the notion of “cold starts”. 
So what is a cold start, anyway?
Imagine this familiar action movie scene: our hero is racing down the highway at 120 mph, probably on his way to save some lives, when he zooms past an unsuspecting highway trooper, on his break, reading a newspaper. By the time the trooper fumbles to start his engine and gets up to highway speed to give chase, our hero is miles away already, and the trooper will need some time to catch up. 
Similarly, a FaaS instance in an inactive state will require some additional time to respond to a request. This initial delay encountered is known as a cold start.
Contrast that with an FaaS instance that is already in an active state and receives a request, in this case no initial delay is experienced and the instance can start processing the request almost instantly. Using our example, this would be analogous to a highway trooper driving down the highway at normal speed when he gets passed by a speeding driver. In that case, the trooper can very quickly accelerate and catch up in mere seconds.
While cloud providers do not publish their cold start statistics, the following table shows estimated averages as observed by industry analysts:
Cold starts can affect the performance of FaaS workloads that are very sensitive to delay but there are ways to mitigate it, and it appears to affect the Azure platform more than its two competitors. Additionally, AWS now offers “provisioned concurrency” as an approach to eliminate cold starts with Lambda. We discuss this feature in more detail later in the article.
Automating AWS Cost OptimizationAWS provides unprecedented value to your business, but using it cost-effectively can be a challenge. In this free, on-demand webinar, you’ll get an overview of AWS cost-optimization tools and strategies.
Not all functions are created equal, so different workloads might require different settings to optimize performance.
Depending on how resource-hungry the code is, memory will have to be adjusted accordingly. If the memory allocated is too low, a function will take longer to execute and could potentially time out, but if the memory is set too high, you might end up over-paying for unused resources.
Cloud providers offer different maximum memory configurations, while CPU power is linearly and automatically configured in proportion to the amount of memory chosen.
Of note is the maximum memory that can be configured on GCP Cloud Functions: at 4096 MB, that limit is considerably lower than what AWS and Azure offer.
The other configurable aspect of FaaS is the maximum execution time. While most functions in the wild take seconds (or less) to execute, some intensive workloads can potentially take much longer, on the order of minutes, or even hours (for example, intensive machine learning or data analysis workloads).
The table below shows the maximum timeouts that each cloud provider offers:
It is important to note that increasing the timeout is not always the solution, and it should be considered in conjunction with adjusting the memory.
Post-COVID DevOps: Accelerating the FutureHow has COVID affected — or even accelerated — DevOps best practices for engineering teams? Watch this free, on-demand webinar panel discussion with DevOps leaders as we explore DevOps in a post-COVID world.
As discussed earlier, functions deployed on a FaaS are, by nature, stateless. In other words, functions are not aware of other functions or of the execution results of other functions.
Even invocations of the same function are completely independent of each other. This stateless paradigm is what makes FaaS so scalable and easy to provision.
While the stateless approach is excellent for executing a large number of short-lived and single-purpose functions (for example, a contact form on a website), it makes it difficult to build any meaningful complex applications that often require some sort of state management. Realizing this, cloud providers built orchestration services that integrate with these functions as “steps” in a workflow, where the output of one step can be passed as input to another step. This enabled building fairly complex workflows in a completely serverless approach!
The following table lists what each provider offers for orchestration services. Those services are often very scalable as well and have many features that are beyond the scope of this article:
The power of FaaS services lies in the fact that they are event-driven, meaning certain “interesting events” can trigger an execution of the function. 
“What are interesting events?” you might ask.
Anything from a simple cron schedule (example: run this function every day at midnight) to other services within the cloud provider’s ecosystem (for example, run this function when a file is uploaded to cloud storage). But one of the most popular scenarios is integrating FaaS with an HTTP endpoint. 
A static web frontend (i.e. HTML/CSS/JavaScript) with an integrated FaaS backend is a very common and popular architectural pattern for building serverless web apps. 
While all three providers support HTTP integration, AWS requires provisioning and configuring a separate resource, API Gateway, which is billed separately as well. Azure and GCP have a much more streamlined HTTP integration.
Given the various needs in terms of availability and latency required as discussed earlier, cloud providers reacted by providing different tiers of availability baked into their FaaS offering.
AWS Lambda historically came as one basic hosting plan, but more recently AWS started offering Provisioned Concurrency, which ensures that functions are initialized and ready to respond to events, cutting down the dreaded cold-start time to mere milliseconds. Azure offers a more complex variety of hosting options, while GCP just offers a one-size-fits-all plan.
When it comes to comparing the FaaS offerings of the three major cloud providers, one thing is very obvious: they are extremely similar and comparable, both in terms of features and cost. 
While AWS Lambda is the more mature and most popular of the three, Azure Functions appears to have some very similar features and in some ways, more options to accommodate edge cases. GCP Cloud Functions has a few less bells and whistles but is still fairly comparable to the other two.
The devil, as they say, is in the details. Most likely, there are other factors to consider when comparing these three FaaS services. But whatever the case is, the key takeaway here is that FaaS is here to stay, and it is a truly transformative technology that will only continue to gain adoption, so if you have not already, make sure that you are leveraging it!
Level up your cloud career
Looking to get certified or level up your cloud career? Learn in-demand cloud skills by doing with ACG’s courses, labs, learning paths and sandbox software.
Get more insights, news, and assorted awesomeness around all things cloud learning.
"
https://posts.specterops.io/introducing-bloodhound-4-0-the-azure-update-9b2b26c5e350?source=search_post---------130,NA
https://medium.com/bb-tutorials-and-thoughts/how-to-pass-microsoft-azure-dp-900-data-fundamentals-exam-180aebdc27b2?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is one of the leading cloud providers. This exam can be taken as an optional first step in learning about Databases and how those concepts are exemplified by Microsoft Azure. It can be taken as a precursor to Microsoft Azure Database Engineer. While it would be a…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/power-bi-connects-to-azure-databricks-44bea6731be7?source=search_post---------132,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Mar 6, 2020·6 min read
Microsoft PowerBI is becoming more and more popular recently as a Data Analytics tool. Also, It is ubiquitous for a company to have a whole bucket of Microsoft products that include Azure.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ericsk/azure-iot-edge-azure-custom-vision-raspberry-pi-3-%E5%AF%A6%E7%8F%BE%E9%82%8A%E7%B7%A3%E9%81%8B%E7%AE%97-edge-computing-%E4%B8%8A%E7%9A%84%E9%9B%A2%E7%B7%9A%E9%9B%BB%E8%85%A6%E8%A6%96%E8%A6%BA%E6%87%89%E7%94%A8-94d6bde2b244?source=search_post---------133,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eric ShangKuan
Jul 8, 2018·15 min read
Azure IoT Edge 是 Microsoft Azure 上的 Edge Computing 解決方案，它會在 Windows/Linux 的機器上執行一個 Azure IoT Edge Runtime，除了保持與雲端（Azure IoT Hub）連接的管道之外，也讓軟體開發人員能夠把程式經過 IoT Edge SDK 及其工具鏈打包成 container image 後，部署到 IoT Edge Runtime 上執行，這是它實現 edge computing 的方式。
另一方面，Azure Custom Vision 是一個幫助軟體開發團隊，能夠快速打造一個智慧的電腦視覺模型的工具，透過 Azure Custom Vision 的平台，你只需要蒐集圖片、標上標籤，不需要撰寫程式碼，就能有一個屬於你自己的電腦視覺模型，除了可以透過平台將模型 host 成一個 web service，但 Custom Vision 最大的特色就是你能匯出訓練好的模型（並不是所有模型都能匯出）到 iOS (CoreML)、Android (Tensorflow)、Windows (ONNX)、以及 Azure IoT Edge 上執行，實現離線智慧電腦視覺運算的概念。
而 Raspberry Pi (RPi) 是一個容易取得、生態系又豐富的開發板，所以我就有了想把 Azure IoT Edge + Azure Custom Vision 在 RPi 上實現的構想，想實驗看看是否可行。
開始之前做了一下功課，發現我的同事已經發表了一個他的實驗成果在 GitHub 上，所以表示並不是不可行，但真正拿他的東西來部署還是遇到了一些小問題，所以才有了這篇文章的誕生。
要使用 Azure IoT Edge 之前，你必須在 Microsoft Azure 上建立一個 S1 等級以上的 Azure IoT Hub，建置的教學可以看這系列文章說明。
所有要與 Azure IoT Hub 連接的裝置都必須在 IoT Hub 上註冊裝置識別，參考這篇文章在 IoT Hub 上建立一個 IoT Edge 裝置，如此一來才可以拿到這個裝置識別的連線字串，之後讓本文中的 RPi 來連線。
我選擇做這個實驗的板子是用 Raspberry Pi 3 Model B+， 攝影機也是用 Pi (NoIR) Camera V2，基本上這樣就能來做 edge 端的電腦視覺了，但為了簡單有效地產生一些輸出效果，所以我在 RPi 上多裝一片 Sense HAT，使用它上面的 8x8 LED 來做一些視覺化的輸出效果，來顯示辨識的結果。
作業系統的部份，安裝了 Raspbian Stretch，開機後做幾件事把 Azure IoT Edge Runtime 以及要實驗的環境準備好：
整個 Azure IoT Edge 的開發都可以在 Visual Studio Code 上完成（這也意味著用 Windows、Linux、或 Mac OS X 都一樣），而為了讓開發過程更順暢，建議安裝下列 Visual Studio Code 的擴充套件來輔助：
上述的環境都準備就緒後，就可以開始規劃及開發這套 Edge computing 的實驗作品了。
既然我們已經有了別人開發好的範本，所以我們可以直接來解析它是如何規劃的，在這套範本中，它在 edge device 上規劃了三個 Edge Module，它們分別的職責為：
Edge Module 可以視作是在 Azure IoT Edge 上一個應用程式單位，它有自己的生命週期以及執行空間（想像實作上它是一個 container）。詳細請見下方的 Azure IoT Edge Module 開發與部署章節，或是這份官方文件。
以下就針對上述這幾個 edge modules 以及 IoT Edge Module 的開發部署的方式深入說明：
首先參考 Azure IoT Edge 上的文件瞭解它的架構：
宏觀地來看，開發一個 IoT Edge Module 最後把它跑在 Azure IoT Edge Runtime 上就是做幾件事情：
首先是 SDK 的部份，目前 Azure IoT Edge 可以使用 C# (.NET Core)、Python、Node.js、以及 Java 來撰寫 edge module，然後根據目標的 edge device 來打包成 container image。由於 edge device 可能是 Windows 或 Linux 的環境（因為 Azure IoT Edge Runtime 可以跑在 Windows/Linux 上），而 device 上的 CPU 可能是 x86、x64、或是 ARM 的架構，所以在編譯程式或建置 container image 時，都必須考量目標 device 的狀況來進行。
實際上，因為 edge module 都是以 container 的形式跑在 IoT Edge Runtime 上面，所以只要能包成合適的 container image，用什麼方式寫/執行程式沒太大差異，最多只是沒有官方的 Azure IoT Edge SDK 可以使用而已，也可以自行根據 Azure IoT Edge runtime 開放出來的程式碼來自行實作與 Azure IoT Edge 溝通的部份。
在這個實驗中，目標 device 是 RPi 並且跑 Raspbian (Linux)，所以要確保 build 出來的 container image 是適用 Linux/ARM 的環境使用。
一旦建置完 container image 後幾乎就完成了，剩下的工作就是找一個 container registry 來放，以便部署時能指定 URI。如果是實驗範例或是公開無傷大雅的 edge module 可以放在如 Docker Hub 這樣公開的 registry 中，否則應該考慮自行架設私有的 registry，或是使用 Azure Container Registry 這樣的服務來放 container image。
在整個實驗中，電腦視覺的核心就由這個模組來提供功能，Azure Custom Vision 服務提供了一個簡單上手的平台，讓你只需要自己蒐集圖片、上標籤，就能訓練出一個專用的電腦視覺模型，而訓練完的模型也可以匯出到 Azure IoT Edge 上使用。
訓練時的 Domain 必須要選擇有標示 (compact) 的才能匯出訓練好的模型，撰文的當下也僅有 Image Classification 的專案才能匯出。
不過，目前在 Azure Custom Vision 的匯出功能上，匯出到 Azure IoT Edge 的內容是以 x86 的環境來設計（可以選擇 Windows/Linux），不過還好它內容物包含了標籤 (labels.txt) 以及模型 (model.pb) 的檔案（下載的模型是 TensorFlow 的格式），所以有這兩個檔案還是可以自行撰寫 edge module。
這個模組可以從 resin/rpi-raspbian 作為 base image 開始建置。同時我們要能解析下載的 TensorFlow 的模型，所以要安裝 Python 3 的相關環境，不過直接用 pip3 來安裝 tensorflow 的版本太舊了（在 RPi 上），不過我們可以從 TensorFlow 的 CI server 中找到適用 RPi 新版本的 pip wheel 來安裝，或是參考 TensorFlow 官網上的指南從原始碼開始建置。
其它需要的 Python 套件包括 pillow、numpy、以及 flask，因為這個模組啟動的 web service 就是以 Flask 來運作的。
所以最後 Dockerfile 就會長得像這樣：
完整的模組程式碼請參考這裡。
這個模組相對單純，就是接收資料然後控制 Sense HAT 來顯示而已，本來在 Raspbian 下就只要透過 pip 安裝 sense-hat 套件就能用 Python 來控制 Sense HAT ，所以建置 edge module 的 base image 也可以是 resin/rpi-raspbian，不過因為這裡會用到 Azure IoT Hub 的 client，而範本中附好了 pre-built for RPi 的 iothub.so 它是基於 Python 3.5 來建置的，所以 base image 才選用 resin/raspberrypi3-debian:stretch。
其它的部份，就是根據 image-classifier-service 回傳的結果，根據機率最高的標籤，而且機率超過設定的 THRESHOLD 值就在 Sense HAT 上的 8x8 LED 燈上畫出圖案（這個範本是辨識香蕉、蘋果、檸檬、以及柳橙）。
完整的模組程式碼請參考這裡。
這個模組要做的事情比較特別，它必須使用 Open CV 來處理攝影機拍下的照片，但如果每次建置 edge module 時都從頭開始 build opencv 就曠日又廢時，所幸已經有人建置好了基於 Raspbian 又建好 Open CV 函式庫的 container image — mohaseeb/raspberrypi3-python-opencv，所以就直接拿來當作 base image 開始。
值得注意的是，這個模組是透過 VIDEO_PATH 這個環境變數拿到相機的位置，如果你跟我一樣是用 Pi Camera（而不是用 USB Cam），而又照著文件中說明把相機位置設定成 /dev/video0 的話，就要在 Raspbian 上多做下列步驟：
完整的模組程式碼請參考這裡。
如果你把這個專案範本 clone 回來，可以直接用 Visual Studio Code 打開這個目錄，在建置模組前先做一些修改：
如果你的 Visual Studio Code 已經正確地安裝了 Azure IoT Edge 擴充套件，那在 deployment.template.json 檔案上按右鍵，就可以選擇 Build IoT Edge Solution 開始建置整個專案。
順利建置完成後（根據我的經驗，image-classifier-service 模組會花比較久的時間），在 config/deployment.json 檔案上按右鍵就可以使用 Create Deployment for IoT Edge Device 直接部署到指定的 device 上了。
過一段時間後，連進 RPi 裡執行 docker ps 應該可以看到像是這樣的畫面，如果用 docker logs -f 進入任何一個 container 都沒有錯誤的話，就可以開始自行實驗囉。
這個實驗主要是幫助開發人員瞭解 Azure IoT Edge 的開發、部署流程，並且搭配 Azure Custom Vision 快速兜出一個電腦視覺模型，不過在 RPi 上執行 TensorFlow 畢竟比較吃力一些，或許在實務運用上，搭配其它加速裝置、或是選用 powerful 的 x86 device 可能會更好。所以在 edge computing 上的應用，針對不同的情境選擇不同的軟硬體搭配才是正確的觀念。
DevRel | Developholic | Technical Evangelist
81 
81 
81 
DevRel | Developholic | Technical Evangelist
"
https://medium.com/@renatogroffe/asp-net-core-apis-rest-na-nuvem-com-docker-e-azure-web-app-4a82f9f594a5?source=search_post---------134,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
May 22, 2017·8 min read
O objetivo deste artigo é demonstrar a implementação de uma API REST na nuvem com o ASP.NET Core, empregando para isto containers Docker, o Docker Hub e o Azure Web App on Linux.
O deployment de softwares através de containers vem ganhando cada vez mais força, com as mais variadas tecnologias se adequando para suportar este tipo de prática. O isolamento de aplicações e um uso mais racional de recursos estão entre as principais vantagens obtidas por meio da adoção dessas estruturas.
Um container pode ser definido, do ponto de vista técnico, como uma unidade isolada para deployment de um projeto e todas as suas dependências. Dentre as tecnologias baseadas neste conceito merece destaque o Docker, uma das opções mais populares da atualidade para a utilização de containers.
A Microsoft não ficou alheia a esta tendência, oferecendo suporte a containers Docker em produtos como o .NET Core, o ASP.NET Core e o SQL Server 2017. No que se refere à nuvem, uma das possibilidades de uso do Docker está no Azure Web App on Linux. Esta alternativa PaaS (Plataform as a Service) viabiliza o deployment de aplicações construídas a partir de tecnologias compatíveis com distribuições Linux (caso de containers Docker), além de oferecer inúmeras facilidades em termos de gerenciamento, escalabilidade, segurança, monitoramento e deploy.
Nas próximas seções será abordado o uso em conjunto de tecnologias como ASP.NET Core, Docker e Azure. Isto acontecerá através de um exemplo prático, o qual dará ênfase à implementação e deployment de uma API REST na nuvem.
A implementação da API descrita neste artigo já foi detalhada anteriormente no seguinte post (embora utilizando a versão do ASP.NET Core baseada no arquivo project.json, com o uso de tal recurso sendo posteriormente descontinuado):
ASP.NET Core: implementando uma API REST em Linux
Este projeto permitirá a conversão de alturas em pés (medida comumente adotada na aviação) para o equivalente em metros, considerando para isto a seguinte fórmula:
Para a construção do exemplo de testes serão utilizados:
Inicialmente será criado um projeto do tipo ASP.NET Core Web Application (.NET Core) chamado APIAlturas:
Selecionar na sequência a versão 1.1 e a opção Web API em ASP.NET Core 1.1 Templates:
A classe ConversorAlturasController será responsável pela conversão de alturas em pés para o equivalente em metros, com isso acontecendo através do método Get (este último responsável pelo tratamento de requisições HTTP):
Na imagem a seguir é possível observar o resultado da conversão de uma altura de 100 pés (30,48 metros):
Com a API de conversão já implementada o próximo passo será a criação e deployment no Docker Hub de uma imagem Docker, a qual permitirá a publicação da aplicação sob a forma de um container no Azure Web App on Linux. Trata-se de um processo extremamente simples, graças ao Visual Studio Tools for Docker - conjunto de ferramentas de suporte ao Docker disponíveis para uso no Visual Studio 2017.
Dentre as funcionalidades que integram o Visual Studio Tools for Docker destacam-se recursos para building, debugging e execução de containers a partir de projetos gerados com o .NET Core.
Os procedimentos descritos nesta seção dependem da instalação prévia do Docker for Windows em um ambiente de desenvolvimento baseado no Windows 10. Na interface de configuração deste aplicativo será necessário acessar a seção Shared Drives e, na sequência, marcar o driver C: (isto permitirá a utilização das diversas funcionalidades do Visual Studio Tools for Docker):
Dentro do Visual Studio 2017 será necessário adicionar os arquivos de suporte ao Docker para a aplicação criada na seção anterior. Um dos caminhos para isto consiste em acionar o menu de contexto para o projeto APIAlturas e, em seguida, as opções Add e Docker Support:
Como resultados desta ação serão criados os seguintes itens:
O contéudo do arquivo Dockerfile pode ser observado na próxima listagem:
Já na listagem a seguir estão as configurações do arquivo docker-compose.yml:
Alternativamente os arquivos de suporte ao Docker também poderiam ser adicionados no momento de criação da aplicação (selecionando para isto a opção Enable Docker Support na tela de seleção de templates do ASP.NET Core 1.1):
Neste momento o projeto APIAlturas já estará devidamente configurado para sua execução e, até mesmo, debugging a partir de um container Docker. O próprio ícone para ativar a aplicação indica isto:
Com aplicação APIAlturas em modo de execução será possível verificar a existência de uma imagem Docker criada para efeitos de desenvolvimento, além de um container baseado nessa estrutura.
Ao executar o comando docker images no PowerShell aparecerão as imagens microsoft/aspnetcore:1.1 e apialturas:dev:
Já o comando docker ps -a exibirá o container gerado para a execução por meio do Visual Studio:
Uma nova imagem Docker deverá ser gerada com base nas configurações de release da aplicação. Selecionar para isto a opção Release (ao invés de Debug) no Visual Studio 2017:
Efetuar em seguida a compilação do projeto (a partir do menu Build > Build Solution). Ao executar novamente o comando docker images no PowerShell constará agora a imagem apialturas:latest:
Uma tag chamada renatogroffe/apialturas deverá ser criada para a imagem apialturas:latest, com este novo elemento contendo o nome a ser registrado no Docker Hub (formado por identificação do repositório/login + nome da aplicação; esses dois itens estarão separados ainda por uma barra - “/”). Executar para isto o seguinte comando no PowerShell:
docker tag apialturas:latest renatogroffe/apialturas
Uma nova execução da instrução docker images trará agora uma imagem chamada renatogroffe/apialturas:
Para registrar a imagem no Docker Hub será necessária uma conta para uso deste repositório. Caso precise criar tal login e/ou obter maiores informações acesse:
https://hub.docker.com/
O próximo passo será logar no Docker Hub (via PowerShell) através da seguinte instrução - que solicitará usuário e senha:
docker login
Para publicar a imagem renatogroffe/apialturas no Docker Hub será utilizado o comando:
docker push renatogroffe/apialturas
O resultado desta ação pode ser observado na próxima imagem:
A imagem renatogroffe/apialturas aparecerá então no Docker Hub após a conclusão deste último procedimento:
No portal do Azure será necessário criar um novo recurso baseado no serviço Web App on Linux:
Informar no formulário de criação do recurso:
Acionar após o preenchimento destes campos a opção Configurar contêiner:
Em Contêiner do Docker selecionar o item Docker Hub, certificando-se de que a opção Público está marcada em Acesso ao Repositório e informando no campo Imagem e marca opcional o valor renatogroffe/apialturas (correspondente à imagem publicada anteriormente no Docker Hub):
Selecionar OK em Contêiner do Docker, confirmando as definições a serem utilizadas para a criação do container na nuvem. Concluir então este processo acionando o botão Criar no formulário em que se especificaram as configurações da API REST que ficará hospedada na nuvem:
Na opção Todos os recursos será possível constatar, após um curto período de tempo, a presença do recurso apialturas:
Ao acessar o item apialturas será exibido um painel no qual constará o endereço da aplicação de testes (http://apialturas.azurewebsites.net), além de outras opções para a configuração e gerenciamento deste recurso:
Um teste com a URL http://apialturas.azurewebsites.net/api/conversoralturas/pesmetros/1000 retornará o resultado da conversão de mil pés (304,8 metros):
Este artigo procurou apresentar uma das alternativas para uso do ASP.NET Core com Docker na nuvem da Microsoft: o Azure Web App on Linux. Esse serviço é uma excelente opção não só para a utilização de containers, como também para a hospedagem de aplicações que empreguem plataformas tipicamente associadas a distribuições Linux. O Azure Web App on Linux conta ainda com funcionalidades que facilitam sua configuração, gerenciamento e, até mesmo, integração contínua de soluções Web.
ASP.NET Core - Documentation
Conteúdos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0
Introduction to Azure Web App on Linux
Visual Studio Tools for Docker
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
73 
2
73 
73 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://blog.jeremylikness.com/nosql-and-azure-cosmos-db-in-20-minutes-9d0c3e0279dc?source=search_post---------135,
https://medium.com/ark-io/ark-bridgechain-azure-guide-774f5fd63333?source=search_post---------136,"There are currently no responses for this story.
Be the first to respond.
This deployment method can be used to jump-start your journey to your own BridgeChain in the process outlined here (ARK Deployer)
Pre-Requisites: Active Microsoft Azure account (Trial is OK) and basic knowledge on how to connect via SSH (E.g. PuTTY for Windows).
End result:- Ubuntu VM in Azure- Azure PublicIP and Firewall configurations pre-built- Your own BridgeChain node and ARK Explorer running in < 20 minutes- Re-usable and customizable deployment script
Script available at : ARK Azure on GitHub
Navigate to: https://github.com/ArkEcosystem/ark-azureClick on the big blue “Deploy to Azure” button to be taken to Portal.Azure.com. Login with your account (Trial or not, either work).
You should only need to input data for 3 empty fields (Resource Group, Admin Password, and DNS Label), but full details for this section:
Subscription — If not already on your current one.Resource Group — ‘Create New’ only option that will work unless you wish to edit the template yourself. We will use My-Ark-RG for this guide.Location — Choose your desired region, ensuring it allows the subscription you chose above. If in doubt, use the default region when using a Free Trial.Admin Username — This is the account you will sign into the server with.Admin Password — Secure string and has high complexity requirements.Dns Label Prefix — This is the unique DNS name that you are giving to this VM. It is mandatory that this name be unique in the Location Datacenter as a whole (It will self-check after input) **Remember this for easy SSHUbuntu OS Version — Only option is 16.04-LTS at this time.ARKNSG Name — This is the name for the firewall group to permit SSH as well as Ark Node/Explorer port access to this VM.
Currently, the VM produced is a Standard_A1 size VM. This is a very low-cost resource VM for tutorial purposes (can be scaled-up after deployment if desired or via template adjustment).
Click ‘Agree’, and ‘Purchase’ to begin deployment. Should take 5–10 minutes.
You are welcome to explore your new VM’s Overview, etc, by clicking on ‘Resource Groups’ and finding your new group, and the VM inside. There are lots of configuration items here.
If you do not remember your Public DNS name or IP address (for SSH), go to: Resource Groups > My-Ark-RG > MyUbuntuVM > Overview. This has all the general information you will need.
The Public DNS Name for all VMs follows this pattern:PublicDNSname.locationid.cloudapp.azure.com
So in our instance it would be here: firstarksidechain.southcentralus.cloudapp.azure.com
SSH into your new VM using the Public DNS Name and login with the credentials used during the VM Template deployment page.
Starting here, is the default quick-installation method with the chain being named “MyTest”. If you wish to customize it, please see the bottom-most section of this article.
Run the following command (It’s a one-liner, copy and paste the full contents from here or from the ARK AZure Github page):
curl -o- https://raw.githubusercontent.com/ArkEcosystem/ark-azure/master/script/arkdefaultinstall.sh | bash
This script will complete all of the installation steps to get both the node and explorer running for your BridgeChain, with all the default values. Total installation time is about 10 minutes.
Just after the node gets installed, there will be 3 lines of text to record. Copy these lines outlined in red below.
This information will be used later on, as you explore ARK past the deployment phase with ARK Deployer. Copy and paste it right out of the console window for safe keeping.
This is the Public IP of your server, and the port required to view the ARK Explorer for your BridgeChain (4200). The API should be available on port 4100.
You can highlight the URL straight from the SSH window, such as: http://13.65.29.3:4200 and hit CTRL+C to copy it. Paste into a browser, and voila!
If you wish to customize your deployment of ARK within the bounds of ARK-Deployer, download a copy of:
https://raw.githubusercontent.com/ArkEcosystem/ark-azure/master/script/arkdefaultinstall.sh
Within this file, you’re welcome to edit the list of variables on lines 21–31 and personalize them. These variables all align with an optional parameter of ARK Deployer (See GitHub: https://github.com/ArkEcosystem/ark-deployer#optional-parameters)
You can then run this new version of your script against a new VM, or, you can uninstall the original node/explorer and re-install using the script again. We would recommend just rolling out a new server for ease of use, but that’s your call.
To quickly make a personalized copy of the script (GitHub account)
For more in-depth and customizable BridgeChain enjoy following along with the ARK Deployer guide going forward in your ARK journey. Welcome aboard ARK.
Special thanks to Walrusface for writing this guide and script, delegate Jarunik for sponsoring its development and our dev Alex Barnsley for testing and modifying necessary things in ARK deployer.
ARK | All-in-One Blockchain Solutions
557 
1
557 claps
557 
1
Written by
Vice President and Co-Founder at ARK.io
ARK | All-in-One Blockchain Solutions
Written by
Vice President and Co-Founder at ARK.io
ARK | All-in-One Blockchain Solutions
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/making-a-wearable-live-caption-display-using-azure-cognitive-services-and-ably-realtime-f4f6667a076f?source=search_post---------137,"There are currently no responses for this story.
Be the first to respond.
Live captioning of speech into text has so many useful applications and Azure Cognitive Services makes it fast and easy to build captioning into your applications. Used together with Ably Realtime, it is possible to make wearable devices which can display what you’re saying, in real time. Wearable live captions!
This article will explain how to use Azure Speech and Ably Realtime and will go through building a web app that will take data from your microphone and turn it into readable text.
Check out this video to see the demo in action:
I’ll admit to a personal desire to see more products like this on the market. The reason is my mother. She has been steadily losing her hearing over the last few years and relies heavily on lip reading and clear pronunciation. Two things which are denied to her when the people talking to her are wearing masks. Of course I also want to keep her safe, so I will always encourage everyone to wear a mask, but there must be ways that technology can make her, and many others, life easier. One of the frustrations with assistive technologies on phones is that they require her to be looking at her phone, rather than at the speaker, which can lead to her being treated differently, often poorly, by whoever is speaking to her.
Inspiration hit when I saw this product on sale (at Cyberdog, of all places)
It is a face mask with a wearable LED display inside. The display is incredibly small, flexible, breathable and has very low power consumption. What if I could send text to the display from my phone’s microphone? It could update the mask display to show what I am saying!
So began my journey to build a wearable live captioning demo.
The demo consists of a web app, which is used to capture microphone data and send it to Azure Cognitive Services. When it receives text back, the web app can display it to the user. It also contains a virtual representation of the hardware display to visualise what the hardware display will be showing. The app uses the Ably Javascript SDK along with their MQTT broker to send messages from the app to the wearable hardware.
The wearable part is a 32 by 8 display of neopixels (very small LEDs) connected to an Adafruit Feather Huzzah (a small, wifi enabled microprocessor) which is powered by a rechargeable USB battery.
The web app is built with HTML, CSS and JS, it will run on your phone or your computer and just requires an internet connection and a microphone.
You can see and clone the code for the entire project on github, it is open source, and I’d be delighted if you used it to create your own wearable tech projects, especially if they can help make someone’s day better! Instructions on how to set up the app and its various dependencies are in there too.
The getUserMedia() API has been in browsers for a while. It allows us to prompt the user for permission to use their microphone and or camera and, once allowed, get a stream of data from their media devices. This app uses just the audio, so it will only prompt for microphone permissions.
This app uses the Cognitive Services Speech service, which allows us to transcribe audible speech into readable, searchable text.
When a user clicks the “Start Listening” button on the app UI, a function called streamSpeechFromBrowser is called. This uses the Azure fromDefaultMicrophoneInput along with the fromAuthorisationToken function to authenticate with Azure and initialise a SpeechRecognizer. This is what will perform the speech recognition on the data coming from the mic. It will return an object which contains the text of what has been said.
Because the phone now has the transcription, and our microcontroller is connected to our LED display, the app needs to send the transcription to the hardware, in a format that it can understand so the code running on the hardware can convert that text into lights.
To communicate between the web app and the microprocessor, a messaging protocol is required. MQTT is a lightweight publish/subscribe protocol, designed specifically for IoT devices and optimised for high latency or unreliable networks. This is perfect for this particular project where the wearer might be on a 3g connection.
In order to use MQTT, a broker is required, this is a service which is responsible for dispatching messages between the sender (or client) and the rightful receivers. The web app is the client, in this case, and the receiver is the microcontroller. This project uses the Ably MQTT broker, which comes for free with the Ably Javascript SDK. The web app can send messages using the Ably SDK and they will be automatically sent out using MQTT too.
The microprocessor on the Adafruit Feather Huzzah is very small and therefore has limited processing power and memory, which means that the code that runs on it needs to be as efficient as possible. It is therefore necessary to avoid doing any complicated text parsing, string splitting or other similarly memory intensive tasks on the microprocessor. Parsing and splitting strings is especially costly, and involves larger buffers than would be ideal.
While at a glance, this may seem like premature performance optimisation, if all of the memory on the board is used parsing the messages as human readable strings, it decreases the amount of memory available to buffer incoming messages. To solve this problem, a binary message format is used to talk to the hardware. The browser app creates a specially coded message to send text strings to the device.
Where most systems would probably use JSON to serialize messages with properties, this app uses a binary format where the order and content of bytes as they are received is relevant to the hardware. For example, sending a command to scroll text across the display involves the browser sending a message that looks like this:
But rather than serializing this message to JSON, and sending it as text, it is packed down into a binary message that looks like this, byte by byte:
These messages are sent as raw bytes — the control codes in the ASCII standard are used to provide some header information in the messages.
Because the message is a raw byte stream, it is not necessary to parse the message on the hardware, it can just loop over the message bytes, and run different processes depending on the byte being looped over at the time. This takes all of the complexity of parsing text away from the microprocessor, and moves it into the TypeScript code in the web app, where it is easier to debug and test.
In the table above, the byte at offset 8 would represent a single character, but the parser on the hardware is looking out for the STX and ETX start and end of text markers. What this means is that any number of ASCII characters can be added in the space between them to form a full sentence in the message.
Since the transcription from Azure Cognitive Services arrives as text, it is trivially simple to display this text to the user in a containing element within the app UI.
The microcontroller needs a way to convert the messages it receives as ASCII characters, into a format which can be shown on an 8 pixel high resolution matrix (it will support displays with a higher resolution, but not lower).
The first thing to do was design some “pixel font” style alphanumeric characters and symbols
Which could be converted to an array of binary values. Let’s take, for example, the letter n. Which, represented visually, would look like this:
which as an array of binary values (where black = 1 and white = 0) would be:
Or, as an array of binary values:
The app uses a JavaScript image manipulation program called jimp to create this conversion from a png to one very long array containing all of the characters. The array also requires a list of indexes, which will point to the starting position of each letter and its width (you’ll notice from the graphic above, that the letters and symbols differ in width). These two byte arrays are small enough to be embedded on the microcontroller.
With the “font” arrays embedded on the device, it is possible to write code in C/C++ (the language that the microprocessor uses) to show the letters on the display. This means that instead of computing all of the individual pixel positions in the browser app, and sending them one by one, the microcontroller will handle the translation.
LED displays, like the one used in this project are made up of addressable RGB LEDs. “Addressable” means that each LED on the display has its own address number, the first LED on the strip is LED 0, the second LED 1, and so on as they move along the matrix.
There is an open source Arduino library, written to interact with these LEDS, called AdaFruit NeoPixel which makes it simple to set the colour value of individual pixels with a pixel address (or ID) and a colour value, set in RGB (red, green blue).
Unfortunately, addressing the LEDs in the matrix isn’t quite as simple as I’d originally hoped. There are many different ways of wiring up an LED matrix, and every manufacturer seems to have a different preference!
Some common wiring patterns look like this:
Not only that, but the displays sometimes have connectors at both ends, which means that the numbering could go from left to right, or right to left, depending on which connector is used. These differences are mostly made to accommodate physical constraints, snaking the wires uses a lot less wiring than bringing the line back to the start of each row.
This variety of wiring patterns meant that the code running on the board needed to be able to translate from (x,y) coordinates into the appropriate pixel ID, so that developers working with this code will be able to use it with the display they own. This is done by ensuring that all of the operations in the JavaScript SDK library refer to pixel locations in (x,y) coordinates rather than pixel IDs. Once these coordinates are sent to the hardware, it will translate the coordinates into the correct pixel ID, based on hardware configuration code running on the board.
The microcontroller code has a single Configuration file included in the project that contains the display configuration, along with configuration for WiFi and MQTT settings:
You can see here that every device needs to be told the GPIO (General Purpose IO) pin the display is connected to, the dimensions of the display, and the index_mode and carriage_return_mode settings that capture the possible difference between displays.
This means that the code will be able to drive cheap displays bought from different manufacturers, without spending time changing the code.
With the microcontroller software now able to set pixels using any kind of display, and with the “font” arrays stored as variables in the Arduino C++ code, the missing piece is to write message handlers that process incoming binary messages, look up the “font” data for each letter in the message, and push the appropriate pixels to the display.
First let’s cover how static text can be written to the display -
When a message to display static text is received, the code loops through each text character present in the message and looks up that character in the embedded font. The font data that gets returned from the font is an array of bytes, with a single byte for every pixel to be set on the display, prefixed by the width of the character in pixels.
Because the font data includes the width, when the code iterates over subsequent characters, it can calculate the location it needs to start drawing at. This location is just called xPosition in the code, and each time a character is drawn on the display, its width and an additional space is added to the xPosition. As the code loops, all of the (x,y) coordinates returned from the font have this xPosition added to them making sure that the characters are drawn in sequence. This continues until the loop reaches the width of the display, where it stops drawing extra characters that would flow off the side.
Writing scrolling text is mostly the same, with one subtle change: the xPosition starts out at the right hand edge of the display. This means that when a message is first received, the xPosition — the initial offset, will equal the display width and no pixels will be drawn.
The scrolling text implementation is relatively simple — it is a loop that decrements that xPosition until it equals the total width of all the characters, as a negative number. So, if the text string has a total width of 100 pixels, it’ll keep on drawing, and decrementing the xPosition, until the xPosition equals -100. This scrolls the text across the screen.
The code on the hardware only ever draws pixels for the characters that will be visible on the display which means that memory is saved by not converting every character into pixels ahead of time.
There is another advantage to this approach — if you refer back to the data that is being sent as part of SetTextMessage, one of the values in the message header is a “scroll interval”. This scroll interval is the time delay between scrolling a single pixel to the left. This is what sets the speed of the scrolling. As the animation is played, the code pauses execution every time it scrolls. This loop prevents control being returned to the main Arduino loop, which would trigger receiving more MQTT messages.
This approach makes it possible to use MQTT as a buffer without using up any memory on the device. When it finishes scrolling, it receives any subsequent messages that have been sent, and starts scrolling the next piece of text.
Deploying code to the microcontroller not only requires the device to be plugged in but also takes time for the code to be verified, compiled and then pushed to the device. This can sometimes take quite a while. In order to make debugging both the code running on the board and its support of different sized displays, the web app comes with a simulated version of the hardware display.
This is a visual, virtual representation of the display which runs transliterated code (from C++ to typescript). It contains ported versions of the Arduino libraries that the microprocessor requires (like AdaFruit_NeoPixel), which made it possible to write code that appears as though it uses the NeoPixel SDK function, but it is targeting a div in the markup, instead of a pixel on the display. This means that code for the board can be written in Typescript (a language I personally much prefer!), tested on the simulated board and, once functional, can be transliterated into C++ to be written to the board.
To validate this approach, it was necessary to transliterate a few of the NeoPixel test programs into TypeScript to run in the simulator. This helped to build confidence that the code that was being written would work similarly on the hardware devices. The simulator was close enough to the real hardware to be a good test target. Fortunately this held true, and the TypeScript code that was written transliterated across to C++ with very few changes.
The same codebase that is used here to scroll text, can be used to send pixels for interactive painting, or even send images across to the device (as long as they are sufficiently tiny to fit on the display) using the TypeScript SDK that was built to communicate with the hardware.
The message protocol that was designed to send the data over the wire can transmit any pixel data, so long as the hardware at the other end can process the messages that it receives.
The final sequence diagram for the app, hardware and services in use looks like this:
Because there’s a message being sent via Ably every time a new transcription arrives, the web app also features a sharable “View” page that just displays the transcription text in real time on the screen. The user can share the URL of this to anyone that wants to open the page and read along.
The app contains a Share link that will trigger the operating system’s default sharing UI (at least on Windows and Android devices), using the Web Share API. This API is paint-dripping-wet new and not supported everywhere yet, so will be replaced with a “Copy Shareable Link” button on unsupported devices.
You can check out the code on GitHub: https://github.com/ably-labs/live-caption-demo
The live demo is up at: https://live-caption.ably.dev/
I started the project after seeing (and buying!) that LED mask in Cyberdog. I had hoped that I’d be able to hack, or reverse engineer that mask for use in this project, but its OEM hardware and software made that impossible for my skill level, so sadly it wasn’t really fit for this project. The displays that I ended up using don’t fit inside a mask comfortably, I bought these because they are what is currently readily available from hardware providers. The micropixel displays inside the mask don’t seem to be commercially available outside of those products just yet, but they probably will be soon, and this app will be ready when they are!
Azure Cognitive Services Speech service can do translation as well as transcription, it would be possible, with few changes to the code, and perhaps a few extra characters in the font, to make the mask display language translation, which could be very useful when travelling!
I, of course, showed the display and its accompanying app to my mother the last time I visited her and she was overwhelmed with what a difference it made to how well she could understand me in a mask. Yes sometimes the transcription is imperfect, but she was able to grasp the meaning of the words that she couldn’t hear and was able to look me in the face while speaking to me and that made all the difference. I hope that someone will take this idea and really run with it, because it could make a difference to so many people during this pandemic, and into the future.
Any language.
272 
3
272 claps
272 
3
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Lead Developer Advocate
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/awesome-azure/azure-difference-between-azure-private-links-and-azure-service-endpoints-private-links-vs-service-endpoints-8fb0f80ca196?source=search_post---------138,"There are currently no responses for this story.
Be the first to respond.
Azure Private Links vs Azure Service Endpoints — Comparison between Private Links and Service Endpoints
Azure Private Link (Private Endpoint) allows you to access Azure PaaS services over Private IP address within the VNet. It gets a new private IP on your VNet. When you send traffic to PaaS resource, it will always ensure traffic stays…
"
https://medium.com/@ymedialabs/the-pros-and-cons-of-jenkins-vs-azure-devops-469c66140b4d?source=search_post---------139,"Sign in
There are currently no responses for this story.
Be the first to respond.
YML
Apr 17, 2020·4 min read
By Sandhya Mungarwadi, QA Engineer, YML
The following document depicts the pros and cons of CI/CD tools (Azure DevOps and Jenkins). Before that, let’s understand what CI /CD means.
Continuous Integration (CI) is a development practice where development teams make small, frequent changes to code. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible.
Continuous delivery (CD) is actually an extension of CI, in which the software delivery process is automated further to enable easy and confident deployments into production at any time.
Azure Pipelines is a service that caters the need for creating pipelines on Azure Cloud Platform. It supports continuous integration (CI) and continuous delivery (CD), hence constantly and consistently tests and builds your code and deploys it to any target by defining a pipeline.
👍Usability (Great User Interface) — Being a new user, it was easy to pick up and go with this tool . We can start with the classic editor which provides an intuitive GUI to create and visualize the integration steps. Later, the user you can define those very same steps in YAML.Hence, we can define the pipeline using two features i.e. Classic Editor and YAML.
👍Categorized Built in Tasks — The tasks are categorized based on the nature of operation. ex:Build tasks,Utility tasks,Deploy tasks etc. This makes easy for the user to add the desired/specific tasks to their pipeline.
👍Group Tasks — It allows you to encapsulate a sequence of tasks, already defined in a pipeline, into a single reusable task ,just like any other task.
👍Configuring CI/CD Pipeline as Code — Using YAML we can achieve this. Reference to understand the hierarchy of YAML file-
👍Request and Add Tasks to your Pipelines — It has a lot of build-in tasks,yet you can download extensions/tasks from the Azure DevOps marketplace.
👍Microsoft Hosted Agents — Azure Pipelines offers cloud hosted build agents for Linux, Windows, and macOS builds. You can have a look as to what software are installed on the agent using the following reference link.
👍Any language, any platform, any cloud — Build, test, and deploy Node.js, Python, Java, PHP, Ruby, C/C++ , .Net, Android, and iOS apps. Run in parallel on Linux, macOS and Windows. Deploy to Azure, AWS, GCP or on-premises
👍Azure Pipelines — Provide unlimited build minutes to all open source projects and up to 10 concurrent jobs across Windows, Linux and macOS.
👍Azure Pipeline — Analytics are provided at the end of each run with parameters like rate and duration of run.
👎Integration with non-Microsoft is difficult — Azure DevOps should provide easier integration with other product lines to improve acceptability.
👎Azure Pipeline — Workflow is straightforward (can’t set if-else or switch-case constructions). This makes it more difficult to develop complex workflows.
👎The deprecated tasks/extensions — They are not removed from the marketplace.
👎Documents are not Up To Date.
A Jenkins pipeline is a suite of plugins that supports implementing and integrating continuous delivery pipelines into Jenkins.
👍Jenkins is open source, easy to install and free.
👍Platform Independent — Available for all platforms and different operating systems, whether Mac OS X, Windows or Linux.
👍Rich Plugin — Jenkins comes with a wide range of plugins.
👎Outdated UI — Its interface seems a bit outdated and not user friendly as it doesn’t follow modern design principles.
👎 Scripted Pipelines — Must be programmed in Groovy.
👎Though it’s rich in Plugin, a lot of plugins are not straightforward or unstable — Even for a basic tasks, plugins needs to be installed.
👎There’s no YAML interface for Jenkins Pipelines.
👎Jenkins doesn’t provide any analytics (there are plugins but they are not enough) at the end of each run.
👎Needs better documentation.
It all depends on the Team/Project need . We know Jenkins is more flexible to create complex workflow indeed Azure DevOps is faster to adapt. It’s very rare that a single CI tool will suffice for all scenarios.
If we decide to use both the tools, then we should know that Azure Pipelines supports integration with Jenkins.
YML is a design and digital product agency. We create digital experiences that export Silicon Valley thinking to the world.
See all (1,516)
139 
4
139 claps
139 
4
YML is a design and digital product agency. We create digital experiences that export Silicon Valley thinking to the world.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/connect-azure-devops-to-aws-b89120599103?source=search_post---------140,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
Azure DevOps (ADO) is a CI/CD platform from Microsoft ($MSFT). It permits a great deal of flexibility in the type of code run, the structure and permissions sets applied to jobs, and many other items of your creation and management of resources and automated jobs. However, support for other cloud providers is (perhaps obviously) weaker than at $MSFT’s native Azure Cloud.
"
https://medium.com/avmconsulting-blog/azure-kubernetes-service-aks-d1e71c7ecbe6?source=search_post---------141,"There are currently no responses for this story.
Be the first to respond.
(Highly available, secure, and fully managed Kubernetes service)
Azure Kubernetes Service (AKS) manages your hosted Kubernetes environment, making it quick and easy to deploy and manage containerized applications.
Create AKS cluster using Azure Portal
Sign in to the Azure portal at https://portal.azure.com.
If you don’t have an account, sign up for the free tier. You will get a $ 200 credit with 1-month validity.
Step 1: In the top search bar, search with AKS and click on “Kubernetes Service” and click on “Add”
Step 2: To create an AKS cluster, complete the following steps:
2. Authentication: Configure the following options:
Step 3: Select Review + create and then Create when validated successfully.
It will take some to provision the AKS cluster for you. Once deployment is completed, click on “Go to resources”.
It will take you to the AKS clusters page.
Step 4: Connect to the cluster
Cloud shell is pre-loaded with kubectl. Open cloud shell using the button on the top right-hand corner of the Azure portal.
To assemble kubectl to attach to your Kubernetes cluster, use the az aks get-credentials command.
To verify the association to your cluster, use the kubectl get command to come to an inventory of the cluster nodes.
Now your cluster is ready to deploy your application.
az login
Note: This login method is configured using the OAuth DeviceProfile flow.
3. If you have multiple subscriptions in Azure, you might need to use az account list and az account set –subscription <Your Azure Subscription ID> to make sure you’re working on the right one:
4. Create a resource group
After several minutes the command completes and returns JSON-formatted information about the cluster.
Important:
Save the JSON output during a separate computer file, as a result of you would like the ssh keys later during this document.
Note:
Note: If you get the below error when running the on top of az aks, create command, then re-run an equivalent command once more
Note:
While creating AKS, internally a new resource group is created (like MC_<Resource Group Name>_<AKS Name>_<Resource Group Location>) which is consists of a Virtual machine, Virtual network, DNS Zone, Availability set, Network interface, Network security group, Load balancer and Public IP address etc.…
Connect kubectl to your Kubernetes cluster by using the az aks get-credentials command and configure accordingly. This step downloads credentials and configures the Kubernetes user interface to use them.
You should also check that you are able to open the Kubernetes dashboard by running
This will launch a browser tab with a graphical representation:
Thanks
For Reference
kishore1021.wordpress.com
azure.microsoft.com
www.ais.com
👋 Join us today !!
️Follow us on LinkedIn, Twitter, Facebook, and Instagram
If this post was helpful, please click the clap 👏 button below a few times to show your support! ⬇
AVM Consulting — Clear strategy for your cloud
486 
We are developing blogging to help community with Cloud/DevOps services Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
486 claps
486 
Written by
Vineet Sharma-Founder and CEO of Kubernetes Advocate Tech author, cloud-native architect, and startup advisor.https://in.linkedin.com/in/vineet-sharma-0164
AVM Consulting — Clear strategy for your cloud
Written by
Vineet Sharma-Founder and CEO of Kubernetes Advocate Tech author, cloud-native architect, and startup advisor.https://in.linkedin.com/in/vineet-sharma-0164
AVM Consulting — Clear strategy for your cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/move-data-from-on-premise-sql-server-to-azure-blob-storage-using-azure-data-factory-bbf67e4e5fde?source=search_post---------142,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Nov 18, 2019·8 min read
Although there is an official tutorial for copying data from on-premise SQL Server to Azure Blob Storage (https://docs.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-portal), this article will focus on some details that were not covered in that tutorial. For example,
"
https://medium.com/@renatogroffe/docker-azure-devops-build-e-deployment-automatizado-de-aplica%C3%A7%C3%B5es-15cec26174c3?source=search_post---------143,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Dec 23, 2019·14 min read
Plataforma com foco em DevOps e mantida pela Microsoft, o Azure DevOps é uma solução bastante abrangente. Dentre as funcionalidades oferecidas por este serviço estão controle de versão do código-fonte de projetos de software (GIT ou TFVC), gestão de tarefas baseando-se para isto em Scrum ou Kanban, integração contínua e deployment automatizado, automação de testes e dashboards para monitoramento de um projeto como um todo.
Neste tutorial demonstrarei como efetuar o build de imagens Docker e o deployment automatizado empregando containers, fazendo uso para isto dos seguintes recursos:
Ao optar pela utilização de Docker em conjunto com o Azure DevOps conseguimos simplificar em muito o processo de build e deployment, já que essa integração nos isenta da necessidade de instalar runtimes específicos e realizar inúmeros ajustes de configuração (as definições presentes no arquivo Dockerfile costumam ser mais do que suficientes para a compilação do projeto e o build da imagem correspondente).
E aproveito este espaço também para um convite…
Que tal aprender mais sobre Azure Functions e desenvolvimento de soluções Serverless, em um workshop que acontecerá durante um sábado (dia 18/01/2020) em São Paulo Capital e implementando um case na prática? Acesse então o link a seguir para efetuar sua inscrição com um desconto especial de 25%: http://bit.ly/anp-serverless-blog-groffe
E caso você precise conhecer mais sobre o Microsoft Azure como um todo, não deixe de aproveitar o preço promocional de lançamento da primeira turma online do treinamento Azure na Prática que acontecerá dia 15/02/2020 (também um sábado). Aproveite para conhecer mais sobre dezenas de serviços e possibilidades oferecidas pelo Azure e, o melhor, no conforto de sua casa ou ambiente de trabalho! Acesse o link a seguir para informações e efetuar sua inscrição: http://bit.ly/anp-online-blog-groffe
O projeto ASP.NET Core 3.1 que servirá de base para esse tutorial já foi disponibilizado no GitHub:
https://github.com/renatogroffe/ASPNETCore3.1-API-REST_Docker-Alpine
Acabei optando por duplicar este repositório, já que nesta cópia será gravado um arquivo com configurações de build do Azure Pipelines.
Já na listagem a seguir está o conteúdo do arquivo Dockerfile, em que estão referenciadas as imagens Alpine do SDK do .NET Core 3.1 (para restauração de pacotes e build da aplicação) e do runtime do ASP.NET Core 3.1 (com o ambiente necessário para a execução da API REST a partir de um container):
O Azure Containter Registry permite o armazenamento de imagens de containers de maneira privada, representando assim uma alternativa dentro da nuvem da Microsoft ao Docker Hub.
Nesta seção demonstrarei a criação de um novo recurso baseado no Container Registry, a fim de que no mesmo constem as imagens geradas através do build automatizado do projeto via Azure DevOps. Acionar para isto no Portal do Azure a opção Create a resource:
Em Containers selecionar Container Registry:
E finalmente preencher em Create container registry:
Confirmar finalmente a geração do Container Registry clicando no botão Create:
Projetos do Azure DevOps estarão agrupados em Organizations. Para o exemplo descrito nesse tutorial será criada uma Organization chamada groffe-demos. Acessar para isto a opção New organization:
Será solicitado então um aceite dos termos de utilização do Azure DevOps:
Informar em seguida o nome da Organization, bem como o Data Center em que novos recursos no Azure DevOps estarão hospedados; concluir este procedimento acionando a opção Continue:
Na tela inicial da Organization groffe-demos será solicitada a criação de um novo projeto, no qual constarão as definições para build e release/deployment da aplicação ASP.NET Core. Preencher para isto os campos Project name e Description, selecionando ainda a opção Private , Git em Version control, Basic em Worker item process e finalmente clicando na opção Create project:
A próxima imagem traz o projeto APIContagem já criado; acionar na sequência a opção Project settings:
Aparecerão agora diversas opções para configuração do projeto:
Deslocar a barra de rolagem para baixo até Pipelines, clicando então na opção Service connections:
Teremos agora que criar uma nova conexão para o recurso do Azure Container Registry gerado anteriormente. Em New service connection selecionar a opção Docker Registry:
Em New Docker Registry service Connection:
Confirmar este procedimento acionando a opção Save:
Na imagem a seguir podemos observar a conexão ao Azure Container Registry já criada:
Será por meio do Azure Pipelines que definiremos o processo automatizado (pipeline) de build de imagens Docker para o projeto APIContagem. Retornando à seção Summary deste projeto acessar na barra lateral Pipelines > Pipelines, como indicado na imagem a seguir:
Acessar agora a opção Create Pipeline:
Em Where is your code? selecionar a opção GitHub YAML:
Já em Select a repository definir o repositório do GitHub ao qual estará atrelado o pipeline de build:
Será solicitado neste momento que o usuário se autentique junto ao GitHub. Realizado este procedimento, aparecerá agora uma tela com informações do Azure Pipelines:
Descer então com a barra de rolagem até o final da página, certificando-se de que o repositório a ser utilizado está selecionado em Repository access. Confirmar esta escolha acionando o botão Approve and install:
Em Configure your pipeline selecionar a opção Starter pipeline, a fim de iniciar a montagem do pipeline com um mínimo de configurações:
Neste momento aparecerá a tela Revise your pipeline YAML. Posicionar agora o cursor ao final do conteúdo do arquivo YAML com definições de pipeline:
Acionar na sequência o botão Show assistant:
Localizar agora a task Docker e clicar sobre a mesma:
A partir deste ponto serão preenchidas as configurações da task correspondente ao build de uma imagem Docker. Em Container registry selecionar a conexão criada anteriormente para o Azure Container Registry:
Em Container repository informar o nome da imagem a ser gerada (apicontagem-azuredevops para o exemplo aqui descrito):
Já em Commands:
Concluir o processo acionando o botão Add:
Na próxima imagem podemos observar o conteúdo do YAML do pipeline de build já atualizado:
Foram acrescentadas as definições destacadas na imagem a seguir (em vermelho):
Concluir a configuração deste pipeline acionando a opção Save and run:
Em Save and run:
Acionar finalmente o botão Save and run:
Neste momento terá início um Job para build da imagem Docker da aplicação de testes (ícone azul):
Clicando sobre este Job podemos observar o andamento do mesmo:
Após algum tempo o status do Job indicará que o processo de geração da imagem Docker teve sucesso (ícone verde):
Observando o repositório no GitHub será possível constatar a presença do arquivo azure-pipelines.yml, gerado via Azure DevOps e no qual constam as definições de build:
Já na próxima imagem temos o conteúdo do arquivo azure-pipelines.yml, com as configurações especificadas nos passos anteriores desta seção:
Acessando o recurso do Azure Container Registry criado na seção anterior será possível notar a presença da imagem apicontagem-azuredevops em Repositories:
Clicando sobre esta imagem serão listadas as diferentes tags/versões existentes para a mesma (11 e latest, que basicamente correspondem à mesma imagem):
A gravação de uma alteração na branch master fará com que o processo de build das imagens apicontagem-azuredevops seja disparado automaticamente. A fim de simular isto farei aqui uma alteração no arquivo ContadorController.cs:
Uma nova execução do build terá início e poderá ser consultada via Azure DevOps (detalhes como o que acontece no Job envolvido neste processo podem ser obtidos seguindo as orientações da seção anterior):
Após algum tempo o processo concluirá, indicando sucesso na geração de uma nova imagem:
Voltando ao recurso do Azure Container Registry poderemos constatar a presença de uma nova tag de número 12 (a tag latest é a mais atual, juntamente com 12):
Nesta seção demonstrarei a criação de um novo recurso baseado no Azure Web App for Containers, o qual fará uso da imagem apicontagem-azuredevops (hospedada no Container Registry groffeazuredevops) para a execução de instâncias da API de testes a partir de containers na nuvem Microsoft.
Utilizando o Portal do Azure incluir um novo recurso, utilizando para isto a opção Web App for Containers:
Em Web App informar:
Dar andamento à criação do recurso acionando a opção Next: Docker>.
Informar na seção Docker as definições da imagem utilizada no deployment da aplicação:
Clicar em seguida no botão Review + create:
Após a revisão acionar finalmente o botão Create, para assim finalizar este processo com a geração de um novo recurso do Azure Web App for Containers:
Na próxima imagem podemos observar o recurso já criado, com o endereço de acesso ao mesmo destacado em vermelho:
Um teste de acesso à API REST trará no resultado a alteração realizada na branch master:
Ainda no Azure Pipelines iremos configurar o pipeline para release/deployment automatizado da aplicação, fazendo uso para isso de imagens do Azure Container Registry e do recurso do Azure Web App for Containers descrito na seção anterior.
Acessar para isso em Pipelines a opção Releases, clicando em seguida no botão New pipeline:
Em Select a template acionar a opção Azure App Service deployment, posicionando o mouse sobre a mesma e efetuando um clique no botão Apply:
Preencher o campo Stage name em Stage com o valor Deploy Web App:
Retornando ao diagrama do Pipeline clicar na opção 1 job, 1 task do item Deploy Web App:
Preencher agora as seguites configurações para a execução do stage Deploy Web App (seção Parameters):
Acionar então o botão Save, a fim de confirmar as configurações realizadas:
Um comentário poderá ser preenchido na janela Save; concluir este procedimento clicando no botão OK:
Clicar agora na opção + Add em Artifacts:
Em Add an artifact:
Na sequência teremos que ativar o deployment automatizado. Acionar para isto a opção Continuous deployment trigger em Artifacts (ícone com o sinal de um raio):
Marcar então o item Continuos deployment trigger como Enabled:
Neste momento um sinal de check aparecerá do lado do ícone com o raio em Artifacts, indicando que o deployment contínuo/automatizado foi devidamente configurado. Concluir os ajustes clicando no botão Save:
Informar se necessário um comentário na janela Save, confirmando os ajustes realizados através de um clique no botão OK:
Sucessivos commits na branch master resultarão no build de novas imagens da APIs REST (como já demonstrado anteriormente). A novidade agora está também no deployment automatizado após os últimos ajustes, fazendo uso de tais imagens para publicação no recurso baseado no Azure Web App for Containers.
Considerando uma alteração como a indicada a seguir:
Logo depois do build das imagens acontecerá início o deployment automatizado, como indicado na próxima imagem:
Na imagem a seguir podemos observar o deployment em andamento:
E finalmente concluído:
Analisando o recurso do Azure Container Registry veremos que 18 foi a tag da última imagem gerada:
Verificando a seção Container settings do recurso do Azure Web App for Containers é possível constatar que o mesmo já foi atualizado para trabalhar com containers baseados nesta tag (18):
Acessando a API para testes notaremos que no retorno da mesma já consta a alteração efetuada na classe ContadorController.cs:
Azure DevOps Services
Canal Julio Arruda - MVP
Azure DevOps Sprints - Canal Vinicius Moura - MVP
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
83 
1
83 claps
83 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/aris-recommended-azure-dev-ops-resources-a6d819c4935e?source=search_post---------144,"There are currently no responses for this story.
Be the first to respond.
Tldr; Recently I was asked to put together a list of resources by a member of the local Israeli developer ecosystem, for getting started with Azure DevOps for someone who is familiar with the cloud but not with Azure here is that list.
azure.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
If you need help from the League of Extraordinary Cloud DevOps Adocates just use #LoECDA. It is like our bat signal. Members are @AbelSquidHead @damovisa @DonovanBrown @StevenMurawski and @jldeen
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
If you have any questions, comments, or topics you would like me to discuss feel free to follow me on Twitter.
About the AuthorAaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
51 
1
51 claps
51 
1
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@renatogroffe/asp-net-core-2-0-deployment-na-nuvem-com-docker-azure-container-registry-e-azure-web-app-on-linux-17a7dc80f723?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 18, 2017·8 min read
O objetivo deste tutorial é demonstrar o deployment de um projeto ASP.NET Core 2.0 no Microsoft Azure, fazendo uso para isto de containers Docker e de serviços de hospedagem que integram esta plataforma de cloud computing (Azure Container Registry e Azure Web App on Linux).
A aplicação que será utilizada - um site MVC - já foi descrita no artigo indicado a seguir, o qual abordou o consumo de APIs REST disponibilizadas gratuitamente pela NASA (Agência Espacial Norte-Americana):
Consumo de APIs em .NET Core: utilizando APIs REST da NASA
Esse projeto está disponível inclusive no GitHub:
https://github.com/renatogroffe/ASPNETCore2_NASA-Open-APIs
Caso queira saber mais sobre o Azure Web App on Linux e o uso de Docker em aplicações ASP.NET Core consulte também o seguinte post:
ASP.NET Core: APIs REST na nuvem com Docker e Azure Web App
Para gerar uma imagem Docker contendo a aplicação mencionada neste artigo serão utilizados recursos do próprio Visual Studio 2017. Isto acontecerá por meio de um conjunto de ferramentas conhecidas como Visual Studio Tools for Docker, as quais possibilitam o building, o debugging e a execução de containers a partir de projetos gerados com o .NET Core.
No Visual Studio 2017 será necessário adicionar os arquivos de suporte ao Docker no projeto SiteDadosNASA. Acionar então o menu de contexto para esse site e, em seguida, as opções Add e Docker Support (este processo também pode ser realizado durante a criação de um novo projeto):
Aparecerá neste momento a janela Docker Support Options. Selecionar em Target OS a opção Linux:
Este conjunto de ações produzirá como resultados os seguintes itens:
O contéudo do arquivo Dockerfile está na próxima listagem:
Já a listagem a seguir traz as configurações do arquivo docker-compose.yml:
Após estes procedimentos o projeto SiteDadosNASA já estará devidamente configurado para execução e, até mesmo, debugging a partir de um container Docker. O próprio ícone do Visual Studio para iniciar a aplicação indica isto:
Com a aplicação SiteDadosNASA em modo de execução será possível constatar a existência de uma imagem Docker criada para testes de desenvolvimento, além de um container baseado em tal estrutura.
Ao executar o comando docker images no PowerShell aparecerão as imagens microsoft/aspnetcore:2.1 e sitedadosnasa:dev:
Já o comando docker ps -a trará o container gerado para utilização a partir do Visual Studio:
Será preciso agora gerar uma nova imagem Docker com base nas configurações de release da aplicação. Selecionar para isto a opção Release (ao invés de Debug) no Visual Studio 2017:
Efetuar na sequência a compilação do projeto (a partir do menu Build > Build Solution). Ao executar novamente o comando docker images no PowerShell aparecerá então a imagem sitedadosnasa:latest:
OBSERVAÇÃO: os procedimentos descritos neste artigo foram realizados a partir do Visual Studio 2017 Update 15.3, utilizando ainda o Docker for Windows em um ambiente de desenvolvimento baseado no Windows 10.
O Azure Containter Registry permite o armazenamento de imagens de containers de maneira privada, trazendo assim uma alternativa dentro da nuvem da Microsoft ao uso de planos pagos do Docker Hub. Este serviço pode ser empregado em conjunto com tecnologias como Azure Container Services (com seus diferentes orquestradores - Docker Swarm, DC/OS e Kubernetes) e Azure Web App on Linux.
No portal do Azure será criado um novo recurso baseado no serviço Azure Container Registry (em português Registro de Contêiner do Azure):
Informar no formulário de criação:
Após alguns segundos o item renatogroffe (um Container Registry) aparecerá na lista de recursos disponíveis:
Uma tag chamada renatogroffe.azurecr.io/sitedadosnasa deverá ser criada para a imagem sitedadosnasa:latest. Este novo elemento contém o nome que será gravado no Azure Container Registry (formado pela identificação do registro de containers + nome da aplicação/imagem; esses dois itens estão separados ainda por uma barra - “/”). Executar para isto o seguinte comando no PowerShell:
docker tag sitedadosnasa:latest renatogroffe.azurecr.io/sitedadosnasa
Ao acionar novamente a instrução docker images no PowerShell aparecerá uma imagem chamada renatogroffe.azurecr.io/sitedadosnasa:
O próximo passo agora será efetuar o login no recurso do Azure Container Registry criado na seção anterior. Executar para isto o seguinte comando no PowerShell (em que serão fornecidos o usuário e uma senha disponibilizados pelo Microsoft Azure):
docker login renatogroffe.azurecr.io -u USUÁRIO -p SENHA
As credenciais necessárias estão na seção Access keys do Container Registry (é possível utilizar qualquer uma das senhas indicadas em password e password2):
A imagem a seguir mostra que a autenticação teve sucesso:
Para publicar a imagem renatogroffe.azurecr.io/sitedadosnasa no Azure Container Registry será utilizado o comando:
docker push renatogroffe.azurecr.io/sitedadosnasa
O resultado desta ação pode ser observado na próxima imagem:
A imagem renatogroffe.azurecr.io/sitedadosnasa aparecerá então no Azure Container Registry (seção Repositories), logo após a conclusão deste último procedimento:
Mais uma vez será criado um novo recurso no portal do Azure, desta vez baseando-se no serviço Web App on Linux:
Preencher no formulário de criação do recurso:
Acionar após informar o conteúdo destes campos a opção Configurar contêiner:
Em Contêiner do Docker selecionar o item Registro privado, especificando na sequência as seguintes configurações:
Acionar então o botão OK em Contêiner do Docker, confirmando as definições a serem utilizadas para a geração do novo container. Concluir este processo clicando sobre o botão Criar, a partir do formulário em que se especificaram as configurações do site a ser hospedado na nuvem:
A opção Todos os recursos listará, após um curto período de tempo, o recurso consultadadosnasa:
Acessando o item consultadadosnasa aparecerá um painel no qual constará o endereço da aplicação de testes, bem como opções para a configuração e gerenciamento deste recurso:
Testes com a URL http://consultadadosnasa.azurewebsites.net/ trarão resultados similares aos encontrados nas imagens a seguir:
Muito embora este artigo tenha focado no deployment de aplicações ASP.NET Core 2.0 na nuvem, o uso de serviços como Azure Container Registry e Azure Web App on Linux não está restrito à nova plataforma de desenvolvimento Web da Microsoft. Projetos implementados em linguagens como Node.js, Java, Ruby, Python e PHP também podem se valer dos recursos aqui descritos, sendo que isto é possível graças ao suporte que tais tecnologias oferecem para o trabalho com containers Docker.
ASP.NET Core - Documentation
Azure Container Registry - Documentation
Conteúdos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0
Introduction to Azure Web App on Linux
Visual Studio Tools for Docker
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
62 
2
62 claps
62 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/serverless-from-the-beginning-using-azure-functions-azure-portal-part-i-7334a083b8f0?source=search_post---------146,"Follow me on Twitter, happy to take your suggestions on topics or improvements /Chris
Serverless is Cloud-computing execution model in which the cloud provider runs the server, and dynamically manages the allocation of machine resources. So essentially you can focus on writing code as your Cloud Provider does the rest
This article has been moved to https://softchris.github.io/pages/serverless-one.html
"
https://medium.com/@thisiszone/image-recognition-using-the-azure-custom-vision-service-c0bfc74a3343?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zone
Nov 22, 2018·9 min read
Zone’s head of .NET development, Andy Butland, explores the AI and machine learning possibilities offered by Azure’s Custom Vision Service…
In recent years, the availability of machine learning algorithms, made available as services, has transformed our ability to add artificial intelligence features to applications. Functionality that was once the remit of hardcore AI experts can now be accessed by a much wider range of developers armed with just a cloud subscription.
I’ve recently been working with one such service: the Custom Vision Service, part of the suite of Cognitive Services, provided by Microsoft and hosted at Azure.
The Cognitive Services offer several specific AI or machine learning tasks, provided as services via an API, that we can integrate into web, mobile and desktop applications, using JSON-based REST calls over HTTP or, in many cases, a higher-level client library.
Payment for the services is via a pay-as-you-go model where cost will depend on usage, with generous free tiers that can be used for experimentation, development and low-volume production requirements.
There are two services offering features related to image recognition. The first – and more established – is Computer Vision, which uses a pre-built model created and maintained by Microsoft. The Custom Vision service, currently available in preview, allows you to build and train your own model, dedicated to a given image domain.
Although there’s more work involved in sourcing images, tagging them, training and refining the model, the advantage of a dedicated one is that we’ll likely get improved accuracy when using only images from our chosen subject. This is due to the model being able to make finer distinctions between images and to avoid distractions from superficially similar but in reality quite different image subjects.
To create a Custom Vision Service Model, you’ll need an Azure subscription. Sign in and, after typing “custom vision” into the search box, you’ll find a link to the service. Similar to almost all other Azure resources, you’ll be presented with a blade where you need to select the name, pricing tier, location and some other details, following which the service will be instantiated.
Pricing details can be found here. Payment is based on usage and a small amount for storage, but for investigating the service the free tier is perfectly adequate and gives access to all features, though you are limited to a single project.
The location can currently only be a single Azure region in the US, but I’d expect the service to be rolled out to more data centres as it comes out of preview, reducing latency for accessing it from websites and applications hosted in other areas of the world.
Although some aspects common with other Azure services are managed from the standard Azure portal, in most cases we work in a custom portal dedicated to the service, accessed via the “Quick start” menu.
Once in this custom portal we can create a project, giving it a name, description, selecting between single and multiple subject per images options, and selecting a domain. The section here is important if you want to use the model in offline contexts, which I’ll discuss later in this article, but for now only if one of the selections (such as “landmarks” or “food”) matches our chosen subject matter should we choose anything other than “General”.
With the project created we then need to provide some pre-classified source images that we can upload via the interface and associate them with the required tag. Make sure to keep some back from the training set of images for each tag, so we have some images the model hasn’t seen directly to evaluate with.
The next step is to train the model via the green button at the top, which, once complete, will show us some statistics of the expected model performance.
We can continue to tweak the model to look to improve these numbers by providing further images. We should aim for at least 50 in each category, with a balanced number across each, and look to provide the range of viewpoints, backgrounds and other settings in which test images may be provided when the model is in use. Each time we do this we can retrain and get a new iteration of the model with an updated set of statistics.
Up to now we’ve interacted with the Azure portal interfaces for the purposes of creating and working with our model. While we might continue to use this method for initial training of the model, and ongoing refinement of it, the real power of the service comes from the API – known as the prediction API – that allows us to integrate it within our own applications.
Before doing that though, we need to extract three pieces of information from the portal, which we’ll need to pass for identification and authorisation purposes in the API requests. These are the project ID, the project URL and the prediction API key.
There are actually two ways to access the prediction API. The first, and only if working cross-platform, is via REST services called over HTTP, passing requests and parsing response in JSON. If on Windows, we also have the option of leaning on a client library, that abstracts away some of the underlying details of the HTTP requests and allows us to work at a higher, and more strongly-typed, level.
Although the latter is simpler, the lower-level, HTTP-based approach is also quite straightforward. The key code sample for using this is shown below:
As we’ll interact with the API by making REST calls over HTTP, first we need to create a HttpClient object.
Authorisation of access to our model is carried out by the checking of a passed header value, which we need to set before we make the request. The name of the header is Prediction-Key and the value is that which we obtained from the custom vision portal for our project. We’ve stored this in a settings file, which, within Azure function projects from which the above sample is taken, are surfaced in the form of environment variables.
Similarly, we’ve stored and can access the prediction URL end point of our model that we’re going to make our request to.
We’re using the API option where we provide the image as part of the body of our request – rather than providing a URL to the test image which is also supported – so need to convert the uploaded file from the stream we have access to, to an array of bytes, which is then wrapped within a HttpContent instance.
Having set the content type header to application/octet-stream we can then make our HTTP request using the POST method.
The response comes back as a JSON string in the example you can see, which we can deserialise into an instance of a strongly typed class we’ve also created. As well as some detail about the project and iteration used, we get back a list of predictions – one for each tag, and for each tag a probability of how confident the model is that the provided image matches the tag.
With the Windows client library, the code is a little simpler:
We’re leaning here on two NuGet packages. First Microsoft.Cognitive.CustomVision.Prediction, which is the client library itself, and then Microsoft.Rest.ClientRuntime – another library it depends on used for wrapping the REST HTTP calls.
From these, we have a model class provided, so we don’t need to create our own and ensure that it matches with the expected JSON response. We are also abstracted away from the HTTP calls, so there’s no HttpClient to work with, and need to set headers and parse responses.
Rather we just create a new endpoint object, providing the prediction key that we store in settings and is made available via an environment variable. We then make a call using the PredictImageAsync method, passing the project ID –also held in settings – and a stream representing the image file itself. We get back a strongly typed response that we can then handle as we need.
The Custom Vision Service provides a second API – known as the training API – that we can use for integrating features of building and training models into our applications. While this won’t always be useful – and we can of course continue to use the portal for such tasks – there could be cases where maintaining a model becomes part of a business process and as such makes sense to provide such features within a custom application.
We work with the training API in a similar way to that shown for the prediction API, though I’d suggest in this case, although we could use the lower-level HTTP calls, the nature of the training features mean we’ll likely be making a number of different requests to the API, and as such, would likely be wanting to create some form of wrapper to keep our code maintainable. Rather than doing that though, it makes sense to use the open-source client library that’s already available, at least if supported on the platforms you need to work with.
Sometimes, a real-time request over the internet isn’t feasible or appropriate – perhaps for a mobile or desktop application where network connectivity may not be permanently available. The Custom Vision Service supports scenarios like this allowing us to export the model into a file we can embed and use within our applications.
I mentioned earlier when creating a Custom Vision Service project that we had the option to select a domain. We had a choice between various high-level subject matters and also the option to select a “compact” option, and this is what we need to use for a model we want to be able export and use offline. Fortunately, we don’t have to start creating our model all over again if we’ve already created it with a standard domain; we’re able to change the domain of an existing model and then retrain it to create a new iteration using the selected domain.
Only “compact” domains can be exported, but in choosing this we do have to accept a trade-off in a small reduction in accuracy, due to some optimisations that are made to the model for the constraints of real-time classification on mobile devices.
When we export and download a model iteration, we have the choice of a number of formats, each of which are suitable for different application platforms. For example, we can select CoreML for IoS, TensorFlow for Android and Python or ONNX for Windows UWP.
The primary reason I’ve been investigating and working with the Custom Vision Service is due to working on authoring a course for Pluralsight, where I go into the subject in a lot more detail than covered in this article, illustrate demos and sample applications, walk through code and discuss more background of image classification theory and how we can measure and refine our models.
If you have a Pluralsight subscription, or are interested in taking out a free trial, you can access the course here.
As part of working on this course I’ve released the code samples as open-source repositories on GitHub, that you can access here:
Further useful links and references I used in authoring the course, samples and writing the article are:
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
105 
105 claps
105 
We write about customer experience, employee experience, design, content & technology to share our knowledge with the wider community.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/building-rest-api-with-python-flask-and-azure-sql-4e99e2b53180?source=search_post---------148,"There are currently no responses for this story.
Be the first to respond.
Azure SQL has native JSON support which is a key factor to simplify a lot — and make developer-friendly — the interaction between the database and any service that needs to handle data in even the most exotic way.
"
https://medium.com/swlh/azure-devops-yml-terraform-pipeline-and-pre-merge-pull-request-validation-6352b841376a?source=search_post---------149,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
tl;dr: Here’s YML code that will build an Azure DevOps pipeline that can be run automatically as part of pull request validation (pre-merge) and requires manual approval by definable admin groups in order to proceed to modifing resources.
Microsoft’s Azure DevOps (ADO) is an incredibly powerful CI/CD platform that is being rapidly…
"
https://medium.com/wavesprotocol/microsoft-azure-cloud-features-waves-smart-assets-and-smart-accounts-1a71b3c23c2b?source=search_post---------150,"There are currently no responses for this story.
Be the first to respond.
Waves steps up integration with Microsoft Azure, making its Smart Asset and Smart Account functionality available to developers on the cloud platform.
Waves’ smart accounts and smart assets are now available to developers on Azure Marketplace in a special extension for Visual Studio Code, which simplifies the process of creating and operating smart contracts.
Two new virtual machine templates in Azure can be used for setting up new options on a public or private blockchain.
‘Unlike other similar solutions, non-Turing complete smart contracts from Waves offer increased security and protection from vulnerabilities due to the option of restricting functionality to avoid undesirable use’, comments Sasha Ivanov, Founder and CEO of Waves Platform.
‘A special extension for Microsoft Visual Studio Code editor will simplify the process of writing smart contracts for developers, allowing them to work in complete integration with Microsoft Azure cloud,’ he continues. ‘I am confident that users will hugely appreciate the variety of opportunities and possibilities this opens up for them!’
‘We believe that supporting open-source platforms and facilitating the widespread availability of contemporary technological tools for the developer community is very important,’ says Konstantin Goldstein, Microsoft Russia’s principal technical evangelist.
‘Waves is one of world’s leading blockchain platforms, which contributes substantially to the development of an ecosystem for global decentralised solutions, and we are glad to support the company in its development,’ he adds.
Waves Platform has been deployed in Azure cloud since 2017.
The functionality of Waves’ smart accounts and smart assets can be applied in a variety of use cases, including supply chain management, verification of documentation, organization of internal transactions, filing accounting reports and game development.
Read more about Waves Smart Accounts and Smart Assets.
Join Waves CommunityRead Waves News channelFollow Waves TwitterSubscribe to Waves Subreddit
Open blockchain protocol and development toolset for Web 3.0 applications and decentralized solution
303 
303 claps
303 
Waves is an open blockchain protocol and development toolset for Web 3.0 applications and decentralized solutions, aiming to raise security, reliability and speed of IT systems. It enables anyone to build their apps, fostering mass adoption of blockchain.
Written by
Waves Tech is a powerful blockchain-agnostic ecosystem focused on inter-chain DeFi, the embodiment of technological freedom for blockchain-based finance.
Waves is an open blockchain protocol and development toolset for Web 3.0 applications and decentralized solutions, aiming to raise security, reliability and speed of IT systems. It enables anyone to build their apps, fostering mass adoption of blockchain.
"
https://medium.com/@renatogroffe/asp-net-core-azure-web-app-for-containers-escalando-uma-api-rest-com-containers-docker-f16bca26a019?source=search_post---------151,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 15, 2018·6 min read
Preocupação muitas vezes relegada a um segundo plano, a necessidade de escalar uma API REST pode resultar em grandes esforços em termos de preparação e configuração de recursos de infraestrutura em ambientes on-premises (locais). Como podemos então simplificar e tornar mais rápido este processo, de forma a atender a uma demanda crescente de acessos em nossas APIs?
Uma resposta a isto seria a utilização de algum serviço de hospedagem na nuvem. Alternativas do tipo PaaS (Plataform as a Service) permitem uma maior ênfase na construção e entrega de uma solução, dispensando os envolvidos no projeto de grandes preocupações no que se refere à montagem de toda a infraestrutura requerida. Esta é sem sombra de dúvidas uma grande vantagem, mas ainda assim existe o risco de se ficar atrelado a uma tecnologia específica disponibilizada por um provider de cloud computing. E como superar tal limitação?
A resposta a esta segunda questão passa pelo uso de Docker. A escolha por serviços compatíveis com containers Docker contribui para que um projeto fique menos dependente de tecnologias proprietárias na nuvem, tornando assim viável a troca de um fornecedor de serviços de cloud por outro (frente a condições econômicas ou técnicas que estimulem tal movimento).
No caso específico do Microsoft Azure, temos diversas opções para a hospedagem de aplicações containerizadas. Uma destas alternativas é o Azure Web App for Containers, um serviço que permite a publicação de APIs e sites a partir de imagens Docker e que conta inclusive com suporte a soluções de CI/CD (Continuous Integration/Continuous Deployment) como Visual Studio Team Services (VSTS), Jenkins, Maven e Travis CI.
Este artigo aborda a publicação de uma API REST a partir de uma imagem Docker empregando para isto o Azure Web App for Containers, bem como o processo de escalar horizontalmente a aplicação correspondente a fim de que 2 instâncias (containers) atendam a solicitações HTTP enviadas à mesma. Trata-se de um cenário comum a aplicações de médio e pequeno porte, sendo que o uso do Web App for Containers provê toda a infraestrutura necessária sem grandes complicações no que se refere ao deployment e gerenciamento de recursos.
E aproveito este espaço para deixar aqui um convite.
Dia 25/07/2018 (quarta-feira) às 21h30 - horário de Brasília - teremos mais um hangout no Canal .NET. Confira esta apresentação online com o MVP Elemar Júnior e aprenda mais sobre a modelagem de Microsserviços com base em processos de negócio.
Para efetuar a sua inscrição acesse a página do evento no Facebook ou então o Meetup. A transmissão acontecerá via YouTube, em um link a ser divulgado em breve.
Para os testes descritos neste artigo será utilizada uma API REST baseada no ASP.NET Core. Esta aplicação produzirá como retorno a quantidade de acessos à API, além de exibir o nome do host/máquina e do sistema operacional utilizado pelo container Docker. O projeto em questão já foi detalhado no seguinte artigo:
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem - parte 1
Esta mesma aplicação conta com uma imagem (renatogroffe/apicontagem) no Docker Hub e que servirá de base para a publicação da API no Azure:
Os fontes empregados para a geração desta imagem (incluindo o arquivo Dockerfile) já foram disponibilizados no GitHub:
https://github.com/renatogroffe/ASPNETCore2.1_Docker
Importante destacar que a imagem em questão faz uso da distribuição Alpine Linux. Maiores detalhes sobre a utilização desta alternativa com Docker e ASP.NET Core 2.1 podem ser encontrados no artigo a seguir:
ASP.NET Core 2.1: Release Candidate 1 e Docker Alpine
Outras informações sobre o uso de Docker com ASP.NET Core e Azure podem ser encontrados no seguinte post, no qual estão todos os conteúdos que venho produzindo sobre o uso conjunto dessas tecnologias:
Docker para Desenvolvedores .NET - Guia de Referência
No portal do Azure será necessário criar um novo recurso baseado no serviço Web App for Containers:
Informar no formulário de criação do recurso:
Acionar após o preenchimento destes campos a opção Configure container:
Em Container Settings selecionar Single Container e o item Docker Hub, certificando-se de que a opção Public está marcada em Repository Access e informando ainda no campo Image and optional tag o valor renatogroffe/apicontagem (correspondente à imagem publicada anteriormente no Docker Hub):
Selecionar OK em Container Settings, confirmando as definições a serem utilizadas para a criação do container na nuvem. Concluir então este processo acionando o botão Create no formulário em que se especificaram as configurações da API REST que ficará hospedada na nuvem:
O recurso apicontagem e o Service Plan correspondente aparecerão após alguns segundos dentro do grupo de recursos TesteAPIEscalavel:
Ao acessar o item apicontagem será exibido um painel no qual constará o endereço da aplicação de testes (https://apicontagem.azurewebsites.net/), além de outras opções para a configuração e gerenciamento deste recurso:
O valor do machineName para um primeiro teste com a URL https://apicontagem.azurewebsites.net/api/contador foi b2594f7d5e8d, com o mesmo correspondendo ao nome do container gerado ao publicar a aplicação através do Azure Web App for Containers:
A opção Scale up (App Service plan) possibilita escalar verticalmente a aplicação, através da seleção de configurações do hardware a ser empregado tais como número de cores, memória e storage (este conjunto de funcionalidades não será abordado neste artigo):
Já a opção Scale out (App Service plan) permite escalar horizontalmente a API publicada no Azure Web App for Containers. Este procedimento pode tanto ser efetuado manualmente, através do campo Override condition em Configure:
Quanto de forma automática, por meio da opção Enable autoscale. Neste último caso temos a possibilidade de seleção de uma métrica envolvendo critérios como memória, uso de CPU e volume de dados para a configuração do processo de escalonamento da aplicação (este tipo de configuração também não será coberto neste artigo):
Serão definidas em Override condition 2 instâncias para esta API de testes, acionando-se na sequência a opção Save para confirmar esta configuração:
Após alguns segundos uma nova instância/container já estará disponível (machineName = 8cd8c75151e7), além daquela vinculada ao primeiro container (machineName = b2594f7d5e8d). É o que demonstram as imagens a seguir, obtidas após o envio de diversas requisições ao endereço da API de testes:
Esses testes demonstram o quão fácil é escalar uma aplicação Web através do Azure Web App for Containers, dispensando assim os responsáveis pelo deploy de um projeto da necessidade de configuração de mecanismos de load balancer e outros recursos associados a um cenário com múltiplas instâncias.
Docker para Desenvolvedores .NET - Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
35 
3
35 
35 
3
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/azure-synapse-for-data-analytics-create-workspaces-with-cli-bd5ef90fd489?source=search_post---------152,"There are currently no responses for this story.
Be the first to respond.
One of the challenges of large scale data analysis is being able to get the value from data with least effort. Doing that often involves multiple stages: provisioning infrastructure, accessing or moving data, transforming or filtering data, analyzing and learning from data, automating the data pipelines, connecting with other services that provide input or consume the output data, and more. There are quite a few tools available to solve these questions, but it’s usually difficult to have them all in one place and easily connected.
If this article was helpful or interesting to you, follow @lenadroid on Twitter.
This is the first article in this series, which will cover what Azure Synapse is and how to start using it with Azure CLI. Make sure your Azure CLI is installed and up-to-date, and add a synapse extension if necessary:
What is Azure Synapse?In Azure, we have Synapse Analytics service, which aims to provide managed support for distributed data analysis workloads with less friction. If you’re coming from GCP or AWS background, Azure Synapse alternatives in other clouds are products like BigQuery or Redshift. Azure Synapse is currently in public preview.
Serverless and provisioned capacityIn the world of large-scale data processing and analytics, things like autoscale clusters and pay-for-what-you-use has become a must-have. In Azure Synapse, you can choose between serverless and provisioned capacity, depending on whether you need to be flexible and adjust to bursts, or have a predictable resource load.
Native Apache Spark supportApache Spark has demonstrated its power in data processing for both batch and real-time streaming models. It offers a great Python and Scala/Java support for data operations at large scale. Azure Synapse provides built-in support for data analytics using Apache Spark. It’s possible to create an Apache Spark pool, upload Spark jobs, or create Spark notebooks for experimenting with the data.
SQL supportIn addition to Apache Spark support, Azure Synapse has excellent support for data analytics with SQL.
Other featuresAzure Synapse provides smooth integration with Azure Machine Learning and Spark ML. It enables convenient data ingestion and export using Azure Data Factory, which connects with many Azure and independent data input and output sources. Data can be effectively visualized with PowerBI.
At Microsoft Build 2020, Satya Nadella announced Synapse Link functionality that will help get insights from real-time transactional data stored in operational databases (e.g. Cosmos DB) with a single click, without the need to manage data movement.
Prepare the necessary environment variables:
Create a resource group as a container for your resources:
Create a Data Lake storage account:
The output of this command will be similar to:
Retrieve the storage account key:
Retrieve Storage Endpoint URL:
You can always check what your storage account key and endpoint are by looking at them, if you’d like:
Create a fileshare:
Create a Synapse Workspace:
The output of the command should show the successful creation:
After you successfully created these resources, you should be able to go to Azure Portal, and navigate to the resource called $SynapseWorkspaceName within $ResourceGroup resource group. You should see a similar page:
What’s next?
You can now load data and experiment with it in Synapse Data Studio, create Spark or SQL pools and run analytics queries, connect to PowerBI and visualize your data, and many more.
Stay tuned for next articles in this series to learn more! Thanks for reading!
If this article was interesting to you, follow @lenadroid on Twitter.
Any language.
145 
145 claps
145 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Solution Architecture. Distributed systems, big data, data analysis, resilient and operationally excellent systems.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/aws-certified-user-group-turkey/azure-devops-server-2019-rc-1-ile-ci-cd-s%C3%BCre%C3%A7lerine-genel-bak%C4%B1%C5%9F-b4277e9a730a?source=search_post---------153,"There are currently no responses for this story.
Be the first to respond.
Merhabalar tekrardan daha önceki yazılarımızda Atlassian ürün ailesine ait araçları kullanarak DevOps süreçlerini uçtan uca kurgulayarak yazılım takımlarının çevik süreçlere dahil olmalarını ve üretilen çıktıların sürekli entegrasyon kavramı ile kaliteli ve izlenebilir bir hale getirilmesini ele almıştık. Bu yazımız ile bu mimari kurguyu Microsoft tarafından çok yakın zamanda duyurulan Azure DevOps Server 2019 RC1 üzerinde kurgulayacağız.
Microsoft firması çok yakın bir zaman önce DevOps süreçlerini yürütmek üzere kullanıcılara sunduğu TFS ürünü yeniden gözden geçirerek tüm araçlarını ve yeni özelliklerini sunduğu Azure DevOps Server 2019 RC1 uygulama sunucusunu duyurdu. Artık tüm devops süreçlerinizi aynı yapı üzerinden kurgulayarak bir izleme ekranı ile yönetme ve geri izleme yapabilme şansına sahipsiniz.
Azure DevOps Server ile ilgili detaylara bu makale üzerinden ulaşabilirsiniz. Bu sunucu ile birlikte aslında Microsoft tarafında daha önceden kullandığımız TFS ile birlikte azure hizmetleri olarak sunulan Azure Boards , Azure Repos , Azure Pipelines , Azure Test Plans ve Azure Artifacts servislerini on-prem bir sunucuda kullanabiliyoruz.Bu servisler ile ilgili genel bilgileri ilgili link üzerinden takip edebilirsiniz.
Yazımızın genel konusu süreçlerin tasarımı ve kullanımı olduğu için servislerin genel bilgilerini ilgili yerlerde kısaca üzerinden geçerek anlatmaya çalışacağım. Bu süreçleri tasarlamak adına Azure DevOps Server 2019 RC1 kurulum dosyasını indirerek gerekli kurulum aşamalarını tamamlamış olmamız gerekmektedir.
Kurulum ile ilgili olarak ben tek sunucu üzerinde bir kurguyu tercih ettim. Detaylı kurulum adına ilgili medium yazısını incelemenizin yararlı olacağını düşünüyorum.
Sunucu için azure VM modellerinden B2ms ile hemen gerekli alt yapımızı oluşturuyoruz.
Üzerinde windows server 2019 işletim sistemi kurulu olan sanal makinamızda önceden indirdiğimiz Devops Server 2019 RC 1 uygulamasını kurmamız gerekmektedir. En basit şekilde kurulumu aşağıdaki linkten takip ederek gerçekleştirebilirsiniz.
Sunucu kurulum aşamaları :https://1drv.ms/b/s!AskWoAU3NqUug_VU9vKkn8mjTriUmQ
Azure repos , Microsoft ürün ailesinin bizlere sunduğu istediğimiz IDE, düzenleyici veya Git istemcisinden Git depolarına güvenli bir şekilde bağlanabildiğimiz , kod geliştirme ve branch stratejilerimizi yürüttüğümüz ve kontrol ettiğimiz bir web depolama servisidir.
Burada öncelikli olarak git hesabımda bulunan bir örnek uygulama reposundan nasıl kodlarımızı azure repos’a aktardığımızı göstermek istiyorum.
Git hesabı üzerinden DevOpsCore adı altında bir repo oluşturarak , Visual Studio ile oluşturmuş olduğum örnek uygulama kodlarını bu repoya gönderiyorum.
Sonrasında ise kurulumunu gerçekleştirdiğim devops server üzerindeki oluşturduğum “DevOpsTest” üzerinde “TestProject” isimli projeyi oluşturarak artık tüm ilgili devops süreçlerimi bu proje üzerinden yürütmeyi planlıyorum.
Burada bulunan repoya , git üzerindeki kodlarımı çekerek artık devops server üzerinde versiyonlama işlemlerimi yürütebileceğim alt yapıyı hazırlamış oluyorum.
Azure Repos kullanım aşamaları :https://1drv.ms/b/s!AskWoAU3NqUug_VSCB6u_Xp6KnuvtA
Azure repos üzerinde oluşturduğumuz kodlarımızı bir takım kriterlere tabi tutarak build etmemizi sağlayacak ve sürekli entegrasyon kavramının mimari yapımızdaki karşılığı olan azure pipeline kurgusunu oluşturmaya başlayabiliriz.
Öncelikle Devops Server 2019 RC 1 kurulumunu yaptığımız sunucu üzerinde kod derleme ve kontrol etme açısından Visual Studio Code ve Git uygulamalarını kurarak başlıyorum.
Uygulamaları kurduktan sonra bir pipeline oluşturmak istediğimde , bu pipeline için hangi repositoryde çalışmam gerektiğini seçtiğim bir ekran ile karşılaşıyorum.
Gerekli bilgileri doldurup , artık nasıl bir işlemler zinciri oluşturmam gerektiğine karar verip adımları uygulayabildiğim arayüz ile işlemlerime başlıyorum. Burada kendi kurgunuzu oluşturabileceğiniz gibi devops server’ın bizlere sunduğu hazır şablonları da kullanabiliriz. Ben uygulama kodu build edilirken nuget paketlerinin güncellendiği , build işleminin gerçekleştiği , test senaryosunun işlediği ve build çıktısının release için kullanılabilecek bir uygulama olduğu hazır bir şablon seçerek işleme başlıyorum.
Burada build işlemlerimiz, birer pipeline üzerinde bulunan job ve içlerindeki adımları temsil eden tasklar sayesinde işlemektedir. Her task bir adım gibi çalışarak yapılması gereken işlemleri kurgulamamızı sağlamaktadır.
Bizim senaryomuzda şimdilik bir test işlemi gerçekleştirmeyeceğimiz için ben test adımlarını pasif konuma getirerek işlemlere devam ediyorum.
Oluşturmuş olduğumuz pipeline için işlemleri bizim adımıza yürütecek bir agent kurulumu gerçekleştirmemiz gerekmektedir.İlgili Proje ayarlarında pipeline sekmesi altında bulunan agent pool sekmesi ile nasıl bir yol izlememiz gerektiğine ve bir agent isosu indirebileceğimiz linklere ulaşabiliriz.
Buradaki adımları sunucuya kurmuş olduğumuz visual studio code terminal ekranını kullanarak gerçekleştiriyoruz.
Tam bu kısımda bizden bir PAT (Personel Access Token) istiyor. Gerekli işlemleri kullanıcı ayarları menüsü altında bulunan personel access token sekmesinden oluşturmamız gerekmektedir.
Oluşturmuş olduğumuz PAT ile işlemlerimize kaldığımız yerden devam ediyoruz.
Ve en sonunda bizim için tüm işlemleri yürütmekle görevli olan agentımızı bir servis olarak ayağa kaldırmış bulunmaktayız.
Bu kısımda taskların içinde bulunan build işlemini gerçekleştirebilmek adına sunucumuza Visual Studio 2017 Build tools kurulumu gerçekleştirerek gerekli yetkinliği pipeline içerisinde bulunduruyoruz.
Bu build tool ile gelen özelliklerin agent tarafından algılanması için servislere (services.msc) ulaşarak VSTS Agent Servisini tekrardan başlatma işlemini yapmamız gerekmektedir.
Tüm bu işlemler tamamlandıktan sonra ilgili pipeline üzerinde build kurgumuzu bir kez elle tetikleyerek işlemlerin doğru ve kriterlere uygun şekilde tamamlandığını kontrol ediyoruz.
Manuel tetikleme ile birlikte sürecimiz belirlediğimiz şekilde işleyerek gerekli buildi tamamlamış ve uygulama çıktısını bizler için belirlemiş olduğumuz konuma oluşturmuştur.
Artık build işleminin sonucunda bizlere sağlanan çıktıya artifacts sekmesnden ulaşabiliriz.
Azure Pipeline kullanım aşamaları :https://1drv.ms/b/s!AskWoAU3NqUug_VXY8NBx1qScoOgHQ
Build mekanizması ve sürekli entegrasyon adımlarının düzgün şekilde işlediğini manuel olarak test ettikten sonra asıl kurgulamak istediğimiz mimari adına küçük bir test senaryosu uygulamaya başlayabiliriz. Öncelikli olarak Azure Boards kullanarak geliştirmekte olduğumuz yazılım için bir feature isteği gelmiş gibi bir görev oluşturalım .
Bu istek adına ekimizdeki görevi alan geliştirici işlemi development sekmesinden üzerine alır.
Gerekli değişiklikler için bir feature branch oluşturarak ilgili görev ile ilişkisini oluşturur.
Geliştirici kod değişikliklerini yapar ve master branch üzerine merge etmek adına bir adet pull request oluşturur.
Pull request isteği takım içindeki ilgili sorumlu yada başka bir takım arkadaşı tarafından kontrolden geçirilerek merge için izin verilir.Bu senaryoda tek başıma bir işlem gerçekleştirdiğim için merge kontrolünüde ben yapmak durumundayım.
Sonrasında ise tamamlanan merge ile birlikte pipeline üzerindeki build kurgumuz devreye otomatik şekilde girer.
Build işlemi sağlıklı şekilde çalışarak gerekli adımları tamamlamış bulunmaktadır.
Gerekli işlemler tamamlandıktan sonra tekrardan board üzerinde göreve tıklayarak artık tüm geliştirme sürecini takip etmemiz mümkündür.
Test Senaryosu aşamaları :https://1drv.ms/b/s!AskWoAU3NqUug_VV55n70XsZWwCu2A
Devops süreçlerinde sıklıkla kullanılan görev takibi , yazılım yaşam döngüsü izlenebilirliği ve versiyon kontrol sistemlerini bir arada kurgulayabildiğimiz bir yazımızın sonuna daha geldik. Başka bir yazıda görüşmek dileği ile.
“ Çevik süreçler sürdürülebilir geliştirmeyi teşvik etmektedir.Sponsorlar, yazılımcılar ve kullanıcılar sabit tempoyu sürekli devam ettirebilmelidir.” — Agile Manifesto Madde 8
AWS Certified Users Group Turkey experience sharing page.
41 
41 claps
41 
Written by
Muzur bir oğlan babası, hayvan sever, Harry Potter hayranı, bazen maceracı düz yazılımcı.
AWS Certified User Group dedicated certification training for cloud services provided by Amazon. In this group AAI‘ s and certified pro’s are going to share articles about training and certification on AWS, useful tips for AWS services, news and features announced from AWS.
Written by
Muzur bir oğlan babası, hayvan sever, Harry Potter hayranı, bazen maceracı düz yazılımcı.
AWS Certified User Group dedicated certification training for cloud services provided by Amazon. In this group AAI‘ s and certified pro’s are going to share articles about training and certification on AWS, useful tips for AWS services, news and features announced from AWS.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/azure-machine-learning-deployment-workflow-475fc939e96?source=search_post---------154,"There are currently no responses for this story.
Be the first to respond.
This blog post is co-authored by Jaya Mathew and Francesca Lazzeri, data scientists at Microsoft.
The Artificial Intelligence Conference in London is a relatively addition to the list of conferences hosted by O’Reilly worldwide. The aim of this conference is to create a forum for the ever-growing AI community to explore the most essential issues and innovations in applied AI. In the conference the various talks covered topics ranging from practical business applications of AI, to compelling AI enabled use cases, to various technical trainings and deep dive into successful AI projects etc.
In our session “A day in the life of a data scientist in an AI company”, we presented a scientific framework to help organizations to systematically discover opportunities to create value from data, qualify new opportunities and assess their fit and potential, then how to build a team to smoothly implement end-to-end advanced analytics pilots and projects, and produce sustainable ongoing business value from data. Specifically we shared a few important concepts, such as the Machine Learning Workflow and the Team Workspace.
In the session, we also introduced the audience to Azure AI’s latest offering Azure Machine Learning Service. Azure Machine Learning Service (Preview) is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning Service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.
The Machine Learning workflow is an agile, iterative data science framework to deliver predictive analytics solutions and intelligent applications efficiently. It helps improve team collaboration and learning. It contains a distillation of the best practices and structures that facilitate the successful implementation of data science initiatives.
The goal is to help companies fully realize the benefits of their analytics program. The life cycle outlines the major stages that projects typically execute, often iteratively:
The following diagram provides a view of the tasks (in blue) associated with each stage of the life cycle:
In Azure Machine Learning Service, the workspace represents a central location for a team to collaborate and it manages access to compute targets, data storage, models created, docker images created, webservices deployed and it keeps track of all the experiment runs that were performed with it. Data scientists can manage the authorization and creation of workspaces and experiment from the Python SDK.
You can use Python to get started with Azure Machine Learning. In the snippet above, we are creating a workspace called “Demo” in the resource group “Contoso” which resides in the given subscription. The workspace will be created in the Azure region “eastUS2”.
You can create multiple workspaces, and each workspace can be shared by multiple people. When sharing a workspace, control access to the workspace by assigning the following roles to users:
When you create a new workspace, it automatically creates several Azure resources that are used by the workspace:
With Azure Machine Learning Service, once the data scientist builds a satisfactory model, the trained model can be easily put into production and monitored.
The following diagram illustrates the complete deployment workflow:
In the next few paragraphs, we will show how to perform the following steps:
In this second step, you need to create your scoring script, your environment file and your configuration file.
You can deploy registered images into the cloud or to edge devices. here we deploy it to Azure Container Instances, that offers a simple way to run a container in Azure, without having to provision any virtual machines and without having to adopt a higher-level service.
In this blog post, we outline the various aspects that need to be addressed from data collection to metrics for a successful AI model to be used in a production environment. In particular, we introduced the audience to Azure’s latest cloud analytics environment that makes it easy to collect data, analyze, experiment, and build a model for any organization to use. In the last part, we showed how to use Azure Machine Learning Service to deploy your models to to Azure Container Instances.
Any language.
111 
2
111 claps
111 
2
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/visual-studio-code-the-swiss-army-knife-for-threat-hunting-with-azure-sentinel-503e7ef38c96?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Sep 30, 2019·6 min read
Jupyter is a great platform for threat hunting where you can work with data in-context and natively connect to Azure Sentinel using Kqlmagic, but adding Visual Studio Code to the mix will give you even more superpowers!
When working with Visual Studio Code and Jupyter you get intellisense, debugging, a variable and data explorer, and live sharing; making the life of security analysts a bit easier. In this blog I’ll show you how.
Threat Hunting with python, Jupyter and Kusto
Jupyter Notebook, formerly called IPython, is an open-source application that allows you to create and share documents that contain live code, equations, visualizations and narrative text through markdown. It is already broadly used in cybersecurity for threat hunting, and has support for lots of programming languages such as R, Python, etc. The multi-user version of Jupyter is called JupyterHub.
Microsoft created a magic extension for Jupyter called Kqlmagic that allows you to work with Kusto-based workspaces such as Log Analytics, Azure Security Center, Azure Sentinel and more from a Jupyter notebook using KQL (Kusto Query language).
PRO TIP: I’ve written a blog earlier this year that helps you get started and provides a step-by-step tutorial to connect to Azure Security Center and Azure Sentinel. Read it here.
How do I get Jupyter up and running?
Microsoft’s offers a free hosted service called Azure Notebooks to develop and run Jupyter notebooks in the cloud with no installation. Although it is a free service, each project is limited to 4 Gb of memory and 1 Gb data. However, if the Azure Active Directory account you sign in with is associated with an Azure subscription, you can connect a Data Science Virtual Machine (DSVM) instances. DSVM’s are available from the Azure Marketplace and provide you with better processing power and removes any of those limits.
If you want to run Jupyter locally, you can install it using pip. However, I strongly recommend installing Python and Jupyter using the Anaconda distribution, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.
For even more convenience, Jupyter is available as a Docker image. The Jupyter team maintains a set of Docker image definitions in the https://github.com/jupyter/docker-stacks GitHub repository. Not sure which Docker image to choose? Here’s documentation describing them, and their images and relationships. My suggestion: use scipy-notebook.
PRO TIP: Use this command to load a Docker-based Jupyter container on port 8888: docker run -p 8888:8888 — name jupyter jupyter/scipy-notebook
Why use Visual Studio Code?
Jupyter is a great platform for threat hunting where you can work with data in-context and natively connect to security backends in Microsoft Azure using Kqlmagic, but adding Visual Studio Code to the mix will give you even more superpowers!
Microsoft released a Python extension for Visual Studio Code in the extension marketplace that now supports Jupyter and works both on Windows and MacOS. If you are working on Windows and want an isolated environment for working with Python, the Windows Subsystem for Linux (WSL) is a great option. You can enable WSL and install a Linux distribution on your Windows machine, completely isolated from your normal development environment and use the VS Code Remote — WSL extension.
When working with Visual Studio Code and Jupyter you get:
A familiar interface
You might already be using Visual Studio Code for other things, and can just add this to the list of great things to do with VSCode :-)
IntelliSense
The Python Interactive window has full IntelliSense — code completions, member lists, quick info for methods, and parameter hints. You can be just as productive typing in the Python Interactive window as you are in the code editor.
Live Sharing
The Python Interactive window also supports Visual Studio Live Share for real-time collaboration. Live Share lets you co-edit and co-debug while sharing code, terminal, comments and more.
Variable Explorer and Data Viewer
Within the Python Interactive window, it’s possible to view, inspect, and filter the variables within your current Jupyter session. By expanding the Variables section after running code and cells, you’ll see a list of the current variables, which will automatically update as variables are used in code.
Debugger
The Visual Studio Code debugger lets you step through your code, set breakpoints, examine state, and analyze problems. Using the debugger is a helpful way to find and correct issues in notebook code. Open the command palette (Ctrl+Shift+P) and run the Python: Debug Current File in Python Interactive Window command.
Show me the money!
To work with Jupyter notebooks, you must activate an Anaconda environment in VS Code, or another Python environment in which you’ve installed the Jupyter package. To select an environment, use the Python: Select Interpreter command from the Command Palette (Ctrl+Shift+P).
To connect to the Jupyter server running in the Docker container:
PRO TIP: You define Jupyter-like code cells within Python code using a #%% comment.
Run Cell applies to only the one code cell. Run Below, which appears on the first cell, runs all the code in the file. Run Above applies to all the code cells up to, but not including, the cell with the adornment. You would use Run Above, for example, to initialize the state of the runtime environment before running that specific cell. Selecting a command starts Jupyter (if necessary, which might take a minute), then runs the appropriate cell in the Python Interactive window:
When you’ve activated an environment with Jupyter installed, you can import a Jupyter notebook file (.ipynb) in VS Code as Python code. Once you’ve imported the file, you can run the code as you would with any other Python file and also use the VS Code debugger. When you open a notebook file, the Python extension prompts you to import the notebook as a Python code file:
To export content from VS Code to a Jupyter notebook (with the .ipynb extension), open the command palette (Ctrl+Shift+P) and select Python: Export Current Python File as Jupyter Notebook. It creates a Jupyter notebook from the contents of the current file, using the #%% and #%% [markdown] delimiters to specify their respective cell types.
PRO TIP: You define Jupyter markdown text cells within Python code using the #%% [markdown] comment.
Conclusion
While Jupyter, together with Azure Sentinel, already provides a powerful combination for threat hunting, adding Visual Studio to the mix supercharges it even more. Intellisense, debugging, live sharing and the variable viewer are very useful if you’re a security analyst working with notebooks.
Happy hunting!
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
42 
2
42 claps
42 
2
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/create-a-private-ethereum-consorium-blockchain-in-azure-3667185276b5?source=search_post---------157,"In this tutorial, I demonstrate how to create a private Ethereum Consortium Blockchain network in Azure using one of the Azure Marketplace templates.
To begin the process, login to Azure portal and click on the “+” icon on top left corner.
"
https://medium.com/@renatogroffe/azure-devops-guia-de-refer%C3%AAncia-gratuito-bac4fba3ff5?source=search_post---------158,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 4, 2020·2 min read
Neste post agrupo os conteúdos gratuitos que venho produzindo sobre Azure DevOps, o que inclui artigos e vídeos (em alguns casos de eventos/lives que ajudei a organizar) sobre o uso desta plataforma no build e deployment automatizado de aplicações.
Vocês poderão encontrar aqui materiais cobrindo o uso de tecnologias como .NET Core/ASP.NET Core, Azure App Service/Web App for Containers, Docker, Kubernetes, Azure Container Registry e Azure Kubernetes Service (AKS). Meu intuito é manter este guia sempre que possível atualizado, adicionando ao mesmo novos conteúdos gratuitos que vier a disponibilizar.
E para aqueles interessados em se aprofundar ainda mais nos recursos do Azure DevOps, deixo listados aqui os seguintes canais do YouTube:
Docker + Azure DevOps: build e deployment automatizado de aplicações
Kubernetes + Azure DevOps: build e deployment automatizado de aplicações
Azure DevOps Pipeline + Azure Container Registry + Azure Web App for Containers + Environment Variable
Azure DevOps + Azure Container Registry + Azure Kubernetes Service
Azure DevOps Pipeline + Azure Container Registry + Azure Web App for Containers
Docker - Guia de Referência Gratuito
Kubernetes - Guia de Referência Gratuito
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
42 
42 claps
42 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/a-new-look-at-azure-durable-functions-aa43f37955ff?source=search_post---------159,"There are currently no responses for this story.
Be the first to respond.
Zone’s principal architect, Andy Butland examines how the durable functions framework has matured over the past couple of years…
A couple of years ago I had opportunity to speak, write and work with what was then a new serverless Azure technology, Durable Functions, based on top of the Azure Functions framework for the purposes of handling long-running or multi-stage tasks. As part of investigating the technology I built a couple of sample applications using both durable functions and the standard functions framework by itself, and compared and contrasted the…
"
https://medium.com/@bankexcom/bankex-protocol-a-trusted-instrument-of-crowdfunding-d4226aa75750?source=search_post---------160,"Sign in
There are currently no responses for this story.
Be the first to respond.
BANKEX
Jul 24, 2017·2 min read
On the 27th of March 2017, BANKEX team got $120.000 from Microsoft Azure. BANKEX team is grateful for the trust displayed to us.
Azure Active Directory is a IaaS (identity-as-a-service) which currently provides and manages access of BANKEX team to services that are currently available on Microsoft Azure. Storage Account is a durable, robust and scalable storage solution which allows us to store all the essences that are deployed within our solution.
As an open, flexible, and scalable platform, Azure supports a rapidly growing number of distributed ledger technologies that address specific business and technical requirements for security, performance, and operational processes.
In the case of BANKEX, we use the five most potent services provided by Microsoft Azure:
- Azure Active Directory
- App Service Plan
- App Service
- Applications Insights
- Storage Account
Microsoft Azure allows us to firstly deploy our smart contracts on the TestNet and then once all the tests are successful it can be then deployed on the Ethereum production net.
Microsoft Azure Setup Overview
BANKEX is available at:
Website: https://bankex.com/Telegram: https://t.me/bankexFacebook: https://www.facebook.com/BankExchange/GitHub: https://github.com/BankExSlack: http://bit.ly/slack-bankex
Reddit: https://www.reddit.com/r/bankex/
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
200 
200 
200 
Open source PROOF-OF-ASSET protocol to facilitate #digitisation, #tokenisation & exchange of traditional assets. All things #cryptocurrency #fintech #blockchain
"
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-152edbcc1f2c?source=search_post---------161,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 3, 2020·1 min read
Para aqueles que colocaram como meta aprender algo novo na area de programação em 2020, eu estou disponibilizando alguns módulos do meu curso: Criando API’s RESTful utilizando TypeScript, Node e mongoDB.
Segue abaixo as primeiras video aulas liberadas :)
Introdução ao curso:
Ambiente de desenvolvimento:
Instalação do Node.js
Instalação do TypeScript
Aproveitando, segue link para download das ferramentas que nós iremos utilizar nesse curso abaixo:
Espero que gostem :)
Enjoy your life
131 
131 
131 
Aqui você vai encontrar os principais conteúdos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/awesome-azure/difference-between-scale-set-and-availability-set-in-azure-9b2da03b891c?source=search_post---------162,"There are currently no responses for this story.
Be the first to respond.
Comparison: Azure VM Scale Sets vs Availability Sets in Azure.
Availability Set consists of a set of discrete VMs.VM Scale Set consists of a set of identically configured VMs.
Availability Set consists of a set of discrete VMs which have their own names and individual…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-free-microsoft-azure-fundamentals-az-900-online-courses-for-beginners-in-2021-efd01d8be403?source=search_post---------163,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for the AZ-900 certification or Azure Fundamentals exam and looking for free online courses to start your preparation then you have come to the right place.
In the past, I have shared both free and paid courses to learn AWS, Azure, and Google Cloud as well as the best AZ-900 courses and practice tests, and today, I am going to share the free AZ-900 courses to pass the Azure Fundamentals exam.
If you are thinking to learn Cloud Computing and Microsoft Azure platform then this is the best cloud certification to start with.
Microsoft is among the top cloud service providers; it is popular for its reliability and security. Besides this, the cloud industry is the most rapidly growing industry of this era, and in the coming years, it will join the league of top industries.
To lead a successful career as a cloud engineer, you must have certifications of expertise from the big players of cloud computing. If you have plans to appear in certification exams like Azure Fundamentals AZ-900 then congratulations, you made a great decision. However, if you are still wondering about the certificates, then you must hurry, time is a limited entity.
In this guide, I’ll share seven free courses that will help you pass the Azure Fundamentals AZ-900 exam with flying colors. All the below-listed courses are created by the industry experts, and each of them is picked after hours of research. None of the courses will disappoint you.
This particular certification exam is designed specially to test the fundamental knowledge of the candidates. Therefore even if you are a beginner and have no prior knowledge, you can clear this exam, but the only catch here is that you need to pursue each of the courses thoroughly.
And, if you don’t mind spending few bucks to acquire a useful skill like Azure and Cloud Computing and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out the AZ-900: Microsoft Azure Fundamentals Exam Prep course by Scott Duffy on Udemy. It’s one of the best online courses to prepare for the AZ-900 certification exam in 2021.
udemy.com
Here is a list of the best free online AZ-900 courses to become a certified Azure practitioner. You can join these courses to pass the Azure Fundamentals certification exam in the very first attempt.
These online courses have been created by experts and trusted by thousands of developers. You can also join them to learn online and become a certified Azure professional. Remember, they are absolutely free and you don’t need to pay for anything.
If you are running short on time and need something that can help you prepare for the Azure Fundamental AZ-900 Exam in a limited time, then this course is for you. It is a 2-hour long video course available on Udemy created by Maruti Makwana, and so far over 6 thousand students have enrolled in this course.
In this course, you’ll learn about the basics of the Azure platform and how you can use it for your day to day projects. Besides this in this course, the instructor will give a brief introduction to other Azure concepts like the Security and Deployment of Apps.  Graphics and illustrations are smartly combined with each other to make the understanding process easy for the students, apart from this, the instructor will also perform some live implementations and ask the students to follow his steps.
If you are looking for a course that can teach you all the concepts with practical implementation, then this course is a perfect pick for you.
Here is the link to join this free Azure course — AZ-900 Microsoft Azure Fundamentals
This is another free course that is great to learn essential Microsoft Azure concepts and perfect for anyone preparing for the AZ-900 exam and cloud computing.
The course begins with fundamental concepts such as, what is the cloud? From here it focuses on explaining little by little how you can work on azure etc. This course is strictly visual and conceptual. You’ll get the conceptual knowledge of the cloud and its services which would help you in further learning of advanced features, which is important to pass the AZ-900 certification exam.   Here are the key things you will learn in this course:
This free online Azure course works wonders by providing you frame-of-reference learning which is highly effective to solve complex problems in advanced azure learning.
Here is the link to join this free AZ-900 course — Microsoft Azure Concepts
Also, it provides bonus features and access cards to a hands-on lab to enhance learning. The course is regularly updated so all your queries will be answered.
If you are interested in a practical approach, then this course is for you. It is available on the Pluralsight website, though it is a paid platform but using the 10-day trial option, and you can quickly complete this course.
It is created by Matt Milner, and through this course, the instructor will introduce you to the advantages, limitations, and restrictions of cloud computing, and later in the timeline, you’ll be taught about various aspects of Azure Security and SQL.
Every second of this course is informative, and the instructors have used the graphics quite intelligently. Through these graphics, even challenging concepts seem relatively easy.
Here is the link to join this course — Microsoft Azure Fundamentals
Therefore if you are looking for an easy to learn and practical course, then this course is for you. By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount).
I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-trial to watch this course for FREE.
pluralsight.pxf.io
If you have an adequate amount of time and can attend in-depth explanatory videos, then this cloudacademy.com course is for you. In this 6-hour long video course, you’ll learn about every element that is linked with the Azure platform.
This course is created by Guy Hummel; he is an Azure expert. So far, he has created 60 different courses regarding cloud computing and taught over 54 thousand students. He has teaching experience of more than 25 years, thus understanding the concepts won’t be a problem for you.
One of the fascinating features of this course is that it has special lectures from other industry experts, and the instructor is regularly updating the course content. Therefore, you’ll get the latest information about the Azure platform.
This is another quality free resource to prepare for Microsoft Azure Fundamentals certifications. Created by Andrew Brown, a leading AWS and Cloud expert, this 3-hour course is perfect to prepare for Azure Fundamentals certification in quick time.
The Azure Fundamentals exam is an opportunity to prove knowledge of cloud concepts, core Azure services, Azure pricing, SLA, and lifecycle, and the fundamentals of cloud security, privacy, compliance, and trust. You will learn about all these things in this course.
Here is the youtube link to watch this course for free:-
If you are looking for a course that covers a wide range of topics related to the Azure Fundamental AZ-900 exam in a very limited time without compromising with the information, then this course is for you.
It is available on Udemy, and over seven hundred students have enrolled in this course. It is among the latest courses uploaded on Udemy but its instructor “Xaas Technologies” has a long history with cloud computing.
So far “Xaas Technologies” have taught more than 83 thousand students through their Udemy courses. This is fast-paced, and to the point course that deals with the following concepts of Azure:
All these concepts are taught in-depth using appropriate graphics and illustrations. You must enroll in this course if you need more output in less time.
Here is the link to join this free course — AZ-900 Prep — Azure Virtual Machines (VMs) Master Class
If you are just starting with Azure and want to learn everything in detail, then this Coursera course is a perfect pick for you. In this course, over 9 thousand students have enrolled so far, and it is rated as a top seller on Coursera.
It is a 20 hours long video course, and throughout this course, the instructor will assign different tasks to the students, and they’ll only be promoted to the next videos if they complete those tasks on time. Though the tasks have flexible deadlines, you can’t skip them. In this course, you’ll be taught about the followings;
Through the course, you’ll have practical implementations of various concepts, and the rest will be taught with the help of simple yet engaging graphics.
Here is the link to join this Azure course — Getting Started with Azure
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking a Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
It cost around $399/year but it’s completely worth your money as you get unlimited certificates.
click.linksynergy.com
That’s all about the free AZ-900 courses to prepare for Azure Fundamentals certification. All the above-listed courses are designed for the beginner, and gradually throughout the instructor, enhance the level.
If you are just starting with the preparation for the Azure Fundamental AZ-900 exam, then you must enroll in all the courses and attend at least a few of the videos, and then decide which one is more understandable for you. If you come across some courses that aren’t listed here but deliver quality information, then do let me know.
Other Azure Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure courses useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P.S. — If you want to pass the Microsoft Azure platform in the first attempt and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out the AZ-900: Microsoft Azure Fundamentals Exam Prep course on Udemy. It’s the best online course to prepare for the AZ-900 certification exam in 2021.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
128 
128 claps
128 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@gmusumeci/getting-started-with-terraform-and-microsoft-azure-a2fcb690eb67?source=search_post---------164,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 12, 2020·13 min read
In this post, we will explore the basics (and a few tips of many years of experience) to configure your development environment to start writing Terraform code for Microsoft Azure.
Installing Terraform is very simple. We just need to download the file from https://www.terraform.io/downloads.html and extract the…
"
https://medium.com/javarevisited/how-to-prepare-for-microsoft-azure-fundamentals-certification-az-900-exam-in-2021-4a258b3006a?source=search_post---------165,"There are currently no responses for this story.
Be the first to respond.
Hello there, if you are aiming for Microsoft Azure Fundamentals certification in 2021 but not sure how to prepare for it then you have come to the right place. Earlier, I have shared the best Azure courses, and today, I am going to share a complete guide to prepare for the AZ-900 certification exam, with links to books, tutorials, guides, whitepapers, and online courses.
Cloud computing skills are in demand and companies are increasingly looking for people who know and worked in public cloud platforms like AWS, GCP, and Microsoft Azure.
If you are looking to get started in Cloud Computing, particularly on the Microsoft Azure side then AZ-900 or Azure Fundamentals certification is probably the best way to start.
You will not only learn about Azure but also learn essential Cloud Computing fundamentals like storage, network, compute, and memory among all things.
You will also learn about things like IaaS (Infrastructure as a Service), PaaS (Platform as a Service) and SaaS (Software as a Service), Sales and Pricing, which are very important for both technical and non-technical IT professionals.
Earlier, I have shared a few tips, courses, and practice tests to pass the AZ-900 or Microsoft Azure Fundamentals exam and today, I’ll talk about how to crack the Microsoft Azure Fundamental certification (AZ-900) in 2021. But, before that, let’s first understand what is Microsoft Fundamentals or AZ-900 certification exam? Microsoft has established a certification in Azure’s new role-based certification path, specifically “Microsoft Certified Azure Fundamentals”.
This exam is intended for people with non-technical training, as well as technical people who wish to test their basic knowledge of Azure cloud services. To obtain this certification, it is compulsory to pass the AZ-900 exam.
Now that you know What is Microsoft Azure Fundamental certification and why should you get certified for it, it’s time to make a plan to succeed in this prestigious exam.
Here is your step by step guide to crack Microsoft’s new AZ-900 or Azure Fundamentals certification in 2021:
Azure AZ-900 is a light exam compared to many other Microsoft role-based exams. This exam does not require experienced cloud professionals or programming assistants.
If you are an individual involved in buying and selling cloud services, this review will benefit you even if it comes from a non-technical experience. In addition, this exam will be useful for people who want to validate their basic knowledge of cloud services or solutions. Again, it will be helpful if the exam candidate has general computer knowledge or experience before starting formal preparation for the AZ-900 exam. Let’s say we want to pursue a career in one of Azure’s key roles, including developer, solution architect, administrator, etc. and you don’t know where to start your journey. Here come this exam and certification. This exam can confirm your mastery of the Azure basics and make you competent enough to undertake future Azure certification activities. This warranty may reflect the results of the test when other preparations for the Microsoft Azure certification exam are also taken.
We recommend that you know the basic details of this exam before you begin preparing for the AZ-900 exam. Microsoft has a defined policy regarding exams.
Here, each type of exam has specific details depending on the role or function. There will be several details for Microsoft MCSA exams, role-based exams, etc. However, let’s see how these details apply to our AZ-900 exam.
As with most Azure exams, you can expect 40 to 60 questions on an AZ-900 exam and you will get 85 minutes.
And there will be a different question format like a case study, a short answer, repeated answer options, a build list, a hotspot, a multiple-choice, a brand review, a review screen, an active screen, better answer, drag, and drop, etc. during your exam
You can answer any number of exam questions. There will be no penalty for giving incorrect answers. Just don’t check the wrong answers, that’s all.
It is mandatory to check the exam price before starting preparation for the AZ-900 exam. These exam fees are subject to change depending on where the exam is taken. If you live in the United States, the exam will cost $ 99.
You will benefit from a reduced rate if you are a member of the Microsoft Imagine Academy program, a member of the Microsoft Partner Network program, or a Microsoft Certified Trainer. Students are also entitled to a fee reduction.
You have to get a total of 700 points to pass this exam. The candidate who receives votes below this number will be considered bankrupt. You can find out the results of your exam a few minutes after the end of the exam. However, to get a detailed dashboard, you have to wait a few days.
The dashboard can contain many details, including general exam performance, pass / fail status, a bar chart showing performance in key areas of the exam, and instructions on how to interpret. The results of your exam.
Many candidates expressed doubts about the exam repetition policies before starting preparation for the AZ-900 exam or during preparation. If you are trying this exam and cannot pass it for the first time, you must wait at least 24 hours before repeating the exam.
If this happens to you a second time, the waiting time for the next exam can be increased to 14 days. This way you can take a maximum of 5 reps in a year.
If you cancel or reschedule an appointment at least 6 working days before the exam, the cancellation will not be invoiced. If the cancellation/rescheduling takes place within 5 working days, you will be charged a small fee.
However, if you do not reschedule/cancel the appointment within 24 hours or if you cannot participate in the exam, all exam fees will be forfeited.
It is very important to prepare well if you want to take the AZ-900 certification exam. Since the exam is new, you need to follow the correct preparation procedure to complete the exam the first time. Below is the complete guide to preparing for the AZ-900 exam.
This portal serves as a command center for all Microsoft exams and certifications. You can find a list of all Microsoft certifications on the Microsoft Learning page.
Without any difficulty, you can also find the AZ-900 page on this portal. Frankly, this portal should be the starting point for preparing for the AZ-900 exam. This is because you can find everything you need to know about this exam on this portal, including the option to register for the exam, details on AZ-900 exam modules, links to the AZ-900 Microsoft Azure Fundamentals study materials, exam prerequisites, exam group study, exam links, exam policies, and structure, etc. On top of all that, this is one of the few places where you can get important updates on the exam schedule, module changes, price changes, etc. You can find links to schedules available only on this portal for the AZ-900 Exam, it is almost certain that there is no way that this portal will go unnoticed while preparing for the exam AZ-900.
Online training courses play an important role in preparing for Azure Fundamentals or AZ-900 certification exam. While Microsoft also provides official training it's very expensive and there are many affordable options available to people who are preparing for Azure Fundmatnals or AZ 900 exam.
Here are some of the recommended course from Udemy and Pluralsight to prepare for Azure Fundamentals or AZ-900 certification exam:
This is one of the most popular Udemy courses to prepare for AZ-900 or Azure Fundamentals exam. Created by Scott Duffy, this course is updated to cover the latest exam curriculam. The course also includes a practice test for better preparation.
udemy.com
2. AZ-900 Exam-Prep: Microsoft Azure Fundamentals (JAN 2021)
This one is another awesome course from Udemy which I highly recommend for developers going for Azure Fundamentals or AZ 900 exam. This course is not just up-to-date and covers all exam topics it also comes with Interactive Labs, Flash Cards, and a 50-Question Practice Exam.
udemyy.com
3. Microsoft Azure Fundamentals (AZ-900) Path on Pluralsight
If you are a Pluralsight member then you should be happy that Pluralsight has a complete Path and array of courses to prepare for the Azure Fudnemtnals certification exam in 2021.
This is an excellent starting point for those looking to get started in working with cloud services and solutions in the Azure space.
pluralsight.pxf.io
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount). I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
If you have a reliable Azure book, your impact would be very high in preparing for the AZ-900 exam. But unfortunately, most of the Azure books available do not meet the expectations of the necessary readers or meet industry standards.  However, we have shared some of the Azure books which can be very helpful in preparing for the AZ-900 exam.
Practice with Simulator is the best way to analyze your skills and abilities in a particular area. You can also find many websites during the AZ-900 exam preparation that claim they can provide the best Simulator for the AZ-900 exam. Never trust these websites because they don’t provide a real exam environment and valid questions. Sometimes using these simulators can even result in the main exam being failed. Therefore, always keep these difficulties in mind when preparing for the AZ-900 exam. I highly recommend David Mayer’s Microsoft Azure Fundamentals AZ-900 Simulator because they are providing a real exam environment and valid questions.
You can also try with these free 10 questions and then purchase a premium version to get ready for the exam and check your preparation level.
If you need more options, Here are some more AZ-900 Practice questions from Udemy and Whizlabs.
Three complete timed practice tests for AZ-900 Azure Fundamentals exam, 150 questions, 100% original material
udemy.com
2. AZ-900 Practice Tests | Microsoft Azure Fundamental | Jan 21
This practice exam is completely up-to-date with new requirements. New questions added for the latest changes to the exam. It contains 6 full-length practice tests to build speed and accuracy.
udemyy.com
3. Microsoft Azure Exam AZ-900 Certification
This is one of my favorite practice tests. It’s high quality and provides in-depth explanations. It contains 6 Full-Length Mock Exams (325 Unique Questions) and 7 Section Tests (55 unique questions). You also get reports to assess strengths & weaknesses.
Here is the link to join this practice test — Microsoft Azure Exam AZ-900 Certification
This step can always be ignored when preparing for the AZ-900 exam. This is as important as related books or documents because these things are published by Microsoft itself.
From these sources, you can find many relevant Azure documents. Also, be sure to subscribe to Azure Notifications to help you stay up to date on the latest Azure updates.
azure.microsoft.com
That’s all about how to crack Microsoft Azure Fundamentals (AZ-900) Cloud Certification in 2021. If you want to start working with Microsoft Azure or any other cloud platform then this is a really good certification to get started.
You will not only learn about Microsoft Azure Fundamenta but also Cloud Computing fundamentals which apply to all kinds of cloud providers like AWS, GCP, and Microsoft Azure.   Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like these AZ-900 dumps or practice tests, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. — If you are new to Azure Cloud Platform and looking for free online training courses to learn Azure basics and prepare for Azure Fundamentals certification then you can also see Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It’s completely free and all you need is a free Udemya account to join this course online.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
217 
1
217 claps
217 
1
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/microsoftazure/10-azure-ml-code-examples-every-cloud-ai-developer-should-know-a2899bf2f8bd?source=search_post---------166,"There are currently no responses for this story.
Be the first to respond.
TLDR; The Azure ML Python SDK enables Data scientists, AI engineers,and MLOps developers to be productive in the cloud. This post highlights 10 examples every cloud AI developer should know, to be successful with Azure ML.
If you are new to Azure you can get a free subscription using the link below.
azure.microsoft.com
The scripts in this example are used to classify iris flower images to build a machine learning model based on scikit-learn’s iris dataset the code can easily be adapted to any scikit-learn estimator.
Code:
docs.microsoft.com
This example shows you how to run your TensorFlow training scripts at scale using Azure Machine Learning’s TensorFlow estimator class. This example trains and registers a TensorFlow model to classify handwritten digits using a deep neural network (DNN) and MNIST but can be scaled to other more complex models.
Code:
docs.microsoft.com
PyTorch the example scripts in this article are used to classify chicken and turkey images to build a deep learning neural network based on PyTorch’s transfer learning tutorial and can be adapted to more complex projects.
Code:
docs.microsoft.com
This example demonstrates how to deploy a production model on Azure Kubernetes Service (AKS). AKS is good for high-scale production deployments. Use AKS if you need one or more of the following capabilities:
Code:
docs.microsoft.com
Data in the wild is dynamic, this example walks through how to monitor changes in you data distribution and model performance over it’s production lifespan so that you can be alerted and update it more readily. ‘
Code:
docs.microsoft.com
This code example walks through using BERT model for question and answering in an end to end pipeline on the AzureML platform. From how to fine tune it from scratch using the distributed training with Horovod and how to optimize model performance with Azure ML Hyper Drive.
Code:
github.com
This example shows how to use Azure Machine Learning to run distributed training using Distributed Data Parallel in Pytorch for extractive summarization.
Code:
github.com
The automation of detecting anomalous events in videos is a challenging problem that currently attracts a lot of attention by researchers, but also has broad applications across industry verticals. This code example provides an an end to end template for creating Video Anomaly Detection service with Azure ML and AML Pipelines.
Code:
github.com
At its best, AI advances society through critical high-impact applications such as Heathcare, Security and Self Driving Cars. However at its worst AI can amplify existing societal biases with unintended consequences, such as ethnic, gender or racial discrimination. Model interpretability is a critical component of the Machine Learning Engineering process. This code example shows how to use the interpretability package of the Azure Machine Learning Python SDK to better understand why your model made its predictions.
docs.microsoft.com
For more information about Shapley Values one of the key interpretability measures check out my previous post on the topic.
medium.com
This End To End Notebook demonstrates how to train a custom estimator in Azure ML using the Intel NLP Architect Open Source Aspect Based Sentiment model. This model enables more granular insight into sentiment analysis as well contains best practices for configuring custom estimators from remote GitHub branches and custom environmental variable settings.
github.com
See also:
medium.com
Now that you have all the code you need to get started for your own production Azure ML project check out my previous posts on 9 Advanced Tips for Production Machine Learning and how Setting up AML Notebook VM.
medium.com
medium.com
Aaron (Ari) Bornstein is an avid AI enthusiast with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
47 
47 claps
47 
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/getting-started-with-vuejs-nodejs-and-azure-cosmosdb-35dadbf2dd3c?source=search_post---------167,"Looking at the Frontend frameworks that showed up in the past few years, it seems like we are down to very few main frameworks that people are talking about.
I believe, for the enterprise solutions, it seems like the battle is between Angular and React. Of course there are other ones out there that have their community and market (like EmberJs, or BackboneJs, etc), but I…
"
https://medium.com/microsoftazure/the-power-of-azure-devops-projects-for-java-apps-cda6c69bef0e?source=search_post---------168,"There are currently no responses for this story.
Be the first to respond.
If you are unfamiliar with Azure DevOps Projects, create right now an Azure Trial account to try this beauty. It is one of the coolest, most amazing features of Azure and many developers are missing out. #FOMO. Because what you really want is commit to production.
If you like this article, please clap for it down below or to the left side.
This is the ability of being able to commit a change to a project on Git or GitHub, and see automated continuous integration, testing, and deployment being triggered automatically.
This article covers exactly how to import, build, deploy, and test two Java applications, one written with Spring Boot Reactive, and another with Eclipse MicroProfile using Payara Micro as the runtime.
Your key take away? You can now scaffold Java projects on Azure extremely easily and fast, and independent from the Cloud.
Before diving in, quick pause.
As you can see above, I posted the content of this article originally as a series of tweets, and someone said I invited ""tutorials via Twitter"".
I guess I just invented the #Twitorial. :P … Moving on!
Assuming that you by now should already have an Azure Trial account to experiment this, let's dive right in!
Both projects are hosted on GitHub, and you can clone and run them locally just to see them in action before you push them to you Azure account:
Clone either project to your computer (or both). They have equal building and running instructions except for which Maven plugin to call, and it should be pretty easy to get done with this, as long you of course have the necessary tools installed (JDK 8, Maven 3+, Docker is optional)
This is a neat feature in recent versions of Docker and has helped me circumvent some limitations and deliver a developer experience that is equally awesome on your local computer as well on the CI/CD platform.
The basic springboot-hello-azure/Dockerfile contains the following instructions, and it is useful for building/testing the Docker image locally.
Dockerfile:
Dockerfile.cicd:
The second one above as stated earlier on, has the multi-state build surpport and configuration. This is the version we will use later in this article to import/create Docker image on Azure DevOps project wizard.
Go right now to the Azure Portal, and hit Create DevOps Project. If you don't see this tile on your dashboard, search for DevOps on the left menu inside ""All Services"". You must find ""DevOps Projects"". Once you open it, you should be able to Add a new project.
The steps below will walk you through the project creation wizard, so stay tuned as we check up to 8 steps.
Make sure you select Bring your own code
Here you have two options again: either go with MicroProfile or Spring Boot. Below the links to the Git repositories to facilitate your copy/paste.
We will be building and deploying using Docker, so doesn't matter the framework or platform, we will build a Docker image. Therefore, here you select Dockerized application.
Make sure you mark the option that says Web App for Containers.
This is a simple service that can spin up dockerized web applications with a load balancer in front and other perks for scaling. It costs less than Kubernetes but of course won't provide the entire feature set of Kubernetes.
I did not test this with AKS (Azure Kubernetes Service), but please feel free to try it out and post in the comments!
Since this is a Dockerized application, it simplifies things if we also use Docker to run the entire build cycle of this project. For that, we have Dockerfile.cicd for both projects which you can use to compile/package the Maven project, and then right away build a Docker image with the artifact produced by Maven.
This is exactly what we want.
The deployment will be in progress, so keep an eye and wait for everything to be complete.
Once the deployment is ready, here's the amazing dashboard you will have for your DevOps project, and all of its resources created for you automatically:
Your application is now running in the Cloud — Microsoft Azure to be more precise! Visit http://<your-app>.azurewebsites.net/api/hello once deployment is done.
Try this now. And please let me know in the comments what you think. And don't forget to clap if you liked it :-)
Any language.
45 
45 claps
45 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Brazilian, Product and Program Manager for Java at Microsoft. Promoting great developer technologies to the world. Previously at Oracle.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/ursium-blog/slock-it-working-with-microsoft-to-bring-its-dapp-to-the-azure-cloud-c7a39720fdb3?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
Stephan Tual
Mar 15, 2016·4 min read
Business analytics, data management, identity and messaging: meet Slock.it.
We’re pleased to announce that we’re working with Microsoft to bring the Slock.it Đapp to the Microsoft Azure Cloud, powered by Ethereum.
Microsoft has heavily invested in their cloud infrastructure, now leveraged by thousands of large corporate partners to deploy countless applications, ranging from machine learning to data warehousing — and now, interacting with the decentralized sharing/collaborative economy.
For Slock.it UG, this means unprecedented exposure to ‘mainstream IT’ and new potential partners in our mission to grow a DAO ecosystem. For the DAO to which we will make a Proposal, it would means expanding its serviceable available market to include thousands of new channels across a variety of verticals it could only have dreamed of a few short weeks ago.
This will also make it very easy for developers to build Đapps for the Ethereum Computer, by simplifying retrieving data from existing services (e.g. Oracles) or triggering hosted applications and devices based on messages and payments made on the blockchain. We’ll soon make available an Ubuntu core image bundling the Ethereum framework and the Slock.it Snappy Đapp for developers to build on. These Đapps can then be deployed without modifications to the Ethereum Computer, making the Azure cloud the perfect environment to experiment with Slock.it and smart objects such as the Samsung SmartThings line.
Some of you may wonder if there is added value of deploying a blockchain on top of a cloud service — an infrastructure which is by definition centralized. The Ethereum “Blockchain as a Service” allows Microsoft customers and partners to test and integrate blockchains as part of their network easily, within an environment they can understand and control. It’s a sandbox, a playground. It might not make sense for the hacker who enjoys spending 3 days compiling the Dev branch on CAELinux. But if your name is GE, Pearson or 3M — it’s the ideal way to get started — with a one-click deployment.
We’re particularly excited about what this will mean for companies that are keen to explore Slock.it in order to decentralize access control, but are hesitant with regards to how this integration might impact existing services.
Think about a warehouse provider wanting to:
I didn’t choose this example at random — this is paraphrased from a conversation with a company currently exploring our technology. Big chunks of their access control can be automated and decentralized with the Slock.it App, and now with Azure integration, it can be done as part of an incremental transition toward decentralization, eliminating the need to call an expensive integrator or spending months experimenting with costly PoCs.
For more information, please see the Microsoft Azure website, or join the discussion on our Slack channel at https://slock.it:3000
About the Author
Stephan Tual is the Founder and COO of Slock.it.
Previously CCO for the Ethereum project, Stephan has three startups under his belt and brings 20 years of enterprise IT experience to the Slock.it project. Before discovering the Blockchain, Stephan held CTO positions at leading data analytics companies in London with clients including VISA Europe and BP.
His current focus is on the intersection of blockchain technology and embedded hardware, where autonomous agents can transact as part of an optimal “Economy of Things”.
Twitter: @stephantualContact: stephan@slock.it
If you enjoyed reading this, please log in and click “Recommend” below.This will help to share the story with others.
CEO at Genomics Startup 3BP. Former CCO Ethereum, Blockchain & Smart Contracts expert. Moonlights as Indy Film Producer.
See all (15)
28 
1
28 claps
28 
1
Epigenomics, Blockchain, Smart Contracts
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/mensageria-na-nuvem-com-rabbitmq-net-core-e-azure-functions-7c2a4f890448?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 2, 2020·5 min read
Uma dúvida bastante recorrente entre aqueles que optam pelo Microsoft Azure para a implementação de soluções na nuvem é quanto ao suporte oferecido ao uso de RabbitMQ. Como trabalhar com esta que é atualmente uma das soluções de mensageria?
Temos algumas opções para a criação de ambientes baseados no RabbitMQ no próprio Portal do Azure, como demonstrado na imagem a seguir:
Existe ainda a possibilidade de configurarmos em produção a utilização do RabbitMQ em um cluster Kubernetes, com isto acontecendo por meio do AKS (Azure Kubernetes Service).
Mas e no que diz respeito à implementação de aplicações que processarão as mensagens enviadas a uma fila/queue do RabbitMQ?
Até recentemente o melhor caminho para isto seria a implantação de uma aplicação em uma máquina virtual ou, até mesmo, a execução desta em um container criado com o Azure Container Instances ou via cluster Kubernetes.
A grande novidade agora está no fato de que poderemos também trabalhar com RabbitMQ a partir de aplicações baseadas em Azure Functions, abrindo caminho assim para a implementação de soluções em arquitetura serverless com um custo reduzido. Entranto, é necessário ressaltar que o package para .NET Core ainda se encontra em versão Beta no momento da publicação deste artigo (início de Março/2020).
O anúncio sobre o suporte a RabbitMQ está descrito no seguinte post:
dev.to
Neste artigo trago um exemplo simples de uso de RabbitMQ em uma Function App, através da implementação de um projeto Serverless utilizando .NET Core, Azure Functions 3.x e o Azure SQL/SQL Server na manipulação de cotações de moedas estrangeiras.
Para ficar por dentro das novidades do Azure Functions 3.x acesse o artigo a seguir:
.NET Core 3.x + Serverless: configuração, dicas e exemplos com Azure Functions 3.x
E aproveito este espaço para deixar aqui um convite.
Que tal aprender mais sobre Docker, Kubernetes e a implementação de soluções baseadas em containers utilizando o Microsoft Azure, em um workshop que acontecerá durante um sábado (dia 04/04/2020) em São Paulo Capital e implementando um case na prática?
Acesse então o link a seguir para efetuar sua inscrição (inclui camiseta, emissão de certificado e almoço para todos os participantes) com desconto:http://bit.ly/anp-docker-blog-groffe
A aplicação detalhada nesta seção é uma variação do seguinte projeto:
.NET Core + Azure Functions 3.x + Queue Storage + Azure SQL/SQL Server + Queue Trigger + HTTP Trigger
Detalhes sobre a criação de Function Apps a partir do Visual Studio Code podem ser encontrados no artigo:
.NET Core + Serverless: implementando jobs com Azure Functions e o VS Code
Será necessário incluir no novo projeto o package Microsoft.Azure.WebJobs.Extensions.RabbitMQ, utilizando para isto uma de suas versões Beta compatíveis com Azure Functions 3.x:
Adicionar ainda a biblioteca Dapper:
E o novo provider de acesso a dados do .NET Core 3.x para bases do SQL Server/Azure SQL:
No arquivo local.settings.json estarão as strings de conexão para o SQL Server (BaseCotacoes) e o RabbitMQ (BrokerRabbitMQ):
Teremos neste projeto a classe Cotacao representando uma cotação de moeda estrangeira
Um HttpTrigger implementado pelo tipo CotacoesHttpTrigger, o qual permitirá consultas ao valor de cotação mais recente para uma moeda (definidos na tabela dbo.Cotacoes):
E finalmente na classe MoedasRabbitMQTrigger estará o código que define a Function de mesmo nome, estrutura essa responsável por processar mensagens direcionadas a uma fila do RabbitMQ:
O código deste projeto já se encontra no GitHub:
https://github.com/renatogroffe/DotNetCore-AzureFunctions3x-RabbitMQ-Moedas
Para a criação de um ambiente de testes com RabbitMQ utilizei a tag/versão 3-management-alpine da imagem oficial do RabbitMQ no Docker Hub (chamada rabbitmq) e o Azure Container Instances. A string de conexão foi alterada, a fim de apontar para o broker representando por este recurso do Azure Container Instances.
Uma Function App chamada groffemoedas também foi criada:
O deployment das Azure Functions detalhadas na seção anterior aconteceu por meio do próprio Visual Studio Code:
Caso queira conhecer mais sobre a publicação no Microsoft Azure de uma Function App acesse:
.NET Core + Serverless: publicando uma Azure Function via VS Code
Criei para os testes uma Console Application chamada CargaMoedasRabbitMQ, a qual também já foi disponibilizada no GitHub:
https://github.com/renatogroffe/DotNetCore3.1-RabbitMQ-CargaCotacoes
Ao executar via PowerShell o primeiro teste da Console Application poderemos constatar que a cotação é processada pela Function App publicada no Azure:
Enviando uma requisição à função HttpTrigger teremos como resultado:
Na imagem a seguir temos o segundo teste da Console Application, com o processamento da mensagem correspondente:
Já a função HttpTrigger trará como resposta:
Serverless + Azure Functions: Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
95 
95 claps
95 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-provision-infrastructure-on-azure-with-terraform-4065430a3d72?source=search_post---------171,"There are currently no responses for this story.
Be the first to respond.
Terraform is an infrastructure as a code tool that makes it easy to provision infrastructure on any cloud or on-premise. Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Terraform can manage existing and popular service providers as well as custom…
"
https://medium.com/forloop/meet-with-microsoft-azure-experts-at-2019-forloop-summit-eb76d5d7ac19?source=search_post---------172,"There are currently no responses for this story.
Be the first to respond.
We are glad to announce that Microsoft will join us at this year’s Forloop Summit.
Microsoft Azure Experts will be on hand to share information on the cloud society and how Azure increases developers’ productivity with the platform’s support of over one hundred services — including 1st and 3rd party developer languages and tools.
There will also be an Azure DevOps and Microsoft Intelligent Kiosk demo at the event — a collection of demos that leverage Microsoft Cognitive Services and other Artificial Intelligence tools running on Azure.
Visit the Microsoft stand to see all the exciting features.
As companies rapidly turn to the cloud and seize the opportunities it brings, we are excited to arm our developer community with the latest thinking, tools and systems to support enterprises in their cloud journeys.
See you at the event!
forLoop developers publications
183 
183 claps
183 
forLoop developers publications
Written by
Developer & Technology Advocate. Writer of all things Technical & Magical. Software Craftsman.
forLoop developers publications
"
https://medium.com/@tsuyoshiushio/azure-devops-yaml-pipeline-with-net-core-and-coverage-report-b84b5b282a95?source=search_post---------173,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Sep 24, 2018·4 min read
I played with the new feature of Azure Pipeline. For the initial setup, you can see this video on the announcement. As my personal project, I developing a tool which use Go lang as CLI and Azure Function V2(C#) as backend. I’d like to share how we can create Continuous Integration pipeline with .NET Core with Coverage Report.
azure.microsoft.com
We have a good instruction from the Azure DevOps official documentation. However, it tightly coupled with Visual Studio. My project is much Open Source Project, so I don’t want to limit the contributor someone who has Visual Studio.
docs.microsoft.com
Other problem is, this pipeline never show the Coverage Report on the website. We need to download and click the detail report on the Visual Studio.
For someone who want to know the pipeline I’d like to share the yaml file, first.
This is the target project for the CI.
github.com
I use coverlet for collecting coverage. We can use it not only for Windows but also Linux and Mac.
github.com
For generating HTML report, I use ReportGenerator.
github.com
To Generate the coverage report, you can write like this. You need to add nuget package `coverlet.msbuild` on your test project. Then you can use /p:CollectCoverage=true parameter. Also you can choose the format of the coverage like cobertura , opencover , etc. However, for the Azure DevOps, I recommend to use cobertura since the following task request cobertura or jacoco as a format to upload coverage. StrikesLibrary.Test is the target test project. Unfortunately, We need to specify one by one. By default the report is written under the target test project directory called coverage.cobertura.xml
Since we need to specify several test projects, we need to merge the result.
This script test the second test project called StrikesRepository.Test You need to specify the report which is specified the last one. /p:MergeWith=....\coverage.cobertura.xml will merge the report and write it under the target test directory as coverage.cobertura.xml as well
For generating, the report, you need to install report generator.
When you use this on your client machine, you don’t need --tool-path however, on the CI environment, I recommend to specify it for find the tool. Because the shell should be reloaded.
Execute the command with specifying the report and target directory. In this case, I output the HTML report on the .\results directory.
The point is the reporttypes: Use HTMLInLine for enabling the output on the Azure DevOps page. Azure DevOps Coverage page show index.html on the web. However, the CSS and Javascript should be included. For this purpose we can use HTMLInline to include CSS and Javascript on the index.html.
Using YAML, we can use full functionality of the VSTS tasks. Now you can see the Unit testing and coverage report on the web. IMO. The important thing is getting feedback quickly and notice an issue. That is why I want make it available on the web. This YAML feature which is the pipeline as code works very well and we can include it on our repository. I love this new feature.
Some useful links.
docs.microsoft.com
docs.microsoft.com
You can see both YAML and GUI configration
docs.microsoft.com
If you can’t find the spec, you can see the task.json on this repo. This link is an example of the script.
github.com
Senior Software Engineer — Microsoft
123 
4
123 
123 
4
Senior Software Engineer — Microsoft
"
https://medium.com/@tsuyoshiushio/how-to-validate-request-for-azure-functions-e6488c028a41?source=search_post---------174,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Sep 1, 2018·2 min read
You might want to validate the HttpRequest using Azure Functions. However, we don’t have any implementation on the Azure Functions now. I see the issue , however it is not yet.
This is just a small idea to achieve validation on Azure Functions. I discussed with my Master Sensei, Kazuki Ota. I’ll share with the outcome.
I want to validate a model object. C# have a great functionality of Validation in System.ComponentModel.DataAnnotations. You can validate using this library.
We need to call Validator.TryValidateObject(movie,...) However, I don’t write it on my Functions everytime. Ideally, I want Validate(model) method on my Functions. Since Functions is static class, it might be difficult. I eventually, write an extension method on HttpRequest. It might be second best. However, if you know better way, please let me know.
This class represent a body of HttpResponse<T>. This method deserialize the target object with model validation. IsValid method will return if the model is valid or not. Also it contains, deserialized object and Validation Results.
This class extend GetBodyAsync<T>() method which returns HttpResponseBody . Inside the method, it validate the object.
Then, Azure Functions looks like this.
Since Azure Functions bindings have converter, If we send pull request for HttpTrigger, then we might directory return the HttpRequestBody object. However, I don’t know whether people loves this idea or not.
I share this blog for thinking about the best way to handle validation for Azure Functions. If you have better idea please let me know.
This is the whole sample code.
github.com
Senior Software Engineer — Microsoft
108 
4
108 
108 
4
Senior Software Engineer — Microsoft
"
https://doublepulsar.com/detecting-dns-cve-2020-1350-exploitation-attempts-in-azure-sentinel-1f2efb26422e?source=search_post---------175,"In my personal honeypot, BluePot, I’ve built out detection for a wide variety of situations — from BlueKeep exploitation to SMB MS17–010 abuse that lead to WannaCry.
"
https://itnext.io/getting-started-with-graph-databases-azure-cosmosdb-with-gremlin-api-and-python-80e57cbd1c5e?source=search_post---------177,"In this article I will give a gentle introduction to Graph Databases using Azure Cloud Platform.
I will start by giving a quick intro to graph databases explaining their use cases and the pros and cons.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/net-core-serverless-implementando-jobs-com-azure-functions-e-o-vs-code-f0d657f64824?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 28, 2019·7 min read
Um equívoco bastante comum em cloud computing é considerar que a implementação de soluções Serverless esteja limitada à construção de APIs REST. No caso específico das Azure Functions, este tipo desenvolvimento faria uso de HTTP Triggers.
Contudo, esta não é a única opção disponível ao trabalharmos com esta tecnologia (felizmente!). Triggers de execução periódica (Timer Trigger), baseadas em mensageria (Queue, Service Bus Queue e Service Bus Topic Triggers) e atreladas a atualizações em uma base do Cosmos DB estão entre as possibilidades oferecidas via Microsoft Azure.
Tal fato aumenta o escopo de utilização das Azure Functions, ao mesmo tempo que constitui uma escolha que prioriza uma rápida entrega de soluções e descarta as preocupações envolvendo diferentes aspectos de infraestrutura como: a criação de VMs, configurações destes ambientes, instalação de componentes necessários à execução das aplicações….
Neste novo artigo demonstrarei a implementação de um job/rotina de processamento disparado periodicamente e empregando Azure Functions + Timer Trigger, .NET Core e o Visual Studio Code. A função em questão irá checar se uma base de dados se encontra no ar, logando o resultado disto em uma Table Storage e na eventualidade de um problema enviando um alerta para um canal do Slack via Azure Logic App. Num segundo post detalharei a publicação deste projeto no Microsoft Azure.
Caso queira conhecer mais sobre Azure Functions já abordei este tema anteriormente no seguinte artigo:
Desenvolvimento Serverless com .NET Core: implementando sua primeira Azure Function
E aproveito este espaço também para um convite…
Que tal aprender mais sobre Azure Functions e desenvolvimento de soluções Serverless, em um workshop que acontecerá durante um sábado (dia 18/01/2020) em São Paulo Capital e implementando um case na prática? Acesse então o link a seguir para efetuar sua inscrição com um desconto especial: http://bit.ly/aznp-devops-blog-groffe
A criação do projeto descrito neste artigo acontecerá dentro de um diretório chamado ServerlessMonitorBD. Acionar então neste local o Visual Studio Code através da instrução code . (no exemplo a seguir foi utilizado o PowerShell, mas tal procedimento aconteceria de maneira idêntica utilizando Bash e uma distribuição Linux como Ubuntu Desktop):
Será preciso que no Visual Studio Code esteja instalada a extensão Azure Functions:
Outros complementos necessários para a emulação deste serviço do Azure num ambiente de desenvolvimento são o Microsoft Azure Storage Emulator e as Azure Functions Core Tools. Maiores informações sobre estas ferramentas podem ser encontradas nos links a seguir:
Use the Azure storage emulator for development and testing
Work with Azure Functions Core Tools
Para efeitos de desenvolvimento e testes esse artigo simulará o comportamento da Azure Logic App empregada na integração com o Slack, a partir de uma requisição HTTP do tipo POST (a geração do recurso real acontecerá no segundo post). Como tal recurso será exposto como um endpoint de uma API REST, temos a possibilidade de “mockar” gratuitamente essa estrutura via portal Mockable.io:
https://www.mockable.io/
Na imagem a seguir está destacado em vermelho o botão + REST Mock do Mockable; acionar essa opção para prosseguir com a criação do mock:
Em REST MOCK preencher as seguintes configurações:
Fornecer então uma descrição detalhando o objetivo do mock em Description:
E finalmente acionar o botão Save:
Neste momento o mock gerado para testes terá seu status como Stopped:
Acionando o botão Stopped o mesmo entrará em execução (status Started):
Consultando o mock teremos acesso aos endpoints gerados para testes com o mesmo (com e sem HTTPS):
Um teste via Postman mostrará que o mock gerado possui o comportamento esperado, gerando uma resposta do tipo 202 — Accepted quando do envio de uma requisição HTTP do tipo POST:
Acessar agora o ícone do Microsoft Azure; será a partir desta opção que acontecerá a criação de novas Function Apps e Azure Functions por meio do VS Code:
Em FUNCTIONS acionar a opção Create New Project… (um ícone representado por uma pasta e um raio):
Neste momento será solicitado o diretório que servirá de base para a geração do Function App; o projeto em questão assumirá o nome desta pasta (neste exemplo ServerlessMonitorBD):
Definir na sequência linguagem (C#) a ser utilizada para a implementação de Azure Functions:
A escolha de um template também será solicitada para a criação de uma primeira Azure Function; acionar para isto a opção TimerTrigger:
E uma cron expression, a fim de determinar a periodicidade de execução da função DBCheckTimerTrigger. Para este exemplo será utilizada a expressão a seguir, a qual fará com que a Azure Function seja disparada a cada 30 segundos:
Maiores informações sobre a montagem de cron expressions (um padrão originário de ambientes UNIX) podem ser encontradas na Wikipedia:
en.wikipedia.org
Será solicitado ainda a utilização de uma storage account; para este exemplo selecionar a opção Use local emulator:
Passados alguns segundos serão gerados o projeto ServerlessMonitorBD e seus diversos arquivos:
No arquivo local.settings.json deverão ser incluídas as seguintes definições:
Adicionar também ao projeto ServerlessMonitorBD uma referência para o package System.Data.SqlClient (neste caso optei pela versão estável 4.6.0):
A classe LogEntity herda do tipo TableEntity (namespace Microsoft.WindowsAzure.Storage.Table) e representa os dados de log a serem gravados no Table Storage:
E por fim temos a implementação da classe DBCheckTimerTrigger e seu método Run:
Os testes descritos nesta seção serão conduzidos a partir da execução do projeto ServerlessMonitorBD no Visual Studio Code:
Os primeiros testes serão executados com servidor SQL Server fora do ar, o que resultará em falhas:
Com o SQL Server em execução os resultados finalmente serão satisfatórios:
Consultando os dados via Microsoft Azure Storage Explorer teremos registros indicando falhas e sucessos no acesso a BaseIndicadores:
Recentemente aconteceu também uma live no Canal .NET sobre a implementação de soluções Serverless empregando Azure Functions. A gravação está disponível no YouTube e pode ser assistida gratuitamente por todos aqueles interessados em conhecer mais sobre as tecnologias mencionadas neste artigo:
Azure Functions documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
99 
99 claps
99 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devopsturkiye/grafana-ile-azure-kaynaklar%C4%B1n%C4%B1z%C4%B1-monitor-edin-d5e01d88253a?source=search_post---------179,"Sign in
There are currently no responses for this story.
Be the first to respond.
Emre Özkan ☁️ 🐧 🐳 ☸️
Apr 10, 2019·3 min read
Azure’un kendi Monitor servisi ve Log Analytics servisi dışında, kaynaklarınızı diğer uygulamalar üzerinden de monitor edebiliyorsunuz.Araştırmalarım sonrasında hem entegrasyon kolaylığı hem de kullanımı kolaylığı nedeniyle OpenSource olan Grafana’yı deneme fırsatı buldum.
Marketplace üzerinden uygulamayı deploy ederek başlıyorum.
Yaratılan kaynaklar aşağıdaki gibidir.
Kurulum tamamlandıktan sonra yaratılan makineden serial console alıp ilk giriş için Bitnami tarafından yaratılan şifreyi temin etmemiz gerekiyor.
Sonrasında browser üzerinden Grafana arayüzüne(3000 port)ulaşıyorum.
Kullanıcı adı olarak “admin” ve yukarda temin ettiğim şifre ile login oluyorum.
Login olduktan sonra bir data source eklemem gerekiyor.Datasource sekmesini açtığımda entegre edebileceğim birçok ürünü görebiliyorum.
Ben bu makalede Azure Monitör’ü seçerek ilerliyorum.
Entegrasyonu yapmam için aşağıdaki bilgileri temin etmem gerekiyor.İsterseniz browser üzerinden cloud shell’i kullanarak isterseniz de CloudShell komut aracını bilgisayarınıza kurarak alabilirsiniz.
Subscription id ve Tenant id bilgisini almak için alttaki komutu çalıştırıyorum.
Subscription id=45435kjhjh-bvb7-dfxf-09sk-xcvjhdzfsd98
tenant id=23423öbsf-789t-jkh8–675g-fsdhafh435kh
Sonrasında Client id ve Client Secret’i ister arayüzden istersenizde komut satırından temin edebilirsiniz.
Client id==> Azure Active Directory- App Registrations-Choose your app-Application id
Client Secret ==> Azure Active Directory- App Registrations-Choose your app-Keys
Ben alttaki komut satırını çalıştırıyorum.
Client id==>xcvxcvtret34–3qwq-49b8–8250-fgzsdfd879
Client Secret==> xdfsdfsdfsd-342o-23423-a657-asdadlkd23
Aynı bilgiler ile LogAnalytics Kısmını da ekleyebiliyoruz.
Sonrasında yeni bir dashboard oluşturup grafiklerimizi oluşturuyoruz.
Data kaynağı olarak Azure Monitor servisini seçip grafiğimizi oluşturuyoruz.
Diğer bir seçenek Azure Log Analytics servisini kullanıyorsanız ,Grafana üzerinde ilgili query ‘leri yazarak grafiğe dökebilirsiniz.Altta CPU ve Memory ile alakalı basit anlamda 2 query örneği paylaştım.
Diğer yazılarıma göz atmak isterseniz;
medium.com
Kişisel Bloguma göz atmak isterseniz;
sysaix.com
Solution Architect at @RedHat CK{A | AD | S} https://www.linkedin.com/in/emreozkann/
See all (56)
44 
Haftalık olarak yayımınızdan alacağınız Email Bülteni Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
44 claps
44 
Medium independent DevOps publication. Join thousands of aspiring developers and DevOps enthusiasts
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-difference-between-azure-sql-database-and-azure-sql-managed-instance-sql-mi-2e61e4485a65?source=search_post---------180,"There are currently no responses for this story.
Be the first to respond.
Comparison — Azure SQL Database vs SQL Managed Instance (MI).
SQL Managed Instance (SQL MI) provides native Virtual Network (VNet) integration while Azure SQL Database enables restricted Virtual Network (VNet) access using VNet Endpoints.
"
https://towardsdatascience.com/azure-data-studio-or-ssms-which-should-i-use-1db49824a39?source=search_post---------181,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nikola Ilic
Jun 15, 2020·6 min read
Question from the title already became extremely popular and it will become more and more as time passes by. Since Microsoft made Azure Data Studio generally available in September 2018 and investing…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-manage-azure-key-vault-with-terraform-943bf7251369?source=search_post---------182,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Jan 14, 2020·2 min read
The purpose of Azure Key Vault is to store cryptographic keys and other secrets used by cloud apps and services in a HSM (Hardware security module). A HSM is a physical computing device that safeguards and manages digital keys for strong authentication and provides cryptoprocessing.
In this example, we will create a Terraform module to manage an Azure Key Vault.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-pass-microsoft-azure-ai-900-ai-fundamentals-exam-c5aec07e3078?source=search_post---------183,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is one of the leading cloud providers. This exam can be taken as an optional first step in learning about Machine Learning and Artificial Intelligence and how those concepts are exemplified by Microsoft Azure. It can be taken as a precursor to Microsoft Azure AI…
"
https://medium.com/analytics-and-data/setting-up-airflow-on-azure-connecting-to-ms-sql-server-8c06784a7e2b?source=search_post---------184,"There are currently no responses for this story.
Be the first to respond.
Setting up Airflow on Azure, isn’t quite as easy as on Google Cloud, where Airflow exists as a managed service called “Cloud Composer”.Microsoft has, however, come up with a quick-start template to setup Airflow on Azure, this template sets up both a web-app hosting an Airflow instance and the postgres database backing it up, make it easier to deploy to the cloud.
"
https://medium.com/@lizrice/azure-container-instances-with-multiple-containers-512c022c04ec?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Liz Rice
Sep 14, 2017·4 min read
Earlier this summer Microsoft announced Azure Container Instances (ACI), a very interesting concept allowing users to quickly spin up containers on demand. You no longer have to provision a virtual machine for the containers to run on — ACI takes care of the infrastructure for you. I have taken to describing ACI as “like Lambda for containers” in the sense that you can create and destroy containers quickly, and be billed for them by the second.
Microsoft have a quick-start guide that walks you through deploying a single container instance. This gets you a simple, single-container hello world server that shows a web page. What if you want to use multiple Azure Container Instances together?
There are two quite different approaches at present. The first is an ACI Connector for Kubernetes project (described as “experimental” at time of writing this post) which allows your Kubernetes cluster to spin up containers within ACI rather than on pre-provisioned virtual machines. This is a really neat idea as it means your cluster (and your billing) can use resources as required, without having to provision, scale or indeed pay for unused VM resources.
The other approach is Container Groups, which looks a lot like the concept of a pod in Kubernetes. You create a container group by writing a JSON-format template and then deploying that template.
Let’s walk through that process using a marginally more complex hello-world. This has a Postgres container to store a count of page hits, and a web server container that displays that count (oh, and it also shows a joke. Sadly, it’s always the same joke.) We’ll deploy both those containers in one Azure Container Instance Container Group.
I created a JSON deployment template called hellodeploy.json. I already have an Azure resource group in place called lizRgWest and I’m going to deploy my container group within it.
Initially I got error messages about a lack of resources. I sorted this out by limiting the total requested by my container group to 1 CPU and 1Gb RAM in total. I’m not sure if this is a deliberate or documented limit, so your mileage may vary.
Once the containers have had a chance to get up and running (bearing in mind that this includes pulling the images to whichever machines Azure is going to run them on) you can use az container show to see them.
This gives output like this:
You can see both containers listed in the Image column, and there is also an address and port where the web server can be located. This is available because the deployment template specified exposing a public IP address.
Should you need more detail on each of the running containers, simply omit -o table from the command.
You get to specify the container port number(s) that should be exposed, although as far as I can tell so far, you can’t map to a different port number. My web server app expects to serve on port 8080 so that’s the port I have to hit in my browser.
My very simple webserver code expects to connect to a Postgres database with a hardcoded address at localhost:5432. This matches the port specified in the deployment template definition of the database container.
As you might expect from a container group, the web server container is indeed able to connect to the database container on localhost. I know it’s working because the server increments a counter and stores the result in the database every time it serves a page, and I can see that number increasing when I refresh the page.
When you’re done with the container group you can delete it with a single command:
Regular readers will know that I’m part of the Aqua Security team, and our product helps enterprises secure their containerized deployments wherever they run. Even on ACI? Well…let’s just say I’m quietly confident…
Containers / security / open source @isovalent @ciliumproject / cycling / music @insidernine
See all (475)
123 
2
123 claps
123 
2
Containers / security / open source @isovalent @ciliumproject / cycling / music @insidernine
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/reactive-programming-with-event-grid-and-azure-functions-52b24003935d?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jan 28, 2018·6 min read
I play with the Azure Event Grid . Azure Event Grid is a fully-managed intelligent event routing service that allows for uniform event consumption using a publish-subscribe model. Azure Functions have some integration with Event Grid. I’d like to share some of the tips which you to enable Event Grid.
At the first time, you might think what you can use it for? I also thought the same thing. Now I gradually understand several benefits.
According to the Pricing page .
It costs 0.3$ / millions operations with 100, 000 operations for free for a month. It cost around 5 USD per month. Compared with Service Bus Standard plan, which has Publish-Subscribe service, is $10 for month for 5 million operations. For the Event Grid, it is $2.97. Service Bus is very good for highly reliable system like banking system. Event Grid is fit for Event Driven Architecture with scalability. It supports millions events per second.
Event Driven Architecture is not good at Low Latency scenario. If you use Queue for messaging method, it requires Polling/Long Polling. It aims for consistency rather than low latency. However, sometimes, You want to build event-driven serverless system with Low latency. If you create a printing system with event-driven architecture with Queue, it is difficult to print less than 1 min from the request. Also, If you want to move from Microservices architecture to Serverless architecture, you might need Low Latency scenario.
Since Event Grid using Pub-Sub model, Event Handlers need to subscribe a Topic. When The Topic gets an message, message is routed event Handlers, Event Grid allow you to filter the message. You can chose which event you receive. For example, If you use a blob storage, you can filter via directory/file patterns.
Once an Event Grid Topic accept a message and if it fails to send message to event handler, it will retry with exponential back off method. It is much reliable than just use http request. You can substitute some use cases that you use queue for sending message for asynchronous messaging.
You can use Event Grid with Azure Functions. Azure Functions support EventGrid Trigger. However, I can’t find explicit document until now. So, I’d like to share some learning.
Currently Event Grid is preview. The Location which you can use with Azure Functions is limited. Now West US 2 and West Central US is for the Blob Integration. You can check which location you can use in here.
To understand the behavior of EventGrid, let’s start with Custom Topic. On the Azure Portal, create Event Grid Topic.
Create an Event Grid Topic from the Azure Portal.
Get the Topic Endpoint and Topic Key for sending messages from the Event Topic.
Now ready to use Topic.
Let’s write code for Event Grid Trigger. I wrote an sample with C#. You can find whole code in here. You need to install this nuget package.
Microsoft.Azure.WebJobs.Extensions.EventGrid
You might want to know the EventGridEvent definition. You can get a lot of meta data from this. You can get the message from “Data” attribute from the senders.
Then just upload your code to a Function App.
You can create an Event Subscription from the Azure Portal. Go to the Function App which you deploy your code. Just click this link.
You will see this screen. Choose the Event Grid Topic name which you want to subscribe. Then save.
Now you are ready to accept Event Grid message at the Azure Functions.
Sending message is also simple. Just install Microsoft.Azure.EventGrid package as nuget package. You need topicHostName and topicKey which you can get already. One tips, you can pass Json Serializable Poco as the data object. I don’t know why but if you pass Anonymous Json Serializable Object, it will cause an error.
Additional NOTE (30th Jan, 2018):
The data object is not restricted to JObject by EventGrid runtime. It should be cast to JToken as the API level. Now have some issue for Event Grid Trigger. see https://github.com/Azure/azure-functions-eventgrid-extension/issues/25.
Send message using this method. Then you will see the Event Grid Trigger works.
Also, you can filter by Event Type and Subject. When you configure Event Subscription like this, it will trig by only “some/” prefix subject. This feature is handy.
Trigger is almost the same as calling a webhook. If you create a simple webhook function with Event Subscription like this,
Open the Event Grid Topic page, then click Event Subscription
Then create webhook with the HttpTrigger function’s URL.
Then send message to the topic, the HttpTrigger function is also triggered.
I recommend to use EventGrid Trigger. It provides the endpoint url verfication with Event Grid. If you use HttpTrigger, you probably will see 3 initial requests, they are verfication errors, in preview they are just ignored.
You can check if your messages are routed as expected. on the Event Grid Topic page. You can see the Metrics page for each Event Grid Subscription.
One of the difficulty of the debugging is, if you’ve got an error during messaging, sometimes you can’t see the error messages. It looks just not delivered the messages since this is still preview. I encounter that issue three times. Once it was just an shortage of Azure specific region.
The second was I tried to send EventGridEvent with passing non-json-serializable object to the data field. At that time, I didn’t use the Microsoft.Azure.EventGrid package. Just send message with HttpClient. Since now you can use EventGrid package, I recommend to use this. It prevent these issue.
The third was, The medhod name is not match with Function Name. This is the WRONG sample. :)
Also if you encounter the issue which you can’t see an error on your portal and Metrics page, you can ask support request on Azure Portal.
Event Gird is new way to Reactive Programming world. Enjoy the Event Grid with Azure Functions.
Senior Software Engineer — Microsoft
55 
55 
55 
Senior Software Engineer — Microsoft
"
https://medium.com/@maarten.goet/threat-hunting-in-the-cloud-with-azure-notebooks-supercharge-your-hunting-skills-using-jupyter-8d69218e7ca0?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Feb 20, 2019·8 min read
Robert M. Lee has a great quote: “Threat hunting exists where automation ends”. Threat hunting is large manually, performed by SOC analysts, trying to find a ‘needle in the haystack’. And in the case of cybersecurity, that haystack is a pile of ‘signals’.
These analysts often use separate tools for querying the data, manipulating the data set, reversing the potential malware, etcetera. What if we could provide an environment where you can perform all these tasks in context, and share the outcome with your team?
Azure Notebooks, with a little KQL magic sauce, is exactly that. Let’s supercharge your hunting skills with Azure, Jupyter, Python and KQL!
Kusto Query Language (KQL)
Kusto Query Language or KQL in short is the default way to work with data in Azure Data Explorer powered services such as Log Analytics, Azure Security Center, Azure Monitor and many more. It is a powerful yet easy to learn language.
Robert Cain, a Microsoft MVP, has written a 4-hour long course on Pluralsight that you can take for free, to learn the language all the way up to the advanced queries. KQL skills is something you’ll need if you will be doing threat hunting in Azure; most of the security data will be in Log Analytics workspaces.
Jupyter
Jupyter Notebook, formerly called IPython, is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text through markdown. It is already broadly used in data science, and has support for lots of programming languages such as R, Python, etc. The multi-user version of Jupyter is called JupyterHub.
The cool thing is that you can share your notebook with others, and that you can produce interactive output using HTML etc. and display that through a so called “presentation mode”. This makes it great for threat hunting and sharing signals within the SOC team.
On GitHub you’ll find ready-to-run Docker images containing Jupyter.
Azure Notebooks
Azure Notebooks is currently in public preview and is a free hosted service to develop and run Jupyter notebooks in the cloud with no installation. Azure Notebooks is a freeservice, but each project is limited to 4GB memory and 1GB data to prevent abuse. Legitimate users that exceed these limits see a Captcha challenge to continue running notebooks.
However, if the Azure Active Directory account you sign in with is associated with an Azure subscription, you can connect to any Azure Data Science Virtual Machine (DSVM) instances within that subscription. DSVM’s can be found in the Azure Marketplace. With these dedicated DSVM’s you can add better processing power and remove any of those limits.
PRO TIP: You need to deploy the Ubuntu version of the DSVM. The Windows version of DSVM does not contain JupyterHub by default. The Ubuntu template of DSVM has an extra bonus: it will open up the right ports by default in your NSG!
In the case of Azure Notebooks, it allows you to share your notebooks using GitHub.
Pandas, KQLMagic and other libraries
One of the things you will find out early using Jupyter is that you will want to manipulate data. This is where a library called Pandas comes in. Pandas is an open source Python framework, maintained by the PyData community and mostly used for Data Analysis and Processing.
Another library you will need is KQLMagic. Michael Binshtock, who works at Microsoft, wrote this and allows you to directly query Log Analytics-based workspaces in Azure, for instance when working with data in Azure Monitor, Azure Security Center, etcetera. The great thing about this library is that it uses the Kusto Query Language (KQL). Which means that you can use your favorite KQL queries directly in Jupyter.
The big picture
Putting all the pieces together you get something like this:
Real-world threat hunting
Let’s look at a real-world example. In this case we have a number of virtual machines running in Microsoft Azure, and Azure Security Center is turned on at the subscription level to capture relevant security events.
We’re suspicious of a machine called APPSERVER, based on an Alert we got fromAzure Security Center, and want to do some investigation.
We go to Azure Notebooks and login:
We create a new Project called ‘Threat Hunting’:
We create a new Notebook called ‘Azure threat hunting’:
We will use the Free Compute option and open the notebook:
The docker image that the Free Compute option provides *already contains* the Kqlmagic library that we will need. If you’re using a dedicated DSVM, or are running Jupyter locally, you should run the install command to get the library installed:
PRO TIP: The Free Compute is a docker container in a shared compute environment and therefore it will take a couple of minutes before the library loads. You can look ‘behind the scenes’ by using the Terminal button:
Through the terminal window you can issue commands such as ‘ps’ or ‘top’:
Now we will need to authenticate to the Log Analytics workspace we will be using. In this case we will be connecting to Azure Security Center:
We can now run KQL queries to look at the data being captures by Azure Security Center for this machine. In this case we’ll have a look at the network connections it has:
PRO TIP: While in the KQL query interface in Azure you’ll be using the double quote character for specifying input, you’ll be using the single quote in Jupyter. Make sure to change your queries so that they work properly in Jupyter.
If you want to go multi-line to make things better readable, you need to use double %. As our application server is in The Netherlands, I will apply a filter and only show the connections that are going to IP addresses that our outside of our country:
Let’s see if any of these IP addresses match a TOR node. There is a current list of TOR nodes and their IP addresses at https://www.dan.me.uk/torlist. We can load that into our notebook using Pandas:
The next step is to compare the two lists to see if there are any matches:
Another great way of visualizing your data is taking the Longitude and Latitude points from the KQL query and putting them on a world map. Add the following line to your KQL query:
And use the following Python code in Jupyter:
From this point on you’ll likely want to do some more investigation and assess whether or not there is a real threat. Use your own hunting skills for that ;-)
Sharing your findings
An unique feature of Jupyter is the Presentation mode. It allows you to easily share key items from your audience to other people in a visual friendly way, without having to copy/paste data to another application.
You can use Markdown text to annotate your notebook. Enable the Slide picker by going to the View menu, Cell Toolbar, then Slide Show. Go to any row and on the right-hand side select to Skip it, be part of a Slide, etcetera.
Lastly, click on ‘Enter/Exit RISE Slideshow’ to share your findings:
I’ve published this Jupyter notebook on my GitHub repository. Another great thing about Azure Notebooks is that you can clone any repository and turn it into a Jupyter project:
Other examples
John Lambert, distinguished engineer at Microsoft’s Threat Intelligence Center, has some other great examples on threat hunting with Jupyter which he has shared here:
There is also a sample notebook on MyBinder that shows you step-by-step which Kqlmagic commands are available, and how to use them.
Conclusion
Jupyter is a great platform for threat hunting. You can work with data in-context and natively connect to security backends in Microsoft Azure using Kqlmagic.
Best of all, using Azure Notebooks and Azure Security Center, we didn’t spend a dollar and got our threat hunting platform for free :-)
Start learning KQL, Python and Jupyter today and supercharge your hunting skills!
Thank you
A big thank you goes out to Michael Binshtock, John Lambert, Giovanni Lanzani and Paul Shealy. They have been invaluable on providing support, and background information, while I wrote this article.
Happy hunting!
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
40 
1
40 claps
40 
1
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/threat-hunters-forge/azure-sentinel-to-go-b5f6848d3c61?source=search_post---------188,"There are currently no responses for this story.
Be the first to respond.
Recently, I started working with Azure Sentinel, and as any other technology that I want to learn more about, I decided to explore a few ways to deploy it. I got a grasp of the basic architecture and got more familiarized with it. As a researcher, I also like to simplify deployments in my lab environment and usually look for ways to implement the infrastructure I work with as code. Therefore, I started to wonder if I could automate the deployment of an Azure Sentinel solution via a template or a few scripts. Even though, it made sense to expedite the deployment of the solution, I realized I still did not have data or other resources to play with. Then, I wondered If I could integrate the deployment of an Azure Sentinel instance and other resources through the same scripts or templates covering different scenarios.
In the end, this approach allows me to also share the process with others in the community 🌎 in a more practical way.
This post is part of a four-part series where I will show you how to deploy your own Azure Sentinel solution in a lab environment via Azure Resource Management (ARM) templates along with a custom logs ingestion pipeline to consume pre-recorded datasets and other resources such as network environments for research purposes.
In this post, I show you how to use ARM templates to deploy an Azure Sentinel solution and ingest pre-recorded datasets via a python script, Azure Event Hubs and a Logstash pipeline.
The other parts of this series can be found in the following links:
Microsoft Azure Sentinel is a scalable, cloud-native, security information event management (SIEM) and security orchestration automated response (SOAR) solution. An Azure service that empowers organizations to bring disparate data sources from resources hosted both on-premises and in multiple clouds and be able to detect, investigate and respond to threats.
If you want to learn more about Azure Sentinel, I would recommend to explore this Microsoft Azure document page. Also, if you want to know what you can do with it, make sure you read the articles available in the Microsoft Tech Community Sentinel blog and take a look at these awesome webinars.
Technically, all we need to do to deploy an Azure Sentinel solution is:
That basic set up allows you explore all the main features of Azure Sentinel as well as preloaded out-of-the-box resources such as queries, visualizations, response playbooks, and notebooks. You could also upload other resources and even enable data connectors in Azure Sentinel via code. Javier Soriano blogged about it in this post, and it is a great reference for production deployments.
One of the things I wanted to do different for this post was execute Azure Sentinel On-boarding steps, but in a declarative way with Azure Resource Manager (ARM) templates without having to run Powershell commands.
To implement infrastructure as code for your Azure solutions, use Azure Resource Manager templates. The template is a JavaScript Object Notation (JSON) file that defines the infrastructure and configuration for your project. The template uses declarative syntax, which lets you state what you intend to deploy without having to write the sequence of programming commands to create it.
The Azure Resource Manager is the deployment and management service for Azure and below you can see some of the ways you could interact with it.
A few things that I like about ARM templates are the orchestration capabilities to deploy resources in parallel which makes it faster than serial deployments, and also the feature to track deployments via the Azure portal.
Now that we know a little bit more about Azure Resource Manager services, we are ready to deploy Azure Sentinel. One document that I recommend to get familiar with to learn more about Azure resources mapped to ARM template resource types is this one. In this section, we are going to deploy a Log Analytics workspace and enable Azure Sentinel. Remember that I provide the template for you so that you can follow along.
A Log Analytics workspace can be found under the Microsoft.OperationalInsights resource types as Microsoft.OperationalInsights/workspaces
I created an initial template with some parameters to make it modular for anyone to use. This is the initial template:
Next, I needed to define the Azure Sentinel solution and enable it on the top of the Log Analytics workspace. You can do it with a resource type found under the Microsoft.OperationsManagement resource types as Microsoft.OperationsManagement/solutions .
I added that to our initial ARM template and this is the final result:
That’s it! You can download it and use it for the next steps.
There are a few ways to execute ARM templates, and it all depends on how comfortable you are with the Azure portal and Azure tool-kits (e.g. Azure CLI)
If you want to use one command to deploy an ARM template, then this option is for you. The Azure command-line interface (CLI) is Microsoft’s cross-platform command-line experience for managing Azure resources. It can be installed in Windows, macOS and Linux environments. In addition, there is a PowerShell version of it and also an interactive, authenticated, browser-accessible option known as the Azure Cloud Shell.
We can start using Azure CLI and create a Resource Group if you have not done it yet. Run the following command to create one in a specific location:
Next, you can run the following command to execute the ARM template:
Track your deployment: Azure Portal>Resource Group Name>Deployments
That’s it! once your deployment completes, you will be able to access the main Azure Sentinel interface. Before we do that, let me show you another way to execute our ARM template.
It takes a few more clicks to do it via the Azure portal, but it is easy to follow:
That’s it! once your deployment completes, you will be able to access the main Azure Sentinel interface.
Search for “Azure Sentinel”
Select the Azure Sentinel workspace that you just created.
You will be taken to the main Azure Sentinel interface 🍻 That was easy right?
“Why do I have to do all that with ARM templates when I can just follow these instructions and with a few clicks I can deploy one too?”
Deploying the solution while working in a lab environment is not enough. You need to have other resources and data to start exploring and learning about all the capabilities Azure Sentinel provides. That will take more than just a few clicks. What if we can take the ARM template that we just used and run other nested templates in parallel to deploy other resources and even ingest pre-recorded data for additional research?
Azure-Sentinel2Go is an open source project developed to expedite the deployment of an Azure Sentinel lab along with other Azure resources and a data ingestion pipeline to consume pre-recorded datasets for research purposes.
Azure-Sentinel2Go is part the Blacksmith project
The Blacksmith project focuses on providing dynamic easy-to-use templates for security researches to model and provision resources to automatically deploy applications and small networks in the cloud.
Azure Sentinel2Go is a work in progress, and I welcome feedback on what it is that you would like to see being deployed along with an Azure Sentinel solution and datasets you would like to work with in your lab environment.
One of the features that I have noticed security analysts get interested the most while using Azure Sentinel for the first time is the Log Analytics capabilities. Log Analytics is the primary tool in the Azure portal for writing log queries written in Kusto Query Language (KQL) to quickly retrieve, consolidate, and analyze security events. Therefore, I decided to find a way for researchers to learn about KQL with pre-recorded datasets.
Fortunately, the Log Analytics workspace allows the collection of custom logs via its HTTP Data Collector API. If you want to learn how to do it with code, there are some basic examples in Azure docs for Powershell, C# and Python.
In this section I will share a few of my favorite ways to send pre-recorded datasets a Log Analytics workspace custom log table.
This is one of the simplest ways to send data directly to the log analytics workspace. I took the basic example available here, and extended it a little bit to be able to read from a JSON file or a folder, show a progress bar, and send smaller sized chunks of 5MB per POST request. Make sure you read the Data Limits while using a similar approach. I also extended the PowerShell script available and created a proof of concept here.
The script has the following options and all the information you need from the your log analytics workspace can be found in Azure Portal>Log Analytics Workspace>Advanced Settings.
Next, we need a data sample for these exercises. Therefore, the project comes with a few data samples in this folder. Download the dataset-sample-small.tar.gz to your local computer and decompress it.
Next, send it over by running these commands in your local computer:
Once it completes go to your Azure Sentinel interface and click on Logs. You can see that there are no events yet. It usually takes from 5–10 mins.
You can see a new table under customs logs with the event schemas. Remember that not every event will have the same schema. Make sure you understand the schema of your events before running queries.
Based on the event schemas, we can run the following query to see what events we are working with:
That’s it! This is a very practical way to ingest custom logs, but might not scale with larger files or hundreds of files in a loop. Therefore, I wanted to also provide another option that would allow me to send events to a more robust pipeline and let it handle the whole process. This is a proof of concept and works very well in a lab environment.
I like to use existing tools that are proven to work at scale and this is not the exception. TL;DR — I use Kafkacat to read json files stored locally and send them over to an Azure Event Hub. Next, Logstash reads them from Azure Event Hub, and sends them over to a Log Analytics workspace.
In more details the following is happening in the image above:
I already provide the following configurations as part of Azure Sentinel2Go:
This is the Logstash input config file to consume events from the Azure Event Hub. The plugin used is the Logstash Azure Event Hubs input plugin.
I do not use the input codec => ""json"" property because I do not want to unpack the event Message field and exceed the max number (500) of custom fields per data type in the Log Analytics workspace.
This is the Logstash output config file to send the events that it collects from the Azure Event Hub to the Log Analytics workspace. The plugin used is the Azure Log Analytics output plugin for Logstash developed by Microsoft.
One thing I added to the Azure Sentinel2Go repository is a “Deploy to Azure” badge used on Azure quick-start templates to upload the ARM template directly to the Azure portal. Very convenient! Click on the badge!
You will be taken to the interface to set deployment parameters. Set the Deploy Pipeline parameter to Logstash-EventHub. One thing to pay attention to is the virtual machine size. If you are in westus, you need to switch it to Standard_A3 . Also, make sure you set the AllowedIPAddresses parameter to restrict access to the Logstash box. Add your company or your house Public IP address.
Monitor your deployment. It should take around 8–10 minutes.
Once it completes, you should be able to send prerecorded data from your local computer to the Azure Event Hub.
First, create a local Kafkacat configuration file to define a few properties to be able to access the Azure Event Hub. I created one for you as shown below.
You will need to get the following values and paste them in the config file.
Second, we need a sample dataset to send over to our Azure Event Hub. We can use the same dataset we used earlier with the Python script.
Next, in your local computer, run Kafkacat in Producer mode as shown below.
Once you run that command, you can check the events flowing through the Event Hub. Go to Azure Portal > Resource Group Name > Event Hub Namespace and filter the Show Metrics view to show Messages only. It might take a few minutes for the view to update.
The Azure Sentinel view also will take a a few mins to update.
As you already know, click on Logs (Log Analytics) to explore the custom logs and their schema. One thing to remember is that the events flowing through this pipeline are packed inside of the Message field. This is to avoid exceeding the max number (500) of custom fields per data type in case you send a lot of events with different schemas. I will show you an example later.
You can unpack the Message field and get to specific nested field with the Kusto Query function parse_json(). This function interprets a string as a JSON value and returns the value as dynamic .
Remember that not every event will have the same schema. Make sure you understand the schema of your events before running queries.
Azure Sentinel2Go also comes with the option to load pre-recorded datasets right at deployment time. It leverages the same Logstash VM for the data ingestion. You do not have to send anything from your local computer.
I use the following commands to download and decompress all small mordor datasets. The commands are part of the deployment and are executed inside of the Linux VM when you choose to add the item “mordor-small-datasets” to the Add to cart parameter while deploying Azure Sentinel2Go. You do not have to run anything in your local computer.
If you choose to add the item “mordor-large-apt29” to your Add Mordor Dataset parameter while deploying Azure Sentinel2Go, the following commands are executed inside of the Linux VM.
This is the additional Logstash input config to read all the JSON files. The plugin used is the Logstash File Input plugin.
If you have resources running from the earlier deployment, I recommend to delete them (Lab environment). Similar to our previous deployment, go to Azure-Sentinel2Go > grocery-list > custom-log-pipeline. Select Logstash for the Deploy Custom Logs Pipeline parameter as shown below and add a mordor dataset to your cart (Add Mordor Dataset) . For this example, we are going to use the mordor-small-datasets. Also, once again, make sure you set the AllowedIPAddresses parameter to restrict access to the Logstash box. Add your company or your house Public IP address.
Monitor the deployment. It might take around 8–10 minutes for it to be done. When it is complete, go to your Azure Sentinel interface. Give it 2–3 mins for events to start showing. You will start getting thousands of events (200K+)
What I do while I wait for all the events (300k+) to be ingested 😆 🐶 ❤️
Take advantage of the time you have and stretch a little bit! Take a break! ❤️
We can do the same as before and explore a few events to understand the event schemas. Also, since those events were generated as part of the Mordor project, you could focus on datasets mapped to specific ATT&CK tactics and techniques. The project comes with a Navigator View for the specific platforms that it supports (Currently only Windows).
One thing that I like to look for when looking for lateral movement techniques is processes created under logon sessions that were initially created as part of a network authentication event (Logon Type 3). One example can be adversaries leveraging the Windows Management Instrumentation (WMI) and its Win32_Process class to execute command over the network. This behavior would generate something similar to this:
We can use KQL and its JOIN operator to look for a similar behavior without filtering on the parent process wmiprvse.exe. We can use events 4624 (An account was successfully logged on) and 4688 (A New process has been created) from the Microsoft-Windows-Security-Auditing event provider.
Use the following query and run it in log analytics as shown below
As you can see in the image below, that query got some hits from a few datasets that were created after emulating adversaries using WMI and Powershell Remoting to execute commands over the network 🏹
That’s it for this first part! I hope you enjoyed it and found the design and deployment of Azure Sentinel2Go helpful. In the next post, I will show you how to deploy additional resources along with an Azure Sentinel solution to focus on a few use cases that go beyond just using the Log Analytics features. I want to make sure Azure Sentinel2Go also allows the exploration of other capabilities provided by Azure Sentinel.
Azure Sentinel2Go Link: https://github.com/OTRF/Azure-Sentinel2Go
https://mordordatasets.com/introduction
https://docs.microsoft.com/en-us/azure/azure-monitor/faq
https://docs.microsoft.com/en-us/azure/azure-monitor/terminology
https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-platform
https://docs.microsoft.com/en-us/azure/azure-monitor/platform/data-sources#custom-sources
https://docs.microsoft.com/en-us/azure/sentinel/overview
https://techcommunity.microsoft.com/t5/azure-sentinel/deploying-and-managing-azure-sentinel-as-code/ba-p/1131928
https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/overview
https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/overview
https://docs.microsoft.com/en-us/azure/azure-monitor/insights/solutions
https://azuremarketplace.microsoft.com/en-us/marketplace/apps/Microsoft.SecurityOMS?tab=Overview
https://azure.microsoft.com/en-us/pricing/details/azure-sentinel/
https://azure.microsoft.com/en-us/pricing/details/monitor/
https://www.elastic.co/guide/en/logstash/current/plugins-inputs-azure_event_hubs.html
https://azure.microsoft.com/en-us/services/event-hubs/
https://github.com/yokawasa/logstash-output-azure_loganalytics
https://docs.microsoft.com/en-us/azure/kusto/query/
Threat Hunting, Data Science & Open Source Projects
49 
49 claps
49 
Threat Hunting, Data Science & Open Source Projects
Written by

Threat Hunting, Data Science & Open Source Projects
"
https://medium.com/@renatogroffe/docker-kubernetes-azure-devops-it-experts-superdigital-b6d436632e9b?source=search_post---------189,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 14, 2019·4 min read
No dia 15/04/2019, uma segunda, meu amigo Milton Câmara e eu (Renato Groffe) participamos como palestrantes do It Experts, um evento promovido pela Superdigital (uma empresa do grupo Santander) em São Paulo-SP.
Deixo aqui meu agradecimento ao Natanael Sá pelo convite e por todo o apoio para que este evento acontecesse.
E aproveito este espaço e o grande interesse por Docker também para um convite.
Tem interesse em conhecer mais sobre Docker? Que tal então fazer um curso completo, cobrindo desde fundamentos a diferentes possibilidades de uso de containers com tecnologias em alta no mercado? Adquira conhecimentos profundos sobre Docker, evolua e se diferencie no mercado, seja você um profissional DevOps, um Desenvolvedor ou um Arquiteto de Software!
Acompanhe o portal Docker Definitivo para ficar por dentro de novidades a serem anunciadas em breve!
Site: https://dockerdefinitivo.com/
Esta iniciativa contou com as seguintes apresentações:
Público: 25 pessoas
A seguir estão os slides que utilizamos como base para as apresentações:
As demonstrações realizadas tomaram como base o seguinte projeto:
ASP.NET Core + Docker + Alpine Linux + Docker Hub + Kubernetes + AKS (Azure Kubernetes Service)
A implementação deste projeto, bem como a criação de objetos para deployment em um cluster Kubernetes foram abordadas por mim no seguinte artigo:
Orquestração de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
O post a seguir agrupa traz estes materiais e outras referências sobre a utilização de Docker e diversas outras tecnologias (principalmente .NET Core):
Docker para Desenvolvedores .NET - Guia de Referência
Demonstrei também o deployment de aplicações empregando containers Docker na nuvem no seguinte vídeo do Canal .NET, em que cobri a utilização do serviço conhecido como Azure Web App for Containers:
O uso do Kubernetes foi tema ainda de 2 eventos online gratuitos do Canal .NET, com a gravação dos mesmos estando disponível no YouTube e abordando desde os primeiros passos ao deployment automatizado via Azure DevOps (antigo VSTS):
A seguir estão fotos gerais do evento e de cada uma das palestras realizadas.
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
58 
58 claps
58 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/mensageria-net-core-3-1-exemplos-com-rabbitmq-kafka-azure-service-bus-e-azure-queue-storage-c594bf30c091?source=search_post---------190,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 12, 2020·4 min read
Recentemente participei de algumas apresentações no Canal .NET em que foi abordado o uso de mensageria em soluções construídas com .NET Core 3.1, com exemplos envolvendo tecnologias como Azure Queue Storage, Azure Service Bus, RabbitMQ e Apache Kafka.
Neste novo post trago diversos exemplos que implementei e utilizei durante estes eventos (além de projetos adicionais), bem como o link com a gravação de cada uma destas lives. Na seção de Referências foram incluídos também artigos cobrindo algumas das soluções de mensageria aqui mencionadas.
E aproveito este espaço para um convite…
Caso você precise conhecer mais sobre o Microsoft Azure como um todo, não deixe de aproveitar o preço promocional (apenas R$ 255,00) da segunda turma online do treinamento Azure na Prática com foco em Desenvolvimento Web que acontecerá dia 20/06/2020 (um sábado). Aproveite para aprender mais sobre as possibilidades oferecidas por serviços do Microsoft Azure como App Service (para hospedagem de Web Apps, com suporte a ASP.NET Core e Docker), Azure Storage, Azure Functions + Logic Apps (soluções serverless), Azure App Configuration + Key Vault… E o melhor, no conforto de sua casa! Acesse o link a seguir para informações e efetuar sua inscrição:
https://bit.ly/anp-web3-blog-groffe
.NET Core 3.1 + Console Application + Envio de Mensagens + RabbitMQ + Azure Service Bus (Queues e Topics) + Apache Kafka
.NET Core + Azure Functions 3.x + Consumo de Mensagens + Queue Storage + RabbitMQ + Azure Service Bus (Queue)
.NET Core + Azure Functions 3.x + Azure Service Bus + Apache Kafka + Topics
ASP.NET Core 3.1 + API REST + Apache Kafka + Tópico (Envio de Mensagens)+ Manipulação de Ações
.NET Core + Azure Functions 3.x + Azure Cosmos DB + Apache Kafka (Tópico/Consumo de Mensagens) + HTTP Trigger + Cotações de Ações
.NET Core + Azure Functions 3.x + MongoDB + Apache Kafka (Tópico/Consumo de Mensagens) + Cotações de Ações
.NET Core 3.1 + Console Application + Apache Kafka (Tópico/Consumo de Mensagens) + Redis + Serilog + Manipulação de Ações
.NET Core + Azure Functions 3.x + RabbitMQ (Consumo de Mensagens)+ Azure SQL/SQL Server + RabbitMQ Trigger + HTTP Trigger
.NET Core 3.1 + Console Application + RabbitMQ (Consumo de Mensagens) + Moedas Estrangeiras
ASP.NET Core 3.1 + API REST + Azure Service Bus + Tópico (Envio de Mensagens) + Manipulação de Ações
.NET Core + Azure Functions 3.x + Azure Cosmos DB + Azure Service Bus (Topic/Consumo de Mensagens) + Queue Trigger + HTTP Trigger + Cotações de Ações
.NET Core + Azure Functions 3.x + Azure Service Bus (Topic/Consumo de Mensagens) + SQL Server + Dapper + HTTP Trigger + Cotações de Ações
.NET Core 3.1 + Console Application + Azure Service Bus (Topic/Consumo de Mensagens) + Redis + Serilog + Cotações de Ações
Nesta live utilizei o projeto a seguir para o envio de mensagens a uma fila ou tópico (permitindo assim o teste de tecnologias como Azure Queue Storage, Azure Service Bus, RabbitMQ e Apache Kafka):
.NET Core 3.1 + Console Application + Envio de Mensagens + RabbitMQ + Azure Service Bus (Queues e Topics) + Apache Kafka
O consumo de mensagens associadas a filas criadas com Azure Queue Storage, Azure Service Bus e RabbitMQ envolveu um projeto baseado em Azure Functions + .NET Core 3.1:
.NET Core + Azure Functions 3.x + Consumo de Mensagens + Queue Storage + RabbitMQ + Azure Service Bus (Queue)
E um segundo projeto também criado com Azure Functions + .NET Core 3.1 e vinculado a tópicos do Azure Service Bus e Apache Kafka:
.NET Core + Azure Functions 3.x + Azure Service Bus + Apache Kafka + Topics
A gravação está disponível no YouTube:
Para esta apresentação utilizei um projeto ASP.NET Core 3.1 que possibilita o envio de mensagens a um tópico do Apache Kafka:
ASP.NET Core 3.1 + API REST + Apache Kafka + Tópico (Envio de Mensagens)+ Manipulação de Ações
E ainda implementei 3 projetos para o consumo de mensagens vinculadas ao tópico em questão:
.NET Core + Azure Functions 3.x + Azure Cosmos DB + Apache Kafka (Tópico/Consumo de Mensagens) + HTTP Trigger + Cotações de Ações
.NET Core + Azure Functions 3.x + MongoDB + Apache Kafka (Tópico/Consumo de Mensagens) + Cotações de Ações
.NET Core 3.1 + Console Application + Apache Kafka (Tópico/Consumo de Mensagens) + Redis + Serilog + Manipulação de Ações
A seguir temos a gravação do YouTube:
Nesta live um dos assuntos foi AMQP (Advanced Message Queuing Protocol), um dos protocolos mais populares entre soluções de mensageria. O Azure Service Bus constitui um bom exemplo de alternativa com suporte a este padrão, sendo que fiz uso desta tecnologia em uma aplicação ASP.NET Core 3.1 para o envio de mensagens a um tópico:
ASP.NET Core 3.1 + API REST + Azure Service Bus + Tópico (Envio de Mensagens) + Manipulação de Ações
Os próximos projetos exemplificam o consumo de mensagens vinculadas ao tópico do Azure Service Bus utilizado na aplicação anterior:
A gravação desta live também se encontra no YouTube:
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
.NET + Apache Kafka: Guia de Referência
.NET Core + Serverless: melhorando a experiência de Desenvolvimento com Azure Functions 3.x | pt 1
.NET Core 3.x + Serverless: configuração, dicas e exemplos com Azure Functions 3.x
Serverless + Azure Functions: Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
48 
48 
48 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/no-code-data-enhancement-with-azure-synapse-analytics-and-azure-auto-ml-cb9d97fb0c26?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
TLDR; This post will walk through how to train and evaluate Azure ML AutoML Regressions model on your data using Azure Synapse Analytics Spark and SQL pools.
Before we get started let’s make sure we are all on the same page with the core Azure concepts needed to take your data to the next level.
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
Azure Synapse Analytics is an integrated service that accelerates extracting insightful across data warehouses and big data systems.
Azure Synapse ties together traditional relational SQL enterprise data warehousing, unstructured data stores and serverless Apache Spark , to enable limitless pipelines for ETL and ELT operations. Furthermore Synapse Studio provides a unified interface for data monitoring, coding, and security.
Azure Machine Learning (Azure ML) is a cloud-based service for creating and managing machine learning solutions. It’s designed to help data scientists and machine learning engineers to leverage their existing data processing and model development skills & frameworks.
In Azure Machine Learning, Auto ML allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality.
With Auto ML we can transform Synapse Analytics Data into actionable baseline models to enrich datasets at scale without writing a single line of machine learning code.
Regression is used to build models to forecast numeric values such as taxi fares based on learned input features.
In the next section, I will walk you through an end to end example of how to enrich you Synapse data by training and evaluating a model with the NYC Taxi Dataset. Once you get preform these steps you’ll be able to train and run your own Auto ML on models on any tabular dataset of your choosing.
First if you do not have them already we need to create our Azure ML and Synapse workspaces.
Once we have deployed our two workspaces we need to link them. Full steps for linking Azure ML and Synapse Workspaces can be found here
To actually ingest and process our data we need to use pools. Azure Synapse Analytics offers various analytics engines to help you ingest, transform, model, analyze, and serve your data.
For this tutorial since we are using a toy dataset we can use the cheapest pools available for you own data you may want to configure your pools accordingly.
The serverless Apache Spark pools offers open-source big data compute capabilities. This is where the majority of our data processing and Auto ML code will run.
A dedicated serverless SQL pool offers T-SQL based compute and storage capabilities. We will use this pool to store the data we want to enhance with our AutoML model.
One key advantage of Azure Synapse Analytics is if you configure a time out you only pay for the compute when it’s in use.
Once we have our serverless Spark and SQL pools up and running we can now ingest our data setup our Spark and SQL tables for training and testing respectively.
Download this Spark Create-Spark-Table-NYCTaxi- Data.ipynb notebook and import it into your workspace.
Once the notebook is uploaded, change the sql_pool_name value to match the name of your sql pool and then select the desired spark pool and run click all.
Once the data is ingested we can use our spark nyc_taxi and Spark Pool to train an AutoML regression model for forecasting taxi fares.
Follow the three steps in wizard below to train your model.
This will kick off your Auto ML Regression training job. It should take about 2hrs to run when it is complete we can then evaluate it on our SQL test table.
Once we have the best model we can now evaluate it on our test SQL table using our SQL Pool.
First we need to select the table we want to enhance with the model we just trained.
Then we select our new Auto ML model, map the our input table columns to what the model is expecting and choose or create a table for storing our model locally.
The wizard will generate a T-SQL script that evaluate our model against the test data and outputs the fare predictions.
There you have it all you need to know to train and test you own AutoML models and make them actionable.
Additional Synapse Documentation and Walkthroughs worth checking out can be found below:
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
techcommunity.microsoft.com
Now that you’ve finished the steps above it is time to try them out on you own synapse data. Feel free to post in the comments if you have any questions and to share the cool models you make!
Look forward to seeing what AutoML and Azure Synapse can do for you!!
Thanks to Nellie Gustafsson, Yifan Song and Chang Xu from the Azure Synapse product team for their great documentation and support during the writing this post.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with the Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
154 
2
154 claps
154 
2
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/azure-na-pratica/azure-na-pr%C3%A1tica-gratuito-2-docker-saiba-como-foi-conte%C3%BAdos-gratuitos-1720aa45005?source=search_post---------192,"There are currently no responses for this story.
Be the first to respond.
Neste último sábado (06/06/2020) o Azure na Prática promoveu seu segundo minicurso online e gratuito, com foco desta vez em Docker. Além de uma introdução englobando conceitos básicos e benefícios da adoção de containers, ao longo deste treinamento foi coberto também o uso de Docker em conjunto com tecnologias como Docker Hub, Docker Compose, Docker Desktop, Windows, Linux, macOS (com o auxílio do Ericson da Fonseca) SQL Server, PowerShell, Bash, .NET 5/ASP.NET 5, NGINX, RabbitMQ, MongoDB, Microsoft Azure, Azure Web App for Containers e Visual Studio Code.
Caso você queira ter acesso ao conteúdo do primeiro treinamento (que aconteceu em Maio/2020) gratuitamente ou, até mesmo, deseje revê-lo, acesse o link a seguir:
Azure na Prática Gratuito #1 - Desenvolvimento Web: saiba como foi + conteúdos gratuitos
Fui instrutor e um organizadores desta iniciativa, juntamente com meus amigos Milton Câmara (Microsoft MVP, MTAC) e Ericson da Fonseca (Microsoft MVP). O resultado geral foi muito além de nossas expectativas iniciais, com 4269 inscrições via Sympla:
E um pico de 1100 pessoas nos assistindo ao longo da live no YouTube, com mais de 4 mil visualizações até o momento da publicação deste post:
A gravação já está disponível no canal Azure na Prática no YouTube e pode ser assistida gratuitamente (aproveitamos para convidar você que está lendo esse post para que se inscreva no YouTube):
Mais uma vez tivemos espectadores de outros países, como Portugal, Angola, Argentina e Samoa (além claro de participantes de norte a sul do Brasil). Recebemos inúmeros agradecimentos em redes sociais (YouTube, Facebook, LinkedIn) por este esforço, algo que sempre nos motiva para seguir em frente com esse tipo de iniciativa. Seguem alguns feedbacks:
Os slides utilizados estão disponíveis no SlideShare:
A seguir temos o repositório do GitHub com os exemplos usados durante o minicurso:
https://github.com/azurenapratica/ANP-Docker-Minicurso
Aproveitamos para agradecer:
A Microsoft disponibilizou brindes sob a forma de conteúdos gratuitos e que podem ser acessados no link:
http://bit.ly/anp0606
Nas próximas seções temos descontos para os próximos cursos pagos do Azure na Prática, além de conteúdos gratuitos sobre Docker.
O canal do Azure na Prática no YouTube é uma excelente fonte de conteúdos, com gravações gratuitas incluindo mesas redondas, dicas e truques na utilização de Docker e serviços do Microsoft Azure com suporte a containers:
www.youtube.com
Temos ainda o Canal .NET e o Coding Night, canais em que o uso de containers Docker é abordados com frequência:
www.youtube.com
www.youtube.com
A seguir estão também diversos artigos abordando a utilização de Docker(há vídeos sendo referenciados em alguns destes posts):
Docker — Guia de Referência Gratuito
Docker + Bancos Relacionais: cuidados importantes na criação de containers
Docker + Portainer: gerenciando containers a partir de um browser
MongoDB + mongo-express + Docker Compose: montando rapidamente um ambiente para uso
Kubernetes — Guia de Referência Gratuito
Docker para Desenvolvedores .NET — Guia de Referência
ASP.NET Core + Docker: trabalhando com variáveis de ambiente
Microservices: alternativas para a implementação no Microsoft Azure
Docker + Azure DevOps: build e deployment automatizado de aplicações
Kubernetes + Azure DevOps: build e deployment automatizado de aplicações
Docker + GitHub Actions — parte 1: build automatizado de aplicações
Docker + GitHub Actions — parte 2: deployment automatizado de aplicações
Como o Microsoft Azure pode simplificar a publicação de suas Web Apps? — Dica Rápida
E concluo este post com os links para os blog de cada um dos responsáveis por este minicurso:
Renato Groffe — Blog
Ericson da Fonseca — Blog
Milton Camara Gomes — Blog
Além de convidar a todos para que acompanhem e sigam o blog Azure na Prática:
Azure na Prática — Blog
Blog do Azure na Prática
60 
60 claps
60 
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/aplicando-testes-automatizados-com-selenium-e-azure-devops-mvpconf-latam-2019-33b2d5840a7?source=search_post---------193,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 28, 2019·3 min read
Nos dias 12 e 13 de Abril/2019 (sexta e sábado) aconteceu o MVPConf Latam 2019, o maior evento técnico brasileiro de experts Microsoft.
Em sua segunda edição e organizado pela comunidade de MVPs Microsoft no Brasil, o MVPConf teve caráter beneficente (o valor arrecadado foi repassado a instituições beneficentes das 5 regiões do Brasil: Sudeste, Sul, Centro-Oeste, Nordeste e Norte) e contou com mais de 2 mil pessoas inscritas de todo o país.
Realizei com meu amigo Milton Câmara Gomes (Microsoft MVP) 2 apresentações durante o evento, focando na implementação e execução automatizada de testes de aplicações Web utilizando o Selenium WebDriver e o Azure DevOps:
Gostaria de deixar neste post meu muito obrigado ao André Dias (Microsoft MVP) que coordenou a trilha DevOps pela oportunidade em participar da mesma como palestrante.
Deixo aqui também meus agradecimentos ao Robson Araújo (Microsoft MVP) e ao Thiago Adriano (Microsoft MVP) pelas fotos tiradas durante a apresentação. E também ao Fernando Seguim que atuou no staff por toda a ajuda durante minhas palestras.
Os slides utilizados durante a apresentação foram disponibilizados no SlideShare:
E no próximo link está o projeto de exemplo utilizado durante a minha demonstração:
Selenium WebDriver + .NET Core 2.2 + .NET Standard 2.0 + xUnit
Já descrevi anteriormente esta solução no seguinte artigo:
.NET Core + Selenium WebDriver: testes em modo headless com Firefox e Chrome
O assunto implementação de testes de aplicações Web com Selenium WebDriver foi tema também de um evento online no Canal .NET. A gravação está no YouTube e pode ser assistida gratuitamente:
A seguir estão as fotos tiradas em cada uma das 2 sessões.
12/04/2019 – 1a. Palestra
13/04/2019 – 2a. Palestra
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
92 
92 
92 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/t-t-software-solution/1-virtual-academy-for-azure-fundamentals-%E0%B9%82%E0%B8%94%E0%B8%A2-aipen-studio-3f60db031b0f?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
ผมกำลังมองหาโครงการส่งเสริมการสอบ Microsoft Certified: Azure Fundamentals (AZ-900) (ขอเรียกย่อๆว่า AZ-900 น่ะครับ) มาตั้งแต่ต้นปี แต่โชคร้ายที่เจอ COVID-19 จนทำให้โครงการที่ลงทะเบียนไว้มีอันต้องถูกยกเลิกทั้งหมดครับ
จนกระทั่งได้มาเจอ Post จากคุณ Issaret Prachitmutita ใน FB Group สมาคมโปรแกรมเมอร์ จึงตัดสินใจลงเรียนโครงการนี้ครับ โดยตั้งความคาดหวังไว้ 2 อย่าง
ขอขอบคุณ คุณ อิศเรศ ประจิตต์มุทิตา และทีมงาน AiPEN Studio ทุกๆท่านที่จัดโครงการดีๆ รวมถึงเนื้อหาบทเรียนจากอาจารย์ วิสิทธิ์ ทองภู่ มากๆนะครับ
เป็นโครงการในนามสมาคมโปรแกรมเมอร์ ที่มุ่งเน้นไปที่ความรู้ในการใช้งาน Cloud ครับ โดยเป็นการเรียนออนไลน์ผ่าน Microsoft Teams ทุกวันเสาร์ 14:00–16:00 น. เป็นระยะเวลา 6 สัปดาห์ และทบทวนเนื้อหาย้อนหลังผ่าน Video ที่อัดไว้ครับ
ซึ่งเหตุผลที่เลือก AZ-900 เพราะว่ามีเนื้อหาที่เหมาะสมกับการเริ่มต้นใช้ Cloud ครับ ซึ่งเราสามารถนำความรู้ที่ได้นี้ไปต่อยอดกับ Cloud Provider เจ้าอื่นๆเช่น AWS หรือ GCP ครับ ไม่ได้จำกัดแค่การใช้งานบน Azure อย่างเดียว
ตัวอย่างเช่น
ผมอยากขออธิบายแนวทางการสอบวัดระดับ Microsoft Azure Certifications เพิ่มเติมน่ะครับ เผื่อเป็นแนวทางสำหรับท่านที่ต้องการต่อยอดใบ Certificate น่ะครับ ซึ่งมีด้วยกันอยู่ 4 ระดับครับ ดูรายละเอียดเพิ่มเติมได้ที่นี้
www.microsoft.com
2. Exam AZ-204 -> Azure Developer Associate ★★
3. Exam AZ-400 -> Azure DevOps Engineer Expert ★★★
คือบริการให้เช่าทรัพยากรบนคอมพิวเตอร์โดยที่เราจ่ายเงินค่าบริการเท่าที่ใช้ตามจริงเท่านั้น ตัวอย่างบริการก็เช่น
คือผู้ให้บริการที่มีมาตรฐานทั้ง 5 คุณสมบัตินี้ครับ (Five characteristics of cloud computing) เช่น Microsoft, Amazon และ Google
ศึกษารายละเอียดเพิ่มเติมได้ที่ลิ้งนี้น่ะครับ
www.saladpuk.com
Azure จริงจังกับเรื่องการของบริการที่ตอบรับกับ Compliance ต่างๆในระดับสากล ตัวอย่างเช่น GDPR ในสหภาพยุโรป, HIPAA, ISO และยังรวมไปถึง PDPA ของประเทศไทยด้วยน่ะครับ
ศึกษารายละเอียดเพิ่มเติมได้ที่
docs.microsoft.com
เป็นจุดเด่นของ Cloud Computing ที่ช่วยลดต้นทุนแต่ได้ผลลัพธ์ที่มากขึ้นในการดำเนินกิจการ โดยจะมีแผนการลงทุนสองรูปแบบที่เกี่ยวข้องได้แก่
Capital Expenditure (CapEx)
เป็นการลงทุนค่าใช้จ่ายโดยทั่วไปของ On-Premises Data Center ที่ต้องลงทุนล่วงหน้าไปกับ Physical Infrastructure ซึ่งจะมีค่าใช้จ่ายล่วงหน้าที่สูงมาก เพราะต้องลงทุนกับ Server, Storage, Network, Backup, Disaster Recovery, Data Center Infrastructure และยังต้องการผู้เชี่ยวชาญในการดูแลอีกด้วยครับ
ข้อดีของ CapEx คือทราบตัวเลขการลงทุนที่ชัดเจนว่าจะต้องจ่ายแค่ไหนและเมื่อไร เพราะสามารถวางแผนค่าใช้จ่ายได้ล่วงหน้าตั้งแต่เริ่มโครงการ หรือจะวางแผนตามรอบระยะเวลางบประมาณ
Operational Expenditure (OpEx)
เป็นการลงทุนค่าใช้จ่ายบน Cloud Computing ซึ่งเป็นการลงทุนกับสินค้าและบริการเท่าที่ได้ใช้งานและไม่มีค่าใช้จ่ายล่วงหน้าเหมือน CapEx
ค่าบริการมีได้หลายรูปแบบ เช่น เรียกเก็บตามจำนวนผู้ใช้งาน, ตามระยะเวลาที่ใช้งาน CPU, RAM, IOPS หรือตามปริมาณพื้นที่จัดเก็บข้อมูล รวมไปถึงการคิดค่าบริการตามจำนวน bandwidth ที่ใช้ทั้งนี้ เช่น backup traffic และ data recovery traffic
เราจำเป็นต้องหมั่นตรวจสอบการใช้บริการอยู่เสมอ เพื่อให้ทรัพยากรได้ถูกใช้งานอย่างเต็มที่ไม่สูญเปล่าเพื่อประหยัดต้นทุนให้ได้มากที่สุด
ข้อดีของ OpEx คือ ความสามารถในการปรับเปลี่ยนบริการให้สอดคล้องกับความต้องการที่เปลี่ยนแปลงทางธุรกิจที่มีความผันผวนได้เป็นอย่างดี สามารถทดลองใช้บริการบางอย่างเพื่อทดลองและสามารถยกเลิกได้เมื่อเสร็จสิ้น เพิ่มค่าใช้จ่ายเมื่อมีบริการที่ต้องการสูงขึ้น และลดได้เมื่อมีความต้องการที่ลดลง
คือรูปแบบการให้บริการบน Cloud Computing โดยจะขอพูดถึงเฉพาะรูปแบบที่ได้รับความนิยม 3 ประเภทน่ะครับ คือ IaaS, PaaS และ SaaS
On-Premiseเราจะต้องดูแลทุกอย่างเองทั้งหมด ทั้ง Hardware และ Software เช่น จะตั้งค่า Network ยังไง จะ Patch OS เมื่อไหร่ ติดตั้งโปรแกรมอะไรบ้าง ไฟดับ เน็ทหลุด harddisk เสีย ต่างๆ ซึ่งปัญหาพวกนี้เราต้องดูแลเองทั้งหมดเลย
Infrastructure as a Service (IaaS)เป็นรูปแบบการให้บริการพื้นฐานบน Cloud Cumputing เหมาะสำหรับสถานะการณ์ที่เราอยากจะได้เครื่องเซิฟเวอร์ไปใช้งาน แต่ให้ Cloud Provider เป็นผู้ดูแล Physical Infrastructure โดยเราจะเน้นไปที่ความยืดหยุ่นในการใช้งานที่มากที่สุด เช่นเราสามารถจัดการได้ว่าจะ Patch OS เมื่อไหร่ จะติดตั้งโปรแกรม หรือลง Antivirus โดย
เป็นรูปแบบที่ย้ายจาก On-Premises มาง่ายที่สุด (“Lift & Shift”)
ตัวอย่างบริการ: Container Service, Virtual Machines, Azure Storage Accounts
Platform as a Service (PaaS)เป็นบริการมุ่งเน้นไปที่การอำนวยความสะดวกให้ผู้ใช้บริการสามารถสร้างแอปพลิเคชันได้อย่างรวดเร็ว โดยไม่ต้องกังวลกับ Infrastructure เพราะเราเน้นดูแค่ โปรแกรมและข้อมูลที่ใช้ ส่วนอื่นๆทาง Azure จะเป็นคนดูแลให้ครับ เช่น Windows เราก็ไม่ต้องคอย update หรือ จัดการกับความปลอดภัยต่างๆด้วยตัวเราเอง
ตัวอย่างบริการ: Azure App Service, Azure SQL Database, Azure Functions
Software as a Service (SaaS)เป็นบริการที่มีไว้เพื่อเน้นให้ End Customer ใช้งานโปรแกรมที่อยู่บนคลาว์อย่างเดียวเท่านั้น ดังนั้นในระดับนี้เราจะไม่ต้องไปยุ่งอะไรกับการตั้งค่าเลย
ตัวอย่างบริการ: Dynamics 365, Outlook, Office 365
ถ้าใครสนใจเนื้อหาประเภทของ Cloud Services ต่างๆ ผมขอแนะนำบทความจากสลัดผักน่ะครับ
www.saladpuk.com
Public Cloudเป็นรูปแบบบริการที่นิยมมากที่สุด เพราะลูกค้าไม่ต้องดูแล Hardware เอง ให้ Cloud Provider เป็นผู้ดูแลและปรับปรุงให้ระบบทันสมัยอยู่ตลอดเวลา เข้าถึงด้วย Internet ช่วยลดต้นทุนโดยการแชร์ทรัพยากรกับผู้ใช้บริการรายอื่นๆ ด้วย
ข้อดีของ Public Cloud คือ จ่ายค่าบริการตามการใช้งานจริง ไม่มี CapEx (ค่าใช้จ่ายล่วงหน้า), เราไม่ต้องรับภาระในการบำรุงรักษาหรืออัปเดตฮาร์ดแวร์ ทำให้ได้ ความสามารถในการปรับขนาดของบริการได้อย่างรวดเร็ว, ไม่จำเป็นต้องมีความรู้ในการติดตั้งและใช้งานมากนัก
Private Cloudเราติดตั้ง Cloud Environment ลงใน Data-center ของเราเอง จัดการกำหนดสิทธิการเข้าถึงทรัพยากรเพื่อใช้ภายในองค์กรของเรา โดยเราต้องรับภาระเพิ่มเติมในการการจัดซื้อและบำรุงรักษา Hardware และ Software
ข้อดีของ Private Cloud คือ รองรับการทำงานในองกรณ์ได้เต็มที่, เชื่อมต่อด้วย Network ภายใน, ควบคุมจัดการทรัพยากรได้มาก, ทรัพยากรไม่ได้ถูกแบ่งกับบุคคลหรือองค์กรภายนอก
Hybrid Cloudเป็นรูปแบบผสมระหว่าง Public และ Private Cloud เพื่อปรับใช้ทรัพยากรที่ตอบโจทย์ขององค์กร
เช่น ในกรณีที่เราไม่สามารถนำข้อมูลบางอย่างขึ้น Public Cloud ได้เนื่องด้วยข้อจำกัดด้านกฎหมาย เช่น เรามีข้อมูลทางการแพทย์ที่ไม่สามารถเปิดเผยต่อสาธารณะได้ทำให้เราจำต้องเก็บไว้ในฐานข้อมูลส่วนตัวเท่านั้น เราจึง Host Web Application บน Public Cloud แต่เชื่อมต่อมายัง Database ที่มีความปลอดภัยสูงที่อยู่ใน Private Cloud ของเรา
ข้อดีของ Hybrid Cloud คือ มีทางเลือกในการควบคุมต้นทุน (Economies of Scale) โดยเลือกใช้ Public Cloud ในกรณีที่ต้นทุนที่ต่ำกว่า หรือใช้ Private Cloud ในกรณีที่ Public Cloud มีต้นทุนที่สูงกว่า, รวมไปถึงการควบคุมด้าน Security, Compliance และ รองรับ Legacy Applications ขององค์กร
Digital Skill — Azure Fundamentals (ภาษาไทย)
ExamTopics — AZ-900 Exam Actual Questions
Facebook — Data TH.com — Data Science ชิลชิล (ภาษาไทย)
Github — Microsoft Certified Azure Fundamentals (ภาษาไทย)
Medium — Azure AZ-900 Exam Preparation Guide: How to pass in 3 days
Medium — วีธีลงทะเบียนสอบ AZ-900 Online ที่บ้านด้วย Azure Exam Voucher
Medium — AZ-900 รีวิวแนวข้อสอบและวิธีลงสอบที่ศูนย์สอบ
Medium — AZ-900 สรุปละเอียดสุดๆ
Microsoft Learn-Azure Fundamentals
Udemy — Microsoft Azure — Beginner’s Guide + AZ-900 (มีค่าใช้จ่าย)
WhizLabs — AZ-900 (มีค่าใช้จ่าย)
Workshop เล็กๆจาก Microsoft สำหรับ AZ-900 ครับผม
เนื้อหาในบทความนี้เป็นการปูทางความรู้ในด้าน Cloud Computing ที่ช่วยให้เราเข้าใจรูปแบบและบริการของ Cloud มากยิ่งขึ้นน่ะครับ เพื่อช่วยให้ผู้อ่านที่กำลังสนใจจะสอบ Microsoft Certified: Azure Fundamentals (AZ-900) ได้เข้าใจเนื้อหาเพื่อเตรียมตัวในการสอบมาก
ในบทถัดไปจะเป็นรายละเอียดที่เกี่ยวกับ Azure มากขึ้นน่ะครับ
ขอขอบคุณ คุณ อิศเรศ ประจิตต์มุทิตา , ทีมงาน AiPEN Studio ทุกๆท่านรวมถึงเนื้อหาบทเรียนจากอาจารย์ วิสิทธิ์ ทองภู่ มากๆนะครับ สำหรับโครงการดีๆแบบนี้เพื่อส่งเสริมวิชาชีพโปรแกรมเมอร์ในประเทศไทยครับ
ขอบคุณผู้อ่านทุกๆท่านมากๆน่ะครับ ^^
นายป้องกัน
https://www.tt-ss.net/
31 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
31 claps
31 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-lock-azure-resources-to-prevent-changes-45be494b135c?source=search_post---------195,"There are currently no responses for this story.
Be the first to respond.
Preventing Azure Resource Deletion or Unexpected Changes using Locks.
Resource Manager Locks provide a way for administrators to lock down Azure resources to prevent deletion or changing of a resource. These locks sit outside of the Role Based Access Controls (RBAC) hierarchy and, when applied, will place restrictions on the resource for all…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/battle-for-the-king-of-public-cloud-aws-vs-azure-9c5505dabb1e?source=search_post---------196,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ryan Gleason
Jun 17, 2020·6 min read
The public cloud is taking over whether you like it or not. And what’s not to like about it?
Cloud computing has made it incredibly easy to manage all of our computer system resources. It has provided…
"
https://medium.com/syntropynet/microsoft-adds-noia-platform-to-azure-marketplace-4dd3bf961e76?source=search_post---------197,"There are currently no responses for this story.
Be the first to respond.
As we approach launch, our team is working hard to secure placements on every major cloud marketplace. By adding our solution to these marketplaces, existing cloud users can start using Syntropy Stack in a few clicks.
We’ve had a collaborative relationship with Microsoft for some time. Earlier this year, they added us to their FastTrack program, granting us direct access to Azure engineers to collaborate on development and deployment of the Syntropy.
Now, we’re proud to announce that Microsoft is adding our technology to the Azure marketplace, making Syntropy accessible to millions of cloud users worldwide. Users can now launch an instance with our agent installed, and we’ll soon be distributing API keys and tutorials to guide them through initial deployment and usage.
“With the Syntropy Sta in the Azure marketplace, customers worldwide will benefit from Syntropy’s next-generation technology and solutions, accessed and used in just a few clicks.”
— Tibbs Pereira, Principal PM Manager, Microsoft Azure FastTrack
The Azure marketplace is a hand-picked collection of certified apps and services that can be deployed seamlessly to any client’s existing infrastructure. Adding our tech to the marketplace will accelerate adoption of our network.
“Microsoft Azure is one of the biggest and most respected cloud providers on the market. By having our technology certified by Microsoft and listed directly on the marketplace, driving adoption becomes a lot less burdensome.”
— Jonas Simanavicius, CTO at Syntropy
Our ultimate goal is to make our technology stack as seamless as possible, from initial adoption all the way through billing. We’ve had a very productive relationship with Microsoft, and we look forward to working with them further to match our technology with their global user base.
Please join our Telegram Channel to learn more.
Rethinking the Internet for Everyone.
478 
478 claps
478 
Syntropy is an open project providing next-generation connectivity technology for the Internet, powered by $NOIA ⚡️
Written by
Co-founder of NOIA Network
Syntropy is an open project providing next-generation connectivity technology for the Internet, powered by $NOIA ⚡️
"
https://blog.jeremylikness.com/azure-event-grid-glue-for-the-internet-e770d94cc29?source=search_post---------198,
https://medium.com/cognitiveclouds/aws-vs-microsoft-azure-vs-google-cloud-cbdb03e0281e?source=search_post---------199,"There are currently no responses for this story.
Be the first to respond.
While collectively, these three cloud providers dominate the space, their approach to cloud computing is dictated strongly by their background. Amazon has immense know-how when it comes to collating and aggregating massive amounts of data, Google’s heritage stems from an analytical background, and Microsoft’s strength comes from computing. This comparison will underline their strengths and weaknesses. As we proceed, tie these activities to your business objectives to find the right fit. Also, it’s important to note that your best fit may not turn out to be a single cloud provider.
Your reasoning for picking one service over another will differ from another user. However, there are particular aspects of competing clouds that offer benefits in certain circumstances. That can always be compared. So, let’s advocate for and against each now.
AWS continues to lead the way, in very broad terms, regarding maturity and offering the widest range of functionality. However, the gap is certainly closing. Its expansive list of services and tools, along with its enterprise-friendly features make it an attractive proposition for big organizations. While its massive, continuously growing infrastructure offers economies of scale that allow aggressive price cuts.
Now it appears, Microsoft has begun to bridge that gap between the two. With its plans to strengthen ties with its on-premise software and ongoing investment in building out the Azure cloud platform, it will continue to bridge that gap. Microsoft Azure will continue to be a strong proposition for organizations already heavily invested heavily in Microsoft regarding technology and developer skills, of which there are undoubtedly many. With Google offering a slightly different proposition, it has made inroads with certain users. Even so, to become a viable enterprise choice, it has a lot of work to do. It might carve a niche for itself in advanced use cases based on machine learning and big data, but whether Google is willing to relinquish the key IaaS market to its largest competitors is another subject.
Originally published on Product Insights Blog from CognitiveClouds: Top Web App Development Company
Tips, advice and insights from our digital product…
43 
1
43 claps
43 
1
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Tips, advice and insights from our digital product strategy, design and development experts.
Written by
See every interaction with a customer — across all digital channels — & quickly determine how to delight your audience with personalization and recommendations.
Tips, advice and insights from our digital product strategy, design and development experts.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/asp-net-core-docker-aplica%C3%A7%C3%B5es-na-nuvem-com-azure-container-instances-49a61db2d79f?source=search_post---------200,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 25, 2017·5 min read
Em posts anteriores abordei o uso de containers Docker em conjunto com o ASP.NET Core, empregando para isto serviços como Docker Hub, Azure Web App on Linux e Azure Container Registry. Caso deseje saber mais a respeito acesse os links a seguir:
ASP.NET Core: APIs REST na nuvem com Docker e Azure Web App
ASP.NET Core 2.0: deployment na nuvem com Docker, Azure Container Registry e Azure Web App on Linux
A Microsoft anunciou em Julho/2017 um novo serviço chamado Azure Container Instances. Ainda em modo Preview, esta é uma alternativa que simplifica a utilização de containers Docker na nuvem. Embora dispense o uso de orquestradores (abordagem comum em cenários complexos, nos quais questões como um uso mais intensivo e escalabilidade são preocupações centrais), o Azure Container Instances pode mesmo assim ser integrado a soluções como o Kubernetes.
As próximas seções demonstram como utilizar este serviço, partindo dos exemplos apresentados nos posts indicados no início deste artigo.
Este primeiro exemplo faz uso de uma API REST para conversão de alturas em pés para o equivalente a metros. Para maiores detalhes sobre a implementação desse projeto baseado no ASP.NET Core 1.1 acesse este link.
A imagem pública que será empregada está disponível no Docker Hub como renatogroffe/apialturas:
No portal do Azure será criado um novo recurso baseado no serviço Azure Container Instances:
Acionar então o botão Criar:
Informar no formulário de criação:
Na próxima tela será possível definir o tipo do sistema operacional (Linux em OS Type), número de cores/núcleos, memória, o uso ou não de um IP público (Yes em Public IP address) e a porta (normalmente 80 em se tratando de aplicações Web):
Em Resumo clicar no botão OK, confirmando a criação de um novo container:
Após alguns segundos o item apialturascontainer-api1 constará na lista de recursos disponíveis:
Ao acessar este item aparecerá então o recurso apialturarcontainer, com o IP público para acesso a este container também destacado em vermelho:
Efetuando um teste com a URL http://13.90.204.247/api/conversoralturas/pesmetros/1000 será retornado o valor da conversão de 1000 pés para o equivalente em metros (304,8):
OBSERVAÇÃO: Além do portal de Azure, novos recursos baseados no serviço Azure Container Instances podem ser criados via Azure CLI 2.0. Para saber mais a respeito consulte este link.
Para o exemplo descrito nesta seção será utilizada uma imagem privada chamada renatogroffe.azurecr.io/sitedadosnasa, a qual se encontra armazenada em um recurso do Azure Container Registry. Trata-se de um site implementado com o ASP.NET Core 2.0 e que acessa uma API gratuita de consulta a imagens da NASA (Agência Espacial Norte-Americana):
As credenciais necessárias para utilização desta imagem estão na seção Access keys do Container Registry:
OBSERVAÇÃO: para maiores informações sobre este site de exemplo e o Azure Container Registry acesse este link.
Preencher no formulário de criação:
OBSERVAÇÃO: Os demais procedimentos de criação são idênticos ao exemplo descrito na seção anterior.
Na próxima imagem é possível observar o item dadosnasacontainer-dad4 em destaque:
Ao acessar este recurso aparecerão o IP público gerado e o container dadosnasacontainer:
Um teste com a URL http://40.76.21.242/ produzirá um resultado similar ao da imagem a seguir:
ASP.NET Core - Documentation
Azure Container Instances - Documentation
Azure Container Registry - Documentation
Conteúdos gratuitos sobre ASP.NET Core, .NET Core e C# 7.0
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
96 
96 claps
96 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/change-data-capture-with-azure-postgresql-and-kafka-4598dbf0b57a?source=search_post---------201,"Change Data Capture (CDC) can be used to track row-level changes in database tables in response to create, update and delete operations. It is a powerful technique, but useful only when there is a way to leverage these events and make them available to other services.
Using Apache Kafka, it is possible to convert traditional batched ETL processes into real-time, streaming mode. You can do-it-yourself (DIY) and write good old Kafka producer/consumer using a client SDK of your choice. But why would you do that when you’ve Kafka Connect and it’s suite of ready-to-use connectors?
Once you opt for Kafka Connect, you have a couple of options. One is the JDBC connector which basically polls the target database table(s) to get the information. There is a better (albeit, a little more complex) way based on change data capture. Enter Debezium, which is a distributed platform that builds on top of Change Data Capture features available in different databases. It provides a set of Kafka Connect connectors which tap into row-level changes in database table(s) and convert them into event streams that are sent to Apache Kafka. Once the change log events are in Kafka, they will be available to all the downstream applications.
Here is a high-level overview of the use-case presented in this post. It has been kept simplified for demonstration purposes.
Data related to Orders is stored in the PostgreSQL database and contains information such as order ID, customer ID, city, transaction amount. time etc. This data is picked up the Debezium connector for PostgreSQL and sent to a Kafka topic. Once the data is in Kafka, another (sink) connector sends them to Azure Data Explorer allow or further querying and analysis.
The individual components used in the end to end solution are as follows:
Data pipelines can be pretty complex! This blog post provides a simplified example where a PostgreSQL database will be used as the source of data and a Big Data analytics engine acts as the final destination (sink). Both these components run in Azure: Azure Database for PostgreSQL (the Source) is a relational database service based on the open-source Postgres database engine and Azure Data Explorer (the Sink) is a fast and scalable data exploration service that lets you collect, store, and analyze large volumes of data from any diverse sources, such as websites, applications, IoT devices, and more.
Although Azure PostgreSQL DB has been used in this blog, the instructions should work for any Postgres database. So feel free to use alternate options if you’d like!
The code and configuration associated with this blog post is available in this GitHub repository
Apache Kafka along with Kafka Connect acts as a scalable platform for streaming data pipeline — the key components here are the source and sink connectors.
The Debezium connector for PostgreSQL captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database, generates data change event records and streams them to Kafka topics. Behind the scenes, it uses a combination of a Postgres output plugin (e.g. wal2json, pgoutput etc.) and the (Java) connector itself reads the changes produced by the output plug-in using the PostgreSQL’s streaming replication protocol and the JDBC driver.
The Azure Data Explorer sink connector picks up data from the configured Kafka topic, batches and sends them to Azure Data Explorer where they are queued up ingestion and eventually written to a table in Azure Data Explorer. The connector leverages the Java SDK for Azure Data Explorer.
Most of the components (except Azure Data Explorer and Azure PostgreSQL DB) run as Docker containers (using Docker Compose) — Kafka (and Zookeeper), Kafka Connect workers and the data generator application. Having said that, the instructions would work with any Kafka cluster and Kafka Connect workers, provided all the components are configured to access and communicate with each other as required. For example, you could have a Kafka cluster on Azure HD Insight or Confluent Cloud on Azure Marketplace.
Check out these hands-on labs if you’re interested in these scenarios
Here is a breakdown of the components and their service definitions — you can refer to the complete docker-compose file in the GitHub repo
The Kafka and Zookeeper run using the debezium images — they just work and are great for iterative development with quick feedback loop, demos etc.
The Kafka Connect source and sink connectors run as separate containers, just to make it easier for you to understand and reason about them — it is possible to run both the connectors in a single container as well.
Notice that, while the PostgreSQL connector is built into debezium/connect image, the Azure Data Explorer connector is setup using custom image. The Dockerfile is quite compact:
Finally, the orders-gen service just Go application to seed random orders data into PostgreSQL. You can refer to the Dockerfile in the GitHub repo
Hopefully, by now you have a reasonable understanding of architecture and the components involved. Before diving into the practical aspects, you need take care of a few things.
Finally, clone this GitHub repo:
To begin with, let’s make sure you have setup and configured Azure Data Explorer and PostgreSQL database.
During the ingestion process, Azure Data Explorer attempts to optimize for throughput by batching small ingress data chunks together as they await ingestion — the IngestionBatching policy can be used to fine tune this process. Optionally, for the purposes of this demo, you can update the policy as such:
Refer to the IngestionBatching policy command reference for details
3. Create a Service Principal in order for the connector to authenticate and connect to Azure Data Explorer service. If you want to use the Azure Portal to do this, please refer to How to: Use the portal to create an Azure AD application and service principal that can access resources. The below example makes use of Azure CLI az ad sp create-for-rbac command. For example, to create a service principal with the name adx-sp:
You will get a JSON response:
Please note down the appId, password and tenant as you will be using them in subsequent steps
4. Add permissions to your database
Provide appropriate role to the Service principal you just created. To assign the admin role, follow this guide to use the Azure portal or use the following command in your Data Explorer cluster
You can setup PostgreSQL on Azure using a variety of options including, the Azure Portal, Azure CLI, Azure PowerShell, ARM template. Once you’ve done that, you can easily connect to the database using you favourite programming language such as Java, .NET, Node.js, Python, Go etc.
Although the above references are for Single Server deployment mode, please note that Hyperscale (Citus) is another deployment mode you can use for “workloads that are approaching — or already exceed — 100 GB of data.”
Please ensure that you keep the following PostgreSQL related information handy since you will need them to configure the Debezium Connector in the subsequent sections — database hostname (and port), username, password
For the end-to-end solution to work as expected, we need to:
If you’re using Azure DB for PostgreSQL, create a firewall rule using az postgres server firewall-rule create command to whitelist your host. Since we’re running Kafka Connect in Docker locally, simply navigate to the Azure portal (Connection security section of my PostrgreSQL instance) and choose Add current client IP address to make sure that your local IP is added to the firewall rule as such:
To change the replication mode for Azure DB for PostgreSQL, you can use the az postgres server configuration command:
.. or use the Replication menu of your PostgreSQL instance in the Azure Portal:
After updating the configuration, you will need to re-start the server which you can do using the CLI (az postgres server restart) or the portal.
Once the database is up and running, create the table. I have used psql CLI in this example, but feel free to use any other tool. For example, to connect to your PostgreSQL database on Azure over SSL (you will be prompted for the password):
Use the below SQL to create the table:
The purchase_time captures the time when the purchase was executed, but it uses VARCHAR instead of a TIMESTAMP type (ideally) to reduce the overall complexity. This is because of the way Debezium Postgres connector treats TIMESTAMP data type (and rightly so!)
Over the course of the next few sections, you will setup the source (PostgreSQL), sink (Azure Data Explorer) connectors and validate the end to end pipeline.
Starting up our local environment is very easy, thanks to Docker Compose — all we need is a single command:
This will build (and start) the order generator application container along with Kafka, Zookeeper and Kafka Connect workers.
It might take a while to download and start the containers: this is just a one time process.
To confirm whether all the containers have started:
The orders generator app will start inserting random order events to the orders_info table in PostgreSQL. At this point you can also do quick sanity check to confirm that the order information is being persisted - I have used psql in the example below:
This will give you the five most recent orders:
To stream the orders data to Kafka, we need to configure and start an instance of the Debezium PostgreSQL source connector.
Copy the JSON contents below to a file (you can name it pg-source-config.json). Please ensure that you update the following attributes with the values corresponding to your PostgreSQL instance: database.hostname, database.user, database.password.
At the time of writing, Debezium supports the following plugins: decoderbufs, wal2json, wal2json_rds, wal2json_streaming, wal2json_rds_streaming and pgoutput. I have used wal2json in this example, and it's supported on Azure as well.
To start the connector, simply use the Kafka Connect REST endpoint to submit the configuration.
Notice that port for the REST endpoint is 9090 - this is per service port mapping defined in docker-compose.yaml
Let’s peek into the Kafka topic and take a look at the change data capture events produced by the source connector.
You will be dropped into a shell (inside the container). Execute the below command to consume the change data events from Kafka:
Note that the topic name myserver.retail.orders_info is as a result of the convention used by the Debezium connector
Each event in topic is corresponding to a specific order. It is in a JSON format that looks like what’s depicted below. Please note that the payload also contains the entire schema which has been removed for brevity.
So far, we have the first half of our pipeline. Let’s work on the second part!
Copy the JSON contents below to a file (you can name it adx-sink-config.json). Replace the values for the following attributes as per your Azure Data Explorer setup - aad.auth.authority, aad.auth.appid, aad.auth.appkey, kusto.tables.topics.mapping (the database name) and kusto.url
Notice that Kafka Connect Single Message Transformation (SMT) have been used here — this is the ExtractNewRecordState transformation that Debezium provides. You can read up on it in the documentation
It removes the schema and other parts from the JSON payload and keeps it down to only what's required. In this case, all we are looking for the order info from the after attribute (in the payload). For e.g.
You could model this differently of course (apply transformation in the source connector itself), but there are a couple of benefits to this approach:
To install the connector, just use the Kafka Connect REST endpoint like before:
Notice that port for the REST endpoint is 8080 - this is per service port mapping defined in docker-compose.yaml
The connector should spin into action, authenticate to Azure Data Explorer and start batching ingestion processes.
Note that flush.size.bytes and flush.interval.ms are used to regulate the batching process. Please refer to the connector documentation for details on the individual properties.
Since the flush configuration for the connector and the batching policy for the Orders table in Azure Data Explorer is pretty aggressive (for demonstration purposes), you should see data flowing into Data Explorer quickly.
You can query the Orders table in Data Explorer to slice and dice the data. Here are a few simple queries to start with.
Get details for orders from New York city;
Get only the purchase amount and time for orders from New York city sorted by amount
Find out the average sales per city and represent that as a column chart:
The total purchase amount per city, represented as a pie chart:
Number of orders per city, represented as a line chart:
How do purchases vary over a day?
How does it vary over a day across different cities?
Learn how to visualize data with Azure Data Explorer dashboards
To stop the containers, you can:
To delete the Azure Data Explorer cluster/database, use az cluster delete or az kusto database delete. For PostgreSQL, simply use az postgres server delete
Kafka Connect helps you build scalable data pipelines without having to write custom plumbing code. You mostly need to setup, configure and of course operator the connectors. Remember that Kafka Connect worker instances are just JVM processes and depending on your scale and requirements you can use choose to operate them using Azure Kubernetes Service. Since Kafka Connect instances are stateless entities, you’ve a lot of freedom in terms of the topology and sizing of your cluster workloads!
If you want to explore further, I would recommend
ITNEXT is a platform for IT developers & software engineers…
80 
80 claps
80 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://faun.pub/microservices-in-azure-an-introduction-ba2a0fcc0385?source=search_post---------202,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is a powerful cloud computing platform designed to make big businesses agile and infinitely scalable for a fraction of the cost.
While often overlooked, Azure is used by 90% of Fortune 500 companies to build low latency, high data-volume apps. Between Azure clients like Linkedin, News Corp, and Wikipedia, you probably use an Azure app at least once per day without knowing it. The secret to these popular apps lies in Azure’s microservices support, which allows apps to be both resilient and reactive.
Today, we’ll give you a quick introduction to Azure and its impressive microservices functionalities.
Here’s what we’ll cover today:
Microsoft Azure, or simply Azure, is a cloud computing platform created by Microsoft. It was launched in 2010 to compete with the Google Cloud Platform and Amazon’s AWS.
Azure includes all the standard cloud services like Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS), which each offer a different scale of cloud implementation to fit each company’s unique needs.
While relatively late to the cloud platform scene, Azure offers unique functionalities:
These factors combine to make Azure a great cloud platform choice for existing mid to large-scale companies looking to get the most out of their cloud service and embrace modern DevOps techniques.
One way companies can get the most out of their cloud is to design web applications with event-driven architecture (EDA).
An event-driven architecture uses events to trigger and communicate between decoupled services. Components send events anytime a state is updated within their scope such as a user adding an item to their cart or posting a comment on a post.
This structure makes each component responsible for notifying other areas of the system about state changes. Event-driven architecture is the opposite of traditional request-driven architecture, in which the state of a component is only revealed when another component asks for its current state.
Event-driven architecture is common in systems built with microservices. Microservices architecture is a way to break down an application into separate, independent components that make it easier to maintain and scale.
In Azure, you can either use Microsoft’s Azure Service Fabric or the open-source Azure Kubernetes Service (AKS) for building microservice applications as well as handling deployment lifecycle and container orchestration.
Event-driven architecture synergizes with a microservice architecture to further increase scalability because it allows the separate components to communicate reactively regardless of their size or separation.
Some benefits of an event-driven architecture over traditional monolithic applications are as follows:
Event-driven architecture is becoming a popular design pattern because of how perfectly it meets modern needs. However, it can be difficult to get an event-driven system started because all event-handling systems must be built upfront. It’s best to only use event-driven architecture when its unique strengths fit the product’s design.
Use event-driven architectures when:
In Azure, event-driven systems are best implemented with three components: an event grid (EG), one or more FIFO queue, and a collection of Azure Functions
The event grid is the primary event processor and essentially acts as a post office that intakes, evaluates, manipulates, and redistributes any events traveling from event producer to handler. Event grids have a few major parts:
Event grids can be configured very differently based on what type/frequency of event should be allowed through. All event grids will at least have a dead-letter queue and some kind of event filter.
The dead-letter queue is a FIFO queue data structure that stores the events which did not successfully reach their handler. Once stored, the events can be resent and analyzed by developers to find possible bugs.
The event filter is a set of criteria that the Topic uses to decide which events move on to their handler and which will be blocked. It can also manipulate the event to a different type if the desired handler can only handle certain types.
If an event filter is working correctly, only actionable and applicable events will be delivered to a particular handler.
Building applications with EDAs in Azure is a great way to learn the platform. To tackle this project yourself, you’ll need to complete the following 4 steps.
1. Set up an Azure Account
You’ll need an Azure account to create an Azure environment and create a DevOps project workspace to practice in. You can get a free trial of Azure on Microsoft’s official site.
2. Build the foundational structure
Next, you’ll need to create resource groups, an automation account, and a database.
Resource groups are containers that we can use to gather all our resources in one place. They allow you to organize how resources are managed and grouped based on your needs.
Azure accounts act as containers for your source files. Automations are a container for all your runbook, runbook executions (jobs), and the assets that your runbooks depend on.
A runbook is a collection of routine operations that help you evaluate the system.
Finally, you can create a database using the Azure-supported mySQL database software. Once you install Az.mySQL, you can enter New-AzMySqlServer into your command-line or PowerShell to initialize the server.
3. Create Event Storage
Your application needs a way to store the events it can’t complete right away. Queues are a great option because they store events in the order they’re received and ensure events aren’t skipped.
A 3 queue system is very effective for event-driven applications:
4. Add Azure functions
These functions act as an intermediary between your runbook and Event Grid. They also perform event validation and process messages.
You’ll at least need the following:
This is just the first step on your journey to learn Azure. While tricky to get started, learning Azure will set you up for success when you learn other cloud platforms.
As you continue your journey, check out these intermediate topics:
Happy learning!
👋 Join FAUN today and receive similar stories each week in your inbox! ️ Get your weekly dose of the must-read tech stories, news, and tutorials.
Follow us on Twitter 🐦 and Facebook 👥 and Instagram 📷 and join our Facebook and Linkedin Groups 💬
The Must-Read Publication for Creative Developers & DevOps Enthusiasts
84 
84 claps
84 
Written by
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Written by
Coding is like skateboarding: you can’t learn new skills just by watching someone else. Master in-demand coding skills through Educative’s interactive courses.
The Must-Read Publication for Creative Developers & DevOps Enthusiasts. Medium’s largest DevOps publication.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mauridb/automatic-import-of-csv-data-using-azure-functions-and-azure-sql-63e1070963cf?source=search_post---------203,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
May 30, 2017·2 min read
Lately .csv (or related format, like .tsv) became very popular again, and so it’s quite common to be asked to import data contained in one or more .csv file into the database you application is using, so that data contained therein could be used by the application itself.
In the era of the Cloud, what can we do to simplify such popular requirement so that, for example, the user can just drop a file into a folder, and have it automagically imported into Azure SQL? (With small changes the same principle could be used for any database)
Azure Functions (that I love more and more every day) are the answer. Azure functions support the concept of trigger, which is an event that (as the name implies) when happens, will have an Azure Function executed, passing to that function information about the event that triggered it.
One of the available trigger is the Blob Trigger. So 40% of the work that is needed to implement what the title of this article says is already done.
The other 40% is provided by a new feature that has been added to the BULK INSERT command in Azure SQL: that ability to use a Blob Storage as a source.
The remaining 20% is the code that needs to be written to “glue” these two functionalities together. And this is the beauty of Azure Functions! Out of 100% of the code you would have had to write, you actually have to care only for 20% of it!
The full solution, with detailed instruction on how to set it up using, when possible, the new Azure CLI 2.0, is here:
github.com
Each time a file will be saved into the Azure Blob Store’s “csv” folder, within a couple of seconds, if the format is the expected one, data will be available in Azure SQL for you to be used as you wish. Yeah, is that simple and easy, really!
Enjoy!
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
54 
6
54 
54 
6
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://posts.specterops.io/azure-privilege-escalation-via-service-principal-abuse-210ae2be2a5?source=search_post---------204,"On-prem Active Directory is here to stay, and so is Azure Active Directory. While these products have similar names, they in fact work very differently from one another. There are hints of vanilla AD’s discretionary access control model in Azure, but permissions in Azure are mostly controlled through granted roles. Regardless of these differences, configuration-based attack primitives are alive and well in Azure, and researchers discover new attack primitives in Azure all the time.
In this blog post, I’ll explain how a particular kind of attack path can emerge in Azure based on Azure’s RBAC system — an attack path we have seen in the vast majority of Azure tenants we’ve gotten access to. I’ll show you how the attack primitive works, how to protect yourself against it, and offer my own commentary on the opportunity this presents to Microsoft.
This work is not new — in fact, Microsoft’s own documentation describes these attack paths. Additionally, some great researchers have discussed this attack primitive at length:
Dirk-jan Mollema wrote about abusing the Application Administrator role to add new secrets for service principals and escalate privileges here: https://dirkjanm.io/azure-ad-privilege-escalation-application-admin/
Karl Fosaaen spoke about Azure privilege escalation techniques in this talk: https://www.netspi.com/webinars/lunch-learn-webinar-series/adventures-in-azure-privilege-escalation/
Emilian Cebuc and Christian Philipov describe abusing service principals to escalate in an Azure tenant in this talk: https://www.youtube.com/watch?v=QwVApszlIdY
Before we talk about the Service Principal-based privilege escalation, I want to describe Azure’s built-in attack path prevention system. Azure Active Directory has a built-in system to protect against the emergence of attack paths, particularly around password reset privileges. When looking at the documentation for administrator roles that provide password reset privileges, you will often see wording like this:
Source: https://docs.microsoft.com/en-us/azure/active-directory/roles/permissions-reference#helpdesk-administrator
In particular, note the highlighted text: “Whether a Helpdesk Administrator can reset a user’s password and invalidate refresh tokens depends on the role the user is assigned.”
This, to me, is confusing. So I set out to understand all the different possibilities for password reset privileges in Azure, and wound up creating this table:
This brings a little more clarity, but look at what happens when we model these permissions using a graph:
This is where the brilliance of this system finally becomes clear. Looking at the top row, we can see that Global Admins and Privileged Authentication Admins can reset each others’, plus all other users’ passwords, but the opposite is not true. In other words, only GA and PAA users can reset a GA password. This is part of the back-end Azure system and something that Azure admins have no control of.
I love this system because it provides a highly effective, non-configurable, frictionless safety rail for admins that prevents the emergence of attack paths that include resetting a Global Admin’s password. Azure admins can safely dole out the “Password Admin” role without worrying about the safety of their Global Admins — at least within the context of this particular attack primitive.
When you create or register an application in your Azure tenant, an application object is created with a unique application (client) ID:
Here, the app “MyCoolApp” has a unique identifier starting with d6f118bc. Below that, you can see the ID of my tenant as well. This app object “resides” within my tenant. Let’s start to think of this in terms of a graph and see how the attack path emerges:
Because the app “resides” in my tenant, anyone with control of my tenant has control of the app. Let’s model this with showing how my own Global Admin user has control of the tenant:
Nothing scary here: my user is a global admin against the tenant, the tenant contains the app, therefore the “path” here is that I have control of the app. Now let’s extend this model to include service principals.
Azure apps needing to authenticate to the tenant to perform some action do so using an object called a Service Principal. Service Principals work kind of like users — you authenticate to the tenant with a “username” (object id) and a “password” (a certificate or secret). You can see the object id of the app’s service principal in this screenshot:
Let’s go ahead and add that to our graph model:
Again, nothing surprising here: my Global Admin user has control of everything here.
Service Principals can have admin roles in Azure just like users. For example, we have seen several Azure environments where at least one Service Principal has the Privileged Role Admin role. Let’s recreate that in my own tenant:
And let’s extend the graph model to include this:
Again, nothing scary: an attack path from a Global Admin to the PRA role isn’t a problem. Let’s bring another user into the mix: Alice App Admin. Like the user’s name suggests, we will grant this user the “Application Administrator” role in my tenant:
Let’s also add this user and their role assignment into our graph model:
And… there it is. Our attack path has emerged, connecting the Alice App Admin user to the PRA role. The attack path works like this:
Here’s a video of the attack path in action, escalating from “Application Administrator” to “Global Administrator”:
As discussed earlier, Azure has built-in mechanisms to prevent privilege escalation through role-based attack paths. Only those users with Global Admin or the Privileged Authentication Administrator role can reset a Global Admin’s password. This simple, brilliant mechanism provides a highly effective safety rail to protect Azure admins from unknowingly introducing attack paths into their tenants. But this mechanism could go further.
I believe the best possible prevention against the privilege escalation technique described in this blog post is for Microsoft themselves to extend this mechanism to cover service principals as well. Just as the system provides built-in protections for Global Admin users, Azure could (perhaps should) also provide built-in protection for Global Admin service principals. I don’t know the inner-workings of this system and whether this extension is even technically feasible, but I believe Microsoft can completely eliminate this attack primitive by extending the system to cover service principals, creating an even more secure platform for all of their customers.
In the meantime, Azure administrators should audit roles held by service principals, and determine the exposure of those service principals to the rest of their identities.
Azure admins can prevent this attack path by auditing roles held by service principals and comparing those roles to the other identities with control of apps. The most dangerous role in Azure is “Global Admin”, so let’s start there. Open the Azure portal, navigate to your tenant, then click “Roles and Administrators”:
Scroll down to and click on “Global Administrator”. Here, you will see the list of identities with this role currently activated. In the “type” column, look for “ServicePrincipal” entries:
There are two service principals in my tenant with the Global Admin role: “CanYouAddASecretToMe” and “ThisAppHasGlobalAdminRole”. These are the names of the service principals, and they are also the names of the apps associated with these service principals.
This is your first prevention opportunity: determine whether these apps actually need Global Admin rights in order to function. If these are third party apps, work with your vendor to get this level of privilege reduced to only the level required for the application to function.
If these roles must persist, then your next step is to look at the identities in your tenant with the tenant-level roles that grant a principal the ability to abuse service principals. Those roles are:
Additionally, these legacy/hidden roles can be abused to take over service principals:
Let’s audit the “Application Administrator” role by navigating to our tenant, clicking “Roles and Administrators”, then click on the Application Administrator role:
Here we see all users and service principals with this role currently active. This is your second opportunity for prevention: do all these users need the ability to manage credentials for all service accounts in your tenant?
We also need to check the app-level roles and per-app owners. Earlier we saw that the service principal, “ThisAppHasGlobalAdminRole”, was currently a Global Admin. Navigate to your tenant, click on “App registrations’’, click on “All Applications”, then find the app associated with that Service Principal:
First, click on “Owners”, which will show you all principals that own the app (there can be more than one):
Do you trust Matt Nelson to add a secret to an app with Global Admin powers? I wouldn’t.
Next, click on “Roles and administrators”, which will show the app-level roles particular to this app. Click on “Cloud Application” administrator, for example, which will show the principals that have this role against the app both explicitly (scoped to the app), and where that role inherits down from the tenant:
These are your third and fourth opportunities for prevention: do you trust all these principals to have control of a service principal with Global Admin rights?
You should repeat this process for Service Principals that hold the following roles:
This is also the point where the built-in password reset protections against Global Admins can break, creating attack paths from low-privilege users all the way to Global Admin by abusing control of service principals. In the real world, this is the most common avenue of escalating to Global Admin we have seen without needing to pivot down to on-prem AD first.
Going through this process manually, you may find yourself asking, “Is there one screen that tells me all roles held by any Service Principals?” To my knowledge, no such screen in Azure Portal exists, but you can use the Microsoft-authored PowerShell cmdlets to answer this question:
This will produce output like this:
This can make it much easier to quickly identify Service Principals with dangerous admin roles.
Microsoft is continually making improvements to Azure, whether those improvements are related to uptime, features, or security. There is an opportunity here for Microsoft to extend its existing design choices related to password reset rights to also cover Service Principals. I do not know the inner-workings of how that system works, or whether it’s even feasible to extend that system to cover Service Principals. Either way, this is a great opportunity for Microsoft to protect all of its customers and make Azure an even more secure platform.
Posts from SpecterOps team members on various topics…
38 
38 claps
38 
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
"
https://medium.com/@tsuyoshiushio/enable-different-configuration-for-multi-agent-pipeline-with-azure-devops-dca5b5a786ff?source=search_post---------205,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Mar 6, 2019·4 min read
Some project has a testing that we can not run parallel. In my case, one test create and remove a blog and others refer that. Obviously it can’t run at the same time. However, you might want to reduce the testing time with multi agent strategy. You can change the connection string per agent.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-pass-the-microsoft-azure-fundamentals-az-900-exam-854f93de5541?source=search_post---------206,"There are currently no responses for this story.
Be the first to respond.
Microsoft Azure is one of the leading cloud providers. This exam can be taken as an optional first step in learning about cloud services and how those concepts are exemplified by Microsoft Azure. It can be taken as a precursor to Microsoft Azure or Microsoft cloud services exams. While it…
"
https://medium.com/bb-tutorials-and-thoughts/building-an-angular-app-with-azure-static-web-apps-service-8fe84ebe4709?source=search_post---------207,"There are currently no responses for this story.
Be the first to respond.
Nowadays there are so many ways to build and deploy Angular apps such as angular with Java, Angular with Nodejs, serverless, etc. Building with Azure Static Web apps service is one of them and it is recently released by Microsoft Azure and it's in the preview mode. With this service, you can…
"
https://medium.com/hackernoon/azure-sql-transient-errors-7625ad6e0a06?source=search_post---------208,"There are currently no responses for this story.
Be the first to respond.
One thing people will notice when migrating to using cloud services are intermittent errors occur at a higher rate than you’re used to when running on premise.
These errors are often called transient errors and can be mitigated by just retrying the operation. While some operations might be fine to retry in all cases i.e. fetching data, others like creating orders, deducting money, etc. might not or at least need a little more fine grain control. Also, some errors makes no sense to retry and as a result it’s better to fail fast.
Writing the code to handle can easy turn to spaghetti code and it isn’t code you want to copy paste as it’s easy to get wrong and hard to maintain — so you really do want a framework to assist you with these scenarios.
This is how the authors describe Polly:
Polly is a .NET resilience and transient-fault-handling library that allows developers to express policies such as Retry, Circuit Breaker, Timeout, Bulkhead Isolation, and Fallback in a fluent and thread-safe manner.
Which really sums up what it does, it lets you create policies using a fluent API, policies which then can be reused and can range from being generic to target very specific scenarios.
Polly is available as NuGet package and works with both old .NET and shiny .NET Core, which will be what I’m using in this post.
Also worth noting Polly is open source available on GitHub.Polly’s also member of the .NET Foundation.
Defining a policy with Polly is really straightforward in the Polly namespace you Policy class which is what you utilize to create your policies
Above we’ve via the Handle<TException>method defined a policy that if a TimeoutException occurs it’ll wait 3 seconds and retry the operation 5 times before failing on the 6th time which is defined using the WaitAndRetry method.
Because the delay of each retry attempt in this case is retrieved using a Func<int, TimeSpan> delegate it’s really easy to implement more advanced scenarios like a sliding delay
to let our policy handle more exceptions we can use one to manyOr<TException> method after theHandle<TException> call
you can also make your policy even more granular by inspecting the exception with an exception predicate
In this case we’ll fail fast if it’s a .tmp file that’s not found. You can also set policies based on result values, besides retries there’s also support for circuit-breaker, timeout, fallback, composing policies together and more, so the policy “engine” is very flexible.
We now have our policy in place and want to put it into good use, here the policies Execute method comes in play
You can also create asynchronous policies to and then we can utilize the policies ExecuteAsync method
It’s almost too good to be true, there’s a lot happening under the hood — but in my opinion “hidden” under a very easy to understand and maintainable API.
When is comes to SQL server there’s a few errors known to be safe to retry, so we explicitly look for those, so if we begin with it could look something like this:
where SqlRetryCount is just a constant for how many retries to do and ExponetialBackoff if a method that exponentially increases the delay between each attempt
TimeoutException:s will always be retried but SqlExceptions will be passed to AnyRetryableError method for assessment if it’s retryable or not
AnyRetryableError will iterate all errors and with the RetryableError method check if it’s an error known to be retriable and here is where the magic happens
we will switch on SqlException error number and use a few known constants to determine if it should fail fast or retry.
In this case, I’m going to do a simple API that extends SqlCOnnection with WithRetry methods, i.e. open SQL connection
giving us an open sync and async method, which in its simplest form usage could look something like this
combining with a simple SQL to .NET object mapper like Dapper results in pretty clean code
the above code will open the sql connection using the retry policy, but the query will be executed without retry, which could be what you want for some operations, but for a select it’s often safe/what you want to retry the whole operation. For that we add an overload that lets us execute code within the policy boundary
this will open the connection and then invoke and return the result of Func you pass to it all within the scope of policy and just small refactoring of the calling code
and async variant usage could look something like this
Polly is really powerful, yet really easy to get started with and also fairly easy to retrofit into an existing application — so I definitely think you should take it for a spin.
I’ve created a complete sample repository on GitHub with the code from this post, thanks for reading! ❤
github.com
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
28 
3
28 claps
28 
3
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Partner & Technical fellow at WCOM AB. Microsoft Azure & Developer Technologies MVP. Been coding since I 80’s (C128 & Amiga). Father of 2, husband of 1.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/microsoftazure/credit-card-fraud-detection-with-azure-data-science-virtual-machine-a121dc2fd1b5?source=search_post---------209,"There are currently no responses for this story.
Be the first to respond.
This blog post is co-authored by Jaya Mathew and Francesca Lazzeri, data scientists at Microsoft.
People from across the data world came together in New York last week for the Strata Data Conference. In our session “A day in the life of a data scientist: How do we train our teams to get started with AI?”, we presented a scientific framework to help organizations improve data science skill set, systematically discover opportunities to create value from data, qualify new opportunities and assess their fit and potential, smoothly implement end-to-end advanced analytics pilots and projects, and produce sustainable ongoing business value from data.
We also walked through a detailed credit card fraud detection use case, from how the data typically gets collected to data wrangling, building a model, tuning the model, and operationalizing the model for a business to use in their production environment.
The goal of this blog post is to share with you more details on this end-to-end credit card fraud detection solution that we built using Python and Azure Data Science Virtual Machine.
Recent advancements in computing technologies along with the increasing popularity of eCommerce platforms have radically amplified the risk of online fraud for financial services companies and their customers. Failing to properly recognize and prevent fraud results in billions of dollars of loss per year for the financial industry. This trend has urged companies to look into many popular artificial intelligence (AI) techniques, including deep learning for fraud detection. Deep learning can uncover patterns in tremendously large data sets and independently learn new concepts from raw data without extensive manual feature engineering. For this reason, deep learning has shown superior performance in domains such as object recognition and image classification.
For this solution we used a sample data set from Kaggle that contains transactions made by credit cards in September 2013 by European cardholders. These transactions occurred in two days:
The data set can be summarized as follow:
· Features V1, V2, … V28: are the principal components obtained with PCA, the only features which have not been transformed with PCA are ‘Time’ and ‘Amount’
· Feature Time: contains the seconds elapsed between each transaction and the first transaction in the dataset
· Feature Amount: is the transaction Amount
· Feature Class: is the response variable and it takes value 1 in case of fraud and 0 otherwise
For this scenario we used a specific type of neural network called Autoencoder. This neural network is trained to attempt to copy its input to its output. Internally, it has a hidden layer h that describes a code used to represent the input.
The network may be viewed as consisting of two parts:
· an encoder function h = f(x)
· a decoder that produces a reconstruction r = g(h)
We optimize the parameters of our autoencoder model in such way that a special kind of error, reconstruction error is minimized.
To build our solution, we used a Data Science Virtual Machine, that is a Windows Azure virtual machine (VM) image. It is preinstalled and configured with several tools that are used for data analytics and machine learning. The Data Science Virtual Machine jump-starts your analytics project. You can work on tasks in various languages including R, Python, SQL, and C#.
To create an instance of the Microsoft Data Science Virtual Machine, follow these steps:
· Navigate to the virtual machine listing on the Azure portal. You may be prompted to login to your Azure account if you are not already signed in.
· Select the Create button at the bottom to be taken into a wizard.
· The wizard that creates the Microsoft Data Science Virtual Machine requires input. The following input is needed to configure each of the steps shown on the right of the figure:
a. Basics:
i. Name. The name of the data science server you’re creating.
ii. VM Disk Type. Choose SSD or HDD. For an NC_v1 GPU instance like NVidia Tesla K80 based, choose HDD as the disk type.
iii. User Name. The admin account ID to sign in.
iv. Password. The admin account password.
v. Subscription. If you have more than one subscription, select the one on which the machine is to be created and billed.
vi. Resource Group. You can create a new one or use an existing group.
vii. Location. Select the data center that’s most appropriate. For fastest network access, it’s the data center that has most of your data or is closest to your physical location.
b. Size. Select one of the server types that meets your functional requirements and cost constraints. For more choices of VM sizes, select View All.
c. Settings:
i. Use Managed Disks. Choose Managed if you want Azure to manage the disks for the VM. If not, you need to specify a new or existing storage account.
ii. Other parameters. You can use the default values. If you want to use nondefault values, hover over the informational link for help on the specific fields.
d. Summary. Verify that all the information you entered is correct. Select Create.
We developed our model using Python and Azure Notebooks. First of all, you need to prepare your environment and import the necessary components:
You can now enter the credentials to access the data from the cloud and then download the file for analysis:
Import the credit card data set:
For the modelling piece, you first need exclude the variable ‘Time’. Since the spread of the variable ‘Amount’ is large, this variable is standardized. Then you have to define the framework for the autoencoder and then compile and fit using the training data:
Finally, you can save your model:
In this blog post, we dived into a specific credit card fraud detection use case. Most importantly, we showed how the right cloud analytics environment, such as an Azure Data Science Virtual Machine, makes it easy to collect data, analyze, experiment, and build a model for any organization to use in a production environment.
· Strata Data NYC Slide Deck: https://www.slideshare.net/FrancescaLazzeriPhD/a-day-in-the-life-of-a-data-scientist-how-do-we-train-our-teams-to-get-started-with-ai
· Data Science Virtual Machine: https://aka.ms/AzureDSVM
· Data Source for demo: https://www.kaggle.com/mlg-ulb/creditcardfraud
· Blog Post by Venelin Valkov: https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd
· Deep Learning Book by Ian Goodfellow, Yoshua Bengio, Aaron Courville: http://www.deeplearningbook.org/
Any language.
50 
1
50 claps
50 
1
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@pluralsight/featured-channel-everything-net-developers-should-know-about-azure-8df78684f4b6?source=search_post---------210,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pluralsight
Dec 8, 2017·1 min read
We are experiencing a monumental shift towards cloud computing. As more data is moved to the cloud, Microsoft Azure’s footprint increases. Pluralsight expert and Microsoft MVP Scott Allen has created a custom channel to help .NET developers get up to speed on all things Azure.
Channel courses:
Microsoft Azure for Node.js Developers — Cloud Patterns and Architecture
Microsoft Azure for Node.js Developers — Building Secure Services and Applications
Developing with .NET on Microsoft Azure — Getting Started
Follow Scott’s channel now.
Channels offer Pluralsight users a focused, guided way to learn. With the ability to create, follow and share channels, skilling up and accomplishing goals becomes easier and more customizable. Learn more about channels and the other products that comprise our platform.
We are *the* technology learning platform, dedicated to helping teams create the future.
114 
1
114 
114 
1
We are *the* technology learning platform, dedicated to helping teams create the future.
"
https://medium.com/avalanche-hub/setting-up-an-avalanche-node-with-microsoft-azure-cheaper-than-aws-2683c5d3d197?source=search_post---------211,"There are currently no responses for this story.
Be the first to respond.
Running a validator and staking with Avalanche provides extremely competitive rewards of between 9.69% and 11.54% depending on the length you stake for. The maximum rate is earned by staking for a year, whilst the lowest rate for 14 days. There is also no slashing, so you don’t need to worry about a hardware failure or bug in the client which causes you to lose part or all of your stake. Instead with Avalanche you only need to currently maintain at least 60% up time to receive rewards. If you fail to meet this requirement you don’t get slashed, but you don’t receive the rewards. You also do not need to put your private keys onto a node to begin validating on that node. Even if someone breaks into your cloud environment and gains access to the node, the worst they can do is turn off the node.
Not only does running a validator node enable you to receive rewards in AVAX, but later you will also be able to validate other subnets in the ecosystem as well and receive rewards in the token native to their subnets.
You only need modest hardware requirements of 2 CPU cores, 4 GB Memory and 40 GB SSD to run a validator and it doesn’t use enormous amounts of energy. Avalanche’s revolutionary consensus mechanism is able to scale to millions of validators participating in consensus at once, offering unparalleled decentralisation.
Currently the minimum amount required to stake to become a validator is 2000 AVAX (which can be reduced over time as price increases). Alternatively, validators can also charge a small fee to enable users to delegate their stake with them to help towards running costs. You can use a calculator here to see how much rewards you would earn when running a node, compared to delegating.
I encourage everyone to run their own validators where possible, but for those that don’t meet the minimum staking requirements and want to delegate I am currently running a node which you can find below:
avascan.info
In this article we will step through the process of configuring a node on Microsoft Azure. This tutorial assumes no prior experience with Microsoft Azure and will go through each step with as few assumptions possible.
At the time of this article, spot pricing for a virtual machine with 2 Cores and 8 GB memory costs as little as $0.01060 per hour which works out at about $113.44 a year, a saving of 83.76%! compared to normal pay as you go prices. In comparison a virtual machine in AWS with 2 Cores and 4 GB Memory with spot pricing is around $462 a year.
docs.avax.network
Now that we have a node, it’s time to set up our node to be a validator on the network. There’s already a great tutorial on that written by Rado Minchev, so we’ll head over there next and declare our node to be a validator for Avalanche!
medium.com
Please see this excellent article / tool from community member Burcusan, on how to set up real-time alerts from your Avalanchego Validator Node. You can receive Telegram alerts if there is an issue with your node and further action is required.
medium.com
To make your node more recognisable for potential delegators to select to trust their stake with, consider verifying the identity of your node with the following providers Avascan, VScout, and Stakingrewards
Avalanche, a Revolutionary Consensus Engine and Platform. A Game Changer for Blockchain
Avalanche Consensus, The Biggest Breakthrough since Nakamoto
Comparison between Avalanche, Cosmos and Polkadot
Why Avalanche (AVAX) has the potential to be an incredible store of value
Avalanche 101: An Overview of the Internet of Finance
Avalanche Hub
316 
1
316 claps
316 
1
Avalanche Hub is the Community Growth Platform: from expansion efforts to open-source development, members directly contribute to vital research, education, and engineering initiatives. The Avalanche Hub is also on Twitter -> https://twitter.com/AvalancheHub
Written by
DLT Enthusiast and Writer. Interoperability is key for DLT to achieve its true potential. Avalanche $AVAX, Injective Protocol $INJ and Quant $QNT
Avalanche Hub is the Community Growth Platform: from expansion efforts to open-source development, members directly contribute to vital research, education, and engineering initiatives. The Avalanche Hub is also on Twitter -> https://twitter.com/AvalancheHub
"
https://medium.com/s-c-a-l-e/azure-cto-on-how-linux-and-devops-are-remaking-microsoft-s-cloud-d1a6436f1fce?source=search_post---------212,"There are currently no responses for this story.
Be the first to respond.
Mark Russinovich rose to fame in the 1990s as an expert in Microsoft Windows, helping users get the most out of the operating system, while also uncovering some serious issues with it. In 2006, he joined Microsoft to help improve the operating with which he was so familiar.
Now, Russinovich is CTO of Microsoft Azure. In this interview, he discusses what makes a good cloud platform and how new cloud-native application architectures are forcing enterprises to once again view IT as a competitive advantage
SCALE: How did you make the move from working on Windows — Microsoft’s desktop and server operating system — to working on Azure, its cloud operating system?
MARK RUSSINOVICH: I worked on Windows through the end of Vista and then Windows 7. I was looking at Windows 8, but at that point in time I stuck my head up and was talking with people in the company and seeing the changes that the industry was starting to go through.
The first change was the mobile change, which was happening while I was in Windows. I was working on making sure that Windows could adapt to that mobile change. Some of the things that you’re seeing, like Windows show up on small devices, that was some of what I was doing back then.
When I stuck my head up, I saw that there was another aspect to the mobile transformation that we’re going through, and that was the cloud. You have this periphery of mobile devices — not just phones, but devices that are now called Internet of Things devices, and headless devices that are sensors and controllers. All of these devices need to connect and store and process data someplace off the device. That is the cloud.
As I was talking with the people around the company, including some who had been in Windows and left to start this project called “Red Dog,” which became Azure, I realized there was a huge opportunity be part of that side of the disruption — to be at the center of the constellation of devices. That’s what attracted me to come over to Azure.
They might appear different, but Azure is what I consider an operating system for the cloud. It is a platform for the cloud. A lot of the learnings that I had for how to design a platform for a single system, like Windows, are directly applicable to a cloud platform.
Now, there’s a lot of things that are different, of course. You’re talking about massive-scale, distributed systems. Essentially the same problems that a technology like Mesos addresses. That’s also what made it exciting to me.
“I’ve heard people sour on the term DevOps in the last few months, and talk about it being an empty buzzword. I actually believe that it’s a meaningful word.”
What’s the defining feature of a cloud platform? Is it something like scale or “composability?”
I think scale-out, not scale-up, is one. Designing for failure is another one. If I was to lead with two principles for what cloud is, from a technology perspective, it would be those.
What’s the co-engineering like between the cloud and on-premises worlds within Microsoft?
A big strength of Microsoft is the hybrid play. The fact is that we do have these technologies that customers are using on-premises, and we’re using the exact same core pieces to bring those technologies up into the cloud. The SQL engine that runs behind Azure Database is the exact same one that runs the SQL Server boxed product. The engineering team is consolidated.
This kind of sharing of technologies allows us to take things back and forth from the on-premises to the cloud. Actually, more and more, it’s cloud-first. We have a “cloud-first” mantra at the company, which means we take more and more things back to the on-prem world from the cloud.
A place where you see us actually doing that is something we call Azure Stack, which is taking a subset of core Azure services and making them available for customers to deploy on-premises. We actually think that having the boxed-product technologies, and the connections to the companies using those, is a huge strength for us.
Does Microsoft have customers actually using these hybrid tools in production?
Absolutely. Not Azure Stack yet, because it’s not released but the predecessor to it called Azure Pack. We’ve had that out for about a year and a half, and we’ve got thousands of customers using it.
“What you’re hearing now is that every business segment is being disrupted by somebody like an Airbnb or an Uber, and to be competitive, the companies that aren’t necessarily born in the cloud need to operate like the ones that were.”
I think this is another benefit that we’ve got in Azure, actually. Not just things like machine learning, but first-party applications like Dynamics and Office 365, which are SaaS applications. If you think about any platform you’re building, it’s not just about supporting operating system services. What really drives a great platform are first-party applications where you’re getting direct feedback from people building big apps. In this case, something like Office 365 is much bigger and more demanding than the vast majority of our customers will ever be.
By making sure those applications work right on the platform, that makes sure that any applications that most of these enterprises or CSVs are going to be building will work great on the platform.
“I’ve heard people sour on the term DevOps in the last few months, and talk about it being an empty buzzword. I actually believe that it’s a meaningful word.”
Can walk me through the history of those principles at Microsoft? I don’t Azure was the company’s first experience with building for scale.
I think that to be able to design for hyperscale, and be able to adapt to failures and deliver services, you need to adopt a DevOps way of development and operations. I’ve heard people sour on the term DevOps in the last few months, and talk about it being an empty buzzword. I actually believe that it’s a meaningful word that really represents the transformation that Microsoft, starting with groups like Azure, has undergone over the last 5 to 10 years. Which is to move from a boxed-product way of operating to a world where we develop and operate the software, and have to do that in a very agile way to get new features and functionality out.
That requires culture, that requires tooling, and that requires the processes that support that — which are very different than the equivalents that you have for operating efficiently in the boxed-product world.
“What you’re hearing now is that every business segment is being disrupted by somebody like an Airbnb or an Uber, and to be competitive, the companies that aren’t necessarily born in the cloud need to operate like the ones that were.”
One of my criticisms early on was that companies were just rebuilding old architectures on top cloud platforms. Do you see customers now coming around to the idea of building applications in new ways?
I think that you saw cloud providers like us realize it, but it isn’t just in the context of cloud providers. It’s in internet service providers that have been living in this world for a long time. That forces you to discover these things because, at the kind of hyperscale that we’re getting to, you can’t operate any other way. We were forced to adapt to a new way of operating in order to accomplish the things that we needed to accomplish.
If you take a look at the early adoption of cloud among enterprises, a lot of it was taking what they had and moving it to the cloud. There’s not a lot of major new architecture, necessarily, going on with a lot of those projects. But enterprises are also starting to realize that to stay competitive in this world where they are going to have a competitor that’s taking advantage of a DevOps model, they need to also adopt a DevOps model. If you’ve got a competitor that is in the segment that you’re in, that is adopting things like containers and orchestration, and DevOps delivery of those applications, they’re going to have an advantage against you if you’re still operating in this traditional IT way of doing things.
That recognition is driving enterprises to take these things seriously and adopt them. What you’re hearing now is that every business segment is being disrupted by somebody like an Airbnb or an Uber, and to be competitive, the companies that aren’t necessarily born in the cloud need to operate like the ones that were.
“Last fall, it was one in five virtual machines were Linux virtual machines. Now, it’s up to one in four.”
Is there a challenge for Microsoft trying keep competitive with a company like Amazon Web Services, which runs primarily Linux instances and has a perception of catering to a new generation of developers?
For one, I want to point out that our public cloud, Azure, is not a Windows-centric public cloud. It’s a system-agnostic one. That’s the way that we’re operating, and we’ve been operating that way for a long time. The use of Linux in Azure is reflecting that. Last fall, it was one in five virtual machines were Linux virtual machines. Now, it’s up to one in four.
We are being recognized as a good place to run Linux, and the usage is reflecting that. From our perspective, if people want to run Linux and want to build their hyperscale applications on Linux, that’s great. We want Azure to be a fantastic place to do that. If they want to do it on Windows, we want to make sure that Azure is a great place to do that, as well.
And we also want to make sure that our Windows customers can take advantage of microservices and DevOps and containers the same way that someone that chooses Linux can take advantage of them. That’s why we’re building container technologies into Windows and why we’re doing these partnerships with Docker and Mesosphere to bring those technologies to the Windows space.
Microsoft and Mesosphere have successfully ported Mesos onto Windows. Why is this significant?
It’s significant because the orchestration layer is the place where we’re seeing a lot innovation and need for tooling and support for these new cloud application models. Mesos is a very popular option here, and bringing Mesos’s capabilities to Windows unlocks this new style of orchestration for Windows developers. And any organization that has applications mixed between Linux and Windows servers can now allocate a pool of resources and them manage them holistically using the same interfaces and the same deployment technologies. So we’re really excited about bringing that to the Windows world.
We’ve already supported Mesos on top of Azure, something that was done using the resource group templates that we co-developed with Mesosphere. That’s an example of bringing Linux technologies and Mesos onto the Azure platform because our customers were asking for it.
What considerations are you looking as you continue to evolve Azure? Are there certain technologies or use cases driving its evolution?
It’s actually kind of challenging because we’ve got very diverse use cases and very diverse applications that are taking advantage of the cloud. You mentioned one, which is what we call lift and shift, which is enterprises taking their existing workloads and architectures and moving them into the cloud. That requires a very special architecture, because you have to make the cloud look a lot like an on-premises environment.
Then, we also have the new, modern, cloud-native applications. They are hyperscale, scale-out, very much designed for resilience, and are taking advantage of things like microservices and the agility that those provide. Those have very different requirements, as well.
“While we do have a commitment to match some of the core infrastructure pricing with Amazon, we don’t hear pricing come up in conversations with customers.”
How much does data, or big data, as a use case affect your day-to-day decision making and your future planning?
Data is a key part of the cloud scenarios. It’s also another great example of how we got very diverse types of workloads requiring different types of data technologies. From the lift-and-shift or OLTP type workloads that enterprise potentially use, to the massive scale-out, NoSQL-type solutions, to the large data stores like we’ve got with our data lake offering. There is just a full spectrum of different technologies addressing all of these that we’ve got to make sure we support and offer.
One of the big story lines in the past few years has been these “price wars” among the cloud providers. Is that something that’s going to come to an end at some point?
There was a lot of focus on pricing about a year and a half ago. A lot of news about price wars and this cloud provider dropping, another one matching. And while we do have a commitment to match some of the core infrastructure pricing with Amazon, we don’t hear pricing come up in conversations with customers, and we don’t see the industry at large — or analysts or customers — looking and focusing on that. It’s more about features and high-value services at this point.
“People talk about how every enterprise is now a data company, every enterprise now needs data scientists.”
Is that because people have just become familiar with how the cloud operates and how to architect for it, or with the differences among providers?
I think we’ll be seeing a lot more of the cluster-oriented microservices applications, as far as compute goes. And we’ll see more and more of the connected services in pipelines. IoT’s another area that we see a huge amount interest and momentum in, where you’ve got a whole bunch of cloud services connected together to deliver an IOT solution — from event-ingestion to live stream analysis, to dumping huge amounts of data into some store. You can then come along later and do data processing on top of it, and machine learning on top after that, to drive feedback that goes out to those devices.
We talked about the data services. Does the cloud provider have all of the data services that I’m going to be needing? What are the SLAs behind them? Can they support the hybrid scenarios that I’ve got in mind? These are the high-order bits, much more so than pricing at this.
If you were to look at Microsoft, the average Windows workload a decade ago and then project out a few years, what’s the difference in terms of what they might look like?
Absolutely. I think if you just take a look at that website-plus-database model, it came about because that’s the way that IT has created those applications. Once you can add these other services, like data analytics services, across your databases, websites and the different properties that your enterprise has, then you unlock a lot more learning.
What I just described there touches on a whole bunch of services. Everything from infrastructure-as-a-service to platform-as-a-service to machine learning. We’re going to see a lot more of those kinds of applications.
So if the past was about a web server talking a database, the future is more about data pipelines and connected systems?
Absolutely. AI think if you just take a look at that website-plus-database model, it came about because that’s the way that IT has created those applications. Once you can add these other services, like data analytics services, across your databases, websites and the different properties that your enterprise has, then you unlock a lot more learning.
People talk about how every enterprise is now a data company, every enterprise now needs data scientists. It goes to that, which is all my applications, even if they’re a web database, can make use of data analysis to drive better value. I mean, even if I’m just looking at the traffic on the website, or looking at the access patterns to the data, I can learn something about my customers, how to optimize my application and how to optimize my business.
What’s next in computing, told by the people behind the…
26 
1
26 claps
26 
1
What’s next in computing, told by the people behind the software
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
What’s next in computing, told by the people behind the software
"
https://medium.com/hackernoon/migrating-wemove-co-from-azure-to-ubuntu-vm-on-digital-ocean-96280ce24777?source=search_post---------213,"There are currently no responses for this story.
Be the first to respond.
Back in July 2016, when I started writing the code of what will become WeMove.co, I chose to use ASP.NET Core because C# is my go-to static language (I blogged about how I came upon that choice here: http://ezeokoyecelestine.blogspot.com.ng/2016/07/microsofts-confusing-dilemma-with.html). ASP.NET Core was new and very edgy at the time, I was using it at version 1.0 and I faced some really interesting issues. As I alluded in the post, Microsoft did sort out the issues and yesterday, I successfully migrated our website from Azure to Ubutu VM on Digital Ocean.
Long story short: To cut down expenses.
On January 2nd, WeMove Technologies moved into our new office at Bode Thomas Surulere — a thriving business unit in Lagos. Running a business in Lagos, it’s imperative that we cut cost as much as possible. So it’s important I think medium term on how we can fix costs. One of the major strategies for me was either joining the Microsoft BizSpark program or getting rid of Azure, as the cost had started piling.
I’d applied for BizSpark since November and having not gotten any response, I decided to start shopping for an alternative cloud service. Azure’s PaaS is my go-to hosting service because of the ease of setting-up and deployment, directly from my BitBucket repo. Letting go meant I had to set-up a build pipeline while moving.
Even though the primary driver is reducing the cost of business, a close number 2 reason for migration was reduction of error in the future.
For all past projects and for WeMove.co, I’d preferred hosting code at BitBucket. Primarily because I get a lot of free private repository and my development team hasn’t exceeded 5. However, as we make plans for new hires, I decided that we need to upgrade our plan on BitBucket.
Then I discovered GitLab.
I went to GitLab to checkout the free CI/CD, but ended up using it because of the unlimited number of team members available (Sorry BitBucket, I still ❤️ you). Ended up not using their CI/CD, instead I rolled out my own build server.
For the build server, we got a Windows Server box at Interswitch Cloud (https://cloud.interswitch.com). I chose Jenkins for the build server because that’s what I have the most experience with and trust.
It took me 1 week (of switching between coding and running the business), 71 builds & copying a folder from my development PC to the server, to successfully get Jenkins to deploy my nuget server as I want it. The major issue was getting Jenkins to make MSBuild to work with Web Deploy (the Nuget server is an ASP.NET 4.5 project). I’m not even sure I remember all the steps involved, so I apologise for not doing a step-by-step guide.
After doing this successfully, it was a lot easier to set-up scripts for the Vehicle Hire ASP.NET Core app.
Notice that on line 2, I told dotnet to publish for ubuntu, using the -r ubuntu.16.04-x64 option. Yes, I used the JDK “jar” utility to create a zip file on the third line. Did you see what I did there? Also, the curl command on line 4 only works after you download curl for windows from https://dl.uxnr.de/build/curl/curl_winssl_msys2_mingw64_stc/curl-7.57.0/curl-7.57.0.zip
Now I need to go to the Ubuntu server and set it up to accept the ZipPackage.zip. I decided to do this using PHP. That meant writing a little PHP script, which accepts the request and unzip the zip file to a specified /var/www/ path, depending on the action specified for ?tasktoperform=<action>.
For the web server, we acquired a VM from Digital Ocean. It took a little over 10 minutes from registration to the point of setting up the SSH keys. I was super impressed, to be honest.
I created a PHP script which receives the zip from the build server. The port it runs on is only open to the build server’s IP address, configured using the ufw firewall utility. It takes the file and unzips it into the appropriate directory. It also copies the appropriate appsettings.json file for the ASP.NET Core. I’ll explain why below under Gotchas.
The following links contain in-depth, step-by-step guides on how to set-up ASP.NET Core for Ubuntu and configure it to work with Nginx. Then installing as a service, so it starts up with the server. So I won’t bore you by repeating them just follow the links:
docs.microsoft.com
blog.bobbyallen.me
— —
Also, I went ahead to set-up Let’s Encrypt as the SSL provider for WeMove. The link below tells you how to do that:
www.digitalocean.com
These are the things that could kill your time if you decide on following these set-up to get ASP.NET Core running on your Ubuntu server.
You might be wondering that with this complex set-up, how is it that we’re saving more money than just pushing to BitBucket and letting Azure PaaS build it into an App Service. The answer is below:
Not only can it listen to multiple ports, it can also serve multiple (sub-) domains. That means if it’s not being maxed-out or broken in any way, I ain’t setting-up another server to serve WeMove sub-domains.
For each ASP.NET Core application we deploy to Azure, we need to set-up a new App Service. To get SSL, it has to be a tier (or two) above entry payment tier. Average monthly use for us per App Service per tier is between $25 and $30. Other services like SQL Azure raises the cost way higher, especially when you have multiple databases for staging and production.
A Digital Ocean droplet, running Nginx lets us utilise the same server for all our web apps, at a $20 charge. The Nginx lets us reverse proxy to each ASP.NET Kestrel server, depending on the request that comes in.
Our database is still in Azure and I plan to keep it that way for a long time.
Thanks for taking out time to read this.
We’re launching a new service next week, lookout for it.
If you want to hire any vehicle for anything in Lagos, Nigeria, please use our service https://wemove.co. Follow us everywhere on social media: @WeMoveCo
twitter.com
www.instagram.com
www.facebook.com
#BlackLivesMatter
47 
2
47 claps
47 
2
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Rebel • Founder & CEO, WeMove Technologies (owners of WeMove.co) • Follow @WeMoveCo
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.datadriveninvestor.com/tips-for-az-104-microsoft-azure-administration-certification-9689c5925cc8?source=search_post---------214,"Recently Microsoft made quite few changes in Azure certifications. Earlier you will need to pass 2 exams Az-100 and AZ-101 to be a certified Azure administrator. Microsoft combined both skills into one exam which was AZ-103.
AZ-104 is new version of AZ-103. This tests the candidates knowledge on implementing, managing, and monitoring an organization’s Microsoft Azure…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gavinlewis/multi-cloud-architectures-for-the-enterprise-part-2-41df71d959e3?source=search_post---------215,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gavin Lewis
Nov 4, 2019·5 min read
A few months back I wrote about Multi-Cloud Architectures for the Enterprise: Part 1; summarized, it was an example of how you could connect AWS, Azure and On-Premise using IPSec VPNs. Part 2 was intended to talk about platform services, however, after a few months working with Azure outside of my sandbox environment and seeing what some of the clients I work with are doing, I thought I’d pivot a little and talk about the big three cloud providers…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/spiritual-tree/spring-azure-324f8f6b61b5?source=search_post---------216,"There are currently no responses for this story.
Be the first to respond.
A poem for early Spring
A sliver of skymore violet than blueThe Spring Azure driftslike a petal in view.
Sailing on the currents,floating through the flowersas early song birds calland buds slip off their cowls.
Tiny blue butterflywrestling the wind,what spirit lifts and leads yousafely home again?
"
https://medium.com/awesome-azure/azure-reading-application-settings-in-azure-functions-asp-net-core-1dea56cf67cf?source=search_post---------217,"There are currently no responses for this story.
Be the first to respond.
Reading settings from a Azure Functions in .NET.
For the second (v2) and third (v3) generation of Azure Functions, the logic changes a bit. Not quite as straightforward, but more flexible. ConfigurationManager is not used anymore. Instead, you’re supposed to use ConfigurationBuilder. OR
"
https://medium.com/awesome-azure/azure-difference-between-azure-sql-database-and-sql-server-on-vm-comparison-azure-sql-vs-sql-server-vm-cf02578a1188?source=search_post---------218,"There are currently no responses for this story.
Be the first to respond.
Comparison — Azure SQL Database vs SQL Server on Virtual Machine
Azure SQL Database offers Database-as-a-service (DBaaS-PaaS). With SQL Database, you don’t have access to the machines that host your databases. In contrast, Azure Virtual Machine offers Infrastructure-as-a-service (IaaS). Running SQL Server on an…
"
https://medium.com/dotnet-hub/use-azure-key-vault-with-net-or-asp-net-core-applications-read-azure-key-vault-secret-in-dotnet-fca293e9fbb3?source=search_post---------219,"There are currently no responses for this story.
Be the first to respond.
Store app settings in Azure Key Vault for .NET 5.x or ASP.NET Core 3.x applications
Azure Key Vault is a service that you can use to store secrets and other sensitive configuration data for an application. It allows you to define settings that can be shared among multiple apps, including apps running in App Service.We will use…
"
https://towardsdatascience.com/how-to-deploy-web-apps-with-azure-52ca340b41b9?source=search_post---------220,"Sign in
There are currently no responses for this story.
Be the first to respond.
James Briggs
Jul 17, 2020·8 min read
Azure — or any other cloud service — has made sharing the fruits of our creative labor easier than ever.
We can take an app built with Angular (an incredibly easy framework to pick-up) and deploy it to the world quicker than the time it takes to make coffee.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-deploy-static-angular-website-with-azure-devops-46546b536aeb?source=search_post---------221,"There are currently no responses for this story.
Be the first to respond.
There are a lot of deployment strategies when you deploy your angular applications to production and your deployment strategy is entirely depends on your application architecture. For example, If you are using Java or Nodejs with Angular you need to deploy your application on respective environments. If…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/protecting-personal-identifiable-information-with-azure-ai-aa4b7afc4839?source=search_post---------222,"There are currently no responses for this story.
Be the first to respond.
TLDR; The following post will outline both first party and open source techniques for detecting PII with Azure.
Personally Identifiable information (PII), is any data that can be used used to identify a individuals such as names, driver’s license number, SSNs, bank account numbers, passport numbers, email addresses and more. Many regulations from GDPR to HIPPA require strict protection of user privacy.
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
Azure Cognitive Search is a cloud solution that provides developers APIs and tools for adding a rich search experience to their data, content and applications. With cognitive search you can add cognitive skills to apply AI processes during indexing. Doing so can add new information and structures useful for search and other scenarios.
The Azure PII Detection skill (Currently in Preview) extracts personally identifiable information from an input text and gives you the option to mask it from that text in various ways. This skill uses the machine learning models provided by Text Analytics in Cognitive Services.
docs.microsoft.com
In addition to the first party cognitive search Microsoft also provides an open source PII detection tool for Azure called Presidio which was developed by the Microsoft Commercial Software Engineering team in Israel.
github.com
Presidio is open-source, transparent, and scalable. Presidio allows developers and data scientists to customize or add new PII recognizers via API or code to best fit your anonymization needs. Presidio leverages docker and kubernetes for workloads at scale.
Presidio automatically detects Personal-Identifiable Information (PII) in unstructured text, annonymizes it based on one or more anonymization mechanisms, and returns a string with no personal identifiable data. For example:
For each PII entity, presidio returns a confidence score:
Text anonymization in images (beta)
Presidio uses OCR to detect text in images. It further allows the redaction of the text from the original image.
Check out a public demo to try out on your own data with the link below.
presidio-demo.azurewebsites.net
Installation Steps
More information can be found on the github repo and near one click deployment options for Azure are coming soon!
Additional deployment options can be found here.
In this post you learned two of my favorite options for detecting PII in your data with Azure. If you are interested in Azure and AI be sure to check out my other posts and the Azure medium blog.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
158 
1
158 claps
158 
1
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/hackernoon/kubernetes-installing-mongodb-replicaset-on-azure-using-helm-20f6bfcf92b3?source=search_post---------223,"There are currently no responses for this story.
Be the first to respond.
It’s time to install a MongoDB ReplicaSet on a Kubernetes cluster on Azure and try to kill it in all possible ways!Starring:
Let’s install a Cluster with 1 Master and 3 Nodes all running Linux using ACS Engine with following commands.
I described detailed steps on ACS Engine usage for installing a cluster in my Kubernetes Adventures on Azure — Part 3 (ACS Engine & Hybrid Cluster) article. Please refer to it for details. Here I list quickly commands to be used.
This is needed to group all resources for this tutorial in a single logical group, to be able to delete everything with a single command at the end.
I usually create an ssh pair for my test on acs. Please check my article here on how to do it. I will change the examples/kubernetes.json file to use previously created ssh pair, dnsPrefix and servicePrincipalProfile (following Deploy a Kubernetes Cluster suggestions from Microsoft).
my kubernetes.json file with changes in bold is:
Create your cluster with:
Wait for the cluster to be up running:
Connect to it using kubeconfig file generated during deployment in _output folder.
Following commands can be used to determine when cluster is ready:
Now you can open Kubernetes Dashboard if you want to use a UI to check you cluster status: kubectl proxy and then open a browser at http://127.0.0.1:8001/ui
Helm is the Package Manager for Kubernetes. It simplifies installation and maintenance of products and services like:
We will use it to install and configure a MongoDB Replica Set.
Prerequisites, installation steps and details can be found in the Use Helm to deploy containers on a Kubernetes cluster article from Microsoft.
With all prerequisites in place, Helm installation is as simple as running command:
Let’s clone charts repository to be able to examine and change MongoDB chart files before deploying everything on our cluster:
Now go in the /charts/stable/mongodb-replicaset folder. Here you will find all artifacts composing an Helm Chart. If needed you can change values.yaml file to tailor installation based on your needs. For now let’s try a standard installation.
Run following command: helm install . and wait for following output:
MongoDB Replicaset is up and running! Helm is ultra easy and powerful!
From output of helm install pick up the NAME of your release and use it in the following command:
Here we follow a different path from Helm Chart. Let’s open an interactive shell session with remote Mongo server!
In theory Pod 0 should be the master as you can see rom rs:PRIMARY> prompt. If this is not the case tun following command to find the Master:
Take note of the primary Pod because we are going to kill it soon and connect to it using last kubectl exec used above.
We need to create some data to check persistence across failures. We are already connect to the mongo shell, creating a document and leaving the session is as simple as:
Use following command to monitor changes in replica set:
Here it is: kubectl delete pod $RELEASE_NAME-mongob-replicaset-0
MongoDB will start an election and another Pod will become master:
And in the meantime Kubernetes will immediately take corrective actions instantiating a new Pod 0.
Now we have to simulate a real disaster: let’s kill all Pods and see the StatefulSet magically recreating everything with all data available.
After few minutes our MongoDB replicaset will be back online and we can test it again to see if our data are still there.
Run following command to verify that key created is still there:
As always you can delete everything with a simple Azure CLI 2 command: az group delete --name k8sMongoTestGroup --yes --no-wait
This is a topic for a future post. It seems trivial as a kubectl expose but it is not so easy. If you expose the existing service, it will be LoadBalanced on 3 nodes behind it and this is wrong. We need 3 LoadBalancers, exposing 3 services, 1 for each Pod in the StatefulSet. Moreover we have to activate Authentication and SSL to ensure best practice from security perspective.
I will find best way to do it, while playing with Heml, Kubernetes, MongoDB and Azure!
#BlackLivesMatter
94 
5
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
94 claps
94 
5
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/azure-messaging-service-in-a-picture-f8113cec54cd?source=search_post---------224,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jul 12, 2018·4 min read
When you start serverless on Azure, you might be wondering which messaging services to use. I create a simple diagram how to choose the messaging service.
If you have a asynchronous message between Azure functions, the Storage Queue comes as a first choice. If you find Queue messaging service, Storage Queue have these characteristics
azure.microsoft.com
If you find reliable solution for Queue and Pub-Sub with enterprise reliability you can choose Service Bus.
Before moving on the detail, I’ll share the Queue and Pub-Sub in a picture.
A queue is simple messaging service for sending messaging asynchronously. A function send a queue, then other function polling the queue then consume the queue. Since it is polling, it is not very fast.
As for publish-subscribe, a message receiver is a subscriber. A subscriber subscribe a topic. If topic receive a message, it send message to the subscribers. Subscriber can filter the messages. It is the overview of the Publish-Subscribe.
In case of Service bus, it has these charactersitics
You can use Service Bus for reliable use case. E.g. Financial / Banking solution.
azure.microsoft.com
Event Grid is interesting offer. It has these characteristics
You can use Event Grid
azure.microsoft.com
If you need to ingest a lot of messaging at a time, you need to choose EventHub or IoTHub.
azure.microsoft.com
azure.microsoft.com
This is very good to read for EventHub with a lot of messages.
blogs.msdn.microsoft.com
medium.com
This is not asynchronous, but, if you want to have bi-directional messaging, like server to client, you can pick SignalR Service. It is serverless offer for the SignalR. SignalR is based on websocket. However, you don’t need to create a websocket server, also it scale!
azure.microsoft.com
If you want to send message through company to company with firewall, you can choose Auzre Relay. It support WebSocket and WCF. This service is also serverless and it is designed for hybrid communication between intranet through the internet.
docs.microsoft.com
I use these service to communicate between hololens and hololens through the internet.
Notification Hub is used for mobile push notifications. If you want to communication a lot of clients, you can consider this.
azure.microsoft.com
This article is written for a guy wants to try Azure Functions. It is good to know the messaging services. :) This is quick summary, however, you can learn more detail, please refer these articles.
docs.microsoft.com
docs.microsoft.com
Senior Software Engineer — Microsoft
See all (200)
90 
1
90 claps
90 
1
Senior Software Engineer — Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/azure-na-pr%C3%A1tica-gratuito-1-desenvolvimento-web-saiba-como-foi-conte%C3%BAdos-gratuitos-5908938c85e6?source=search_post---------225,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
May 11, 2020·5 min read
Neste último sábado (09/05/2020) o Azure na Prática promoveu seu primeiro minicurso online e gratuito, com foco em Desenvolvimento Web e utilizando o Microsoft Azure. Além de uma breve introdução a conceitos de cloud computing, ao longo deste treinamento foi coberto também o uso de tecnologias como Azure Storage, Azure Functions, Azure App Service, Application Insights, ASP.NET Core, .NET Core e Visual Studio Code.
Fui um dos instrutores e organizadores desta iniciativa, juntamente com meus amigos Milton Câmara (Microsoft MVP, MTAC) e Ericson da Fonseca (Microsoft MVP). O resultado geral excedeu em muito nossas expectativas e nos, já que contamos com 1819 inscrições via Sympla:
E um pico de 609 pessoas nos assistindo ao longo da live no YouTube, com mais de 3 mil visualizações até o momento da publicação deste post:
A gravação já está disponível no canal Azure na Prática no YouTube e pode ser assistida gratuitamente (aproveitamos para convidar você que está lendo esse post para que se inscreva no mesmo):
Foi surpreendente para nós 3 ao longo da live constatarmos que pessoas residentes em Portugal, Angola, Itália e Reino Unido estavam nos assistindo (além claro de participantes de norte a sul do Brasil). Recebemos inúmeros agradecimentos em redes sociais (YouTube, Facebook, LinkedIn) por este esforço, algo que nos enche de orgulho e motivação para seguir em frente mesmo em uma época difícil para todos. Eis alguns feedbacks:
Os slides que utilizamos foram disponibilizados no SlideShare:
Aproveitamos para agradecer:
Nas próximas seções estão avisos incluindo conteúdos gratuitos sobre o Microsoft Azure, eventos online gratuitos nos próximos dias cobrindo esta plataforma e descontos para os próximos cursos pagos do Azure na Prática.
O canal do Azure na Prática no YouTube é uma excelente fonte de conteúdos, com gravações gratuitas incluindo mesas redondas, dicas e truques na utilização do Microsoft Azure:
www.youtube.com
Temos ainda o Canal .NET e o Coding Night, canais em que serviços do Microsoft Azure são abordados com frequência:
www.youtube.com
www.youtube.com
Uma iniciativa promovida anualmente pelo Canal .NET é o Azure Tech Nights, evento online e gratuito cobrindo diferentes tecnologias que integram a nuvem Microsoft. A edição 2020 aconteceu recentemente (Fevereiro a Abril), com os links da gravação de cada palestra podendo ser encontrados no seguinte post:
Azure Tech Nights 2020: saiba como foi - Vídeos Gratuitos
Diversos projetos exemplificando o uso de serviços do Azure estão no repositório do Azure na Prática no GitHub:
https://github.com/azurenapratica
E também no meu GitHub:
https://github.com/renatogroffe
A seguir estão também diversos artigos abordando diferentes serviços do Azure (há vídeos sendo referenciados em alguns destes posts):
Docker - Guia de Referência Gratuito
Kubernetes - Guia de Referência Gratuito
GitHub Actions - Guia de Referência Gratuito
Azure DevOps - Guia de Referência Gratuito
Serverless + Azure Functions: Guia de Referência
Serverless é muito mais que apenas APIs REST!
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
Microservices: alternativas para a implementação no Microsoft Azure
ASP.NET Core + Azure App Configuration: manipulando configurações de forma mais inteligente
Como o Microsoft Azure pode simplificar a publicação de suas Web Apps? - Dica Rápida
GitHub + Azure App Service: deployment automatizado e sem complicações de Web Apps na nuvem
Application Insights + Logic Apps + Aplicações Web: enviando alertas de monitoramento via Slack
ASP.NET Core + Entity Framework Core: monitoramento descomplicado via Application Insights
ASP.NET Core + Application Insights: monitorando a disponibilidade dos BDs de uma aplicação
E concluo este post com os links para os blog de cada um dos palestrantes do minicurso:
Renato Groffe — Blog
Ericson da Fonseca — Blog
Milton Camara Gomes — Blog
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
85 
1
85 claps
85 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/azure-na-pratica/azure-na-pr%C3%A1tica-gratuito-3-azure-devops-saiba-como-foi-conte%C3%BAdos-gratuitos-314df31d447c?source=search_post---------226,"There are currently no responses for this story.
Be the first to respond.
Neste último sábado (18/07/2020) o Azure na Prática promoveu seu terceiro minicurso online e gratuito, focando desta vez nos primeiros passos com Azure DevOps. Além de uma introdução englobando conceitos de automação e DevOps em geral, ao longo deste treinamento foi coberto também o uso de Azure DevOps em conjunto com tecnologias como Docker, Azure App Service, Azure Container Registry, ASP.NET Core e Application Insights.
Caso você queira ter acesso ao conteúdo do primeiro e do segundo minicursos (promovidos em Maio e Junho/2020, respectivamente) de forma gratuita ou, até mesmo, deseje revê-los, acesse os links a seguir:
Azure na Prática Gratuito #1 - Desenvolvimento Web: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #2 - Docker: saiba como foi + conteúdos gratuitos
Fui instrutor e um organizadores desta iniciativa, juntamente com meus amigos Milton Câmara (Microsoft MVP, MTAC) e Vinicius Moura (Microsoft MVP). O resultado geral foi muito além superou mais uma vez nossas expectativas e os 2 minicursos anteriores, com 5738 inscrições via Sympla:
Tendo um pico de 1252 pessoas nos assistindo ao longo da live no YouTube, com mais de 5,2 mil visualizações até o momento da publicação deste post:
Os testes de acesso que pedimos gentilmente àqueles que assistiam ao evento motraram participantes nas seguintes cidades (141 diferentes ao todo, com dados capturados através do uso do Azure Application Insights na aplicação que serviu de base para a demonstração): Adamantina, Aguas de Lindoia, Altinopolis, Araraquara, Barbacena, Barcelona, Barra do Piraí, Barra Mansa, Barueri, Bayeux, Belém, Belford Roxo, Belo Horizonte, Benavente, Betim, Birigui, Blumenau, Boa Esperanca, Braganca Paulista, Brasília, Brumado, Cacapava, Cachoeirinha, Cachoeiro de Itapemirim, Caetanopolis, Caetite, Cajamar, Cajazeiras, Camaqua, Campina Grande, Campinas, Campo Grande, Carapicuiba, Caxias do Sul, Columbus, Conselheiro Lafaiete, Contagem, Cotia, Criciúma, Cuiabá, Curitiba, Diadema, Divinópolis, Duque de Caxias, Embu, Espirito Santo do Pinhal, Ferraz de Vasconcelos, Florianópolis, Fortaleza, Funchal, Garanhuns, Goiânia, Guaratingueta, Guarulhos, Ibipora, Indaiatuba, Ipatinga, Itapevi, Itaquaquecetuba, Itu, Ivaipora, Jaboatao dos Guararapes, João Pessoa, Joinville, Juiz de Fora, Jundiaí, Lima, Lisbon, Lorena, Luanda, Macapá, Manaus, Manhuacu, Marica, Marília, Maringá, Matao, Matozinhos, Maua, Mogi das Cruzes, Mogi Mirim, Montevideo, Montreal, Mossoro, Nantes, Navegantes, Niterói, Nova Europa, Nova Luzitania, Novo Hamburgo, Osasco, Pacajus, Paraguacu Paulista, Passo Fundo, Paulinia, Pedreira, Piracicaba, Poços de Caldas, Pomerode, Ponta Grossa, Porto, Porto Alegre, Porto Velho, Presidente Prudente, Québec, Recife, Ribeirão Preto, Rio das Pedras, Rio de Janeiro, Salvador, Santa Fe do Sul, Santa Luzia, Santa Maria, Santo André, Santos, São Bernardo do Campo, São Carlos, Sao Goncalo, Sao Jose, Sao Jose do Rio Preto, São José dos Campos, Sao Lourenco da Mata, São Luís, São Paulo, Sao Roque, Sao Vicente, Seattle, Sete Lagoas, Silvares, Suzano, Taboao da Serra, Taquara, Uba, Uberlândia, Vargem Grande Paulista, Videira, Vila Velha, Vitória, Votuporanga, Waterloo, Woodstock.
Uma análise destes resultados mostra que esta iniciativa atingiu não apenas cidades do Brasil de norte a sul, como também contou com público em países como países Angola, Canadá, Espanha, Estados Unidos, França, Peru, Portugal e Uruguai:
O projeto utilizado na demonstração, assim como as queries para análise da origem (cidade, país/região) das requisições e processamento (por qual container Docker as solicitações foram processadas) já estão nos seguintes repositórios do GitHub:
ASP.NET Core 3.1 + Razor Pages + Docker + Simulação de Falhas + Azure Application Insights
Application Insights + Kusto Query Language + Queries com informações de requisições por containers Docker
Application Insights + Kusto Query Language + Queries com informações de requisições por cidade de origem
Application Insights + Kusto Query Language + Queries com informações de requisições por país/região de origem
A gravação já está disponível no canal Azure na Prática no YouTube e pode ser assistida gratuitamente (aproveitamos para convidar você que está lendo esse post para que se inscreva no mesmo):
Os slides que utilizamos foram disponibilizados no SlideShare:
Aproveitamos para agradecer:
Recebemos inúmeros agradecimentos em redes sociais (YouTube, Facebook, LinkedIn) por este esforço, algo que sempre nos motiva para seguir em frente com esse tipo de iniciativa. Seguem alguns feedbacks:
Nas próximas seções estão avisos incluindo conteúdos gratuitos sobre o Microsoft Azure, eventos online gratuitos nos próximos dias cobrindo esta plataforma e descontos para os próximos cursos pagos do Azure na Prática.
No blog Azure na Prática temos várias postagens semanais, cobrindo o uso de tecnologias como Azure DevOps, Docker, Kubernetes e diversos serviços do Microsoft Azure. Deixamos o convite para que você se inscreva aqui, recebendo assim notificações de nossos conteúdos gratuitos:
medium.com
O canal do Azure na Prática no YouTube também é uma excelente fonte de conteúdos, com gravações gratuitas incluindo mesas redondas, dicas e truques na utilização do Microsoft Azure:
www.youtube.com
Temos ainda o Canal .NET e o Coding Night, canais em que serviços do Microsoft Azure são abordados com frequência:
www.youtube.com
www.youtube.com
Uma iniciativa promovida anualmente pelo Canal .NET é o Azure Tech Nights, evento online e gratuito cobrindo diferentes tecnologias que integram a nuvem Microsoft. A edição 2020 aconteceu recentemente (Fevereiro a Abril), com os links da gravação de cada palestra podendo ser encontrados no seguinte post:
Azure Tech Nights 2020: saiba como foi - Vídeos Gratuitos
E os canais dos MVPs Vinicius Moura (Azure DevOps Sprints) e Julio Arruda:
www.youtube.com
www.youtube.com
No link a seguir reuni diversos conteúdos gratuitos (artigos, vídeos, exemplos) sobre Azure DevOps, incluindo a utilização desta solução em conjunto com tecnologias como Azure App Service/Web App for Containers, Docker, Kubernetes, Azure Container Registry e Azure Kubernetes Service (AKS):
Azure DevOps — Guia de Referência Gratuito
A seguir estão também diversos artigos e projetos de exemplo abordando diferentes serviços do Azure (há vídeos sendo referenciados em alguns destes posts):
Sobrevoando os serviços do Azure
Docker — Guia de Referência Gratuito
Kubernetes — Guia de Referência Gratuito
GitHub Actions — Guia de Referência Gratuito
ASP.NET Core + Application Insights: monitorando o uso de Dapper, Entity Framework e NHibernate
.NET Core + Serverless: melhorando a experiência de Desenvolvimento com Azure Functions 3.x | pt 1
.NET Core + Serverless: melhorando a experiência de Desenvolvimento com Azure Functions 3.x | pt 2
Mensageria + .NET Core 3.1: exemplos com RabbitMQ, Kafka, Azure Service Bus e Azure Queue Storage
Serverless + Azure Functions: Guia de Referência
Serverless é muito mais que apenas APIs REST!
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
Microservices: alternativas para a implementação no Microsoft Azure
ASP.NET Core + Azure App Configuration: manipulando configurações de forma mais inteligente
Como o Microsoft Azure pode simplificar a publicação de suas Web Apps?- Dica Rápida
GitHub + Azure App Service: deployment automatizado e sem complicações de Web Apps na nuvem
Application Insights + Logic Apps + Aplicações Web: enviando alertas de monitoramento via Slack
ASP.NET Core + Entity Framework Core: monitoramento descomplicado via Application Insights
ASP.NET Core + Application Insights: monitorando a disponibilidade dos BDs de uma aplicação
Blog do Azure na Prática
72 
72 claps
72 
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/monitorando-recursos-com-o-asp-net-core-3-0-health-checks-azure-logic-apps-e-o-slack-241bd61b8835?source=search_post---------227,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 14, 2019·8 min read
Em um post anterior demonstrei a implementação de uma aplicação Web voltada ao monitoramento de recursos de infraestrutura. Utilizei para isto o ASP.NET Core 2.2, Health Checks e diversos packages que integram o projeto open source Xabaril/AspNetCore.Diagnostics.HealthChecks. Dentre as tecnologias possíveis de serem acompanhadas e oferecidas por esta solução estão servidores do SQL Server, Oracle, PostgreSQL, MySQL, MongoDB e Elasticsearch, aplicações Web, projetos SignalR, brokers de mensageria baseados em RabbitMQ e Kafka, clusters do Kubernetes, diversos serviços do Microsoft Azure e AWS…
Aqueles interessados em conhecer mais a respeito deste exemplo podem fazê-lo acessando o link a seguir:
ASP.NET Core + Health Checks: implementando rapidamente uma solução de monitoramento
Neste novo artigo retomo esse projeto (SiteMonitoramento), fazendo agora uso de uma nova versão do mesmo criada com o ASP.NET Core 3.0 e já disponibilizada no GitHub:
ASP.NET Core 3.0 + Health Checks + Dashboard + Monitoramento de Recursos
A intenção desta vez é ir além, criando um Worker Service que consumirá um endpoint com os status dos diferentes Health Checks/dependências cadastrados em SiteMonitoramento. Worker Services são uma nova opção oferecida pelo .NET Core 3.0 e que contam com um template que simplifica a implementação de soluções como Windows Services e processos, valendo-se para isto de parte da infraestrutura e do runtime do ASP.NET Core.
Este Worker Service irá analisar periodicamente cada Health Check, enviando notificações de alerta para um canal em um grupo do Slack. Essa comunicação com o Slack ocorrerá por meio de uma Logic App criada no Azure. Logic Apps são um tipo de recurso baseado em workflows e disponibilizado na nuvem Microsoft, simplificando em muito a integração com diversos serviços corporativos: Microsoft Teams, SAP e Office 365 também constituem exemplos de tecnologias suportadas em uma Logic App.
E aproveito este espaço para um convite.
Dia 22/10/2019 (terça) a partir das 21:30 — horário de Brasília — teremos mais uma live no Canal .NET. Desta vez será abordado o uso da biblioteca Polly, solução esta que possibilita um melhor tratamento de falhas em projetos .NET e contribui assim para a obtenção de aplicações mais estáveis.
Para efetuar a sua inscrição acesse a página do evento no Meetup. A transmissão acontecerá via YouTube, em um link a ser divulgado em breve.
Conforme indicado anteriormente, a aplicação SiteMonitoramento faz uso de packages que compõem o projeto Xabaril/AspNetCore.Diagnostics.HealthChecks no monitoramento de recursos através de Health Checks. Na imagem a seguir está o dashboard disponibilizado por esta aplicação, com a checagem dos status de instâncias do SQL Server, Redis e MongoDB, um site publicado no Microsoft Azure, além de uma conta de armazenamento do Azure e do DocumentDB/Cosmos DB:
Para facilitar o consumo de informações envolvendo o status de Health Checks defini o endpoint /status-monitoramento na classe Startup, através de uma chamada ao método UseHealthChecks em Configure (a partir da linha 65). Ajustes foram realizados de maneira a formatar o resultado deste endpoint, com o retorno gerado contendo a identificação do endpoint, seu status e a descrição de um eventual erro; o resultado em questão foi serializado por meio da classe JsonSerializer (namespace System.Text.Json):
O resultado do endpoint /status-monitoramento pode ser observado na próxima listagem, com nenhuma das dependências apresentando qualquer tipo de problema:
Visando a integração com o Slack será criada uma nova Logic App no portal do Azure:
Informar um nome para a Logic App (AlertasMonitoramentoInfra-LogicApp neste exemplo), definindo ainda um Resource Group e uma Location. Confirmar este procedimento acionando o botão Create:
Concluído o processo de criação, a Logic App estará disponível para se configurar o workflow de integração com o Slack:
Para implementar o workflow acessar então a opção Logic app designer. Um vídeo estará disponível apresentando detalhes sobre este tipo de serviço:
Avançando com a barra de rolagem aparecerá a opção Start with a common trigger. Será necessário selecionar um trigger/gatilho. Para o tipo de solução que estamos implementando será escolhido When a HTTP request is received:
A primeira parte do workflow estará montada, conforme indicado na próxima imagem:
Acionar a opção Use sample payload to generate schema, a fim de definir um contrato com os campos contendo informações a serem recebidas pela Logic App (os dados em questão servirão de base para o envio de uma mensagem ao Slack):
Aparecerá agora o popup Enter or paste a sample JSON payload. Preencher o campo correspondente com o conteúdo JSON indicado na imagem a seguir, em que foram especificadas as propriedades dependency e message; concluir este processo clicando no botão Done:
Em Request Body JSON Schema constará a estrutura do objeto a ser recebido via solicitação HTTP do tipo POST pela Logic App. Um pouco abaixo deste trigger estará a opção + New step, a qual permitirá incluir a integração com o Slack como uma nova ação no workflow (através do assistente Choose an action):
O Azure oferece diversas ações e conectores pré-definidos (seção All) para a implementação de uma Logic App, como demonstrado na próxima imagem:
A pesquisa desta funcionalidade possibilitará encontrar as ações disponíveis para integração com o Slack. Selecionar para este exemplo o item Post message:
Na ação que possibilita a integração com o Slack aparecerá o botão Sign in, que deverá ser acionado para permitir a conexão com um grupo e um canal desta ferramenta de colaboração (o ideal para a realização deste procedimento é que já se tenha conectado anteriormente ao grupo do Slack):
Ao clicar em Sign in será exibida a tela a seguir, que solicitará a permissão de acesso da Logic App ao grupo do Slack; acionar então a opção Allow:
Um canal (Channel Name) e uma mensagem (Text Message) deverão ser especificados para o envio da notificação:
Preencher então o Channel Name:
E o campo Message Text, utilizando para isto os itens dependency e message disponíveis em Add dynamic content:
Concluir este procedimento acionando o botão Save. Neste momento um endereço terá sido gerado em HTTP POST URL; esta configuração servirá de base para o envio de mensagens ao Slack através do Worker Service de monitoramento:
O Visual Studio 2019 é uma das alternativas para a criação de projetos baseados no template Worker Service:
Temos ainda a possibilidade de gerar um projeto deste tipo via linha de comando (dotnet new), fazendo uso para isto da opção worker:
Já abordei anteriormente o uso de Worker Services com o .NET Core 3.0 no seguinte artigo:
Novidades do .NET Core 3.0: Worker Services
Nesta seção será detalhada a implementação de uma aplicação do tipo Worker Service chamado MonitoramentoInfraProcess. Os fontes deste projeto já foram disponibilizados no GitHub:
.NET Core 3.0 + Worker Service + Monitoramento de Recursos + Health Checks + Envio de Alertas para Azure Logic App/Slack
No arquivo appsettings.json estão os itens:
A classe StatusHealthCheck corresponde à representação dos dados com status dos Health Checks configurados em SiteMonitoramento:
Já na classe Worker (gerada quando da criação do projeto) está a implementação do processo de monitoramento:
Supondo que as instâncias do Redis e do MongoDB não estejam no ar, como indicado na imagem a seguir:
E também no endpoint a ser consumido pelo Worker Service:
Ao executar o Worker Service de monitoramento será possível visualizar que notificações foram enviadas ao Slack:
E dentro do canal monitoramento no Slack estarão as mensagens com os Health Checks e seus respectivos erros:
.NET Core 3.0 e ASP.NET Core 3.0: Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
78 
78 
78 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/asp-net-core-2-1-protegendo-uma-api-rest-com-jwt-identity-core-e-azure-key-vault-6a80d94ceb63?source=search_post---------228,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Oct 22, 2018·2 min read
Recentemente (09/10/2018) realizei uma apresentação em um meetup promovido pelo grupo Azure Brasil e que abordou a implementação de APIs REST seguras na nuvem. Os slides que utilizei estão disponíveis no SlideShare:
Um dos exemplos que abordei nesta apresentação era uma API REST implementada em ASP.NET Core 2.1 e que fazia uso do SQL Server/Azure SQL, Entity Framework Core, Entity Framework Core InMemory, Identity Core, JWT (JSON Web Token), Bearer Authentication e Azure Key Vault. O projeto em questão já foi inclusive disponibilizado no GitHub:
https://github.com/renatogroffe/ASPNETCore2.1_JWT-Identity-Azure_Key_Vault
Este mesmo exemplo foi descrito em detalhes durante a demonstração prática na palestra que realizei durante o Azure Tech Nights 2018. A gravação se encontra no YouTube e pode ser assistida gratuitamente (todos os passos para configuração e uso do Azure Key Vault foram detalhados neste vídeo):
O uso de tokens em APIs REST é um tema recorrente entre os conteúdos que venho produzindo, sendo que diversas informações sobre a adoção de JWT (JSON Web Tokens), Bearer Authentication e o ASP.NET Core Identity podem ser encontradas nos seguintes posts:
ASP.NET Core + JWT: Guia de Referência
ASP.NET Core 2.0: JWT + Identity Core na autenticação de APIs
Quanto ao Azure Key Vault, este serviço que integra a plataforma de cloud computing da Microsoft permite o armazenamento, o gerenciamento e o controle de acesso a configurações consideradas sensíveis em um projeto. Tais segredos ficarão hospedados e vinculados a um recurso criado na nuvem, estando acessíveis apenas para aplicações devidamente registradas via Azure Active Directory.
Todo o processo de configuração para uso do Azure Key Vault está indicado no vídeo mencionado neste post. No projeto exemplo específico aqui apresentado uma string de conexão ficará armazenada em um recurso do Key Vault, sendo necessário indicar no arquivo appsettings.json:
Alterações também serão realizadas na classe Program e Startup para que o projeto se integre ao Azure Key Vault.
.NET Core 2.1 e ASP.NET Core 2.1: Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
22 
22 
22 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/georgian-impact-blog/comparing-google-cloud-platform-aws-and-azure-d4a52a3adbd2?source=search_post---------229,"There are currently no responses for this story.
Be the first to respond.
A starting point to help you choose the right platform for your ML project.
By Jing Zhang
If you’re looking for an end-to-end machine learning (ML) platform, you’re spoiled for choice. There are three main choices for cloud providers: Google Cloud Platform (GCP), Amazon Web Service (AWS) and Microsoft Azure Platform (Azure). The question is: how do you choose between the three? What functionality do they provide to build ML pipelines? We set out to answer these questions in a recent hackathon.
The R&D team at Georgian, where I work as an ML Engineer, decided to organize a hackathon to explore how each provider can help with the workflow. Our team builds machine learning software components ourselves, but we’re typically working with the growth-stage software companies in the Georgian family to deploy, so we wanted to familiarize ourselves with the different platforms and to be able to adapt to their workflows faster.
In this blog post, we’ll give a high-level overview of the ML platform solutions provided by GCP, AWS and Azure based on our experience during the hackathon. This list is not exhaustive but shows the results of our hackathon research.
Specifically we’ll cover:
We hope this can offer help as a starting point to help you choose the right platform for your ML project.
To help compare the different platforms, we chose a project that we could run on all three so we could compare apples to apples. Our project was to build a binary classification model with a given dataset and an end-to-end pipeline on a cloud provider. We used a dataset of companies that we use for our own machine learning platform, Spring. Spring helps us identify companies that fit our investment profile.The dataset contained general information about companies including their management team, financial information and office locations. The goal was to identify whether a company fit well into our investment profile (labeled 1) or not (labeled 0).
Since we used our own data for this project, we can’t share the specifics here, but to give you a sense what we were working with, here’s an overview of the dataset:
We also intentionally injected noise and error in the dataset so that we could run some tests on the data cleaning functionality. Specifically, we introduced:
One of the goals of this hackathon was to explore and assess each aspect of the end-to-end ML pipeline shown in the figure below. We decided to approach the challenge this way so that we would be able to assess the needs of a given project against the providers. For example, if explainability is important to a project, which is the best choice? Or, which provider performs the best if you want to run heavy testing?
Specifically, we wanted to assess each platform’s functionality in these areas:
Preprocess
Discover
Develop
Train
Test & Analyze
Deploy
Since we were building a model, we needed to think about model performance. The metric we tracked was Area Under the Receiver Operating Characteristic Curve (ROC_AUC). We weren’t, however, using model performance to make a judgment on which platform was better, so long as the results were roughly comparable.
Fairness in ML has emerged as an enormous issue area for policy makers, industry and the public. It’s important to have tools at your disposal to verify that your model is not treating any group and individual unfairly. To be able to address fairness, explainability tools are important too. These tools allow you to query why a model reached a certain decision. We were looking for the availability of the tools rather than assessing their relative performance.
Unlimited budgets are nice to dream about, but the reality is that cost is a constraint we should always keep in mind, so we tracked the costs for each provider.
As you would expect with these three platforms, there is a product or service for each step in the ML pipeline. The differences between them are in how well the services integrate with each other to build an end-to-end ML pipeline experience.
We have summarized the available services in the areas we’re interested in the table below. Going into detail is beyond the scope of this blog post. You can use this summary table as a starting point to see what’s available at each step for each provider.
For our team, the most important features are the model development and model deployment sections and, to a lesser extent, data visualization and QA. We were glad to see that all three providers have hosted notebook services, experiment tracking and version control, and easy deployment methods.
As for AutoML, all three providers have developed their own offerings. We used it to build our initial models and check whether there were valuable signals in the dataset. It would be great if the explainability component could be integrated so we could understand how the models are built for further model analysis and development.
Speaking of model explainability, while all three platforms all provide some tools, the functionality varies. If you have specific requirements, make sure you check if the current functionality satisfies your needs.
GCP uses a package called “the what-if tool”. You can integrate it with your notebook and play with the model by changing threshold or feature value for a given example. This allows you to check how certain changes affect the prediction result.
If you are using the Sagemaker debugger, it allows you to analyze how feature engineering and model tuning are done.
Azure provides a built-in module in their SDK, which seems to have the best integration.
So how did the models perform? As measured by ROC_AUC, performance was comparable across all three platforms. Azure and GCP scored slightly higher than AWS. This doesn’t necessarily mean one platform is better than another. It matched our expectation that the scores vary but should be close.
Cost, on the other hand, was more interesting. The cost on AWS was considerably lower than GCP and Azure. Please note this isn’t strictly an apples-to-apples comparison. The Azure team on our hackathon explored more services since Azure was a completely new platform to our team and we wanted to use the opportunity to learn more about it, so that may account for some of the cost difference.
One question we asked was: “Is it worth spending 4 to 6 times of money to get a 5% performance improvement?” The answer depends on the problem you’re addressing. For example, we’re looking for companies that could potentially generate large returns for our fund, so it may be worth spending the extra few hundred dollars. If you have budget concerns and the spending doesn’t justify the return, then it’s a different story.
Based on our observations, all three cloud providers cover the aspects of the ML workflow we care about.
Two hot topics in the industry right now are the rise of AutoML and the need for an end-to-end machine learning workflow in one place to provide a frictionless experience. As we mentioned earlier, AutoML products are already available on all the platforms but the end-to-end pipeline experience doesn’t seem to be mature yet.
On GCP and AWS, you’ll need to assemble multiple products together to get to the desired outcome. Azure, on the other hand, provides a machine learning designer service with drag and drop UI. This might imply that they are targeting different customer bases.
With the drag and drop UI interface, Azure’s machine learning designer may be more friendly to those new to data science, with little coding and technical background, to try out machine learning projects and evaluate whether it brings value to the business. Many corporations already use Microsoft products, so it might be an easy choice for teams at larger companies who are looking to try machine learning for the first time.
AWS and GCP seem to be more developer-focused. Though it’s a little more work to assemble a pipeline, they’re more customizable with the different components available. These components and the connection of the pipeline are often developed through code and configurations rather than an user interface. Companies who are more familiar with the options and know what they want to achieve may prefer this option.
We certainly learned a lot from this Georgian hackathon and it puts us in a better position to undertake projects on any of these three platforms in future. We hope that this is useful to you and helps you pick the right platform to start your next project.
A blog focused on machine learning and artificial…
139 
139 claps
139 
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
Written by
Investors in high-growth business software companies across North America. Applied artificial intelligence, security and privacy, and conversational AI.
A blog focused on machine learning and artificial intelligence from the Georgian R&D team
"
https://medium.com/@renatogroffe/apis-seguras-com-asp-net-core-jwt-docker-e-azure-azure-talks-junho-2016-3c80f0c08ab3?source=search_post---------230,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 25, 2018·3 min read
No dia 19/06/2018 (terça) participei como palestrante e organizador de mais um evento presencial do Azure Talks (meetup do qual sou coordenador, juntamente com o Luigi Tavolaro).
Desta vez realizei uma apresentação sobre o desenvolvimento seguro de APIs REST na nuvem, utilizando para isto as seguintes tecnologias:
Este evento aconteceu no auditório do Bradesco Inovabra Habitat, contando ainda com outras apresentações sobre tecnologias oferecidas pelo Microsoft Azure ao longo da noite de terça.
Gostaria de deixar neste post meu muito obrigado ao Rodrigo Guerra (Microsoft) por todo o suporte para a realização deste meetup. E também ao Edson Vieira, ao Milton Câmara Gomes e ao Ewerton Rodrigues Jordão pelas fotos tiradas durante as apresentações.
Os slides da apresentação já estão no SlideShare:
A aplicação utilizada como base para a demonstração também foi disponibilizada no GitHub:
https://github.com/renatogroffe/ASPNETCore2.1_JWT-Identity-Docker
Caso desejem ainda outras referências sobre o uso de Docker com .NET Core/ASP.NET Core acessem também o seguinte post:
Docker para Desenvolvedores .NET - Guia de Referência
O uso do Azure Web App for Containers foi também assunto de uma apresentação recente que fiz no Canal .NET. A gravação está disponível no YouTube e pode ser assistida gratuitamente:
Se deseja saber mais sobre o suporte a HTTPS no ASP.NET Core 2.1 (ativado por default) consulte também o seguinte artigo:
ASP.NET Core 2.1: instalando o Preview 1 e suporte a HTTPS
Referências adicionais sobre o uso de JWT com ASP.NET Core podem ser encontradas no seguinte post que escrevi recentemente (inclui exemplos com implementações customizadas de controle de acesso, ASP.NET Core Identity, Refresh Tokens e consumo de APIs que empreguem JWT):
ASP.NET Core + JWT: Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
40 
1
40 
40 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-8db4be7d1c6a?source=search_post---------231,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 19, 2020·1 min read
4-Parte: Criação da primeira rota do projeto
Dando continuidade a liberação dos módulos do meu curso: Criando API’s RESTful utilizando TypeScript, Node e mongoDB, hoje eu irei demonstrar como criar a primeira rota do nosso projeto.
Caso tenha interesse em ver os primeiros passos dessa serie, segue link de cada um deles abaixo:
Criação da primeira rota do projeto
Nessa vídeo aula nós vimos como criar a nossa primeira rota e as configurações iniciais para rodarmos o projeto em um ambiente local.
Espero que tenham gostado e até um próximo artigo pessoal ;)
Enjoy your life
132 
132 
132 
Aqui você vai encontrar os principais conteúdos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/windows-developer/develop-a-mind-reading-twitter-client-with-azure-cognitive-services-f73f0c937671?source=search_post---------232,"There are currently no responses for this story.
Be the first to respond.
Adding machine learning, artificial intelligence and cognitive abilities to your Windows applications is easier than ever, thanks to Microsoft Azure services. Imagine a Twitter client that practically checks the user’s state of mind before tweeting, potentially saving them an embarrassing tweet made in anger. With the Emotion and Text Analytics APIs from Azure Cognitive Services, not only is this possible, it’s easy to do, analyzing both the text of a tweet and the user’s facial expressions, before the send button is clicked.
Azure Cognitive Services includes a portfolio of machine learning powered APIs that fall into five primary areas:
· Vision — Image/video processing and classification services, etc.
· Speech — Speech-to-text, voice verification, etc.
· Language — Natural language processing including translations, linguistics, spell check, etc.
· Knowledge — Recommendation systems, academic knowledge, etc.
· Search — Bing Search API that covers web, media, news, etc.
To access these services, API keys are necessary:
1. Navigate to the Cognitive Services API Console.
2. Login or create a free account.
3. Select the Vision tab.
4. Locate the row for Emotion API and click Get API Key button.
5. Select the Language tab.
6. Locate the row for Text Analytics API and click the Get API Key button.
7. Make a note of the endpoint and API keys.
To get started creating the Safe Tweet app, follow these simple steps to set up your project:
· Open Visual Studio 2017
· Choose File -> New -> Project from the main menu
· Select Windows Universal from the left navigation pane
· Select Blank App (Universal Windows) from the right pane
· Enter the name of your app in the Name field (we’re using “SafeTweet”)
· Enter a Location, or keep the default
· Select the OK button
After creating the initial project, Visual Studio will prompt you to select the target and minimum platform versions.
· Ensure the Target version is set to Windows 10 Fall Creators Update (10.0; Build 16299)
· Set the Minimum version to Windows 10 Fall Creators Update (10.0; Build 16299)
We’re setting both to the Windows 10 Fall Creators Update because some of the packages we’re using have a dependency on .Net Standard 2.0.
The Text Analytics API is a great algorithm for determining the overall sentiment of a block of text to determine its positivity or negativity.
Microsoft provides a package via NuGet that simplifies integrating the Text Analytics API into a UWP app. Install the NuGet package from the NuGet console:
With this package installed into the project, a piece of text can be analyzed with the following steps:
1. Create an instance of the SentimentClient class using the API key previously obtained for the Text Analytics API in the Cognitive Service Console.
2. Create a list of SentimentDocument objects containing the text to be analyzed.
3. Call the GetSentimentAsync() method providing the list of SentimentDocument objects as a parameter.
4. Process the collection of Document objects returned from GetSentimentAsync().
Note, SentimentDocument contains an Id property that correlates a request to the corresponding response. In the sample Twitter client app, the Id happens to be unnecessary since only one line of text is being sent for analysis.
The Emotion API uses facial expression recognition to detect eight different emotions: anger, contempt, disgust, fear, happiness, neutral, sadness and surprise. This algorithm can detect multiple faces in a single image and provide confidence measurements for each emotion for each face.
Microsoft provides a package via NuGet that simplifies integrating the Emotion API in UWP Apps. Install the NuGet package from the NuGet console:
After installing this package, analyze the emotions in an image with the following steps:
1. Create an instance of the EmotionServiceClient class using the API key previously obtained for the Emotion API in the Cognitive Service Console.
2. Call the RecognizeAsync() method passing in the image itself as a Stream, or as a String that represents a Url to the image.
3. Process the collection of Emotion objects returned from RecognizeAsync().
The following code demonstrates these steps:
Note: The EmotionServiceClient class implements IDisposable, and therefore the instance needs to be disposed. This is different behavior than the SentimentClient class from the Text Analytics API.
Processing the collection of the returned Emotion objects is also straightforward. For every face detected in the image, the collection will contain a corresponding Emotion object. Each Emotion object corresponds to a face recognized in the image and includes:
· A Rectangle object with the coordinates and size of the face found
· An EmotionScores object, with a numeric score ranging from 0 to 1 for each emotion detected (the sum of which will equal 1).
The ToRankedList() helper method from the EmotionScores object can be used to rank the scores for easier consumption.
The following code demonstrates consuming the Emotions API and determines the primary emotion:
The above Emotion API sample uses a Stream of the image to analyze. There are several methods in the UWP to obtain an image — reading it from disk or directly from a web cam. For the Safe Tweet app, reading directly from the web cam would create a seamless user experience. A good starting place to learn more about using the web cam in a UWP app can be found here.
Since UWP are sandboxed and have restricted rights, developers need to request access to specific capabilities:
· In Visual Studio, open the project manifest and select the Capabilities tab.
· Select the Webcam capability.
The MediaCapture class is a mechanism used to capture video and photos from a webcam and provides a more seamless approach than using the CameraCaptureUI class (which simply uses the Windows built-in camera app).
Before using the MediaCapture class, it must be initialized, as follows:
Note, failure to set the capture mode to StreamingCaptureMode.Video value, MediaCapture will default to both video and audio and will require adding the Microphone capability in the project manifest.
Once MediaCapture is initialized it can be used to take a photo as follows:
Converting the captured photo to a Stream that the Emotion API can use is a little more involved:
While not part of the Cognitive Services API, the sample app wouldn’t be complete without demonstrating how to connect to Twitter and post the tweet.
Start by visiting https://apps.twitter.com/app/ to create and register the app.
Note: The callback URL is optional and does not need to be provided.
After submitting the above form, click the Create my access token button and select the level of access needed (read only, read and write, or read, write and access direct messages). For this app, select Read and Write.
Once completed, the Twitter registration page will present a summary of the app’s setting including the consumer key, the consumer secret as well as the access token and access token secret.
This consumer key and consumer secret will be needed to connect the UWP app to the Twitter infrastructure (note: the consumer secret should never be human-readable in your app or checked into source code repositories).
Back in Visual Studio, install the UWP Community Toolkit Services NuGet package.
This package not only allows developers to easily integrate with Twitter, but many other services including Bing, Facebook, LinkedIn, OneDrive, and more.
The Twitter Service provides a rich API allowing consuming apps to tweet, retrieve the user profile or timeline, or live-stream tweets. The following snippet shows how to connect to the Twitter service and send a text-based tweet.
There may come a day when most apps will perform cognitive tasks previously thought only possible by humans. Saving us from ourselves may just be a small component of the larger set of capabilities available with a few lines of code and Azure Cognitive Services.
We’ve only scratched the surface of what is possible in a UWP app consuming Azure Cognitive Services and leveraging the UWP Community Toolkit. Check out the links below to explore more opportunities.
· Find sample source code on GitHub.
· Documentation
· Source
· Text Analytics Documentation
· Emotions Documentation
· 20-minute introduction video
· Check out the hard-hitting online training series on Microsoft Cognitive Services at Microsoft Virtual Academy
· Where would you use Cognitive Services? In Games, Mixed reality apps or maybe in beautiful Fluent apps.
Learn more about developing for Windows 10 here.
Everything you need to know to develop great apps, games…
60 
60 claps
60 
Everything you need to know to develop great apps, games and other experiences for Windows on PCs, phones, IoT, Xbox & HoloLens. wndw.ms/dev
Written by
Everything you need to know to develop great apps, games and other experiences for Windows.
Everything you need to know to develop great apps, games and other experiences for Windows on PCs, phones, IoT, Xbox & HoloLens. wndw.ms/dev
"
https://medium.com/house-of-haiku/azure-9fae0b67ad9?source=search_post---------233,"There are currently no responses for this story.
Be the first to respond.
Frictionless azure,dangling sparrows in time, are,prism refraction,infused color, spins madly, define blue synonyms, such.
Anna Rozwadowska 2020
"
https://medium.com/@jaychapel/7-ways-to-get-azure-credits-2df14d3a228f?source=search_post---------234,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Feb 18, 2020·4 min read
Azure credits are a perk offered by Microsoft that help you save money on your cloud bill. Like a gift card for a retail store, credits are applied to your account to help cover costs until they are exhausted or expire. In a sense, these credits act as a spending limit because any usage of resources or products that are not free will be deducted from the credit amount. We found 7 different ways that you can earn credits and start saving on your Azure bill.
If you’re a Visual Studio subscriber, you get monthly Azure credits that can be used to explore and test out different Azure services. The amount of Azure credits that you receive will depend on the type of Visual Studio subscription that you have.
With a Visual Studio Enterprise subscription, you get a standard of $150 in monthly credits. For subscriptions through MSDN Platforms you get $100 a month. For Visual Studio Professional and Visual Studio Test Professional, you get $50 a month.
Full-time students at an accredited, two or four-year educational institution in a STEM-related field are eligible for these credits.
When a student signs up with their school email address, Microsoft gives them $100 in credit in order to help them further their career and build their skills in Azure thanks to the free access to learning paths, labs, and professional developer tools.
With a free account, you get access to a number of popular Azure services for no cost. In addition to access to free services, you’ll also get a $200 credit. It’s important to note that while the free account lasts for 12 months, your credits must be spent in the first 30 days.
Whether you’re just getting started in Azure or are looking to further your knowledge, a free account is always a great way to test the waters without having to make a long term commitment.
In the Partner Network, those that are members of Microsoft’s Action Pack program receive $100 of Azure credits every month. Based on your computing needs, you can use these credits for any Azure service; some examples include, Virtual Machines, Web Sites, Cloud Services, Mobile Services, Storage, SQL Database, Content Delivery Network, HDInsight, Media Services, and more.
The great part about this is that there are a handful of usage scenarios that won’t consume all of the $100 credit — you can use this pricing calculator to estimate how much you could use with a $100 credit.
Any of the unused monthly credits can’t be carried over to succeeding months or transferred to other Azure subscriptions, so make sure to use it while you can!
This global program is designed to help startups as they build and scale their organizations. Part of the technical enablement features that are always free and available to all startups is $200 of Azure credits that can be used towards any service for 30 days. This is a great option for startups because it’s free and gives you the ability to explore all the different offerings without having to spend any money.
With Azure for Education, users are given access to the learning resources and developer tools that educators and students need in order to build cloud-based skills. This program is available to students, educators and institutions — once signed up, educators get $200 of Azure credits.
Whether you’re teaching advanced workloads, interested in building cloud-based skills, or just getting started in your Azure learning journey, this program provides guidance and resources for individuals looking to further their knowledge in Azure.
In an effort to make their technology more affordable and accessible for nonprofit and nongovernmental organizations, Microsoft offers donated and discounted products. Each year, approved organizations receive $3,500 in Azure credits which can be used to purchase all Azure workloads created by Microsoft (excluding Azure Active Directory, which is licensed under EM+S).
No matter the industry you’re in or learning level you’re at, there are a wide variety of credits and resources offered that can help make Azure an affordable option for you.
Further Reading:
Top 3 Ways to Save Money on Azure
How to Save Money with Microsoft Azure Enterprise Agreements
9 Ways to Get AWS Credits
Originally published at www.parkmycloud.com on February 11, 2020.
CEO of ParkMyCloud
52 
1
52 
52 
1
CEO of ParkMyCloud
"
https://medium.com/@gmusumeci/how-to-create-an-azure-remote-backend-for-terraform-67cce5da1520?source=search_post---------235,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 24, 2020·5 min read
For simple test scripts or for development, a local state file will work. However, if we are working in a team, deploying our infrastructure from a CI/CD tool or developing a Terraform using multiple layers, we need to store the state file in a remote backend and lock the file to avoid mistakes or damage the existing infrastructure.
"
https://koukia.ca/introduction-to-azure-event-grid-87b058408c70?source=search_post---------236,"Azure Event Grid is a managed event routing platform which enables you to react in real-time to changes that are happening in your applications hosted in Azure or any Azure resources that you own.
Today to get notified of a stage change in a resource, such as a new record in a database, or when someone creates a Virtual Machine…
"
https://medium.com/@maarten.goet/supercharge-your-powershell-defenses-with-azure-sentinel-mitre-att-ck-and-sigma-714e1e1825d3?source=search_post---------237,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
May 6, 2019·8 min read
Jeff White from Palo Alto Networks recently wrote: PowerShell has continued to gain in popularity over the past few years as the framework continues to mature, so it’s no surprise we’re seeing it in more attacks. PowerShell offers attackers a wide range of capabilities natively on the system and with a quick look at the landscape of malicious PowerShell tools flooding out; you have a decent indicator of its growth.
Enter stage left — the PowerShell ‘-EncodedCommand’ parameter. This command intends to take complex strings that may otherwise cause issues for the command-line and wrap them up for PowerShell to execute. By masking the “malicious” part of your command from prying eyes you can avoid strings that may tip-off the defense.
How can we detect this? MITRE’s ATT&CK framework, Sigma’s open source project and Azure Sentinel can be teamed up to supercharge your defenses against these types of attacks. Let’s look at how we can do this.
Malicious PowerShell usage
Quite a number of droppers use these Base64 encoded PowerShell commands. For instance, they might try and abuse a Word document through a macro that will try and run cmd.exe which in turn runs PowerShell.exe which then parses an encoded command starting with a $.
Here’s a sample PowerShell script for you to try out encoding some sample command yourself:
PRO TIP: My good friend and PowerShell MVP Ben Gelens pointed me at a Microsoft blog here that describes how PowerShell 5+ now supports the “Blue Team” better.
MITRE ATT&CK
One of the first things you might ask yourself is how do I know which tactics, techniques and procedures (TTP’s) my adversaries are using? Unless you’ve been involved with research on bad actors and/or have been working on the “Red” (offensive) side yourself, you might not have all the information available on what people are using to attack you.
This is why the InfoSec community is about sharing information. Each might have a piece of the puzzle, and putting them together will provide all of us with the bigger picture, and will allow us to up our defenses in a bigger way. There are numerous conferences, blogs and other ways to get those learnings, but the MITRE ATT&CK project is one well worth mentioning.
What is MITRE? They describe themselves as:
“MITRE ATT&CK is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations.”
It’s a community project where many people with hands-on experience contribute. For instance Christiaan Beek, lead at McAfee, shares his learnings on TTP’s actively with MITRE.
The information MITRE ATT&CK provides can be consumed in many ways. They provide website which allows you to search the information, there is a ‘attack navigator’ that gives you an interactive way to work with the TTP’s, and there is a programmatic way (API) to retrieve the information as well.
If we query the ATT&CK framework, we find a TTP with number T1086 called “PowerShell”. MITRE provides information about the attack vector, which APT groups typically use it, and information on which phase of the ‘kill chain’ it maps to: Execution.
Two paragraphs in the TTP information MITRE provides could be of interest to you: Mitigation (how to defend against it) and Detection (monitoring, and possibly hunting):
“[..] If proper execution policy is set, adversaries will likely be able to define their own execution policy if they obtain administrator or system access, either through the Registry or at the command line. This change in policy on a system may be a way to detect malicious use of PowerShell. [..]”
Sigma
So you’ve learned about the TTP’s that your adversaries might use. But we still need to ‘translate’ this approach to some real detection logic for our SIEM system. We need to provide the query the alert rules will use, and/or need to capture the relevant data to use for hunting later on.
Again, if you have been working on the offensive side, you might already have this information readily available. But most defenders will need to investigate the attack vector and come up with a plan as to which data sources to monitor, what to look for, and how the alerting logic would need to be configured.
And if you do find out what to monitor for, you’ll be most likely configuring it directly into your SIEM configuration and maybe not first write down the abstract of what you’re trying to achieve. (This is also the reason that replacing SIEM system is tough; no history of why something is configured there in the first place — but that’s a topic for another blog).
Florian Roth started a community project called Sigma which aims to provide a Generic Signature Format for SIEM systems. The detection logic is written in YAML format, and these .yml files can then be converted/translated into queries or rules for your specific SIEM systems (fi: Azure Sentinel, ArcSight, etc).
“Sigma is for log files what Snort is for network traffic and YARA is for files.”
The Sigma project has fully embraced MITRE’s ATT&CK framework as a way to classify the attack vector. If you look at the rule specification, in the tags section you’ll find fields called “attack.<number>” and “attack.<tactic>”. For our PowerShell TTP this would translate to attack.t1086 and attack.execution.
On Sigma’s Github repository you’ll find a folder called ‘Rules’ that contain quite a number of Sigma rules that Florian, other members of this project, or the broader community have developed and contributed. Under /rules/windows/powershell you’ll find a lot of rules for detecting PowerShell attack vectors.
PRO TIP: Interested in what MITRE ATT&CK coverage the public Sigma rules repo covers? Florian posted an overview here.
There are many ‘implementations’ of abusing PowerShell, but the one we were looking for is the one that uses Base64 encoded commands. The Sigma rule/definition for this one can be found here. A big thank you to Florian Roth and Markus Neis for the research and for sharing it to the Sigma repo.
PRO TIP: Want to get some technical details of Markus’ and Florian’s research? In the YAML file they point to this URL which contains some great information!
Azure Sentinel
Now that we have the Sigma rule, you will want to put that detection logic into your Cloud SIEM, Azure Sentinel. However, we need to take a final step: converting that YAML file into something that Azure Sentinel can read/process.
Guess what, Florian and his team also thought of that! It’s called SIGMAC as in Sigma Converter. In the /tools folder of the GitHub repo you will find it as a Python script. The scripts can output to a couple of common SIEM formats, fi: Splunk, Kibana, Arcsight and one that is called “ala”.
The target called ‘ala’ stands for Azure Log Analytics. This is the correct one, as Azure Sentinel uses Azure Log Analytics as its backend as you could read in my Azure Sentinel Design Considerations blog. Choosing ‘ala’ as the target in the Sigma Converter will produce the right KQL query as the result:
If you want to know more about Event ID 4688 which this signature uses, you can have a look on UltimateWindowsSecurity.com and get detailed information.
PRO TIP: Don’t forget to check the basics — am I collecting the events from the security eventlogs on my Windows servers? Is auditing properly enabled and configured in the OS? Are my Microsoft Monitoring Agents healthy and reporting into Azure Sentinel?
Putting it all together
Open up Azure Sentinel in the Azure portal and click on ‘Analytics’. Select ‘Add’ to create a new rule. Provide a name, description, severity, and paste in the KQL query. In the Alert Simulation section you’ll see if the query would have triggered.
PRO TIP: Make sure you select the right fields in the Entity Mapping section because you will need this later for Hunting. Then select the alert scheduling and alert suppression parameters, and Save the rule.
Now sit back and relax, and wait for the rule to kick in when a malicious Powershell command is run in your environment:
Because you’ve used the Entity Mapping, the alert maps back to the corresponding server, IP address, etcetera, which will assist you in Hunting. Not sure how to do this? Read my blog on using Jupyter and KQL to go Threat Hunting with Azure Sentinel.
Summary
Having a common open framework and taxonomy around TTP’s such as MITRE ATT&CK helps a great deal to organize our defense efforts. Combining that with a generic SIEM format to define the logic needed to detect these TTP’s such as SIGMA is really helpful. Having a community that actively contributes to these SIGMA rules is probably the best thing since sliced bread!
Downloading the alert rules and converting them to be used in Azure Sentinel has never been easier thanks to the community efforts of MITRE and SIGMA. Supercharge your cloud SIEM today!
A big shout out to Florian Roth (SIGMA), Jeff White (Powershell), Markus Neis (Alert Rule), Christiaan Beek (TTP),the people behind MITRE and many many others in the community who generously provide this to all of us!
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
41 
41 claps
41 
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jeffhollan/using-entity-framework-with-azure-functions-a32d09382b94?source=search_post---------238,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
May 28, 2019·4 min read
One of the exciting developments from Build this year was support for dependency injection in Azure Functions. This means you can register and use your own services as part of functions. While you’ve been able to use Entity Framework Core in the past, the pairing with dependency injection makes it a much more natural fit. Let’s create a simple Azure Function that can interact with stateful data using Entity Framework Core. I’m going to create a very simple API that can get and set blog data in an Azure SQL Database.
First up, I created a brand new v2 (.NET Core 2) Azure Functions project. I selected the HTTP Trigger template to start with. In order to use dependency injection I need to verify two things:
While we’re at it, let’s install the Entity Framework libraries. ⚠️ These versions are very particular with the version of .NET you will be running. So choose your version of these libraries thoughtfully. At the time of writing this Azure is running .NET Core 2.2.3 so that’s what I’ll use.
I also want to take advantage of the design-time migration, so I’ll install the tools needed to drive that.
Next, I’ll create a very simple DbContext for the data models I want to interact with. Let's just stick with the simple Blog and Posts entities.
I need to verify that I’m using constructors in my function classes so I can inject in the right dependencies (in this case, the entity framework context). This means getting rid of static and adding a constructor. I’ll also modify the constructor to accept a BloggingContext created above (which itself will need a DbContextOptions<BloggingContext> injected).
To register services like BloggingContext, I create a Startup.cs file in the project and implement the FunctionsStartup configuration. Notice I use an attribute to signal to the functions host where it can run my configuration.
As mentioned, I want to use the design-time DbContext creation. Since I’m not using ASP.NET directly here, but implementing the Azure Functions Configure method, entity framework won't automatically discover the desired DbContext. So I'll make sure to implement an IDesignTimeDbContextFactory to drive the tooling.
There’s one last thing I need to do. When you build an Azure Functions project, Microsoft.NET.Sdk.Functions does some organizing of the build artifacts to create a valid function project structure. One of the .dlls it moves to a sub-folder is the project .dll. Unfortunately, for the design-time tools like entity framework migration, it expects the .dll to be at the root of the build target. So to make sure both tools are happy, I'll add a quick post-build event to copy the .dll to the root as well. I added this to my .csproj file for the project.
I went into the Azure Portal and created a simple Azure SQL database. I copied the connection string into the local.settings.json file with a new value for SqlConnectionString. You can see in my previous code samples I used that as the environment variable that would have the connection string. This way I don't have to check it into source control 🥽.
To make sure the connection string is also available to the CLI tooling for migrations, I’ll open my Package Manager Console and set the environment variable before adding a migration and updating the database.
When browsing to the SQL database in the Azure Portal, I can see the tables were successfully created!
That’s all the heavy lifting. Now I just write a few simple HTTP triggered functions that can return and add data to the BloggerContext I injected in. When running the app, I can then call the different GET and PUT methods to validate data is being persisted and returned correctly.
After publishing, I just make sure to set the appropriate application setting for SqlConnectionString with my production SQL connection string, and we're in business!
You can see the full project code on my GitHub account
Originally published at https://dev.to on May 28, 2019.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
87 
2
87 
87 
2
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
"
https://koukia.ca/faster-build-and-deploy-by-using-kubernetes-and-azure-dev-spaces-within-visual-studio-bc850ecd9810?source=search_post---------239,"One of the issues that you will have while working with container images and Kubernetes, is the build process when you are coding.
Like when you have everything running in a Kubernetes Cluster in Azure and you just want to make some changes and push it to the cluster, normally you would have to compile the code, build…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/bootstrapping-azure-cloud-to-your-terraform-ci-cd-f06ddd951f69?source=search_post---------240,"There are currently no responses for this story.
Be the first to respond.
Pairing Terraform with a CI/CD like Azure DevOps, Terraform Cloud, or GitHub Actions can be incredibly empowering. Your team can work on code simultaneously, check it into a central repo, and once code is approved it can be pushed out by your CI/CD and turned into resources in the cloud.
"
https://medium.com/@jeffhollan/serverless-doorbell-ring-com-and-azure-functions-part-2-98bc8fb43e3c?source=search_post---------241,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Dec 21, 2017·5 min read
This is part 2 in a series of how I have been using Azure Serverless to extend and connect the functionality of my ring.com video doorbell. Part 1 is here.
During my first afternoon I got my doorbell sending events to Azure Event Grid whenever it detected motion or was pressed. The next step was to create subscribers to those events. At the time of writing this I’ve created two subscribers — create a log entry in Cosmos DB, and analyze the recording with Azure Cognitive Services. I’ll save the Cognitive Services for part 3. This blog will go into detail of wiring-up Event Grid to Azure Functions, and how to use Cosmos DB bindings in the new v2 runtime to create a document.
This was my first opportunity to build a function trigger from Event Grid and I learned a lot in the process. The first is that you actually have 2 options for triggers when working with event grid.
The first option is to use the Event Grid extension with the eventGridTrigger type in your function.json file. This is the method that is used if you select the Event Grid template in the Azure Functions portal experience. There are a few aspects worth calling out with this method:
You can also just use and manually register any HTTP triggered function to fire on events from Event Grid. The difference is pretty much the counter to each of the points above (requires manual debatch, wouldn’t leverage queues in the same way, but can locally test).
For purposes of easier local debugging I went with an HttpTrigger function which will create a document in Cosmos DB about the ring event.
Azure Functions has a concept in addition to triggers called ‘bindings.’ A binding is a small bit of standard code to allow for faster integration with different services. Bindings include things like Azure Storage (pull in or push out blobs), Azure Queues, Cosmos DB, and about 20 more.
For my ring.com Azure Functions, I have been developing them in VS Code on my MacBook, which means I’ve been using the current preview (and cross-platform) v2 Function Runtime. This is relevant because in v2 a few changes happened in relation to the Cosmos DB (previously DocumentDB) integration with Azure Functions.
The first big difference when developing my Cosmos DB ring.com functionality is I had to manually install the Cosmos DB binding extension into my project. One of the goals of the v2 runtime is keep the base image minimal, so unused extensions wouldn’t exist on each function instance. This means I need to specify which NuGet packages to include in the function app that the host will include when running.
The experience isn’t quite as seamless as we want it to be yet, but for now here’s how I enabled Cosmos DB:
Once the setup was complete, I could add the Cosmos DB binding to the function.json config file for my HttpTrigger function.
NOTE: There are some differences between this binding and the v1 DocumentDB binding, like connectionStringSetting and the type
Once setup was complete, the function code is actually extremely simple. It will trigger on an Event Grid event, pull out the first event, and add it to the Cosmos DB collection via the binding configured above.
The final step was to deploy the function, and create an event subscription in the Event Grid topic created in part 1. Now whenever my doorbell gets pressed or motion is detected, an event is automatically stored in Cosmos DB for me.
In hindsight doing something this simple would have likely only taken me 1/10th of the time to build in Azure Logic Apps with the Event Grid trigger and Cosmos DB connector, but was worth building this way to learn some of the ins and outs of the Azure Functions v2 Cosmos DB binding.
In the next part I will be extending my serverless solution to include Logic Apps to orchestrate some powerful video AI whenever someone approaches the doorbell.
Continue to Part 3 — Facial Recognition powered by Cognitive Services and Logic Apps
https://github.com/jeffhollan/functions-node-ring-doorbell
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
66 
66 
66 
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
"
https://medium.com/@ben11kehoe/funamentally-i-look-at-envoy-istio-or-what-azure-provides-inside-service-fabric-and-i-dont-b7a9f740624e?source=search_post---------242,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ben Kehoe
May 30, 2017·2 min read
Paul Johnston
Funamentally, I look at Envoy + Istio, or what Azure provides inside Service Fabric, and I don’t see the ability to achieve the majority of those features through serverless platforms in AWS, Azure, or GCP. I can build a service myself (using serverless architecture) to provide those features, but what I want is the serverless platform (for me, AWS) to provide the service mesh (discovery, incremental rollouts, circuit breaking, etc.), and at lower levels than needing an API Gateway per microservice.
Just to be clear, using an API Gateway per microservice does not provide the features of a service mesh today anyway, and I also don’t want API Gateway to be the focus of such features.
There are two reasons I don’t want to use an API Gateway in front of every microservice. The first is performance: it introduces latency without providing much inherent benefit. The reason there is little inherent benefit is the second reason: that within my application, everything understands AWS resources anyway, so the benefit of an HTTP API is reduced. It’d be great to have a way to communicate the expected input format for a Lambda, but for internal invocations that’s more documentation than anything else.
I’ve already accepted a large amount of vendor lock-in by virtue of building a serverless application dependent on the FaaS and SaaS services in the platform — and, as I’ve argued, this lock-in should not be seen as troublesome. But the lock-in means that inside the system, I don’t need to abstract over the details of the platform.
API Gateways, to me, are useful exactly as gateways: entry points for external clients to access the system, whether that’s an IoT device, a mobile app, a web client, or a 3rd party application. And for those clients, I want to present a single endpoint to access. If every microservice is fronted by its own API Gateway, that means putting another API Gateway in front of those, adding additional latency.
Cloud Robotics Research Scientist at @iRobot
86 
86 
86 
Cloud Robotics Research Scientist at @iRobot
"
https://medium.com/@serhatcan/aws-microsoft-azure-google-cloud-gibi-bulut-bilisim-servisleri-pahali-mi-e191928f89e3?source=search_post---------243,"Sign in
There are currently no responses for this story.
Be the first to respond.
Serhat Can
Dec 9, 2017·3 min read
10 - 100 milyon kullanıcı seviyesinde olan ve insanların çok vakit geçirdiği SnapChat, Slack, AirBnb gibi firmalardan tutun, devasa endüstri devi GE, Siemens gibi firmalar bile kendi sunucularını yönetmek yerine bulut bilişim servislerini kullanıyorlar, kullanmak zorundalar. Peki bu firmalar için bile bulut bilişim pahalı değilse, size neden pahalı geliyor?
Bulunduğunuz ülkenin yasa ve yasaklamalarından dolayı kullanmamanız dışında bulut bilişimin sınırsız kaynaklarından neden faydalanmadığınızı açıklamanız çok zor hale geldi.
Bulut bilişim dediğimiz zaman aynı şeyi anladığımızdan emin olmak için çok kısa bulut bilişim servislerinin genel özelliklerini şöyle sıralamak istiyorum:
Amazon’da 1 GB RAM ve 1 CPU çekirdeği sunan standard ve güçsüz diyebileceğimiz Linux sunucu bile aylık 8.5$ (30+ TL).
Haklısınız, olaya sadece böyle bakarsak pahalı gelebilir. OpsGenie’de ilk işe başladığımda, AWS’in sağladığı nimetlerin farkına varmadan önce, bulut bilişim servisleri bana da pahalı görünüyordu.
Peki aslında ne zaman pahalı:
Güvenlik ve güvenilirlikten bahsetmek istememin sebebi bulut bilişim denilince insanların aklına gelen ilk soru işaretinin bu konular ile ilgili olması. Teknik detaya girmek istemediğim için bu konularda şu servis şunu sağlar bu servis bunu sağlar diye yazmayacağım. Ama bulut bilişim servislerinin sağladığı imkanları kendiniz sağlamanız günümüzde neredeyse imkansız. Ücret açısından bakarsak bulut bilişim servislerinin sağladığı güvenlik sertifikalarını almanız veya uygulamanızın problemlere karşı daha dayanıklı olması için dünyanın değişik yerlerinde çalışmasını sağlamanız ise çok pahalı işler.
Yani bulut bilişim sanılanın aksine çok özel bir uygulama alanınız (örneğin savunma sanayi) yoksa çok daha az paraya güvenlik ve güvenilirlik sağlayabileceğiniz bir ortam.
Nasıl faydalanacağınız ve yöneteceğinizi biliyorsanız bulut bilişim servisleri çok çok ucuz ve her geçen gün daha da ucuz hale geliyor.
Ulaşmak ve iletişimde kalmak isterseniz sosyal medya hesaplarım:
Medium: https://medium.com/@serhatcanTwitter: https://twitter.com/srhtcnLinkedin: https://www.linkedin.com/in/serhatcan/
Başka yazılarımı okumak isterseniz:
medium.com
medium.com
linkedin.com/in/serhatcan/
80 
80 
80 
linkedin.com/in/serhatcan/
"
https://medium.com/microsoftazure/creating-a-massively-scalable-wordpress-site-on-azures-hosted-bits-db6e95bb35b0?source=search_post---------244,"There are currently no responses for this story.
Be the first to respond.
I had yet another discussion about WordPress the other day with a friend here at my co-work space (Impact Hub in Honolulu). I told them I was working on a deployment script for spinning up WordPress and Ghost on Azure’s infrastructure and their response was what I’m used to:
WordPress? Dude… is this new gig warping your mind?
It might be. My response was a variation of the same one I always give:
WordPress powers 30% of the entire web. I think supporting it well on Azure is a no-brainer.
I use WordPress. I like it and yes there are things that, as a developer, I wish could be different. The simplicity of the thing, however, is what really attracts me to it. So, if you’re a person who uses WordPress and you also use Azure, this post might prove useful!
Disclosure: I work at Microsoft in Developer Relations. My job is to figure things like this out and then 1) tell the product teams what could work better and 2) report to you how I managed to get it all to work. If this is helpful, hooray!
There are other ways to do this, but I like “scripted recipes”. There are ARM (Azure Resource Management) templates as well that allow you to click a button and “deploy to Azure” which I think is great, but I’m a bit of a control freak and dig my shell scripts (which may not be the best so sound off if you see improvements).
The other reason I like shell scripts is that you can add comments and use them as a bit of a learning tool. At least that’s what I’m hoping to do here — learning this Azure stuff for me hasn’t been easy and hopefully my notes help some of you.
If you want to jump right to it, here’s a gist you can read through. Yes, I know, I could just use a Docker Compose file and be done. I wanted a better database solution and I also wanted the challenge to… back under the bridge with you!
Finally — if you want to play along, make sure you have the Azure CLI installed and that you’re logged in with a default subscription set.
The first thing we need to do is declare our variables, etc:
At the very top I’m setting the resource group as RGbecause it’s used everywhere. I’m then setting the APPNAMEand LOCATION, the latter with a default of “Central US”.
When you create things in Azure, you use “resource groups” for logical groupings. This is extremely helpful when you want to wholesale remove everything when you’re playing around, which I am. It also keeps project-based stuff nice and tidy.
The name of your website needs to be unique across Azure as well. I usually try to name mine with the resource group appended to it somehow. I’ll talk about the user name and password stuff later.
The final step is to create the resource group. Now let’s create MySQL.
This, to me, is the main benefit of using Azure. You get a super-scalable MySQL instance to back your fully-managed website. Most one-click installs you see online give you a single VM with MySQL installed right next to WordPress. This usually works fine (such as with this blog you’re reading) but if you’re planning on ramping up, it can be a bit scary.
The first thing to decide is the size of our database. You can ramp this thing to the moon if you want, but I suggest keeping it small to start. I’ve listed out the SKU structure in the script:
This code creates a smaller MySQL instance — in fact it’s the smallest one you can create. I ran into some issues with certain SKUs not being supported in certain regions, but if you get an error just drop the resource group and try again.
We then spin up MySQL. The one big thing to notice here is --ssl-enforcement Disabledwhich caused me some headaches. WordPress doesn’t use SSL out of the box, which you can change with a plugin or config changes (surprise surprise). If you’re behind a firewall or part of a VPN, this might not be a huge deal. You can change this later on, however, when the plugin is installed.
The final thing to point out is the admin-user and admin-passwordsettings. I am not a fan of leaving the root level stuff to the user, but right now that’s the way it is. To softly get around this, the script creates a random user name and generates a GUID for the password, which you can see in the first script above.
We’re now ready to open a firewall and create our WordPress database.
Our server is up and running but is locked down completely, which I think is marvy. We need to be explicit about who/how/what can access our server, so we need to open a few holes in the firewall. The first will be for us, the second will be for our website later on:
The first hole is for my local IP address, which I’m grabbing using curl. This is a subshell that pings ipinfo.io and reads the response, setting it to a variable. I use that response to create the first firewall rule: AllowMyIP.
The next rule looks a bit scary. I’m creating a firewall rule that allows any service within Azure to ping my server. This is what the Azure docs have to say about it (emphasis mine):
To allow applications from Azure to connect to your Azure SQL server, Azure connections must be enabled. When an application from Azure attempts to connect to your database server, the firewall verifies that Azure connections are allowed. A firewall setting with starting and ending address equal to 0.0.0.0 indicates these connections are allowed. If the connection attempt is not allowed, the request does not reach the Azure SQL Database server.
ImportantThis option configures the firewall to allow all connections from Azure including connections from the subscriptions of other customers. When selecting this option, make sure your login and user permissions limit access to only authorized users.
Azure SQL Database and SQL Data Warehouse firewall rules
This might seem suboptimal when it comes to security because it is. However, this is how every service works within any data center. We can, if we want, further lock this thing down later on when we know the IP of our site (assuming it won’t change).
The final step in the code above is to use the mysqlclient to create the remote database. I couldn’t find a way around this. I know that most people don’t have MySQL installed locally, so this could be a problem for you. I’m hoping to find a better solution someday.
I’m a control freak and I like to have access to my database. I also hate trying to remember login information so to help myself out I’ll setup a .envfile with the relevant bits:
If you don’t know, a .envfile typically contains environment variables and other things intended to help you with development. They might contain local database configuration, etc. Here, I’m outputting some environment variables and I’m also setting up an alias so I can connect to prodas needed.
Side note: if you use zshell and oh-my-zsh! there’s a plugin called “dotenv” that will automatically load any .env file in a directory when you navigate to it.
You don’t want this checked into source control!
We want our WordPress site to be fully managed, from the database all the way down to the website itself. This will allow us to flip a few bits in the future to add more cores to either the site or the database.
This is not a VM. Well, at least as far as you’re concerned. All of that is abstracted into Azure’s “service fabric”. That said, the first thing you need to do is to pick the size of your VM in the form of a plan :p:
You can think about this plan in terms of its capacity and compute power. B1 is the lowest but won’t work if you’re using Linux. If your WordPress site is going to power some serious traffic, you might consider a plan with a bit more horsepower to it. It’s not the most straightforward thing, but you can have a read about plans here.
Now we get to spin up our WordPress site. We could pull down the WordPress source (PHP) and then send it up to Azure, specifying deployment and runtime settings etc. Or we could simply specify the image on DockerHub:
When you create a webappon Azure, it needs to know 1) how to run it and 2) how to deploy it. You can specify a Python runtime, for instance, and “local Git” as the deployment option. This would allow you to push from your local machine and have your Flask app “just run”.
If you’re using Docker, however, the runtime is built into the image. All that Azure needs to do is to know where to get it. It’ll use your plan to create and run a container. In the code above, I’m telling Azure to use the wordpress image, which Azure knows to pull down from DockerHub.
Having a look at the image description on DockerHub, you can see there are a number of environment variables that I can set for the image, namely the database connection bits. This is perfect for our needs, but how do you pass this through from Azure?
The good news is that Azure will pass whatever app settings you have for your webapp directly to your container:
The environment variables are expected by the WordPress container, and I’m setting them to the MySQL database that I created above.
The last setting, WEBSITES_PORT, tells Azure which port to forward to in the container. I think the default is port 80, but I’m setting it here just for clarity.
The final thing we need to do is a bit of a bugger for me: we need to explicitly turn on logging. I wish this just happened by default, like with Heroku, but it doesn’t so we need to:
I’m outputting everything here because I want to know exactly what happens when Azure tries to spin up this container. I’ll be honest: debugging the container stuff has been difficult.
As I mention, I’m a huge fan of laziness and I like to add things to .env files when I can. Let’s do that now so we can tail our application logs with a simple command:
I’m setting the alias for the current shell and I’m also appending it to .envso I can access it later on.
We’re ready to go! The only thing left to do is to navigate to our new site. The first request will take a while as Azure is pulling down our image and running it for the first time. You can see this if you tail the logs while the site is loading, which is the very last step of our script:
I’m opening our new URL, loading our new .env file into the shell and then calling our new alias, logs. This will allows us to see what’s happening at Azure as WordPress spins up. Hopefully, after a minute or so, you see something like this:
It’s a good question, one I’ve asked myself repeatedly over the last few days. The answer is simple: this WordPress site is massively scalable. That’s kind of a big deal. It’s running right on the Azure “metal”, so to speak and I don’t need to worry about upscaling a VM.
As I mention: I really like WordPress and I run my business on it. Having it all on Azure would be a nice peace of mind.
As for costs, you’ll have to take a look at your subscription and decide if something like this is right for you. I have a regular subscription that I pay for and I can tell you that this setup is comparable with what I’ve seen for WordPress hosting.
Every time I do stuff like this I end up with a big writeup for the product teams to look over. It’s important to note that this isn’t a situation where I “just send an email” or file a bug. My team works closely with the product teams and they like (I think) that we’re beating on their stuff.
I’ll be letting them know my thoughts on the MySQL stuff (admin user/pass, executing a remote command, etc) and a few more things that I need to get clarity on first. That’s a bit of a problem I have right now as I step into all of this: learning it is not straightforward.
I suppose that will be my biggest point of feedback. There’s so much to learn! It’s hard to get going and often I find myself doing things the “old way”. It’s also my job to help get this stuff in order, which I like.
Originally published at rob.conery.io on January 9, 2019.
Any language.
51 
2
51 claps
51 
2
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author of The Imposter’s Handbook, founder of bigmachine.io, Cofounder of tekpub.com, creator of This Developer's Life, creator of lots of open source stuff.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@renatogroffe/asp-net-core-kubernetes-azure-community-bootcamp-abril-2018-115455a486e6?source=search_post---------245,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 5, 2018·2 min read
Nesta última quarta (04/04/2018) participei como palestrante do Community Bootcamp, realizando uma apresentação sobre o uso do Kubernetes no Microsoft Azure como solução para a orquestração de containers Docker (tomando como base imagens de aplicações ASP.NET Core).
Este evento aconteceu em um dos auditórios da Microsoft em São Paulo-SP, contando ainda com apresentações sobre TypeScript e MongoDB ao longo da noite.
Gostaria de deixar neste post meu muito obrigado à Cynthia Zanoni (Microsoft) pelo convite e ao Marcelo Mendonça Miranda (Microsoft) por todo o suporte para que eu pudesse participar como palestrante. Deixo aqui também meus agradecimentos ao Thiago Adriano (Microsoft MVP) e ao Milton Câmara Gomes pelas fotos tiradas durante a palestra.
Os slides da apresentação já estão no SlideShare:
Disponibilizei também o projetos e os scripts utilizados durante a demonstração prática no GitHub:
https://github.com/renatogroffe/ASPNETCore2_Kubernetes
Caso desejem ainda outras referências sobre o uso de Docker com o ASP.NET Core acessem também o seguinte artigo:
ASP.NET Core + Docker Compose: implementando soluções Web multi-containers
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
92 
1
92 
92 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@gmusumeci/how-to-bootstrapping-azure-vms-with-terraform-c8fdaa457836?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 26, 2020·15 min read
Note: this story was updated on May 23, 2020 to add Complex PowerShell Scripts using Template_File and Variables (point 6).
A very common task when we deploy Azure Virtual Machines using Terraform is deploying applications and/or code into the machine at the boot time.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hashmapinc/doing-devops-for-snowflake-with-dbt-in-azure-db5c6249e721?source=search_post---------247,"There are currently no responses for this story.
Be the first to respond.
by Venkatesh Sekar
I recently wrote about the need within our Snowflake Cloud Data Warehouse client base to have a SQL-centric data transformation and DataOps solution. In my previous post, I stepped through how to create tables using custom materialization with Snowflake.
Continuing in that vein, I was recently asked by a customer to provide a path for them to do database DevOps for Snowflake. In general, database DevOps has involved quite a bit of complexity and ongoing tweaking to try and get it right. There are some tools available in the market today including:
But, other than Sqitch, they don’t support Snowflake yet, although, with the amount of momentum that Snowflake has in the market, I expect they will provide support in the not too distant future.
Enter dbt
Having used dbt as a data transformation and Jinja template-based tool, I was interested to see if it could potentially be the key to help unlock database DevOps for Snowflake.
As noted above, I was able to create the ‘persistent_table’ materialization which provided an answer for creating ‘source tables’ in DBT, and having done that I next developed a simple CI/CD process to deploy database scripts for Snowflake with dbt in Azure DevOps.
Stay with me and I’ll step you through how to setup dbt to deploy the scripts. As always, the code is available in my git repo venkatra/dbt_hacks.
Here is a glimpse into the tools and solutions that I am using to make this happen…
dbt is a command line tool based on SQL and is primarily used by analysts to do data transformations. In other words, it does the ‘T’ in ELT.
It facilitates writing modular SQL Selects and takes care of dependencies, compilation, and materialization in run time.
Azure Devops provides developer services to support teams in planning work, collaborating on code development, and building and deploying applications.
Organizations across industries rely on Snowflake for their cloud data warehousing needs — net new data warehouses, migrations from legacy DW appliances (Netezza, Teradata, Exadata, etc.), and migrations from traditional Hadoop and Big Data platforms (Hive, HBase, Impala, Drill, etc.). Our clients are also using Snowflake for high-value solution areas such as Security Analytics and Cloud Visibility Monitoring.
Snowflake is fully relational ANSI SQL cloud data warehouse and allows you to leverage the tools that you are used to and familiar with while also providing instant elasticity, per second consumption-based pricing, and low management overhead across all 3 major clouds — AWS, Azure, and GCP (GCP is in private preview).
The Continous Integration (CI) process is achieved using Azure Pipelines within Azure DevOps. This pipeline is typically invoked after the code has been committed, and the pipeline tasks generally handle:
In the case of a database scripts file there isn’t a great deal of validation that can be done, other than the following:
Snowflake currently does not have a tool that validates the script before execution, but it can validate during deployment, so in the Build phase I typically do these checks:
Given the set of all scripts, it’s essential to determine which scripts were added or updated. If these scripts can’t be identified, you will end up re-creating the entire database, schema, etc. which is not desired.
To solve this issue, I’ll use the Azure DevOps Python API. Going through the docs, you will see different REST endpoints and determine detailed information on what was committed, when it was committed, and who committed it, etc.
The python script IdentifyGitBuildCommitItems.py has been developed in response to this. Its sole purpose is to get the list of commits that is part of the current build and their artifacts (the files that were added/changed). Once identified it would write them into a file ‘ListOfCommitItems.txt’ during execution.
I’ll review the results in the below sections.
During the course of development, the developer might have created scripts for table creation as well as developed transformation models, markdown documentation, shell scripts, etc. The ‘ListOfCommitItems.txt’ that was created earlier would contain all of these scripts. Note that if a file was committed multiple times, the script will not de-dup the commits.
To keep things modular, the script FilterDeployableScripts.py was created. Its responsibilities are to:
The build pipeline is a series of steps and tasks:
These are captured in azure-build-pipeline.yml
The following screenshot highlights the list of artifacts that get published by the build. It also provides a sample output of ‘ListOfCommitItems.txt’ which was captured in the initial run.
Notice that the ‘DeployableModels.txt’ file contains only the CONTACT table definition file, and ignores all other files that are not meant to be run.
Now take a look at the next screenshot from a different build run — during this build run we saw the following:
Again, the ‘DeployableModels.txt’ file contains only the ADDRESS table definition file and is not concerned with any other files that are not meant to be run.
A Continuous Deployment (CD) process is achieved with Azure Release Pipelines. The pipeline we are working on is geared towards the actual deployment to a specific snowflake environment, e.g. Snowflake Development Environment.
The “Stage” section is usually specific to the environment in which the deployment needs to happen. It consists of the following tasks as seen below:
This is a Bash task with inline code below:
This is a Bash task with inline code below:
This sets up the various env configurations needed for dbt and used as part of the execution.
The ‘ENV_’ are variables that will be substituted at run time. They need to be defined in the variable section as below:
Upon release, the table will be created in Snowflake. Here is a screenshot of the successful run and the logs of the DBT_RUN task:
and below is the artifact in Snowflake:
Keep these limitations in mind when leveraging dbt for CI/CD with database objects…
I hope you’ll try to replicate this simple CI/CD process to deploy database scripts for Snowflake with dbt in Azure DevOps. While there are some limitations, the potential is there to add value to your data ops pipelines.
You should also check out John Aven’s recent blog post (a fellow Hashmapper) on Using DBT to Execute ELT Pipelines in Snowflake.
If you use Snowflake today, it would be great to hear about the approaches that you have taken for Data Transformation, DataOps, and CI/CD along with the challenges that you are addressing.
I hope you’ll check out some of my other recent stories also…
medium.com
medium.com
medium.com
Feel free to share on other channels and be sure and keep up with all new content from Hashmap here.
Venkat Sekar is Regional Director for Hashmap Canada and is an architect and consultant providing Data, Cloud, IoT, and AI/ML solutions and expertise across industries with a group of innovative technologists and domain experts accelerating high-value business outcomes for our customers.
Innovative technologists and domain experts helping…
87 
1
87 claps
87 
1
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/i-think-google-cloud-is-the-best-best-tech-best-pricing-best-support-best-roadmap-and-best-4b4e17856505?source=search_post---------248,"There are currently no responses for this story.
Be the first to respond.
I think Google Cloud is the best — best tech, best pricing, best support, best roadmap, and best people. I work for Google Cloud, so I may be biased. So when folks ask me why Google Cloud is better than it’s competitors, perhaps it’s better for real users to speak for the platform themselves.
So I put together a list of customer opinions — folks who live and breathe cloud every day, folks who take the time to understand the differences between vendors, folks who knows performance and cost characteristics because they actually run real-life workloads and deal with scaling, billing, and maintenance. Here it goes!
(this is just a small quick gathering — send me more, and I’ll be sure to update the list!)
Quizlet compares Google and Amazon VMs based on networking, performance, and price. Guess who comes out on top!
2. May 2016 — “The future of cloud computing: Google Cloud!”
Obulpathi from Monsanto opines on Easy of Use aspects of Google Cloud Platform and Amazon Web Services.
3. May 2016- “5 unique features of Google Compute Engine that no cloud provider could match”
This Forbes editorial discusses what sets apart Google’s VM service, Compute Engine (TL;DR: sustained use discounts, preemptible VMs, custom VM shapes, online disk resizing, shared storage).
4. June 2016 — “GCE vs AWS in 2016: Why you should NEVER use Amazon!”
A rather one-sided piece by thehftguy that talks about limitations of Amazon EC2 and how Google Compute Engine’s architecture overcomes such limitations.
5. July 2016- “Why we moved from Amazon Web Services to Google Cloud”
Michael Lugassy of adtecho.com explains his reasons for migrating from AWS to Google Cloud, and some downsides of Google as well!
6. February 2016 “Announcing Spotify Infrastructure’s Googley future”
Spotify describes reasons for their move to Google Cloud — level of innovation, big data technologies, and people. And, remember folks, next time someone tells you “Spotify chose GCP because they got a good deal”: when the market is not commoditized, it’d be foolish for companies to make decisions on cost.
7. September 2016 “Top 5 Advantages of Choosing Google Cloud”
A nice quick recap, although “Google Cloud Hosting” doesn’t have the same ring to it!
When you do the math, Google’s SSDs are 10x-ing the competition on the price-performance spectrum.
2. October 2016 — “A Survival Guide for Containing your Infrastructure”
Tripstr switched from AWS to GCP, containerized, and saved 75% on their infra bill. Not bad!
3. November 2016 “Google is 50% cheaper than AWS”
A comparison of GCE and EC2 prices.
Google Cloud Storage comes out ahead on throughput, but loses on latency. Mr. Johnson correctly points out that the latency tradeoff is due to Google Cloud Storage being a multi-region service by default, unlike Amazon/Azure options, which are closer to “GCS regional buckets”.
2. October 2016 “Google Cloud Storage vs AWS S3”
Jishnu from Vuclip finds GCS from 4x to 20x faster than S3 for uploads while being 35% cheaper than S3.
This O’Reilly blog compares EMR to Google Cloud Dataproc.
2. September 2016 — “Fun with .. Google BigQuery”
Mr. Alvarez is entirely blown away by BigQuery, and leverages the GHCN Public Dataset to come up with some fun conclusions.
Google Cloud community articles and blogs
85 
2
85 claps
85 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
VP of PM at Firebolt. All views are my own.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/javarevisited/my-favorite-free-microsoft-azure-cloud-courses-for-beginners-to-learn-in-2020-3418524bb531?source=search_post---------249,"There are currently no responses for this story.
Be the first to respond.
Hello guys, Cloud computing is an in-demand skill and essential for Software developers, but with clouds also comes doubt. Among, AWS, Google Cloud, and Microsoft Azure, which one should you learn first?
Does learning Azure make sense given AWS is the most popular public cloud platform? Well, I think learning the Azure cloud makes a lot of sense especially if you are looking for a job in the technical and financial world as Microsoft has the largest market share in the corporate world.
Microsoft has done a great job with their cloud services; they have successfully created services such as testing, deploying, building, and managing applications through data centers. Microsoft is changing different elements in terms of cloud computing with all these data centers and high-speed internet, we are exploring new boundaries. It’s uncanny how things are progressing, but we need to keep with new technology. If you want to learn Microsoft Azure concepts and services and looking for free online training courses and classes then you have come to the right place. In the past, I have shared free and paid courses to learn Microsfot Azure, AWS and Google Cloud Platform and today, I am going to share free courses to learn the Microsoft Azure Platform. We have sorted out and handpicked the best and free Microsoft Azure online courses from places like Udemy, Youtube, and Pluralsight. These courses are going to provide you with great insight into Microsoft Azure Cloud services and functioning. Each course has focused on a certain area of learning, so it’s of utmost importance that you take a look at all those courses personally as well. We will be providing you with a short description of the course that can provide you with the summary, of course, their content, and the vision behind them.
By the way, if you don’t mind paying few bucks for learning a useful skill like the Microsoft Azure platform and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out AZ-300/AZ-303 Azure Architecture Technologies Exam Prep 2021 course by Scott Duffy on Udemy. It’s one of the most comprehensive courses to learn the Azure Cloud platform.
udemy.com
Without wasting any more of your time, her is the best and free online course to learn the Microsoft Azure cloud platform from Udemy, Pluralsight, Youtube, and other popular online learning websites. We have sorted out the best out of all the free course for learning, here’s our list:
This course is designed for novices in the cloud computing section. To start learning this course you’ll need basic knowledge about how things work in the IT industry, to which you don’t need to be a part of one.   You can easily gain this knowledge on the internet by talking to people who are in the industry.  Course content:
This course is going to behave as a launchpad for the people who are willing to learn. Guys who are looking to go in sales can also benefit from this course, the course focus on explaining the technical concept of working so guys in sales can look like a veteran with all the information available.
Here is the link to join this course — Getting Started with Cloud Computing using Microsoft Azure
This course is developed to provide basic conceptual learning to enthusiasts. It doesn’t have any prerequisites, and you can get started now if you want.  The course begins with fundamental concepts such as, what is the cloud? From here it focuses on explaining little by little how you can work on azure etc. This course is strictly visual and conceptual. You’ll get the conceptual knowledge of the cloud and its services which would help you in further learning of advanced features.  Content of Course:
This course works wonders by providing you frame-of-reference learning which is highly effective to solve complex problems in advanced azure learning. Also, it provides bonus features and access cards to a hands-on lab to enhance learning. The course is regularly updated so all your queries will be answered.
Here is the link to join this course for FRE — Microsoft Azure Concepts
The course focuses on pure learning with short video lectures. Theoretical parts are not the main focus here you will be applying your knowledge. This course would be more meaningful if you have knowledge of dot net technologies and C.  The target audience of this course is people who are looking to make a career in cloud computing and web development.  Course Content:
This course is going to provide you the ease of learning that you’ll end up using azure for your day to day purpose. You’ll learn the sense of coding from a certified teacher who has worked many hours in the IT industry. The course is divided into 13 lectures which mainly focuses on visual learning.
Here is the link to sign up this course for FREE — AZ — 900 Microsoft Azure Fundamentals
Cloud ranger network on youtube has solely dedicated the channel to provide you the course for the azure cloud. They have all sets of the playlist from the beginners to the advance. The focus on visual learning and implementation. You’ll be able to learn a lot from real-time implementations. For the beginners we would like to recommend watching the discovery series, it’s going to give you a great start as you go on from learning about basics and service to advanced all through the course. It’ll give a classroom experience as you can keep in touch with all the users from different professions. They are not targeting any audience but focusing on deep learning, hope you found that useful. It’s a guided course from beginners to advance, you’ll get to learn all the latest work in this technology.
This is a free Coursera course to learn the basics of the Microsoft Azure platform online. In this introductory course, you will learn about essential Microsoft Azure services related to computing, storage, network, and memory. You will also gain familiarity with core Azure topics and practice the implementation of infrastructure components.  Here are the main things you will learn in this course
Overall a nice free online course from Coursera to learn key skills like cloud computing, cloud platforms, Microsoft Azure, and cloud databases. The course is offered by Learn2Quest, and delivered by instructor Kenny Mobley.
Here is the link to join this course for FREE — Getting Started with Azure
And, if you find Coursera courses and certifications useful, then I also suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but its complete worth of your money as you get unlimited certificates.
In this course, you’re going to learn about all the services that you can use in your solution and the ambition of business. This course is really helpful for the web developers as well as business enthusiasts who are targeting to go big with their ideas. Microsoft has done a terrific job with their cloud service, learning about these services can help you solve any modern-day problem. This course is going to provide a great start for beginners as it covers a lot of topics.  Content of course:
These visual presentations will give you a bigger picture and a point of reference to help things put into perspective. This course is taken from PluralSight which generally provides courses on a premium basis only, try learning that within the free trial period.
Here is the link to join this course — Microsoft Azure: The Big Picture
Btw, you would need a Pluralsight membership to watch this course which costs around $29 per month or $299 per year. If you can afford then I highly recommend this subscription to you as it not only gives access to this course but more than 7000+ others. Alternatively, you can also use their 10-day-free-trial to watch this course for FREE.
That’s all about free online courses to learn Microsoft Azure and become a certified Cloud developer in 2021. All the courses are chosen after taking feedback from a large group of learners and tech enthusiasts.
We tried to cover all the zones of interest of learners by providing you with the best courses available. Hope you found them useful and we would like to suggest you open those personally to get better clarity about the courses.  By the way, if you don’t mind paying few bucks for learning a useful skill like the Microsoft Azure platform and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out Microsoft Azure — Beginner’s Guide + AZ-900 — UPDATED 2021 course on Udemy.
udemy.com
This course will not only help you to learn Azure basics and core services but also prepare you for Azure Fundamentals — AZ 900 exam, which is a great way to mark your entry into the cloud computing world. You can also buy this course on Udemy sales at just $10, which happens every now and then.
Other Azure Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure courses useful, then, please share it with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P.S. — If you are serious about learning the Microsoft Azure platform and looking for more comprehensive and in-depth courses to learn Azure services then I also suggest you check out the Microsoft Azure — Beginner’s Guide + AZ-900 — UPDATED 2021 course on Udemy.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
181 
181 claps
181 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://itnext.io/how-a-naughty-docker-image-on-aks-could-give-an-attacker-access-to-your-azure-subscription-6d05b92bf811?source=search_post---------250,"DISCLAIMER: the contents below are for general information purposes only. The exploit is only to emphasise the need to secure your AKS clusters and not to use untrusted docker images. Use of information contained here is therefore strictly at your own risk.
I will walk-through an exploit I created that allows a simple container to find the Azure service principal details from an AKS cluster and send that to a remote URL.
Most of the things used here are based on the taking over AKS clusters post I wrote a while ago. And a fundamental piece to achieve this is the Kubelet API trick, which allows you to poke around any running pod within a node, giving you get extra powers even if your pod runs as a non-root/least-privileged user.
To understand a bit better how we will get access to the Azure Subscription, you need to know that AKS is based on the ACS engine, which is an open-source project that eases the provisioning of clusters on Azure. It internally uses a file to store the Service Principal Name (SPN) so it can interact with the Azure API. That file exists in every single cluster node and is located in the path below:
There are a few different ways to get the contents of this file, but in essence, we will need to mount the host (node) volume onto a privileged container.
The sneakiest way I found to do this, is to manipulate the kube-proxy daemonset which already runs as privileged and has a mount to /etc/kubernetes/certs. As people tend to not pay much attention to what goes on within the kube-system namespace, the change we will make can easily be overlooked.
Depending on how your image is scheduled, you may or may not get an API Server token mounted in your pod. To make this more generic, we will get the token from a kube-proxy container:
Notice that we are leveraging the unauthenticated access to Kubelet API on IP 10.240.0.4. That is generally the first node within an AKS cluster. However, a cleverer exploit could find this dynamically.
The kube-proxy requires access to the path /etc/kubernetes/certs. We will amend that, so we actually mount its parent folder. That simple change will keep the system working, as the certs mapping will still be available, however, it will also provide us access to the azure.json file. This are the full contents we will get access to:
In theory, that could be done by exporting its yaml file, replacing kubernetes/certs with /kubernetes, then applying the change:
By default, the changes on daemonsets are only rolled-out on delete. So in the actual exploit we do it in three steps: 1. export, 2. remove existing kube-proxy, 3. apply changes.
Here’s the full exploit.sh:
When executed it will print the Azure SPN and also post it to an external URL.
With that information, anyone could login onto your Azure Subscription through Azure CLI.
Query its permissions:
By default, that SPN would be a contributor of the cluster resource group. However, if the person who provisioned the cluster did a bad job, this SPN may be a subscription owner, contributor, etc.
Either way, from this point an attacker have enough access to disrupt your services, or spin up new resources in the cluster resource group to do whatever they want — did I just hear crypto-mining?
I have shared a similar exploit with the Microsoft Security team back in February 2018, and they are working on fixing it. In the mean time, make sure you only use trusted docker images in your clusters.
UPDATE 08/07/2018: Really good news here, this has since been fixed by Microsoft. The commands below will now result in HTTP 401 — Not Authorized:
ITNEXT is a platform for IT developers & software engineers…
79 
1
79 claps
79 
1
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Software craftsman on the eternal learning path towards (hopefully) mastery. Security enthusiast keen on SecDevOps. My opinions are my own.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://blog.jeremylikness.com/migrating-azure-functions-from-v1-net-to-v2-net-standard-b2d724f9faf?source=search_post---------251,
https://medium.com/microsoftazure/training-your-first-distributed-pytorch-lightning-model-with-azure-ml-f493d370acb?source=search_post---------252,"There are currently no responses for this story.
Be the first to respond.
TLDR; This post outlines how to get started training Multi GPU Models with PyTorch Lightning using Azure Machine Learning.
Full end to end implementations can be found on the official Azure Machine Learning GitHub repo.
github.com
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
PyTorch Lighting is a lightweight PyTorch wrapper for high-performance AI research. Lightning is designed with four principles that simplify the development and scalability of production PyTorch Models:
Once you do this, you can train on multiple-GPUs, TPUs, CPUs and even in 16-bit precision without changing your code which is perfect for taking advantage of distributed cloud computing services such as Azure Machine Learning.
github.com
Additionally PyTorch Lighting Bolts provide pre-trained models that can be wrapped and combined to more rapidly prototype research ideas.
github.com
Azure Machine Learning (Azure ML) is a cloud-based service for creating and managing machine learning solutions. It’s designed to help data scientists and machine learning engineers to leverage their existing data processing and model development skills & frameworks.
Azure Machine Learning provides the tools developers and data scientists need for their machine learning workflows, including:
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
You can even use external open source services like MLflow to track metrics and deploy models or Kubeflow to build end-to-end workflow pipelines.
Check out some AzureML best practices examples at
github.com
github.com
With the advantages of PyTorch Lighting and Azure ML it makes sense to provide an example of how to leverage the best of both worlds.
Connect to the workspace with the Azure ML SDK as follows
docs.microsoft.com
To run PyTorch Lighting code on our cluster we need to configure our dependencies we can do that with simple yml file.
We can then use the AzureML SDK to create an environment from our dependencies file and configure it to run on any Docker base image we want.
Create a ScriptRunConfig to specify the training script & arguments, environment, and cluster to run on.
We can use any example train script from the PyTorch Lighting examples or our own experiments.
For GPU training on a single node, specify the number of GPUs to train on (typically this will correspond to the number of GPUs in your cluster’s SKU) and the distributed mode, in this case DistributedDataParallel (""ddp""), which PyTorch Lightning expects as arguments --gpus and --distributed_backend, respectively. See their Multi-GPU training documentation for more information.
We can view the run logs and details in realtime with the following SDK commands.
Now that we’ve set up our first Azure ML PyTorch lighting experiment. Here are some advanced steps to try out we will cover them in more depth in a later post.
This example used the MNIST dataset from PyTorch datasets, if we want to train on our data we would need to integrate with the Azure ML Datastore which is relatively trivial we will show how to do this in a follow up post.
docs.microsoft.com
In this example all our model logging was stored in the Azure ML driver.log but Azure ML experiments have much more robust logging tools that can directly integrate into PyTorch lightning with very little work. In the next post we will show how to do this and what we gain with HyperDrive.
github.com
github.com
In this example we showed how to leverage all the GPUs on a one Node Cluster in the next post we will show how to distribute across clusters with the PyTorch Lightnings Horovod Backend.
In this example we showed how to train a distributed PyTorch lighting model in the next post we will show how to deploy the model as an AKS service.
docs.microsoft.com
If you enjoyed this article check out my post on 9 tips for Production Machine Learning and feel free to share it with your friends!
medium.com
I want to give a major shout out to Minna Xiao from the Azure ML team for her support and commitment working towards a better developer experience with Open Source Frameworks such as PyTorch Lighting on Azure.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with the Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
114 
1
114 claps
114 
1
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/aplica%C3%A7%C3%B5es-escal%C3%A1veis-com-asp-net-core-docker-e-o-microsoft-azure-net-sp-julho-2018-fec27463e34d?source=search_post---------253,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 29, 2018·3 min read
No dia 26/07/2018 (quinta-feira) participei como palestrante em mais um meetup do grupo .NET São Paulo (grupo do qual também sou organizador), realizando uma apresentação sobre a construção de aplicações Web escaláveis com ASP.NET Core, Docker e o Microsoft Azure.
Gostaria de deixar neste post meu muito obrigado à Eliane Barbosa (Accesstage) e ao Diogo Brito (Accesstage) por todo o apoio para que este evento pudesse acontecer, o que incluiu a disponibilização do auditório utilizado e o coffee break para os participantes. E também à Cynthia Zanoni (Microsoft) e à Silene Velasco (Microsoft) pelos brindes fornecidos para sorteio.
Os slides da apresentação já estão no SlideShare:
Esta palestra focou no uso de soluções escaláveis na nuvem como Azure Web App for Containers e Azure Kubernetes Service (AKS). Caso tenha interesse em conhecer mais sobre as mesmas acesse os artigos a seguir:
ASP.NET Core + Azure Web App for Containers: escalando uma API REST com containers Docker
ASP.NET Core + Azure + Kubernetes: Guia de Referência
E também as gravações de 2 eventos recentes realizados no Canal .NET:
Termino este post agradecendo mais uma vez ao Diogo Brito e ao Luigi Tavolaro, agora pelas fotos tiradas durante a apresentação.
Referências
Docker para Desenvolvedores .NET - Guia de Referência
ASP.NET Core + Azure + Kubernetes: Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
33 
33 
33 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/bb-tutorials-and-thoughts/how-to-get-started-with-azure-aks-275fe5d2db40?source=search_post---------254,"There are currently no responses for this story.
Be the first to respond.
AKS is Microsoft Azure’s managed Kubernetes solution that lets you run and manage containerized applications in the cloud. Since this is a managed Kubernetes service, Microsoft takes care of a lot of things for us such as security, maintenance, scalability, and monitoring. This makes us quickly deploy our…
"
https://faun.pub/lets-do-devops-auto-approve-safe-terraform-apply-in-azure-devops-ci-cd-f1b41afaf4c8?source=search_post---------255,"There are currently no responses for this story.
Be the first to respond.
This blog series focuses on presenting complex DevOps projects as simple and approachable via plain language and lots of pictures. You can do it!
Hey all!
I’ve written a series of blogs about running an Azure DevOps Terraform CI/CD in an enterprise environment (for more info please see my profile). One item my business very much wanted, and which CI/CDs twist themselves up in knots to support is manual approvals for particular stages or steps.
"
https://medium.com/awesome-azure/azure-organize-and-manage-multiple-azure-subscriptions-and-resources-with-azure-management-groups-3472170b1407?source=search_post---------256,"There are currently no responses for this story.
Be the first to respond.
Introduction to Azure Management Groups — Azure Subscriptions Planning and Designing Best Practices, Benefits, and Use Cases.
Azure Management Groups, Subscriptions, and Resource Groups are used together to establish the entire organizational structure in Azure, and they…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/data-science-video-series-to-get-started-with-machine-learning-on-azure-28d7c67b02eb?source=search_post---------257,"There are currently no responses for this story.
Be the first to respond.
Data science is about extracting knowledge from data. Data science is an important area of study because it is a tool that data scientists leverage to gain insights from data and prepare it for the machine learning modeling phase. By “doing data science”, data scientists actually apply techniques, such as data pre-processing and cleaning, feature engineering and descriptive statistics, to their data in order to understand it and start building AI solutions.
In this sense, data science has become an area of study that universities and companies should look at as a first step to start their machine learning journey:
You can read the full article here: https://techcommunity.microsoft.com/t5/educator-developer-blog/data-science-video-series-to-get-started-with-machine-learning/ba-p/1521995
Additional resources:
More videos coming:
The month of July 2020 is Data month for the Microsoft Reactors global live streams on Twitch! For the middle two weeks of July, you will get to dive even deeper on these and similar concepts with Sarah, Francesca, and other Cloud Advocates and Microsoft employees!
Any language.
33 
33 claps
33 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Principal Data Scientist Manager @Microsoft ~ Adjunct Professor @Columbia University ~ PhD
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@jthake/introducing-azure-bot-service-5daaaaf8af7f?source=search_post---------258,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeremy Thake
Nov 22, 2016·2 min read
I’ve been working on the Azure Bot Service for the last few months internally as it got ready for its preview launch at Connect(); event last week. Andrea Orimoto who is a Program Manager in engineering did a great 7 min video that highlighted the benefits of the service.
channel9.msdn.com
I did a great 45 min video with Lars Liden who is a program manager in the team. The video shows how easy it is to get started and shows how the Language Understanding Intelligence Service (LUIS) can help make your bot seem more natural in conversation.
channel9.msdn.com
For me personally, this is really exciting as it allows developers to get started with a bot in a matter of minutes. My 3 top things:
This is a preview and there is room for improvement. My personal wish list is:
Microsoft Graph Team, Senior Program Manager at Microsoft.
10 
10 
10 
Microsoft Graph Team, Senior Program Manager at Microsoft.
"
https://blog.jeremylikness.com/deploy-angular-and-net-core-2-1-to-the-azure-cloud-part-four-d68594807c7a?source=search_post---------259,
https://medium.com/@gmusumeci/how-to-use-packer-to-build-a-windows-server-image-for-azure-52b1e14be2f2?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Apr 6, 2020·7 min read
Packer is an open-source tool used to create virtual machine templates from a .json file.
In this story, we will learn how to use Packer to define and build custom images in Azure, and then use this image to build a Windows virtual machine using Terraform.
About
Write
Help
Legal
Get the Medium app
"
https://posts.specterops.io/azure-privilege-escalation-via-azure-api-permissions-abuse-74aee1006f48?source=search_post---------261,"Microsoft’s Azure is a complicated system of principals, securable objects, and the various ways access is granted to those objects. Some privileged actions are tightly controlled by Azure AD roles, while other actions are controlled by roles and object ownership. Many objects in Azure are subject to distinct permissions systems, which can make effective access auditing very difficult.
In this post, I will explain how one of those permissions systems can be abused to escalate to Global Admin. I’ll explain how you as an attacker can abuse this system, and I will also explain how you as a defender can find, clean up, and prevent these abusable configurations.
This is not wholly original work. A few folks in the Azure world have spoken about this risk and possible mitigations already:
In the Azure offensive security world, at least two people have discussed API permissions abuse:
In the Azure defensive security world, Doug Bienstock, Juraj Sucik, and Jacob Skiba have created a tool called Mandiant Azure AD Investigator to help find evidence of adversaries abusing Azure app roles.
I also found Marius Solbakken’s blog to be an absolute treasure trove of Azure information which really helped me understand some of the more nuanced details of Azure.
Azure AD uses the concept of “roles” to dole out privileges to principals. For example, “Global Admin” is an Azure AD directory role. Azure API permissions are a wholly distinct, parallel set of permissions that can be granted to Azure service principals. There is some overlap between Azure AD directory roles and Azure API permissions, but I think it’s best to think of them as parallel privilege systems.
These parallel systems can be used to control access to the same objects. They can be used to grant access to the same objects. But the particular privileges granted within the Azure API permissions system are only taken into account when a principal is operating against the target object through that API:
Before we go any further, let’s establish some vocabulary here. These systems are very complex and it’s easy to get confused when learning about this.
Principal — an identity that can be authenticated. In Azure-land, a principal can be a user or service principal. When logging in with a username and password, you are authenticating to Azure with your user principal.
Azure AD App Registration — an application object residing in an Azure tenant. Azure Apps are where all the nifty configurations happen, where you grant users access to the app and have the app do “things”.
Service Principal — the identity an Azure App uses when it needs to authenticate to Azure. Service Principals can authenticate with a username and password — just like a user can. And just like a user, Service Principals can have control of other objects in Azure.
API Permission — an atomic, uniquely identifiable privilege, scoped to a particular Azure App. API Permissions come in two flavors: “Delegated” and “Application”. API Permissions describe what particular privilege is granted to the Azure App.
API Permissions in the MS Graph API are written in “Resource.Operation.Constraint” format. Example: “Directory.ReadWrite.All” means that the principal granted this permission can Read and Write to All objects in the Directory.
App Role — a permission granted by the Azure App, directly usable by the principal it has been granted to.
Delegated Permissions — a permission granted by the Azure App, but only usable on behalf of a user that has authenticated to the app. Principals cannot use Delegated roles themselves, but they can impersonate a logged on user who *does* have that role, using the role on the user’s behalf.
Application App Role — a permission held by the Azure App itself. The app can use this role without a user needing to log into the app first.
Resource App — the uniquely identified service principal associated with the application that your Azure App accesses. App Roles are defined per Resource App.
Depending on the context, all of these terms can refer to the same object: Service Principal, Enterprise Application, Resource App, and First Party Application.
Confused yet? I sure as hell was. Let’s get visual and explain these moving parts and how they connect to form attack paths.
One of the most common Resource Apps you will interact with as an Azure admin, defender, or attacker is Microsoft Graph. Basically, all of the abusable administrative actions you’d ever want to take are possible through the Microsoft Graph API.
Every Azure tenant has a Microsoft Graph Resource App. You can find it in your own tenant by searching for its (current) display name, “GraphAggregatorService”. In my tenant (and yours, and every other tenant), the “Application ID” for Microsoft Graph is 00000003–0000–0000-c000–000000000000:
Why the same ID? Because this app actually lives in a Microsoft-controlled Azure AD tenant. Let’s start thinking about these things in terms of a graph:
The app resides in the Microsoft tenant, but is instantiated as a service principal (or “resource app”) in the SpecterDev tenant. These objects have the same display name, but they are different objects with different IDs. Additionally, the trust boundary represented in blue above means a Global Admin in the Microsoft tenant can’t control the Resource App in the SpecterDev tenant, and that a Global Admin in the SpecterDev tenant can’t control the Azure App in the Microsoft tenant.
Now let’s add some app roles into the mix. App roles are specific to each resource app. For the sake of explaining one of the privilege escalation possibilities here, we are going to focus on two app roles: AppRoleAssignment.ReadWrite.All and RoleManagement.ReadWrite.Directory:
At this point, these app roles are merely available for an admin to grant to a service principal, but no one actually has these permissions yet. Let’s go ahead and grant the “AppRoleAssignment.ReadWrite.All” app role to another service principal. We are going to make this an “Application App Role” (as opposed to a “Delegated Permission”), so that the service principal itself has this privilege:
And let’s not forget that our “MyCoolAzureApp” service principal is associated with the Azure App, “MyCoolAzureApp”:
The stage is now set for “MyCoolAzureApp” to turn itself or anyone else into a Global Admin. To understand this, let’s discuss what these two particular app roles allow our service principal to do:
The Microsoft documentation says that the “AppRoleAssignment.ReadWrite.All” permission:
“Allows the app to manage permission grants for application permissions to any API (including Microsoft Graph) and application assignments for any app, without a signed-in user.”
What does this mean? This means that “AppRoleAssignment.ReadWrite.All” lets you grant yourself whatever API permission you want. This particular role also bypasses the manual, human-involved admin consent process. Having this role means that “MyCoolAzureApp” can grant itself “RoleManagement.ReadWrite.Directory”:
And what can we do with “RoleManagement.ReadWrite.Directory”? The documentation says:
“Allows the app to read and manage the role-based access control (RBAC) settings for your company’s directory, without a signed-in user. This includes instantiating directory roles and managing directory role membership, and reading directory role templates, directory roles and memberships.”
In other words, you can grant yourself any directory role you want, including Global Admin:
And with that, our attack path has emerged:
Here is a video of this attack path in action:
And here is the example attack code from the demo above:
In our last post about Azure privilege escalation, we provided some commentary around what Microsoft themselves may be able to do to mitigate or eliminate that particular attack path. An existing mechanism protects Global Admins from having their passwords reset by someone without the Global Admin or Privileged Authentication Admin role. We see that mechanism as potentially shutting down the entire attack primitive discussed in that post.
But this situation is different. It’s not immediately clear what Microsoft would be able to do to prevent privilege escalation via Azure App role abuse. These app roles are used by various organizations to support automated processes, and so the fact that the AppRoleAssignment.ReadWrite.All role allows you to bypass the admin consent experience really can’t be seen as a flaw, but as a crucial feature that enables full automation of privileged actions.
So what can we as defenders do about this? First, it’s prudent to proactively audit which, if any, of your apps have one of these two very dangerous app roles. As with everything in Azure, there are several ways to do this, but my favorite method is definitely using PowerShell.
To use this script, you will need a valid user and password for the Azure AD tenant. This user does not need any special roles or permissions — every authenticated principal can read this information. Populate the $AzureTenantID variable with your tenant ID, the $AccountName variable with your username, and the $Password variable with your plain-text password. Then, you can copy and paste the entire script into a PowerShell console, or dot invoke it:
Here’s the script in action:
This is your first prevention opportunity. The output of that script is telling us that two apps, “ThisAppHasOAuth2Permissions” and “MyCoolAzureApp” have app roles allowing those apps to promote themselves or another principal to Global Admin. Carefully determine whether those apps actually require those app role assignments, whether they can be changed to delegated permissions, or removed in favor of less powerful app roles.
If these app role assignments must remain, then recall from our last post that certain roles allow other users to create new credentials for service principals. The tenant-level and app-level “Application administrator,” “Cloud application administrator,” and “Hybrid identity administrator” roles directly or indirectly allow for this. Additionally, owners of application objects and their associated service principals can add new credentials for the app. Finally, any service principal with the Application.ReadWrite.All app role can add new credentials to any other service principal.
Don’t forget that Privileged Identity Management (PIM) means you need to audit principals with roles currently active, and users who can grant themselves a role.
Here’s what all those different possibilities look like in a graph:
Seems intimidating, right? Don’t forget that security groups can have roles now, so the above illustration can actually be rather simple in comparison to what’s actually possible.
This is your second (and third, fourth, fifth, etc.) prevention opportunity. Carefully audit the principals that can control highly privileged service principals and remove role assignments, ownership, and app roles where possible.
I very strongly recommend investing your efforts into removing the most dangerous configurations you can whenever possible. If an adversary abuses these configurations to escalate to Global Administrator in your tenant, it can be very expensive to completely remove the various persistence mechanisms available to the adversary with this level of privilege. Even then, new research is coming out all the time about novel persistence tradecraft in Azure, so you may not even be able to guarantee you’ve identified and removed all persistence installed by the adversary.
Even so, you may find yourself in a situation where these dangerous configurations must remain. There is also value in detecting when administrators legitimately add these dangerous configurations to your environment. There are a couple of options for detection particular to dangerous API permission grants.
First, you have the built-in “Audit-Logs” feature in Azure, which will by default log every API permission grant, telling you who made the change, who the permission was granted to, and what the permission and its scope are:
Next, you have the option of building a log analytics workspace and viewing all relevant events in a roll-up view in a monitoring workbook. Here’s how you set this up:
First, you need a Log Analytics workspace. You can either use an existing workspace or create a new one. Then, in our Azure tenant view, select “Workbooks” under “Monitoring” to scope your Workbooks to your Log Analytics Workspace:
Then, under the “Troubleshoot” section, you can see an example workbook which includes new app role assignments. This workbook is called “Sensitive Operations Report”:
Click this workbook, then select “New permissions granted to service principals”, which will show you whenever a service principal has been granted an app role:
Azure is a complicated and dynamic system, with new attack paths appearing, disappearing, and reappearing over time. There are very well-designed mechanisms in Azure that prevent the emergence of some attack paths, but those mechanisms are not guaranteed to exist or go unchanged in the future. Microsoft has some insanely clever people working for them, but at the end of the day, we must remember two things:
https://samcogan.com/provide-admin-consent-fora-azure-ad-applications-programmatically/
https://graphpermissions.merill.net/permission
https://winsmarts.com/automating-application-permission-grant-while-avoiding-approleassignment-readwrite-all-554a83d5b6f5
https://docs.microsoft.com/en-us/graph/permissions-reference#permissions-availability-status
https://dirkjanm.io/azure-ad-privilege-escalation-application-admin/
https://techcommunity.microsoft.com/t5/azure-active-directory-identity/understanding-quot-solorigate-quot-s-identity-iocs-for-identity/ba-p/2007610
https://docs.microsoft.com/en-us/azure/active-directory/develop/consent-framework
https://tech.nicolonsky.ch/exploring-the-new-microsoft-graph-powershell-modules/
https://practical365.com/connect-microsoft-graph-powershell-sdk/
https://www.inversecos.com/2021/10/attacks-on-azure-ad-and-m365-pawning.html
https://docs.microsoft.com/en-us/graph/resolve-auth-errors
https://tech.nicolonsky.ch/explaining-microsoft-graph-access-token-acquisition/
Thank you Stephen Hinck and Matt Hand for reviewing this post.
Posts from SpecterOps team members on various topics…
32 
32 claps
32 
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
Written by
BloodHound Product Architect
Posts from SpecterOps team members on various topics relating information security
"
https://medium.com/@mauridb/azure-sql-managed-instances-and-azure-data-factory-a-walk-through-bfb93e79ac0c?source=search_post---------262,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
May 11, 2018·5 min read
Now that Azure SQL DB Manages Instances are here, a lot of companies are trying to finally migrate their complex (multi-database, multi-dependency and database-centric) SQL Server database solutions to Azure SQL DB.
Once you have your Azure SQL DB Managed Instance running, you may also want to load or extract data from it. On-prem you may have used SQL Server Integration Services, and you may well continue doing so since SSIS Packages can run in the cloud thanks to Azure Data Factory:
docs.microsoft.com
but maybe you just want to create a shiny new Azure Data Factory pipeline to orchestrate your data curation and data movement activities using SQL MI along with other Azure technologies (Spark, HDInsight, Azure Data Lake or anything else Azure offers), without having to resort to SSIS Packages.
Well, in order to allow Azure Data Factory be able to connect to an Azure SQL MI there are some steps involved that may be not obvious at the first time. This post is here to help.
The Integration Runtime (IR) is the engine that allows Azure Data Factory to perform all its activities. The default IR doesn’t provide VNet support and thus it can’t be used to connect to SQL MI VNet, which means that it can’t be used to move data in and out of a SQL MI.
Using VNet is possibile on via the a Self-Hosted Integration Runtime. Self-Hosted means that you can install the IR engine in an Azure VM that has been configured so that in can connect to the SQL MI and thus can be used as a bridge between the SQL MI VNet and the outside world. There are two options to do that. The first is to create the VM in a SQL MI VNet subnet as described here (section “Create a new subnet in the VNet for a virtual machine”)
docs.microsoft.com
The other is to use an Azure VM in a separate VNet and peer it to the SQL MI VNet (look for the “Connect an application inside different VNet” section):
docs.microsoft.com
Once you have the VM ready you can install the IR Engine. Create an Azure Data Factory (v2) resource and click on the “Author & Monitor” link to open the Azure Data Factory portal. Then click on the pencil icon on the left to open the “Authoring” pane and from there click on the “Connections” link you can find at the bottom left of the screen. The “Connections” tab will open, and there you’ll have also the “Integration Runtimes” section. Click on it and you should see something like the following:
In my Azure Data Factory I can see also the ssis-runtime IR since I’ve enabled SSIS support. If you’ve just created the ADF resource and haven’t enabled SSIS support you won’t see that line.
It’s now time to create a Self-Hosted IR. Click on the “New” link an then select the first option: “Perform data movement and dispatch activities to external computers”:
The next option will ask you if the IR needs to access private resources. Of course it will, so make sure to select the “Private Network” option:
then give your IR a name and a description and finally you’ll see a page where you can download the integration runtime.
Download the installer via “Option 2: Manual Setup”, copy it to the VM you have created before and run it.
After the installer has finished its work it will ask for a key. Pick one of the key generated for you at the end of the creation of a Self-Hosted IR process and use it to register the engine:
after a couple of seconds you should receive the confirmation that the IR has been registered correctly:
and you should also be able to see it in the “Integration Runtimes” list in the portal:
It’s now time to create a pipeline that uses it!
The process is now just the same you follow to create a pipeline to load data into an Azure SQL Database with the exception that when creating the Linked Service that will allow pipelines to connect to Azure SQL you have to specify the newly created runtime:
Or, if you’re using the Copy Data Wizard you have to select “VNET in Azure Environment” as value for the “Network Environment” option in order to be able to choose which IR to use:
Once this is done, your pipeline will run just fine, moving data from a Blob Store to your database in the SQL MI:
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
34 
1
34 claps
34 
1
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/is-azure-profitable-3531a14f6233?source=search_post---------263,"There are currently no responses for this story.
Be the first to respond.
No one wants to buy into faltering products that are here today and gone tomorrow and the same is true of businesses with regard to their IT infrastructure. Given the productivity dependencies, a traditionally slow rate of return, and steep financial burden of IT infrastructure, CTOs and the like around the world often look to key performance indicators (KPI) including revenue, profitability, customer counts, and so forth, much like a prospective shareholder, in order to anticipate the viability of infrastructure as a service (IaaS) platforms such as AWS and Azure prior to making long-term investments in them.
For example, if a service provider is breaking close to even or operating at a loss after 8 years, this could be indicative of operational inefficiencies, architectural oversights, and looming changes which increase the likelihood of costly outages. All of which may also highlight the possibility of rate hikes down the road or that the platform in question may not be standing up to the test of time. But if revenue and profit are on point, this is indicative of long-term stability and much less risk. Comparatively, it is like being given the choice of paddle boarding on a calm summer morning or in the middle of a winter snow storm.
Needless to say, you can tell a lot about a solution by measuring its profitability. This is why common frontrunners such as Amazon, Apple, and Google are not bashful about disclosing individual revenue, profit, and user counts of their products. But this is also why the competition behind them tends to get creative rather than simply reporting on the same metrics. And when considering the lengths that Microsoft goes though in order to suppress the individual merits of Azure, I am forced to question just how profitable Azure truly is.
Before anything else though, it’s worth highlighting that any refresher course focused on lying with statistics would remind you that omission is by far the easiest way to lie with them. Along with shying away from helpful metrics while promenading with misleading and less valuable metrics instead, essentially leveraging data like an octopus jetting its ink, their application in unison is a formidable and proven recipe for statistical gaslighting that companies behind the frontrunner of their industry love to resort to. Put simply, some businesses choose to plea ignorant and resort to diversionary tactics rather than throwing it on the table so to speak and shining themselves in a negative light with the truth; Microsoft is no exception to this. Apple is also guilty of this as of late.
As mentioned before and as the #1 cloud infrastructure provider, Amazon happily reports on the individual merits of AWS by posting revenue, profit, and user counts. Why hide being the best? As the #2 cloud infrastructure provider though, Microsoft opts to bundle Azure’s earnings into a container called Intelligent Cloud which averages Azure’s revenue, profits, and losses with legacy server software such as Windows and SQL Server, Active Directory, Hyper-V and so on, making it impossible to compare the two platforms on equal ground.
Azure reports total users but not revenue or profit outside of the Intelligent cloud (crutch?). Meanwhile, LinkedIn reports revenue, profit, and total users while omitting monthly usership statistics such as monthly active users (MAU). Although it’s Microsoft policy not to report on helpful metrics such as MAU, hence why LinkedIn no longer reports it, they seem to have no problem reporting it for Azure AD, Office 365, Windows, Edge, Cortana, Bing, Skype (ooops now they don’t), XBox, Minecraft, and other services that perpetuate the narrative of having a strong foothold in their market. With the above in mind, you can see how Microsoft seems to flop their policies tactically, but you can also see a clear trend of omissions being a tell just the same while seemingly trying to mask them with arbitrary policies.
Sometimes not to speak is to speak with statistics and omitting data along with creative bundling tactics which Microsoft is leveraging at present are not exceptions to this, but the billion-dollar standard. Although they claim to be changed now, they still have the same general council, Brad Smith, that they’ve had since their laughable antitrust days, and we can also see Microsoft putting in extra effort into muddying the waters so to speak with run rates, obscure metrics, massive marketing overspending rather than simply presenting their data; as they have done historically.
However, we can still speculate by giving Azure the benefit of the doubt and simply assume that it is solely responsible for all of the Intelligent Cloud’s revenue in FY18 Q4, which was $9,606,000,000, so that we can have a look at Azure’s average revenue per account (ARPA) on its best hypothetical day and compare it to AWS. While we’re at it, let’s also assume that Azure has 13 million accounts rather than their 12.8 million just to account for growth since this data is over a year old. So let’s take $9,606,000,000/13 million accounts = $738.92 average revenue per account for their latest quarter. Not bad. Good job Hypothetical Azure.
AWS, on the other hand, has reported a paltry 1 million accounts subscribing to it at the moment while only generating 6.68 billion in revenue, 2.1 billion of which was profit, in FY18 Q3. So we can take $6,680,000,000/1 million accounts = $6,680 average revenue per account for their latest quarter. For the sake of comparison, we can then divide AWS’s ARPA by Azure’s ARPA ($6,680/$738.92) which shows us that AWS is monetizing its accounts 9.04x more effectively than Azure is. Also and if AWS could maintain this ARPA with Azure’s account-base (6,680*13,000,000), then it would be generating $86,840,000,000 in revenue per quarter. 😳
AWS being 31.4% efficient (profit/revenue) while being more 9.04x more efficient than Azure when measured by its ARPA also indicates that Azure efficiency could be as low as 3.5% or $359,320,000 in Q4 which in itself would easily rationalize bundling it into a container such as the intelligent cloud comprised of more efficient products. This would also mean that Azure could be generating could be generating as little as $27.64 average profit per account compared to the $2,100 average profit per account that AWS is seeing at present which is still 76x more than Azure; when it is given a significant benefit of the doubt.
These differences only become greater if Azure represented half of the intelligent cloud’s revenue at $4.8 billion though. If that were the case, then they would be making averaging $369.23 in revenue per account and netting anywhere in between $163 million on the low end to $1.5 billion in profit if they’re as efficient as Amazon this quarter. If accurate, this would also show AWS to be as much as 18x more efficient than Azure from the perspective of ARPA. But I digress.
In summary, when a data-driven technology company as equipped as Microsoft turns their pockets inside out instead of posting basic KPIs such as itemized profits or MAU while reporting on metrics that no one asked for instead with regard to a service that’s been in production for 8 years now, it is usually a consequence of these KPIs being contradictory of the narrative that they’re selling, not because they are not readily available. In lieu of these metrics and even when giving Azure the benefit of the doubt and compared to AWS as done above, there appears to be more disparity between AWS and Azure than Microsoft would like us to believe after being in production for 8 years. Azure may indeed be profitable, but when considering the disparity in operational efficiency between AWS and Azure and the exhaustive effort that Microsoft makes towards suppressing its individual performance metrics, I am given no option but to ask how profitable Azure is or whether it is profitable at all.
#BlackLivesMatter
94 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
94 claps
94 
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-best-courses-for-az-104-microsoft-azure-administrator-associate-certification-exam-in-2021-7b620d61dcd8?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for the AZ-104 exam or Microsoft Azure Administrator certification and looking for the best online courses then you have come to the right place. Earlier, I have shared the best AZ-900 courses and best AZ-204 courses, and today, I am going to share the best courses to crack the AZ-104 exam in 2021.
Acquiring skills in Cloud computing is fast becoming an opportunity and asset since most companies are applying cloud technologies in all their operations or looking for ways to learn how to do that. This is why becoming certified as a Microsoft Azure Administrator is now a big step towards advancing one’s carrier.
With the current industry trend, it is safe to say learning how to create applications that have some if not all of their components hosted in the cloud has become a skill that is very high in demand.
Candidates, who are considering studying for this course, must have a working knowledge of cloud infrastructure, ARM templates, Azure Portal, PowerShell, virtualization, storage structures, networking, command Line Interface, and operating systems.
If you’re not sure if this certificate would benefit you, then ask yourself if you are an Azure Administrator who works with cloud computing like computing cloud capabilities and services like storage, security, networking, etc. If you do, then having this certificate will be a massive step towards a more advanced career. The Microsoft Certified Azure Administrator — Associate Exam AZ-104 (newer version of exam) tests your ability to manage Azure subscriptions and resources. They want to make sure you can use the storage and virtual machines (VMs), configure virtual networks, and handle identities. Below are courses that will help you prepare for your Microsoft Azure exam.
Without wasting any more of your time, here is a list of the best online training courses to prepare for Microsoft Certified Azure Administrator certification with code AZ-104.
These courses are created by expert trainers and certified Azure administrators. You can join them on Udemy and Pluralsight, two of the most reputed online learning platforms.
This is the best online course for Microsoft Azure Administrator Certification or AZ-104 certification. Created by Scott Duffy this course is termed a best seller on Udemy and has almost 80,000 students enrolled.
This course is fully updated to cover recent changes on the AZ-104 exam and it's also one of the most comprehensive courses for the Azure Administrator exam.
The course content covers:
Its target audience is senior technical people with exposure to Azure, operations teams who want to learn more about implementing cloud solutions, and people taking the Azure AZ-104 test. It, however, does not cover Azure development or architecture
Here is the link to join this awesome AZ-104 course — AZ-104 Microsoft Azure Administrator Exam Certification 2021
This course is very good for mastering the fundamentals of Azure. It covers the concept of portal navigation and is an excellent choice for beginners in MS Azure. It, however, demands you to have some knowledge of PowerShell / CLI commands. Additional materials are recommended after this course if you are a newbie. It has more than 6000 students enrolled with a rating of 4.5 and is taught in the English language. The course was created by Anand Nednur. Before you take it, you need an intermediate understanding of Azure resources such as virtual networks, virtual machines, Azure AD, storage, and subscriptions and access to a paid or trial Azure to complete assignments. It also covers various Microsoft Azure administrator concepts from how to manage Azure subscriptions to how to integrate them on-premises networks with Azure virtual networks.
Here is the link to join this full course — AZ-104: Microsoft Azure Administrator
If you have registered for the Microsoft Certified Azure Administrator — Associate Exam AZ-104, then this course will help you pass it. In this course, you will learn how to manage subscriptions and resources on Azure and secure storage, implement and manage virtual machines and advanced virtual networking  This course is divided into three sections so you can go straight to the level you belong to at the moment. It has beginners, intermediate and advanced levels.  The beginners’ section curriculum covers:
Here is the link to join this AZ 104 training path — Microsoft Azure Administrator (AZ-104)
It also has an Intermediate section that covers the following key skills from the Azure Administrator exam point of view:
And, the Advanced section curriculum covers:
By the way, to access this course you would need a Pluralsight membership which costs around @29 per month or $299 per year. Alternatively, you can also try their 10-Day free trial to check out these courses.
pluralsight.pxf.io
This course contains well-structured videos that give an overview of each topic but still get specific to help you learn the material.
It is available in English, and more than 13,748 students enrolled with a rating of 4.4 out of 5.0. The target audiences for this course are Azure Administrators and Engineers and Systems Administrators, looking to expand into Azure. You need basic knowledge in Networking, OS (Windows/Linux) Background with experience in PowerShell Client on macOS or Windows and Azure Trial from Microsoft or Paid Subscription. This course follows the Microsoft Certified Azure Administrator — Associate Exam AZ-104 outline that will enable candidates to study specifically for the exam with Skylines Academy SKYLABS Guide to practice even when you are offline.
There are an ARM and Automation bonus course included which, although not mandated, is a very solid foundation for administering Azure.
Here is the link to join this Azure course — Microsoft AZ-104: Azure Administrator Exam Certification
This is one of the highest-rated Azure administrator course from Udemy. created by Alan Rodrigues this course prepares you to take the Microsoft Azure Administrator Exam AZ-104 and It has a rating of 4.6, with 26,444 students enrolled.
The course is very engaging and you will find both quizzes, exercise as well as lab tasks to learn essential Azure concepts from the Azure Administrator's point of view.   The course content reveals the following topics.
But before you take this course, an understanding of on-premises virtualization technologies, network configuration, Active Directory concepts, and resilience and disaster recovery is required.
Here is the link to join this AZ-104 online course —AZ-104 Microsoft Azure Administrator
This is another set of practice questions that I highly recommend you to check before you go for Microsoft AZ 104 exams. This AZ 104 Exam simulator is offered by David Mayer of Certification-questions.com and it contains around 250 questions and most importantly it is always updated to cover recent exam changes.
Here are some important details about this exam simulator:
- Microsoft AZ-104 Practice Exam: Microsoft Azure Administrator
- Number of Questions: 246
- Exam Tests: 5
- Last Update: 2021–01–10
Here is the link to join this AZ 104 exam simulator — Microsoft AZ-104 Questions
Practice tests are a very important part of your Azure Certification preparation strategy as you can use this to build the speed and accuracy required to solve exam questions within the stipulated amount of time.
This Practice AZ-104 Azure Administrator exam on Udemy has 250 high-quality questions with detailed explanations so that you can pass AZ-104 with confidence!
There are a lot of courses out there that are claiming that their courses are fully updated, but they’re actually not! This practice test on Udemy contains the latest additions on the new AZ-104 topics, the exam received an update in Jan 2021.
These Microsoft Azure Administrator practice tests are patterned after the latest exam format and were crafted based on the feedback of their 40,000+ students that have enrolled in our Azure preparation courses.
Each question has detailed explanations at the end of each set that will help you gain a deeper understanding of the Azure services.
Here is the link to join this AZ 104 Practice test — AZ-104 — Microsoft Azure Administrator Practice Tests 2021
That’s all about some of the best courses to learn the Microsoft Certified Azure Administrator exam. These online training courses are very good to prepare for this prestigious exam and they cover all exam topics and provide structured learning experienced.
This means you can prepare for this exam in less time and also score high at the same time. Though, You don’t need to join all these courses, you can do well even if you join one or two courses from this list.   Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure Administrator courses for the AZ-104 exam useful, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. — If you are keen to pass the Microsoft Azure Administrator certification but looking for free online training courses for your preparation then you can also see Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It’s completely free and all you need is a free Udemy account to join this course online.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
222 
2
222 claps
222 
2
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://towardsdatascience.com/full-stack-machine-learning-on-azure-f0f6b77be07e?source=search_post---------265,"Sign in
There are currently no responses for this story.
Be the first to respond.
Doug Foo
Oct 29, 2019·20 min read
Guide to building a Full Stack Machine Learning site on Azure using MERN (Mongo, Express, React (Node.js))
Having gotten rusty on the latest tech, I took it upon myself to concurrently learn a whole bunch of things at once and build http://diamonds.foostack.ai which is now live. (Basic machine learning app to price diamonds).
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-run-and-deploy-react-with-nodejs-backend-on-azure-app-services-b853f6e5234f?source=search_post---------266,"There are currently no responses for this story.
Be the first to respond.
If you want to deploy your application on the managed platform by selecting the runtime, An App Service is the right choice. You can create a function app, web app, logic app, etc under Azure App Services. When it comes to React…
"
https://medium.com/hashmapinc/business-wont-wait-migrating-to-azure-for-data-analytics-15736d2bc59?source=search_post---------267,"There are currently no responses for this story.
Be the first to respond.
by Ed Fron, Enterprise Architect
I recently worked with an industrial company that was retiring an on-premise, traditional, Big Data platform that was operational, but it just wasn’t keeping up with business scenarios that spanned multiple business areas or data domains, and it was also struggling with handling data volumes and data loading requirements without requiring significant administrative overhead and budget.
In this post, I’ll highlight the solution set they chose and discuss some of the benefits they are realizing already in moving to a new approach that takes full advantage of the cloud.
Importantly, they weren’t looking for a like-for-like style replacement (moving to another traditional Big Data platform or IaaS-based solution), but were focused on reviewing the range of SaaS-based, cloud-native solutions that were available in Microsoft Azure (their chosen cloud partner) so that they could either eliminate or significantly reduce the infrastructure effort associated with operating and maintaining existing analytics applications and delivering new apps at a more rapid pace.
It was also compelling to move to cloud solutions that would eliminate static limitations on usage, configuration, and capabilities while providing a consumption-based approach and paying only for the services that they used versus a static capital sunk cost model — an IaaS cloud model wasn’t going to cut it.
Before jumping straight into the cloud solution set they chose, below is a quick recap on how the existing traditional solution was being used. They had centralized a significant number of structured and semi-structured datasets into their Hadoop-based enterprise data lake for original data fidelity and then worked with a combination of Apache Hive as a consumption zone and a big data analytics workbook solution that ran on the core platform that helped their business analytics parse through the datasets to extract value and insights.
From a data flow perspective, analysts would create workbooks which would be processed in the Hadoop environment and then exported to Hive tables for broader use by data consumers that were using the workbooks for data interaction or PowerBI for visualization. Workbooks sourced data from other workbooks or from datalinks, and a series of joins, calculations, etc. produced a final sheet containing all required data for export within a single workbook.
This architecture and overall process, while it worked, came with a corresponding amount of overhead and limitations.
When we got the call to assist the client in their cloud migration and deployment in Azure, they reviewed with us the core solution components that they had selected (after an in-depth review and testing process) and that we would be working with during the project — I’ll quickly highlight those below to give you a sense for the architecture and how each component is being used.
Below is a high level architectural diagram with each component slotted in.
Moving from left to right on the diagram you’ll see the common data sources such as on-prem data shares, cloud storage, on-prem data warehouses, and both Oracle and SQL databases.
Azure Data Factory is a managed orchestration service that allows moving data using multiple data source connectors from a source into Azure Blob Storage for original format storage. Once the data lands in Blob Storage it’s then available for processing by the Azure Databricks Spark engine. The file ingestion into Snowflake’s Cloud Data Warehouse was defined in an Azure Databricks Notebook and orchestrated using Azure Data Factory pipeline.
Dropping down to the lower part of the diagram, for relational data the Attunity Data replication solution was used to simplify and accelerate the process of migrating and consolidating data from different internal and external database sources into Snowflake. Attunity moves data from Oracle and SQL databases into Snowflake without having to write complex ETL code and works in for both batch and real time. Once the lands in Snowflake it can be picked up and processed by the Azure Databricks Spark engine.
From that point, each “traditional” Big Data workbook was analyzed and refactored into one or more Azure Databricks Notebooks. Each Azure Databricks notebook read data from Snowflake tables into a Spark dataframe, and then executed transforms (aggregations, grouping, joins, filters, calculations, etc.) to ultimately produce the desired final Spark dataframe which is published to Snowflake for highly concurrent, interactive data consumption by PowerBI users (interactive dashboards and reporting) and also other Azure Databricks notebooks. Azure Data Factory provided an orchestration service for the pipelines to ensure that the schedule was enforced and to track any runtime dependencies.
That’s the basic pattern that was used to migrate well over 1,000 existing Big Data analytics workbooks to Spark jobs in Azure Databricks.
I’d say that the biggest challenge during the cloud migration was interpreting how each individual business analyst wrote their individual workbooks since the tool that was used previously did not enforce and normalize concepts on breaking the workbooks down into smaller workbooks, transforming them, and creating the final workbook — there were alot of one-offs that required fairly significant business analysis.
Looking back, here’s my perspective as an implementation and consulting partner on some of the high level benefits that were realized by the client:
I hope this gave you a sense for the solution used to retire a significant base of on-prem, traditional Big Data infrastructure and associated workbooks using a combination of high value, complementary Microsoft Azure services.
Although you should always work from your own desired business outcomes and use case requirements keeping in mind restrictions around organization, existing technology frameworks, data access, etc., in general, the approach above can be used as a template for other data and analytics cloud migrations and the benefits that can be expected.
We continue to be asked to assist clients with this type of cloud-first solution approach while building and engineering end-to-end pipelines to derive quicker, more cost-effective value from their data. We look forward to seeing more customers succeed and doing a lot more together in the near future!
If you’d like additional assistance in this area, Hashmap offers a range of enablement workshops and assessment services, cloud migration services, and consulting service packages — we would be glad to work through your specific requirements — please reach out.
www.hashmapinc.com
Feel free to share on other channels and be sure and keep up with all new content from Hashmap by following our Engineering and Technology Blog and subscribing to our IoT on Tap podcast.
Ed Fron is an Enterprise Architect with Hashmap providing Data, Cloud, IoT, and AI/ML solutions and consulting expertise across industries with a group of innovative technologists and domain experts accelerating high value business outcomes for our customers. Connect with Ed on LinkedIn.
In you enjoyed this story, here are some other recent posts from Ed for quick access:
medium.com
medium.co
Innovative technologists and domain experts helping…
44 
44 claps
44 
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
"
https://medium.com/microsoftazure/autoscaling-azure-sql-hyperscale-d6a30efb1f95?source=search_post---------268,"There are currently no responses for this story.
Be the first to respond.
Azure SQL Hyperscale is the latest architectural evolution of Azure SQL, which has been natively designed to take advantage of the cloud. One of the main key features of this new architecture is the complete separation of Compute Nodes and Storage Nodes. This allows for the independent scale of each service, making Hyperscale more flexible and elastic.
In this article I will describe how it is possible to implement a solution to automatically scale your Azure SQL Hyperscale database up or down, to adapt to different workload levels dynamically and automatically without requiring any manual intervention.
Now, first things first. I will not go into details here, as they are well described in the official doc, but it is important to understand how the SQL Server engine has been refactored into Azure SQL Hyperscale, as it is the foundation for a fast scale up and down of the selected service tier. Here is a picture that explains it very well:
One of the main benefits of this architecture is that scaling up or down the Service Level Objective, which means cores and memory, is something completely unrelated to the size of the database. Let’s say, for example, that you want to move from an HS_Gen5_2 to HS_Gen5_4. This is an overview of what is happening:
As there is no data movement involved, this workflow happens quickly. In several tests I ran lately, I had a constant value of a maximum of 2 minutes to spin up a new compute node of the Service Level Objective you are asking for, and something around 12 seconds to switch the control to the newly created node.
For all the time needed to spin up the new compute node, all existing connections still are open and transactions against the database are processed as usual. It’s only during the 12 seconds of switch that, at some point during that time range, connections will be moved to the new compute node. If you have a long-running transaction being executed at the time of cut-off, it will be rolled back: thanks to the new Accelerated Database Recovery, this operation will be almost immediate, and it will not prevent the scaling action from being completed. Of course, it is always good to avoid re-running transactions, as they may be resource-intensive, so follow the best practice to keep the connection open only for the time needed. If you are used to working with HTTP and REST, you are already doing this: just keep doing the same with Azure SQL too. If you are using some tool like Dapper, you don’t even have to bother about managing connections, unless you have some specific requirements. It will automatically open the connection just before you run a command, and automatically close it once the command is done. This is all you need to write:
Azure SQL databases provides a useful DMV, sys.dm_db_resource_stats, to monitor resource usage:
It keeps data for about 1 hour, aggregated by 15-seconds buckets. With such data, it would be possible to do some fancy stuff using machine learning and some cool forecasting algorithms, but for the sake of simplicity, we will use a very simple moving average to smooth out peaks and figure out if we need to scale up our Azure SQL Hyperscale database. If you are interested in better algorithms to decide if you need to scale up or down, you will find some links and resources at the end of this post.
For now, we can use a window function to calculate the moving average over the last minute.
Aside from the moving average, the query will also return the number of data points used to calculate the average. This is useful when determining if scale up or down action should really happen. In fact, just after a change of Service Level, say from 2vCores to 4vCores, the used DMV will be completely empty. So, for one minute, you will have an average calculated on way less data that what you want…and this could lead you to make wrong assumptions about the real resource usage. Due to the application reconnecting to the database, the first seconds could be even without load…and you should not take any action (for example scaling down) based on that incomplete set of data. For this reason, the sample solution will ignore all actions suggested by a dataset that doesn’t have at least a minute of data.
If you want to calculate the moving average over a longer time duration, to smooth out very short spikes of workload that could trigger unwanted scaling actions, you can just increase the number of data points, keeping in mind that each data point represents 15 seconds of workload. This is the code you want to look at:
As you can see, it now takes the current data point plus the 4 earlier data points (as data is ordered in descending order by time).
In case you need more than one hour, you can use the sys.resource_stats view, where data is stored, with a 5-minute granularity, for 14 days.
Look at OVER Clause documentation to understand what other options you have and how it works.
Now that we have a simple algorithm that will tell us when scale-up is needed or when scale-down is recommended, we just have to implement a monitoring job that will get such data and kick off the scaling operation.
This can be done using Azure Functions, as they support Timer Triggers, which will allow us to do exactly what we want.
Every 15 seconds the Azure Function will wake up and will get the current performance data from an Azure SQL Hyperscale database, along with the 1-minute moving average. If the moving average is within some defined boundaries it will go back to sleep. Otherwise, autoscaling action will be started. Here’s an example of such Azure Function in action, logging into dbo.Autoscaler table so that it is possible to check what happened and why:
As visible in the picture above, in the red box (box “1”), around 22:12 GMT a workload spike began, bringing Azure SQL Hyperscale CPU usage close to 80%. After a minute of this sustained workload, the moving average went over the 70% boundary and thus the autoscaler started to request a bump in the Service Objective, asking for a scale up to 4 vCores.
The scale-up happened within 30 seconds and then autoscaler waited for the warmup of the new Service Objective to finish (the NULL values in the RequestedSLO column), as during warmup too few data points were available to make any decision. Once the number of data points was 5 again, it started to evaluate the workload and found that 4 cores were enough to handle the existing workload. In fact, in the yellow box (box “2”), you can see that CPU usage is half of the value existed before, when the workload spike started.
One thing that is important to keep in mind, is that every time that scale up or down happens, the local RPBEX cache will start fresh. If the workload is critical, this can have an impact on initial performance, as data needs to be pulled from page servers into the local compute node cache. To help the cache to be warm as fast as possible you may want to execute some specific query as part of the autoscaler solution, just to make sure the most-used data is in the local cache. This is not a requirement and really depends on the workload you have but is some cases can really help a lot to boost the performances after the new Service Level Objective is ready to be used, so keep this option in mind if you see slower than expected performances just after scaling up or down happens.
At a later point, around 22:20 GMT the workload spike ends and, in a minute, the autoscaler detects that 4 vCores are now even too much for the current workload and initiates a scale-down to 2 cores (green box, number “3”).
To simulate a simple workload, you can use powerful load testing tools like Locust, K6 or Apache JMeter where you can simulate even the most complex and realistic workload. To create a simple workload, a nice tool originally written by Adam Machanic is also available: SQL Query Stress.
Albeit quite old now, it allows you to execute a query using the desired number of parallel threads, so that you can create some workload in the database. It is very easy to use, configurable enough and is great to quickly test the autoscaling sample. In fact, a .json configuration file is also included in autoscaler’s GitHub repo, so you can get started right away.
Sample source code is available here. If you want to contribute, by adding some of the improvements described below, please feel free to do so. I’ll be more than happy to accept contributions to this repo.
Using the moving average to figure out if Azure SQL Hyperscale tier should be scaled up or down can be a little bit too simplistic if you have a scenario where workload can vary with very complex patterns. It works well when you have a situation where the workload spike endures for a consistent amount of time, and if your workload fits that use case, great! Just keep in mind that in the sample we’re only monitoring CPU usage, but there are also data and log IO usage that could be monitored. As you can guess, what you want to monitor, and how you want to use that data to activate the scale up or down, is up to you and very specific to your workload scenario.
As mentioned already, this is just a sample, and it should be used a starting point to create something there fits your own use case. But that’s the only improvement you can think of.
In fact, the current sample just reacts to an increased or decreased workload…which means that it will starts to take an action when the workload has already started to change. If the workload spike is huge, the allocated resources may not be able to satisfy it within the desired performance SLA you must guarantee, as scaling will happen because of that workload change, and sometimes even causing a larger impact due to the necessary failover during the spike.
Another approach would be to try to be proactive and predict the workload so that the Azure SQL Hyperscale tier could be changed before the spike happens, and thus always guaranteeing the best performance possible.
Machine learning is an option in this case, of course, as well other existing prediction algorithms. A nice blog post on how Netflix approaches to this problem is available on their blog: Predictive Auto Scaling Engine.
More in general the problems fall under the umbrella of “Demand Forecasting” and there are already some cool algorithms in Azure Machine Learning services:
If you want even more options this blog post is amazing: Time Series in Python, Exponential Smoothing and ARIMA.
Azure Machine learning can be easily integrated with Azure Functions, and you can really create spectacular solutions using Azure SQL Hyperscale, Azure Machine Learning and Azure Functions.
There are two alternatives to the proposed sample. The first is to use Azure Alerts, where you can do the same as described here, but without writing code. Just be aware that the telemetry log available to Azure Alerts have some latency — usually between 2 and 5 minutes — which means that response to workload spike will be not as fast as polling the database DMV as the sample does. If you don’t need to respond to workload spikes in less than 5 minutes, Azure Alerts can be a great, simpler, alternative.
Another “sort of” alternative is to use Azure SQL Serverless database, which incorporates scale up and down natively.
I said “sort of” because the Serverless option is not available to Azure SQL Hyperscale now so it is not really an alternative, but if you are fine using the General Purpose service tier, you’ll get autoscaling right away, just by configuring it.
Autoscaling a service is a challenging job, especially if the workload is not easy predictable. But thanks to Azure SQL Hyperscale elasticity and quick scalability this can be done using an Azure Function, with minimal service interruption, especially if following the best practices and implementing correct connection retry logic. Remember also to use the latest .NET driver as they already implement some reconnection logic that in several common cases will make the reconnection completely transparent.
Any language.
96 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
96 claps
96 
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.cloudboost.io/how-cloudboost-uses-docker-kubernetes-and-azure-to-scale-60-000-apps-d54d7eaf02c9?source=search_post---------269,"CloudBoost.io is a server-less platform + backend as a service (BaaS) which helps developers build apps in half less time by taking care of all the mundane tasks like Authentication, Notifications, Emails, Managing and Scaling your Database, Files, Cache, and a whole lot more. We use MongoDB and Redis clusters for data store and NodeJS powers most of our micro-services. CloudBoost is completely open source under Apache 2 License, so you can modify it the way you like and install it on your own servers for free. You can check out our GitHub here : https://github.com/cloudboost/cloudboost (Pull Requests are a LOT appreciated!)
Before I dive into how we use Docker in production, for readers who are very new to containers. Let me explain what a “container” actually is. If you’re someone who knows about Docker and containers. Please feel free to skip to the implementation part of this post.
Before containers, we used to install our pieces of our application stack on a VM. The problem with that approach was — Let’s say you need to install MongoDB to power your app then you would have three (or more) machines running MongoDB and you would literally name them by what’s installed on each of these machines — mongo-1, mongo2, mongo-3. You could not have any other piece of your stack (let’s say Redis) installed on the same VM because the memory and compute were not isolated between these two.
Think of containers as tiny self-contained isolated packages which has enough software to run a particular service. For example: MongoDB container has enough software backed in to run MongoDB and nothing else. Containers can then be installed on a VM (or bare metal) and you can have as many containers you want on a machine. Containers help you isolate these services, so you can literally install multiple services on one machine.
Consistency and Isolation : The biggest advantage is consistency and isolation. Whatever works in your test — will work in staging and production because you’re literally using the same container image for those environments. It also helps you have consistent environment across your entire ops pipeline. All of the containers are isolated with each other so you can run multiple containers on one machine.
Resource Utilisation: We’ve had problems with resource utilisation before — For example if your MongoDB uses just 10% of compute on a VM. 90% of compute would be rendered as waste which would be a LOT of compute and resources. Containers help us pack one or more services and isolates them from each other which actually helps us utilise the majority of the compute on a machine before scaling out.
Before deploying our stack on containers (when we were on plain vanilla VM’s), our average cluster utilisation was under 15% which was a HUGE waste of compute.
Scale: Scale was hard. Really hard. We used tools like Chef which was responsible for managing and installing software on a VM. The problem with these tools was they were clunky, they did not work well and had a huge learning curve. Most of the times, they needed multiple modules to be installed and configuring each one of them was tough and even if we could configure them — they were not reliable. Installation failed a lot of times, and we had to try over and over again for it to succeed. Looking at Chef forums and StackOverflow, a lot of other DevOps team faced similar issues and not just us. Ultimately, working with Chef created too many internal and external conflicts for us to produce significant ROI.
Container ecosystem has better tooling by far and is built for scale (like Kubernetes or Docker Swarm) which helped us scale our services flawlessly to many machines. More on this later.
Micro-services : Microservices are a very attractive DevOps pattern because it helps teams deploy and scale each of these services independently which tremendously increases speed to market. Engineers responsible to work with few services do not need to know about the code in other services which also helps a new hire get started fairly quickly and keeps the codebase for each of these services small. With each micro-service being developed, deployed, run, and maintained independently (often using different languages and technology stacks), these allow companies like us to “divide and conquer,” and scale teams and applications more efficiently. When the pipeline is not locked into a monolithic configuration — of either toolset, component dependencies, release processes, or infrastructure — there is a unique ability to better scale development and operations. It also helps companies easily determine what services don’t need scaling in order to optimise resource utilisation saving them cloud expenses.
Containers (like Docker) are just packages which has your service (and everything else that you need to run that service baked in) and they need to be installed on a VM / Bare Metal for it to run. On a production workload — you would literally have many machines (sometimes thousands of machines) that form a cluster and need an orchestrator like Kubernetes / Marathon / Docker Swarm to make sure containers are installed properly, can talk to each other and can scale across the entire cluster.
Orchestrator basically takes a pool of servers and makes it into one giant machine which has the combined compute and storage of all the VM’s in a cluster. You can now install any number of containers on this giant machine, and orchestrator will “orchestrate” containers for you — It takes care of all the heavy lifting for you — like which underlying machine to actually install a particular container on, takes care of updating these containers and moving it to new machines when a machine fails, and a whole lot more.
CloudBoost uses a 50 node Kubernetes Cluster (D2 VM’s) on Azure Container Service. Why not Marathon (DC/OS) or Docker Swarm? We did review all of these options back when we started migrating to Docker a year ago. Docker Swarm was too early back then and now to all its fairness has matured a lot and can be used in production. DC/OS is a lot more than an orchestrator. Its a Datacenter OS. We just needed an orchestrator to work with and Kubernetes was being used in production by a number of companies and was a perfect match for us back then (and it still works like magic!).
Data Store: CloudBoost uses MongoDB containers configured in a replica-set and Redis containers which form a cluster for our data store layer. MongoDB containers need a Persistent Volume (and a claim) so the data which is hosted on these containers are retained even if container crashes and restarts. We’ve plugged into Azure File Storage for elastic storage. We also have a container that is basically connects to MongoDB cluster and backs up data every single day and retains the last 7 days of data for every app that’s hosted on the platform. Backup container is hooked into Azure File Storage as well.
Micro-services: CloudBoost uses number of micro-services. CloudBoost Core which serves API calls and data to apps that are hosted on our platform. User Service which handles authentication and session management. Analytics measures API analytics for every single app, and more. All of these are packaged on NodeJS Docker containers which you can find here. All of these micro-services have K8s replication controller attached to it which is responsible for self-healing, and scaling a particular service which we then hook K8s service and Azure Load Balancer — to expose these to the outside world.
User Interface: User Interface containers serve static HTML, CSS and JS out to clients. They’re plain express containers which serve static files. They’re usually React apps for most part and we use CloudFlare on top, to CDN and cache most of our content which saves us a lot of bandwidth every single month.
Build Pipeline: We use TravisCI to build all of our docker images, run tests on it, push it to docker hub if all the tests succeed. Once out on Docker Hub our build pipeline runs rolling-update on our K8s cluster to update all of our containers in production. You can check out our Travis file here. We also do a GitHub release whenever our build succeeds.
Cloud Expenses: We save 45–48% of cloud expenses after migrating to Docker because of better resource utilisation.
Scale: Scale is painless. You would just ask kubernetes to scale a particular container for you and it does it seamlessly. K8s team also launched auto-scaling which takes care of this automatically. We used to get paged 7–10 times/month before moving to Kubernetes + Docker, now its just once in a month or two.
Simplified Builds: Code bases are MUCH smaller and understandable. Build runs and code gets deployed to production in parallel between services. Each engineer can scale and focus on one or two services instead of a huge codebase and more importantly — when we receive a pull request from our open source contributors and customers, we ship their code into production in minutes instead of weeks which surprises most of them and makes them smile. :)
We also have a Docker Cloud / Compose file here which you can use to install CloudBoost on any cloud or machine you want. We’re also working closely with Docker Store team to launch CloudBoost on the store for enterprises which contains advanced enterprise features like granular control of your team, cluster admin controls and more. More on this in another post.
The Realtime JavaScript Backend.
78 
2
78 claps
78 
2
Written by
Founder, HackerBay.io
The Realtime JavaScript Backend.
Written by
Founder, HackerBay.io
The Realtime JavaScript Backend.
"
https://medium.com/@wuehler/global-peering-on-ethereum-with-azure-3034fb31f568?source=search_post---------270,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Wuehler
Jun 21, 2017·2 min read
At INFURA, one of the primary tenets of our mission is to be a source of truth to the decentralized world. To be that reliable source of truth, some of the most fundamental data elements we want to provide are accurate and timely block and transaction data. As an infrastructure pillar, we aim to peer as widely as we can across the global Ethereum network so that we are receiving the block and transaction data as fast as possible and relaying it as effectively as we can. To address these goals, we implemented the following:
The default peer count limits for the two popular Ethereum clients take into account reasonable P2P limits so as not to overdo network traffic. For geth, the default is a max of 25 peers, for Parity, the default max is 50 peers. These max values are adequate for a single local node relaying transactions occasionally to the network. However, for a platform built for high throughput, such as INFURA, we target as many as several hundred peers to position us as widely as possible.
With a goal of achieving several hundred peers, at times we found it difficult under the existing libp2p discovery mechanism to achieve that goal. To address this, we decided to deploy a single node under our own control in data centers around the world. We reached out to our partners at Microsoft and were able to identify 32Azure data centers as targets:
Using the Azure CLI, we scripted the deployments making this a relatively painless process. As these nodes start, we record the enode for each and feed it into a central list. Then, as we start our primary INFURA nodes, we can feed all of the 26 remote enodes to our primary using the command-line argument --bootnodes [comma-separated list of nodes] in geth. The effect is that we instantly have bootstrapped with 26 nodes from every corner of the planet. Following this strategy, we have been able to achieve and sustain a significantly higher peer count. We continue to fine tune the approach and are using our remote nodes for our Fukurou project which is an entire network crawler. The goal of Fukurou is to understand the topography of the entire Ethereum network.
Over time, we expect these strategies will reinforce the INFURA service offering as a robust and solid source of truth for the Ethereum network as a whole. We continue to invest in our strategy as an infrastructure powerhouse to serve the entire Ethereum ecosystem as it matures.
For more information about INFURA and to start using our service immediately for free, please visit https://infura.io.
Infura Co-Founder, ConsenSys, Ethereum NYC Founder
69 
1
69 
69 
1
Infura Co-Founder, ConsenSys, Ethereum NYC Founder
"
https://blog.jeremylikness.com/lift-and-shift-your-net-app-to-azure-41c1fd6a9e43?source=search_post---------271,
https://medium.com/@renatogroffe/azure-tech-nights-2020-saiba-como-foi-v%C3%ADdeos-gratuitos-5e8e33d438e?source=search_post---------272,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 20, 2020·4 min read
Entre os meses de Fevereiro a Abril/2020 aconteceu a terceira edição do Azure Tech Nights, um evento online, gratuito e noturno do Canal .NET focado no uso de tecnologias e serviços que integram o Microsoft Azure.
Seguem alguns dados sobre o Azure Tech Nights 2020:
Aproveito este post para agradecer a todos os organizadores, palestrantes e ao público que nos acompanhou por todos o apoio para que esta iniciativa fosse um sucesso! Deixo muito obrigado ainda ao Jackson Feijó (Microsoft) pelo auxílio divulgando diversas das sessões do evento.
A seguir estão alguns prints mostrando os picos de espectadores simultâneos ao longo do evento:
Pensando naqueles que não puderam acompanhar ou, até mesmo, gostariam de rever alguma apresentação, foram agrupados neste post os links (indicados abaixo) de todas as palestras realizadas ao longo dos 9 dias de evento:
E aproveito este espaço para deixar aqui um convite…
Caso precise conhecer mais sobre Azure DevOps, não deixe de aproveitar o desconto de 15% para inscrições na segunda turma online do treinamento promovido pelo Azure na Prática e que acontecerá dia 23/05/2020 (um sábado). Aproveite para ficar por dentro do build e deployment automatizado de aplicações utilizando diversos serviços oferecidos pelo Microsoft Azure e, o melhor, no conforto de sua casa! Acesse o link a seguir para informações e efetuar sua inscrição: https://bit.ly/anp-devops2-blog-groffe
E para finalizar, ainda não segue o Canal .NET nas redes sociais? Faça sua inscrição então, para ficar por dentro de novidades sobre eventos, tecnologias Microsoft e outros conteúdos gratuitos:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
44 
44 claps
44 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/orquestrando-containers-na-nuvem-com-o-azure-kubernetes-service-thorarch-17-ita%C3%BA-unibanco-70f7f69bd91c?source=search_post---------273,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 29, 2019·3 min read
No dia 27/06/2019 (quinta-feira) participei como palestrante do ThorArch #17, um encontro técnico com profissionais de Arquitetura de Software do Itaú Unibanco em São Paulo-SP e no qual tive a oportunidade de realizar uma apresentação focada no uso do Azure Kubernetes Service na orquestração de containers Docker.
Deixo aqui meu agradecimento ao Marco Oliveira (Itaú Unibanco) e à Paula Santana Rosa (Itaú Unibanco, Devs Java Girl) e pelo convite, pelo presente (uma garrafa da iniciativa ThorArch) e por todo o apoio para que este evento acontecesse, sendo que tivemos um excelente público mesmo com as palestras acontecendo durante o horário de expediente (tarde de quinta, com 60 pessoas presentes!).
Os slides da apresentação já estão no SlideShare:
Os materiais que utilizei como referência para esta apresentação podem ser encontrados no seguinte post, em que estão agrupadas referências gratuitas como artigos, vídeos e projetos de exemplo:
Azure Kubernetes Services - AKS: referências gratuitas e dicas para solução de problemas comuns
E aproveito este espaço para deixar aqui ainda um convite.
Dia 02/07/2019 (terça) a partir das 21:30 — horário de Brasília — teremos mais uma live no Canal .NET. Desta vez eu (Renato Groffe) e o MVP Thiago Adriano faremos uma apresentação demonstrando o uso do Azure Application Insights, Logic Apps e do Slack no monitoramento de aplicações Web (com exemplo em ASP.NET Core e Node).
Para efetuar a sua inscrição acesse a página do evento no Meetup. A transmissão acontecerá via YouTube, em um link a ser divulgado em breve.
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
47 
47 
47 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/github-azure-app-service-deployment-automatizado-e-sem-complica%C3%A7%C3%B5es-de-web-apps-na-nuvem-4c0a0439e096?source=search_post---------274,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 16, 2019·5 min read
Principal plataforma online na atualidade para gerenciamento de código-fonte em projetos de software, o GitHub foi concebido a partir do Git e nos disponibiliza todas as vantagens oferecidas por esta solução de versionamento. Grandes iniciativas open source, projetos corporativos, desenvolvedores individuais e o próprio mundo acadêmico estão entre os usuários do GitHub.
E se pudéssemos tirar proveito de todos os benefícios do GitHub e do uso da nuvem na publicação de nossas aplicações Web, aplicando ainda boas práticas de DevOps?
Seja um repositório privado ou público, temos a opção de configurar sem grandes dificuldades o deployment automatizado de um projeto Web no Azure App Service e empregando para isto um repositório do GitHub. O App Service é uma alternativa para hospedagem de aplicações que conta hoje com suporte às seguintes plataformas de desenvolvimento: .NET Core, ASP.NET clássico (Web Forms, MVC, Web API), Java, Node, PHP, Python e Ruby.
Neste novo artigo demonstrarei como configurar uma aplicação Web com a automação de seu deployment, fazendo uso para tanto do Azure App Service e de um repositório do GitHub (um projeto ASP.NET Core 2.2 neste caso).
Caso deseje conhecer mais sobre o Azure App Service acesse então os seguintes conteúdos:
Como o Microsoft Azure pode simplificar a publicação de suas Web Apps? — Dica Rápida
Como posso utilizar Linux e o Microsoft Azure para hospedar minhas aplicações? - Dica Rápida
Para o exemplo demonstrado neste post será criado um novo recurso do App Service a partir do Portal do Azure:
Para associar um repositório do GitHub a um App Service acessar a opção Deployment Center:
Além de repositórios do GitHub, o Azure App Service também pode trabalhar em conjunto com opções como Azure Repos (Azure DevOps), Bitbucket, Local Git e FTP. Selecionar GitHub para prosseguir com a configuração do deployment:
Um popup será então exibido, solicitando a conexão a uma conta do GitHub:
A conta selecionada aparecerá agora na opção GitHub (destacada em vermelho na imagem), sendo necessário acionar a opção Continue para prosseguir com as configurações:
Em BUILD PROVIDER selecionar a opção App Service build service e clicar em Continue:
Em CONFIGURE indicar qual o repositório do GitHub (o repositório que utilizei pode ser acessado em https://github.com/renatogroffe/ASPNETCore2.2_GitHub), bem como a Branch (selecionei a branch master para efeitos de testes); acionar na sequência o botão Continue:
Na imagem a seguir podemos observar este repositório no GitHub:
E em SUMMARY concluir este procedimento clicando em Finish:
Após alguns segundos aparecerá em Deployment Center uma notificação de que o deployment teve sucesso:
Testes
Um acesso a https://groffegithub.azurewebsites.net/api/contador trará como retorno os dados gerados pela API REST de testes:
Será agora efetuada uma alteração a partir do próprio GitHub utilizando a branch master e no arquivo ContadorController.cs, como indicado a seguir (ajuste destacado em vermelho). Confirmar esta alteração acionando a opção Commit changes:
Um novo deployment acontecerá de forma automática, sendo possível observar que o mesmo teve sucesso na seção Deployment Center:
Ao acessar novamente a API de testes via browser a mudança realizada na branch master já constará no resultado gerado:
E para concluir este post aproveito a oportunidade para um convite…
Que tal aprender mais sobre o Azure App Service na prática, em um workshop que acontecerá durante um sábado (dia 21/09) em São Paulo Capital e implementando na prática um case que combina o uso deste serviço com outras tecnologias como Azure SQL, Azure Storage, Azure Functions e Application Insights? Acesse então o link a seguir para efetuar sua inscrição com um desconto especial: http://bit.ly/anp-promo-azure-github
Deixo aqui ainda algumas referências gratuitas que podem ser úteis para que você conheça um pouco mais sobre os diversos serviços do Microsoft Azure que podem ser utilizados para a hospedagem de aplicações Web:
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
Hospedando projetos Web no Azure: de um site estático a um cluster Kubernetes
Orquestração de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
Kubernetes: do Pod ao Deployment Automatizado [Vídeo]
Docker para Desenvolvedores .NET - Guia de Referência
Azure Kubernetes Services - AKS: referências gratuitas e dicas para solução de problemas comuns
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
77 
1
77 
77 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@selcukusta/azure-time-series-insight-%C3%B6n-i%CC%87nceleme-4ede1e2b09a3?source=search_post---------275,"Sign in
There are currently no responses for this story.
Be the first to respond.
Selçuk Usta
Jun 3, 2017·5 min read
Henüz Preview aşamasında olan, Microsoft Azure ailesinin yeni üyesi Time Series Insight servisine göz atmakta fayda var dedim ve yaptığım ön çalışmayı da paylaşmanın keyifli olacağını düşündüm, şimdi de bu satırları yazıyorum.
Adından da anlaşılacağı üzere bu servis, bir TSDB (Time Series Database) servisi. Bu veritabanı sistemiyle uğraşanlar konsepte oldukça hakimdir ancak kısaca açıklamak gerekirse anlık olan gelişen milyonlarca metrik veriyi güvenli bir şekilde muhafaza eden ve sağladığı arayüz desteği ile son kullanıcıya sunan veritabanları olarak ifade edebiliriz. “Yazılımlarımızı Nasıl İzliyor ve Kontrol Altında Tutuyoruz?” başlıklı yazımda da bu kavrama kısaca değinme fırsatına sahip olmuştum.
Aslında yukarıdaki kısa tanımdan da anlaşılabileceği üzere metrik verilerinizi tutmak için biçilmiş kaftan diyebilirim. Hürriyet evreninde biz de bu veritabanı sistemini aktif olarak kullanıyoruz. Sunuculara düşen anlık istek sayıları, sunuculardan gelen anlık hata sayıları, CPU sıcaklık değerleri, hafıza kullanımı gibi dış parametrelere bağlı olarak anlık değişen yığın dataları tutuyor ve bir arayüz eşliğinde gözlemliyoruz.
IOT kavramının hayatımıza girmesiyle birlikte de bu veritabanı sistemleri önemini gün geçtikçe arttırmakta. Odanızın anlık sıcaklık ya da nem değerlerini bir Raspberry cihazından okuyup TSDB veritabanında tutabilirsiniz. Mağazanızın önünden geçen kişilerin raflarınıza ne kadar süre baktığını algılayan bir cihaz kurabilir ve cihazdan gelen verileri yine bu veritabanı sistemi üzerinde saklayabilirsiniz. Senaryoları ihtiyaçlar doğrultusunda arttırmak işten bile değil hayal ettiğiniz üzere.
Kendilerinin de ifade ettiği gibi henüz en başında. InfluxDB, OpenTSDB hatta aynı çerçevede değerlendirilmese dahi Elasticsearch gibi teknolojiler şu anda aktif olarak yukarıda sayılan senaryolar için aktif olarak profesyonel sahnede yer alıyor. Bu sahneye çıkış adımı olarak da geçtiğimiz günlerde Time Series Insights adını verdikleri servisi kullanıma sundular.
Bu yazıda işin maliyeti, diğer Cloud çözümlerinin sunduğu alternatif servislerle karşılaştırma gibi kalemlere değinmeyeceğim. Amacımız yazının sonunda bu servisi kullanarak lokal ortamda bir uygulama geliştirmek ve arayüz üzerinden göndermiş olduğumuz yığın verileri gözlemleyebilmek.
Bir sonraki paragrafta başlayacağımız örnek çalışma için;
yeterli olacaktır. Ben de bu gereksinimleri karşıladığınızı düşünerek örneğime başlıyorum:
Öncelikle nasıl bir çalışma modelimiz olduğunu tartışmakta fayda var. Elimde bir adet TSDB çözümü var. Bu çözüme veri sağlamakla sorumluyum. Global çözümde bir TSDB içerisinde farklı metrik değerleri tutmanız mümkün. Yani CPU takibi için ayrı, yük oranı için ayrı bir veritabanına ihtiyacım yok. Azure bunun karşılığı olarak “Event Hubs” servisini kullanıyor. Bu hub — bundan sonra kanal olarak ifade edeceğim — üzerinden olay kayıtlarını veritabanına ileteceğim. Bunun için yeni bir Event Hubs servisi oluşturuyorum:
Azure üzerinde oluşturduğunuz servisler bir Resource Group içerisinde tutulmalıdır. Eğer bir grubunuz varsa bunu seçebilir ya da benim örneğimde olduğu gibi bu panel üzerinden yeni bir grup oluşturabilirsiniz. Olmazsa olmazımız ise “Name” alanında belirttiğimiz servis adı. Bunu bir yere not edelim, ileride kullanacağız.
Servisi oluşturduktan sonra açılan pencereden “Shared access policies” menü elemanına tıklıyor ve sağa doğru açılan yeni pencereden “PRIMARY KEY” kolonundaki değeri kopyalıyoruz. Bunu da not etmeyi unutmayalım:
Adından da anlaşılacağı üzere Event Hubs, kanalların tutulduğu bir container servisi aslında. Dolayısıyla bu depoya açılan küçük küçük kanallar açabilir ve bu kanallar üzerinden verilerinizi iletebilirsiniz. Biz de “cpu-usage-event-hub” adını vereceğimiz kanalımızı oluşturuyoruz. Bu kanal üzerinden veritabanına CPU kullanım yüzdesini anlık olarak göndereceğiz.
Artık olayları göndereceğimiz kanalımız hazır. Sıra bu kanalı dinleyen time-series veritabanımızı oluşturmakta. Bunun için de yeni bir servis ekleme alanına gidiyor ve “Time Series Insights Preview” servisini seçiyoruz.
Bu veritabanın bir arayüzü olacak elbette. Bu arayüze erişim sağlayacak, arayüz üzerinde sorgu çalıştırabilecek, sorguları global olarak ya da kendine özel kaydedebilecek kullanıcılara ihtiyacımız var. Hadi şimdi bunları tanımlayalım ve servis penceresindeki “Data Access Policies” menü elemanını seçelim. Açılan pencereden “Select User” seçeneği ile erişim sağlayacak kullanıcıyı seçiyoruz. “Select Role” seçeneği ile de bu kullanıcının okuyucu ve katkı sağlayıcı yetkilere sahip olması gerektiğini ifade ediyoruz:
Daha sonra ise yine servis penceresine dönüyor ve verinin alınabilmesi için gerekli ayarların yapıldığı “Event Sources” menü elemanına tıklıyoruz:
Burası ilk etapta otomatik olarak doluyor (her bir servisten zaten bir tane tanımlı olduğu için). Kısaca açıklamak gerekirse;
Artık TSDB’imiz hazır. Arayüz adresini de TSDB servisi penceresinden alıyoruz:
Aşağıda vermiş olduğum Github repomdan uygulamayı indirdikten sonra testini yapabilirsiniz. NOT: main.py dosyasında yer alan; “SERVICE_NAMESPACE”, “SHARED_ACCESS_KEY_NAME”, “SHARED_ACCESS_KEY_VALUE”, “EVENT_HUB_NAME” değişkenlerinin değerlerini kendi hesap ve servis bilgileriniz ile değiştirmeyi unutmayın.
github.com
Uygulamayı çalıştırdığınızda her şey yolunda ise aşağıdakine benzer bir konsol çıktısı alıyor olmanız gerekiyor:
Son olarak arayüze gidiyorum ve bir süreliğine göndermiş olduğum değerlerin yansıyıp yansımadığını kontrol ediyorum:
Bu ekranda (1) ile işaretlenmiş alandan, çizim için kullanılacak metriği seçmeyi; (2) ile işaretlenmiş alandan çizilecek alana bir isim ve renk vermeyi; (3) ile işaretlenmiş alandan verileriniz arasındaki süreyi; (4) ile işaretlediğim alandan ise tüm işlemleri yaptıktan sonra grafiği güncellemeyi de unutmayın.
Azure her ne kadar yolun başında olsa da sunmuş olduğu kolay arayüz, işlem adımlarının net olması ve servislerin kolay bir şekilde birbirine bağlanması geleceğe dair umutlarımı yeşertti.
Sizler de yorum alanından TSDB alanındaki deneyimlerinizi paylaşırsanız mutlu olurum. Tekrar görüşene dek, yığın datalarınız hep güvenle saklı olsun :)
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
24 
24 
24 
Engineering Manager (at) Hepsiburada. Former trainer & consultant. Family member of C# and Python. Newbie on Unix. Articles are mostly about coding.
"
https://medium.com/@borakasmer/automated-publish-from-tfs-to-azure-b3eb6a7ff869?source=search_post---------276,"Sign in
There are currently no responses for this story.
Be the first to respond.
Bora Kaşmer
Aug 18, 2021·4 min read
Hi, Today we will talk about Publishing from any Source Control to any Cloud Automatically. As a Microsoft MVP, I prefered TFS for Source Control and Azure as a Cloud :)
If you’ve ever been a bit interested in TDD or DDD, you must figure out the importance of automation. Why do we try to Automate everything? Because of saving time and making fewer mistakes. So let’s publish a website from the specific branch on TFS to Azure automatically.
“Software is the language of automation.” ―Jensen Huang
We will create the AutoDeployTFS project as an Asp .Net Core Web App with .Net 5.0 framework.
When the project is created, you could see the default index page as below.
1-) Now let’s Create the Azure DevOps Project (https://dev.azure.com) for source repo (TFS). Firstly, if you didn’t create any organization before, create a new one as below.
2-) Secondly, create a project under the organization as below.
3-) Create an empty folder. Open Visual Studio 2019 and Connect to Azure DevOps. Enter the credentials.
4-) Connect to AutoDeploy project on TFS and Map&Get the empty folder.
5-) Now, move all the project files from the AutoDeploy to this empty folder. And “Commit All” files. And finally “Push”.
“Automation is going to cause unemployment, and we need to prepare for it.” ―Mark Cuban
Your AutoDeploy project must be seen as below on TFS (Azure DevOps Project).
Now we created the AutoDeploy project on Visual Studio. We created AutoDeploy repo on TFS (Azure DevOps Project) and pushed the project to this Repo.Now we need the server to publish this website. So let’s create WebSite on Azure. https://portal.azure.com/
Go to “Create Resource => Web App” and select it.
Now fill the necessary fields below in the form.
After creating the Website on Azure, now let’s connect it with TFS (Azure DevOps).
Select AutoDeploy => Deployment Center from the menu. In the below picture, we create a webhook from Azure Web Apps to TFS.
Source: You can select different kinds of sources(GitHub, BitBucket). For TFS, we select “Azure Repos”.
Now let’s setup the TFS connection configurations.
When someone pushes the final Web Application code to the “master” branch of “Azure DevOps”, it automatically publishes the Azure. So you don’t worry about the IIS publish for new updates anymore.
“Automation is driving the decline of banal and repetitive tasks.” ―Amber Rudd
Conclusion:
In this article, we talked about Automation importance. DevOps is the most needed expertise these days. When we change and push the application to the specific branch, we don’t need the updated Azure Web Application because it will happen automatically. But we ignored the test for this scenario, and we can get live some broken pages accidentally on Azure. So firstly, we do not push the application directly to the master branch. And before pushing the brunch, we have to run some tests automatically. Maybe we can use some tools like Jenkins or TravisCI for running tests. And if everything is ok, we can push the source to the dev TFS brunch.
I hope this article helps you understand how to publish Azure automatically from the TFS and the importance of Automation. See you later until the next article or video. Bye.
“If you have read so far, first of all, thank you for your patience and support. I welcome all of you to my blog for more!”
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
See all (22)
133 
133 claps
133 
I have been coding since 1993. I am computer and civil engineer. Microsoft MVP. Senior Software Architect. Ride motorcycle. Gamer. Have two daughters.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devopsturkiye/azure-automation-servisi-ile-paran%C4%B1z-cebinizde-kals%C4%B1n-721f30255659?source=search_post---------277,"Sign in
There are currently no responses for this story.
Be the first to respond.
Emre Özkan ☁️ 🐧 🐳 ☸️
Apr 18, 2019·3 min read
İhtiyaç doğrultusunda Azure üzerinde mesai saatleri dışında kullanılmayan kaynakları otomasyon ile açma kapama özelliğini deneyimleme fırsatı buldum. Maliyet açısında sizlerin cebini de rahatlatacağını düşündüğümden sizlere de göstermeye çalışacağım.
İlk önce bir automation account’u yaratarak başlıyoruz.
Aşağıdaki bilgileri kendimize göre uyarlıyoruz.
Yarattıktan sonra aşağıdaki gibi görünmesi gerekiyor.
İçine girip credentials sekmesine tıklıyoruz.
Burada runbook’u çalıştıracak bir kullanıcı yaratmamız gerekiyor.
Sonrasında Automation account içinden Runbook Gallery sekmesine gelip sanal makineleri başlatıp kapatılacak daha önceden yazılmış Powershell scriptini hesabımıza import ediyoruz.
Script detayını aşağıdaki şekilde inceleyebilirsiniz.
İmport ettikten sonra Runbook sekmesinden ilgili runbook içine giriyoruz.
Runbook’u çalıştırmadan önce “edit” sekmesinden “Publish” etmem gerekiyor.
İsterseniz bu arayüz üzerinden Test pane adımından test edebilirsiniz.Publish ettikten sonra “Start” sekmesi aktif oldu.
“Start” adımından sonra 3 adet input olarak subscription id’sini, makine ismini ve “Start” ve “Stop” seçeneklerinden birini seçmemizi istiyor.
Sonrasında takip edebileceğiniz bir arayüze yönlendiriyor.
İsterseniz de bunu bir schedule haline getirip istediğiniz saatte açıp istediğiniz saatte kapatabilirsiniz.”Link to schedule” seçeneğine tıklıyoruz.
İsterseniz günlük olarak isterseniz de haftanın belirli günlerini seçebilirsiniz.
Daha önce belirttiğim input ları tekrar giriyoruz.
Paranızın cebinizde kalması dileğiyle
Diğer makalelerime profilime tıklayarak ulaşabilirsiniz.
medium.com
Kişisel Bloguma göz atmak isterseniz;
sysaix.com
Solution Architect at @RedHat CK{A | AD | S} https://www.linkedin.com/in/emreozkann/
See all (56)
17 
1
Haftalık olarak yayımınızdan alacağınız Email Bülteni Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
17 claps
17 
1
Medium independent DevOps publication. Join thousands of aspiring developers and DevOps enthusiasts
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/2b-1-better-2day/microsoft-azure-dddf42264fcd?source=search_post---------278,"There are currently no responses for this story.
Be the first to respond.
Өнгөрсөн жил Flutter жоохон сонирхоод ганц 2 хичээл Udacity дээр үзсэн юм. Тэгж байхдаа 1 онлайн тэтгэлэгийн зар олж харваа.
Иймэрхүү юманд дуртай, бас 2020 SMART goal дотор сургууль тэтгэлэгт өөрийгөө сорих гэсэн зорилт тавьсан байсан болохоор шууд л бүртгүүлэв. (2020/9/16) Хариу нь 3 сарын дараа 12 сард ирсэндэг.
Миний хувьд Data, AI, Cloud гэсэн 3-н төрлийн чиглэлээс өөрт илүү ойр болохоор нь Cloud Track-г нь сонгосон юм. Хичээлийн хувьд дотроо үндсэн 5 сэдэвтэй, хичээлийн талаарх мэдээлэл нь Google Site дээр, харилцаа холбоо нь Slack-аар явагддаг маш сайн зохион байгуулалттай хөтөлбөр байна лээ.
Cloud Track нь Microsoft Azure-н талаар бөгөөд
гэсэн хичээлүүдтэй. Эхний шатаа давж чадвал дараачийн $1356-н төлбөртэй Become a Developer for Microsoft Azure - Nanodegree Program-г үзэх цаашлаад сертификаттай болох боломжтой юм билээ.
Хичээл нь богино бөгөөд амархан байсан тул үргэлжлүүлэн Linux Academy-н AZ-900 Microsoft Azure Fundamentals (13 цаг), Microsoft Official Learning Path — Azure Fundamentals (13 цаг) хичээлийг үзээд сертификатын шалгалт өгөхөөр шийдсэн юм.
Үүлэн технологи учраас AWS-тай төстэй. Гэхдээ мэдээж өөрийн онцлогийг шингээсэн ялгаанууд бий. Region pair, Azure Active Directory гэх мэт.
Дэд бүтцийн хувьд томоор нь харвал Orchestrator гэдэг нөхөр бүх хүсэлтийг зохицуулж өгдөг харин түүнийг нь эцсийн шат буюу hardware түвшинд сервер рак дээрх Hypervisor-луу Fabric controller гэдэг зүйл хуваарилж өгдөг байна.
Мэдээж шинэ хэрэглэгч өөрийн account-г үүсгэх бөгөөд 1 account дотор олон subscription үүсгэж хэрэглэх боломжтой.- Free (тодорхой сервисүүдийг үнэгүй хэрэглэх боломжтой)- Temporary (Sandbox буюу зөвхөн туршилт хийж зорилготой)- Pay as you go (Хэрэглэснээрээ төлөх хэрэглэгч — ихэнх)- Member offer (Өөр өөр хямдралын багцууд)
Hierarchy-н хувьд дараах байдлаар зохион байгуулалт хийдэг юм билээ.(Management Group → Subscriptions → Resource Group → Resource)
Region pair : AWS-аас нэг ялгаатай зүйл нь region хослолыг санал болгодог гэсэн. Ихэвчлэн region доторх 2–3 ширхэг AZ-г ашиглаж disaster recovery, failover-г шийддэг байсан бол одоо илүү том хүрээнд зохион байгуулах нь хялбар болсон байна.
Нэр болон боломж нь л бага зэрэг ялгаатай болохоос ижилхэн сервисүүд бас зөндөө. Үүл рүү шилжихээс өмнөх тооцоолол хийх TCO, хэрэглээгээ хянах Advisor, урьдчилж бага мөнгө төлөх Reservations гэх мэт. Түүнээс гадна Azure Preview гэж олон нийтэд гаргаагүй ч туршилтын сервисүүдийг ашиглах боломж байдаг.
Remote хүчээ авч байгаа энэ үед бас нэг хэрэгтэй зүйл нь онлайнаар Windows компьютер ашиглах юм. Зүгээр ч нэг хэрэглэхгүй 1 машиныг олон хүн ашиглах боломж олгодгоороо давуу талтай бөгөөд вэб хөтөч байхад л хангалттай.
Хурд, аюулгүй байдал, дата хэрэглээ гэх мэт олон асуудлыг шийддэг.
Тун чухал сэдэв болох Security-н хувьд Firewall, DDoS protection, Security Center (AWS : Trusted Advisor) тэгээд бүр ухаалаг SIEM төрлийн Sentinel гэх сервистэй. Ер нь тэгээд мөнгөө л төлчихвөл бүх зүйлийг нь өмнөөс нь хийгээд өгчихдөг болчихоод байна дөө.
AWS Cognito шиг Azure Active Directory (Cloud version of AD) гэж бас бий. Хэрэглэгчийг хэн гэдгийг таних, юу хийж чадахыг шалгах бүхий л боломжуудтай юм билээ. (SSO, MFA, Conditional Access)
21Vianet нь Microsoft болон Хятадын засгийн газар хамтарсан компани юм билээ. Хятадын хуулийн дагуу Microsoft-н үйлчилгээг үзүүлдэг region гэж хэлж болно. Компанийн 51 хувийг Хятадын засгийн газар эзэмшдэг бөгөөд асуудал гарвал шийдвэрийг дангаараа гаргах чадалтай гэсэн үг. Хятад яалт ч үгүй их гүрэн юм аа.
Ингээд сервисүүдийн талаарх хэсгээ дуусгая. Бүх үйлчилгээний талаар дурдаж чадахгүй тул өөрт сонирхолтой санагдсан хэдийгээ л багтаалаа. Одоо харин шалгалтын талаар ярилцацгаая.
Шалгалтын хувьд 4 төрлийн нийт 40–60 асуулттай. 40–60 гэдэг нь тухайн үеээс хамаараад өөр өөр байдаг юм билээ. Магадгүй асуултын хэлбэрээс шалтгаалдаг байх.
Linux Academy-н хичээлээ үзэж дуусгаад, хэсэг болгоны тестүүд, дасгал ажлыг хийгээд, төгсгөлийн тестийг 5 удаа хийхэд 60, 83, 90, 85, 90 авсан.
Бас IT exams болон Whizlabs дээрх тестийг хийсэн. LinuxAcademy-н тестийг бодвол илүү бодит тест байна лээ.
Өмнөх Oracle-н шалгалтуудтай адил хүнгүй, илүү дутуу ном сонин байхгүй, чимээгүй орчинд шалгалтаа өгсөн бөгөөд ID шалгуулахдаа жолооны үнэмлэхээ ашигласан.
MS : AZ900 хувьд шалгалт эхлэхээс өмнө тухайн шалгалтын сэдвүүдийг хэр сайн мэдэх вэ гэж асууж байна лээ. Яг шалгалтын асуулт ямар байх эсэхэд нөлөөлдөг юмуу гүй юм уу мэдэхгүй юм. Би бүгдийг нь Moderate буюу дундаж/ерөнхийд нь гадарлана гээд бөглөөд тестээ эхлүүлсэн дээ.
Надад ирсэн 42 асуултаа 18 минутад дуусгаад, тэмдэглэсэн (mark) 4 асуултаа дахиж хийгээд, эхнээс нь бүгдийг нь review-дээд шалгалтаа дуусгахад яг 30 минут болж байна лээ. Хариу нь ч бас тестээ дуусгаад л шууд гараад ирдэг.
820 оноогоор давсан.
AWS-тай харьцуулахад portal нь өөр, гар утасны апптай, зарим сервисүүд нь илүү юм шиг санагдсан. Зах зээлийн хувьд хуучин Microsoft-н хэрэглэгчид байгаа учир тэд нарыгаа татсан үйлчилгээ гаргаж, улам томроод байгаа гэж болохоор. Хэрвээ би Microsoft-н хэрэглэгч байсан бол ямар ч эргэлзээгүй Mcrosoft : Azure лүү шилжих байх аа.
Энэ хүргээд нийтлэлээ дуусгая. Дараачийн удаа харин Alibaba-н үүлийг ухаж төнхөнө өө. Where is Jack Ma?
To be +1% better today : Learn & Try & Share (LTS)
159 
1
159 claps
159 
1
Written by
I am who I am... || өөрийнхөөрөө байхаас ичихгүй
To be +1% better today : Learn & Try & Share (LTS)
Written by
I am who I am... || өөрийнхөөрөө байхаас ичихгүй
To be +1% better today : Learn & Try & Share (LTS)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/daylightnightlite/azure-af4bd5610411?source=search_post---------279,"There are currently no responses for this story.
Be the first to respond.
Azure pure as the driven snow
Blue is beautiful
Coruscated and dancing
Daring and wonderful
Everyone here is different
Feelings can be fickle
Give an inch take a mile
"
https://medium.com/@eekayonline/connecting-atlassian-sourcetree-with-your-azure-devops-git-repo-b88aeab91b00?source=search_post---------280,"Sign in
There are currently no responses for this story.
Be the first to respond.
Edwin Klesman
Jan 31, 2020·3 min read
In this short post I will show you how you can setup Atlassian SourceTree so it will connect with Microsoft Azure DevOps project’s GIT repo.
I like using SourceTree in my development teams since it is an easy interface, is available for both Windows and Mac OS, and it supports the…
"
https://koukia.ca/push-docker-images-to-azure-container-registry-ed21facefd0c?source=search_post---------281,"In the previous post I demonstrated how to create docker images using docker-compose. Now I will show you how you can push those images into Azure Container Registry.
I will be using Azure CLI version 2.0 to interact with Azure from the command prompt.
"
https://medium.com/@tsuyoshiushio/writing-unit-test-for-azure-durable-functions-80f2af07c65e?source=search_post---------282,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Mar 8, 2018·2 min read
One of the Frequently Asked Questions related Azure Functions is How to do a unit testing. On this article, I’d like to explain how to write a unit testing for C# dll v2 application.
You can find the unit testing example on the Azure-Samples / functions-unittesting-sample.
Writing a unit test of Azure Functions is quite easy. Just mock the parameter of a function.
For example, this is the code of HttpTrigger template. You can ignore the Attributes for the unit testing.
Now I try to write a unit test for this method.
Helper method HttpRequestSetup create a mock object using Moq. I wrote a helper method to enable test makes easier. You can refer the whole test in here. The unit test is just mock the parameters and set the values. Then the assert the output.
Sometimes, Azure Functions use custom class, like IAsyncCollector<T>. You also mock this by Poco or Moq.
I also create a simple Mock object on the FunctionTest.cs which is just a helper of unit testing.
If you want to create a Mock object by yourself, you may want to know the original code for that.
You might see the code by right-click the Class on your Visual Studio and select `Go to definition`.
If you can’t see that, you can find it on the following github.
For the Function App V1, you can find these at Azure WebJobs SDK.
e.g. if you want to find ServiceBus, you can go `src/Microsoft.Azure.WebJobs.ServiceBus/Bindings`.
For the Durable Functions testing, you might encounter the issue of mocking. The parameter classes are sealed class. Which means you can’t mock it. Then how to mock these?
Make sure if your Durable Functions version is beta3 or upper.
The Azure Functions production team get an issue about this and add base class for mocking. Then you can change the signature like this.
Now you can test it. On this part, I only test the Orchestrator responsibility.
You can write Unit Tests for Azure Functions simply mock the parameter. In case of Durable Functions, you need to use base class for it. If you find an issue which prevent you to write unit testing, please let us know as an issue of GitHub.
Senior Software Engineer — Microsoft
See all (200)
81 
81 claps
81 
Senior Software Engineer — Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/10-best-microsoft-azure-courses-for-beginners-and-experienced-developers-d41a454834c0?source=search_post---------283,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn about the Microsoft Azure Cloud Platform in-depth and looking for the best online training courses then you have come to the right place. In the past, I have shared the best courses to prepare for Azure Fundamentals, Azure Administrator, and Azure Architect certifications, and today I am going to share the best online course to learn Azure platform for Beginners.
These are top-quality courses from expert trainers and instructors and picked from websites like Udemy, Coursera, Pluralsight, and edX.
You can use these courses to not only learn Microsoft Azure core services but also to prepare for different Microsoft Azure certifications like AZ-900, AZ-300, and AZ-200 or Azure developer associate certification.
Microsoft Azure, is a cloud computing service created by Microsoft meant for building, testing, deploying, and managing applications and services through Microsoft-managed data centers. It provides software, platform, and infrastructure as a service and supports many programming languages, tools, and frameworks, including both Microsoft-specific and third-party software and systems. This makes it a very good platform because of its flexibility. You can have different projects in the same interface so you don’t have to be changing or learning the peculiarities of every different platform. So if you have different projects or you work with different programming languages is a good option to optimize your time and resources. Also, demand for Azure certified developers, administrators, and cloud architect is increasing exponentially as more and more companies are using Azure for their migrating their on-premise software and application. This means it’s the best time to learn Azure and boost your career profile.
At the same time, Microsoft Azure is a huge collection of services for building applications in the cloud and can often be overwhelming.
That’s why choosing the best online training courses is crucial and that’s where this article will help you. here you will find the best Azure Cloud Computing courses from sites like Udemy, Coursera, Pluralsight, and edX which is developed by experts and trusted by thousands of people like you.
Without wasting any more of your time, here you have a top 5 of the best tutorials to learn about Azure. These are the best courses from Udemy, Coursera, and edX and you can use them to learn Azure and prepare for different Azure certifications like Azure Fundamentals and Azure Architecture TEchnology.
This is another best course to learn Azure on Udemy. If you want to be an expert with Azure this is an online course you should join.  This course has a total of 12 sections, during the course you will see:
As you can see it comprehends a lot of subjects. All the course includes more than 30 hours of video, 6 articles, and 103 downloadable resources, which makes this is one of the most comprehensive Azure course on Udemy.
Here is the link to join this course — Microsoft Azure cloud — Beginner Boot camp
This is another Udemy Azure course which is great to learn Microsoft Azure from scratch. As its name suggests this course is for beginners but to start this course you’ll need some basic IT knowledge on networks, databases, and how Web servers work.  You will learn:
By the end of the course, you’ll be prepared for taking the AZ-900 certification exam. The course includes 2 simulation exams for you to practice. Apart from that, it includes 10 and a half hours of video tutorials, 13 articles, and 12 resources.
Here is the link to join this course — Microsoft Azure — Beginner’s Guide
This is the best Azure cloud course from Udemy and you can use this to learn Azure in-depth as well as to prepare for the AZ-300 exam. This course is meant for someone that has knowledge in more than one language and want to be more efficient.
During the course you will learn how to:
The course consists of 20 and a half hours of explanation videos that you can consult the times you want. By the end, you will master the Azure environment and you will be considerably more efficient.
Here is the link to join this Azure course — AZ-300 Azure Architecture Technologies Exam Prep 2021
This course focuses on the Fundamentals of Azure Infrastructure. It begins with understanding the subscription system, configuring the security, and acquiring storage. Then you’ll build virtual machines and VNETS to start working.  While doing the tutorial you will be able to:
It takes 30 hours to complete the course, and by the end, you will have a certificate. It is meant for an advanced profile, that already knows some different languages and wants to unify all his projects together on the same platform.
Here is the link to join this course — Azure Infrastructure Fundamentals by Learn Quest
By the way, If you are planning to join multiple Coursera courses or specializations then consider taking the Coursera Plus subscription which provides you unlimited access to their most popular courses, specialization, professional certificate, and guided projects. It cost around $399/year but it's completely worthy of your money as you get unlimited certificates.
coursera.com
This course’s focus is to use virtual machines in Azure for workloads, emphasizing basic configuration, planning, deployment, and management tasks.
You’ll be introduced to how to secure and make your virtual machines highly available and scalable. It is a good start for entering the Azure world but not having to see all its features.  Here are the key things you will learn in this course:
This course can be done completely free, and then when you finish you can pay for the certificate of finalization if you desire to.
Here is the link to join this course — Microsoft Azure Virtual Machines by Microsoft
This is one of the best beginner-level Azure course from Pluralsight, another leading online portal for programmers and developer.s
In this course, you will learn foundational knowledge to begin planning solutions using Microsoft Azure. First, you will learn about cloud computing and the different ways to run your application code.
After that, you will discover the data storage, processing, and analysis capabilities in Azure. Finally, you will explore how to create networks; integrate, manage, and secure your applications; and develop for Azure.
After completing this online Azure course, you will have the skills and knowledge of Microsoft Azure needed to begin working on your cloud solutions.
Here is the link to join this course — Microsoft Azure: The Big Picture
This is another great Pluralsight course to learn Microsoft Azure from scratch. Created by Matt Milner, this course will introduce you to the new world of cloud computing and how to build on the Windows Azure Platform.
You will not only learn about Windows Azure compute and storage and SQL Azure but also learn about cloud computing tradeoffs and help you understand the constraints and limitations imposed by the cloud computing model offered by the Windows Azure Platform today.
Matt is one of the Microsoft Tech experts and he has a knack for making things simple which really helps to understand complex cloud computing concepts.
here is the link to join this course — Microsoft Azure Fundamentals
By the way, you would need a Pluralsight membership to join this course which costs around $29 per month or $299 per year (14% discount).
I highly recommend this subscription to all programmers as it provides instant access to more than 7000+ online courses to learn any tech skill. Alternatively, you can also use their 10-day-free-pass to watch this course for FREE.
pluralsight.pxf.io
This is an excellent Udemy course for both beginners and experienced software developers and tech leads who want to learn Learn how to plan, Manage, and Deploy Your Very First Azure DevOps Application Through Hands-On Demos and Use Cases.
In this Azure DevOps Fundamentals course, Microsoft Certified Trainer and bestselling Udemy instructor Brian Culp take you on a hands-on tour of Azure DevOps and shows you how to manage development teams, code, and application deployments.
Best of all, Brian understands that students taking a Fundamentals course may not yet be familiar with all the vocabulary and terms and technologies that are part of the DevOps landscape, hence he explains all of those things whenever they come in like Agile, Scrum, App Containers, Commits, Code Requests, Sprints, Unit Testing, Code Artifacts, and other DevOps jargons.
Since this course is designed for people who are new to DevOps terms and concepts, I highly recommend this to both beginners and experienced software developers to join this course.
Here is the link to join this course — Azure DevOps Fundamentals for Beginners
As the title suggests, this Azure course is for people who want to become Data Engineer and wants to work on the Microsoft Azure Cloud platform. Microsoft Azure Date Engineering is one of the fastest-growing and in-demand fields for Data Science practitioners.
In this course, you will learn about Azure data technologies like Microsoft Azure SQL Database, Data Lake, Data Factory, Synapse Analytics, Cosmos DB, Databricks, and HDInsight.
This is a massive course with 34.5 hours of content and over 50 articles to learn all essential data technologies you want to learn in Azure. This course is also good for DP-200 and DP-2001.
Here is the link to join this Azure course — Azure Data Engineer Technologies for Beginners
This is another course that I recommend to an experienced developer who wants to master the Azure platform. In this course, you will learn about essential DevOps tools and technologies like Docker, AKS, Azure Disks, DevOps, Virtual Nodes, ACR, DNS Zones, Active Directory, Kubernetes, Ingress, and Terraform.
This course is ideal for Azure Architects, Sysadmins, or Developers who are planning to master Azure Kubernetes Service (AKS) for running applications on Kubernetes.
The best thing about this course is that it is full of hands-on experience with clear explanations which really helps to understand what, why, and how part of any concept. In short, a great course to master Azure DevOps, particularly AKS or Azure Kubernetes Service.
Here is the link to join this course — Azure Kubernetes Service with Azure DevOps and Terraform
That’s all about the best courses to learn the Microsoft Azure Cloud Platform in-depth in 2021. These online courses are also good for various Azure certifications like Azure Fundamentals and Azur Cloud Architect. If you are thinking to go cloud native then Microsoft Azure is a very good alternative to improve your workflow and make things easier for you. It is very extensive, so you have the articles explaining what they offer, so you can choose the one that adapts better to what you are looking for.  Other Azure Cloud Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you find these Microsoft Azure courses useful, then, please share it with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. — If you are keen to learn Microsoft Azure Platform from scratch but looking for free online training courses to start your journey then you can also see Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It’s completely free and all you need is a free Udemy account to join this course online.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
155 
3
155 claps
155 
3
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://koukia.ca/can-you-implement-a-chat-bot-with-microsoft-bot-framework-without-azure-1446ce8333bf?source=search_post---------284,"Microsoft Bot Framework is an amazing framework that people are using it more and more to build Bots these days, because it is so simple to put together a Bot very quick.
I have also published a post here a while ago, that you can go through and learn a bit more about this Microsoft Bot Framework if you are not familiar with it, but today, I…
"
https://medium.com/hackernoon/azure-brute-farce-17e27dc05f85?source=search_post---------285,"There are currently no responses for this story.
Be the first to respond.
Azure AD is the de facto gatekeeper of Microsoft cloud solutions such as Azure, Office 365, and Enterprise Mobility. As an integral component of their cloud ecosystem, it is serving roughly 12.8 million organizations, 950+ million users worldwide, and 90% of Fortune 500 companies on a growing annual basis. Given such a resume, one might presume that Azure Active Directory is secure, but is it?
Despite Microsoft itself proclaiming “Assume Breach” as the guiding principle of their security strategy, if you were to tell me a week ago that Azure or Office 365 was vulnerable to rudimentary attacks and that it could not be considered secure, then I probably would have even laughed you out of the room. But when a client of ours recently had several of their Office 365 mailboxes compromised by a simple brute-force attack, I was given no alternative but to question the integrity of Azure AD as a whole instead of attributing the breach to the services merely leveraging it and what I found wasn’t reassuring.
After a simple “Office 365 brute force” search on google and without even having to write a line of code, I found that I was late to the party and that Office 365 is indeed susceptible to brute force and password spray attacks via remote Powershell (RPS). It was further discovered that these vulnerabilities are actively being exploited on a broad scale while remaining incredibly difficult to detect during or after the fact. Skyhigh Networks named this sort of attack “Knock Knock” and went so far as estimating that as many as 50% of all tenants are actively being attacked at any given time. Even worse, it seems as if there is no way to correct this within Azure AD without consequently rendering yourself open to denial of service (DOS) attacks.
In fact, this sort of attack is so prevalent that it happens to be one of the biggest threats to cloud tenant security at Microsoft according to Mark Russonivich (CTO of Azure) and is among several reasons that Microsoft itself advises their customers to enable multi-factor authentication (MFA) for all users and implement advanced threat intelligence available only to E5 subscription levels or greater; basically requiring companies to give Microsoft more money to secure their own solutions. But MFA also doesn’t impede hackers from cracking passwords or protect businesses from a DOS attack nor does it help those that are unaware of its necessity as many tenants are at present.
Further, since RPS does not work with deferred authentication (DAP) and MFA, partners consisting of consultants, managed services and support providers also cannot use their partner credentials to connect to the tenants of their clients via RPS for advanced administration and scripting. Even though they can easily manage their clients via a browser-based admin center with MFA, they often have to resort to creating admin accounts within Office 365 tenant itself instead, but others do it simply for ease of access to the admin console or for when they are not the Partner On Record. These accounts are precisely what many of these attacks are targeting, often unbeknownst to admins, and Deloitte’s breach is a perfect example of such a scenario.
Unfortunately, these accounts are often stripped of MFA security to make them more convenient and accessible for the multitude of support and operations staff to use while working for various companies offering support services and they seldom expire or change upon company exit. By default in Office 365 and on top of being vulnerable to being cracked and breached, the password expiration policy is further set to a 730-day expiration and further disabled, rendering accounts vulnerable to a prolonged breach at that. Needless to say, they are ripe for attack and this exact scenario is what enabled a hacker to have unabridged administrative access to Deloitte’s Exchange Online tenant for 6+ months.
Complicating matters even further, the natural solution to this problem renders the tenant vulnerable to DOS attacks by virtue of being able to lock users out of their accounts for a fixed duration imposed by Azure AD; but this is still in preview phases. For example, by default Azure AD Smart Lockout (Preview Stage), which is still in preview, is configured to allow 10 password attempts before subjecting the account to a 60-second lockout, giving attackers a theoretical limit of 14,400 attempts per account/per day. You could decrease the threshold to 5 and increase the duration to 5 minutes protect against breaches, reducing attempts to 1,440 per day, but this would create the potential for downtime for users whenever their accounts are being attacked with brute force and password spray attacks.
However, Tyler Rusk at CSSI also called out that Microsoft doesn’t seem to throttle or limit authentication attempts made through RPS. As shown, Tyler was able to surpass the theoretical 14,400 per day limit listed in Azure AD Smart Lockout Preview without added logic, moving at a rate of 48,000 per day had he let it run for a 24 hour period or an est. 17,520,000 attempts over 365 days. However, there are obvious ways to optimize these efforts even further through via background jobs (start-job cmdlet) by essentially running attacks asynchronously instead of synchronously while optimizing for custom lockout limits, max attempts, and minimal detection. The possibilities are endless with regard to password spray attacks for obvious reasons. To be fair to Tyler and CSSI though and in my opinion, they didn’t need to leverage such measures to validate their concern.
If their lockout feature were to work though and if you were able to reduce the threat surface in the manner above, you would then have to contend with the hard countdown of the duration time. It’s immutable which means that users have to wait for it expire in order to render the account accessible again. The unlock cannot be expedited administratively at present. As such, it can just as easily result in an intentional DOS for end users if they or an unintentional DOS while running the possibility of exposing the attack; that is when/if it starts actually working. Obviously protecting from breach takes precedent over downtime, but becoming prone to DOS attacks is hardly a consolation prize.
Banned passwords nor MFA cannot protect against DOS or brute-force attacks either, only against the breach itself. In fact, when brute forcing an account protected by MFA, the MFA challenge itself can be treated as confirmation of a valid cracked username and/or password. In turn, they can then begin to try these credentials in other places which may not be protected by MFA as users and admins alike tend to keep them as similar as possible in multiple directories so that they’re easy to remember. I’ll defer to Ned Pyle of Microsoft as to whether this applies to his employer and their partners.
Summarizing matters thus far, you can brute force accounts housed in Azure AD via RPS. Obvious solutions for this such as MFA, customized password blocking, and advanced threat intelligence are either ineffective, insufficient, paywalled, and/or generate significantly more overhead in order to offset these vulnerabilities. Further, these solutions are often ignored by lazy admins, consultants, and managed services providers and many may be oblivious to this threat entirely; possibly even to breaches of their own. Deloitte has proven that this can even hit the best of them.
As offensive as all of this may seem though, it’s important to remember that AD was never designed to be public facing, quite the opposite. It has actually always been inherently vulnerable to brute-force, password spray, and DOS attacks by design. AD has always been designed to be implemented in conjunction with various other counter-measures in order to maintain its integrity. This includes but certainly is not limited to relying on physical security measures such as controlled entry and limiting the ability to access the domain to those that make it past physical security measures successfully; with the obvious exception of VPN users. This is nothing new.
That said, AD was never, ever, meant to be the sole source of security for IT infrastructure and is fundamentally dependent on other security measures in order to be effective. Consequently, AD becomes markedly more vulnerable when other pre-emptive methods fail or are non-existent. Put simply, such breaches should be the expectation when depending on Azure AD alone for IT security, and this sadly applies to any Office 365 tenant with its default security settings. However, understanding its limitations helps us illuminate ways to harden Azure AD and mitigate these problems just the same.
It almost goes without saying, but none of the measures necessary to patch these vulnerabilities are free to companies leveraging these services at present. Even if Microsoft were to fix this, who is to say that something else just as simplistic and embarrassing isn’t hiding around in the corner or already being used? That said, avoiding products backed by a 20-year-old security system streamlined for vendor lock-in seems like a viable solution to avoiding this problem in the first place.
Before anything else, I truly think that the onus is on Microsoft to ensure that their baseline configuration for cloud accounts doesn’t expose their tenants unnecessarily. Sure, we could blame ignorant users and lazy admins, but I don’t think that this is fair given the scope of this vulnerability, which is essentially 46% of AzureAD’s user-base (password hash sync + cloud only = 46%). It is unknown how many have MFA enabled and the scope of this is ultimately an unknown both with regard to those who are vulnerable to it, actively being attacked, and/or those already breached though. But as a former tier 3 support engineer for Exchange Online at Microsoft, I can confirm that a significant amount of individuals as well as small-medium businesses are relying on Azure AD exclusively without further counter-measures and that they account for a sizable amount of Office 365’s user-base. That said, telling customers that pay you to secure their mailboxes or to disable basic auth to address this doesn’t cut it.
Microsoft has clearly acknowledged this problem, but rather than hardening their tenants from such attacks as other cloud services have, they have offered solutions only available to their high tier plans so as to capitalize on this problem rather than fixing it. As expensive as they are to migrate away from now, or sticky as they like to call it, their products are just going to become more costly to manage, vulnerable, and difficult to migrate away from over time. This is the malady of any legacy solution.
One easy way for Microsoft to mitigate such attacks is to update their RPS module to support DAP and develop other creative avenues for admins and the like to efficiently and securely manage their clients’ tenants. They should also extend their threat intelligence and advanced customizations available only to costly, high tier license subscribers to all license levels, at least until proper solutions are implemented for all tenant levels.
As an immediate mitigation step though, Microsoft could simply swap the order of authentication. Rather than requiring a password prior to doing a two-step verification on your phone, they could require the phone verification through authenticator app or a third party MFA app such as Duo as the initial means of authentication. By deferring their password in Azure AD as the second step instead of the first, they could buffer its weak password security at present and buy time to implement a proper solution. However, this only applies to users and tenants with MFA enabled and in-use.
Just as Active Directory seems to create necessity for other costly ancillary solutions, Microsoft seems to have built AzureAD to generate further necessity for more costly solutions coincidentally offered by them just the same. On top of this and if they had their way, their solution to enable MFA would also require employers to buy phones and mobile plans for two-step verification for all of their employees which can cost more on an annual basis than any of their plans.The same can be said of the costs associated with a proper MFA solution and/or an on-premises or hosted ADFS solution (if none exist) as they drastically complicate the solution as a whole while consequently inflating the ownership costs associated with it. As complexity increases, stability falters while costs skyrocket. All of which is why I recommend avoiding their solutions entirely.
But if a company is entrenched with Microsoft products and migration is out of reach, there are options. One solution that companies can implement is ADFS which defers authentication attempts to your own domain controllers on-premise rather than Azure AD while immediately granting more granular control of password policies with Active Directory on-premise and as much protection as money can buy on the network layer. All of which can be quite costly from a licensing perspective alone, let alone the hardware, network infrastructure, and labor required to implement it all let alone the staff to maintain it. This creates a single point of failure, often on-premise, for a cloud solution unless implemented in a highly available manner though.
They can also implement an MFA solution as well but there still remains added exposure and vulnerabilities which may require further consideration. But as mentioned before, there are also added costs and MFA may not protect accounts entirely. Users tend to manually synchronize their passwords across multiple platforms for the sake of remembering it, but not all of them have the same protections, MFA or otherwise. Similar to ADFS, access to your mailbox and other apps are restricted when MFA services are degraded, also becoming a single point of failure, as shown today by Azures MFA outage. So if you go with an MFA solution, diversify with a 3rd party MFA provider.
While the existence of dirsync can do little to protect against brute-force attacks, enforcing a strong password policy including a customized banned password list on premise can be mirrored in the cloud. Customers with dirsync already pay for this functionality with Active Directory on premise and can simply have it be mirrored in the accounts synced to the Azure AD forest. Although this cannot protect from brute force, password spray, or denial of service attacks, it can absolutely harden accounts against prolonged breaches.
I suppose they could also call support to complain about it and see if they’ll fix it, but you will likely be met by someone difficult to understand without experience on such matters. Or maybe they could even get a technical account manager to yell into the void or possibly even find someone with half of an ass on your behalf if you have deep enough pockets for a premier membership. While you’re at it, maybe you could upgrade your E3 plan to an E5 plan at almost double your monthly cost of E3 just to pay Microsoft to compensate for its own vulnerabilities.
In summary, Microsoft services built on Azure AD along with the businesses leveraging them are vulnerable to brute-force and password spray attacks which can be carried out by anyone with the capacity to run a script in RPS. Also, there isn’t an adequate means of hardening these services without incurring significant financial burden and paying for more of Microsofts services. All of which has probably been the case for as long as the ability to access tenants via RPS has been widely available to admins and ultimately why you would be wise to assume breach with Microsoft cloud solutions just as Microsoft does. Entities can absolutely mitigate these vulnerabilities, but Office 365 and Azure would cease to function as true cloud solutions while generating significantly more overhead costs in the process. All things considered though, it seems as if there is no way to harden Azure AD or the services such as Azure or Office 365 when leveraged by itself without incurring significant costs in addition to the aforementioned introduction of further complexity, points of failure, and on-premise dependencies for your cloud architecture.
By default , Azure AD is more of a security problem than a cloud. This is not to say that Azure cannot be made to be secure but it comes at a cost while sacrificing cloud resiliencies. Although they advise others to assume breach, Microsoft seems to be omitting this reality from Office 365 and Azure advertisements and such inconsistencies are indicative of this stance being more of a cop out than a tenable security strategy because of this. Rather than hardening the vulnerabilities inherent to Active Directory and Azure AD which makes them susceptible to some of the oldest tricks in the book, Microsoft seems to be attempting to capitalize on them instead while exposing those unaware to a haunting amount of risk.
#BlackLivesMatter
95 
95 claps
95 
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Engineer, Farmer, and Hellion
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/levvel-consulting/saml2-vs-jwt-apigee-azure-active-directory-integration-a-jwt-story-a3eb00769a1f?source=search_post---------286,"There are currently no responses for this story.
Be the first to respond.
In our next SAML2 vs JWT post, we are going to use a JWT with a very simple API that is proxied through Apigee Edge Public Cloud. The JWT token will be an OAuth2 access token generated by Azure Active Directory. In the last post in this series, we explored what JSON Web Tokens (JWTs) are and the information it contains. This post builds on the capabilities presented by Dino Chiesa at the Apigee I Love APIs conference in October, 2015. The example presented in this post is available in this github repository. This post briefly describes how to adapt the solution to working with other Azure Active Directory tenants, but primarily focuses on the details of making work with the example tenant setup here — keeping a functioning AAD tenant up and available for this example costs money and presents some security issues.
The figure below shows the system actors that will be involved in this story.
An API Consumer will obtain an access token (a JWT) by authentication against Azure Active Directory using an OAuth2 Resource Owner Password Credentials Authorization Grant. The JWT is then placed into the Authorization header of an API request and sent to the Apigee Edge API Gateway that we’ve constructed. The API Proxy pulls down and caches the federation metadata that is published by Azure Active Directory. The API Proxy attempts to validate the JWT token included in the request; the token is determined to either be valid or not. If it is valid, then the JWT is removed from the request and passed back to the API Provider. If the token is not valid, then an error is returned to the API Consumer.
First, we need an API. Let’s use https://timeapi.org. This will create a nice example that is harmless and mostly pointless (there are probably easier ways to find the time). Without the API Gateway layer, this API call will look something like:
Request:
Response:
The full documentation of this API is available at https://www.timeapi.org.
There are numerous API interface documentation paradigms out there. I like to use Swagger 2.0 or OpenAPI v2.0. Apigee is built around this interface language, I work for an Apigee partner, and it is simple to setup for most APIs. I created a simple Swagger definition that describes the functionality of this API that we are using. It can be found here. Apigee has the ability to define an API Proxy based upon the structure of an existing Swagger interface definition. This can definitely eliminate some of the busy work associated with creating a new API Proxy on Apigee Edge, but it will lead to a much more complex example for what we are trying to show.
Now, our goal is to offload the authentication and authorization functions of this API onto the API Gateway layer. To do this securely, we would normally establish a trust relationship between the API Gateway and the API Provider. Unfortunately, timeapi.org is a public API that doesn’t take any steps to provide such a mechanism. So, we won’t be able to do that for this example. If we did control the API Provider, that trust relationship could be accomplished through a variety of mechanisms including, but not limited to:
The authentication step will be the validation of a JWT token (per the spec) including:
The Authorization step could be quite elaborate, but for our purposes, we will rely upon the OAuth2/JWT authorization mechanisms that are provided to us in the form of the audience (“aud” parameter). As long as the this field contains the expected string, it is accepted — a very coarse grained authorization policy. The audience parameter in the JWT token, multiple roles described by its values, and a non-trivial mapping of API endpoints to roles provides the basis of a rich Role-Based Access Control (RBAC) policy. But, again, for our purposes, as long as the JWT token included with the request contains the expected audience value, the request will be considered authorized.
First, let’s construct a simple API Proxy that can proxy requests back to https://www.timeapi.org. This can be done with this tutorial.
The resulting API Proxy will look something close to the API Proxy in the image above. To access this API endpoint, one must call:
If your organization name is rcbjBlueMars (see upper right-hand corner), environment is called test (see upper right-hand corner), and the base path is “/time”, then this would be:
Since Apigee automatically does a translation of the front-end path and backend path,
will be translated to:
To have this solution look a little more professional and have the DNS names and OAuth2 scope information match it, we will setup a DNS CNAME entry called rcbj0001-api.rcbj.net that points at rcbjbluemars-test.apigee.net. This particular domain is registered at godaddy.com. So, that is done through standard configuration mechanisms in its domain management application. Apigee Support would need to add this name to the Virtual Host configuration in Apigee Edge Public Cloud. I’m doing this in the community edition; so, this isn’t available, but in any of the paid-for versions of the platform, this can be done quite easily. Then, the API request could be sent to:
The response looks just like the original API call response above. The Apigee Trace (think of it as an API processing policy debugger) session for this request looks like the following:
Obviously, there isn’t much going on here at this point. We are going to change that.
Now, consider this presentation from the Apigee I Love APIs conference in October, 2015, describing the use of JWT with Apigee API Proxies. This presentation and accompanying code were created by Dino Chiesa who I did two webcasts with in 2016. The code that accompanies this presentation can be found here. Apigee doesn’t have out of the box support for JWT token generation or validation. But, this project contains custom Java Code that can be used to validate JWT tokens from a variety of sources (Google, Azure Active Directory, SalesForce, etc). Instructions for how to deploy the sample API Proxy are included on the GitHub repository; I’m not going to cover the build and deploy process for the sample proxy.
I extracted the conditional rule that can validate JWTs generated by Azure Active Directory. We are going to use AAD as the Identity Provider that generates JWT tokens. Azure Active Directory uses JWT as the OAuth2 access token, which works out well for our goals. The details of how an Azure AD tenant was configured to work with this tutorial can be found here.
If we add the “parse + validate alg=RS256-ms” conditional rule to our sample proxy from above, we have something that looks similar too:
I added additional actions that extract the JWT token from Authorization header and, if valid, remove the Authorization header before forwarding the API request to the API Provider endpoint.
CORS has also been added to this project following the instuctions outlined here.
This API Proxy can be downloaded here.
The original example from the Apigee conference requires the APigee Edge enterprise pricing plan. To get this to run in a non-Enterprise Apigee organization, a couple of tweaks had to be made to the Java code. The source code and instructions for building the modified libraries are available in the github repository.
To run this in your own environment a couple of configuration changes must be made to the API Proxy including:
For an API Consumer, we will use the following script that runs a series of curl commands. The first curl command is the call to AAD to obtain an access token (JWT); the second curl command is the call to the API.
This script does not require any arguments. If it is called test-client.sh (as it is in the github repository), then one would simply run:
If you are trying to get this to work in your own environment and your own AAD tenant, then the CLIENT_ID, USERNAME_, PASSWORD_, RESOURCE_URL, and TENANT_ID variables must be updated to match the details for your tenant.
The first curl command (OAuth2 call) response looks something like:
The access_token property is a JWT token. This can be copied directly into the Authorization HTTP Request Header (per RFC 6750) as “Bearer JWT…”.
The payload of this JWT token looks like:
The details of what is in this token are explored in our last post.
An application such as a web application, SPA app, or Mobile App could cache the access token and refresh token. The access token can be used across API calls until it expires. The refresh token can be used to obtain a new access token — more on this later. Eventually, the user would have to log into the app again; this would be a configurable set of parameters. Applications such as these would likely use an interactive login with the OAuth2 Authorization Code Authorization Grant or Implicit Authorization Grant.
The trace session associated with running the test script will look similar to the following:
The first policy (colored box) in the request is a Cache Lookup Policy. This looks in an Apigee Cache called signer-cert for the cached copy of the Azure Active Directory signer certificate included in the Federation Metadata for the AAD tenant that this API Proxy trusts. Since this isn’t the first time this API Proxy has been run in the past hour, the certificate is found and assigned to a flow variable called “cached.ms.cert”. Since there was an entry in the cache, the next four policies are skipped (as indicated by the arced arrow in the upper, left-hand corner of each. Had the cached metadata not been found, the second policy (a Service Callout Policy) would have made a call out to:
The metadata document would be written to a flow variable called “msCert”.
The third policy (which also didn’t run because the metadata was found in the cache) is a Javascript callout that removes the XML Declaration from the XML document stored in the “msCert” flow variable. The fourth policy (also didn’t run this time) is an Extract Variable policy that reads the signer certificate value from the retrieved XML document via an XPath expression and would assign the certificate value to the “ms.certificate” flow variable. The fifth policy (ran this time) is a Populate Cache Policy that writes the “ms.certificate” flow variable to the signer-cert Apigee Cache. The sixth policy (also didn’t run this time) writes the value stored in the “ms.certificate” flow variable to the signer-cert cache so that it can be efficiently retrieved later. The seventh policy, which always runs, extracts the JWT token from the Authorization header and places it in “authn.jwt” flow variable. The eighth policy is a Java Callout that runs the code from the original GitHub project and validates the JWT token. The next policy is a Raise Fault Policy that only runs if an error condition flow variable was set in the Java Callout Policy that just ran — any failure of JWT validation returns a 401. The final policy is a Javascript Callout that removes the Authorization header and the JWT token it contains.
The response processing only runs one Policy, which sets several CORS-related HTTP Response Headers.
Another way to test this API is to use Swagger Editor. Load the Time API Swagger document into the Swagger Editor by choosing File->Import File from the menu. Then, click the “Choose File”. Load the $REPOSITORY_HOME/swagger/swagger.json. Finally, click the Import button. You will see a screen similar to the following.
Click the “Authentication” button.
Run the test-client.sh script again. Use the ASSERTION variable value as the access token value that is put in this pop up’s field. Then, click the Authenticate button.
Now, scroll down to the “Try this operation” button for the GET on /{timezone}/{when}.json. The screen expands to show fields to put values in for all varibles.
Make sure the check box next to “oauth2ResourcePasswordGrant” is checked. In the “timezone” field, add a mainland US timezone (as defined in the swagger interface definition) such as “pst”. In the “when” field, add a natural language time description as defined by the chronic documentation (such as “now”). The request should look similar to the following at this point.
Click the “Send Request” button. The response will look something like the following.
With that, we have covered this JWT-based authentication and authorization example end-to-end. As always, please leave comments and questions. For more from Robert and the Levvel team, visit our website.
Ask us how we can help transform your business.
We help big companies innovate like startups, & small…
69 
1
69 claps
69 
1
Written by
My focus within Information Technology is API Management, Integration, and Identity–especially where these three intersect.
We help big companies innovate like startups, & small businesses scale like industry leaders. Ask us how we can help you transform your business.
Written by
My focus within Information Technology is API Management, Integration, and Identity–especially where these three intersect.
We help big companies innovate like startups, & small businesses scale like industry leaders. Ask us how we can help you transform your business.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/extracting-form-data-to-json-excel-pandas-with-azure-form-recognizer-160488a2d11e?source=search_post---------287,"There are currently no responses for this story.
Be the first to respond.
TLDR; This post shows how to extract data from table images for pandas and Excel using the Azure Form Recognizer Service in Python.
The Azure Form Recognizer is a Cognitive Service that uses machine learning technology to identify and extract text, key/value pairs and table data from form documents. It ingests text from forms and outputs structured data that includes the relationships in the original file.
There is a free tier of the service which provides up to 500 call a month which is more than enough to run this demo.
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
The Azure Form Recognition Service can be consumed using a REST API or the following code in python.
docs.microsoft.com
However this code returns the result in JSON format with a lot of additional information not relevant to the actual processing of the form data.
The following code sample will show you how to reformat this JSON code with python into a pandas DataFrame so it can processed in a traditional data science pipeline or even exported to Excel.
We will use the pre-trained receipt model for this tutorial. End to End Code Can be Found in the following gist.
Once your data is an pandas DataFrame it can be converted to CSV to process with Excel in just one line of code.
Hope you enjoyed this demo of the power of the Azure Form Recognizer Cognitive Service. Check out the next steps to see how to train your own custom models and then use this code to extract them to pandas and or Excel.
docs.microsoft.com
docs.microsoft.com
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
38 
1
38 claps
38 
1
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/azure-sentinel-helping-your-soc-with-investigation-and-hunting-ba1a8442deaa?source=search_post---------288,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Jul 15, 2019·5 min read
Separating the wheat from the chaff in cybersecurity is hard. Often you find yourself handling enormous volumes of events. And more than not, data quality is an issue. False positives often lead to triage fatigue.
Being able to do triage quickly is important. The time window to respond when under attack is short, advanced adversaries typically only need hours to gain access, elevate privileges and exfiltrate data.
Azure Sentinel just released their Investigation feature (as a preview). But what is the difference between investigating and hunting? And how exactly does Azure Sentinel help my SOC with both?
How does a SOC operate?
Before we dive into Azure Sentinel’s new investigation features, let’s rewind and first look at how a Security Operations Center (SOC) operates. Most SOC’s will have two critical functions: (1) setting up and maintaining security monitoring and related tooling. And (2) find suspicious or malicious activity by analyzing alerts.
For the latter, typically a SOC will have a 3-tier model. Tier 1 is where security analysts do the triaging; they review the latest alerts to determine relevancy and urgency. Alerts that signal an incident will be forwarded to Tier 2 analysts. The second tier reviews those cases and uses threat intelligence (IOC’s, etc.) to identify affected systems and the scope of the attack. Tier 3 are often the most experience people on the team, mostly referred to as Threat Hunters. They explore the environment to identify stealthy threats and conduct continuous vulnerability tests.
In most SOC’s the Tier 2 folks will do the investigations, and Tier 3 folks will do the hunting. And while there is no clear line, most people refer to the term Investigation when they are following up with a (by Tier 1 forwarded) case. Robert M. Lee has a great quote on this:
“Threat hunting exists where automation ends”. Threat hunting is large manually, performed by SOC analysts, trying to find a ‘needle in the haystack’. And in the case of cybersecurity, that haystack is a pile of ‘signals’.
Investigation UI in Azure Sentinel
Microsoft just released an investigation experience in Azure Sentinel as a preview. Before, you would find an investigation UI in Azure Security Center, but as Azure Sentinel is becoming the central place to aggregate security the investigations will likely happen from there, and therefore Microsoft is deprecating the investigation UI in ASC.
To get to the new Investigation Experience in Azure Sentinel you will need navigate to a Case. For every Case you’ll find two buttons: View Full Details and Investigate. Previously, the Investigate button would show a placeholder page of Coming Soon, but today you’ll be launched into a new window.
The investigation experience window has three sections: the top will show the Case name, and other Case details. On the right you’ll find four buttons: Timeline, Info, Entities and Help. The main window will show all the entities related to this Case in a graph style manner.
Clicking an entity will show the details, hovering over the entity will give you some quick actions, for instance these: Related Alerts, Hosts the Account Failed on, Hosts which the Account Logged On to.
Clicking these will show these results as extra entities in your graph, expanding on your search.
There is also the opportunity to dive into the raw results, pivoting from the graph to a KQL query window:
The timeline button on the right allows you to ‘bookmark’ items/results during your investigation and have them readily available as information on this ‘notebook’.
Entity mapping is important!
When you’re creating Alert Rules in Azure Sentinel, that will then trigger Cases when the criteria is met, you have the option to do Entity Mapping. From the underlying KQL query, you can pick any field and map it into either ‘Account’, ‘Host’ or ‘IP addresses.
This allows Azure Sentinel to recognize that data as such and provide the right Quick Investigation items, and more importantly link data/Cases together. More entities are coming soon, but for now these three are available.
OK, so what about hunting and supporting Tier 3 SOC analysts?
I wrote about that in a previous blog called ‘Threat Hunting in the cloud with Azure Notebooks’. I talked about how you could take the ‘Kqlmagic’ extension that Microsoft wrote, and use KQL queries in Jupyter notebooks to hunt for malicious actors.
Something I did not mention in that blog is the built-in support for Hunting that Azure Sentinel has. Microsoft provides you with pre-compiled KQL queries to find known indicators in your environment. These are available in the Hunting node under the Threat Management section and are mapped back to the Tactics of the MITRE ATT&CK framework. You can add your own favorite Hunting to the workspace as well.
Directly from this section of Azure Sentinel you can run the query by using the Run Query button.
When running the query, you can expand (one of) the results and use the [..] button to access the Bookmark function. This saves results and allows you to relate them to an ongoing campaign by using the Tags field.
You’ll find these Saved Queries in the Hunting section of Azure Sentinel under the Bookmarks tab.
Conclusion
The new Investigation Experience in Azure Sentinel is an easy way to start your investigations, for instance by your Tier-1 SOC analysts. It visualizes your Case in a graph, which makes it easy to find connections between data points.
It will replace the investigation functionality Azure Security Center, but compliments the fact that your Tier-2 an Tier-2 SOC analysts can use their favorite investigation and hunting tools in combination with Azure Sentinel, like for instance Jupyter Notebooks.
Happy investigating!
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
79 
1
79 claps
79 
1
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-host-a-react-static-website-on-azure-438e0a915295?source=search_post---------289,"There are currently no responses for this story.
Be the first to respond.
There are a number of ways you can build a website with React such as Java with React, NodeJS with React, NGINX serving React, etc. For the single-page applications, all you need to do is to load the initial index.html. Once you load the index.html the React library kicks in and do the rest of the job like loading…
"
https://koukia.ca/azure-documentdb-vs-sql-azure-performance-comparison-revised-178a90038146?source=search_post---------290,"A while ago I wrote some code to compare the performance of Azure DocumentDb and SQL Azure and the conclusion was:
"
https://koukia.ca/managed-kubernetes-on-azure-aks-8514ac0cd8aa?source=search_post---------291,"In a previous post, I showed you how to create a Kubernetes Cluster, on Azure Container Service (Considered an IaaS offering, since you manage all the resources) and as you could see , it was pretty straightforward.
Today I will walk you through a new service on Azure called Managed Kubernetes or AKS, and this is even easier.
"
https://medium.com/@renatogroffe/azure-functions-dicas-e-truques-no-desenvolvimento-serverless-parte-1-77f033a95576?source=search_post---------292,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Dec 2, 2019·4 min read
Neste novo artigo inicio uma série com diversas dicas e truques envolvendo o desenvolvimento de aplicações serverless baseadas em Azure Functions. Sempre que possível darei andamento a esta iniciativa, com novos conteúdos e orientações úteis.
E como Azure Functions é o assunto deste post, deixo aqui um link com todos os conteúdos gratuitos (artigos, vídeos, projetos de exemplo) que venho criando sobre esta tecnologia:
Serverless + Azure Functions: Guia de Referência
Aproveito este espaço também para um convite.
Que tal aprender mais sobre Serverless e Azure Functions em um treinamento inteiramente prático e com um preço super camarada já que estamos em Black F̵r̵i̵d̵a̵y̵ Week? Acesse o link a seguir para obter um desconto de 25% na primeira edição do Azure na Prática em 2020, no dia 18/01 (sábado) em São Paulo-SP: http://bit.ly/black-week-blog-groffe
Corra, porque este preço promocional é por um tempo bem limitado!
Independente do sistema operacional, ferramenta de implementação ou linguagem escolhidos, a configuração de um ambiente local de desenvolvimento baseado em Azure Functions dependerá da instalação de um conjunto de ferramentas conhecido como Azure Function Core Tools. Maiores informações podem ser encontradas no link a seguir:
Work with Azure Functions Core Tools
O próprio Visual Studio Code emitirá alertas para a atualização deste complemento sempre que uma nova versão estiver disponível:
Atualmente temos como ferramentas para desenvolvimento de soluções com Azure Functions o próprio Portal do Azure (alternativa mais limitada), IDEs como Visual Studio (atualmente na versão 2019) e Visual Studio for Mac, além do Visual Studio Code. Exceto pelo Portal do Azure, todas essas alternativas dependem da instalação das Azure Functions Core Tools. É justamente este complemento que fornecerá todo o suporte para a implementação de Function Apps (projetos contendo uma ou mais Azure Functions), incluindo a execução local de aplicações deste tipo.
No caso específico do Visual Studio Code, será necessária também a instalação de uma extensão que viabilizará o desenvolvimento de soluções baseadas em Azure Fuctions:
Na imagem a seguir temos um exemplo envolvendo o debugging de uma Function App no Visual Studio Code em Windows:
Para saber mais sobre o desenvolvimento serverless com Azure Functions e o Visual Studio Code acesse os seguintes links:
.NET Core + Serverless: implementando jobs com Azure Functions e o VS Code
.NET Core + Serverless: publicando uma Azure Function via VS Code
Já na próxima imagem podemos observar a execução e debugging de uma Function App a partir do Visual Studio 2019:
Suporte a múltiplas linguagens
No momento da publicação deste artigo (Dezembro/2019) temos como opções para a implementação de Azure Functions as seguintes linguagens/plataformas:
Trata-se de uma ampla gama de alternativas para a criação de soluções serverless no Microsoft Azure, além de permitir até mesmo que SysAdmins habituados ao uso de PowerShell possam se beneficiar das vantagens oferecidas pelas Azure Functions.
Conforme demonstrado em prints nas seções anteriores, conseguimos facilmente trabalhar na implementação de Azure Functions em ambientes como Windows e Linux. O desenvolvimento a partir de máquinas com macOs também é uma possibilidade, com isto acontecendo através do Visual Studio for Mac ou do Visual Studio Code.
Na próxima imagem é possível observar o debugging de uma Azure Function no Visual Studio Code, com isto acontecendo em uma máquina com o Ubuntu Desktop 18.04 instalado:
No vídeo a seguir do canal Coding Night temos um exemplo de desenvolvimento com Azure Functions no macOS, utilizando para isto o Visual Studio for Mac:
Uma vez instaladas, as Azure Function Core Tools permitirão que executemos Function Apps via linha de comando (Bash ou PowerShell). Isso é possível através da instrução:
Podemos assim efetuar testes sem a necessidade de abrir uma aplicação numa IDE ou no VS Code. É o que demonstram as próximas imagens, com a inicialização de um projeto de testes a partir do PowerShell no Windows 10 Professional:
A seguir temos um exemplo similar em Linux (Ubuntu Desktop 18.04), com a execução via Terminal de uma Function App:
Azure Functions documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
29 
29 claps
29 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/aplica%C3%A7%C3%B5es-web-escal%C3%A1veis-no-azure-app-service-docker-e-kubernetes-global-azure-bootcamp-2019-ead08d12e278?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 17, 2019·3 min read
No dia 27/04/2019 (sábado) aconteceu a edição local do Global Azure Bootcamp no auditório da Venturus em Campinas-SP. Esta foi uma iniciativa promovida pelas comunidades Campinas .NET, .NET SP, Azure Talks, DevOps Professionals e Open Source SP (das quais também sou um dos Coordenadores).
Este post é um registro da apresentação que realizei sobre o uso de tecnologias como App Service, Docker e Kubernetes para a implementação de aplicações Web escaláveis no Microsoft Azure. Esta palestra foi realizada para um público estimado em 70 pessoas.
Os slides que utilizei foram disponibilizados no SlideShare:
As demonstrações realizadas tomaram como base o seguinte projeto:
ASP.NET Core + Docker + Alpine Linux + Docker Hub + Kubernetes + AKS (Azure Kubernetes Service)
A implementação deste projeto, bem como a criação de objetos para deployment em um cluster Kubernetes foram abordadas por mim nos seguintes artigos:
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem — parte 1
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem — parte 2
O post a seguir traz estes materiais e ainda outras referências sobre a utilização do Azure Kubernetes Service (AKS):
Azure Kubernetes Services - AKS: referências gratuitas e dicas para solução de problemas comuns
O uso do Azure App Service com projetos ASP.NET Core foi também abordado nos seguintes artigos:
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
ASP.NET Core: dicas úteis para o dia a dia de um Desenvolvedor - Parte 4
Para conhecer mais sobre o Azure Web App for Containers acesse o post a seguir:
ASP.NET Core + Azure Web App for Containers: escalando uma API REST com containers Docker
Já demonstrei também o deployment de aplicações empregando containers Docker na nuvem no seguinte vídeo do Canal .NET, em que cobri a utilização do serviço conhecido como Azure Web App for Containers:
O uso do Kubernetes foi tema ainda de 2 eventos online gratuitos do Canal .NET, com a gravação dos mesmos estando disponível no YouTube e abordando desde os primeiros passos ao deployment automatizado via Azure DevOps (antigo VSTS):
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
115 
115 
115 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/ey-ireland/introduction-to-azure-machine-learning-service-87135cfbf78b?source=search_post---------294,"There are currently no responses for this story.
Be the first to respond.
In September of 2018, Microsoft launched Azure Machine Learning Service that helps data scientists and machine learning engineers build end-to-end machine learning pipelines in Azure without worrying too much about dev-ops behind training, testing and deploying your model. Azure has a number of offerings in this space such as Azure Notebooks, Azure Machine Learning Studio and Azure Batch AI. Comparison of the offerings is available here.
This article will focus on the newest offering by Azure, and it will cover the basic concepts including an example of training your own machine learning model. Following is the breakdown of the article:
In order to work with AML, you need to be aware of the following concepts:
Workspace: It is a centralised place for all your artefacts (experiments, runs, deployments, images). It helps keep a history of all your work including registering machine learning models that can be used for prediction. To create a workspace:
When you create a workspace, Azure will create container registry, storage, application insights and key vault for you. This enables you to store docker images in the container registry, store data in storage, monitor model performance in application insights and store sensitive information in key vault including compute target keys.
Experiment: Within the workspace, you can define experiments that contain individual training runs. Each training run you perform will associate itself with an experiment and a workspace. Defining logical high level experiments will help you monitor various training runs and their outputs.
Model: This is the heart of any machine learning process. AML provides the ability to register (version) models produced during each training run. Each registered model is physically stored in the storage provided while creating the workspace. This enables machine learning practitioners to test and deploy variety of versioned models without having to store them locally. Models produced from any of the machine learning libraries (scikit-learn, TensorFlow, PyTorch, etc) can be registered.
Image: Docker images containing the model and the prediction (scoring) script can be created once the model is tested. AML provides the ability to create these images and version them, similar to model versioning. This enables multiple Docker images to be created and deployed with different versions of the model.
Deployment: The Docker images created can be deployed using Azure Container Instances. This is where the true power of AML lies. Automatically creating a load-balanced HTTP endpoint without having to worry about underlying infrastructure or deployment configuration helps machine learning practitioners focus on training and evaluating their model. AML helps collect application insights to monitor the performance of your deployment.
In the above section, we went through the basic concepts of AML. It is important to understand these concepts before one can start working with this service. In this section, we will focus on setting up our AML environment.
Setup VS-Code: My preferred IDE for working with AML is VS-Code (referred to VSC from now on). This is motivated from the fact that there is an Azure extension available in VSC that enables seamless connection to the AML workspace helping us get a visual view of our workspaces, experiments, models, images, and deployments.
Follow the steps below to setup VSC:
Fork and clone AML repo: In order to get quickly started with AML, I have created a GitHub repo with an example that will help you quickly train and deploy a simple sklearn regression model. Please fork and clone the repository before continuing.
Setup conda environment: In the repo, there is a conda environment file available (environment.yml). As a good practice, I suggest creating a new virtual environment using conda. This will isolate your dependancies for the AML pipeline without breaking dependencies for your existing projects. To create a new conda environment with all the required packages:
This will create a new conda environment called myenv. Note: You can change the name of the environment by editing environment.yml file. Now you are ready to train your first model using the AML pipeline.
To configure the pipeline, a configuration file is provided in the repo. Let us go over each section in the configuration file and its corresponding module in the pipeline.
Part1 — Workspace Configuration (ws_config):
In order to configure your workspace, a separate configuration file is produced under aml_config/config.json. This can be done by entering the above parameters in the ws_config section of the ml-config.ini file and running, (Note: you can run this via VSC but make sure to activate the conda environment first)
Note: This step needs to be done once for each project you create in the pipeline unless you change your resource group and/or workspace.
Part2 — Training (train):
As an example, my configuration file looks like this:
train.py will contain your training code and an example of a simple sklearn regression model training is provided in the repository. main_train.py is the driver for training. The code is well commented and self explanatory, but I will go through the key parts of the code.
Now you can run your training by and it should produce the following output:
Part3 — Create Docker Image (docker):
As an example, my configuration file looks like this:
The above downloaded model can be evaluated and it can be used to create a Docker image. score.py script is used for prediction and an example is provided in the repository. create_docker.py is the driver for creating the docker image and is well commented. I will go through the key parts of the code.
Now you can create the docker image by running,
You can now see the Docker image being created under Images section of your workspace in the VSC IDE.
Part4— Create Deployment (deploy):
As an example, my configuration file looks like this:
To deploy the above created Docker image, run:
I will go through the important parts of the create_deployment.py script,
The output will look something like this,
After deploying the service, you can access the HTTP endpoint from the properties.json file of the deployment.
Part5— Test Deployment:
I am using Postman to test the service with the following input for the above created sklearn regression service.
To recap,
I encourage you use this pipeline for your machine learning use-cases including training and deploying deep learning models.
Disclaimer: The views represented in this article our those of the author and not of EY Ireland.
For more information: https://www.ey.com/gl/en/issues/business-environment/ey-global-innovation
This publication is contributed to by EY Ireland community…
99 
99 claps
99 
This publication is contributed to by EY Ireland community of analytics and AI professionals. In this publication we will bring you coverage of latest developments in the field of AI, Analytics and RPA.
Written by
Data Science Manager @EY and Chief Data Scientist @IdeaChain; A hub for ideas, discussion and collaboration -http://ideacha.in
This publication is contributed to by EY Ireland community of analytics and AI professionals. In this publication we will bring you coverage of latest developments in the field of AI, Analytics and RPA.
"
https://medium.com/@olafusimichael/easy-steps-to-creating-and-deploying-a-predictive-model-using-azure-machine-learning-studio-d35adb018543?source=search_post---------295,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Olafusi
Apr 22, 2018·7 min read
Yesterday, I demoed how in 10 mins you can create a predictive model, deploy it as a web service and test it without having to install anything or pay any money. That is the awesomeness made possible by Microsoft with the super easy to use Azure Machine Learning Studio. The yesterday event was the Lagos edition of the Global Azure Bootcamp held at Microsoft office, Lagos.  Participants were able to follow along, created and deployed their own predictive models too. In today’s post I will be guiding you with easy steps to follow on how you too can in a few minutes create and deploy a predictive model cost-free with Azure Machine Learning Studio.  Step 1 Download the sample data we would use: Bank Marketing data from UCI Machine Learning Repository. If you download from UCI Machine Learning Repository directly, then it is the bank-additional-full.csv file in the zip file you end up with. Then you have to make sure that you break the data into separate columns rather than leave them comma separated, using Excel’s Text to Columns. For you ease, I have shared a cleaned version you can directly use without any extra work by you: Bank Marketing data download  The sample data is a marketing campaign data of a Portuguese bank from May 2008 to November 2010 recording the details of prospects reached via phone calls and whether they eventually took up the service the bank was trying to sell them.
Below is the explanation of the different fields in the data records.  Input variables: # bank client data: 1 — age (numeric) 2 — job : type of job (categorical: ‘admin.’,’blue-collar’,’entrepreneur’,’housemaid’,’management’,’retired’,’self-employed’,’services’,’student’,’technician’,’unemployed’,’unknown’) 3 — marital : marital status (categorical: ‘divorced’,’married’,’single’,’unknown’; note: ‘divorced’ means divorced or widowed) 4 — education (categorical: ‘basic.4y’,’basic.6y’,’basic.9y’,’high.school’,’illiterate’,’professional.course’,’university.degree’,’unknown’) 5 — default: has credit in default? (categorical: ‘no’,’yes’,’unknown’) 6 — housing: has housing loan? (categorical: ‘no’,’yes’,’unknown’) 7 — loan: has personal loan? (categorical: ‘no’,’yes’,’unknown’) # related with the last contact of the current campaign: 8 — contact: contact communication type (categorical: ‘cellular’,’telephone’)  9 — month: last contact month of year (categorical: ‘jan’, ‘feb’, ‘mar’, …, ‘nov’, ‘dec’) 10 — day_of_week: last contact day of the week (categorical: ‘mon’,’tue’,’wed’,’thu’,’fri’) 11 — duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=’no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. # other attributes: 12 — campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact) 13 — pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted) 14 — previous: number of contacts performed before this campaign and for this client (numeric) 15 — poutcome: outcome of the previous marketing campaign (categorical: ‘failure’,’nonexistent’,’success’) # social and economic context attributes 16 — emp.var.rate: employment variation rate — quarterly indicator (numeric) 17 — cons.price.idx: consumer price index — monthly indicator (numeric)  18 — cons.conf.idx: consumer confidence index — monthly indicator (numeric)  19 — euribor3m: euribor 3 month rate — daily indicator (numeric) 20 — nr.employed: number of employees — quarterly indicator (numeric)  Output variable (desired target): 21 — y — has the client subscribed a term deposit? (binary: ‘yes’,’no’)  Step 2 Sign up for Azure ML studio. It is easy and free: https://studio.azureml.net
Step 3 Upload the Bank Marketing dataset. From Datasets section on the left menu pane, click New at the bottom left.
Step 4 Create a new experiment. From Experiments section on the left menu pane, click New at the bottom left. Choose a blank experiment, as we are creating ours from scratch.
Step 5 Now we start dragging the tasks we want to carry out into the Experiment workspace, after renaming the Experiment.
Drag in the dataset we uploaded, it is in the Saved Dataset section on the left.
Next, we need to isolate the fields that would be useful for our predictive model. If you look at the description of all the fields in the dataset, it is obvious that some are not practically useful in creating a prediction of whether a prospect will take up the marketed service or not. Example is the length of the call, there is no way you would know that until the end of the call — so not useful for profiling who to call (targeted marketing). By my thinking, the fields I that would be of real world use in creating an actionable predictive model are — age, job, marital, education, default, housing, loan.  Drag Select Columns in Dataset in the Manipulation subsection of Data Transformation section. Connect the dataset previously dragged in to the select columns task. Then click on Launch column selector, and select the columns needed (including the outcome we want to predict, so as to be able to train the model).
Next, split the data into training set and testing set for building our predictive model. Drag Split Data, connect to the select columns task and on the settings pane on the right, set the training set to 0.75 (75%) of the entire dataset.
Drag in the model algorithm to use. It’s under the Initialize Model. I chose to use the Two-Class Decision Forest. In the end, you would evaluate the model to see if it fits well or you should try another algorithm.
Drag in Train Model. Connect to both the already dragged in algorithm and the left side of the Split Data (training set). Select the outcome to predict.
Drag in Score Model. Connect to the Train Model and the testing set of the Split Data.
Lastly, drag in Evaluate Model. Connect to Score Model.
Now run the entire experiment.
Wait for it to finish running.
Right click on Evaluate Model and visualize the evaluation result to see the fitness/accuracy of the algorithm.
If you are okay with the fit, then what’s left is to publish. Otherwise, you can change the algorithm, re-run and re-evaluate the fit.
Step 6 Now you set up the model as a web service that can be deployed online.
Change the input connector to point to the Score Model.
Also, remove the predicted column from the Selected Column as it was only needed for training the model.
Now re-run and deploy as web service.
You are presented with the web service details to use for integrating with any app or online tool. You can even test the API directly.
And that’s how you create and deploy a predictive model in Azure Machine Learning Studio without installing anything on your computer and without paying a cent/kobo.  Enjoy!
Originally published at www.olafusimichael.com.
Founder of www.urbizedge.com www.msexcelclub.com, www.nigeriamarketdata.com and www.investmentng.com I also blog daily at www.olafusimichael.com
114 
114 
114 
Founder of www.urbizedge.com www.msexcelclub.com, www.nigeriamarketdata.com and www.investmentng.com I also blog daily at www.olafusimichael.com
"
https://medium.com/@jthake/lets-build-the-office-365-and-azure-community-on-medium-b38c0c5ca9b5?source=search_post---------296,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeremy Thake
Nov 22, 2016·1 min read
So I’m pretty amped about medium right now after exploring it this evening. I’ve followed it for a while and the latest improvements to the platform are incredible. @ev has done a great job of explaining it all himself.
Looking at the current tag followers of Microsoft (7757), Office 365 (381) and Azure (448)…there is certainly work to do.
I suspect there are a few reasons bloggers haven’t engaged here.
From reading authors already on medium, the benefit here is the network it provides. It’s simply incredible how engaged people are on here and how quickly posts spread. Once the tags for the Office 365 and Azure community grow, the value will be enormous and it won’t take long.
I built both SharePoint dev wiki.com and NothingButSharePoint.com (still gets 60K views a month on legacy content) communities with amazing friends before!
I’ll be rounding up the troops to see what we can start here…
Microsoft Graph Team, Senior Program Manager at Microsoft.
21 
5
21 
21 
5
Microsoft Graph Team, Senior Program Manager at Microsoft.
"
https://medium.com/@tsuyoshiushio/how-to-pass-variables-with-pipeline-trigger-in-azure-pipeline-5771c5f18f91?source=search_post---------297,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jan 11, 2020·2 min read
Pipeline triggers are introduced. It enables one pipeline is completed then subsequent pipeline works. Then, how to pass the variables between two?
If you want to execute subsequent pipeline automatically, all you need is to add this section on your pipeline yaml. It shows that when the Parent.CI completed, this pipeline start working. The point is trigger: none Azure Pipeline seems trigger: master by default. So if you didn’t add trigger: none and you commit something to master branch, it automatically start this pipeline. Also, pipeline triggers also triggers this pipeline after the Parent.CI complete.
docs.microsoft.com
Then how to pass the variables from Parent to Child? We have no way to directly pass the variables. However, we can pass it through artifact.
Look at this example. It is simply save environment as file. We can choose the format, however, I save it as logging command.
Child.CI
The child pipeline echo the file. Then the variables are restored. You need to fill ` <YOUR_PROJECT_ID_HERE>` section. However, if you use editor on the Azure Pipeline, you can choose a Project and a Pipeline as a drop down list.
Senior Software Engineer — Microsoft
90 
1
90 
90 
1
Senior Software Engineer — Microsoft
"
https://medium.com/@gmusumeci/how-to-import-an-existing-azure-resource-in-terraform-6d585f93ea02?source=search_post---------298,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Feb 19, 2020·3 min read
In this story, we will learn how to import an existing Azure Resource in Terraform.
This allows us to use resources that we have created by some other means and bring it under Terraform management.
Update February 27, 2020: This procedure is valid for both AzureRM v1.x and AzureRM v2.x.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-deploy-an-azure-app-service-using-terraform-33f69b72e099?source=search_post---------299,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Jun 26, 2020·3 min read
Azure App Service Web Apps is a PaaS (Platform as a Service) platform service that lets us quickly build, deploy, and scale enterprise-grade web, mobile, and API apps.
We can focus on the application development and Azure App Service will take care of the infrastructure required, and automatically scale our apps.
"
https://medium.com/@aallan/the-azure-sphere-starter-kit-fa2dc546eac7?source=search_post---------300,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alasdair Allan
Jul 17, 2019·3 min read
Last year, Microsoft unveiled Azure Sphere — an end-to-end solution for securing microcontroller based smart things. Alongside the announcement was hardware support for boards built around the MT3620 microcontroller.
Introduced earlier in the year at CES in Las Vegas, the Azure Sphere Starter Kit is based around the Avnet Azure Sphere MT3620 Module, and if you’ve been thinking about taking a look, it might be the right time. That’s because Avnet and Microsoft just launched a contest that not only might bag you free hardware, but a share of $35,000 in prizes.
The starter kit is built upon the Azure Sphere module, which makes use of the MT3620 processor, a single-core Arm Cortex-A7 processor running at 500 MHz with 4MB RAM, as well as a dual-core Arm Cortex-M4F real-time core running at 200 MHz with 64KB RAM, and support for dual-band 802.11 a/b/g/n wireless. The 33 × 22 mm module is production-ready, both FCC and CE certified, and comes in two versions either with an on-board chip antenna or one with an external U.FL connector. The starter kit is based around the on-board chip version of the module.
The module has 3× ISU interfaces pre-configured for UART, SPI, I2C, along with 3× 12-bit ADC inputs (or 3 GPIOs) and 9× PWM outputs (or up to an additional 24 GPIOs). There is also support for an RTC.
The carrier board supports two MikroE Click board expansion sockets, and a Grove System expansion connector, and has a variety of on-board sensors including a 3-axis accelerometer and gyro, an ambient light sensor, as well as temperature and pressure sensors. There are also two user programmable buttons, and footprints to support a 128×64 pixel OLED display, and both +5V and VBAT supplies. Power and data are connection to the board are normally provided via a micro USB connector.
The Azure Sphere Starter Kit costs $75 plus $8 for ground shipping, or $12 for two-day air delivery. However, if you register to take part in the Secure Everything with Azure challenge, you can pick up one of 20,000 available starter kits for free.
Your project for the contest should integrate a new or existing Internet of Things edge device with the Azure Sphere, or secure a consumer electronics project. It should also show innovation around smart retail, factory solutions, buildings or home automation, or around renewables and energy solutions.
The focus of the Azure Sphere is to securely connect edge devices to the cloud, so to be eligible for the top prizes your project should be able to consistently stay online and connected for at least 15 consecutive days.
The top three competitors will win a Microsoft Hololens 2, while the next seven competitors will pick up a Surface Laptop 2. There are even brand new Raspberry Pi 4 boards for the next 500 entrants. Judging criteria for projects will focus on documentation, details of the Bill of Materials, the availability of schematics and code, and creativity. While your project doesn’t have to be original, it should be a creative take on the idea.
Applications for free hardware run all month, with recipients announced on the August 8th. Submissions close on September 29th, with the winners revealed on October 17th.
Scientist, Author, Hacker, Maker, and Journalist.
90 
2
90 claps
90 
2
Scientist, Author, Hacker, Maker, and Journalist.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bb-tutorials-and-thoughts/how-to-write-serverless-python-rest-api-with-azure-functions-504c0113c1c8?source=search_post---------301,"There are currently no responses for this story.
Be the first to respond.
An Azure Function is a simple way of running small pieces of code in the cloud. You don’t have to worry about the infrastructure required to host that code. You can write the Function in C#, Java, JavaScript, PowerShell, Python, or any of the languages that are listed in the Supported…
"
https://koukia.ca/deploying-docker-containers-to-azure-app-services-672d587c5c39?source=search_post---------302,"When you containerize your application and want to use Azure to host it, there are 3 main options out there right now that you can use. And each one comes with its own features and level of management and administration.
Different options to host Containers in Azure are:
"
https://towardsdatascience.com/how-to-connect-azure-data-factory-to-an-azure-sql-database-using-a-private-endpoint-3e46984bec5e?source=search_post---------303,"Sign in
There are currently no responses for this story.
Be the first to respond.
Adam Prescott
Jan 21, 2021·4 min read
Azure Data Factory (ADF) is great for extracting data from multiple sources, the most obvious of which may be Azure SQL. However, Azure SQL has a security option to deny public network access, which, if enabled, will prevent ADF from connecting without extra steps.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/azure-difference-between-azure-service-bus-queues-and-topics-comparison-azure-servicebus-queue-vs-topic-4cc97770b65?source=search_post---------304,"There are currently no responses for this story.
Be the first to respond.
Comparison — Azure Service Bus Queue vs Topic.
Queues and Topics are similar when a sender sends messages, but messages are processed differently by a receiver. A queue can have only one consumer, whereas a topic can have multiple subscribers.
Difference between Azure Storage Queue and Service Bus Queue
"
https://medium.com/bb-tutorials-and-thoughts/how-to-host-an-angular-static-website-on-azure-1257eed9d47e?source=search_post---------305,"There are currently no responses for this story.
Be the first to respond.
There are a number of ways you can build a website with Angular such as Java with Angular, NodeJS with Angular, NGINX serving Angular, etc. For the single-page applications, all you need to do is to load the initial index.html. Once you load the index.html the Angular framework kicks in and do the rest of the…
"
https://medium.com/bb-tutorials-and-thoughts/how-to-build-ci-cd-for-nodejs-azure-functions-using-azure-devops-4f32fb55a1a0?source=search_post---------306,"There are currently no responses for this story.
Be the first to respond.
An Azure Function is a simple way of running small pieces of code in the cloud. You don’t have to worry about the infrastructure required to host that code. You can write the Function in C#, Java, JavaScript, PowerShell, Python, or any of the languages that are listed in the…
"
https://medium.com/azure-na-pratica/10-servi%C3%A7os-do-azure-que-voc%C3%AA-precisa-conhecer-na-pr%C3%A1tica-conte%C3%BAdos-gratuitos-f6c18c3f3e68?source=search_post---------307,"There are currently no responses for this story.
Be the first to respond.
No dia 15/07/2020 (quarta-feira) aconteceu a primeira edição online do Azure Nights, um evento com foco em cloud computing e cobrindo a utilização na prática de 10 serviços que integram o Microsoft Azure.
Esta iniciativa foi uma parceria entre as comunidades TOTVS Developers, .NET SP, Azure na Prática, Azure Talks, DevOps Professionals e SampaDevs, contando com um excelente público: pico de 174 pessoas online durante a transmissão! Deixamos aqui nosso muito obrigado ao John Calistro (TOTVS Developers) e ao Jackson Feijó (Microsoft) pelo apoio para que realizássemos a live. E também agradecemos ao Fabricio Veronez (Coders in Rio, CNCF Speaker), que disponibilizou seu tempo com uma das apresentações.
Foram organizadores deste evento:
Tivemos ao longo da live as seguintes apresentações:
A gravação está disponível no YouTube e pode ser assistida gratuitamente:
Os slides já se encontram também no SlideShare:
Caso queira conhecer mais sobre o Microsoft Azure de forma gratuita acesse os posts a seguir, no qual estão links com as gravações de diversas palestras realizadas durante a edição 2020 do Azure Tech Nights (evento online promovido pelo Canal .NET):
Azure Tech Nights 2020: saiba como foi — Vídeos Gratuitos
E as gravações dos 3 primeiros minicursos gratuitos com foco em nuvem realizados pelo canal Azure na Prática:
Azure na Prática Gratuito #1 - Desenvolvimento Web: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #2 - Docker: saiba como foi + conteúdos gratuitos
Azure na Prática Gratuito #3 - Azure DevOps: saiba como foi + conteúdos gratuitos
A seguir estão também diversos artigos e projetos de exemplo abordando diferentes serviços do Azure (há vídeos sendo referenciados em alguns destes posts):
Docker — Guia de Referência Gratuito
Kubernetes — Guia de Referência Gratuito
GitHub Actions — Guia de Referência Gratuito
Azure DevOps — Guia de Referência Gratuito
Sobrevoando os serviços do Azure
ASP.NET Core: identificando a cidade e o país de origem de requisições com Application Insights
ASP.NET Core + Application Insights: monitorando o uso de Dapper, Entity Framework e NHibernate
.NET Core + Serverless: melhorando a experiência de Desenvolvimento com Azure Functions 3.x | pt 1
.NET Core + Serverless: melhorando a experiência de Desenvolvimento com Azure Functions 3.x | pt 2
Mensageria + .NET Core 3.1: exemplos com RabbitMQ, Kafka, Azure Service Bus e Azure Queue Storage
Serverless + Azure Functions: Guia de Referência
Serverless é muito mais que apenas APIs REST!
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
Microservices: alternativas para a implementação no Microsoft Azure
ASP.NET Core + Azure App Configuration: manipulando configurações de forma mais inteligente
Como o Microsoft Azure pode simplificar a publicação de suas Web Apps?- Dica Rápida
GitHub + Azure App Service: deployment automatizado e sem complicações de Web Apps na nuvem
Application Insights + Logic Apps + Aplicações Web: enviando alertas de monitoramento via Slack
ASP.NET Core + Entity Framework Core: monitoramento descomplicado via Application Insights
ASP.NET Core + Application Insights: monitorando a disponibilidade dos BDs de uma aplicação
Blog do Azure na Prática
163 
163 claps
163 
Blog do Azure na Prática
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
"
https://medium.com/@attores/creating-a-free-kovan-testnet-node-on-azure-step-by-step-guide-8f10127985e4?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Attores Pte Ltd
May 15, 2017·6 min read
Kovan is a new testnet for Ethereum using Parity’s Proof of Authority (PoA) consensus engine, with benefits over Ropsten:
Kovan, a public Proof-of-Authority testnet, uses parity to provide a stable, secure testnet environment for Ethereum developers, due to the instability of the existing Ropsten testnet.
PoA is a replacement for PoW (Proof of Work), which can be used for both public and private chain setups. There is no mining involved to secure the network with PoA, and relies on trusted ‘Validators’ to ensure that valid transactions are added to blocks, processed and executed by the EVM faithfully.
Because mining does not occur on Kovan, malicious actors are prevented from acquiring testnet Ether, solving the spam attack that Ropsten faces.
You can use the Azure automated installation of Parity, however, that will cost almost $200 a month in total- above the free monthly resource level. So to get it for free, you will need to set it up manually.
Let’s create a Kovan testnet node on Azure.
Jump to:
A. Create a Ubuntu VM on Azure
B. Configure DNS
C. Setting up Kovan Testnet Node
First, let’s create and set up new Ubuntu VM on Azure portal. If you don’t have a free Microsoft azure account, let’s create it. If you have an azure account, you can skip this section.
The application below will require a credit card and may be chargeable after the first month. Therefore, it’s recommended that you try to get a higher tier of usage using the Bizspark program or other developer signup pages such as:
https://azure.microsoft.com/en-us/offers/ms-azr-0064p/
Open Account on Azure:
2. Continue with account setup and provide the required info
3. Get your identity verified
4. Once it’s done, agree to the terms and continue. Once the account is created, go to Portal in the menu.
You’ll be directed to Azure portal and dashboard.
2. In search bar under New, search for ubuntu. You should see this:
Choose Ubuntu Server 16.04 LTS
3. In ’Select a deployment model’ → Choose ‘Resource Manager’ and then Create.
4. Fill in the info:
Type in your password and make a note of it. We’ll need it later. Rest of the info can be the same as above.
5. Choose the size of the machine: Choose F1S machine with 1 core and 2 GB of memory which works well.
6. Configure optional settings: No need to change anything here. Azure configures it.
7. Done. The summary now is shown and our VM is created.
8. Our Ubuntu VM is created. Now Azure will deploy it for you.
We need to change the firewall rules for the setup.
We need to add 3 rules:
Click on Add.
2. Search the marketplace for ‘Network Security Group’. Choose Network security group from the options.
3. Once it’s created, then go back to all resources again. Choose KovanTestnetSecurity. Under Settings → Inbound security rules.
4. There are no rules as of now. Click on ‘Add’.
Create first rule for SSL.
5. Create the similar rules for TCP listening port 30303 and UDP port 30301 (in advanced). Set the priorities as 100, 101, 102. Finally, add SSH to allow Any (default setting).
After adding all 4 rules, the Network Security Group — Inbound security rules should look like this:
6. We now need apply these rules to our Kovan network. In the same window of Network Security Group → go to Subnets.
Click on ‘Associate’.
Choose virtual network.
Choose the subnet.
Okay, good so far? Now let’s configure the DNS.
Use A name, pointing the subdomain to the IP address of the VM.
Go to your DNS manager on dashboard (The domain name you’ve bought from e.g. GoDaddy or Namecheap)
Create ‘A’ record. Point it to the IP address of the VM. Write the subdoamin you want in ‘Host’.
2. Install and start Parity:
a. It’ll ask to install parity → say ‘Yes’
b. It’ll ask to download and install netstats client → say ‘no’
3. Exit tmux with [ctrl + b], then d.
(not needed at the moment, just in case you want to watch the node running)
You are now running Parity on your Azure node!
— — -
For RPC Usage
— — -
Use the default /etc/nginx/nginx.conf
Edit /etc/nginx/sites-enabled/default
Start nginx
Get Kovan Testnet Ether (KETH)
That’s all. RPC node for Kovan is now set up on Azure Ubuntu VM.
Smart Contracts as a Service (SCaaS) platorm
81 
3
81 
81 
3
Smart Contracts as a Service (SCaaS) platorm
"
https://medium.com/hashmapinc/make-the-most-of-your-azure-data-factory-pipelines-68496da5257?source=search_post---------309,"There are currently no responses for this story.
Be the first to respond.
by Shekhar Parnerkar
Azure Data Factory (ADF) is one of the most powerful tools for building cloud data pipelines today. As with everything else, you need a well-thought-out approach in order to get the most from it. While approaches vary from project to project, some patterns remain consistent. This blog post provides some insights into best practices and potential loopholes to watch out for when planning to use ADF.
Best practices are generally common across all platforms. I will cover some of these practices that were implemented in our recent project using ADF:
ADF runs on a Spark cluster behind the scenes, and the integration is robust. You are unlikely to face any major issues unless your volumes or velocity are exceptional. However, ADF does have some limitations that must be accounted for in your design:
ADF has proven to be a robust framework for most of our data pipelining needs. It is easy to use and equally easy to deploy, which led to increased developer productivity.
Hopefully, what I’ve shared through my experience gives you some insights into best practices and potential loopholes to watch out for when planning to use ADF.
Be sure and checkout Hashmap’s Azure focus page at hashmapinc.com and reach out if you’d like additional assistance in this area. Hashmap offers a range of enablement workshops and consulting service packages as part of our consulting service offerings, and would be glad to work through your specifics in this area.
Hashmap’s Data Integration Workshop is an interactive, two hour experience for you and your team where we will provide you with a high value, vendor-neutral sounding board to help you accelerate your data integration decision-making process, and selection. Based on our experience, we’ll talk through best fit options for both on premise and cloud-based data sources and approaches to address a wide range of requirements. Sign up today for our complimentary workshop.
www.hashmapinc.com
To listen in on a casual conversation about all things data engineering and the cloud, check out Hashmap’s podcast Hashmap on Tap as well on Spotify, Apple, Google, and other popular streaming apps.
A rotating cast of Hashmap hosts and special guests explore technologies from diverse perspectives while enjoying a drink of choice. www.hashmapinc.com
www.hashmapinc.com
www.hashmapinc.com
medium.com
medium.com
www.hashmapinc.com
Shekhar Parnerkar is a Solution Architect and Delivery Manager at Hashmap, Pune. He specializes in building modern cloud data warehouses for Hashmap’s global clients.
Innovative technologists and domain experts helping…
23 
23 claps
23 
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Written by
Innovative technologists and domain experts accelerating the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our customers http://hashmapinc.com
Innovative technologists and domain experts helping accelerate the value of Data, Cloud, IIoT/IoT, and AI/ML for the community and our clients by creating smart, flexible and high-value solutions and service offerings that work across industries. http://hashmapinc.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/selenium-webdriver-azure-devops-automatizando-o-teste-de-aplica%C3%A7%C3%B5es-web-tdc-2019-porto-2b2e198845d0?source=search_post---------310,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Dec 7, 2019·2 min read
No dia 27/11/2019 (quarta) eu e meu amigo Milton Camara Gomes (Microsoft MVP) participamos como palestrantes na Trilha Testes no TDC Porto Alegre, realizando uma apresentação focada no teste automatizado de aplicações Web utilizando Selenium WebDriver e o Azure DevOps.
Essa edição do The Developers’s Conference aconteceu na UniRitter em Porto Alegre-RS ao longo dos dias 27 a 30/11 (quarta a sábado), contando com diversas trilhas simultâneas e participantes do Brasil inteiro.
Gostaria de deixar neste post meu muito obrigado à Ariane Izac, à Fabrice Nunes e à Joyce Bastos que coordenaram a Trilha Testes pela oportunidade em participar como palestrante nesta edição TDC.
Os slides que utilizamos já estão no SlideShare:
A seguir temos um exemplo de implementação de testes com Selenium WebDriver utilizando .NET Core 3.0, Chrome Driver e xUnit:
.NET Core 3.0 + xUnit + Selenium WebDriver + .NET Standard + Chrome Driver
Diversos conteúdos sobre a implementação de testes (incluindo o uso de Selenium WebDriver) empregando Azure DevOps, .NET Core e o Visual Studio 2019 podem ser encontrados no seguinte artigo:
Testes de Software com .NET Core e o Visual Studio 2019
A implementação de testes com .NET Core e o Visual Studio 2019 foi também assunto de uma live recente do Canal .NET, em que apresentei exemplos de utilização dos frameworks xUnit, NUnit, MS Test, Moq, NSubstitute, Fluent Assertions e Selenium Web Driver. Nesta mesma live meu amigo Milton Câmara (Microsoft MVP) demonstrou a execução automatizada de testes a partir do Azure DevOps.
A gravação da apresentação pode ser assistida gratuitamente no YouTube:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
28 
28 
28 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://itnext.io/tutorial-using-azure-event-hubs-with-the-dapr-framework-81c749b66dcf?source=search_post---------311,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This blog uses an example to walk you through how to use Azure Event Hubs integration with Dapr, a set of distributed system building blocks for microservices development.
Azure Event Hubs will be used as a “binding” within the Dapr runtime. This will allow services to communicate with Azure Event Hubs without actually knowing about it or being coupled to it directly (via SDK, library etc.), using a simple model defined by the Dapr runtime.
Sounds too good to be true? 😉 Read on…
You will:
Azure Event Hubs is a fully managed Platform-as-a-Service (PaaS) for streaming and event ingestion. It also provides Apache Kafka support enabling clients and applications to talk to Event Hubs without need to set up, configure, and manage your own Kafka clusters!
Dapr stands for Distributed Application Runtime. You can get all the scoop in this announcement blog, but here is a (buzzword compliant!) gist to get you started.
It is an open source, portable runtime to help developers build resilient, microservice stateless and stateful applications. It does so by codifying the best practices for building such applications into independent components. The capabilities exposed by the runtime components can be accessed over HTTP or gRPC, making it completely agnostic and allows it to work with any language and/or framework.
Dapr adopts a sidecar architecture and runs either as a separate container or as a process. This means that the application itself does not need to include Dapr runtime as a dependency. This allows for easy integration as well as providing separation of concerns.
Dapr also includes language specific SDKs for Go, Java, JavaScript, .NET and Python. These SDKs expose the functionality in the Dapr building blocks, such as saving state, publishing an event or creating an actor, through a typed, language API rather than calling the http/gRPCAPI.
Dapr is platform agnostic. You can run your applications locally, on any Kubernetes cluster and other hosting environments that Dapr integrates with.
At the time of writing, Dapr is in alpha state and supports the following distributed systems building blocks which you can plug into your applications - Service invocation, State Management, Pub/Sub messaging, Resource Bindings, Distributed tracing and Actor pattern
For a comprehensive overview, please refer to the Dapr documentation
As mentioned earlier, the example application in this post makes use of the Dapr bindings for Azure Event Hubs. Here is a peek at what “bindings” are.
Bindings provide a common way to trigger an application with events from external systems or invoke an external system with optional data payloads. These “external systems” could be anything: a queue, messaging pipeline, cloud-service, filesystem, etc.
Currently supported bindings include Kafka, Rabbit MQ, Azure Event Hubs etc.
In a nutshell, Dapr bindings allow you to focus on business logic rather than integrating with individual services such as databases, pub/sub systems, blob storage etc.
Alright, enough theory!
You can run Dapr anywhere, including Kubernetes, thanks to its first class Operator based support. Since this is an introductory blog post, let's focus on the end to end flow, keep things simple and run Dapr locally as a standalone component.
If you’re itching to run Dapr on Kubernetes, check out this getting started guide!
For the sample app, you will need:
Start by installing the Dapr CLI which allows you to setup Dapr on your local dev machine or on a Kubernetes cluster, provides debugging support, launches and manages Dapr instances.
For e.g. on your Mac, you can simply use this to install Dapr to /usr/local/bin
Refer to the docs for details
You can use the CLI to install Dapr in standalone mode. All you need is a single command
.. and that’s it!
If you don’t already have a Microsoft Azure account, go ahead and sign up for a free one! Once you’re done you can quickly set up Azure Event Hubs using either of the following quickstarts:
You should now have an Event Hub instance with a namespace and associated Event Hub (topic). As a final step you need to get the connection string in order to authenticate to Event Hubs — use this guide to finish this step.
An Input Binding in Dapr represents an event resource that Dapr uses to read events from and push to your application. Let's run an app that uses this to receive data from Azure Event Hubs.
Start by cloning the repo and change into the correct directory
Update components/eventhubs_binding.yaml to include Azure Event Hubs connection string in the spec.metadata.value section. Please note that you will have to append the name of the Event Hub to end the connection string i.e. ;EntityPath=<EVENT_HUBS_NAME>. This is what the value for connectionString attribute should look like:
Start the Go app which uses the Azure Event Hubs Input Bindings
You should see the logs
This app uses the Azure Event Hubs native Go client to send messages.
Set the required environment variables:
Please ensure that the name of the Event Hub is the same as what you configured for the connection string in the input binding configuration
Run the producer app — it will send five messages to Event Hubs and exit
Check Dapr application logs, you should see the messages received from Event Hubs.
Here is a summary of how it works:
Input Binding
The eventhub_binding.yaml config file captures the connection string for Azure Event Hubs.
The key attributes are:
Notice that the connection string contains the information for the broker URL (<EVENT_HUBS_NAMESPACE>.servicebus.windows.net), primary key (for authentication) and also the name of the topic or Event Hub to which your app will be bound and receive events from.
Using the binding in the app
The Go app exposes a REST endpoint at /eventhubs-input - this is the same as the name of the Input Binding component (not a coincidence!)
Dapr runtime does the heavy lifting of consuming from Event Hubs and making sure that it invokes the Go application with a POST request at the /eventhubs-input endpoint with the event payload. The app logic is then executed, which in this case is simply logging to standard output.
An output binding represents a resource that Dapr will use invoke and send messages to. Let's use an output binding to send data to Event Hubs.
Change to the correct directory
Update components/eventhubs_binding.yaml to include Azure Event Hubs connection string in the spec.metadata.value section. Please note that you will have to append the name of the Event Hub to end the connection string i.e. ;EntityPath=<EVENT_HUBS_NAME>. This is what the value for connectionString attribute should look like:
Start the Go app which uses the Azure Event Hubs Output Bindings. It will send five messages to the Dapr binding HTTP endpoint in quick succession and exit.
In the app, you will see logs:
This means that the messages were sent to Azure Event Hub via the Dapr output binding
In the receiver (input bindings) app, you should see
To keep things simple, we used the same Event Hub topic for input and output binding example. In reality these can be used independently i.e. an application which needs to be triggered by Event Hubs in an event-driven manner can subscribe to a topic (via Dapr) and another app which needs to push data to Event Hubs (to a different topic) can use the Dapr binding HTTP endpoint to POSTevent data.
The output binding configuration remains the same as Input binding.
The Go app sends a message to Event Hubs via the output binding. It does so by sending a POST request to the Dapr HTTP endpoint http://localhost:<dapr_port>/v1.0/bindings/<output_binding_name>.
In this example, the name of the output binding is eventhubs-output and the Dapr HTTP port is 3500, the endpoint URL is:
As was the case with Input Binding, the Dapr runtime takes care of sending the event to Event Hubs. Since we have the Input binding app running, it receives the payload and it shows up in the logs.
In this blog post, you saw how to use Dapr ""bindings"" to connect integrate your applications via Azure Event Hubs without having to know about it! Instead, you connected through the Dapr runtime (just a sidecar) using the Dapr HTTP API.
It is also possible to do it using gRPC or language specific SDKs (as previously mentioned)
As the time of writing, Dapr is in alpha state (v0.1.0) and gladly accepting community contributions 😃 Vist https://github.com/dapr/dapr to dive in!
If you found this article helpful, please like and follow 🙌 Happy to get feedback via Twitter or just drop a comment.
ITNEXT is a platform for IT developers & software engineers…
140 
140 claps
140 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/participate-in-applied-f-challenge-2019-6100545b5f09?source=search_post---------312,"There are currently no responses for this story.
Be the first to respond.
F# is an open-source, functional, general-purpose programming language that originates from Microsoft Research and is used by many engineers across the globe. F# is actively developed by the open-source community and Microsoft’s developer division.
One of the great examples showing F# strengths is Azure Storage Type Provider. It helps ensure strongly-typed access to Azure storage blobs, tables, and queues with automatic generation of the schema, and makes programming more efficient and less error-prone.
F# can be used with many Azure services, including App Service, Cosmos DB, Azure Functions, Azure Storage, and more. For example, there is a great developer reference for that can help you get more insight into using F# for Azure Functions. The F# on Azure guide has many references and examples to help you get started. If you’d like to try F# on Linux, MacOs, or Windows, take a look at the useful quickstart.
F# Software Foundation has recently announced their new initiative — Applied F# Challenge! We encourage you to participate and send your submissions about F# on Azure through the participation form.
Applied F# Challenge is a new initiative to encourage in-depth educational submissions to reveal more of the interesting, unique, and advanced applications of F#.
The motivation for the challenge is uncovering more of advanced and innovative scenarios and applications of F# we hear about less often:
We primarily hear about people using F# for web development, analytical programming, and scripting. While those are perfect use cases for F#, there are many more brilliant and less covered scenarios where F# has demonstrated its strength. For example, F# is used in quantum computing, cancer research, bioinformatics, IoT, and other domains that are not typically mentioned as often.
You have some time to think about the topic for your submission because the challenge is open from February 1 to May 20 this year.
Publish a new article or an example code project that covers a use case of a scenario where you feel the use of F# to be essential or unique. The full eligibility criteria and frequently asked questions are listed in the official announcement.
There are multiple challenge categories you can choose to write about:
F# for machine learning and data science.F# for distributed systems.F# in the cloud: web, serverless, containers, etc.F# for desktop and mobile development.F# in your organization or domain: healthcare, finance, games, retail, etc.F# and open-source development.F# for IoT or hardware programming.F# in research: quantum, bioinformatics, security, etc.Out of the box F# topics, scenarios, applications, or examples.
All submissions will receive F# stickers as a participation reward for contributing to the efforts of improving the F# ecosystem and raising awareness of F# strengths in advanced or uncovered use cases.
Participants with winning submissions in each category will also receive the title of a Recognized F# Expert by F# Software Foundation and a special non-monetary prize.
Each challenge category will be judged by the committees that include many notable F# experts and community leaders, including Don Syme, Rachel Blasucci, Evelina Gabasova, Henrik Feldt, Tomas Perticek, and many more.
As the participation form suggests, you will also have an opportunity to be included in a recommended speaker list by F# Software Foundation.
Help us spread the word about the Applied F# Challenge by encouraging others to participate with #AppliedFSharpChallenge hashtag on Twitter!
Any language.
119 
119 claps
119 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Solution Architecture. Distributed systems, big data, data analysis, resilient and operationally excellent systems.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/azure-na-pratica/net-core-serverless-melhorando-a-experi%C3%AAncia-de-desenvolvimento-com-azure-functions-3-x-pt-1-caf4090180a3?source=search_post---------313,"There are currently no responses for this story.
Be the first to respond.
Recentemente participei de 2 lives no Canal .NET com meu amigo Milton Câmara e que focaram na implementação de soluções serverless com .NET Core + Azure Functions 3.x. Nosso objetivo com estes eventos foi demonstrar que é possível aproximar a experiência de desenvolvimento com Azure Functions daquilo a que muitos Desenvolvedores estão habituados em ASP.NET Core. Para isto abordamos:
Neste novo artigo (primeiro post de uma série de 3 artigos) trago as gravações de cada uma destas lives (que podem ser assistidas gratuitamente), alguns dos exemplos utilizados, bem como dicas/orientações sobre mensageria e injeção de dependências com Azure Functions.
E aproveito este espaço para um convite… Que tal participar do próximo treinamento online promovido pelo Azure na Prática e que acontecerá durante o dia 29/08/2020 (sábado), tendo como foco Serverless + Azure Functions e que engloba ainda o uso de tecnologias como Azure Logic Apps, RabbitMQ, Apache Kafka, SQL Server, MongoDB, Redis, Application Insights, Azure Cosmos DB e GitHub Actions? Acesse então o link a seguir para efetuar sua inscrição com o desconto especial de pré-venda (apenas R$ 200,00):
https://bit.ly/anp-serverless3-blog-groffe-pre
Atualmente podemos implementar Functions para consumir mensagens utilizando:
Além do preço reduzido para se hospedar uma Function App no Microsoft Azure, a utilização de Azure Functions simplifica em muito a implementação de código consumindo as mensagens de uma fila. A seguir temos um exemplo disto com Azure Queue Storage:
Já o próximo exemplo apresenta o uso de uma fila baseada no Azure Service Bus:
Temos ainda a possibilidade de utilizar RabbitMQ com Azure Functions (suporte ainda em Preview), como indicado no exemplo a seguir:
Já abordei inclusive em detalhes o uso de RabbitMQ + Azure Functions no seguinte artigo:
Mensageria na nuvem com RabbitMQ, .NET Core e Azure Functions
Tópicos tornam possível o envio de mensagens a múltiplas aplicações. No caso de uma Function vinculada ao Azure Service Bus e outra aplicação qualquer, as mesmas apontarão para um mesmo tópico (topic-servicebus no exemplo) e estarão registradas com diferentes Subscriptions cada uma (groffe0 para esta Function específica):
O suporte a tópicos do Apache Kafka ainda está em Preview para as Azure Functions no momento da publicação deste artigo (final de Maio/2020). O exemplo a seguir vincula uma Function a um tópico chamado topic-kafka; já o topic-kafka-group0 indica o grupo ao qual a aplicação correspondente está associada (funcionando de maneira similiar às Subscriptions do Azure Service Bus):
Maiores informações sobre o uso de Kafka com .NET Core podem ser encontradas no link a seguir:
.NET + Apache Kafka: Guia de Referência
Os diferentes exemplos apresentados nesta seção foram agrupados em 2 repositórios do GitHub:
.NET Core + Azure Functions 3.x + Queue Storage + RabbitMQ + Azure Service Bus (Queue)
.NET Core + Azure Functions 3.x + Azure Service Bus + Apache Kafka + Topics
Por fim, deixo aqui um projeto que criei para testes simples de envio de mensagens envolvendo as tecnologias aqui mencionadas:
.NET Core 3.1 + Console Application + Envio de Mensagens + RabbitMQ + Azure Service Bus (Queues e Topics) + Apache Kafka
Essencial para o uso de ORMs (Entity Framework Core, FluentNHibernate) ou visando um melhor tratamento de falhas por meio da biblioteca Polly, já abordei como o mecanismo de injeção de dependências do ASP.NET Core também pode ser ativado em um projeto baseado em Azure Functions no artigo:
.NET Core + Serverless: utilizando injeção de dependências com Azure Functions
Tais ajustes passam pela inclusão de uma extensão própria para isto em uma Function App:
E pela implementação de uma classe Startup baseada no tipo FunctionsStartup (namespace Microsoft.Azure.Functions.Extensions.DependencyInjection):
No caso específico de uma Function, a implementação dessa estrutura deverá ser alterada para o uso de uma classe contendo um construtor e um método não-estáticos:
O código-fonte do projeto detalhado nesta seção já está disponível no GitHub:
.NET Core + Azure Functions 3.x + Injeção de Dependências
Serverless + Azure Functions: Guia de Referência
.NET Core 3.x + Serverless: configuração, dicas e exemplos com Azure Functions 3.x
Blog do Azure na Prática
27 
27 claps
27 
Blog do Azure na Prática
Written by
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
Blog do Azure na Prática
"
https://burkeknowswords.com/new-react-azure-cosmos-db-videos-are-here-bb75f29c28af?source=search_post---------314,NA
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-6bb8a0d04a4e?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 5, 2020·1 min read
2-Parte: Configurando o ambiente de banco de dados com Docker
Dando continuidade a liberação dos módulos do meu curso: Criando API’s RESTful utilizando TypeScript, Node e mongoDB, hoje eu irei demonstrar como subir um ambiente Docker com o MongoDB e como acessar essa base de dados.
Caso você seja iniciante em Docker, eu recomendo a leitura do seguinte artigo: Comando básicos docker.
Artigo contendo um passo a passo de como configurar um ambiente MongoDB com Docker: Docker: Criando servidor MongoDB
Configurando ambiente de banco de dados MongoDB com Docker
Acessando a base de dados mongoDB
Link para download do Robo 3T: Download
Espero que gostem e qualquer dúvida podem postar aqui ou no vídeo do Youtube.
Enjoy your life
69 
69 
69 
Aqui você vai encontrar os principais conteúdos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/xp-inc/azure-devops-angular-github-pages-6fe2ae45b39b?source=search_post---------316,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Oct 27, 2019·3 min read
Veja nesse artigo como criar uma pipeline no Azure DevOps para publicar o seu projeto Angular no GitHub Pages
Dando continuidade ao meu artigo anterior: Angular 8.3: publicando projeto com o Angular CLI em 5 passos, hoje eu irei demonstrar como criar uma pipeline no Azure DevOps para automatizar o processo de deploy demonstrado no artigo anterior.
Para os próximos passos sera necessário ter uma conta no Azure DevOps Service. Caso você ainda não tenha se cadastrado la, acesse o seguinte link para acessar o portal e criar uma conta: Azure DevOps.
Com o passo da conta OK, vamos a criação de uma nova pipeline. Para isso, siga os passos abaixo:
1- Clique em pipelines:
2 -Em seguida no canto superior direito, clique no botão New pipeline:
3- Selecione o repositório do seu código.
Para esse artigo, eu irei utilizar um repositório do meu GitHub.
4- Selecione a pipeline do Node.js:
Esse passo deve criar uma pipeline básica para projetos Node.js.
Caso seja necessário adicionar um novo step na sua pipeline, você pode utilizar o assistente no canto direito demonstrado na imagem abaixo:
Agora para ajustar pipeline para publicar o nosso projeto, altere o comando npm run build para npm run deploy.
Obs.: Esse comando e com base no meu arquivo package.json, nele eu adicionei o comando ng deploy da nova versão do Angular 8.3
Clique em Save and Run, de um nome para o seu commit e clique em Save and Run novamente.
Assim que o Job finalizar, acesse a sua página no GitHub pages e verifique as alterações que você subiu nessa nova release do seu projeto.
Link do meu projeto publicado em uma pagina do GitHub: Angular 8.3 DevOps.
Bom, a ideia desse artigo era demonstrar como automatizar o processo de deploy de um projeto angular, utilizando um arquivo .yml no Azure DevOps Service.
Espero que tenham gostado e até um próximo artigo pessoal ;)
Enjoy your life
See all (155)
74 
74 claps
74 
Aqui você vai encontrar os principais conteúdos de tecnologia, design, dados e produto da XP Inc.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/azure-tech-nights-2018-saiba-como-foi-cada-uma-das-apresenta%C3%A7%C3%B5es-5fa5ecd750f8?source=search_post---------317,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 3, 2018·3 min read
Entre os dias 20 e 28 de Agosto de 2018 aconteceu a terceira edição do Azure Tech Nights, um evento online, gratuito e noturno do Canal .NET focado no uso de tecnologias e serviços que integram o Microsoft Azure.
Seguem alguns dados sobre o Azure Tech Nights 2018:
Pensando naqueles que não puderam acompanhar ou, até mesmo, gostariam de rever alguma apresentação, foram agrupados neste post os links (indicados abaixo) de todas as palestras realizadas ao longo dos 5 dias de evento.
Dia 1 - 20/08/2018
Palestra 1 - Conheça o AKS, o serviço de Kubernetes do Azure - Giovanni Bassi (Microsoft MVP)
Palestra 2 - Conhecendo as APIs do Azure Cosmos DB - Dani Monteiro (Microsoft MVP)
Palestra 3 - Implementando APIs seguras na nuvem - Renato Groffe (Microsoft MVP, MTAC)
Dia 2 - 21/08/2018
Palestra 1 - Cognitive Search nos seus Aplicativos - Thiago Custódio (Microsoft MVP)
Palestra 2 - Automatizando a entrega das suas Aplicações ASP.NET Core com Docker, VSTS e Azure - Milton Câmara Gomes
Palestra 3 - Azure SignalR + Functions + Logic Apps: um exemplo prático - Ericson da Fonseca (Microsoft MVP) e Robson Araújo (Campinas .NET)
Dia 3 - 22/08/2018
Palestra 1 - Monitorando uma Aplicação ASP.NET com Application Insights e Power BI — Rafael Cruz (Microsoft MVP)
Palestra 2 - Boas Práticas com o Microsoft Azure — Jaqueline Ramos (Microsoft MVP)
Palestra 3 - Inteligência artificial para desenvolvedores .NET — Angelo Belchior (Microsoft MVP)
Dia 4 - 27/08/2018
Palestra 1 - Crie, implemente e escale aplicações com o Azure App Service - Lucas Romão (Microsoft MVP)
Palestra 2 - Criando seu primeiro experimento no Azure Machine Learning Studio - Luis Felipe
Palestra 3 - Conhecendo o Azure CDN - Mateus Rodrigues
Dia 5 - 28/08/2018
Palestra 1 - Estratégias de Disaster Recovery para APIs - Rafael dos Santos (Microsoft MVP)
Palestra 2 - Introdução ao Azure IoT Edge - André Secco (Microsoft MVP)
Palestra 3 - Introdução ao Azure Maps - Joel Rodrigues (Microsoft MVP)
E por falar em cloud computing, ao longo de todo o dia 20/09/2018 (um sábado) acontecerá o Azure Conference nos auditórios da Microsoft em São Paulo Capital.
Maior evento sobre nuvem Microsoft da América Latina, o Azure Conference contará com 3 trilhas: Dev, Data & AI e Infra. Os ingressos já estão a venda (por um preço bem acessível), garanta sua participação acessando o site oficial do evento:
http://www.azureconference.com.br/
E para finalizar, ainda não segue o Canal .NET nas redes sociais? Faça sua inscrição então, para ficar por dentro de novidades sobre eventos, tecnologias Microsoft e outros conteúdos gratuitos:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
73 
73 
73 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/minha-participa%C3%A7%C3%A3o-no-mvpconf-2018-azure-cosmos-db-tecnologias-relacionais-e-nosql-no-azure-34bfa0c1f9ac?source=search_post---------318,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Apr 30, 2018·3 min read
Nos dias 6 e 7 de Abril/2018 (sexta e sábado) aconteceu a primeira edição do MVPConf, o maior evento técnico brasileiro de experts Microsoft.
Criado e organizado pela comunidade de MVPs Microsoft no Brasil, o MVPConf teve caráter beneficente (todo o valor arrecadado foi repassado a instituições como a APAE-SP) e contou com mais de mil pessoas inscritas de todo o país.
Tive a honra de poder atuar como palestrante nos 2 dias de evento, realizando as seguintes apresentações:
Gostaria de deixar meu agradecimento neste post à comissão organizadora do evento (em especial ao Heber Lopes, com quem tive mais contato durante a preparação para o MVPConf), além de todos aqueles que participaram e acompanharam as minhas apresentações.
Deixo registrado aqui também meu muito obrigado ao Rodrigo Wanderley de Melo Cardoso (iMasters) e à Ticiani Aguiar (Grupo Bandeirantes) pelo apoio na divulgação do evento através das seguintes matérias:
Evento reúne maiores experts Microsoft em São Paulo - Band.com.br
Microsoft MVP Conference acontece nos dias 06 e 07 de abril, em São Paulo - iMasters
A seguir estão os materiais utilizados em cada uma das palestras que realizei, além de fotos tiradas durante estas apresentações.
Slides:
Fotos (deixo aqui meus agradecimentos ao MVP André Secco e ao Ericson Fonseca pela colaboração durante esta palestra):
Slides:
Exemplos utilizados:
Fotos (deixo aqui meus agradecimentos ao Edson Vieira, ao MVP Erick Wendel, e ao MTAC Alexandre Malavasi pela colaboração durante esta palestra):
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
16 
1
16 claps
16 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jaychapel/aws-vs-azure-vs-google-free-tier-comparison-19b68578e7f?source=search_post---------319,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 20, 2020·6 min read
Whether you’re new to public cloud altogether or already use one provider and are interested in trying another, you may be interested in a comparison of the AWS vs Azure vs Google free tier. The big three cloud providers — AWS, Azure and Google Cloud — each have a free tier available that’s designed to give users the cloud experience without all the costs. They include free trial versions of numerous services so users can test out different products and learn how they work before they make a huge commitment. While they may only cover a small environment, it’s a good way to learn more about each cloud provider. For all of the cloud providers, the 12-month free trials are available to only new users.
AWS free tier includes more than 60 products. There are two different types of free options that are available depending on the product used: always free and 12 months free. To help customers get started on AWS, the services that fall under the free 12-months are for new trial customers and give customers the ability to use the products for free (up to a specific level of usage) for one year from the date the account was created. Keep in mind that once the free 12 months are up, your services will start to be charged at the normal rate. Be prepared and review this checklist of things to do when you outgrow the AWS free tier.
The Azure equivalent of a free tier is referred to as a free account. As a new user in Azure, you’re given a $200 credit that has to be used in the first 30 days after activating your account. When you’ve used up the credit or 30 days have expired, you’ll have to upgrade to a paid account if you wish to continue using certain products. Ensure that you have a plan to reduce Azure costs in place. If you don’t need the paid products, there’s also the always free option.
Some of the ways people choose to use their free account are to gain insights from their data, test and deploy enterprise apps, create custom mobile experiences and more.
The Google Cloud Free Tier is essentially an extended free trial that gives you access to free cloud resources so you can learn about Google Cloud services by trying them on your own.
The Google Cloud Free Tier has two parts — a 12-month free trial with a $300 credit to use with any Google Cloud services and always free, which provides limited access to many common Google Cloud resources, free of charge. Google Cloud gives you a little more time with your credit than Azure, you get the full 12 months of the free trial to use your credit. Unlike free trials from the other cloud providers, Google does not automatically charge you once the trial ends — this way you’re guaranteed that the free tier is actually 100% free. Keep in mind that your trial ends after 12 months or once you’ve exhausted the $300 credit. Any usage beyond the free monthly usage limits are covered by the $300 free credit — you must upgrade to a paid account to continue using Google Cloud.
It’s important to note that the always-free services vary widely between the cloud providers and there are usage limitations. Keep in mind the cloud providers’ motivations: they want you to get attached to the services so you start paying for them. So, be aware of the limits before you spin up any resources, and don’t be surprised by any charges.
In AWS, when your free tier expires or if your application use exceeds the free tier limits, you pay standard, pay-as-you-go service rates. Azure and Google both offer credits for new users that start a free trial, which are a handy way to set a spending limit. However, costs can get a little tricky if you aren’t paying attention. Once the credits have been used you’ll have to upgrade your account if you wish to continue using the products. Essentially, the credit that was acting as a spending limit is automatically removed so whatever you use beyond the free amounts, you will now have to pay for. In Google Cloud, there is a cap on the number of virtual CPUs you can use at once — and you can’t add GPUs or use Windows Server instances.
For 12 months after you upgrade your account, certain amounts of popular products are free. After 12 months, unless decommissioned, any products you may be using will continue to run, and you’ll be billed at the standard pay-as-you-go rates.
Another limitation is that commercial software and operating system licenses typically aren’t available under the free tiers.
These offerings are “use it or lose it” — if you don’t use all your credits or utilize all your usage, there will be no rollover into future months.
AWS has 33 products that fall under the one-year free tier — here are some of the most popular:
For the always-free option, you’ll find a number of products as well, some of these include:
Azure has 19 products that are free each month for 12 months — here are some of the most popular:
For their always free offerings, you’ll find even more popular products — here are a few:
Unlike AWS and Azure, Google Cloud does not have a 12 months free offerings. However, Google Cloud does still have a free tier with a wide range of always free services — some of the most popular ones include:
Check out these blog posts on free credits for each cloud provider to see how you can start saving:
Originally published at www.parkmycloud.com on July 15, 2020
CEO of ParkMyCloud
41 
41 
41 
CEO of ParkMyCloud
"
https://medium.com/velotio-perspectives/continuous-deployment-with-azure-kubernetes-service-azure-container-registry-jenkins-ca337940151b?source=search_post---------320,"There are currently no responses for this story.
Be the first to respond.
Containerization has taken the application development world by storm. Kubernetes has become the standard way of deploying new containerized distributed applications used by the largest enterprises in a wide range of industries for mission-critical tasks, it has become one of the biggest open-source success stories.
Although Google Cloud has been providing Kubernetes as a service since November 2014 (Note it started with a beta project), Microsoft with AKS (Azure Kubernetes Service) and Amazon with EKS (Elastic Kubernetes Service) have jumped on to the scene in the second half of 2017.
AWS had KOPS https://kubernetes.io/docs/getting-started-guides/kops/
Azure had Azure Container Service https://docs.microsoft.com/en-us/azure/container-service/kubernetes/container-service-kubernetes-walkthrough
However, they were wrapper tools available prior to these services which would help a user create a Kubernetes cluster, but the management and the maintenance (like monitoring and upgrades) needed efforts.
With container demand growing, there is always a need in the market for storing and protecting the container images. Microsoft provides a Geo Replica featured private repository as a service named Azure Container Registry.
Azure Container Registry is a private registry for hosting container images. It integrates well with orchestrators like Azure Container Service, including Docker Swarm, DC/OS, and the new Azure Kubernetes service. Moreover, ACR provides capabilities such as Azure Active Directory-based authentication, webhook support, and delete operations.
The coolest feature provided is Geo-Replication. This will create multiple copies of your image and distribute it across the globe and the container when spawned will have access to the image which is nearest.
Although Microsoft has good documentation on how to set up ACR in your Azure Subscription, we did encounter some issues and hence decided to write a blog on the precautions and steps required to configure the Registry in the correct manner.
Note: We tried this using a free trial account. You can setup it up by referring the following link: https://azure.microsoft.com/en-in/offers/ms-azr-0044p/
Note: If version 2.0.24 is used it has the following bugs:- https://stackoverflow.com/questions/48201850/login-to-the-azure-container-ser vice-fails-with-error-bool-object-has-no-attri/48204384#48204384- https://github.com/Azure/azure-cli/issues/5300
Steps to install Azure CLI 2.0.23 or 2.0.25 (ubuntu 16.04 workstation):
Steps for Container Registry Setup:
Login to your Azure Account:
Note: SKU defines the storage available for the registry for type Basic the storage available is 10GB, 1 WebHook and the billing amount is 11 Rs/day.
For detailed information on the different SKU available, click here.
Push the image to the Azure Container Registry:
Example:
Microsoft does provide a GUI option to create the ACR.
Example :
Microsoft released the public preview of Managed Kubernetes for Azure Container Service (AKS) on October 24, 2017. This service simplifies the deployment, management, and operations of Kubernetes. It features an Azure-hosted control plane, automated upgrades, self-healing, easy scaling.
Similarly to Google AKE and Amazon EKS, this new service will allow access to the nodes only and the master will be managed by Cloud Provider. For more information, you can read this.
Let’s now get our hands dirty and deploy an AKS infrastructure to play with:
Make sure you create a resource group under the following regions.
Example with different arguments :
Create a Kubernetes cluster with a specific version.
Create a Kubernetes cluster with a larger node pool.
To connect to the Kubernetes cluster from the client computer Kubectl command line client is required.
Note: If you’re using Azure CloudShell, kubectl is already installed. If you want to install it locally, run the above command:
Example :
We had encountered a few issues while setting up the AKS cluster at the time of writing this blog. Listing them along with the workaround/fix:
Error: Operation failed with status: ‘Bad Request’.
Details: Required resource provider registrations Microsoft.Compute, Microsoft.Storage, Microsoft.Network are missing.
Fix: If you are using the trial account, click on subscriptions and check whether the following providers are registered or not :
Error: We had encountered the following mentioned open issues at the time of writing this blog.
https://github.com/Azure/AKS/issues/144
https://github.com/Azure/AKS/issues/141
https://github.com/Azure/AKS/issues/145
Microsoft provides a solution template which will install the latest stable Jenkins version on a Linux (Ubuntu 14.04 LTS) VM along with tools and plugins configured to work with Azure. This includes:
Refer the below link to bring up the Instance: https://docs.microsoft.com/en-us/azure/jenkins/install-jenkins-solution-template
What the pipeline accomplishes :
Stage 1:
The code gets pushed in the Github. The Jenkins job gets triggered automatically. The Dockerfile is checked out from Github.
Stage 2:
Docker builds an image from the Dockerfile and then the image is tagged with the build number. Additionally, the latest tag is also attached to the image for the containers to use.
Stage 3:
We have default deployment and service YAML files stored on the Jenkins server. Jenkins makes a copy of the default YAML files, make the necessary changes according to the build and put them in a separate folder.
Stage 4:
kubectl was initially configured at the time of setting up AKS on the Jenkins server. The YAML files are fed to the kubectl util which in turn creates pods and services.
Sample Jenkins pipeline code :
*****************************************************************
This post was originally published on Velotio Blog.
Velotio Technologies is an outsourced software product development partner for technology startups and enterprises. We specialize in enterprise B2B and SaaS product development with a focus on artificial intelligence and machine learning, DevOps, and test engineering. We combine innovative ideas with business expertise and cutting-edge technology to drive business success for our customers.
Interested in learning more about us? We would love to connect with you on ourWebsite, LinkedIn or Twitter.
*****************************************************************
Thoughts and ideas on startups, enterprise software &…
20 
20 claps
20 
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
Written by
Velotio Technologies is an outsourced software and product development partner for technology startups & enterprises. #Cloud #DevOps #ML #UI #DataEngineering
Thoughts and ideas on startups, enterprise software & technology by the Velotio team. Learn more at www.velotio.com.
"
https://medium.com/serverlessguru/companies-moving-from-aws-and-azure-to-google-cloud-platform-55fe74f54bd2?source=search_post---------321,"There are currently no responses for this story.
Be the first to respond.
Amazon Web Services (AWS) is the oldest and most mature provider of cloud computing, offering the most services. However, Google Cloud Platform (GCP) and Microsoft Azure are innovating at a quick pace, and all three of these industry leaders have their pros and cons.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-increase-azure-databricks-cluster-vcpu-cores-limits-37beb07f3905?source=search_post---------322,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Mar 2, 2020·5 min read
It is quite common to be asked by my customers:
Why my Azure Databricks cluster is limited?
About
Write
Help
Legal
Get the Medium app
"
https://blog.jeremylikness.com/create-a-serverless-angular-app-with-azure-functions-and-blob-storage-20164c083c88?source=search_post---------323,
https://medium.com/javarevisited/7-best-az-204-online-courses-for-microsoft-azure-developer-associate-certification-exam-in-2021-1bee42a03e7c?source=search_post---------324,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are aiming to become a Microsoft Certified: Azure Developer Associate in 2021 and preparing for AZ -204 Developing Solutions for Microsoft Azure exam then you have come to the right place. Earlier, I have shared the best AZ 900 courses and today, I am going to share the best AZ 204 online courses and practice tests for the AZure Developer associate exam.
Disclosure — Btw, some of these links are affiliate links and I may get paid if you join these courses using my links.
Microsoft Azure is a cloud-based service that serves as a framework for businesses to manage their services and applications, amongst other things that enable them to run a successful business.
Microsoft Azure certifications are currently becoming some of the highest demanded certifications in the IT industry.
Businesses that have a reason to influence cloud servers and have a high demand for various remote smart services from different locations are the ones using Microsoft Azure the most.
Now, developers proficient in Microsoft Azure are those that know how to design and create cloud solutions in Microsoft Azure like services and applications. This covers the A-Z of project development from design, development, deployment, or launching and continuous maintenance. The AZ 204, Developing Solutions for Microsoft Azure exam tests for a developer’s abilities to Develop Azure Infrastructure as a Service compute solution, Azure storage, monitor, troubleshoot, and optimize Azure solutions and Implement Azure security. It also checks the ability to connect to and consume Azure and third-party services. If you are a Software developer looking to become Microsoft Certified as an Azure Developer, then you can register for it at the Microsoft page. The exam costs an amount of 165 USD in the United States and is offered in English. When you register for it, Microsoft offers you two options to prepare for the exam. One is free, and the other is for a stipulated price. The free course covers most aspects of Microsoft Azure while the paid course is led by an instructor and is specifically for Microsoft Certified: Azure Developer — Associate certification Exam AZ-204. If you can’t afford the paid one then don’t disappoint because there are many more affordable options available. I have shared the best Microsoft Azure Developer associate courses in this article which you can easily buy for just $10 on Udemy.
Some of them are also free. You can use these free and paid online courses to prepare for the Microsoft Azure Developer Associate certification in 2021.
medium.com
Without wasting any more of your time, here is a list of the best online training courses from Udemy, Pluralsight, and edX that teach how to pass the Exam AZ-204: Developing Solutions for Microsoft Azure Exam to become a Microsoft Certified: Azure Developer — Associate in 2021.
These online courses not only covers exam topics and syllabus but also provide some real-world experience of using Core Azure services which makes them great for learning as well.
Apart from being the highest-rated Microsoft Certified: Azure Developer — Associate Exam AZ-203 course on Udemy; it is also available in German, Italian, Polish, Portuguese, Spanish, French, and English. It has almost 48,000 students enrolled and was created by Scott Duffy. In this course, you will be offered a timed practice exam to check how well you are doing and how ready you will be for the actual exam. Before you consider taking this course, you must have a free or paid subscription to Microsoft Azure with zeal to learn about the Microsoft cloud platform. A background in programming languages like .NET, NodeJS, and PHP will prove very valuable. This course is once in a lifetime. Once you buy it, you will always have access to it. In this online course, you will learn how to become a Microsoft Specialist: Developing Azure Solutions certified, pass the Microsoft AZ-203 Developing Microsoft Azure Solutions test, and master the main concepts of Azure, beyond the ones you normally use.
Here is the link to join this AZ 204 course — AZ-204 Developing Solutions for Microsoft Azure Exam Prep
Just in case you’re still not convinced, here is a sneak peek of the course content. It covers the new AZ-204 syllabus updated last year and includes Virtual Machines, Azure Batch Services, Containerized Solutions, Azure App Service, Mobile Apps, API App, Function App, Azure Storage Accounts.
It also covers Cosmos DB, SQL Database, Blob Containers, Azure Authentication, Azure Access Control, Scaling Apps, and Services, Caching and Content Delivery Networks, Monitoring, and Logging, and Consuming Azure Services.
This is another well-designed course that prepares students for the Microsoft AZ-203 certification exam with almost 200 mock questions at the end. It has a rating of 4.4, with 3700 students enrolled. It was created by Alan Rodrigues and is taught in English. Before you consider taking this course, you have to be well versed in at least one programming language (most not be object-oriented) with a year or more experience as a programmer. You also need to have little experience with cloud computing. This course does not only teach the concepts of Microsoft Azure but also the important aspects of what is required from an exam perspective and makes you better equipped to write the AZ-203 certification exam. A look at the curriculum reveals the following topics covered. Starting with Azure (Optional), Develop Azure Infrastructure as a Service compute solutions, Develop Azure Platform as a Service compute solutions, Develop for Azure Storage, Implement Azure Security, Monitor, troubleshoot, and optimize solutions, Connect to and consume Azure and third-party services with a mock Exam.
Here is the link to join this Azure course — Exam AZ-204 — Developing Solutions for Microsoft Azure
This is an introductory course on AZ 204 exam to get you familiar with Exam objectives and how to prepare for this exam. In this course, you’ll learn everything you need to know about beginning the process of studying for the AZ-204 Exam.
First, you’ll explore the purpose and importance of the AZ-204 Exam. Next, you’ll discover all of the different topic areas that will be covered throughout the entire AZ-204 Exam.
Finally, you’ll learn about the tools and techniques that you will be able to take advantage of in follow-up courses.
When you’re finished with this course, you’ll have the skills and knowledge of what is covered in the AZ-204 Exam and what tools and techniques will be available to help you along the way. Cloud computing is a big bonus, but not compulsory as all the sections on cloud computing are explained in detail.
Here is the link to join this AZ 204 path — Microsoft Azure Developer: Introduction to the AZ-204 Exam
By the way, you would need a Pluralsight membership to access this course which costs around $29 per month or $199 per year (33% discount now).
While I highly recommend this kind of membership because it gives you access to more than 7000+ online courses on the latest technology, you can also try some courses using their 10-day free trial.
pluralsight.pxf.io
This course teaches students more about the capabilities and features of Microsoft Azure and how to explore those to create a more robust and highly available cloud-based solution application. You need a little background knowledge in programming to make the best of this course. The course outline covers how to configure and deploy web applications, create Azure Web Apps from the gallery, deploy and monitor Azure Web Apps, create and configure Azure Virtual Machines, create and manage a storage account, manage blobs and containers in a storage account. You will also learn to create, configure and connect to a SQL Databases instance, identify the implications of importing a SQL standalone database, and handle both user and group subscriptions in an Azure Active Directory instance, create a virtual network and implement a point-to-site network.
Here is the link to join this AZ 204 course — Developing Microsoft Azure Solutions
This is a collection of 12 best online courses to prepare for AZ 204 — Developing Solutions in Microsoft Azure exam from LinkedIn learning.
Cloud developers need to understand the process from design to deployment. The AZ-204 exam covers the skills a developer needs to plan, implement, monitor, and optimize optimal Azure solutions.
This learning path provides the background and support developers need to prepare for the exam.
Here is the link to join this AZ 204 path — Prepare for the Developing Solutions in Microsoft Azure Exam
By the way, you would need a LinkedIn Learning membership to watch this course which costs around $29.99 per month but you can also watch this course for FREE by taking their 1-month-free-trail which is a great way to explore their 16000+ online courses on the latest technology.
linkedin-learning.pxf.io
This course provides 5 complete practice tests & 4 case studies for Microsoft AZ-204 Certification Exam based on the latest syllabus. Before you buy this course, a little background reading on Azure Technology is recommended. This udemy course contains 5 complete timed practice tests. Each test contains 50+ questions, that’s 250+ unique questions to test how well prepared you are for the real exam. These tests also have case studies.
This practice test course is designed to cover every topic, with a difficulty level like a real exam. They explain how to pick the right cloud services for particular applications and create databases that are secured.
Here is the link to join this test — AZ-204 Developing Solutions for MS Azure Practice Tests
This is a brand new course on Udemy to prepare for Microsoft AZ-204 Cert Course: Developing Azure Solutions certification exam in 2021. Created by Skylines Academy and Joe Fecht, this 5.5 hours course will help you prepare for Microsoft AZ-204: Developing Azure Solutions Exam.
This course is based on the AZ-204 certification curriculum from Microsoft and is set up to align with the skills measured documentation. During this course, Skylines Academy Author and Instructor, Joe Fecht, will lead you through:
Using the Skylines Academy approach, lectures will educate you on the terms and principles of Azure solutions, and demos will enable you with a hands-on experience using scenarios to empower you in the real world.
Here is the link to join this AZ 204 course — Microsoft AZ-204 Cert Course: Developing Azure Solutions
That’s all about the best courses to crack the Microsoft Azure Certified Developer Certification exam with code AZ — 203. It’s a good time to become a certified Cloud developer and Microsoft Azure is one of the most promising cloud platforms.
More and more companies are adopting Microsoft Azure for their migration and demand for Azure certified programmers and professionals are increasing. You can join any of these online training courses to start your preparation.   Other Microsoft Azure and Cloud certifications you may like to explore
Thanks for reading this article so far. If you find these Microsoft Azure Administrator courses, both AZ-103 and AZ-104 useful, then, please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. — If you are new to Microsoft Azure and looking for a free beginner course to start learning key concepts of the Microsoft Azure platform then you can also check out Microsoft Azure Concepts- a free course by LinuxAcademy on Udemy. It’s completely free and all you need is a free Udemy account to join this course online.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
101 
1
101 claps
101 
1
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/t-t-software-solution/%E0%B8%A7%E0%B8%B5%E0%B8%98%E0%B8%B5%E0%B8%A5%E0%B8%87%E0%B8%97%E0%B8%B0%E0%B9%80%E0%B8%9A%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%AA%E0%B8%AD%E0%B8%9A-az-900-online-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-azure-exam-voucher-c0d027253d34?source=search_post---------325,"There are currently no responses for this story.
Be the first to respond.
หลังจากที่ได้ Voucher มาจากโครงการ Virtual Academy for Azure Fundamentals เรามาก็มาดูขั้นตอนในการสมัครเพื่อเตรียมสอบกันนะครับ
บทความนี้ได้รับแรงบันดาลใจมาจากบทความของคุณ Rachanee Saengkrajai นะครับ
medium.com
2. กดที่ Link Microsoft Learn Dashboard
3. กด Schedule Exam
4. ค้นหา az-900
5. เลือก Thailand และกดที่ปุ่ม Schedule with Pearson VUE
6. กรอกข้อมูลส่วนตัวให้เรียบร้อย
7. ที่หน้า Exam Discounts ให้เรากดข้ามเลยครับ (เพราะเรามี Voucher ให้สอบฟรี อิอิ อย่างหล่อ)
8. ระบบจะพาเราเข้าไปที่ระบบของ Pearson VUE ให้เรากด Run pre-check เพื่อทดสอบว่าคอมพิวเตอร์ของเราและ internet ของเราพร้อมที่จะใช้สำหรับทำการสอบที่บ้านไหมนะครับ
9. ระบบจะทำการตรวจสอบทั้ง Microphone, Internet Speed และกล้องของเรานะครับ เพราะเวลาสอบจริงจะเราจะต้องเปิดทั้งกล้องและ Microphone ไว้เพื่อให้ผู้คุมสอบเห็น
10. ถ้าตรวจสอบเรียบร้อยจะเจอหน้านี้น่ะครับก็ปิด Tab นี้ได้เบย
11. กดตกลงตามข้อกำหนดทั้งหมดนะครับ
12. เลือกว่าจะสอบภาษาอะไร ผมเลือก English
13. ตรวจสอบรายละเอียดว่าเราเลือกสอบถูกวิชาแล้วก็กด Next
14. เลือกภาษาของผู้คุมสอบ ผมเลือก English
15. เลือกวันที่และเวลาสอบครับผม สังเกตุว่าถ้าเราเลือก Location Thailand ในขั้นตอนก่อนหน้านี้ ในระบบจะแสดง Timezone เป็น Asia/Jakarta-WIB นะครับ ถ้าไม่ตรงตามนี้ลองกลับไปแก้ Location ใหม่น่ะครับ
16. ตรวจสอบความถูกต้องโดยเฉพาะเวลาสอบ, ชื่อผู้สอบ และ เบอร์โทรศัพท์ครับ เสร็จแล้วกด Proceed to Checkout
17. กด Accept เพื่อตอบตกลงตาม Policies
18. กดที่ Add Voucher or Promo Code, Copy Voucher ของเราจาก Email มาแปะครับ เสร็จแล้วกด Apply
19. ราคาลดแว้ววว เหลือ 0 USD เลย หึหึหึหึหึหึ!
20. ตรวจสอบความถูกต้องของข้อมูลอีกครั้งน่ะครับ โดยเฉพาะวันที่สอบ เมื่อมั่นใจแล้วก็กด Submit Order เพื่อยืนยันการลงทะเบียนสอบ
21. เรียบร้อยครับ ตอนนี้เราได้ยืนยันเวลาลงทะเบียนสอบเรียบร้อยและจะได้รับ Email 2 ฉบับบใน Inbox ของเราครับ
22. Email ฉบับแรกเป็นรายละเอียดของ Invoice ครับ 0 USD สบายใจ อิอิ
23. Email ฉบับที่สองจะระบุข้อมูลการข้าสอบนะครับ โดยให้เรากดที่คำว่า Certification Dashboard เพื่อเข้าไปตรวจสอบและเตรียมเครื่องของเราให้พร้อมสอบ
24. เมื่อเราอยู่ใน Microsoft Learn Dashboard เราจะเห็นรายการสอบอยู่ใน Appointments ทางกล่องขวามมือนะครับ
ให้เรากดที่ลิ้ง Start a previous scheduled online proctored exam เพื่อเข้าไปตรวจสอบรายละเอียดการสมัครสอบและเตรียมเครื่องของเราให้พร้อมสอบครับ
25. เมื่อเข้ามาที่ Pearson VUE แล้วเราจะเห็นรายละเอียดการสอบในกล่อง Purchased Online Exams ครับ
ให้เราทดลองกดที่ AZ-900: Microsoft azure Fundamentals
26. เสร็จแล้วให้กดที่ลิ้ง run the system test เพื่อทดสอบติดตั้ง Program ที่ใช้ทำข้อสอบครับ
27. ให้เราติ้กถูก, กดปุ่ม Copy Access Code เพื่อนำ Code ไปใช้ในขั้นตอนถัดไป และกดปุ่ม Download Application ครับ เมื่อโหลดเสร็จแล้วก็กดรันได้เลย
28. หน้าตาโปรแกรมจะคล้ายๆ Meeting Tool นะครับ มีช่องให้เรากรอก Access Code และเบอร์โทรก็กรอกให้ครบครับ
29. ระบบจะทำการตรวจสอบ กล้อง, Microphone, และ Internet Speed ของเราอีกรอบ
30. ถ้าผ่านแล้วก็กด Launch sample exam ได้เลยครับพี่น้อง!
ตอนแรกผมทดสอบไม่ผ่านเพราะว่าที่เครื่องผมมีโปรแกรมชื่อ Bandicam เอาไว้อัด Video หน้าจอนะครับ ทำให้ต้องปิดโปรแกรม Bandicam ก่อนถึงสามารถใช้ได้
ส่วนตัวอย่างข้อสอบ 1 ข้อที่เขาให้เราทดลองทำ เราไม่สามารถ Capture ภาพมาได้เลยไม่มีตัวอย่างให้ดูน่ะครับ
31. เมื่อทำแบบตัวอย่างทดสอบเสร็จแล้วก็จะขึ้นหน้าตาแบบนี้ครับ กด Close เพื่อจบการทดสอบติดตั้งโปรแกรมสอบเรียบร้อยเลย
จบแล้วครับ ขอให้ทุกท่านโชคดีมีชัยในการสอบน่ะคร้าบบบ
สอบผ่านแว้ว เย้ๆๆๆ AZ-900
www.youracclaim.com
Medium — AZ-900 รีวิวแนวข้อสอบและวิธีลงสอบที่ศูนย์สอบ
Medium — AZ-900 สรุปละเอียดสุดๆ
นายป้องกัน ; )
https://www.tt-ss.net/
47 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
47 claps
47 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/five-things-about-azure-devops-with-damian-brady-894c95995259?source=search_post---------326,"There are currently no responses for this story.
Be the first to respond.
Five Things is back! Season 2 is here and our first episode has everything that you want in a low-budget video series about tech — awkward interviews, marginal insight, AND miniature ponies.
In our season opener, I sit down with Damian Brady to discuss the new Azure DevOps. DevOps was formally known as Visual Studio Team Services. The new release brings not just a name change, but a new feature called “Azure Pipelines”.
azure.microsoft.com
Azure Pipelines is a new product which allows you to deploy your code from any source control repository to any cloud. You can pull, do builds, run tests and deploy your code. Furthermore, it’s free for open-source projects. If you’re using GitHub, you can get Azure Pipelines as an extension from the Github extension gallery and setup a pipeline whenever you create a new repo.
Don’t want to watch the video? That’s fine, I’m not offended. Here’s a summary.
Check out the interview with Damian, and be sure to visit the link below to get started with Azure DevOps.
dev.azure.com
Any language.
28 
3
28 claps
28 
3
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@tsuyoshiushio/azure-functions-v-2-0-httptrigger-with-cosmosdb-client-tips-15d313cb1cbe?source=search_post---------327,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Apr 1, 2018·3 min read
I spent several hours to setup .NetCore 2.0 based Azure Functions Environment with CosmosDB. I’d like to share my work around to record what I encountered. These tips are not special. You can find these in the GitHub issues. However, it might be help to reduce the time for some search.
I create an Azure Functions app with .Net Core 2.0 with CosmosDB. Currently I don’t use Cosmos Trigger. However, I guess you can do it the same thing to it.
If you want to use CosmosDB extensions you can use Microsoft.Azure.WebJobs.Extensions.CosmosDB v3.0.0-beta7 It is the same as the DocumentDB.Core 1.5.1
However, I encounter a lot of issues. You can find the version match on this announcement issue.
github.com
I encounter a lot of these.
When you start Visual Studio debugging, it might happen. On the debugging feature, the Azure Functions runtime of your Visual Studio might old. Currently, we have no way to upgrade it. Instead, we can use the Azure Functions CLI instead. For installing Azure Functions CLI, you can refer this.
Visual Studio 2017 > Project right click > Properties > Debug . Then configure like this. This is just configure to use the Azure Functions CLI.
For more details discussion
github.com
You might get this missing library, You might write HttpTrigger with Old Style. Unfortunately, we have no way to solve until now. However, if you switch it to the new style, You might not encounter this error.
This is the old style. (This is the V1 Style)
The new Style V2
github.com
(Update! 03/05/2018) Rather than use this strategy, you can install Microsoft.Azure.WebJobs.Extensions.CosmosDB 3.0.0-beta7 works now much better!
(Duplicated!)To solve this issue, you can Install Microsoft.Azure.DocumentDB v1.19.1 nuget package. I’m not sure why it solve the problem. The nuget package is for .NetFramework! However, it install two DLLs and win7-x64 runtime DLL for these. Maybe I need to read code and understand .Net Core 2.0 nuget architecture. If you know a good resource, and why it happens. please let me know.
github.comConso
Now you can play with Azure Functions V2 with CosmosDB. :) In the future, the same thing might happen, however, you can check these issues.
Senior Software Engineer — Microsoft
53 
1
53 
53 
1
Senior Software Engineer — Microsoft
"
https://medium.com/@renatogroffe/hospedando-projetos-web-no-azure-de-um-site-est%C3%A1tico-a-um-cluster-kubernetes-msp-tech-days-7c02cf3ae62a?source=search_post---------328,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jun 8, 2019·3 min read
No sábado dia 01/06/2019 participei como palestrante do MSP Tech Days, em uma apresentação sobre as diferentes opções para hospedagem de aplicações Web no Azure. Este evento em São Paulo-SP, mais precisamente na Unidade Paulista da Universidade São Judas (situada na Avenida Angélica).
Realizei durante a palestra apresentações sobre os seguintes serviços: Storage Account (para hospedagem de Web Sites estáticos), Azure App Service, Azure Container Registry, Azure Container Instances, Azure Web App for Containers e Azure Kubernetes Service (AKS).
O MSP Tech Days está sendo promovido por MSPs (Microsoft Student Partners) da América Latina em cidades de diversos países, além de contar com eventos online gratuitos cobrindo diferentes tecnologias Microsoft e previstos para este mês de Junho/2019. Sugiro a todos que acompanhem esta iniciativa, fazendo sua inscrição no meetup mantido pelos MSPs LATAM:
www.meetup.com
Gostaria de deixar neste post meu muito obrigado ao Orlando Gomes (Microsoft MVP, MSP) e ao Rogério da Rocha Rodrigues (Microsoft MVP, MSP, MTAC) pelo convite e por todo o suporte para que eu pudesse participar como palestrante. Deixo aqui também meus agradecimentos ao Orlando Gomes, ao Gabriel Florêncio e ao Andre Moreira pelas fotos tiradas durante a palestra.
Os feedbacks que tive sobre a apresentação foram também ótimos. Aproveito para agradecer mais uma vez ao Andre Gomes por inclusive mencionar isto no LinkedIn:
Os slides da apresentação já estão no SlideShare:
Para os interessados em conhecer mais sobre o conteúdo abordado durante a apresentação deixo ainda como referência os seguintes artigos que produzi sobre a hospedagem de projetos Web no Microsoft Azure:
Hospedando um website estático de forma rápida e barata no Azure Storage
Publicando um web site estático na nuvem com Docker, Nginx e Azure Container Instances
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
ASP.NET Core + Azure Web App for Containers: escalando uma API REST com containers Docker
Docker para Desenvolvedores .NET - Guia de Referência
ASP.NET Core + Azure + Kubernetes: Guia de Referência
E também a gravação de uma live que fizemos no Canal .NET, com dicas de utilização do Microsoft Azure para Desenvolvedores Web:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
27 
27 claps
27 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/the-pythic-coders-recommended-content-for-getting-started-with-machine-learning-on-azure-fcd1c5a8dbb4?source=search_post---------329,"There are currently no responses for this story.
Be the first to respond.
Tldr; Since the post on DevOps resources was well received, and it can be difficult tracking down documentation, I was asked to provide a list of recommended resources for Machine Learning on Azure. I will make an effort to keep this list updated as new developments emerge.
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
azure.microsoft.com
docs.microsoft.com
docs.microsoft.com
Our team’s charter is to help every technologist on the planet succeed, be they students or those working in enterprises or startups. We engage in outreach to developers and others in the software ecosystem, all designed to further technical education and proficiency with the Microsoft Cloud + AI platform. If your stuck feel free to send us a tweet.
docs.microsoft.com
docs.microsoft.com
docs.microsoft.com
github.com
github.com
github.com
github.com
github.com
medium.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
www.microsoft.com
docs.microsoft.com
github.com
github.com
github.com
Any language.
36 
36 claps
36 
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/visual-studio-2017-2019-fails-when-i-create-an-azure-functions-project-89e993ef31f?source=search_post---------330,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
May 23, 2019·1 min read
I encounter the issue that I can’t create an Azure Functions project.
We can create other project. However not for Azure Functions. If I try to create an Azure Functions project on VS2017, it says, One or more errors occurred.
For VS2019, it says, Illegal characters in path.
I remove/re-install the VS however, it is the same.
Remove %localappdata%\AzureFunctionsTools . It solves both cases. I encountered this solution before, at that time it was different error. Next time, I should try this work around. If we remove the directory, maybe the tools folder was broken.
developercommunity.visualstudio.com
Senior Software Engineer — Microsoft
See all (200)
37 
4
37 claps
37 
4
Senior Software Engineer — Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/streaming-at-scale-in-azure/apache-drill-azure-blobs-and-azure-stream-analytics-ef34a1360d2b?source=search_post---------331,"There are currently no responses for this story.
Be the first to respond.
Apache Drill is a very interesting project that, if you haven’t heard of it yet, allows you to use the ubiquitous SQL language to query…almost everything. Here’s the description on the website:
Drill supports a variety of NoSQL databases and file systems, including HBase, MongoDB, MapR-DB, HDFS, MapR-FS, Amazon S3, Azure Blob Storage, Google Cloud Storage, Swift, NAS and local files. A single query can join data from multiple datastores. For example, you can join a user profile collection in MongoDB with a directory of event logs in Hadoop.
drill.apache.org
So I though it would have been perfect also to query Azure Stream Analytics results. As you know, Azure Stream Analytics doesn’t support (yet?) a notebook-style environment (à-la Databricks Spark Structured Streaming for example), which makes development and testing a bit trickier. I already described a possible solution here:
medium.com
As said in the mentioned article, an alternative — and actually better — solution, than using Azure SQL, especially if you’re not really comfortable with it, is to use Apache Drill to read the output of Azure Stream Analytics and so check if the query produces the expected results.
Quite easy but with a big caveat if you already have installed Apache Spark. But let’s start from the beginning. Download and install Apache Drill as described here:
drill.apache.org
I used the embedded mode since I didn’t want to install an entire Hadoop cluster to run Drill. I just needed something easy, quick and cheap. The embedded mode, that can run on a single node, is just perfect for this use case. If your machine doesn’t have any Apache Spark or Apache Hadoop installed, you’ll be able to see something like this:
But if you have Apache Spark installed, for example, Drill will notice the HADOOP_HOME environment variable existence and will try to use it. The result is that it won’t be able to start Apache Zookeeper in local mode and you’ll get some errors:
To solve the problem you can just unset the HADOOP_HOME variable just for the Apache Drill session, creating a .bat file like the following:
To allow Apache Drill connect to Azure Blob Store, a specific Data Source needs to be configured.
https://drill.apache.org/docs/connect-a-data-source-introduction
Now, since explicit support to Azure Blob Store is mentioned even in the Apache Drill homepage, you would expect a nice documentation page, just like it exists for Amazon S3, that tells you how to configure everything, right? Wrong, of course.
The first step to make Azure Blob Store working with Apache Drill is getting the correct version of JARS. The one I found working are the following
download and copy them into jars/3rdparty folder. Easy right? The problem was just finding the correct combination of libraries versions that works…but you’re lucky since I’ve already done it for you, so you can just enjoy one more time at the pub, instead of spending hours just testing libraries.
The libraries you just copied will be automatically picked up by Drill when needed…so it’s now time to tell it when it should do so. First of all you need to get the key value of the Azure Blob Store you what to access to. It can be done via the Azure Portal, the AZ CLI or via the nice Azure Storage Explorer:
and then you change (or create if it doesn’t exists) the core-site.xml file in conf folder so that the XML will look like the following:
replacing, of course, ACCOUNT_NAME and ACCOUNT_KEY with the Azure Blob Store account name you want to use and its own secret key.
You then have to create a new Storage Plugin. You can easily do that from the web interface (http://localhost:8047, just keep in mind the Drill must be running in order to access it), using the “Storage” section.
Create a new Storage Plugin, name it az for example, and then copy and paste the following configuration.
https://gist.github.com/yorek/35e2b693fb749f0388db22c2d814ddaf
The configuration has been taken from the dsl Storage Plugin already available in Apache Drill, and the connection has been modified to point to the Azure Blob Store. Of course, just like before, replace CONTAINER and ACCOUNT_NAME with your own values.
You should be able now to run Apache Drill and query the configured Azure Blob Store:
The most complex part is done now. All is needed now is to create an Apache Avro output for the Azure Stream Analytics job you want to monitor
and then just use Apache Drill to query it. Of course you can also use other output format like JSON or CSV, but I suggest to use Avro since it comes with a schema and so you don’t have to do any cast to correct data types. Plus it is fast and compact which make it perfect for streaming scenarios.
Here’s Drill in action:
If you don’t want to mess up your machine with Java, Apache Drill and other stuff that maybe you’re not familiar with, but you still like the idea of using Drill to query streaming results, you can just go for the Docker way. In order to have Apache Drill up and running with the Storage Plugin correctly configured, a trick I used is to inject the file storage-plugin-override.conf and core-site.xml to have my most accessed Azure Blob Stores already configured and ready to be used.
You can grab the Docker solution here:
github.com
Make the changes you need to the aforementioned files, build the image with the provided drill-build.bat script and then run it with drill-run.bat. You’ll have Apache Drill running in a sec.
Notes on creating streaming at scale solution in the Azure…
49 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
49 claps
49 
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/7-best-courses-to-learn-spring-boot-with-aws-and-azure-cloud-platform-9f953d12bb93?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you want to learn how to deploy Spring Boot apps and Microservices on public cloud platforms like AWS, Azure, and Google Cloud Platform and looking for the best resources like online courses then you have come to the right place.
Disclosure — Btw, some of these links are affiliate links and I may get paid if you join these courses using my links.
Earlier, I have shared my favorite Spring Boot Course and best Microservices courses and in this course, I will share some advanced level courses to learn about containerization and deployment of Spring Boot Microservices on AWS, Azure, Google Cloud Platform, and even on legacy Cound Foundry.
If you have been reading technical blogs and articles then you might be hearing about widespread cloud adoption among all sizes of companies. In the last few years, many companies, both big and small have shifted their infrastructure to the cloud or in the process of doing it.
I have no doubt that the next generation of Java applications will be written for and run in the Cloud and that’s why it’s important for Java developers to learn about Cloud platforms like AWS, Azure, GCP, Cloud Foundry, and others.
Thankfully Java frameworks like Spring Framework is taking this cloud move seriously and new frameworks like Spring cloud is getting popular which makes developing cloud-based application easy.
While there will be some challenges to shift the focus from writing in premises to cloud-native applications, adopting Microservices architecture and cloud-native Java can help Java developers stay ahead of the curve.
The microservices architecture perfectly suits the public cloud, with its focus on elastic scaling with on-demand resources. Since most of the web application and Microservice development is happening on Spring Boot, the main thing you can learn as of now is how to deploy your Spring Boot application on different cloud platforms like AWS and Microsoft’s Azure Cloud Platform While I have shared about cloud-computing resources like AWS, Azure, and GCP in past my readers asked me something which is focused on Java and Spring Boot and that’s why I am going to share the 5 best cloud courses that are focused on Java and Spring Boot. These are the practical and hands-on courses that will teach you things like how to deploy your Spring Boot application to AWS with Elastic Beanstalk, ECS, and Fargat, or deploy Java Microservices to AWS and other cloud platforms. These courses are equally useful for both beginner and experienced Java developers how are working with Spring Boot and cloud platforms as well as DevOps Engineers who are responsible for managing Java applications on Cloud.
Without wasting any more of your time, here is a list of the best hands-on cloud courses for Java and Spring Boot developers. Currently, these online training course covers cloud platforms like Amazon Web Service, Microsoft Azure, Google Cloud Platform and Pivotal’s Cloud Foundry environment but I will keep adding new training courses which are focused on Java and Spring boot but teach you how to deploy a Spring Boot application and Microservices in Google Cloud Platform and others.
This is one of the first courses you should take if you want to deploy your Spring Boot applications to AWS. This course will teach you step by step to deploy a Java Spring Boot REST APIs and Full Stack application to AWS using Elastic Beanstalk service. Created by Ranga Rao Karnam, a fellow Java developer and best selling Udemy instructor this course will not only teach you to core AWS services like EC2, S3, AWS CodePipeLine, AWS CodeBuild, SQS, IAM, CloudWatch but also teach you things like how to deploy a RESTful web service into the cloud. You will learn how to containerize your Java and Spring Boot application using Docker and then deploy it into Cloud. You will also learn how to automatically scale your Java applications based on load as well as deploy multiple instances behind a load balancer using Elastic Beanstalk service in AWS.
You will also learn how to create a continuous delivery pipeline with AWS Code Pipeline which is quite important from a DevOps perspective.
Here is the link to join this course — Deploy Java Spring Boot Apps to AWS with Elastic Beanstalk
Overall, a very practical and useful course for experienced Java developer who wants to learn how to deploy, scale, and manager Java and Spring boot application on AWS.
This is another great course for Java developers who wants to learn how to deploy Spring Boot Applications to the Cloud on AWS and how to implement Continuous Integration and Continuous Delivery in AWS (CI/CD) for DevOps.  Created by John Thompson from Spring Framework Guru, one of my favorite Java instructors on Udemy, this course is focused on DevOps for Spring application on the AWS cloud platform.
In this course, you will learn how to deploy Spring Applications to multiple environments including AWS. You will start with basics like creating a server in AWS using the Amazon EC2 service. This is a very hands-on course and to get the most out of this course, you will need an AWS account. Don’t worry, you don’t need to spend any additional money as you should be able to use the AWS free tier to complete the course assignments.
In this course, you will learn how to install Jenkins on a Linux server. A server that you will provide in the AWS cloud. You will also learn how to use Docker and MySQL databases in the AWS environment.
Here is the link to join this course — Spring Framework DevOps on AWS
The course also teaches you best practices used in enterprise software development like using a continuous integration server for continuous delivery.
This is another advanced Spring Boot Microservice course for Java developers who wants to deploy into public cloud computing platforms like AWS and Azure.
One of the key steps before deploying into the cloud is containerizing or Dockerizing your Spring Boot applications and this course will teach you how to extend, refine, harden, test, and “dockerize” your Spring Boot microservices, and turn them into production-ready applications.
You will also learn about how to link to external databases, build secure APIs, use unit and integration testing to uncover application flaws during development and configure scalable deployment options with Docker containers.
Overall an advanced course to extend, refine, harden, test, and “dockerize” your Spring Boot microservices, and turn them into production-ready applications.
Here is the link to join this course — Extending, Securing, and Dockerizing Spring Boot Microservices
By the way, you would need a LinkedIn Learning membership to watch this course which costs around $19.99 per month but you can also watch this course for FREE by taking their 1-month-free-trail which is a great way to explore their 16000+ online courses on the latest technology.
This is a free Spring Boto Microservice course from Coursera where you will learn how to develop Java Microservice with Spring Boot and Spring Cloud Microservices on Google Cloud Platform
This course is created by Google Cloud Training, so you will be learning from the source. In his course, you will use Cloud Runtime Configuration and Spring Cloud Config to manage your application’s configuration.
You’ll send and receive messages with Cloud Pub/Sub and Spring Integration. You’ll also use Cloud SQL as a managed relational database for your Java applications, and learn how to migrate to Cloud Spanner, which is Google Cloud’s globally distributed strongly consistent database service.
It will also teach you about tracing and debugging your Spring applications with Stackdriver.
Here is the link to join this course — Building Scalable Java Microservices with Spring Boot and Spring Cloud
By the way, if you find Coursera courses useful, which they are because they are created by reputed companies and universities around the world, I suggest you join the Coursera Plus, a subscription plan from Coursera which gives you unlimited access to their most popular courses, specialization, professional certificate, and guided projects.
coursera.com
Apart from major cloud platforms like AWS, Azure, and GCP, there also exist specialized cloud platforms like Pivotal’s Cloud Foundry, also known as PFC. If you remember, Pivotal is the company behind Spring Framework and they are also pioneering cloud-native Java development.
By the way, PCF is now called Tanzi. PWS is no longer available. You would need to install PCF Dev on Your Local Machine to play with PCF. If you are looking for a course to learn how to deploy a Java or Spring Boot application, a RESTful API, Full Stack Applications, and Microservices to Pivotal Cloud Foundry then this is the perfect course for you. In this course, you will not only learn Pivotal Cloud Foundry ( PCF ) fundamentals but also things like how to deploy Spring Boot REST API to the Pivotal Cloud Foundry environment.
This course covers a number of PCF Services like Databases, Spring Cloud Services including Service Registry and Config Server which is important for Java developers.
You will not only learn to deploy REST APIS and Microservices but also Full Stack Applications are written in Java and Spring Boot.
Here is the link to join this Spring Boot occurs —Master Pivotal Cloud Foundry (PCF) with Spring Microservices
You will also learn how to Auto Scale applications based on load as well as deploy multiple instances behind a load balancer using Pivotal Cloud Foundry. In short, a good, hands-on course to learn about the Pivotal Cloud Foundry platform from Java and Spring boot developer’s perspective.
This is another advanced course on AWS for Java and Spring Boot developers. It contains over 8-hours of online training material that will teach you everything you need to know about AWS from a Java developer’s perspective. Created by In28Minutes, this course starts with explaining AWS fundamentals and then covers a number of AWS Services like ECS — Elastic Container Services, AWS Fargate, EC2 — Elastic Compute Cloud, S3, AWS CodePipeLine, AWS CodeBuild, IAM, CloudWatch, ELB, Target Groups, X-Ray, AWS Parameter Store, AWS App Mesh and Route 53. You will not only learn how to build Docker images for your Java Spring Boot Microservice Projects but also the basics of implementing Container Orchestration with ECS (Elastic Container Service) — Cluster, Task Definitions, Tasks, Containers, and Services.
You will also learn practical stuff like creating a continuous delivery pipeline with AWS Code Pipeline and how to debug problems with deploying containers using Service events and AWS CloudWatch logs etc. It also covers implementing Centralized Configuration Management for your Java Spring Boot Microservices with AWS Parameter Store.
Here is the link to join this course — Deploy Spring Boot Microservices to AWS
I highly recommend this course to experienced Java developers and DevOps engineers who are responsible for managing Java-based Microservices and Spring boot applications. Overall, an advanced AWS course for Java and Spring Boot developers. You will learn a lot of practical stuff for deployment, scaling, monitoring, troubleshooting, and tracing Java and Spring boot application on AWS.
This is another Spring Boot course by Ranga Karnam and in this course, he will teach you how to deploy Java Spring Boot REST API, Full Stack, Docker, and Web Apps with Azure App Service and Azure Web Apps into the Microsoft Azure platform. It’s not very different from the first course which talks about deploying Spring Boot application on AWS and if you have gone through that course then deploying on Azure will be much easier as both AWS and Azure.
Even though both AWS and Azure have different services for computing, storage, and network but concepts and processes remain the same. Things like deploying a containerized version are applicable for both AWS and Azure. The good thing is that you will learn how to deploy your Java Spring Boot application online for live Internet access, which is what many Java developers always ask. It gives you a lot of satisfaction to see your app live on the web and you can also share the links with your friends and colleagues.
Here is the link to join this course — Take Java Spring Boot Apps to Azure
If you have a startup idea then you can also use the techniques learned in this course to deploy a proof of concept app and share it with your clients and beta tester. Overall a practical and hands-on course to deploy Java and Spring Boot applications on the Microsoft Azure platform. That’s all about some of the best courses to learn how to deploy Spring Boot applications on various cloud platforms like AWS, Microsoft Azure, and Pivotal’s CloudFoundary.
The list not just include basic courses that teach you AWS and Azure basics along with Java deployment but also some advanced courses which will teach you how to deploy your spring boot on the internet and access it via the web and automatically scale up and down based upon load by using sophisticated services provided by AWS.   Other Java and Spring articles you may like to explore
Thanks for reading this article so far. If you like these best Spring Boot and Cloud Computing courses then please share them with your friends and colleagues. If you have any questions or feedback then please drop a note. P. S. — If you are looking for a free course to learn Spring Boot and Cloud then you can also check out this Spring Boot and AWS S3 free course on Udemy. This course is created by Nelson Djaolo and it will teach you how to upload images and files to Amazon S3. The course is completely free and all you need is to create a free Udemy account to enroll in this course.
udemy.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
142 
Collection of best Java articles, tutorials, courses, books, and resources from Javarevisite and its authors, Java Experts and many more.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
142 claps
142 
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/t-t-software-solution/%E0%B8%AA%E0%B8%B2%E0%B8%98%E0%B8%B4%E0%B8%95%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%95%E0%B8%B4%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87-azure-paas-%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A-%E0%B8%81%E0%B8%B2%E0%B8%A3-migrate-asp-net-core-2-%E0%B9%81%E0%B8%A5%E0%B8%B0-entity-framework-core-6196eaeaefec?source=search_post---------333,"There are currently no responses for this story.
Be the first to respond.
สวัสดีครับ ในตัวบทความนี้ ผมจะสาธิตวิธีการ ใช้ PaaS (Platform as a Service) ของ Azure เพื่อวาง Web และ Database ที่รันด้วย ด้วย Technology Stack ดังนี้น่ะครับ
ปล.บทความนี้มีขั้นตอนค่อนข้างเยอะ เลยจะมีเนื้อหาที่ยาวมากน่ะครับ
การใช้งานในรูปแบบ PaaS จะช่วยให้เราจัดการ Server ได้ง่ายขึ้น เพราะเน้นดูแค่ โปรแกรมและข้อมูลที่ใช้ ส่วนอื่นๆทาง Azure จะเป็นคนดูแลให้ครับ เช่น Windows เราก็ไม่ต้องคอย update หรือ จัดการกับ ความปลอดภัยต่างๆด้วยตัวเราเอง
ผมขอยกตัวอย่างภาพประกอบข้างล่างน่ะครับ
ขั้นตอนการติดตั้ง
Subscription จะเกี่ยวข้องกับค่าใช้จ่ายครับ เพราะงั้นเราจึงควรแยก subscription แบ่งออกตาม ลูกค้า หรือ project เพื่อให้เห็นค่าใช้จ่ายได้ชัดเจนครับผม
Resource Group จะเกี่ยวกับการจัดหมวดหมู่ของ Azure Resource เข้าด้วยกันเพื่อง่ายต่อการจัดการน่ะครับ เช่น สร้าง Group Dev สำหรับ Resource ทั้งหมดที่เกี่ยวกับ Development ซึ่ง เวลาลบเราสามารถลบได้ทั้ง Group เลยในครั้งเดียว
เราจะใช้ Storage Account เพื่อทำการเก็บ Database Backup File จาก Server อื่นเพื่อนำมา Deploy ต่อใน Azure Cloud Database ครับ
หลังจากได้ Storage Account แล้ว ขั้นต่อไปเราจะสร้าง Container ไว้ใช้ในนั้น (มองเสมือน Storage Account เป็น Drive, Container เป็น Folder)
เราจะทำการ migrate database ที่เราทำไว้ ย้ายมาอยู่ที่ Azure Cloud Database ซึ่งมี ข้อจำกัดตรงที่ต้องเป็น File ประเภท Data-tier Applications (.bacpac) เท่านั้น และไม่สามารถ ทำการ query ข้าม database
ในบทความนี้ผมทำ ผ่าน MSSQL Express 2016 + Microsoft SQL Server Management Studio 13 (SSMS)น่ะครับ
เพื่อความสะดวกในการเข้าถึง Storage Account ผมแนะนำให้ download Microsoft Azure Storage Explorer มาใช้น่ะครับ
Azure SQL Server จะถูกมองในลักษณะเป็นที่รวม logical database links เข้าด้วยกัน กล่าวคือ database แม้ว่าอยู่ใน Server เดียวกัน แต่จริงๆ อาจจะอยู่แยกเครื่องกัน และไม่สามารถ query ข้าม database ได้
Azure Database จะมี Firewall กันไม่ให้สามารถติดต่อเครื่องได้โดยตรง เพราะงั้นต้อง ปรับแก้ Firewall ด้วย
ทำการ migrate database ที่เรา import มาไว้ใน storage account นำไปใส่ไว้ใน database ที่เราพึ่งติดตั้ง
ประสิทธิภาพของ Azure database จะมีหน่วยวัดเป็น DTU ซึ่งจะผูกอยู่กับ Database โดยตรง ซึ่ง ถ้า DTU มีค่ามากขึ้นก็หมายถึงว่า Database นั้นทำงานได้มีประสิทธิภาพมากขึ้น
App Service Plan จะถูกมองเสมือนว่าเป็น Web Server ที่เราจะใช้ในการ host web ย่อยๆอยู่ภายในครับ (App Service) ซึ่งเราจำเป็นต้องเลือก package แบบ standard เพื่อทำให้สามารถ Backup web และ database ได้ (ไม่เกิน 1 เดือนย้อนหลัง)
คือพื้นที่ๆใช้ในการจัดการ Application ต่างๆ ซึ่งในทีนี้เราจะใช้ในการ จัดการ Web น่ะครับ (จริงๆยังมีอีกหลายอย่างเช่น mobile app, function app)
เราสามารถทำการ deploy web ได้หลายวิธี ซึ่งในบทความนี้จะใช้ 2 วิธีคือ
เริ่มต้นด้วยการ กด เลือก App Service Menu เพื่อทำการ download public profile
เราสามารถทำการ configure App Service ในการ backup ทั้ง web และ database ให้โดยอัตโนมัติครับ แต่จำเป็นต้อง ใช้ App Service Plan ใน package standard เป็นต้นไป
ทดลอง Backup files ที่เกิดขึ้น
ตัวอย่าง Web API ที่รัน บน Azure
ผมตั้งใจว่า จะเขียนอีก 2 บทความที่เกี่ยวกับ Azure น่ะครับ
ขอบคุณมากๆครับ
https://www.tt-ss.net/
15 
1

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
15 claps
15 
1
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/azure-kubernetes-services-aks-refer%C3%AAncias-gratuitas-e-dicas-para-solu%C3%A7%C3%A3o-de-problemas-comuns-975d6d0b19e8?source=search_post---------334,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 17, 2019·3 min read
Neste post trago diversas referências gratuitas (artigos, vídeos, slides, projetos de exemplo) que produzi sobre o Azure Kubernetes Service (AKS), alternativa que permite a utilização do Kubernetes a partir do Microsoft Azure. Além disso, incluí aqui alguns problemas comuns que podem acontecer quando da utilização deste serviço.
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem - parte 1
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem - parte 2
O uso do Kubernetes já foi tema de 2 eventos online gratuitos do Canal .NET, com a gravação dos mesmos estando disponível no YouTube e abordando desde os primeiros passos ao deployment automatizado via Azure DevOps (antigo VSTS):
Ao tentar conceder acesso via PowerShell ou bash para utilização de um cluster criado via Azure Kubernetes Service através do comando (considerando um grupo de recursos chamado TesteKubernetes, assim como um recurso do AKS denominado ContagemService):
az aks get-credentials --resource-group TesteKubernetes --name ContagemService
Poderá ser exibido o erro “A different object named <NOME DO RECURSO> already exists in clusters”, como indicado na próxima imagem (provavelmente um recurso com o mesmo nome foi criado antes e posteriormente excluído):
Para resolver este problema basta apenas acrescentar o parâmetro --overwrite-existing ao comando mencionado anteriormente:
az aks get-credentials --resource-group TesteKubernetes --name ContagemService --overwrite-existing
É o que demonstra a imagem a seguir:
Com o comando a seguir (az aks browse) temos acesso ao Dashboard do Kubernetes:
az aks browse -g TesteKubernetes -n ContagemService
Recentemente, no entanto, o acesso a esta função foi desativado no Azure de forma a restringir o acesso a cluster. Assim, é comum que sejam apresentados erros como os descritos nas próximas imagens:
A instrução a seguir permitirá que se libere o acesso ao Dashboard do Kubernetes:
kubectl create clusterrolebinding kubernetes-dashboard -n kube-system --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
Finalmente teremos sucesso ao acessar o Dashboard do Kubernetes:
AKS troubleshooting | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
See all (49)
22 
22 claps
22 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/xp-inc/curso-criando-apis-restful-utilizando-typescript-node-js-mongodb-redis-e-docker-no-azure-69f502cf7b44?source=search_post---------335,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Jan 14, 2020·1 min read
3-Parte: Criando a base da projeto
Dando continuidade a liberação dos módulos do meu curso: Criando API’s RESTful utilizando TypeScript, Node e mongoDB, hoje eu irei demonstrar como criar a base do nosso projeto.
Caso tenha interesse em ver os primeiros passos dessa serie, segue link de cada um deles abaixo:
Criação da base do projeto
Importando os pacotes NPM
Importando os @types
Abaixo você tem o comando para importar os pacotes NPM no seu projeto e os @types
Configurando o compilador do TypeScript
Link para documentação oficial do arquivo tsconfig.json: Documentação.
configuração default que eu utilizo nos meus projetos:
Bom é isso pessoal, no próximo post nós veremos como criar a nossa primeira rota.
Espero que tenham gostado e até a próxima pessoal :)
Enjoy your life
22 
22 
22 
Aqui você vai encontrar os principais conteúdos de tecnologia, design, dados e produto da XP Inc.
"
https://medium.com/vamp-io/canary-releasing-on-kubernetes-from-0-to-100-with-vamp-and-azure-container-service-7260167197d6?source=search_post---------336,"There are currently no responses for this story.
Be the first to respond.
Releasing containerised application workloads on Kubernetes is almost too easy, and Kubernetes comes with some powerful release patterns out of the box. There are also already some great resources out there describing interesting blue/green and canary release deployment scenarios with Kubernetes — the official docs have a section that proposes a canary release strategy and this write up also talks about a very similar scenario —
In essence, the proposed strategy in most cases is to spin up as much replicas as you need to reflect the user distribution you want, i.e. if you want to have 1:3 of your users hit the “canary” version of your app and 2:3 hit the “stable” version, you spin up two stable versions and one canary version. You then map these replicas to the same (ingress) service and you’re done.
This completely works and totally valid in small scale, stateless and very general situations. It has its weaknesses however:
So, if you want to take it up a notch and gain more flexibility in programmatic routing and workflow you’ll probably want to checkout Vamp. Luckily, getting up and running with Vamp and Kubernetes is incredibly easy and quick, especially when running on Azure Container Service as this enables some neat and out-of-the-box load balancer and endpoint integration.
In this post we’ll walk you through all the initial steps to get up and running.
Full disclosure: Our dev team is still finalising the full Kubernetes integration. 95% of Vamp’s feature work extremely nice with Kubernetes, but we have some open bugs that we still need to squash.
Vamp is an open source, self-hosted platform for managing (micro)service oriented architectures that rely on container technology. Vamp provides a DSL to describe services, their dependencies and required runtime environments in blueprints.
Vamp takes care of route updates, metrics collection and service discovery, so you can easily orchestrate complex deployment patterns, such as A/B testing and canary releases.
If you have your Azure account setup and credentials in place, use the following script to bootstrap a Kubernetes cluster and install Vamp. The steps in this script are described below.
Microsoft has done an excellent job in providing a very easy and quick Kubernetes setup with their Azure Container Services. It takes just a handful of commands to get going. You need an active Azure subscription and the Azure command line interface installed to run the below commands
Integrating Vamp into Kubernetes is made delightfully simple using our install script. It talks directly to kubectl and sets up Vamp and its dependencies. Read the full source of the install script here.
The script should finish with the following output and an SSH tunnel on port 8001, connecting you to your Kubernetes host on ACS.
Open a browser, navigate to http://localhost:8001/ui/ and go to the workloads tab. You will a see all Vamp components installed and running.
Vamp’s UI and API endpoints are available on the external IP and port defined in the Vamp service. Either get it using the following kubectl command or get in from the Kubernetes UI on the “Services” tab
In our example this means:
Vamp UI: http://13.93.81.196:8080/#/vamp/Vamp API: http://13.93.81.196:8080/api/v1/
We’re going to perform a simple canary release using the Vamp CLI. First install it and set the VAMP_HOST environment variable to Vamp’s address.
Use the following script to insert two Vamp blueprints.
Then deploy version 1.0.0 of our simple service…
…and check if our deployment is done.
Vamp integrates directly with Kubernetes LoadBalancer service types by setting up a service and external endpoint. We provide the selector io.vamp.gateway=simple_dep_9050 to filter for the right data where simpleDep is the name we gave to our deployment and 9050 is what we defined as our gateway in the blueprint for simpleservice version 1.0.0. If the EXTERNAL-IP shows <pending> please be patient as the environment bootstraps the necessary infrastructure. Luckily this only needs to happen once as the 9050 is our stable endpoint.
But it doesn’t end there. The Kubernetes LoadBalancer service is in turn now also integrated into Azure’s Loadbalancer, creating a load balancing rule and health probe, exposing our service to the internet in a reliable fashion.
Having said all that, our service is now reachable on 13.73.165.183:9050 , as reported by kubectl . Open a browser and you should get:
As you might have noticed, that next to the LoadBalancer service this deployment will also show up as a Kubernetes Deployment, Pod and ReplicaSet. This demonstrates how Vamp uses the native scheduling and resource management of Kubernetes.
The deployment of 1.0.0 is done. We now merge version 1.1.0 into our existing deployment and expose it to 10% of traffic.
We can now have a look at the internal gateway that Vamp has setup and that allows us to migrate traffic to our new version. This internal gateway is completely separate from the external one described above, neatly separating the stable ingress endpoints and the internal dynamic routing.
Now let’s update the routing and assign 70% to version 1.0.0 and 30% to version 1.1.0.
Now, hitting our endpoint a couple of times should yield the following screen.
You can of course take much smaller steps then 70/30, as long as the number add up to 100. Also, there is no hard limit on the amount of services you split up.
Setting weights on gateways is just one way of doing canary releases. Using Vamp’s conditions you can influence traffic based on HTTP headers like Cookies, User-Agents etc. As this is not Kubernetes specific we won’t dive into that in this write up, but here are some links for further reading:
Vamp.io
58 
58 claps
58 
Vamp.io - Smart & stress free application releasing for modern cloud platforms.
Written by
Code and Product. Writing about solopreneurship, Javascript and containers. Founder at checklyhq.com
Vamp.io - Smart & stress free application releasing for modern cloud platforms.
"
https://medium.com/wso2-learning/wso2-deployment-reference-architecture-on-azure-33fe8f218cbf?source=search_post---------337,"There are currently no responses for this story.
Be the first to respond.
A reference deployment of WSO2 APIM and EI on Microsoft Azure cloud platform
WSO2 is an open-source, enterprise middleware platform that offers software products for API Management, Integration and Identity, and Access Management. WSO2 products can be deployed on infrastructure platforms such as
— Azure
— AWS
— GCP
In addition to these customer-managed infrastructure platforms, WSO2 also has a fully managed cloud version of the products as Software as a Service (SaaS) offering. Having said that, Azure is one of the upcoming cloud infrastructure platforms for deploying applications on the cloud. Hence we will be discussing how to deploy the WSO2 platform on top of the Azure.
Before moving on to the actual deployment details, let’s discuss some of the concepts of Azure that we will be using for the deployment architecture. Azure provides a plethora of services for application development and deployment. For this article, we will be using the below-mentioned services.
Azure Virtual Machines provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for applications. It provides a comprehensive computing environment with a choice of processor, storage, networking, operating system, and purchasing model. It offers computing resources available from a variety of geographical locations across the globe using its regions and zones. It also provides automatic scalability of computing resources within minutes.
Microsoft Azure services are available globally to run business applications at an optimal level. Users can choose the best region based on their needs based on
Regions and AZs allow users to utilize the cloud platform in the best possible way for their applications.
A region is a set of datacenters deployed within a latency-defined perimeter and connected through a dedicated regional low-latency network. Azure gives you the flexibility to deploy applications where you need to, including across multiple regions to deliver cross-region resiliency. Azure maintains multiple geographic regions, including
Azure opens new regions rapidly based on user demand.
An Availability Zone is a high-availability offering that protects your applications and data from datacenter failures. Availability Zones are unique physical locations within an Azure region. Each zone is made up of one or more datacenters equipped with independent power, cooling, and networking. To ensure resiliency, there’s a minimum of three separate zones in all enabled regions. The physical separation of Availability Zones within a region protects applications and data from datacenter failures. Zone-redundant services replicate your applications and data across Availability Zones to protect from single-points-of-failure. With Availability Zones, Azure offers industry best 99.99% VM uptime SLA.
An availability set is a logical grouping of VMs that allows Azure to understand how your application is built to provide for redundancy and availability. We recommended that two or more VMs are created within an availability set to provide for a highly available application and to meet the 99.95% Azure SLA. There is no cost for the Availability Set itself, you only pay for each VM instance that you create.
Azure Virtual Network (VNet) is the fundamental building block for your private network in Azure. VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks. VNet is similar to a traditional network that you’d operate in your own data center, but brings with it additional benefits of Azure’s infrastructure such as scale, availability, and isolation. Key use cases of VNets include
A subnet is a range of IP addresses in the VNet. You can divide a VNet into multiple subnets for organization and security. Each network interface(NIC) in a VM is connected to one subnet in one VNet. NICs connected to subnets (same or different) within a VNet can communicate with each other without any extra configuration. By default, there is no security boundary between subnets, so VMs in each of these subnets can talk to one another. However, you can set up Network Security Groups (NSGs), which allow you to control the traffic flow to and from subnets and to and from VMs.
A network security group (NSG) contains a list of Access Control List (ACL) rules that allow or deny network traffic to subnets, NICs, or both. NSGs can be associated with either subnets or individual NICs connected to a subnet. When an NSG is associated with a subnet, the ACL rules apply to all the VMs in that subnet. In addition, traffic to an individual NIC can be restricted by associating an NSG directly to a NIC.
NSGs contain two sets of rules: inbound and outbound. The priority for a rule must be unique within each set. Each rule has properties of protocol, source and destination port ranges, address prefixes, direction of traffic, priority, and access type. All NSGs contain a set of default rules. The default rules cannot be deleted, but because they are assigned the lowest priority, they can be overridden by the rules that you create.
Azure offers a choice of fully managed relational, NoSQL, and in-memory databases, spanning proprietary and open-source engines, to fit the needs of modern app developers. Infrastructure management — including scalability, availability, and security — is automated, saving you time and money. Focus on building applications while Azure managed databases make your job simpler by surfacing performance insights through embedded intelligence, scaling without limits, and managing security threats. The available database services include
Out of the above database types, Azure SQL database is a fully-managed database that provides the SQL server capabilities to the user.
Take advantage of fully managed file shares in the cloud that are accessible via the industry-standard SMB and NFS protocols. Azure file shares can be mounted concurrently by cloud or on-premises deployments of Windows, Linux, and macOS. Azure file shares can also be cached on Windows Servers with Azure File Sync for fast access near where the data is being used.
The term load balancing refers to the distribution of workloads across multiple computing resources. Load balancing aims to optimize resource use, maximize throughput, minimize response time, and avoid overloading any single resource. It can also improve availability by sharing a workload across redundant computing resources. Azure provides various load balancing services that you can use to distribute your workloads across multiple computing resources — Application Gateway, Front Door, Load Balancer, and Traffic Manager.
Azure Load Balancer is a high-performance, ultra low-latency Layer 4 load-balancing service (inbound and outbound) for all UDP and TCP protocols. It is built to handle millions of requests per second while ensuring your solution is highly available. Azure Load Balancer is zone-redundant, ensuring high availability across Availability Zones. It’s the single point of contact for clients. Load balancer distributes inbound flows that arrive at the load balancer’s front end to backend pool instances. These flows are according to configured load-balancing rules and health probes. The backend pool instances can be Azure Virtual Machines or instances in a virtual machine scale set.
Azure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. Traditional load balancers operate at the transport layer (OSI layer 4 — TCP and UDP) and route traffic based on source IP address and port, to a destination IP address and port.
Application Gateway can make routing decisions based on additional attributes of an HTTP request, for example URI path or host headers. For example, you can route traffic based on the incoming URL. So if /images is in the incoming URL, you can route traffic to a specific set of servers (known as a pool) configured for images. If /video is in the URL, that traffic is routed to another pool that’s optimized for videos.
As per this article, we will be deploying WSO2 API Manager and WSO2 Enterprise Integrator along with the respective Analytics components on Azure infrastructure. Each product and component is deployed with the minimum high availability.
— 2 worker nodes (active/passive)
— 1 dashboard nodes (active)
In addition to the WSO2 products, we will be utilizing the following Azure services
Let’s get on with the deployment architecture.
The preceding figure depicts the deployment architecture of WSO2 APIM, EI, and Analytics on the Azure platform. The deployment is designed so that it is deployed within a single Azure region. The entire deployment resides within a VNet inside the Azure cloud. There are private subnets created for WSO2 product deployment as well as for databases and deployment management tools (puppet).
A public subnet is created to host the publicly accessible components. This public subnet is considered as the DMZ and the following components are deployed within this subnet.
WSO2 API Manager deployment
Two instances of WSO2 APIM are deployed for high availability within an availability set. Azure availability set provides the required redundancy in case there is a failure of one node. For the deployment, the VM instance of type B2MS is used. This can be different based on the capacity requirements of the use case. The API gateway is exposed through the Azure classic LB which is deployed in the public subnet (DMZ). In addition to that, other administrative interfaces such as publisher, carbon, and admin portals are exposed through the Azure application gateway which is deployed in the public subnet and protected via firewall rules. This is accessed by the internal organization staff members who manage the deployment. For sharing the API definitions and throttling policies across APIM instances, Azure files is utilized by the APIM deployment. This deployment is connected to the Azure SQL database service running within the same VNet.
WSO2 Enterprise Integrator deployment
Two instances of WSO2 EI are deployed for high availability within an availability set. For the deployment, the EC2 instance of type B2MS is used. This can be different based on the capacity requirements of the use case. The carbon portal is exposed through the application gateway. The services that are running on WSO2 EI will be exposed to the API Manager and other systems via Azure classic load balancer. If needed, a separate classic LB can be setup within the private subnet for communicating to the EI nodes from APIM and other services. If there is a need to connect with on-premise systems such as databases, ERP, CRM from WSO2 EI, a VPN connection can be utilized as depicted in the above diagram.
WSO2 API Manager/EI Analytics deployment
The analytics component comes with 2 profiles called worker and dashboard. Worker profile receives the analytics events from gateways (APIM) and do the real-time processing of data and store the results in the analytics database. Then the dashboard profile reads the processed data from the database and visualizes it in dashboards. The worker requires 2 nodes of active/passive deployment for high availability and those are deployed in an availability set. The dashboard profile is okay to deploy as a single instance given the lower importance of the tool. All 3 nodes are connected to the database that is running on the same VNet but in a different private subnet. The analytics dashboard is exposed through the application gateway which is running on the public subnet. Azure VM type of F4s v2 is used for workers since it does heavy data processing as and when events are received from the gateways. A VM of type F2s v2 is used for the dashboard profile since it does not have heavy processing requirements.
Learn about WSO2 technology best practices from experts
25 
25 claps
25 
This publication contains best practices, examples and guidelines for WSO2 users.
Written by
Engineer | Author | Speaker | Associate Director @ WSO2
This publication contains best practices, examples and guidelines for WSO2 users.
"
https://itnext.io/easily-deploy-containers-to-azure-directly-from-your-desktop-16efebc87b21?source=search_post---------338,"Containers are now a mature solution providing an additional level of infrastructure abstraction. In many cases, containers can replace workloads traditionally powered by virtual machines.
In this blog, we are going to look at Azure Container Instances and showcase how fast and easy it is to deploy containers directly from your docker CLI to Azure.
If you would like to follow along, you will need to have Azure subscription, Azure CLI and Docker Desktop instance.
Azure Container Instances is a compute offering that bridges the gap between lightweight Azure Functions and more complex, but fully fledged Azure Kubernetes Service.
ACI is best suited for containerized workloads that can operate in isolation, simple apps, batch jobs including data science models, all kinds of tasks automation and integration scenarios.
We are going to deploy a sample web page. The idea is that with docker CLI and ACI we can rapidly prototype, test and deploy directly from docker command line!
Important node: this flow is only for testing purposes, in real code scenario you would have CI/CD pipeline deploying your app for you.
We are going to use bash, but the same is of course possible with powershell.
Docker CLI contains now build-in integration with Azure Container Instances through a context command. When using Azure CLI, you cat activate Azure Interactive by typing az interactive. This is an experimental feature of Azure CLI which gives you parameters completion and more!
First let’s setup variables and authenticate with Azure using docker CLI
This command is interactive and will prompt you to select subscription, resource group (create or select existing one) and location. Make sure to note resource group name if you create a new one, so later it’s easy to cleanup resources.
Now let’s deploy a test container!
We’ve see how easy it is to deploy a container group directly to Azure Container Instances. This could be very useful for testing purposes and quick inner development loop.
This blog barely scratches the surface of what Azure Container Instances can do and how to integrate developer workflow. In my opinion Azure Container Instances is one of the most flexible and powerful serverless offerings in Azure.
There are a lot of great blogs and tutorials to check if you are interested to learn more.
ITNEXT is a platform for IT developers & software engineers…
62 
62 claps
62 
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Opinions: Multi-cloud is real, Microservices are hard, Kubernetes is the future, CLIs are good. Me: Love jogging with my dog and learning new things.
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/microsoftazure/fast-loading-data-into-azure-sql-a-lesson-learned-72ff0bc6d0c8?source=search_post---------339,"There are currently no responses for this story.
Be the first to respond.
I’m preparing a series of post and samples on how to properly load data into Azure SQL using Azure Databricks / Apache Spark that I will start to publish very soon, but I realized today that there is a pre-requisite that in many cases, especially by developers new to the data space, is overlooked: good table design.
Wait! If you’re not a Apache Spark user you might think this post is not for you. Please read on, it will be just a couple of minutes, and you will find something help also for you, I promise.
By good table design, I don’t mean, in this case, normalization, research of the best data type or any other well-known technique…no, nothing like that. They are still absolutely useful and encouraged, but let’s leave them aside for now, and let’s focus on something much simpler.
Simpler but that, in the case I used to build the aforementioned samples, had an impact of 300%. Right, 300%. By changing a very simple thing I could improve (or worsen, depending on where you are starting from) performance by 3 times.
The focal point to understand is that if we overlook something, maybe very small, when data has grown that little mistake or less-than-good decision may have its overhead applied to all the rows in your table. As a result of this it may happen that something negligible in terms of performance impact will instead become very bad once the data amount start to increase. This is the main challenge with database. Data grows and changes.
In the specific sample I want to discuss now, it turns out that using a correct data type for storing string data matters a lot. If you are a .NET developer you probably are already used to take advantage of StringBuilder instead of simpler String objects if you need to create or manipulate a string for several thousand or millions of times. StringBuilder is much more optimized to do this, even if you could obtain the same result just by using String objects.
In Azure SQL you can choose to use varchar(max) or varchar(n). With varchar(max) you can store up to 2GB of data. With varchar(n) you can store up to n bytes and anyway no more than 8000. The same logic applies to nvarchar (with limit now set to max 4000 chars as they use 2 bytes per char), but in this case strings will use UTF-16 encoding.
If you are a developer approaching the data world, you might have the impression that specifying the size of a string is something that only in ancient times made sense. More or less like a nibble, if you ever heard of this curious half-byte thing.
Now that we have gigabytes almost for free, specifying the string size should not matter. With this mindset, a table like the following would look absolutely adequate (for simplicity I’m using a table from TPC-H benchmark, so pardon the weird columns names):
All the strings in the table are treated like strings in any other language (C#, Python and similar) where you assume strings can have an almost infinite length.
In the database space things are quite different. To reduce I/O to a minimum (as accessing data outside RAM is still the slowest operation you can think of), several optimizations are done, so that with just one I/O you can read many rows at once. That’s why, in fact, the concept of data page exists.
If a string is known not to be potentially infinite, some other optimizations can be done. So, if we know what could be the potential maximum — or even the exact size — of a string, we could specify it, helping the database to optimize things a bit. For example, like in the following table, which is exactly the table shown before but with more precise string type definition:
This last table is much better from a physical design point of view. And you can see this yourself by loading data using Azure Databricks, for example.
Loading data into the first table will require something like 7 minutes for 9 GB of data, while on the second, exactly the same data will require only 2.5 minutes.
Loading data into a table that has been better designed (from a physical modeling point of view) is 3 time faster then loading data in a table not so well optimized.
Given that we only had to correctly set maximum string length, I’d say the optimization is totally worth the effort!
I did my experiments on Azure SQL Hyperscale Gen5 8vCore and with Azure Databricks 6.6 (Spark 2.4.5, Scala 2.11), 4 Workers each with 4 nodes, for a total of 16 workers that were loading data in parallel into Azure SQL.
So, the first step to make sure you can load your data as fast as possible is to create the table using the most suitable data type, especially when you are dealing with strings. In the past we used nibbles to spare memory as much as possible, as it was scarce.
Now we live in the time of plenty, but we generate and operate on huge amounts of data, so every byte wasted means wasting CPU cycles, network bandwidth, I/O bandwidth, cache and memory. Wasting something tiny — in this specific case the bad design impacted only for 0.000005 seconds on a single row — for 59,692,838 times (that’s the number of rows in my sample) still result in a huge impact, not matter what. So, let’s start by not doing that: you’ll never know when your table will reach the size where even a single byte will be critical, so better be prepared.
There is also another nice aspect of this little optimization we just have done. As a result of a better physical design, reads will also be much faster too, exactly for the same reasons explained before. So spending just a few minutes to think about our model instead of just throwing some code here and there is a win-win.
Keep this in mind for your next table!
—
Photo by panumas nikhomkhai from Pexels
Any language.
49 
49 claps
49 
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@michaelhenderson/how-to-download-your-source-code-from-azure-app-service-59c848752b0f?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
Michael Henderson
Oct 22, 2018·3 min read
If you are like me you most likely have all of your eggs in one basket. For example, I have one laptop that is my daily driver. That laptop has everything I need on it including all the repositories that I contribute to. Well, the hard drive in my laptop decided to die the other day and I was not able to get my source code to one particular project that I have spend a lot of time on. The project I am speaking of is an API that I am running in Azure.
This API is one that I did not have under source control. I would make changes/update…
"
https://medium.com/@tsuyoshiushio/terragrunt-quick-start-with-azure-4a9a09ab2e21?source=search_post---------341,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Jan 11, 2020·5 min read
Terragrunt is a great tool to help our terraform files keep DRY. However, the tutorial is written for AWS. For my learning, I tried to do it for Azure.
If you want to see the original, please see the Quick start.
terragrunt.gruntwork.io
Download two binaries and put it to your path environment variables.
Download/Install AzureCLI then login it to your subscription. This command will open your browser. For Linux, use --use-device option.
Terragrunt is a thin wrapper that provides extra tools for keeping your configurations DRY, working with multiple Terraform modules, and managing remote state.
A project that I’m joining has several modules and environment. I can see the duplication between the modules and environment. terragrunt might help.
Where is the sample repo?
You can find a whole sample code terragrunt with Azure.
github.com
The repo has two modules. resource-group and virtual-network . You want to share something between two modules for making these D.R.Y. You might notice thatterragrunt.hcl These are the config files for terragrunt.
In this example, I’m sharing two things between two modules.
If you want to share the backend configration, , you can edit terragrunt.hcl under stage directory. That has two modules underneath.
In this example, I use storage account for the backend. You need to create a storage account by your self on Azure Portal. Please edit this file to fit your storage account name and resource group name.
backend is a feature to store terraform state in somewhere. We use storage account for Azure.
stage/terragrunt.hcl
stage/resource-group/terragrunt.hcl
As an example, let’s see the resource-group module configuration. This config is simply inherit the parent folder’s terragrunt.hcl
stage/resource-group/main.tf
The point is terraform part. You can see there is no config at all. terragrunt inject the info from terragrunt.hcl configuration.
Terragrunt is a wrapper of terraform. So all of the command is just by-passed with terraform with the configuration change.
if you want to do terrafrom plan you can simply do terragrunt plan apply is the same as well.
Then aresource_group is might created.
Sometimes, you might want to share some input variables among the modules.
Currently, resource-group module has these two variables.
Go to the parent configration.
stage/terragrunt.hcl
You can find these lines for share the input valuables.
Modules should be small, even if the whole infrastructure is big. However, small modules introduce duplication. This is the structure that is introduced at the original tutorial.
Terragrunt help you to remove the duplication with immutable, versioned, artifacts.
Create a re-usable module. It also might need the config of backend. However, we don’t need specify the specific values. I create a sample module repo.
github.com
The repo’s structure is
app/main.tf
Sample module definition. As you can see backend has no configration, and requires variables for project specific parameters.
Push your module somewhere github. I push my https://github.com/TsuyoshiUshio/infrastructure-module repo with Tag.
I made some mistakes, so, now it is version v0.0.3
For the project side, create the same structure as the module repo. I add the app directory with terragrunt.hcl
terragrunt.hcl
You can re-use the module with version.
Go to the app directory and plan/apply it. You can re-use the module with version!
You will see it is success fully works. and You will find .terragrunt-cache directory is created under app . Don’t forget to add it to .gitignore
I just want to share a sample with Azure.
github.com
For more details, please refer terragrunt .
terragrunt.gruntwork.io
Enjoy deployment.
Senior Software Engineer — Microsoft
22 
2
22 
22 
2
Senior Software Engineer — Microsoft
"
https://itnext.io/introduction-to-azure-functions-using-terraform-eca009ddf437?source=search_post---------342,"In this article I will provide a quick introduction to Azure Functions and Terraform with a hands-on example which is quite easy to follow.
There are many articles out there about Azure Functions, but most fails to explain how to automate their deployment for a real world scenario using CI/CD pipelines.
"
https://medium.com/bb-tutorials-and-thoughts/how-to-build-ci-cd-for-python-azure-functions-using-azure-devops-7087a76e535b?source=search_post---------343,"There are currently no responses for this story.
Be the first to respond.
An Azure Function is a simple way of running small pieces of code in the cloud. You don’t have to worry about the infrastructure required to host that code. You can write the Function in C#, Java, JavaScript, PowerShell, Python, or any of the languages that are listed in the…
"
https://blog.helium.com/from-zero-to-azure-iot-in-five-minutes-f32d74c82b9b?source=search_post---------344,"Can we go from unboxing new Helium hardware to sending hardware-encrypted data to an Azure IoT Hub in under five minutes?
Securing your devices and sensors should be top of mind when considering any IoT wireless and cloud platform. Sound, proven hardware-based encryption and authentication practices specifically are what should set an IoT service apart. X-509 certificates (and their sister mechanism JSON Web Tokens) are the standard when it comes to device-level authentication for big-ticket cloud providers. Both Azure IoT and AWS IoT both use X-509 certificates as their gold standard when authenticating devices. And Google Cloud IoT does the same with JSON Web Tokens.
But building full X-509 cert-based encryption and authentication into a low-power embedded device and securely managing the lifecycle of these certificates can be cumbersome for even the most capable developers. For starters, here’s how Azure IoT recommends you implement security at the device level. To really drive home how hard this is to do on your own with a wireless device like the Atom on a LPWAN like Helium, your end device needs a full TLS stack, a client certificate, and a MQTT client. And you need to keep the MQTT connection alive between the device and the cloud. The radio usage alone will crush your battery and this assumes your sensor is driven by a sufficient MCU. The processing and networking requirements are enough that the official Azure IoT Arduino Library doesn’t support the Arduino UNO. And users are having issues with it (to say the least).
So, to abstract this type of complexity, we built Helium Channels. Channels are prebuilt connections to cloud services like Azure IoT, AWS IoT, and Google Cloud IoT. (You can see a full list of Helium Channels available here.) They handle all the heavy lifting: from hardware secured certificate generation and management to integration with cloud device management APIs and everything in between.
To demonstrate the power of Channels, we thought we would see just how quickly a developer could go from unboxing their Helium Starter Kit to sending fully-encrypted and authenticated data to Azure IoT using an Helium Atom riding on an Arduino UNO. Can we do it in five minutes? Let’s give it a shot. And jump to the bottom if you want to get your hands on the hardware to replicate this yourself.
Want to replicate this yourself? Here’s what you need:
Building the world’s first decentralized wireless network
162 
2
162 claps
162 
2
Written by
VP of Business Development at @helium; Basho OG. - mark@helium.com
Building the world’s first decentralized wireless network
Written by
VP of Business Development at @helium; Basho OG. - mark@helium.com
Building the world’s first decentralized wireless network
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/aspnetrun/deploying-net-microservices-to-azure-kubernetes-services-aks-and-automating-with-azure-devops-c50bdd51b702?source=search_post---------345,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mehmet Özkaya
Jan 13, 2021·6 min read
In this article, we’re going to learn how to Deploying .Net Microservices into Kubernetes, and moving deployments to the cloud Azure Kubernetes Services (AKS) with using Azure Container Registry (ACR) and last section is we will learn how to Automating Deployments with Azure DevOps and GitHub.
The image above, you can find the steps of our article structure.We’re going to containerize our microservices on docker environment, and push these images to the DockerHub and deploy microservices on Kubernetes. As the same setup, we are going to shifting to the cloud for deploying Azure Kubernetes Services (AKS) with pushing images to Azure Container Registry (ACR).
Also we will cover additional topics that;
I have just published a new course — Deploying .Net Microservices with K8s, AKS and Azure DevOps.
In this course, we’re going to learn how to Deploying .Net Microservices into Kubernetes, and moving deployments to the cloud Azure kubernetes services (AKS) with using Azure Container Registry(ACR) and last section is we will learn how to Automating Deployments with CI/CD pipeline of Azure DevOps and GitHub.
See the overall picture. You can see that we will have 3 microservices which we are going to develop and deploy together.
First of all, we are going to develop Shopping MVC Client Application For Consuming Api Resource which will be the Shopping.Client Asp.Net MVC Web Project. But we will start with developing this project as a standalone Web application which includes own data inside it. And we will add container support with DockerFile, push docker images to Docker hub and see the deployment options like “Azure Web App for Container” resources for 1 web application.
After that we are going to develop Shopping.API Microservice with MongoDb and Compose All Docker Containers. This API project will have Products data and performs CRUD operations with exposing api methods for consuming from Shopping Client project.We will containerize API application with creating dockerfile and push images to Azure Container Registry.
Our API project will manage product records stored in a no-sql mongodb database as described in the picture. we will pull mongodb docker image from docker hub and create connection with our API project.
At the end of the section, we will have 3 microservices whichs are Shopping.Client — Shopping.API — MongoDb microservices.
As you can see that, we have
And the last step, we are focusing on automation deployments with creating CI/CD pipelines on Azure Devops tool. We will develop separate microservices deployment pipeline yamls with using Azure Pipelines.When we push code to Github, microservices pipeline triggers, build docker images and push the ACR, deploy to Azure Kubernetes services with zero-downtime deployments.
By the end of this articles, you’ll learn how to deploy your multi-container microservices applications with automating all deployment process seperately.
This is the introduction of the series. This will be the series of articles. You can follow the series with below links.
Get the Source Code from AspnetRun Microservices Github — Clone or fork this repository, if you like don’t forget the star :) If you find or ask anything you can directly open issue on repository.
Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly.
Advantages of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. Docker provides for automating the deployment of applications as portable, self-sufficient containers that can run on the cloud or on-premises. Docker containers can run anywhere, in your local computer to the cloud. Docker image containers can run natively on Linux and Windows.
A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application.
When using Docker, a developer develops an application and packages it with its dependencies into a container image. An image is a static representation of the application with its configuration and dependencies.
In order to run the application, the application’s image is instantiated to create a container, which will be running on the Docker host. Containers can be tested in a development local machines.
As you can see the images above, how docker components related each other.Developer creates container in local and push the images the Docker Registry. Or its possible that developer download existing image from registry and create container from image in local environment.
Developers should store images in a registry, which is a library of images and is needed when deploying to production orchestrators. Docker images are stores a public registry via Docker Hub; other vendors provide registries for different collections of images, including Azure Container Registry. Alternatively, enterprises can have a private registry on-premises for their own Docker images.
If we look at the more specific example of Application Containerization with Docker;
We will use all steps with orchestrating whole microservices application with docker and Kubernetes for the next articles ->
https://docs.docker.com/get-started/overview/https://docs.docker.com/get-started/https://medium.com/batech/docker-nedir-docker-kavramlar%C4%B1-avantajlar%C4%B1-901b37742ee0https://www.mediaclick.com.tr/tr/blog/docker-nedir-docker-ne-ise-yararhttps://www.docker.com/resources/what-containe
I’m currently working as a Software Architect. Focus on microservices architectures on .Net https://github.com/mehmetozkaya
See all (319)
30 
30 
30 
The best path to leverage your aspnet skills. Onboarding to .Net Software Architect jobs. Developing production-ready enterprise .Net applications with applying latest architectures and best practices.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/build-a-serverless-app-using-go-and-azure-functions-c4475398f4ab?source=search_post---------346,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Webhook backend is a popular use case for FaaS (Functions-as-a-service) platforms. They could be used for many use cases such as sending customer notifications to responding with funny GIFs! Using a Serverless function, it’s quite convenient to encapsulate the webhook functionality and expose it in the form of an HTTP endpoint. In this tutorial you will learn how to implement a Slack app as a Serverless backend using Azure Functions and Go. You can extend the Slack platform and integrate services by implementing custom apps or workflows that have access to the full scope of the platform allowing you to build powerful experiences in Slack.
This is a simpler version of the Giphy for Slack. The original Giphy Slack app works by responding with multiple GIFs in response to a search request. For the sake of simplicity, the function app demonstrated in this post just returns a single (random) image corresponding to a search keyword using the Giphy Random API. This post provides a step-by-step guide to getting the application deployed to Azure Functions and integrating it with your Slack workspace.
In this post, you will:
The backend function logic is written in Go (the code is available on GitHub. Those who have worked with Azure Functions might recall that Go is not one of the language handlers that is supported by default. That’s where Custom Handlers come to the rescue!
In a nutshell, a Custom Handler is a lightweight web server that receive events from the Functions host. The only thing you need to implement a Custom Handler in your favorite runtime/language is: HTTP support! This does not mean that Custom handlers are restricted to HTTP triggers only — you are free to use other triggers along with input and output bindings via extension bundles.
Here is a summary of how Custom Handlers work at a high level (the diagram below has been picked from the documentation)
An event trigger (via HTTP, Storage, Event Hubs etc.) invokes the Functions host. The way Custom Handlers differ from traditional functions is that the Functions host acts as a middle man: it issues a request payload to the web server of the Custom Handler (the function) along with a payload that contains trigger, input binding data and other metadata for the function. The function returns a response back to the Functions host which passes data from the response to the function’s output bindings for processing.
Before we dive into the other areas, it might help to understand the nitty gritty by exploring the code (which is relatively simple by the way)
Let’s look at the how the app is setup. this is as defined in the doc
cmd/main.go sets up and starts the HTTP server. Notice that the /api/funcy endpoint is the one which the Function host sends the request to the custom handler HTTP server.
All the heavy lifting is done in function/function.go.
The first part is to read the request body (from Slack) and ensure its integrity via a signature validation process based on this recipe defined by Slack.
Once we’ve confirmed that the function has indeed being invoked via Slack, the next part is to extract the search term entered by the (Slack) user
Look up for GIFs with the search term by invoking the GIPHY REST API
Un-marshal the response sent back by the GIPHY API, convert it into a form which Slack can make sense of and return it. That’s it !
Check the matchSignature function if you're interested in checking the signature validation process and look at slack.go, giphy.go (in the function directory) to see the Go structs used represent information (JSON) being exchanged between various components. These have not been included here to keep this post concise.
Alright! So far, we have covered lots of theory and background info. It’s time to get things done! Before you proceed, ensure that you take care of the below mentioned pre-requisites.
Please note down your GIPHY API key as you will be using it later
The upcoming sections will guide you through the process of deploying the Azure Function and configuring the Slack for the Slash command.
Start by creating a Resource Group to host all the components of the solution.
Start by searching for Function App in the Azure Portal and click Add
Enter the required details: you should select Custom Handler as the Runtime stack
In the Hosting section, choose Linux and Consumption (Serverless) for Operating system and Plan type respectively.
Enable Application Insights (if you need to)
Review the final settings and click Create to proceed
Once the process is complete, the following resource will also be created along with the Function App:
Clone the GitHub repo and build the function
GOOS=linux is used to build a Linux executable since we chose a Linux OS for our Function App
To deploy, use the Azure Functions core tools CLI
Once you’ve deployed, copy the function URL that’s returned by the command — you will use it in subsequent steps
This section will cover the steps you need to execute to setup the Slack application (Slash command) in your workspace:
Sign into your Slack Workspace and start by creating a new Slack App
Click on Create New Command to define your new Slash Command with the required information. Please note that the Request URL field is the one where you will enter the HTTP endpoint of function which is nothing but the URL you obtained after deploying the function in the previous section. Once you’re done, hit Save to finish.
Once you’re done creating the Slash Command, head to your app’s settings page, click the Basic Information feature in the navigation menu, choose Install your app to your workspace and click Install App to Workspace — this will install the app to your Slack workspace to test your app and generate the tokens you need to interact with the Slack API. As soon as you finish installing the app, the App Credentials will show up on the same page.
Make a note of your app Signing Secret as you’ll be using it later
… make sure to update the Function App configuration to add the Slack Signing Secret (SLACK_SIGNING_SECRET) and Giphy API key (GIPHY_API_KEY) - they will be available as environment variables inside the function.
From your Slack workspace, invoke the command /funcy <search term>. For e.g. try /funcy dog. You should get back a random GIF in return!
Just a recap of what’s going on: When you invoke the /funcy command in Slack, it calls the function, which then interacts Giphy API and finally returning the GIF to the user (if all goes well!)
You may see timeout error from Slack after the first invocation. This is most likely due to the cold start where the function takes a few seconds to bootstrap when you invoke it for the very first time. This is combined with the fact that Slack expects a response in 3 seconds - hence the error message.
There is nothing to worry about. All you need is to retry again and things should be fine!
Clean up: Once you’re done, don’t forget to delete the resource group which in turn will delete all the resources created before (Function app, App Service Plan etc.)
Now, there is nothing stopping you from using Go for your Serverless functions on Azure! I hope this turns out to be a fun way to try out Custom Handlers. Let us know what you think.
ITNEXT is a platform for IT developers & software engineers…
57 
57 claps
57 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@maarten.goet/protecting-against-malicious-payloads-over-dns-using-azure-sentinel-b16b41de52fd?source=search_post---------347,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 26, 2019·7 min read
No matter how tightly you control your network, you probably allow DNS queries and UDP/53 traffic on your network.
Bad actors can abuse this to establish a stealthy command & control (C2) channel and/or exfiltrate data using DNS tunneling.
Azure Sentinel can help detect these types of attacks, and provide insights in the various stages of the kill chain of this attacker.
Delivering a payload over DNS
Samy Baiwir recently published a project on GitHub called DNSlivery that aims to deliver payloads over DNS. It is a lightweight solution built on Python and the Scapy library.
No need for a full-fledged DNS server, DNSlivery will listen on UDP/53 and serve the payload through TXT records. Here’s the GitHub repository:
Use DNSlivery to bootstrap DNS tunneling
How is DNSlivery different from for instance PowerDNS? With DNSlivery there is no need for a client on the target, you can just use native PowerShell in the operating system.
However, this does mean that it is one-way communication. But because it will not touch on disk, it can help bootstrap the next phase of your attack, DNS tunneling:
“Even though more complete DNS tunneling tools already exist (fi: dnscat2 and iodine), they all require to run a dedicated client on the target. The problem is that there is probably no other way then DNS to deliver the client in such restricted environments. In other words, building a DNS communication channel with these tools require to already have a DNS communication channel.
In comparison, DNSlivery only provides one-way communication from your server to the target but does not require any dedicated client to do so. Thus, if you need to build a reliable two-way communication channel over DNS, use DNSlivery to deliver the client of a more advanced DNS tunneling tool to your target.”
Setting up the DNS server
You will of course need a domain name and be able to change and administer the NS records. Point the NS record(s) to the external IP of the machine you’ll be using for this.
To set up the rogue DNS server itself you need a Linux operating system instance. You will also need Python3 and the right version of the Scapy library (v2.4.0).
DNSlivery makes things easy for you, just run these commands:
Create a directory on disk that has the file that contains the payload you want to serve over DNS. In this sample we’ll be serving “atp-cat.txt” with an ASCII picture of ATP cat.
Run the following command:
PRO TIP: If you’re getting errors that Python can’t load the Scapy module, make sure that sudo has the same environment variables as your current user. If not, use this command to solve that, before your run DNSlivery: alias sudo=’sudo env PATH=$PATH’
Consuming the payload on the target
As mentioned earlier, DNSlivery is different from other solutions in that it does not require a dedicated client. The content can be consumed by Powershell out of the box.
On the target, start by retrieving the launcher of the desired file by requesting its dedicated TXT record. The following three launchers are supported:
Copy and paste the response into the PowerShell console again, to retrieve the payload over DNS:
How can I defend against these types of attacks?
While most companies will have some form of network security solution in place that might already trigger on these types of attacks, Azure Sentinel can also play a role in detecting this malicious intent.
Looking at the Security eventlog on this specific server we find an event with ID 4688 that shows us the execution of the initial ‘dnslookup’:
If we execute a Kusto (KQL) query on the Log Analytics workspace that this agent is connected to, we quickly find the event:
Capturing the malicious transfer itself
When we dig further, there is no logged event in the Windows security eventlog on the next stage of the attack that receives the multi-part base64 encoded payload. Because it uses Invoke-Expression (IEX) it does not spawn a new (child) process and therefore no extra event is created in the security eventlog.
However, we can use the DNS client logging in Windows. That is not enabled by default, but enterprises could (and should?) enable this:
You will see the following events appear in the DNS client log:
PRO TIP: Type 16 is the TXT record in DNS. There is a Wikipedia page that outlines all the various types and Type ID’s: https://en.wikipedia.org/wiki/List_of_DNS_record_types
Collecting the Powershell log
Going into Workspace Settings in Azure Sentinel, you will find Advanced Settings of the Log Analytics workspace associated and it will allow you to collect the DNS client log:
It will take a couple of minutes to start gathering and processing this new log, but directly afterwards you can query the data in Kusto format, returning the malicious command:
Azure Sentinel
The next step is to create a Case trigger in Azure Sentinel. This can be done through the ‘Analytics’ section:
Here are the properties you can use:
PRO TIP: make sure you map the entities as part of the rule creation — this is a very important step since this will enable the investigation experience that will be released soon as part of Azure Sentinel!
The wizard has an Alert simulation feature that shows you if the specified query and scheduling properties will return events and if they will “meet the bar” (red line):
Azure Sentinel cases
Now that we created an advanced alert rule in Azure Sentinel, it will generate cases that you can assign and use to deeply investigate:
A case can include multiple alerts. It’s an aggregation of all the relevant evidence for a specific investigation:
PRO TIP: clicking the number under “Hits” (in this example: 110) will bring you to the actual query to see the events that triggered the detection.
In the Entities tab, you can see all the entities that are involved:
Conclusion
A potentially better approach for enterprises is to block DNS / port 53 traffic at the edge by default. This ensures that all port 53 traffic has to go out via the corporate DNS servers, providing a central point for logging. At that point it is sufficient to do centralized logging on the DNS server itself.
Without central logging, or if you do allow DNS traffic at the edge, client-level logging is valuable for detecting DNS tunneling attacks. Client-level logging also means you directly get the name of the impacted host in the logs, which is an added value for Threat Hunting.
While DNS client logging wasn’t there out-of-the-box, Azure Sentinel makes it easy to start detecting the DNS attack vector.
Happy hunting!
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
13 
13 claps
13 
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tsuyoshiushio/configuring-microsoft-graph-bindings-for-azure-functions-with-b2c-60fe0b63e81a?source=search_post---------348,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
Feb 17, 2018·4 min read
I had a hackfest with a customer last week. I really enjoyed. We use Microsoft Graph bindings with Durable Functions. We successfully fetch a token from the bindings. I’d like to share how to do it.
On this hackfest, we wanted to access Graph API for Azure Active Directory B2C. The code is very easy. However, the configuration was a little bit confusing for me.
If you want to use Graph bindings for fetching a token for B2C Graph API, you need to create an App Registration. But it should be created by someone who is in the Azure AD B2C directory.
For example, I usually login Azure by ushio@mydomain.com. I create an Azure AD B2C with a tenant named “someorganization.onmicrosoft.com”. I can switch directory between MyDomain and SomeOrganization. However, you can’t create App Registration. You are the Azure AD user. Not Azure AD B2C user. You need to create a new user for the Azure AD B2C on the “someorgranizaiton.onmicrosoft.com” tenant. Then logout the Azure and login with the new Azure B2C user (in this example, chirs@someorganization.onmicrosoft.com) then you need to create the App Registration by the chirs account . If you create it by ushio@mydomain.com, the App Registration doesn’t work for this use case.
Create a new admin account. Go to Azure Active Directory > Users > All users > New user. You need to create a user as admin. Then Login Azure using the new user.
Login as a B2C user (on the example, chirs account) on Azure. Go to Azure Active Directory > App registrations > New application registration
Coin it and Choose “Web app / API” for application type. Sign-on URL is not used for this scenario. You can specify any url.
Choose the App Registration then go Settings > Required permissions > Windows Azure Active Directory
I want to read Graph API of B2C. Enable Read directory data then Save, then Grant Permission. It give the permission to the App Registration.
You need to create a key on the App Registration page. Go to Settings > Keys and create a key (password). Then get the Application ID
Then configure the Function App to fetch the token for the Graph API of B2C. Re-login at the original user to Azure and create a function app. Then Go to Authentication / Authorization on the Platform features
Then Click Azure Active Directory
Then set the App Registration values. Client ID = Application ID. Client Secret = Key. Issuer Url is https://sts.windows.net/{YOUR_TENANT_ID_OF_B2C}
You can also get the Issuer Url from this url.
https://login.microsoftonline.com/{YOUR_B2C_ORGANIZATION}/.well-known/openid-configuration
for example, my demo organization is like this.
https://login.microsoftonline.com/someorganication.onmicrosoft.com/.well-known/openid-configuration
Then you can find the “issuer” attribute.
Done the settings.
Binding code is quite easy, all you need to do is define the Token. This is the binding part to fetch the token.
Then call the Graph API using the token.
This is the whole code.
Thank you for the hackfest team mate. Especially, Naohiro Fujie for teaching me about the B2C settings and Chris Gillum for the writing bindings.
I enjoy hacking with you guys. Let’s have another one.
Senior Software Engineer — Microsoft
See all (200)
30 
4
30 claps
30 
4
Senior Software Engineer — Microsoft
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@gmusumeci/how-to-build-step-by-step-a-multi-cloud-website-running-on-aws-azure-and-google-cloud-with-39dc98436891?source=search_post---------349,"Sign in
There are currently no responses for this story.
Be the first to respond.
Guillermo Musumeci
Dec 4, 2019·14 min read
Running a web site in a single cloud provider is not complex.
We can deploy a web site with a load balancer in a popular cloud provider such as Amazon Web Services (AWS), Microsoft Azure or Google Cloud Platform (GCP) in just a few minutes, using…
"
https://blog.mapbox.com/bringing-the-mapbox-vision-sdk-to-microsoft-azure-iot-platform-b4751c62d851?source=search_post---------350,"By: Eric Gundersen
New Mapbox blog posts are now published on www.mapbox.com/blog. Leave your email to receive email updates when new posts are published or subscribe to the new RSS feed.
Our newly announced Vision SDK integrates with the Microsoft Azure IoT platform. This partnership improves the driving experience inside the vehicle and generates road data on the backend to power analytic solutions for smart cities, insurance companies, and more.
“The intelligent cloud and intelligent edge bring a wide range of possibilities for the future of smart cities, transportation, public safety and more. By integrating Mapbox’s Vision SDK with Azure IoT Hub, developers will have the power of Microsoft’s global-scale cloud platform and advanced AI services to ingest data in real-time.” — Tara Prakriya, Group Program Manager, Microsoft Azure at Microsoft Corp.
The future of location will be building live maps in real time from distributed sensor networks embedded in vehicles and mobile devices at scale. The Vision SDK runs neural networks directly on a user’s mobile device or embedded hardware within a vehicle to segment the environment and detect discrete features like other vehicles, pedestrians, speed limits, construction signs, crosswalks, vegetation, and more.
We’re integrating the open sourced Azure IoT Edge Runtime, which provides custom logic, management, and communications functions for edge devices. Events detected from the Vision SDK integrated with Azure IoT Edge help developers build responsive applications that provide immediate feedback to the driver and stream semantic event data into Azure Cognitive Services for analysis on the back end.
Developers across a range of industries can securely send collision incidents to an insurance platform, for example; or deliver heavy traffic or blocked roadway alerts to a dispatch network. If businesses want to get granular, developers can send regular reports of activity at a crossing intersection to a business intelligence platform to optimize route paths.
For now, the Vision SDK is only available in private beta for select partners. We will be making it publicly available to everyone in September. Sign up now to get access as soon as it’s available .
www.mapbox.com
no passengers on the battleship
32 
32 claps
32 
Written by
mapping tools for developers + precise location data to change the way we explore the world
the official Mapbox blog
Written by
mapping tools for developers + precise location data to change the way we explore the world
the official Mapbox blog
"
https://medium.com/hackernoon/using-ngrok-with-azure-functions-7e209e96538c?source=search_post---------351,"There are currently no responses for this story.
Be the first to respond.
With things like the Azure Functions Cli and Azure Functions tools for Visual Studio you get the full development and debugging story locally on your machine. This is great as you can iterate and test quickly without the need to push the code to the cloud first, the drawback of this is that you can’t do incoming webhooks from. 3:rd party services, i.e. GitHub can’t access your locally running function.
But what if I said there’s a way you have your cake and eat it, wouldn’t that be great?
Introducing ngrok, ngrok is a tool and a service that will let you securely inspect and tunnel traffic to your local computer. It’s a free service with paid plans that will give you extra features like custom and reserved domains, IP address whitelisting etc.
ngrok is available cross plattform for MacOS, Windows, Linux and FreeBSD and it’s just a single binary you can download and unzip from ngrok.com/download. If you’re running Chocolatey on Windows like me, then it’s just a simple command way to get it installed:choco install -y ngrok.portable
Using ngrok is very straightforward, in general you launch the tool with which protocol and port your local service is listening on. ngrok http 8080
ngrok will launch and the forwarding urls is what you use to access your local service from the Internet.
You can find out which local port your Azure Functions by looking at the output of when the host starts, you can also specify the port using the port switch when launching the functions hostfunc host -p 8080
By default, ngrok will forward it’s temporary domain as host header to the locally running service, but as The Azure Functions host only listens to the hostname “localhost” we’ll need to override the default behavior using the host header switchngrok http 8080 --host-header localhost
To use your locally running function externally you just replace the base url provided by the function host to the temporary url provided by ngrok i.e.http://localhost:8080 becomes https://tempsubdomain.ngrok.io.
So say you have a GitHub webhook called GithubWebhookCSharp its local url will be http://localhost:8080/api/GithubWebhookCSharp and it’s external url will be https://tempsubdomain.ngrok.io/api/GithubWebhookCSharp.
Which you then could set up as i.e. a GitHub webhook
now when GitHub webhook triggers it’ll tunnel through ngrok and its payload will be delivered to your locally otherwise externally inaccessible function
On off the real killer features of ngrok is that it provides a local web interface, you’ll find it’s url in the tool’s output
This interface provides deep insight of all traffic that travels through the ngrok tunnel, you can see the response/request body and headers, it also lets you replay a request as many times as you like without needing to trigger an event on the external service, which is really useful when debugging, iterating over an implementation and fixing bugs!
Using a service like ngrok is really powerful and can ultimately speed up the development process.
#BlackLivesMatter
20 
1
20 claps
20 
1
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Partner & Technical fellow at WCOM AB. Microsoft Azure & Developer Technologies MVP. Been coding since I 80’s (C128 & Amiga). Father of 2, husband of 1.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/hackernoon/kubernetes-adventures-on-azure-part-2-windows-cluster-and-trick-for-scaling-pods-27e769edde15?source=search_post---------352,"There are currently no responses for this story.
Be the first to respond.
Part 3 available here: Kubernetes Adventures on Azure — Part 3 (ACS Engine & Hybrid Cluster)
In Part 1 of this series, we have seen how to create a Linux Kubernetes cluster on Azure Container Services.
Today I will try creation of a Kubernetes cluster but with Windows as nodes instead of Linux. Obviously Master will always be Linux.
This time I will follow Deploy Kubernetes cluster for Windows containers step by step and then play with the newly created cluster.
Let’s start with the usual creation of a resource group for this test so that we can easily group all cloud artifacts in it and delete everything on the fly at the end. ARM has been a great addition to Microsoft Azure.
Cluster up and running! Awesome!
Before proceeding, open Kubernetes Dashboard to check what’s going on in the cluster using a browser.
Let’s start with the easy sample of IIS using windowsservercore instead of nanocore as described in Microsoft article. I will follow same steps to introduce concepts like pods and way to expose it once manually deployed. Note: I consider this bad because you end up with a pod, a service, but no controller between them as a Replica Set or a Deployment managing it. Later we will see how to “fix” this issue without downtime.
Create iis.json with following content:
We can now apply configuration to a new Pod named iis: kubectl apply -f iis.json
When you have IP you can try to browse there and you should see:
Now let’s try to scale out our iis pod. Wait… We can’t scale a pod directly. We need a ReplicaSet or a Deployment for this. How would you handle this?
Here I use a trick I found during my research that let you scale your pods without any downtime for final users. Probably there is a better way to do it, if so please add it to the comment.
If we knew what it was we were doing, it would not be called research, would it?Albert Einstein
Brief Explanation needed
The idea here is to create a new deployment that will create 2 new replica of our iis pod using the same label.
Note: during these steps, you can check your service using a browser and see that is up and running without problems.
What if we have a problem with a pod iis-1894189856–9fwsp and we want to isolate it from the rest while keeping it running for debugging purposes?
to the following content and save file:
In Part 3 I will try to create an hybrid cluster with Windows ad Linux nodes!
#BlackLivesMatter
71 
71 claps
71 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Co-founder and CTO of @coreview, loving Kubernetes and Azure, Mensa member, Innovation lover, insatiable Reader. Helping the startup ecosystem in Italy to grow
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@ankitsharmablog/how-to-create-a-multi-language-translator-using-angular-and-azure-cognitive-services-5b0fcd9f592d?source=search_post---------353,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ankit Sharma
Feb 16, 2020·12 min read
In this article, we are going to create a multi-language translator using Angular and the Translate Text Azure Cognitive Service. This translator will be able to translate between more than 60 languages which are supported by the Translate Text API. We will supply the text to translate and the target language to our application and it will return the translated text and the detected language of the input text.
You can get the source code from GitHub.
We will use an ASP.NET Core backend for this application. The ASP.NET Core backend provides a straight forward authentication process to access Azure cognitive services. This will also ensure that the end user won’t have the direct access to the cognitive services.
Log in to the Azure portal and search for the cognitive services in the search bar and click on the result. Refer to the image shown below.
On the next screen, click on the Add button. It will open the cognitive services marketplace page. Search for the Translator Text in the search bar and click on the search result. It will open the Translator Text API page. Click on the Create button to create a new Translator Text resource. Refer to the image shown below.
On the Create page, fill in the details as indicated below.
Click on the Create button. Refer to the image shown below.
After your resource is successfully deployed, click on the “Go to resource” button. You can see the Key and the endpoint for the newly created Translator Text resource. Refer to the image shown below.
Make a note of the key, we will be using this in the latter part of this article to request the translations from the Translator Text API. The values are masked here for privacy.
Open Visual Studio 2019 and click on “Create a new Project”. A “Create a new Project” dialog will open. Select “ASP.NET Core Web Application” and click on Next. Now you will be at “Configure your new project” screen, provide the name for your application as ngTranslator and click on create. Refer to the image shown below.
You will be navigated to “Create a new ASP.NET Core web application” screen. Select “.NET Core” and “ASP.NET Core 3.0” from the dropdowns on the top. Then, select “Angular” project template and click on Create. Refer to the image shown below.
This will create our project. The folder structure of the application is shown below.
The ClientApp folder contains the Angular code for our application. The Controllers folders will contain our API controllers. The angular components are present inside the ClientApp\src\app folder. The default template contains few Angular components. These components won’t affect our application, but for the sake of simplicity, we will delete fetchdata and counter folders from ClientApp/app/components folder. Also, remove the reference for these two components from the app.module.ts file.
Right-click on the ngTranslator project and select Add >> New Folder. Name the folder as Models. Now right-click on the Models folder and select Add >> class. Name the class file as LanguageDetails.cs and click Add. This file will contain the User model.
Open LanguageDetails.cs and put the following code inside it.
Similarly, add a new class file TextResult.cs and put the following code inside it.
Add a new class file Translation.cs and put the following code inside it.
Create a class file DetectedLanguage.cs and put the following code inside it.
Create a class file TranslationResult.cs and put the following code inside it.
Create the class file AvailableLanguage.cs and put the following code inside it.
We will also add two classes as DTO (Data Transfer Object) for sending data back to the client.
Create a new folder and name it DTOModels. Add the new class file AvailableLanguageDTO.cs in the DTOModels Folder and put the following code inside it.
Add TranslationResultDTO.cs file and put the following code inside it.
We will add a new controller to our application. Right-click on the Controllers folder and select Add >> New Item. An “Add New Item” dialog box will open. Select “Visual C#” from the left panel, then select “API Controller Class” from templates panel and put the name as TranslationController.cs. Click on Add. Refer to the image below.
The TranslationController will handle the translation request from the client. This controller will also return the list of all the language supported by the Translate Text API.
Open TranslationController.cs file and put the following code inside it.
The Post method will accept two parameters — textToTranslate and targetLanguage. We will then invoke the TranslateText method which will return an array of TranslationResult. We will then prepare our data transfer object of type TranslationResultDTO and return it to the client.
The TranslateText method will accept two parameters — the text to translate and the target language. We will set the subscription key for the Azure Translator Text cognitive service and define a variable for the global endpoint for Translator Text. The request URL contains the API endpoint along with the target language. We will then create a new HttpRequestMessage. This HTTP request is a Post request. We will pass the subscription key in the header of the request. The Translator Text API returns a JSON object, which will be deserialized to an array of type TranslationResult. This JSON object contain the translated text as well as the language detected for the input text.
The GetAvailableLanguages method is of type HTTPGet. It returns the list of languages supported by the Translate Text API. We will set the request URI and create a HttpRequestMessage which will be a Get request. This request URL will return a JSON object which will be deserialized to an object of type AvailableLanguage. We will then prepare the data transfer object of type AvailableLanguageDTO and return it to the client.
The code for the client-side is available in the ClientApp folder. We will use Angular CLI to work with the client code.
Using Angular CLI is not mandatory. I am using Angular CLI here as it is user-friendly and easy to use. If you don’t want to use CLI then you can create the files for components and services manually.
Navigate to ngTranslator\ClientApp folder in your machine and open a command window. We will execute all our Angular CLI commands in this window.
Create a folder called models inside the ClientApp\src\app folder. Now we will create a file availablelanguage.ts in the models folder. Put the following code in it.
Similarly, create another file inside the models folder called translationresult.ts. Put the following code in it.
You can observe that both these classes have same definition as the DTO classes we created on the server side. This will allow us to bind the data returned from the server directly to our models.
We will create an Angular service which will invoke the Web API endpoints, convert the Web API response to JSON and pass it to our component. Run the following command.
This command will create a folder name as services and then create the following two files inside it.
Open translator.service.ts file and put the following code inside it.
We have defined a variable baseURL which will hold the endpoint URL of our API. We will initialize the baseURL in the constructor and set it to the endpoint of the TranslationController.
The getAvailableLanguage method will send a Get request to the GetAvailableLanguages method of the TranslationController to fetch the list of supported languages for translation.
The getTransaltedText method will send a Post request to the TranslationController and supply textToTranslate and targetLanguage as the parameter. It will fetch the translated text and the language detected for the text to translate.
Run the following command in the command prompt to create the TextTranslatorComponent.
The--module flag will ensure that this component will get registered at app.module.ts. Open text-translator.component.html and put the following code in it.
We have defined two dropdown lists one each for input language and the target language. Both dropdowns contain the list of available languages for translation. The Azure Translate Text API will detect the language of the input text. We will use the detected language to set the value of the input language drop down. The dropdown list for target language will invoke the setTargetlanguage method on selection change.
We have also defined two text areas, one each for the input text and the translated text. The text area for the input text will bind the value to the sting variable inputText. Whereas the text area for the translated text will be populated using the translationOutput property of the TranslationResult class.
Open text-translator.component.ts and put the following code in it.
We are injecting the TranslatorService in the constructor of the TextTranslatorComponent. We will invoke the getAvailableLanguage method of our service in the ngOnInit and store the result in an array of type AvailableLanguage.
The GetTranslation method will invoke the getTransaltedText method of our service and bind the result to an object of type TranslationResult.
The setTargetlanguage method will set the outputLanguage to the value selected in the target language drop down.
We will add the styling for the textarea in text-translator.component.css as shown below.
We will add the navigation links for our components in the nav menu. Open nav-menu.component.html and remove the links for Counter and Fetch data components. Add the following lines in the list of navigation links.
Press F5 to launch the application. Click on the Translator link on the nav menu at the top. You can perform the multilanguage translation as shown in the image below.
We created a Translator Text Cognitive Services resource on Azure. We have used the Translator Text API to create a multilanguage translator using Angular. This translator supports transaltion for over 60 languages. We fetched the list of supported languages for translation from the global API endpoint for Translator Text.
github.com
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
See all (41)
104 
3
104 claps
104 
3
SDE III @cisco | GDE for Angular | Microsoft MVP | Author | Speaker | Passionate Programmer https://ankitsharmablogs.com/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/how-to-prepare-for-azure-technologies-architect-certification-az-303-304-exam-5a2207c9058b?source=search_post---------354,"There are currently no responses for this story.
Be the first to respond.
Hello guys, If you want to become a Microsoft Azure certified Solution architect and wondering how to prepare for this prestigious exam then you have come to the right place.
Earlier, I have shared a few tips, courses, and practice tests to pass the AZ 104, AZ-900, or Microsoft Azure Fundamentals exam and today, I’ll talk about AZ-303 or Azure Solution Architect certification exam.
Based on the new Azure role-based certification path, the Microsoft AZ-303 exam is the main step towards becoming a Microsoft Certified Azure Solutions Architect.
This exam requires experienced candidates with skills in roles including Azure development, Azure administration, and DevOps. In addition, they must have expert-level skills in at least one of these areas.
However, observing these requirements, some of you may think that it would be completely impossible to pass the Microsoft AZ-303 Architect Technologies exam without years of prepration.
This is not true. If you have previous experience in the areas discussed above, you can effortlessly pass the exam with the right strategies and reliable study materials from the AZ-303 Architect Technologies exam.
In the past, I have shared some online courses and practice tests to crack the Microsoft Azure Solutions Architect exam, and today, I will provide you a complete guide on how to prepare and pass this prestigious exam.e Here, my intention is to provide you with this exam with the AZ-303 exam preparation strategies and appropriate study materials.
The eligibility criteria for the AZ-303 exam are so clear that it is easy for people to understand simply by reading the exam instructions.
If you want to take this exam, you need to have the skills of an Azure Solution Architect who can advise interested parties and transform business requirements into scalable, reliable, and secure solutions. Applicants for this exam must have advanced experience and skills in identity, visualization, networking, business continuity, budget, data management, governance, and disaster recovery. As I have already discussed, he/she must have advanced knowledge in Azure development, Azure administration, and DevOps and requires expert-level skills in at least one of these areas. So before you start preparing for the AZ-303 exam, make sure you meet the above criteria in each case.
Azure Exam AZ-303 is part of the Microsoft certification in Azure Solutions Expert. In this exam, you can expect 40 to 60 questions. This exam will have different types of question formats.
The types of questions that can be asked in this exam include the active screen, the review screen, the best answer, the brand review, the build list, the short answer, the case studies, the options for the repeated answer, drag and drop, multiple choices and active area. There are no downsides to incorrect answers. You simply don’t get some or all of the possible points for a question if you give the wrong answer. A review of the answers is also planned before leaving the exam room.
If you live in the United States, it will cost you $ 165 to take this exam. However, this price may vary based on location and many other factors. You may be eligible for a discount if you are a Microsoft Certified Trainer, a member of the Microsoft Partner Network Program, or a member of the Microsoft Imagine Academy Program. In addition, students can benefit from this type of fee reduction if they can present their valid lessons.
You will get the exam results that show the passing/failing status a few minutes after finishing the exam. However, getting a complete dashboard may take a few days.
This printed scorecard contains various elements such as the state of passing/failing, the overall exam score, the performance in the key areas of competencies, and the ways to interpret the results.
A score of 700 is required to pass the exam. Any score greater than or equal to 700 will be marked as “approved”; otherwise “failed”.
If a candidate fails the exam for the first time, he must wait at least 24 hours to repeat the exam. If this happens a second time, the candidate must wait at least 14 days to repeat the exam.
Therefore, a maximum of 5 attempts is allowed for an exam in one year. If a candidate has passed an exam, he cannot resume it under normal conditions.
There are no additional costs if you cancel or reschedule the exam at least 6 days before the appointment. If cancellation/postponement is made within 5 working days, minimum rates will be applied.
If you have not been able to take the exam or if you have not canceled/rescheduled the exam at least 24 hours before the appointment, you will lose all the exam fees that were spent when registering for the AZ exam 300.
There are a total of 5 objectives or topics you should know before starting preparation for the AZ-303 exam. These objectives are designed to effectively analyze a candidate’s skills and experience required for the exam.
Having the right preparation material can increase the chances of success. So we’ve put together the best resources here to help you prepare for and pass the AZ-303 exam. Let’s go on to find the best AZ-303 exam preparation material.
This portal is the perfect starting point for the Microsoft AZ-303 Architect Technologies exam preparation trip. While preparing for the AZ-303 exam, you may need to visit this portal multiple times as it contains most of the AZ-303 Architect Technologies exam study materials needed to take the exam. Here you will find basic exam requirements, links to schedule the exam, skills discussed on the exam, study groups that discuss users’ concerns about the exam, policies, and updates in relation to the exam, links to many other preparation options.
Since exam registration links are only accessible from this page, you cannot even think of skipping this page at any stage of preparation for the AZ-303 exam. In addition to the facts described above, this portal can be considered the most reliable preparation guide for the Microsoft AZ-303 exam, as all the latest updates on exam dates, price changes, etc. will appear first on this portal.
Therefore, be sure to confirm the information on this portal whenever you learn that there is a change in the modules, prices, or exam schedule for this exam.
If you need instructor-led training to prepare for the Microsoft AZ-303 Architect Technologies exam, you can get it from the Microsoft Learning Portal for AZ-303.
Total training options are divided into 6 modules to facilitate preparation for the AZ-303 exam. The availability of this training can change from country to country.
So when you need instructor training for one of these 6 courses, find a Microsoft Learning partner in your area. If you cannot afford instructor-led training then online courses are great options as they are affordable and you can learn from anywhere.
Here are the recommended Online Courses for AZ 303 Exam
udemy.com
2. AZ-303 Azure Architect Technologies Certification 2021 [Udemy]
udemy.com
3. AZ-303 Practice Test 2021 for Azure Architect Technologies [My Course]
www.udemy.com
Since this is a new exam, you may not find enough AZ-303 books or reference resources to prepare you for the Azure certification exam. However, once you’ve come to see a similar book in this area, be sure to verify its authenticity using customer reviews and seller information. This type of verification is very important because many self-proclaimed books often convince readers to follow the slums instead of doing an actual analysis of the model or test procedure. The following brainstorming in preparation for the AZ-303 exam may also result in a ban on completing the exam or trying other exams in Azure. Therefore, be sure not to follow any of these errors when preparing for the AZ-303 exam.
There are many Microsoft forums that can help users prepare for the Azure certification exam. Some of these forums are also applicable to AZ-303 exam candidates.
Since comments on the forum won’t cost you a cent, it’s one of the easiest ways to prepare for the AZ-303 exam. In the forums, you can ask your questions without worries. You will receive replies from many experienced people. Microsoft has organized an official study group on its AZ-303 learning portal. When you go to the AZ-303 page on the Microsoft portal, you can see the study group in the lower section of the page. On this page, you can sort between unanswered and unanswered questions. However, you can also see the exam dumps in this section. So be sure to check each answer carefully to avoid those answers.
www.certification-questions.com
Microsoft provides sample tests for mostly exams that candidates use for their Azure Certification exam. Unfortunately, it is not available for AZ-303 at the moment. So, it is very important to have a sample test and a good Simulator for better preparation. No worries, David Mayer provides Microsoft AZ-303 exam Simulator which includes both sample tests and full-length tests. The questions in this Simulator are prepared to give you the best idea of each type of question that may appear in the exam. You can practice with this simulator to ensure the best exam results.
You can also try with these free 10 questions and then purchase a premium version to get ready for the exam and check your preparation level.
Udemy also has some decent AZ-303 Practice tests which you can use along with this for better preparation, here is a list of the best AZ 303 practice tests from Udemy which also include one of my courses:
www.udemy.com
That’s all about how to crack Microsoft Azure Solution Architect (AZ-303) Cloud Certification in 2021. If you want to start working with Microsoft Azure or any other cloud platform then this is a really good certification to add to your resume.
More and more companies are looking for certified Azure solution architects and there are not many in the market. My friends literally have Recruiter swarming them as soon as they put their certification on Linkedin profile.   Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like this article then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note.
P. S. — If you need more practice then you can also checkout Whizlab’s Microsoft Azure Exam AZ-303 Certification Prep Material which contains both full-length tests and section quizzes to assess your preparation.
www.whizlabs.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
142 
3
142 claps
142 
3
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/@tsuyoshiushio/serverless-idempotency-with-azure-functions-23ed5da9d428?source=search_post---------355,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tsuyoshi Ushio
May 2, 2018·5 min read
Azure Functions and Durable Functions said that “functions should be idempotent.” However, what does it mean for the Azure Functions context?
According to the Wikipedia,
Idempotence is the property of certain operations in mathematics and computer science that they can be applied multiple times without changing the result beyond the initial application.
I can imagine the idempotence for the Infrastructure as Code tools and functional programming. However, what is that with Azure Functions?
For example, We write database, send messages like queue, notification, e-mails. It looks challenging to keep idempotency.
My colleague answer the question. In short, it is important for the cloud environment for keeping the resilient to retries. Especially it is very important for the durable functions. Your activity could be called multiple times by the replay.
By meriting of running in the cloud, assuming the worst case scenario: the underlying hardware might have a power outage at any time. That could be in the middle of your function execution of even after your function gets to the last line of your user code (so it looks like it was successful). This failure will lead to a retry of your function. This operation happens naturally for triggers like QueueTrigger, and even an HTTP client may resend the message if it has a retry loop. This means your function could get executed multiple times and needs to be idempotent so that it’s resilient to retries.
This blog post with the movie is a useful resource for the idempotence for REST-API.
www.restapitutorial.com
Read is idempotent. Upsert or Update is also idempotent. Delete is not idempotent, however, if there is no resource, just return an error. You can compromise it. Create is not idempotent. However, if the database support Upsert it is idempotent. Sometimes, Database state could be changed during the call. However, it is ok. For the update, you can use optimistic lock strategy to conquer that.
In case of the e-mail, you can compromise. If you receive two e-mails, it might not be a big deal. However, if you want to implement idempotency, you can save the state if the e-mail has been sent or not. This strategy is similar to the Queue strategy. Please see the next section.
We use queue frequently with Azure Functions. Imagine that if you use HTTP trigger with Queue output bindings, how we can make it idempotent? If you call HTTP trigger multiple times with the same parameter, obviously, it resends a queue.
We can’t achieve the idempotency for a single function. However, we can accomplish that in the whole system. If the queue trigger receives the queue with the same id, do nothing.
I create a sample code for this. These are the node functions with HTTP and Queue trigger. I use Storage Table bindings to make sure the execution has occurred in the past.
Using postman, send POST request to this body. You can change the id.
The HTTP trigger function sends this message to the Queue trigger function. The second function executes something if the id is new to them.
If you send the same request, you will see
It seems that the queue has been consumed, but the execution has not done yet. Don’t worry about that. Azure Functions Queue trigger looks after that for you. Once the error happens, Azure Functions leave the queue. The Queue is consumed only after the successful execution of the Queue trigger functions. Then it retries to consume the queue. If it fails five times, it is going to the dead queue.
See this article
docs.microsoft.com
I also include some logic for proving that.
When I repeatedly send, sometimes, you can see the error. Just after that, you can see the retry happens.
You can see the Queue Listener which is a part of the Queue Trigger bindings. It make the queue invisible for a while, then if it is completed the function, it will consume the queue.
github.com
To achieve the idempotency with Azure Functions, you don’t need to meet it in a single function. You can do it with the whole system. Sometimes, you can compromise it, however, always think the possibility of the retry. It will gain resiliency of your app.
Since this code is very simple, however, I struggled for a couple of hours. I can’t find the correct way to configure the Storage Table input bindings.
The answer is like this. “rowKey” was very difficult.
I try to find the answer by this document, however I can’t clearly understand the spec.
docs.microsoft.com
Eventually, I dump the context object, and realized that the parameter. The specification says that directly specify the data. however, RowKey didn’t work. :)
It help me to find out the solution. The output is like this
I should have dumped from the start.
Senior Software Engineer — Microsoft
41 
41 
41 
Senior Software Engineer — Microsoft
"
https://medium.com/t-t-software-solution/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87-azure-app-service-%E0%B8%9F%E0%B8%A3%E0%B8%B5-%E0%B9%83%E0%B8%99-10-%E0%B8%99%E0%B8%B2%E0%B8%97%E0%B8%B5-e3a5c25ef749?source=search_post---------356,"There are currently no responses for this story.
Be the first to respond.
สำหรับคนที่สนใจทดลองใช้ Azure Cloud Service น่ะครับผม
ตัวอย่างนี้เป็นแบบ Paas น่ะครับ โดยผมจะจำลองการสร้าง web application และ web api แบบ ง่ายๆผ่าน Azure ครับผม
ขั้นตอน
2. เสร็จแล้วระบบจะนำเราไปที่ my.visualstudio.com
3. ข้อดีของเว็ปนี้คือ3.1 ฟรี Azure 300$ 1 ปี (25$ ต่อเดือน)3.2 ฟรี Pluralsight (เว็ป Learning Centre) 3 เดือน (มี้เนื้อหาดีๆเยอะมากๆครับ)3.3 ฟรี Azure App Service (กดเล่นได้เรื่อยๆ แต่เล่นได้ครั้งล่ะ 1ชั่วโมง)
4. ให้ทดลองเลือก Tool >> Azure App Service >> Use it free
5. จะมีให้เราทดลองเลือก App ได้หลายประเภท, ลองเลือก Web App
6. เลือก MVC Template
7. เลือก Account ที่เราจะใช้ เสร็จแล้วรอสักพักให้ Azure สร้าง Web ให้เรา ซึ่งถ้าเสร็จแล้วจะได้หน้าตาตามภาพ
8. สิ่งที่เราจะไ้คือ
8.1 Link สำหรับ Web ที่พึ่งสร้าง
8.2 Link สำหรับทดลองเข้าใช้งาน Azure
8.3 Edit Code ผ่าน Visual Studio Code ได้ online เลย
8.4 Download sample code ลงมาได้
8.5 Download Azure Publish Profile เพื่อเอาไว้ deploy web ผ่าน Visual Studio ได้จากเครื่องของเรา
8.6 Git
9. ตัวอย่างหน้าเว็ป
10. ตัวอย่าง Azure Portal
11. ตัวอย่าง Edit code online
12. ผม ทดลองสร้าง Web API เพิ่มน่ะครับ โดยกด Previous แล้ว ระบบจะลบ web app ออกไปก่อน
13. เลือก API APP
14. เลือก API To Do List
15. หน้า Web API
16. กด using the Swagger UI, จะได้หน้าตาของ Swagger UI พร้ม list ของ Web API สวยงามน่าใช้เลยครับ ^ ^
17. ทดลอง เรียก service GET /api/ToDoList
18. เท่านี้เราก็จะได้ web api แบบบง่ายๆไว้ทดลองใช้ได้ครับผม
นี้เป็นบทความแรกของผม ใน Medium ผิดพลาดยังไงต้องขออภัยมา ณ ทีนี้ด้วยน่ะครับ
นายป้องกัน
https://www.tt-ss.net/
26 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
26 claps
26 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/6-best-az-900-practice-tests-dumps-and-mock-exams-to-crack-azure-fundamentals-exam-990041818584?source=search_post---------357,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are preparing for Microsoft Azure Fundamentals or AZ-900 certification exam in 2021 and want to pass this in-demand certification in the first attempt then you should practice with mock exams and practice tests, those are great tools to assess your preparation and increase your speed and accuracy required to pass this exam in the first attempt.
Earlier, I have shared the best AZ-900 Online Courses and In this article, I am going to list down some of the best practice tests you can take to pass the Microsoft Azure Fundamentals (AZ-900) exam.
This exam focuses on essential cloud concepts like IaaS, PaaS, SaaS, and Azure Core services like geo availability, storage, network, compute services, security, pricing, and billing.
This is actually one of the easiest Microsoft Azure cloud certifications, much like Amazon’s AWS Cloud Practitioner certification and you can easily pass this exam even if you have never worked in any cloud platforms like AWS, Azure, or GCP. This exam is a good stepping stone for both technical people who want to learn how exactly cloud works and how to use different Azure services to run their application in Cloud, as well as for non-technical people like BA, Project Managers, the Sales guy who are involved in purchasing Cloud-based solution and services. Prior IT experience is not mandatory, but it certainly helps if you have some IT experience and understand common technical terms like server, storage, compute, memory, network, security, VPN, scalability, elasticity, etc. Now, coming back to Microsoft Azure Fundamentals certification, one question I often receive is how to clear AZ-900 certification in one week? Or is one week enough to pass the Azure Fundamentals certification?  The answer to this question is Yes, one week is enough provided you can spend 5 to 6 hours daily. And, the best way to prepare for this exam is by joining an Azure Fundamentals AZ-900 course, like  AZ-900: Microsoft Azure Fundamentals Exam Prep -2021 Edition, which is focused on the exam, much like an exam guide, do some hands-on lab on the Azure portal, and then go through some practice tests and mock exams. Mock exams and Practice tests play an important role in your certification journey. They are often the difference between success and failure but also between satisfactory and excellent results. You may pass your exam without going for Practice tests, but you will do a lot better if you include Practice tests in your preparation strategies. They will help you to find your strong and week areas and also helping with timing and speed, which is very important to complete all questions during real exams. When I gave certification exams like AWS Solution Architect and AWS Cloud Practitioner, I spent hours giving practice tests on the real exam like scenarios and then reviewing what I did good and where I needed to improve, and this single strategy has helped me to score outstanding results on my certification exams.
If you are preparing for Microsoft Azure Fundamentals or AZ-900 certification and looking for some excellent Practice tests to take your preparation and confidence to the next level, then you have come to the right place.  Here are some of the best AZ-990 practice and mock exams from Whizlabs, Udemy, Exam Pass, Certification-Questions.com you can take to clear the Azure Fundamentals exam on the first try.
This is a perfect online course + practice test to prepare for the Microsoft Azure Fundamental exam in one week. Scott Duffy, a cloud expert, has done a fabulous job in this course by teaching you everything you need to pass the AZ-900 exam. The course also comes with a lot of bonuses and supplementary resources like you will get access to a 24-page Microsoft Fundamentals Study guide to prepare offline. There are also quizzes to reinforce your learning through each chapter. As I said, the course also contains a 50 question mock test to find your strong and week areas. If you are in a serious hurry and want to pass AZ 900 exam in a couple of days, just go through this course and practice test. You may not get a high score, but you will definitely have enough knowledge to pass the exam.
Here is the link to join this test — AZ-900: Microsoft Azure Fundamentals Exam Prep -2021
You can also combine this practice test with my topic-wise AZ-900 Udemy course for better preparation and practice, where I have shared 300+ questions covering each topic for Azure Fundamentals certification.
www.udemy.com
This mock simulator is best for passing the Microsoft Azure certification or AZ-900 exam, and you can access them via web and mobile. This practice test from Whizlabs contains 275 unique questions divided into 5 full-length exams, which check your fundamental knowledge of cloud services, and how Microsoft Azure provides the cloud services, irrespective of any specific role. There are also 7 section tests with 35 unique questions, and they provide an exhaustive explanation with every question, which not only helps you to learn why a correct option is correct but also why other options are not correct. When it comes to practice tests for Cloud certifications like Microsoft Azure Fundamentals, I trust Whizlabs. I have used Whizlabs in the past for passing several Java, and AWS certifications like OCAJP, OCPJP, AWS Solution Architect, AWS Cloud Practitioner, and it didn’t disappoint me on AZ-900 as well. Their reporting is also top-notch and really helps you to assess your strengths and weaknesses before the real exam. After a few tests, you will have enough ideas on which topics you need to prepare better.
Overall, one of the best exam simulators for the Azure AZ-900 certification exam, and I highly recommend it if you want to score in excess of 80%.
Here is the link to join Whizlabs — Microsoft Azure Exam AZ-900 Certification Practice Test
By the way, if you are preparing for certification, consider taking Whizlabs subscription which provides full access to all of their online training courses and practice tests for different certifications like AWS and Google Cloud with just $99 per annum (50% discount now).
I highly recommend this subscription plan as Whizlabs has the best materials to prepare for IT certifications.
This is another online course that provides the latest practice questions for the Microsoft Azure Fundamentals exam with detailed explanations. It contains 6 practice papers with 55 questions on each, which means a total of 332 questions for practice. The best thing about these mock tests on Udemy is that they are closely aligned with the exam syllabus which means you have 15–20% questions from cloud concepts, 30–35% questions from core Azure Services, 20–25% questions on pricing, and support, and rest of them on security, privacy, compliance, and trust. Like Whizlabs, they also provide a good explanation for each question and link to official Microsoft documentation where required. In short, these practice tests are enough to pass the Microsoft Azure AZ-900 exam on the first attempt.
It is also very cost-effective, and I bought it for just $10, you can also buy it at a similar or a bit higher price depending upon the Udemy sale.
Here is the link to join this test — Microsoft Azure Fundamentals (AZ-900) — Practice Tests
This is an optional practice test, and if you have gone through Whizlab’s or the previous one, you may not need that. In case you need more practice, and you are not confident enough, or you really want to kill the exam by scoring 100 out of 100, then you can purchase this course on Udemy. This is also from Scott Duffy, author of the first course in this list. You can also use these practice tests along with that cours, but it’s not a pre-requisite. It contains three complete, timed practice tests for the AZ-900 Azure Fundamentals exam, 150 questions, and they are 100% original. This time-bound test is important to develop the speed and accuracy you need to solve all problems in the real exam. If you consistently score over 80% on these tests and able to complete all questions, then you are ready for the real exam. From my experience, I suggest you take these tests a couple of days before the real exam so that you have enough time to rectify weak areas.
Here is the link to join this course — AZ-900: Microsoft Azure Fundamentals Original Practice Tests
This is slightly different than the practice test. These are actually exam dumps that are anonymously shared by candidates going through AZ-900 certification. The best thing about AZ-900 exam dumps is that they are the real questions from the exam, which means you can check the difficulty level and format. It’s not guaranteed that you will see those questions again, but they are still very useful to get yourself up to that level or above. Here is a brief detail about this Microsoft AZ-900 Practice Exam: Microsoft Azure Fundamentals: — Number of Questions: 149 — Exam Tests: 3 — Last Update: 2021–03–15 So, it’s pretty up-t-date, and I saw many comments where candidates mentioned that the question they’ve got during the exam was more than 80% the same as these tests. :-)
Here is the link to check these dumps — AZ-900 Exams Dumps by David Mayer
If you get lucky, let us know, but I don’t recommend that approach. I prefer to over-prepare and score really high on the exam rather than mugging questions from exam dumps. Don’t get me wrong; they still have a lot of utility with respect to difficulty level and format, and that’s why I have included them on this list, but I don’t advise you to solely rely on them for your preparation.
This is the latest practice test designed for Microsoft Azure Fundamentals or AZ-900 exam. It contains AZ-900 Practice questions with detailed explanations which will help you to pass the Microsoft Azure Fundamental exam with confidence.
This test contains 260 questions divided into 6 practice tests, where 4 tests contain 50 questions each and two tests containing 30 questions each. This mock test is designed by Sunil Kumar, who has recently passed the Azure Fundamentals certification exam.
Every practice test includes-
If you are preparing for the AZ-900 Microsoft Azure Fundamentals exam in 2021 then you should take this practice test and solve the given questions to better prepare for the exam.
Here is the link to join this course — AZ-900 Practice Tests | Microsoft Azure Fundamental
Here are some of the important things you should know while preparing for the AZ-900 or Microsoft Azure Fundamentals exam: 1. The cost of the exam is $99 USD, but it may vary depending upon which country you are in. Price is actually based on the country in which the exam is proctored. 2. The good thing about Azure Fundamental certification is that topics are clearly defined, and so is their weightage. Here are some important topics for the AZ-900 exam:
3. The AZ-900: Microsoft Azure Fundamental exam checks your knowledge about basic cloud concepts; core Azure services; security, privacy, compliance, and trust; and Azure pricing and support. 4. This certification is good for both technical people like Developers, Technical Lead, and architects who want to learn about Cloud and how Microsoft provides essential Cloud services as well as non-technical people like BA, Project Managers, Salespersons who are involved in buying and selling Cloud-based solutions and services 5. You can read the exam guide for AZ-900 or Azure Fundamental exam on the Microsoft Azure exam portal. It has some useful information for the exam.
docs.microsoft.com
That’s all about some of the best mock exams and practice tests to prepare for AZ-900 or Microsoft Azure Fundamentals certification. I highly recommend you to go through at least one of the practice tests listed here, if you need to choose one, then choose the first one from Udemy or the second one from Whizlabs, both are excellent, up-to-date, and very relevant to the real exam. Also, the AZ-900 exam is the perfect way to prove that you know about Cloud and Azure. You can also put the AZ-900 badge on your resume and LinkedIn to get recruiters’ attention, who are always looking for cloud-certified professionals.  Other Certification Resources for IT Professionals and Java Programmers
Thanks for reading this article so far. If you like these best AZ-900 certification questions and practice tests, then please share them with your friends and colleagues. If you have any questions or feedback, then please drop a note. P. S. — If you are new to the world of Cloud and looking for some free courses to learn Cloud computing then you can also, check out this list of cloud computing courses with Amazon Web Services — Free AWS Courses for Beginners on Medium. It contains some of the best and free online training courses to learn Amazon Web Services and in-demand skills for Clod engineers and architects.
medium.com
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
185 
2
185 claps
185 
2
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://medium.com/xp-inc/azure-devops-docker-node-js-90cff720af22?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
Thiago S. Adriano
Nov 16, 2019·5 min read
Veja nesse artigo como automatizar o processo de deploy do seu projeto utilizando Docker e o Azure DevOps
Dando continuidade ao meu artigo anterior: publicando imagem Docker no Azure Web App for Containers em 7 passos, hoje eu irei demonstrar como automatizar o processo de deploy demonstrando no artigo anterior, utilizando o Azure DevOps.
Para os próximos passos será necessário ter uma conta no Azure. Caso você ainda não tenha uma, eu recomendo a leitura do seguinte artigo: Azure DevOps + Angular + GitHub Pages, nele eu demonstro a criação de uma nova conta e a criação de uma pipeline no Azure DevOps.
Com o passo da conta OK, a sua imagem no Docker hub (passo demonstrado no artigo anterior) e o seu projeto já publicado no Web App for Containers. Vamos agora criar a nossa pipeline no Azure DevOps. Para isso, siga os passos abaixo:
Clique em pipelines:
Em seguida clique em Create Pipeline:
Agora clique em Use the classic editor:
Selecione o local do seu repositório, para esse exemplo eu irei utilizar um projeto versionado no meu GitHub: node-azure-7-steps. Clique nos três pontos e selecione o seu projeto:
Em seguida selecione a sua branch:
Agora selecione o template Docker container.
Esse template deve criar dois steps: Build an Image para criar uma nova versão da imagem do seu projeto e Push an image, para publicar a imagem no no seu repositório de imagens, nesse artigo eu irei enviar para o Docker Hub.
Agora vamos dar permissão para pipeline subir uma nova versão da sua imagem no Docker Hub. Para isso, siga os passos abaixo:
Clique em Push an image, em Container Registry Type selecione Container Registry, em seguida selecione a sua conexão.
Caso não tenha uma conta registrada ainda, clique em + New e preenche a modal com os seus dados de acesso no Docker Hub.
Em seguida clique em Include Latest Tag:
E no nome da imagem coloque o seu usuário do dockerhub e o nome da sua imagem no passo de build e release:
Build
Release
Para verificar se tudo esta OK, clique em Save & queue e rode o processo:
Quando esse processo finalizar você deve receber o resultado abaixo:
Com o processo do build OK, vamos criar a nossa release. Para isso, clique em Releases -> New pipeline e selecione o template Azure App Service deployment.
Clique em Artifacts e preencha conforme os passos abaixo:
Agora clique em 1 job e forneça os dados do seu projeto no Azure Web App for Containers conforme lista abaixo:
Obs.: Esse sufixo garante que iremos sempre pegar a ultima versão da imagem criada.
Agora para verificar se todos passos anteriores estão OK, clique em Create release para gerar uma nova release do seu projeto:
Ao clicar em Create Release irá subir a seguinte mensagem:
Clique na sua Release para acompanhar o processo de deploy. Caso tudo esteja OK você deve receber o retorno abaixo:
Agora clique em Succeed:
Em seguida clique em Azure Wer App on Container Deploy:
E dentro do log copie a URL do seu projeto:
Agora para finalizar, cole a url no seu navegador e verifique se a ultima alteração do seu projeto esta nessa versão publicada:
Bom, a ideia desse artigo era demonstrar como automatizar o processo de deploy criado em um dos meus artigos anteriores.
Espero que tenham gostado e até um próxima artigo pessoal ;)
Enjoy your life
See all (155)
48 
1
48 claps
48 
1
Aqui você vai encontrar os principais conteúdos de tecnologia, design, dados e produto da XP Inc.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/asp-net-core-protegendo-segredos-de-uma-aplica%C3%A7%C3%A3o-com-o-azure-key-vault-b6e10cfe8a5?source=search_post---------359,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 22, 2019·8 min read
O armazenamento de configurações em que estão presentes informações sensíveis e utilizando arquivos como appsettings.json constitui uma prática bastante difundida. Haverá ocasiões, entretanto, nas quais mecanismos externos precisarão ser empregados a fim de dificultar o acesso a tais definições por parte de um usuário qualquer (que as obteria rapidamente pela simples leitura de um arquivo).
Considerando o desenvolvimento de projetos Web com ASP.NET Core, como proteger segredos de nossas aplicações de maneira simples e eficiente?
Uma boa resposta para esse questionamento está no Azure Key Vault, uma solução de armazenamento de configurações sensíveis oferecida pela plataforma de cloud computing da Microsoft. Possuindo integração com o Azure Active Directory, a manipulação (leitura, escrita) de segredos definidos no Key Vault acontece mediante a concessão prévia de acesso a uma aplicação.
Neste novo artigo demonstrarei o uso do Azure Key Vault em uma API REST criada com o ASP.NET Core 2.2. O exemplo em questão foi disponibilizado também no GitHub:
https://github.com/renatogroffe/ASPNETCore2.2_API-REST_AzureKeyVault
E aproveito este espaço para deixar aqui ainda um convite.
Nesta terça 23/07/2019 partir das 21:30 - horário de Brasília - teremos mais uma live no Canal .NET . Desta vez o MVP Luiz Carlos Faria fará uma apresentação sobre o uso de Docker com Envoy, abordando esta combinação desde um simples proxy reverso a service mesh.
Para efetuar a sua inscrição acesse a página do evento no Meetup. A transmissão acontecerá via YouTube, em um link a ser divulgado em breve.
Neste primeiro momento acessaremos o Portal do Microsoft Azure:
Na sequência acionar na barra lateral à esquerda o item Azure Active Directory:
Clicar agora na opção App registrations:
Aparecerão então eventuais aplicações já registradas no painel. Clicar na opção New registration, a qual permitirá que registremos a API REST empregada nos testes detalhados neste artigo:
Em Register an Application preencher:
Confirmar este procedimento acionando o botão Register.
O registro da aplicação será exibido neste momento. O valor da configuração Application (client) ID deverá ser utilizado posteriormente, ao se configurar a comunicação da aplicação com o recurso do Key Vault:
Acionar agora a opção Certificates & secrets; o botão New client secret permitirá a geração de uma chave, a ser empregada pela aplicação para validar seu acesso ao Azure Key Vault:
Em Add a client secret preencher o campo Description, selecionando ainda em Expires o valor Never. Confirmar as ações clicando no botão Add:
O valor indicado no campo VALUE do segredo deverá ser COPIADO IMEDIATAMENTE (um aviso no alto do painel alerta que tal configuração não estará mais disponível quando o usuário deixar esta funcionalidade no Portal do Azure):
E aproveito este espaço e o grande interesse por Docker também para um convite.
Tem interesse em conhecer mais sobre Docker? Que tal então fazer um curso completo, cobrindo desde fundamentos a diferentes possibilidades de uso de containers com tecnologias em alta no mercado? Adquira conhecimentos profundos sobre Docker, evolua e se diferencie no mercado, seja você um profissional DevOps, um Desenvolvedor ou um Arquiteto de Software!
Acompanhe o portal Docker Definitivo para ficar por dentro de novidades a serem anunciadas em breve!
Site: https://dockerdefinitivo.com/
Ao solicitar a criação de um novo recurso no Portal do Azure pesquisar por Azure Key Vault, selecionado a opção destacada a seguir:
Clicar agora em Create:
Em Create key vault informar o nome do recurso (campo Name), um grupo de recursos (campo Resource Group) e o data center em que o mesmo será criado (Location). Acionar em seguida a opção Access policies:
Clicar em Add new, a fim de vincular ao recurso do Key Vault a aplicação registrada no Azure Active Directory:
No painel Add access policy selecionar em Configure from template (optional) a opção Secret Management, clicando na sequência em Select principal:
Em Principal localizar e selecionar a opção API REST Cotações, confirmando isso através do botão Select:
Já em Secret permissions habilitar apenas as opções Get e List, de forma que a API REST vinculada a este recurso do Azure Key Vault possua somente acesso de leitura às configurações cadastradas:
Concluir este procedimento clicando no botão OK:
Aparecerá agora em Access policies o item API REST Cotações registrado como APPLICATION; confirmar este ajuste por meio do botão OK:
Retornando ao painel Create key vault concluir o processo de criação clicando no botão Create. Após alguns segundos o recurso groffeartigo estará disponível; o endereço indicado em DNS NAME também será também utilizado para configurar o acesso na API REST de testes:
Na seção Secrets do recurso do Key Vault serão cadastradas configurações que não deverão constar no arquivo appsettings.json. Acionar para isto a opção Generate/Import (destacada em vermelho):
Em Create a secret:
Concluir esta ação pressionando o botão Create. O segredo ConnectionStrings--BaseCotacoes estará então disponível para uso:
Acessando o item ConnectionStrings--BaseCotacoes será exibida uma tela similar àquela apresentada na imagem a seguir:
Um clique na definição que está em CURRENT VERSION trará a próxima tela:
Com um simples clique em Show Secret Value podemos consultar o valor da Connection String armazenada no recurso groffeartigo:
O package Microsoft.Extensions.Configuration.AzureKeyVault deverá ser adicionado ao projeto que terá seus segredos armazenados no Key Vault:
Incluir agora a seção AzureKeyVault no arquivo appsettings.json, preenchendo as seguintes configurações:
Conforme é possível observar, não há qualquer menção a uma string de conexão neste arquivo de configurações:
A única alteração a ser realizada neste projeto acontecerá na classe Program:
A seguir temos a implementação da classe Startup, com o uso normal do objeto Configuration. Isto inclui um acesso à chave ConnectionStrings:BaseCotacoes (linha 35), a qual não existe em appsettings.json e que numa situação normal implicaria na existência de dois níveis de propriedades no arquivo de configuração:
Efetuando o debugging da aplicação é possível constatar que o ajuste na classe Program vinculou a string de conexão que está no recurso do Azure Key Vault (o segredo ConnectionStrings--BaseCotacoes) ao objeto Configuration. Tal fato permite a diferentes partes da aplicação acessar configurações sem grandes complicações, esteja uma determinada definição na nuvem ou no arquivo appsettings.json:
Um segundo teste envolverá a classe CotacoesController. Uma chamada a config.GetConnectionString(“BaseCotacoes”) - linha 18 - permitirá acessar o segredo que está no Azure Key Vault, como se a definição correspondente fosse um item chamado BaseCotacoes e estivesse vinculada ao elemento ConnectionStrings em appsettings.json:
É o que demonstra o exemplo de debugging na próxima imagem:
.NET Core 2.2 e ASP.NET Core 2.2: Guia de Referência
Key Vault | Microsoft Azure
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
35 
2
35 
35 
2
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/como-o-microsoft-azure-pode-simplificar-a-publica%C3%A7%C3%A3o-de-suas-web-apps-dica-r%C3%A1pida-6e934e775515?source=search_post---------360,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Aug 27, 2019·2 min read
Já precisou fazer o deployment de uma aplicação em Windows utilizando IIS (Internet Information Services) e encontrou dificuldades de configuração? E quanto a este mesmo tipo de procedimento em Linux, sofreu por não possuir tanto conhecimento deste ambiente?
Que bom seria proceder com esta publicação sem grandes complicações, fosse uma aplicação ASP.NET clássica (Web Forms, MVC, Web API), .NET Core, Java, Node, PHP, Python, Ruby ou até mesmo uma imagem Docker baseada em alguma destas plataformas! E que tal nem perder nosso precioso tempo instalando todo o runtime necessário para essas stacks, processo este que muitas vezes está sujeito a erros?
E ainda se pudermos configurar o deployment automatizado de nossas aplicações, bem como dispor de mecanismos que facilitam a construção de soluções escaláveis… tudo isto com poucos cliques do mouse?
O Azure App Service é uma alternativa para hospedagem de aplicações que integra a plataforma de cloud computing da Microsoft e que nos oferece tudo isso! Em nossos eventos presencias e lives em comunidades como Canal .NET, .NET SP, DevOps Professionals, Coding Night, Campinas .NET e Azure Talks sempre abordamos o uso desta tecnologia.
E que tal aprender a trabalhar com o Azure App Service na prática, em um workshop que acontecerá durante um sábado (dia 21/09) em São Paulo Capital e implementando na prática um case que combina o uso deste serviço com outras tecnologias? Acesse então o link a seguir para efetuar sua inscrição com um desconto especial: http://bit.ly/promocao-azure-na-pratica
Para concluir este post deixo aqui algumas referências que podem ser úteis para que você conheça um pouco mais sobre o Azure App Service:
ASP.NET Core + PowerShell: publicando via linha de comando e em segundos uma Web App no Azure
Hospedando projetos Web no Azure: de um site estático a um cluster Kubernetes
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
40 
1
40 
40 
1
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://koukia.ca/azure-functions-and-how-they-work-6b515007001e?source=search_post---------361,"Azure functions are event or trigger driven functions that you can build on Microsoft Azure which you can implement using variety of languages and leverage a lot of Azure features like Continuous Deployment and Integration, Scaling up and Scaling out, Hybrid connections and etc.
The Azure Functions idea is that you need to implement a simple integration between your On-Premise or Cloud resources and you don’t want to build a complete service and just a simple function in the cloud will do the job.
"
https://medium.com/@renatogroffe/asp-net-core-azure-kubernetes-guia-de-refer%C3%AAncia-272a79248cd5?source=search_post---------362,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Jul 23, 2018·2 min read
Este post reúne os diversos conteúdos que venho produzindo sobre a orquestração de containers Docker com Kubernetes, considerando o uso desta solução em conjunto com o ASP.NET Core e o Microsoft Azure. Vocês poderão encontrar aqui artigos, apresentações e projetos de exemplo envolvendo o uso conjunto destas tecnologias.
Não se trata de um material definitivo, já que pretendo manter este post sempre que possível atualizado com novos artigos, apresentações e vídeos.
E aproveito este espaço para deixar aqui um convite.
Dia 25/07/2018 (quarta-feira) às 21h30 - horário de Brasília - teremos mais um hangout no Canal .NET. Confira esta apresentação online com o MVP Elemar Júnior e aprenda mais sobre a modelagem de Microsserviços com base em processos de negócio.
Para efetuar a sua inscrição acesse a página do evento no Facebook ou então o Meetup. A transmissão acontecerá via YouTube, em um link a ser divulgado em breve.
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem - parte 1
ASP.NET Core + Azure + Kubernetes: orquestração de containers na nuvem - parte 2
ASP.NET Core 2.1 + Docker + Alpine Linux + Docker Hub + Kubernetes + AKS (Azure Kubernetes Service)
ASP.NET Core 2.0 + Docker + Kubernetes + Azure Container Registry + AKS (Azure Kubernetes Service)
E para fechar este post, caso tenha interesse em saber mais sobre o uso de Docker com tecnologias Microsoft acesse o seguinte guia de referência:
Docker para Desenvolvedores .NET - Guia de Referência
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
18 
18 
18 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://levelup.gitconnected.com/manage-azure-event-hubs-with-azure-service-operator-on-kubernetes-be61decb069?source=search_post---------363,"Azure Service Operator is an open source project to help you provision and manage Azure services using Kubernetes. Developers can use it to provision Azure services from any environment, be it Azure, any other cloud provider or on-premises — Kubernetes is the only common denominator!
It can also be included as a part of CI/CD pipelines to create, use and tear down Azure resources on-demand. Behind the scenes, all the heavy lifting is taken care of by a combination of Custom Resource Definitions which define Azure resources and the corresponding Kubernetes Operator(s) which ensure that the state defined by the Custom Resource Definition is reflected in Azure as well.
Read more in the recent announcement here — https://cloudblogs.microsoft.com/opensource/2020/06/25/announcing-azure-service-operator-kubernetes/
In this blog post:
The code is available in this GitHub repo https://github.com/abhirockzz/eventhubs-using-aso-on-k8s
Azure Service Operator supports many Azure services including databases (Azure Cosmos DB, PostgreSQL, MySQL, Azure SQLetc.), core infrastructure components (Virtual Machines, VM Scale sets, Virtual Networks etc.) and others as well.
It also supports Azure Event Hubs which is a fully managed data streaming platform and event ingestion service with support for Apache Kafka and other tools in the Kafka ecosystem. With Azure Service Operator you can provision and manage Azure Event Hubs namespaces, Event Hub and Consumer Groups.
So, let’s dive in without further ado! Before we do that, please note that you will need the following in order to try out this tutorial:
Start by getting an Azure account if you don’t have one already — you can get for FREE! Please make sure you’ve kubectl and Helm 3 installed as well.
Although the steps outlined in this blog should work with any Kubernetes cluster (including minikube etc.), I used Azure Kubernetes Service (AKS). You can setup a cluster using Azure CLI, Azure portal or even an ARM template. Once that's done, simply configure kubectl to point to it
Ok, you’re now ready to…
Nothing too fancy about it… just following the steps to install it using Helm
Start by installing cert-manager
Since the operator will create resource on Azure, we need to authorize it to do so by providing the appropriate credentials. Currently, you can use Managed Identity or Service Principal
I will be using a Service Principal, so let’s start by creating one (with Azure CLI) using the az ad sp create-for-rbac command
Setup required environment variables:
Add the repo, create namespace
Use helm upgrade to initiate setup:
Before you proceed, wait for the Azure Service Operator Pod to startup
Start by cloning the repo:
Create an Azure Resource Group
I have used the southeastasia location. Please update eh-resource-group.yaml if you need to use a different one
Create Event Hubs namespace
I have used the southeastasia location. Please update eh-namespace.yaml if you need to use a different one
Once done, you should see this:
You can get details with kubectl describe eventhubnamespacesand also double-check using az eventhubs namespace show
The namespace is ready, we can now create an Event Hub
You can get details with kubectl describe eventhub and also double-check using az eventhubs eventhub show
As a final step, create the consumer group
This is addition to the default consumer group (appropriately named $Default)
You can get details with kubectl describe consumergroup and also double-check using eazventhubs eventhub consumer-group show
Let’s make use of what we just setup! We’ll deploy a pair of producer and consumer apps to Kubernetes that will send and receive messages from Event Hubs respectively. Both these client apps are written in Go and use the Sarama library for Kafka. I am not going to dive into the details since they are relatively straightforward
Deploy the consumer app:
Keep a track of the logs for the consumer app:
You should see something similar to:
Using another terminal, deploy the producer app:
Once the producer app is up and running, the consumer should kick in, start consumer the messages and print them to the console. So you’ll see logs similar to this:
In case you want to check producer logs as well: kubectl logs -f $(kubectl get pods -l=app=eh-producer --output=jsonpath={.items..metadata.name})
Alright, it worked!
… how did the consumer and producer apps connect to Event Hubs without connection info, credentials etc.?
Notice this part of Event Hub manifest (eh-hub.yaml file):
secretName: eh-secret ensured that a Kubernetes Secret was created with the required connectivity details including connection strings (primary, secondary), keys (primary, secondary), along with the basic info such as Event Hubs namespace and hub name.
The producer and consumer Deployments were simply able to refer to this. Take a look at this snippet from the consumer app Deployment
The app uses env vars EVENTHUBS_CONNECTION_STRING, EVENTHUBS_NAMESPACE and EVENTHUBS_TOPIC whose values were sourced from the Secret (eh-secret). The value for EVENTHUBS_CONSUMER_GROUPID is hardcoded to eh-aso-cg which was the name of the consumer group specified in eh-consumer-group.yaml.
To remove all the resources including Event Hubs and the client apps, simply use kubectl delete -f deploy
Azure Service Operator provides a layer of abstraction on top Azure specific primitives. It allows you to manage Azure resources and also provide ways to connect to them using other applications deployed in the same Kubernetes cluster.
I covered Azure Event Hubs as an example, but as I mentioned earlier, Azure Service Operator also supports other services too. Head over to the GitHub repo and give them a try!
Coding tutorials and news.
101 
101 claps
101 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev
"
https://medium.com/awesome-azure/azure-storage-replication-overview-azure-storage-data-redundancy-options-f8fc1c03042d?source=search_post---------364,"There are currently no responses for this story.
Be the first to respond.
Introduction to Azure Storage Data Redundancy Options.
Data availability is business-critical for most organizations. Data in Azure is replicated to ensure that it’s always available, even if a datacenter or region becomes inaccessible or a specific piece of hardware fails.
In Azure Storage, you have several options for replication. Each replication option provides a…
"
https://medium.com/swlh/crossing-the-streams-with-azure-event-hubs-and-stream-analytics-a-cloud-guru-d81a8020d4b4?source=search_post---------365,"There are currently no responses for this story.
Be the first to respond.
This blog provides a practical example of how to use Azure Stream Analytics to process streaming data from Azure Event Hubs. You should be able to go through this tutorial using the Azure Portal (or Azure CLI), without writing any code. There are also other resources for exploring stream processing with Azure Stream Analytics at the end of this blog post.
What’s covered?
Azure Stream Analytics is a real-time analytics and complex event-processing engine designed to analyze and process high volumes of fast-streaming data from multiple sources simultaneously. It supports the notion of a Job, each of which consists of an input, query, and an output. Azure Stream Analytics can ingest data from Azure Event Hubs (including Azure Event Hubs from Apache Kafka), Azure IoT Hub, or Azure Blob Storage. The query, which is based on SQL query language, can be used to easily filter, sort, aggregate, and join streaming data over a period of time.
Assume you have an application that accepts processed orders from customers and sends them to Azure Event Hubs. The requirement is to process the “raw” orders data and enrich it with additional customer info such as name, email, location etc. To get this done, you can build a downstream service that will consume these orders from Event Hubs and process them. In this example, this service happens to be an Azure Stream Analytics job (which we’ll explore later of course!)
In order to build this app, we would need to fetch this customer data from an external system (for example, a database) and for each customer ID in the order info, we would query this for the customer details. This will suffice for systems with low-velocity data or where end-to-end processing latency isn’t a concern. But it will pose a challenge for real-time processing on high-velocity streaming data.
Of course, this is not a novel problem! The purpose of this blog post is to showcase how you can use Azure Stream Analytics to implement a solution. Here are the individual components:
Azure Stream Analytics jobs connect to one or more data inputs. Each input defines a connection to an existing data source — in this case, its Azure Event Hubs.
An individual order is a JSON payload that looks like this:
Customer information is provided as reference data. Although, the customer information is likely to change (e.g., if the customer changes her phone number), for the purposes of this example, we’ll treat it is static reference data stored in Azure Blob Storage container.
This is the workhorse of our solution! It joins (a continuous stream of) orders data from Azure Event Hubs with the static reference customers data based on the matching customer ID (which is id in the customers data set and id in the orders stream)
Simply put, an Output lets you store and save the results of the Stream Analytics job. In this example, to keep things simple we continue to use Azure Event Hubs (a different topic) as the output.
Now that you have a conceptual overview, it’s time to dive in. All you need is an Azure account. If you don’t have it already, just grab one for free.
In this section, you’ll:
You need to create an Event Hubs Namespace and Hub (topic). There are lots of options including Azure Portal, Azure CLI, ARM template or Azure PowerShell
Please note that you need to create two topics:
You’ll need to create an Azure Storage account. This quickstart walks you through this process and provides guidance for Azure Portal, Azure CLI, etc. Once that’s done, go ahead and create a container using the Azure Portal or the Azure CLI if you prefer.
Save the JSON below to a file and upload it to the storage container you just created.
Start by creating an Azure Stream Analytics job. If you want to use the Azure Portal, just follow the steps outlined in this section or use the Azure CLI instead if you don’t prefer clicking on a UI.
To configure Azure Event Hubs Input
Open the Azure Stream Analytics job you just created and configure Azure Event Hubs as an Input. Here are some screenshots which should guide you through the steps:
Choose Inputs from the menu on the left
Select + Add stream Input > Event Hub
Enter Event Hubs details — the portal provides you the convenience of choosing from existing Event Hub namespaces and respective Event Hub in your subscription, so all you need to do is choose the right one.
To configure Azure Blob Storage Input:
Choose Inputs from the menu on the left
Select Add reference input > Blob storage
Enter/choose Blob Storage details
Once you’re done, you should see the following Inputs:
Azure Stream Analytics allows you to test your streaming queries with sample data. In this section, we’ll upload sample data for orders and customer information for the Event Hubs and Blob Storage inputs respectively.
Open the Azure Stream Analytics job, select Query and upload sample orders data for Event Hub input
Save the JSON below to a file and upload it.
Open the Azure Stream Analytics job, select Query and upload sample orders data for Blob storage input
You can upload the same JSON file that you uploaded to Blob Storage earlier.
Now, configure and run the below query:
Open the Azure Stream Analytics job, select Query and follow the steps as depicted in the screenshot below:
Select Query > enter the query > Test query and don’t forget to select Save query
The query JOINs orders data from Event Hubs it with the static reference customers data (from Blob storage) based on the matching customer ID (which is id in the customers data set and id in the orders stream.)
Explore or dig into the Stream Analytics query reference
It was nice to have the ability to use sample data for testing our streaming solution. Let’s go ahead and try this end to end with actual data (orders) flowing into Event Hubs.
An Output is required in order to run a Job. In order to configure the Output, select Output > + Add > Event Hub
Enter Event Hubs details: the portal provides you the convenience of choosing from existing Event Hub namespaces and respective Event Hub in your subscription, so all you need to do is choose the right one.
In the Azure Stream Analytics interface, select Overview, click Start and confirm
Wait for the Job to start, you should see the Status change to Running
Note: Although I have used kafkacat, you're free to choose any other mechanism (CLI or programmatic). This documentation provides lots of examples
Start a consumer to listen from Event Hubs output topic
To keep things simple, we can use the kafkacat CLI to produce orders and consume enriched events (instead of a program). Just install it and you should be good to go.
Create a kafkacat.conf file with Event Hubs info:
Let’s first start the consumer process that will connect to the output topic ( customer-orders) which will get the enriched order information from Azure Stream Analytics
In another terminal, start sending order info to the orders topic
In a terminal:
This will block, waiting for records from customer-orders.
You can send order data via stdout. Simply paste these one at a time and observe the output in the other terminal:
The output you see on the consumer terminal should be similar to this:
Notice how the order info is now enriched with customer data (name, location in this case). You can use the information in this topic anyway you want. For example, you can persist this enriched materialized view to Azure Cosmos DB, trigger an Azure Function, etc.
As expected, you won’t see a corresponding enriched event corresponding to orders placed by customers whose ID isn’t present in the reference customer data (in Blob Storage), since the JOIN criteria is based on the customer ID.
This brings us to the end of this tutorial! I hope it helps you get started with Azure Stream Analytics and test the waters before moving on to more involved use cases.
In addition to this, there’s plenty of material for you to dig in.
High-velocity, real-time data poses challenges that are hard to deal with using traditional architectures — one such problem is joining these streams of data. Depending on the use case, a custom-built solution might serve you better, but this will take a lot of time and effort to get it right. If possible, you might want to think about extracting parts of your data processing architecture and offloading the heavy lifting to services which are tailor-made for such problems.
In this blog post, we explored a possible solution for implementing streaming joins using a combination of Azure Event Hubs for data ingestion and Azure Stream Analytics for data processing using SQL. These are powerful, off-the-shelf services that you are able to configure and use without setting up any infrastructure, and thanks to the cloud, the underlying complexity of the distributed systems involved in such solutions is completely abstracted from us.
Originally published at https://acloudguru.com on September 1, 2020.
Get smarter at building your thing. Join The Startup’s +750K followers.
114 
114 claps
114 
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +750K followers.
"
https://medium.com/microsoftazure/azure-automated-machine-learning-a4d12eb95602?source=search_post---------366,"There are currently no responses for this story.
Be the first to respond.
Automated Machine Learning (AutoML) is a service available within the Azure Machine Learning (AML) service.
As the name suggests, the main goal of AutoML is that of automating the development and scoring of different types of ML models to eventually pick the one which is the most performing. Basically, AutoML offers the possibility to parallelize…
"
https://medium.datadriveninvestor.com/meet-azure-logic-apps-to-know-its-amazing-benefits-e2a622b06907?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
Wondering what are Azure Logic Apps? No worries I’ve got your back covered!
Azure Logic Apps is a cloud service that helps you automate and orchestrate tasks, business processes, and workflows when you need to integrate apps, systems, data, and services across enterprises or organizations. Logic Apps simplifies how you design and build scalable solutions for app integration, data/system integration, enterprise application integration, and business-to-business (B2B) communication, whether in the cloud, on-premises, or both.
Here are some of the examples where you could use Azure logics apps
These are some of the benefits too, you will have more idea about it as you continue reading about it below.
Have a look at a quick video below
Enterprise integration solutions can be built with Azure logic Apps with the help of readily available connectors. Connectors play an integral part when you create automated workflows with Azure Logic Apps. By using connectors in your logic apps, you expand the capabilities for your on-premises and cloud apps to perform tasks with the data that you create and already have.
Each connector offers a set of operations classified as ‘Actions’ and ‘Triggers’. Once you connect to the underlying service, these operations can be easily leveraged within your apps and workflows.
Actions are changes directed by a user. For example, you would use an action to look up, write, update, or delete data in a SQL database. All actions directly map to operations defined in the Swagger.
Several connectors provide triggers that can notify your app when specific events occur. For example, the FTP connector has the OnUpdatedFile trigger. You can build either a Logic App or Flow that listens to this trigger and performs an action whenever the trigger fires
There are two types of trigger.
Polling Triggers: These triggers call your service at a specified frequency to check for new data. When new data is available, it causes a new run of your workflow instance with the data as input.
Push Triggers: These triggers listen for data on an endpoint, that is, they wait for an event to occur. The occurrence of this event causes a new run of your workflow instance.
Of course, it does come with a big box of benefits or rather why you should consider using Azure Logic Apps.
parameters, triggers, actions, outputs are the main parts of any logic apps.
Parameters are things that you want to reuse across workflows. Re-using values or even complex objects throughout the definition makes it easier to comprehend. Separate out configuration from the definition itself makes sharing easy as well as across different environments.
Triggers are what start the Logic App. Triggers can be evaluated at recurring intervals or have its state maintained across executions.
Actions are the things that happen in a Logic App. Actions can depend on other actions. The dependencies are what will determine the order of the Actions executing. The state is also maintained across executions.
Outputs are what gets sent from calls into the workflow.
I hope the basics of azure logic are clear and you are now ready to roll with the actual implementation. 😄
For more detailed understanding about azure logic apps here is the link
docs.microsoft.com
Happy Learning! 💻
empowerment through data, knowledge, and expertise.
106 
106 claps
106 
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
Written by
I talk about JavaScript, Web Development, No-code to help you stay ultra-modern. See you on Twitter — https://twitter.com/MakadiaHarsh
empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com
"
https://medium.com/@burkeholland/solving-the-azure-functions-challenge-with-javascript-65205b0c4920?source=search_post---------368,"Sign in
There are currently no responses for this story.
Be the first to respond.
Burke Holland
Jun 15, 2017·8 min read
If you’ve found this post, you’re probably looking for help on how to solve the Azure Functions Challenge using JavaScript. If that isn’t why you’re here, and you don’t know what the Azure Functions Challenge is or what Azure Functions are or even what Azure is, that’s ok too! You’ll learn a lot just by going through this exercise. Click here to get started with the challenge.
Let’s get right to it. This is how I solved the Azure Functions Challenge with JavaScript.
The first puzzle is pretty simple as it just wants to show you how to create a function and have it return a value.
Create a new Generic HTTP Function with JavaScript. Now that you’ve done that, you already have a function that returns a response. All you need to do is modify it to echo back out the value you received as the ping but with a key of “pong”. It should look something like this.
Pretty easy. This is the most fundamental way Azure Functions work. It’s about to get more complicated pretty quickly.
Note that the HTTP Function and Generic Webhook function templates are not the same. The challenge will ask you to create a Webhook function, but you can use the HTTP Function template as well. The code samples here assume the HTTP Function so won’t work with Webhook.
This is exercise is a simple translation of a series of numbers into words. Kind of like a Little Orphan Annie Decoder Ring.
Create another JavaScript HTTP Trigger Function in the same project space.
This time the challenge is going to send you an object that contains a unique key, a msg string of numbers and a cipher which maps two digit numbers to characters. They want you to…
First, let’s look at splitting the string of numbers into 2 digit sets. There are a few ways to do this…
You could loop over the number and just count your way through it, storing the numbers in an array.
If the word “regex” makes you queasy, you’re in good company. Everyone feels that way. There are only two types of people in the world: those that hate regex and those who won’t admit it.
You don’t need to master regex to use it. Just know what it can do and then Google. That’s how the rest of us are doing it.
In the case of this string, the regular expression to split the number into two character sets is dead simple…
Now that we’ve got the array of numbers, we need to find their letter equivalents in the cipher object. Unfortunately, the cipher object is not at all in the format we need it. The letters are the keys and the numbers are the values. That’s going to force us to write a loop.
Or, we can just use Underscore to invert the keys and values in the object because Underscore is amazing. Come at me with your Lodash.
We can just install Underscore from npm because you can do that in the Azure Portal.
You can install npm modules via the web interface for Azure functions just like you would do on a terminal. If you are logged into your Azure Portal, just go to the Function project, then select “Platform features” and open the console. Then it’s just npm install underscore.
If you’re using the free Try Azure sandbox, you don’t have access to the console so you can’t install npm modules. Instead you can use this handy function that I shamelessly stole from StackOverflow. Shout out to user tnanoba!
At this point, all we have to do is loop over the array of two digit numbers and compose the result.
On to the next challenge!
This one is going to send us a key and an array and it wants us to store the array by it’s key somewhere and then return it in a second function. This is great because we get to look at input and output bindings in Azure Functions as well as Azure Table Storage.
When it comes to data, you can think of bindings like this: Input bindings are for when you want to read data. Output bindings are when you want to write data. You can create bindings to all sorts of database stores, including CosmosDB and other fancy things, but for our purposes, simple Azure Table Storage will work just fine.
Create a new HTTP Trigger function. Select the “integrate” option beneath the function and then create a new output binding. You can select Azure Table Storage when it asks what kind of binding you want to create.
Once the binding is created, you’ll need to configure it. The “Table parameter name” is the name that you will use to access the table storage in your function. It can be anything. The “Table name” is the name of the table that will be created in storage. Whatever name you give your table here is the same name you will use to access it in the next function. Lastly, you need to choose or select a “Storage account connection”.
Now that the binding is created, we can access the table by calling context.bindings.outputTable. If you recall, outputTable was our “Table parameter name”. We can write to the table storage by specifying an object with a PartitionKey, a RowKey and ArrayOfValues. The PartitionKey and RowKey are required fields. You can tack anything else you like onto the object but you have to at least have those two. In this case, we’re tacking on ArrayOfValues.
Notice that you never to call save on the outputTable. The save happens automatically. Simply setting the outputTable value is enough.
That’s all we have to do to store the key and ArrayOfValues in Azure Table Storage. On to step 2!
In step 2, they are simply going to pass the key that they passed to the first function and we are going to retrieve the ArrayOfValues from table storage.
To do that, create a new HTTP Function. Go to the “Integrations” item and add a new input binding. To configure the input binding…
Your final configuration for the input binding should look something like this…
Now we can access the inputTable on the context.bindings object and read from it. It will only contain our table entry that was created in step one because we’ve already passed the key in the binding.
Note that we have to parse the ArrayOfValues on the input table as it appears that Azure Table Storage stores our array as a string. Then we sort the array numerically per the challenge.
That’s it! And now you know how to use input and output bindings as well as Azure Table Storage. We call that a “two-fer”.
The last step just has you deploy from GitHub to Azure functions, because you can. It’s really just a matter of filling out a form and selecting a resource group.
I really liked the Functions Challenge as I’m new to this world of Serverless applications. I don’t fully understand all of it and we’re all learning every day. One thing is for sure, we are at the very beginning of the Serverless revolution, and Azure Functions is a great place to get onboard.
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
30 
4
Thanks to Faiz Shaikh. 
30 claps
30 
4
Pretty fly for a bald guy. Hacking on Azure at Microsoft.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mauridb/work-with-json-files-with-azure-sql-8946f066ddd4?source=search_post---------369,"Sign in
There are currently no responses for this story.
Be the first to respond.
Davide Mauri
Aug 7, 2017·4 min read
Dealing with CSV or JSON data today is more and more common. I do it on daily basis, since the our application send data to our microservice gateway backend is in a (compressed) JSON format.
Sometimes, especially when debugging or developing a new feature, I need to access that JSON data, before is sent to any further microservices for processing or, after that, being stored in the database.
So far I usually used CloudBerry Explorer to locate and download the JSON I was interested into and that a tool like Notepad++ with JSON-Viewer plugin or Visual Studio Code to load and analyze the content.
Being Azure SQL or main database, I spend a lot of time working with T-SQL, so I would really love to be able to query JSON directly from T-SQL, without even have the need to download the file from the Azure Blob Stored where it is stored. This will make my work more efficient and easier.
I would love to access JSON where it is, just like Hadoop or Azure Data Lake allows you to do
Well, you can. I just find out that with the latest additions (added since SQL Server 2017 CTP 1.1 and already available on Azure SQL v 12.0.2000.8) it is incredibly easy.
First of all the Shared Access Signature needs to be created to allow Azure SQL to access the Azure Blob Store where you have your JSON files. This can be done using the Azure Portal, from the Azure Storage Blade
or you can also do it via the Azure CLI 2.0 as described here:
buildazure.com
Once you have the signature a Database Scoped Credential that points to the created Shared Access Signature needs to be created too:
If you haven’t done it before you will be warned that you need to create a Database Master Key before being able to run the above command.
After that credentials are created, it’s time to point to the Azure Blob Account where your JSON files are stored by creating a External Data Source:
Once this is done, you can just start to play with JSON files using the OPENROWSET along with OPENJSON:
and voilà, JSON content are here at your fingertips. For example, I can access to all activity data contained in our “running session” json:
This is just amazing: now my work is much simpler, especially when I’m traveling and, maybe, I don’t have a good internet access. I can process and work on my JSON file without even have them leaving the cloud.
If you have a CSV file the technique is very similar, and it is already documented in the official Microsoft documentation:
Examples of Bulk Access to Data in Azure Blob Storage
The same approach is doable also via SQL Server 2017 (now in RC2). You can also access file not stored in the cloud, but on your on-premises storage. In such case, of course, you don’t specify the Shared Access Signature as an authentication methods, since SQL Server will just rely on Windows Authentication. Here Jovan showed a sample usage:
Sure, there is a Gist for that:
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
19 
1
19 
19 
1
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
"
https://towardsdatascience.com/access-azure-database-for-mysql-from-azure-functions-with-ssl-certification-verification-b6c6784f76fb?source=search_post---------370,"Sign in
There are currently no responses for this story.
Be the first to respond.
Christopher Tao
Nov 9, 2019·6 min read
Recently I got a customer who has relatively small volume data to be managed. So, I suggested Azure Database for MySQL. Basically, the smallest instance cost about $560 AUD per month will be…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@reefwing/espressif-esp32-tutorial-ir-remote-control-using-microsoft-azure-ed768a5cea4d?source=search_post---------371,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Such
Aug 9, 2018·15 min read
This tutorial will outline how to create an IR Remote using the ESP32 and then control it from the IoT hub on Microsoft Azure. Driving an IR remote transmitter using an Arduino is simple, as there is a library, called IRremote.h which does all the hard work. You just need to connect your IR transmitter module signal pin to the appropriate Arduino pin, via a current limiting…
About
Write
Help
Legal
Get the Medium app
"
https://koukia.ca/create-a-power-bi-dashboard-using-azure-application-insights-analytics-data-9f9e2435640c?source=search_post---------372,"In some previous posts I explained what is Azure Application Insight Analytics, and in this post I will show you how you can create Power BI dashboard using Azure Application Insights Analytics data.
So to start, you should login to the Azure Portal and navigate to your Application Insight resource there which will look like below:
"
https://medium.com/awesome-azure/azure-virtual-network-vnet-peering-overview-introduction-a795517bd83b?source=search_post---------373,"There are currently no responses for this story.
Be the first to respond.
Introduction to Virtual Network (VNet) Peering in Azure — What is VNet Peering?
VNet peering (or virtual network peering) enables you to connect virtual networks. A VNet peering connection between virtual networks enables you to route traffic between them privately through IPv4 addresses. Virtual machines in the peered VNets can communicate with…
"
https://medium.com/awesome-azure/azure-difference-between-azure-expressroute-and-azure-vpn-gateway-comparison-azure-hybrid-connectivity-5f7ce02044f3?source=search_post---------374,"There are currently no responses for this story.
Be the first to respond.
Comparison — Azure ExpressRoute vs Azure VPN Gateway.
ExpressRoute provides direct connectivity to Azure cloud services and connecting Microsoft’s global network. All transferred data is not encrypted, and do not go over the public Internet.
"
https://medium.com/wortell/azure-sentinel-designing-access-and-authorizations-that-meet-the-enterprise-needs-501bfdafaa5f?source=search_post---------375,"There are currently no responses for this story.
Be the first to respond.
You’ve successfully deployed Azure Sentinel and are collecting data and using it for monitoring and hunting purposes. Quickly after, your company’s privacy offer or auditor points out that both the law (for instance: GDPR & AVG) and the company’s requirements don’t allow all admins to have access all the time to all of that personal identifiable data.
You need to come up with a solution to design access to Azure Sentinel in a way that the SecOps people can work with the alerts, the SIEM admins can create/modify rules, and that hunters can sift through all the data to find what they are looking for. How should you do that? In this blog we’ll share some design considerations.
Azure Log Analytics
Azure Sentinel uses a Log Analytics workspace to store its data. Recently, Microsoft introduced a more granular role-based access module for Log Analytics. Previously, we only had the workspace-context access mode, but now we also have a resource-context access mode. For each Log Analytics workspace you can chose the desired access mode.
Resource-context access mode allows you to set permissions all the way down to individual tables in the Log Analytics workspace, aka Table Level RBAC.
You can change the current workspace access control mode from the Properties page of the workspace, which can be found under the Workspace settings node in the Azure Sentinel UI. (Changing the setting will be disabled if you don’t have permissions to configure the workspace)
PRO TIP: Want to quickly know if your Log Analytics workspace is enabled for resource-context access mode? Run this Powershell command:
Get-AzResource -ResourceType Microsoft.OperationalInsights/workspaces -ExpandProperties | foreach {$_.Name + “: “ + $_.Properties.features.enableLogAccessUsingOnlyResourcePermissions}
Want to enabled resource-context permissions for all your workspaces? Run this script:
Azure Active Directory
Because Azure Sentinel uses Log Analytics as the backend, part of the Azure platform, it therefore also uses Azure Active Directory for its identities. Azure has two built-in user roles for Log Analytics workspaces: Log Analytics Reader and Log Analytics Contributor. Members of the Log Analytics Reader role can:
Members of the Log Analytics Contributor role can:
These roles can be given to uses at different scopes:
Custom roles
If the built-in roles don’t meet the specific needs of your enterprise, you can create your own custom roles. Just like built-in roles, you can assign custom roles to users, groups, and service principals at subscription, resource group, and resource scopes.
PRO TIP: Custom roles are stored in an Azure Active Directory (Azure AD) directory and can be shared across subscriptions. Each directory can have up to 5000 custom roles. (For specialized clouds, such as Azure Government, Azure Germany, and Azure China 21Vianet, the limit is 2000 custom roles.)
You implement table access control with Azure custom roles to either grant or deny access to specific tables in the workspace. These roles are applied to workspaces with either workspace-context or resource-context access control modes regardless of the user’s access mode.
Create a custom role with the following actions to define access to table access control.
For example, to create a SecOps role for investigations with access only to the SecurityAlert and AzureActivity tables, create a custom role as follows:
Custom Logs
You can’t currently grant or deny access to individual custom logs, but you can grant or deny access to all custom logs. To create a role with access to all custom logs, create a custom role using the following actions:
PRO TIP: Tobias Zimmergren wrote a great blog how to log custom application security events in Azure Sentinel. You can read it here. These custom log tables end with _CL in their naming in the Log Analytics workspace.
Considerations
PRO TIP: Unsure who has what access? Use the ‘Access control IAM’ node from the Advanced Settings pane of your Log Analytics workspace to find out:
Privileged Identity Management
Azure AD Privileged Identity Management (PIM) enables you to set up IAM in a way that users and accounts don’t carry the required roles and permissions all the time.
Accounts are ahead of time enabled to request a certain role. Once they want to use that role, just in time they put in a request to use that role and for how long and, depending on the configuration in PIM, that request can then be evaluated and granted or denied. The great thing is that Azure AD PIM also works for custom roles.
And example would be where a Threat Hunter would use a regular Azure AD account and then go to the PIM interface to request the SecOps investigator role to access all the required information in Azure Sentinel for his or her investigation.
More information on Azure AD PIM and how to set it up, can be found here.
PRO TIP: Azure AD PIM is a premium feature in Azure. You need either a Azure AD P2 license for the user that needs PIM functionality, or license it through an EM+S E5 or Microsoft 365 E5 license.
Monitoring PIM usage
Most enterprises and auditors will also require you to monitor that role usage. This can be easily done using the Azure AD audit logs. Make sure you enable the Azure Active Directory connector in Azure Sentinel so that the data type ‘AuditLogs’ is collected.
The PIM operations are stored in the AuditLogs table. You need to filter on the category for ‘ResourceManagement’ and operationname containing ‘PIM’. Here’s a base KQL query for hunting:
You could use a similar query to create a rule to alert the SOC when that role is being used:
Conclusion
Using a combination of the new resource-context access mode for your Log Analytics workspace, custom Azure AD roles, and Privileged Identity Management, you can achieve most if not all of the requirements that your enterprise and auditors have for your Azure Sentinel deployment.
Happy hunting!
— Maarten Goet, MVP & RD
UPDATE: Oleg Ananiev, group program manager at Microsoft for Azure Log Analytics, adds the following remark:
“Resource-centric RBAC and table-level RBAC are orthogonal. In fact, you can use table-level RBAC for workspace queries, it does not require access control mode change for the workspace. This is especially important, as most Azure Sentinel users will likely be using workspace-centric access. Why? The prerequisite for using resource-centric access is that data must be tagged with a valid Azure resource id. This is typically not true for many types of security data, for example: Office 365.”
Microsoft Cloud & Enterprise Security
31 
31 claps
31 
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/microsoftazure/multi-node-distributed-training-with-pytorch-lightning-azure-ml-88ac59d43114?source=search_post---------376,"There are currently no responses for this story.
Be the first to respond.
TL;DR This post outlines how to distribute PyTorch Lightning training on Distributed Clusters with Azure ML
Full end to end implementations can be found on the official Azure Machine Learning GitHub repo.
github.com
If you are new to Azure you can get started a free subscription using the link below.
azure.microsoft.com
In my last few posts on the subject, I outlined the benefits of both PyTorch Lightning and Azure ML to simplify training deep learning models and logging. Take a look you haven’t yet check it out!
medium.com
medium.com
Now that you are familiar with both the benefits of Azure ML and PyTorch lighting let’s talk about how to take PyTorch Lighting to the next level with multi node distributed model training.
Multi Node Distributed Training is typically the most advanced use case of the Azure Machine Learning service. If you want a sense of why it is traditionally so difficult, take a look at the Azure Docs.
docs.microsoft.com
PyTorch Lighting makes distributed training significantly easier by managing all the distributed data batching, hooks, gradient updates and process ranks for us. Take a look at the video by William Falcon here to see how this works.
We only need to make one minor modification to our train script for Azure ML to enable PyTorch lighting to do all the heavy lifting in the following section I will walk through the steps to needed to run a distributed training job on a low priority compute cluster enabling faster training at an order of magnitude cost savings.
docs.microsoft.com
Create Azure ML Workspace from the Portal or use the Azure CLI
Connect to the workspace with the Azure ML SDK as follows
docs.microsoft.com
To run PyTorch Lighting code on our cluster we need to configure our dependencies we can do that with simple yml file.
We can then use the AzureML SDK to create an environment from our dependencies file and configure it to run on any Docker base image we want.
Create a ScriptRunConfig to specify the training script & arguments, environment, and cluster to run on.
We can use any example train script from the PyTorch Lighting examples or our own experiments.
Once we have our training script we need to make one minor modification by adding the following function that sets all the required environmental variables for distributed communication between the Azure nodes.
Then after parsing the input arguments call the above function.
Hopefully in the future this step will be abstracted out for us.
For Multi Node GPU training , specify the number of GPUs to train on per a node (typically this will correspond to the number of GPUs in your cluster’s SKU), the number of nodes(typically this will correspond to the number of nodes in your cluster) and the accelerator mode and the distributed mode, in this case DistributedDataParallel (""ddp""), which PyTorch Lightning expects as arguments --gpus --num_nodesand --accelerator, respectively. See their Multi-GPU training documentation for more information.
Then set the distributed_job_config to a new MpiConfiguration with equal to one(since PyTorch lighting manages all the distributed training)and a node_count equal to the --num_nodes you provided as input to the train script.
We can view the run logs and details in realtime with the following SDK commands.
And there you have it with out needing to deal with managing the complexity of distributed batching, Cuda, MPI, logging callbacks, or process ranks, PyTorch lighting scale your training job to as many nodes as nodes as you’d like.
You shouldn’t but if you have any issues let me know in the comments.
I want to give a major shout out to Minna Xiao and Alex Deng from the Azure ML team for their support and commitment working towards a better developer experience with Open Source Frameworks such as PyTorch Lighting on Azure.
Aaron (Ari) Bornstein is an AI researcher with a passion for history, engaging with new technologies and computational medicine. As an Open Source Engineer at Microsoft’s Cloud Developer Advocacy team, he collaborates with the Israeli Hi-Tech Community, to solve real world problems with game changing technologies that are then documented, open sourced, and shared with the rest of the world.
Any language.
79 
2
79 claps
79 
2
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
<Microsoft Open Source Engineer> I am an AI enthusiast with a passion for engaging with new technologies, history, and computational medicine.
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
"
https://medium.com/@jeffhollan/build-serverless-alexa-and-google-assistant-skills-with-azure-f076512c2dfa?source=search_post---------377,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jeff Hollan
Mar 12, 2017·3 min read
OK Google, how many new users did we add yesterday?
Alexa, swap back production and staging slots.
Hey Cortana, send my Office documents for approval.
Sounds like a not-so-distant future? It may be closer than you think.
Personal assistants like Cortana, Google Home, and Amazon Alexa are becoming integration hubs for consumers. They provide instant access to news, weather, shopping, traffic, and more. However there may be certain integrations that are more custom than what is out-of-the-box or readily available, or that leverage services that are not supported (or custom) themselves. Azure Logic Apps provides an easy way to build your own custom skills that can be used to extend these personal assistants into any number of our over 100 cloud and on-premises connectors. The best part? You could build the whole skill without any code.
All right, now to the nitty-gritty. For this blog I’m going to focus on how you can register a custom skill with Amazon Alexa, but the same pattern could be followed for Google Assistant and I assume Cortana.
To integrate with Alexa we will use the webhook pattern:
Logic Apps fits very naturally into steps 2–4. By using the Logic Apps HTTP Request Trigger a secure HTTP endpoint is automatically generated for us. We use connectors and control flow (potentially a switch statement to route intents) to process the response, and send back a response using the HTTP Response action.
After creating a new Logic App, add an HTTP Request trigger and define the request payload you expect to receive. In this case we expect an Alexa request so you can use this schema.
You can then begin to add actions to get the data you need for the response. For example, if I asked “How many customers did we get today?” I may add a Dynamics 365 List Records action with a filter for customers added today and return the @length() of the response.
Once the data has been composed, add a response action to return the response to Alexa. You can use a number of different properties, but a basic speech response looks like this:
That’s it! Now simply go to the Alexa Skills Kit and register a new skill using the Request URL generated when you save. You can test the app in the Alexa console to make sure everything is working end-to-end before publishing.
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
18 
1
18 
18 
1
Senior PM Lead for Microsoft Azure Functions. I help build cloud stuff, and love serverless. Thoughts shared are my own.
"
https://medium.com/@maarten.goet/azure-sentinel-fusion-machine-learning-for-a-secops-world-64ccda3de5f8?source=search_post---------378,"Sign in
There are currently no responses for this story.
Be the first to respond.
Maarten Goet
Mar 14, 2019·7 min read
The annual RSA conference just wrapped up in San Francisco. With the introductions of Chronicle’s Backstory (Google) and Azure Sentinel, 2019 became the year of the ‘Cloud SIEM’.
Why is this important? VisibleRisk summarizes it as: “because these types of products can flip two decades of “normal” on their head and finally position those who defend our enterprises in a way that they can keep pace with the furious pace of change they face.”
Azure Sentinel leverages the immense compute power of the cloud and sophisticated machine learning models to help defenses in the enterprise. Microsoft calls is Azure Sentinel FUSION.
Azure Sentinel FUSION? Say what?
If you go to the Overview page in Azure Sentinel you’ll see a reference in the bottom right corner a section called: Democratize ML for your SecOps. It says:
“Unlock the power of AI for security professionals by leveraging MS cutting edge research and best practices in ML, regardless of your current investment level in ML.”
If you click on the Learn More link it brings you to this page.
Enabling Fusion
There is no UI to enable Fusion, however if you have an instance of Azure Sentinel running, you can use Azure Cloud Shell and the ‘az’ command to enable Fusion for your Log Analytics workspace.
Start Azure Cloud Shell:
Run the following command:
You get back the result that you are now enabled for FUSION:
OK, now what?
Great question.
Because there is only one page of documentation online, I reached out to the Azure Sentinel product engineering team in Israel, and asked them what Fusion does and got this response:
“Fusion looks at alerts coming from different sources and tries to find out if there’s a connection between them in order to fuse them into one case with higher confidence.”
“Think about having multiple low fidelity alerts that no one had the time to investigate, we tell you if you should investigate them by fusing them into one case.”
Microsoft’s documentation does give another couple of clues:
“Machine Learning in Azure Sentinel is built-in right from the beginning. We have thoughtfully designed the system with ML innovations aimed to make security analysts, security data scientists and engineers productive. One such innovation is Azure Sentinel Fusion built especially to reduce alert fatigue.”
“Fusion uses graph powered machine learning algorithms to correlate between millions of lower fidelity anomalous activities from different products such as Azure AD Identity Protection, and Microsoft Cloud App Security, to combine them into a manageable number of interesting security cases.”
Unified SecOps
Not coincidently, Microsoft announced last week that they are integrating Cloud App Security, Azure ATP and Azure AD identity protection into an unified SecOps experience and portal:
Based on three pillars
So why are all security vendors adding machine learning and artificial intelligence to their solution? Well, first of all: sifting through tons of alerts in a SIEM is not something security analysts love doing. Their skill set can also be better put to work to hunt for bad actors, based on pre-filtered signals.
Secondly, it is well known that security analysts are drowning in those alerts and sometimes miss the critical piece to launch to the next step of investigation. In fact, Mark Russinovich laid out Microsoft’s strategy dealing with this three years ago.
Ram Shankar, who works on the Microsoft Azure team, wrote that the ML team behind Azure Sentinel FUSION asked three questions:
The ML team came up with these three ideas:
1: Probabilistic Kill Chain
Garden variety detections assume static kill chain. Not true — real world attacks are complex and multistage. So, the ML Team modeled the probability of moving to the next step is conditioned not only on previous step but also factors like current asset.
2: Iterative attack simulation
A lot of noise looks like legit attacks because detections explore only one line of attack. For every alert, the ML team iteratively simulates multiple lines of attack using random walk style algorithms to evaluate if this attack is truly feasible.
3: Encode domain knowledge as priors!
Incorporating Bayesian methods to tap into expert’s domain knowledge is painfully obvious but the common hurdle inference style algorithms are slow. Not a problem because Azure Sentinel is a cloud based SIEM and the ML team can leverage the cloud’s scalable + compute.
These three ideas form the bedrock of Fusion, that Ram claims has shown to reduce alert fatigue by 90%.
MCAS & Azure ATP
Going back to the Data Collection page in Azure Sentinel and clicking on Azure Advanced Threat Protection (ATP) data source, we find another clue:
“Connect Azure Advanced Threat Protection to Azure Sentinel: if your tenant is running the Azure ATP preview in Microsoft Cloud App Security, connect here to stream your Azure ATP alerts into Azure Sentinel.”
PRO TIP: Both Cloud App Security (MCAS) and Azure Active Directory data sources need to be connected for the current (preview) release of Azure Sentinel Fusion to work.
Azure Sentinel FUSION in action
The scenario we’ll be demonstrating is where a user’s credentials are stolen, and the following actions happen afterwards:
Normally these two alerts are seen in different portals and it would take a security engineer to ‘connect the dots’.
However, when you connect these data sources (Azure AD, Azure ATP and Cloud App Security) to Azure Sentinel, the machine learning models behind Azure Sentinel FUSION kick in and generate a Case, showing that data is being exfiltrated:
In cybersecurity, it’s AI vs. AI
Paul Gillin of SiliconAngle wrote:
“Artificial intelligence research group OpenAI last month made the unusual announcement: It had built an AI-powered content creation engine so sophisticated that it wouldn’t release the full model to developers.
Anyone who works in cybersecurity immediately knew why. Phishing emails, which try to trick recipients into clicking malicious links, originated 91 percent of all cyberattacks in 2016, according to a study by Cofense Inc. Combining software bots to scrape personal information from social networks and public databases with such a powerful content generation engine could produce much more persuasive phishing emails that might even mimic a certain person’s writing style, according to Nicolas Kseib, lead data scientist at TruSTAR Technology Inc.
The potential result: cybercriminals could launch phishing attacks much faster and on an unprecedented scale.”
AI is a new weapon that some people believe could finally give security professionals a leg up on their adversaries.
Conclusion
Microsoft is beating the other security vendors to the punch, having already added some real machine learning models and AI behind their just released Azure Sentinel cloud SIEM offering.
Azure Sentinel FUSION can help reduce alert fatigue, but more importantly ‘connect the dots’ and provide security analysts with a clear picture of the (potential) threat.
— Maarten Goet, MVP & RD
Microsoft MVP and Microsoft Regional Director.
See all (21)
16 
16 claps
16 
Microsoft MVP and Microsoft Regional Director.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@renatogroffe/kubernetes-azure-devops-build-e-deployment-automatizado-de-aplica%C3%A7%C3%B5es-c216c35a5c64?source=search_post---------379,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Mar 9, 2020·8 min read
Em um artigo anterior demonstrei como configurar passo a passo o build e o deployment automatizados de projetos baseados em Docker no Azure DevOps, fazendo uso para isto de um projeto ASP.NET Core 3.1 e do Azure Web App for Containers para a hospedagem da aplicação em questão. Para o armazenamento das imagens geradas durante o build utilizei ainda o Azure Container Registry:
Docker + Azure DevOps: build e deployment automatizado de aplicações
Neste novo tutorial abordarei o mesmo tipo de procedimento empregando o Azure DevOps e desta vez o Azure Kubernetes Service. A intenção com isso é demonstrar o suporte que o Azure DevOps nos oferece para o deployment de aplicações em um cluster Kubernetes.
E aproveito este espaço para deixar aqui um convite.
Que tal aprender mais sobre Docker, Kubernetes e a implementação de soluções baseadas em containers utilizando o Microsoft Azure, em um workshop que acontecerá durante um sábado (dia 04/04/2020) em São Paulo Capital e implementando um case na prática?
Acesse então o link a seguir para efetuar sua inscrição (inclui camiseta, emissão de certificado e almoço para todos os participantes) com desconto:http://bit.ly/anp-docker-blog-groffe
Utilizarei mais uma vez o seguinte projeto:
https://github.com/renatogroffe/ASPNETCore3.1-API-REST_Docker-Alpine
Optei na prática por duplicar este repositório, uma vez que em tal cópia serão gravados arquivos com configurações de build/deployment para um cluster Kubernetes do Azure Pipelines.
Na listagem a seguir temos o conteúdo do arquivo Dockerfile, em que estão referenciadas as imagens Alpine do SDK do .NET Core 3.1 (para restauração de pacotes e build da aplicação) e do runtime do ASP.NET Core 3.1 (com o ambiente necessário para a execução da API REST a partir de um container):
Um recurso do Azure Container Registry chamado groffeazuredevops foi criado para este tutorial:
Maiores informações sobre como criar um recurso do Azure Container Registry podem ser encontradas no tutorial mencionado no início deste artigo.
Para o exemplo aqui descrito foi necessário ainda gerar um cluster Kubernetes, com isto acontecendo por meio da utilização do Azure Kubernetes Service (AKS):
A criação de um novo recurso do AKS também foi descrita em detalhes no seguinte artigo que produzi para o portal Microsoft Tech:
Orquestração de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
Projetos do Azure DevOps encontram-se agrupados em Organizations. Para o exemplo descrito nesse tutorial será utilizada uma Organization chamada groffe-demos.
Acionar dentro da Organization a opção + New project:
Preencher então os campos Project name e Description, selecionando ainda a opção Private , Git em Version control, Basic em Worker item process e finalmente clicando na opção Create:
Na próxima imagem aparecerá o projeto APIContagem-TutorialKubernetes já criado:
Será por meio do Azure Pipelines que definiremos o processo automatizado (pipeline) de build de imagens Docker e deployment em um cluster Kubernetes para o projeto APIContagem-TutorialKubernetes. Na seção Summary deste projeto acessar na barra lateral Pipelines > Pipelines, como indicado na imagem a seguir:
Acessar agora a opção Create Pipeline:
Em Where is your code? selecionar a opção GitHub YAML:
Já em Select a repository definir o repositório do GitHub ao qual estará atrelado o pipeline de build:
Será solicitada neste momento a autenticação junto ao GitHub. Realizado este procedimento, aparecerá agora uma tela com informações do Azure Pipelines:
Descer então com a barra de rolagem até o final da página, certificando-se de que o repositório escolhido está selecionado em Repository access. Confirmar esta escolha acionando o botão Approve and install:
Em Configure your pipeline selecionar a opção Deploy to Azure Kubernetes Service, a fim de iniciar a montagem do pipeline com um mínimo de configurações:
Em Deploy to Azure Kubernetes Service selecionar a Azure subscription em que se encontram o cluster do AKS e o Container Registry criados anteriormente:
Após a autenticação no Microsoft Azure aparecerá agora o painel Deploy to Azure Kubernetes Service com os campos para a seleção do Azure Container Registry e do cluster Kubernetes:
Concluir este procedimento acionando o botão Validate and configure.
A partir da tela Revise your pipeline YAML será possível editar as configurações do pipeline:
Localizar o ponto no qual se encontra a task para build e push da imagem no Azure Container Registry:
Incluir logo após $(tag) o valor latest, a fim de gerar uma imagem Docker também com esta tag (uma convenção adotada para a mais recente imagem criada em um Container Registry). Já a expressão $(tag) conterá o BuildId do projeto do Azure DevOps (com a geração de uma imagem cuja tag será identificada por este valor):
Após esse ajuste o pipeline contará com o seguinte código YAML:
Concluir a configuração deste pipeline acionando a opção Save and run:
Em Save and run:
Acionar finalmente o botão Save and run.
Neste momento terá início um Job para build e deployment da aplicação de testes (estágios destacados em vermelho):
Clicando sobre os Stages podemos observar o andamento dos mesmos:
Após algum tempo o status do Job indicará que os stages de Build e Deploy tiveram sucesso(ícones em verde):
Observando o repositório no GitHub será possível notar a presença do arquivo azure-pipelines.yml e do diretório manifests, gerados via Azure DevOps e no qual constam as definições de build/deployment:
O diretório manifests conterá os arquivos YAML com as definições para a criação dos objetos Deployment e Service, bem como do Pod no qual a aplicação será executada. Para escalar a aplicação podemos alterar o arquivo deployment.yml indicando o número de réplicas/instâncias da mesma:
Acessando o recurso do Azure Container Registry criado na seção anterior será possível notar a presença da imagem apicontagem em Repositories:
Clicando sobre esta imagem serão listadas as diferentes tags/versões existentes para a mesma (81 e latest, que basicamente correspondem à mesma imagem):
Ao acessar via PowerShell as estruturas do namespace tutorialartigo teremos o Deployment, o Pod e o Service criados para a aplicação de testes:
Um teste de acesso à API que está no IP público 23.96.122.184 trará como resultado:
A gravação de uma alteração na branch master fará com que o processo de build e deployment seja disparado automaticamente. A fim de simular isto farei uma alteração no arquivo ContadorController.cs (a mudança realizada aqui foi no conteúdo da propriedade Local):
Acessando o pipeline no Azure DevOps será possível constatar que um novo processo de build e deployment está em execução:
Na imagem a seguir visualizamos detalhes deste processo:
Concluída a execução do pipeline a execução do comando kubectl get pods mostrará que um novo Pod foi gerado para a aplicação:
Um teste via browser trará o nome do novo Pod (propriedade machineName), bem como o conteúdo alterado para a propriedade Local:
Docker - Guia de Referência Gratuito
Kubernetes - Guia de Referência Gratuito
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
14 
14 
14 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/microsoftazure/deploy-webassembly-from-github-toazure-storage-static-websites-with-azure-pipelines-a15f05d26fb8?source=search_post---------380,"There are currently no responses for this story.
Be the first to respond.
WebAssembly is a new technology (just released in 2017) that enables a stack-based virtual machine to run byte code (called Wasm) in your browser without plugins. The latest stable version works in all modern browsers, including mobile. The byte code format, standard instruction set, and simple memory model enables Wasm to run at near-native speeds. It also serves as a viable compilation target for multiple languages. The key benefits of Wasm include better performance compared to JavaScript, a smaller footprint for client-side code, and the ability to reuse existing software written in the language of your choice.
Wasm is stored in a static file that can be loaded along with HTML, JavaScript, CSS, images, and other website assets. This makes Azure Storage Static Websites a perfect hosting platform for WebAssembly apps. Open source projects hosted on GitHub can take advantage of free Azure Pipelines instances to build and deploy your apps. This post explores how to set up continuous integration and deployment (CI/CD) for WebAssembly apps.
I wrote a series of articles to document my investigation of WebAssembly beginning with a unique framework called Blazor. Start with the .NET Framework — including the Common Language Runtime (CLR) — build it on top of WebAssembly, then add functionality to render and manage Single Page Applications using Razor templates and C#, and the result is Blazor (Browser + Razor templates). My first step in learning the framework was to port an existing Angular 2 app. I then created a Blazor presentation with several example applications:
blog.jeremylikness.com
After recording and publishing a video series about Azure DevOps for .NET Developers, I immediately realized that because the demo apps are Single Page Apps deployed as a set of static assets, I can host them on storage and automate the build and deployment.
To begin, open your GitHub project and navigate to the marketplace.
Search for and choose Azure Pipelines.
Follow the prompts to create your Azure DevOps account. After you’ve created or selected a project, you are prompted to apply to all projects or pick specific ones. I recommend one Azure DevOps project per GitHub repository to start with. When prompted, choose public for your project. That doesn’t mean anyone will be able to see your secrets or deployment credentials; it gives them access to the build status and details. The deployment will be separate.
After you select the GitHub repository, a default azure-pipelines.yml file is generated based on preliminary inspection of your source code. For example, a .NET Core project will automatically generate a .NET Core pipeline. The initial pipeline is just a skeleton, you can choose “save and run” then throw away the results. The pipeline is checked into source control (without any secrets) so that users who fork your project can easily create their own automated builds. You can also use an existing pipeline as a starter template for new builds you create.
The first build process I created was for my Blazor examples. There are several examples in the same repository with separate solutions, so I created multiple build steps. You can inspect the build pipeline here.
The start of the configuration looks like this:
The build will automatically trigger based on a commit to the master branch. It uses a hosted Linux image (Ubuntu 16.04) and is configured for “release” or production.
📃 Learn more about hosted agents, including how they are configured and what software is installed, here: Microsoft-hosted agents for Azure Pipelines.
The next section of the Yaml file details the build steps.
Blazor is currently in preview, so I use a .NET Core task to install the correct preview version. Next, each project is built and published into its own directory. Notice the use of the $(Build.ArtifactStagingDirectory) to reference where distributions should reside.
📃 You can read the full list of Azure Pipelines predefined variables.
Notice the last step publishes the artifacts. This places them in a compressed archive that you can inspect and download after the build. It also makes the artifacts available to the release pipeline that will deploy your assets. More on that later.
Someone wiser than me once said that to truly learn a technology, you should always go at least one layer beneath the surface. Underneath Blazor is WebAssembly, so I set out to learn as much as I could. There are several ways to build WebAsssembly, and the original tool chain that allows you to compile C and C++ projects to Wasm is called Emscripten.
I wrote about my experiments here:
blog.jeremylikness.com
Installing the tool chain is not straightforward and involves several steps. Fortunately, the entire development environment is configured and hosted in a Docker container. I chose to take advantage of that for my Emscripten build pipeline.
The first task installs the Docker client (the host is already available on the agent). I mount the source code onto the container and use the compiler to build the Wasm file and an associated JavaScript file that loads it. I copy the HTML and custom JavaScript for my app into the staging directory, followed by the loader and Wasm file. Finally, I publish the build artifacts.
Go is a powerful language that supported multiple platforms from the beginning. After WebAssembly was released, the Go team added a new target operating system (JavaScript) and platform (Wasm) for builds. In an exercise to learn more about Go and how it generates WebAssembly, I ported an old “demo scene” plasma effect I used in the 1980s to run in the browser.
I wrote about it here:
blog.jeremylikness.com
Go also provides a fully configured Docker container. Here is the build pipeline:
Notice there are two steps that use the same Docker container. Emscripten generated its own JavaScript loader for Wasm customized to the project. Go, on the other hand, ships a file named wasm_exec.js with each version that supports WebAssembly. This file is necessary for JavaScript to interact with the Go Wasm app, so I copy it from the container and into the staging artifacts.
Go is available as a build step/environment as well. I could have skipped the Docker image, configured Go on the build agent and built everything directly. However, I already was using a Docker-based build so it was faster for me to use the same steps for my build pipeline. Choices are good.
The Rust 🦀 language is ideally suited to WebAssembly development. Unlike Go and C# that both require runtimes to execute (the Go file is over two megabytes in size as a result), Rust generates low-level platform-ready code like C/C++ but with a mature syntax and many built-in features that provide security and thread safety. Rust also embraced WebAssembly very early and built a set of tools to support Wasm development. As a result, Rust makes it much easier to build and package WebAssembly apps while producing streamlined byte code (the Rust Wasm file is only 61 kilobytes or about 3% the size of what Go generates).
I wrote about the experience here:
blog.jeremylikness.com
The Rust build pipeline is significantly different that the previous ones.
The environment is setup to use Node.js. A command-line task installs the Rust toolchain (“Rustup”), followed by “wasm-pack” that is used to build Wasm apps.
The need to install Rustup manually will go away soon. As of this writing, a pull request is in review to add Rust as a first-class pipeline task.
One folder contains the Rust project. That project is built using wasm-pack and generates the Wasm assets. Rust not only provides code to help connect structures in Rust to JavaScript, it also wraps the WebAssembly memory with code that makes it easier to pass buffers to and from Wasm. The project is linked to a Node project that contains the host HTML and JavaScript. A final npm build step packages everything together as a distinct set of assets to deploy, and the last task publishes artifacts directly from the distribution folder.
Now artifacts exist for multiple flavors of WebAssembly projects. The next step is to host them using inexpensive Azure Storage.
At this stage if you’re like me, you’re probably excited about getting automated builds up and running and want to share it with the world. Azure Pipelines makes this easy for you. Navigate from “Azure Pipelines” to “Builds” and select the three dots in the upper then click on “status badge.”
The resulting dialog will allow you to specify some parameters and provides both a link to the status badge and markdown you can easily cut and paste into your README.md file.
The next few steps require an Azure subscription. If you don’t have one already, you can grab a free Azure account. Use the ➕ in the upper left to add a new resource and choose Storage account.
Give the storage account a name, pick your resource group, and be sure to select “Storage V2 (general purpose v2)” as the account kind. Pick a replication option that suits you (higher availability comes at a higher cost). Review, then create the storage account and wait for the notification that is ready and available.
Static websites are not enabled by default. To enable them, choose the “static websites” option, flip to “enabled” and optionally enter a default document and error document (what will be served when files aren’t found).
After you click “save” a special blob storage container named $web is created. This is the “root” of your static website. You will also be presented with the URL you can use to access the site.
Navigate to “Releases” under “Azure Pipelines.” You will be presented with the option to create a new pipeline. Click “new pipeline” and choose “empty job.” You can name the first stage of your deployment pipeline. I’m using mine for demo assets, so I only have one stage and name it “static websites.”
Next, add an artifact to feed into the release pipeline. See the “add” link.
A new dialog will appear. Select your project and source, and the default artifacts will populate. Click “Add” to add this artifact.
To enable continuous deployment (that will be triggered after every successful build) click on the little “trigger” icon in the upper right.
Finally, flip “continues deployment trigger” to “enabled.”
Next, click “view stage tasks” to begin building the deployment.
I’m using a single static website to host multiple projects. The projects live under a folder path and not at the root of the website, so I need to specify the base URL using the <base href=""""> HTML tag. The easiest way to do this is to click the little ➕ next to the agent job to add a new task then go the marketplace and install the free RegExReplace Build Task. In the resulting dialog, I specify the path to the main HTML file that hosts the app. The regular expression varies from project to project. In the case of the Rust app, there is no existing tag to replace. Therefore, I search for the end of a style tag I know exists and append the base tag to the end of it. The regular expression and replacement look like this:
Notice that I put my Wasm apps under the wasm folder with each project in its own folder, in this case PlasmaWasmRust. I do this for every project that is deployed. The advantage of applying this transformation during the deployment phase is that the exact same build artifacts can be used for any environment. If my staging truly was “staging” and required another step for production, I can use the same build artifact and apply a different transformation to target the production URL.
To copy the artifacts, use the “Azure File Copy Task.”
Give the task a name and click the ellipses after “source” to navigate to your artifact folder. Here I’ve selected zmachine.
Pick “Azure Resource Manager” for the type and choose your subscription from the drop-down.
Note: if your subscription does not display automatically, you may need to use a service account to connect to Azure. To learn how, read: Connect to Microsoft Azure.
Pick “Azure Blob” as the destination type, then select the name of your storage account. The container name will be $web and the blob prefix is where you specify the folder path to place the artifacts in (it is the same “path” as the base URL but without the leading slash). Here is the task for the Rust project:
This step works for copying all of the main files into your static website, but one additional step is needed. The file copy task by default sets MIME types for files based on their extensions. This ensures they load correctly in your web browser. WebAssembly is a newer content type, so the current version of the task uses a default type that prevents it from loading correctly.
To adjust the default behavior, copy the .wasm files a second time and pass an argument that specifies the content type. Start by adding another Azure File Copy task. This time, set the source to a pattern that matches only your Wasm files:
$(System.DefaultWorkingDirectory)/_JeremyLikness.PlasmaWasmRust/plasmawasmrust/*.wasm
Next, add the optional arguments to set the file content type.
This last and final step should be all that is needed to successfully deploy your app. Save it and launch the stage (or commit to master and follow the entire pipeline from build through deploy) and see your app come out the other side!
You have a build badge, so why not a deployment one? To add your deployment badge, open the release pipeline and navigate to “options” then “integrations.” Enable the status badge and you’ll receive the appropriate link.
Azure Pipelines is free for open source projects hosted on GitHub. The goal of Azure Pipelines is to build any language, on any platform, and automate deployment to any destination. Do you have a mobile app? That’s no problem because Azure Pipelines will build your iOS code on a macOS agent (or Android code using a Java-based SDK) and automatically deploy it for review to the app store. Does your app have a comprehensive unit test suite? No problem! Azure Pipelines will run your tests and publish results, including code coverage, to your project dashboard.
Do you have an open source project that will benefit from automation?
▶ Get started building your GitHub repository.
Any language.
125 
3
125 claps
125 
3
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Written by
Author, mentor, speaker with passion to empower developers to be their best. Senior Program Manager for .NET Data at Microsoft. Opinions my own. @JeremyLikness
Any language. Any platform. Our team is focused on making the world more amazing for developers and IT operations communities with the best that Microsoft Azure can provide. If you want to contribute in this journey with us, contact us at medium@microsoft.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ericsk/microsoft-ai-%E5%A4%A7%E8%97%8D%E5%9C%96-1-azure-machine-learning-services-%E7%AC%AC%E4%BA%8C%E4%BB%A3-ae11c2efb08c?source=search_post---------381,"Sign in
There are currently no responses for this story.
Be the first to respond.
Eric ShangKuan
Nov 4, 2017·11 min read
這時一篇介紹由 Microsoft 提供各種與機器學習 (Machine Learning) 相關服務、平台、工具的系列文章，以下是系列目錄：
Azure Machine Learning Services 不是新的服務，2014 年就以 Azure Machine Learning Studio 的形式推出，它提供想要做機器學習的資料科學家或程式開發人員一個圖形化的介面設計機器學習的資料與運算流程，並且提供依照需求的運算資源訓練出學習模型，最後以 Web Services 的形式來取用訓練好的模型。
今年 (2017 年) 9 月，微軟推出新一代的 Azure Machine Learning Services，有別於第一代，新一代的 Azure Machine Learning Services 希望給機器學習的開發人員有一套好用的開發工具，並且能夠更輕易地將開發好的訓練程式部署在各種環境下執行訓練，同時又能管理及部署訓練好的模型。
它重新設計的平台及服務如下：
以下分別介紹每一項服務的細節。
Azure Machine Learning Workbench 提供一個桌面工具（支援 Windows 及 Mac OS X），它可以幫你管理一個機器學習的專案要在本機、docker 或是遠端環境中執行，同時在這個工具裡面也包含了：
簡單地說，這個工具就是讓你在開發機器上寫程式時的小幫手，不僅可以在開發過程中測試，也很方便部署到其它環境來執行運算。
機器學習的專案開發完畢後，最重要的就是跑訓練及測試，Azure Machine Learning Experiment Service 定義了一套機器學習專案的執行環境，只要機器學習專案是按照這個執行環境開發，這樣就能在支援這樣執行環境的：
中來執行專案。而且它也有很好的獨立性，讓你的訓練運算不會受到太多環境的干擾。
當機器學習專案訓練完畢後，產生的模型就可以為後續所套用，透過 Azure Machine Learning Model Management Service 就可以把這些訓練好的模型部署在不同的環境中，像是：
有了這樣的機制，你就可以在雲端或是強大的伺服器上跑機器學習的訓練算，而訓練完的模型則可以部署到 IoT 裝置上直接套用。
這部份是用 docker container 的形式來呈現，所以只要能跑 container 的環境幾乎都能跑，需要 GPU 的就要裝好相關的驅動程式及工具（如：nvidia-docker）
MMLSpark 是一套開源的 Apache Spark 函式庫，它讓開發人員在使用 Spark 做機器學習專案時更容易與 Micorosft Cognitive Toolkit (CNTK) 與 OpenCV 整合，更多細節可以參考這頁說明。
這是一個 Visual Studio Code 的外掛程式，它讓 Visual Studio Code 可以與 Azure Machine Learning Services 結合，除了開發程式之外，也可以直接在 Visual Studio Code 管理訓練的運算工作以及部署模型。
以下以兩個例子來瞭解使用 Azure Machine Learning 的 end to end 體驗。操作之前，建議先閱讀這頁說明將 Azure Machine Learning Services 中的 Workspace 建立好且安裝 Azure Machine Learning Workbench。
當然也建議安裝 Visual Studio Code 並且安裝 Visual Studio Code Tools for AI 的外掛套件。
MNIST 手寫數字辨識是很經典的深度學習專案，我們可以從 Azure Machine Learning Services 中的專案範本直接建立一個使用 CNTK 來開發的 MNIST 手寫數字辨識的學習專案。
建立專案有兩個方式：
不論用哪種方式建立專案都是相同的，在 Azure Machine Learning Workbench 中可以設定專案用 VSCode 開啟；VSCode 在建專案時也會要你填入對應的 Azure Machine Learning Services 的 Workspace。
完成專案建立後，你會得到一個已經用 CNTK 寫成的深度學習專案 (也就是不用寫程式就能動了)，這就可以來體驗一下怎麼把這個專案放在 Azure Machine Learning Experiment Service 中執行。
在 Azure Machine Learning Workbench 開啟這個專案，你會看到專案首頁就可以直接執行專案，而且可以選擇 local 或 docker （系統必須先安裝好 docker 的環境）來執行。
不過不要馬上急著執行，因為你很有可能還沒有在 Workbench 的環境中安裝 CNTK 的函式庫，所以請照著頁面下方說明安裝好 CNTK 的套件再來執行。若是選擇在 docker 中執行則無此問題，因為它會把該安裝的環境在 container 中設定好。
我推薦盡量用 docker 環境跑，因為實務上開發完成應該都會丟到遠端的機器上執行，這也會是用 docker 來執行。
運行後就可以觀看結果：
若是在 Visual Studio Code 操作，在 Workbench 開啟的狀態下，你也可以直接在 VSCode 中提交運算的工作：
如果在本機上執行沒有問題，打算調整參數把它部署到雲端執行（透過 docker container 的形式）也很容易，只要把要運行的環境（例如：在 Azure 上含有 GPU 規格的 N 系列虛擬機器），那就把機器的組態設定一下，讓它變成 Azure Machine Learning Experiment Service 就可以整合在 Workbench 的操作中。
如果要跑在 GPU 的環境，別忘了修改 aml_config 目錄下的組態檔，使用 GPU 版的 CNTK 及 MMLSpark，還有目標機器上也要先裝好 docker 跟 nvidia-docker 的環境。
另外，Workbench 會在目標機器上使用 sudo 來執行 docker/nvidia-docker，但是需要被設定無密碼保護，所以也要在目標機器上修改 /etc/sudoers 把登入的帳號加入不需要密碼。
按照說明（在 Workbench 的專案首頁即可看到）設定 compute target 之後，你就可以在 Workbench 或 VSCode 之中看到新的目標環境可以執行運算：
當工作完畢時，你也可以從 job detail view 中把訓練好的模型保存下來（放在 artifacts 目錄下）：
有了新一代的 Azure Machine Learning Services，最主要在開發、執行訓練測試的環境準備上幫了很大的忙，你可以在一般的機器上進行機器學習專案的開發測試，而真的需要跑大量資料的訓練以及測試時，再把這個專案部署到合適的機器（例如有很強很多顆 GPU 的環境）上進行運算。是個增加生產力的平台。
不如現在就玩玩它的 5 分鐘入門手冊體驗看看吧！
DevRel | Developholic | Technical Evangelist
See all (367)
17 
17 claps
17 
DevRel | Developholic | Technical Evangelist
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/javarevisited/top-10-cloud-certification-to-aim-in-2022-aws-azure-and-google-cloud-platform-bd054fff0538?source=search_post---------382,"There are currently no responses for this story.
Be the first to respond.
Hello guys, if you are aiming for cloud certifications in 2022 but are not sure which cloud certification should you go for then you have come to the right place. Earlier, I have shared a list of the best IT certifications for Java developers, and today, I am going to talk about the best cloud certification to aim for in 2022.
You can go through this list of cloud certifications and choose the best one depending upon your skills and experience. I have shared the best cloud certifications for beginners, developers, system admins, and solution architects from Amazon AWS, Microsoft Azure, and Google Cloud Platform.
Cloud computing services is growing exponentially in nowadays technology and become a priority among big-name organization such as Netflix which uses Amazon AWS to run their business from hosting to database and analytics.
For this reason, many cloud computing has come to the real world with different infrastructure and services such as Google Cloud and Microsoft Azure, and more.
All of those cloud services are complicated in their infrastructure and it requires the person who wants to deal with them to get some sort of certificate to deal with a specific service and companies nowadays are requiring people to have these certifications in order to validate their skills.
This article will discuss with you some of the best cloud certifications from companies like Microsoft Azure, Amazon AWS and Google cloud and having those certifications will make your resume stand out among the other competitor during the hiring process and you can get a higher salary than the others without those certifications.
Without wasting any more of your time, here is a list of the best Cloud certification of 2022. You can aim for this certification to boost your career and also start your career in Cloud Computing as Cloud Professional, Developer, and Solution architect.
This AWS certificate is designed for people who can do solution architecture such as deploying the web applications and securing them and also it targets individuals who have one year working with AWS services.
In other words, this is the best cloud certification for experienced developers who wants to become software architects or solution architects. If you have been working on the AWS cloud platform, personally or for our company then you should aim to pass this exam to get certified for your skills.
The exam is very vast and you need to know a lot of things about AWS services. If you are aiming for this prestigious certification then this AWS Certified Solutions Architect course on udemy can assist you to acquire those skills.
This is the best Cloud certification for beginners or anyone who wants to start with Cloud computing and the AWS cloud platform.
If you want to jump to Associate-level or specialty certification then make sure to get this certification that gives you an overall understanding of AWS cloud services such as security and account management and more.
This is a relatively easier exam and you can easily pass this with a couple of weeks of preparation.
If you need a recommendation, I highly recommend you check out Stephane Maarek’s AWS Certified Cloud Practitioner [NEW] course on Udemy. Stephane is an AWS Guru and this will help you learn and prepare for the exam.
This is the best Cloud certification for programmers and software developer who wants to create cloud-native applications. If you have more than one year of managing AWS services then this associate certification is right for you and it will teach you how to use the AWS core services as well as its architecture, develop, and deploy the application on AWS.
This is also one of the toughest AWS certifications, compared to the previous two certifications like cloud practitioner and solution architect. It’s not enough to just be familiar with different AWS services, you need to know them in-depth so that you can use correct configurations in a given scenario.
If you are a developer and software engineer then I highly recommend this cloud certification to you as it will significantly boost your profile and make you eligible for many more opportunities.
As I said, the exam is tough to crack and you need multiple resources to prepare well but to start with I highly recommend you to go through this course AWS Certified Developer from CloudGuru, which will teach you the skills needed to pass the exam.
This associate certification is for people having at least one year in deployment, management, and operations on AWS and teaches you to choose the appropriate service for your needs as well as control the data flow from AWS and more.
In other words, this is the best Cloud certification for system admins and IT professionals who work on the Infra side.
If you are working in IT support or working as a System admin, you can aim for this certification to further boost your career.
If you need a course to prepare for this certification, I recommend you to this course named AWS Certified SysOps Administrator — Associate is a good resource to learn those skills.
If you notice, all top four cloud certification is from Amazon AWS, and it's because AWS is the most popular public cloud platform for both startups and big companies but Microsoft Azure is catching up quickly which has, in turn, boost the demand for certified Azure Cloud professionals.
The Azure fundamentals certification is better for individuals to know some foundation of the cloud services and this certificate will teach you the cloud concepts as well as how to use Microsoft Azure services, security, privacy, and pricing.
In short, the best cloud certification for beginners who wants to learn Azure. This is very similar to AWS cloud practitioner and you can pass this certification with a couple of weeks’ preparation.
If you need recommendations then this course on udemy AZ-900 Azure Exam Prep will explain all of this in one course and prepare you for the exam.
This is the best cloud certification for experienced programmers, developers, and DevOps engineers who want to become Azure experts.
When you pass this certification exam you will have the skills to design and implement solutions in Microsoft Azure and that includes security, network computing, and storage.
If your companies are migrating into Microsoft Azure cloud then aiming for this Azure certification and boost your profile and also help you to get promoted.
When it comes to preparation, this is a vast certification and you need to cover a lot of topics but thankfully there are many courses to learn these skills but this course on udemy but the AZ-300/AZ-303 Azure Architecture Technologies is the best of them all and it will help you a lot on this journey.
This is another Azure certificate that made the list of top 10 Cloud certifications. This is the best Azure certification for system admins and people who are working in IT support.
This certificate will get you the experiences to implement, manage, and monitor cloud services such as storage, security, and virtual environment, and many more responsibilities.
For preparation, you need to know all the essential Azure services and how to use them, configure them, and troubleshoot in case of any issue.
If you need an online course, I recommend you to check out this Udemy course AZ-104 Microsoft Azure Administrator is a good resource to learn all of those skills.
A list of best cloud certifications cannot be completed without Google cloud certification, another big player in the public cloud market. Google cloud has some of the best capabilities when it comes to dealing with Big Data and Machine learning and that’s why many startups who are working in those fields are opting for Google cloud.
This is the best Google cloud certification for programmers, developers, and software engineers.
The holder of this certificate will have the responsibility of deploying web applications in the cloud as well as monitoring the operations and managing the whole enterprise solutions and configuring access and security.
Regarding preparing for this exam, this Ultimate Google Certified Associate Cloud Engineer course can help you learn and pass the exam.
This is another popular and in-demand Google cloud certification. This is similar to AWS solution architect and Azure Technology expert but on Google cloud.
This is the best Google Cloud certification for experienced IT professionals who wants to become solution architect on Google Cloud technologies like Big Table, Big Query, and other GCP platforms services.
The holder of a professional cloud architect certificate plays an important role inside the organization since he could design develop, deploy, and manage your web application as well as secure them and more responsibilities.
Regarding preparation for this prestigious cloud certification, this course on udemy Ultimate Google Certified Professional Cloud Architect is a good resource for this certificate.
This is the ultimate cloud certification you can aim for in 2022. It’s also regarded as the toughest AWS cloud certification and requires extensive experience and knowledge of the AWS cloud platform.
There is a huge demand for this certification as there is always a shortage of AWS experts and I highly recommend this to expert cloud professionals.
This advanced certification teaches you how to design and deploy scalable web applications on Amazon AWS servers as well as select the appropriate service and power to use for your application and more skills you will have.
Regarding preparation, you may need to consult a lot of resources and AWS papers, documentation, and courses but to start with this course on udemy Ultimate AWS Certified Solutions Architect will help you in this journey.
That’s all about the best cloud Certification you can acquire in 2022. Those certifications are almost the most useful in the field of cloud computing and are issued by the cloud provider itself such as Amazon, Google, and Microsoft, and receiving one of these certifications will open the door to a successful career in this growing industry.
Other Certification Resources for IT Professionals and Programmers
Thanks for reading this article so far. If you like these best Cloud Certifications then please share them with your friends and colleagues. If these courses have helped you to pass the exam, then please spread the word so that other people can also benefit.
P. S. — If you are a complete beginner to Cloud Computing and looking for some free courses to learn Cloud Computing in general then you can also check out this Introduction to Cloud Computing (FREE Course) on Udemy. More than 210,000 people have joined this course and it’s completely free, you just need a Udemy account to join this course.
Medium’s largest Java publication, followed by 14630+ programmers. Follow to join our community.
194 
194 claps
194 
A humble place to learn Java and Programming better.
Written by
I am Java programmer, blogger, working on Java, J2EE, UNIX, FIX Protocol. I share Java tips on http://javarevisited.blogspot.com and http://java67.com
A humble place to learn Java and Programming better.
"
https://levelup.gitconnected.com/build-a-custom-url-shortener-using-azure-functions-and-cosmos-db-c20e59261375?source=search_post---------383,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
This article describes how to build a custom URL shortener service using Azure’s serverless platform with Azure Functions and Cosmos DB. I had this idea after I recently read Jussi Roine’s article, where he built a URL shortener service (such as bit.ly) using a serverless Azure approach, an approach he led with Azure Logic Apps and a…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/t-t-software-solution/3-virtual-academy-for-azure-fundamentals-%E0%B9%82%E0%B8%94%E0%B8%A2-aipen-studio-aa898b2654dd?source=search_post---------384,"There are currently no responses for this story.
Be the first to respond.
ในบทความนี้จะพาทำความเข้าใจในบริการหลักๆบน Azure ให้มีความคุ้นเคยกันมากขึ้นน่ะครับ เพราะว่าก่อนที่เราจะใช้บริการไหน เราควรทำความเข้าใจแนวคิดของบริการนั้นก่อนว่าทำมาเพื่อตอบโจทย์อะไร หลังจากนั้นเราจะพิจารณาถึงค่าใช้จ่ายเพื่อให้ทราบต้นทุนเบื้องต้นก่อนเลือกใช้บริการ
Azure Compute คือทรัพยากรสำหรับการประมวลผลที่เราสามารถเลือกใช้งานได้ตามความต้องการ เช่นเรื่องของ CPUs บน VM, รัน Code โดยไม่ต้องเตรียม Infrastructure ด้วย Serverless Computing
ตัวอย่างบริการได้แก่ Virtual Machines, Containers, Azure App Service, Serverless Computing
เป็นบริการแบบ IaaS ที่อยู่คู่กับ Azure มานานที่ช่วยให้เราสามารถสร้าง Virtual Machine โดยเบื้องหลังจะมีการทำงานอยู่บน Hyper-V
เราสามารถเลือกขนาดความแรงของเครื่องที่เราต้องการและยังรวมไปถึง OS ต่างๆ ทั้ง Windows และ Linux
ซึ่งราคาก็จะคิดตามขนาดและ License ของ Software ที่เลือกน่ะครับ เช่น ถ้าผมเลือก VM ที่ติดตั้ง MSSQL Standard Edition เราก็จะต้องเสียค่า License ของ MSSQL Standard ด้วยน่ะครับ
ผมมีบทความที่เคยทำสรุปไว้เกี่ยวกับการใช้ VMs ในงานต่างๆน่ะครับเผื่อท่านใดสนใจครับผม
medium.com
medium.com
medium.com
medium.com
Availability Setsคือบริการที่ช่วยทำให้เราได้ระบบที่มี High Availability สำหรับปัญหาที่อาจจะเกิดขึ้นได้ใน Data Center เช่น Hardware พัง โดยระบบจะทำการสำรอง VMs ของเราไว้ในตู้ Rack ที่แตกต่างกัน แต่อยู่ใน Data Center เดียววันครับถ้า VMs ตัวนึงพัง เราก็จะยังสามารถใช้ตัวสำรองต่อได้ครับ เราจะได้ SLA 99.95% จากบริการนี้ (ต้องเลือก Harddisk เป็น SSD ด้วยน่ะครับ ถ้า Harddisk ทั่วไปจะไม่ได้ SLA)
Virtual Machine Scale Setsคือบริการที่ช่วยให้เราสามารถเพิ่มประสิทธิภาพในการใช้งาน VMs โดยการเพิ่มจำนวนของ VMs ที่เหมือนกันขึ้นมาและกระจายการทำงานไปยัง VMs หลายๆเครื่องแทนที่จะทำแค่เครื่องเดียว
โดยข้อควรระวัง คือ การจัดการกับ Disk น่ะครับ เพราะการเพิ่มจำนวนเครื่อง ไม่ได้เป็นการ Sync ให้ข้อมูลใน Disk ของแต่ล่ะ VMs ให้มีจำนวนที่เท่ากันอยู่ตลอดเวลา
กล่าวคือ ณ ตอนแรกที่เริ่มสร้างเครื่องที่ 2 นั้น จะได้ทุกอย่างเหมือนเครื่องแรกเลยครับ แต่หลังจากนั้นก็จะทำงานแยกกัน เพราะงั้นจึงไม่เหมาะกับการใช้ ScaleOut Database น่ะครับ เหมาะกับการใช้ ScaleOut Web App มากกว่าครับ (แบบ Stateless)
คือบริการแบบ PaaS ที่ช่วยให้เราสามารถนำ Code หรือ Container ของเราไปรันเป็น Web App, RESTful API, Background Jobs ของเราได้โดยที่ไม่ต้องกังวลในเรื่องของ Infrastructure เลย และยังมาพร้อมกับคุณสมบัติในเรื่องของ High Availability และ Auto ScaleOut อีกด้วยครับ การทำ CI/CD ก็ทำได้สะดวกมากเพราะมีวิธีที่รองรับหลากหลายรูปแบบ
รวมไปถึงยังมี Runtime ที่เตรียมมาให้พร้อมใช้งานในหลายภาษา เช่น Python, Node.js, PHP, ASP.Net/ASP.Net Core, Java เราแค่นำ Code ในภาษาเหล่านี้มาติดตั้งที่ App Service ก็พร้อมทำงานเลยครับ
ผมมีบทความที่เคยทำสรุปไว้เกี่ยวกับการใช้ App Service เบื้องต้นไว้ เผื่อท่านใดสนใจน่ะครับ
medium.com
medium.com
medium.com
Azure App Service Planเป็นบริการที่เสมือนกับให้เราเลือก Web Server เพื่อนำมาการประมวลผล App Service ที่ต้องการ
โดยเราจะต้องเลือกว่าจะเลือก Plan บน OS ไหน (Windows, Linux) เพราะจะมีผลกับทั้งราคาและบริการเสริมที่แตกต่างกัน
เรายังสามารถเลือกขนาดของ Web Server เพื่อให้ตอบโจทย์ต่อการทำงานเช่น Development/Test เราอาจจะเลือก Plan ที่มีไม่แรงมาก ซึ่งก็จะมีค่าใช้จ่ายที่ถูกกว่าหรือเลือกขนาดที่ใหญ่ขึ้นเพื่อใช้เป็น Production ต่อไป
ตัว Container นั้นเป็น Technology ที่ช่วยในการจัดการ Environment ในคอมพิวเตอร์หลายๆเครื่องให้เหมือนกันและทำงานได้ผลลัพธ์ในแบบเดียวกันครับ
โดยจะทำการรวม Code, Runtime, System Tools/Libraries/Settings ที่เกี่ยวข้องเข้าด้วยกัน (เราเรียกว่า Container Image) และนำ Container Image ไปทำงานบนคอมพิวเตอร์เครื่องอื่นๆที่ติดตั้ง Container Technology ครับ
เมื่อ Container Image ถูกนำมารันที่เครื่องจะกลายเป็น Container น่ะครับ
โดย Docker ได้รับความนิยมมากที่สุดในตอนนี้
Azure Container Instance (ACI)เป็นบริการที่ทำให้เราสามารถนำ Container Image มาทำงานได้อย่างรวดเร็ว ไม่จำเป็นต้องตั้งค่าอะไรเพิ่มมากนัก
โดยค่าใช้จ่ายจะคิดตามปริมาณการใช้งานจริง เช่นเราเปิดระบบไว้ 30 วัน แต่มีคนใช้งาน Web เราแค่ 1 วัน เราก็เสียค่าใช้จ่ายแค่ 1 วันน่ะครับ
Azure App Service for Containersเป็นบริการที่คล้ายกับ Azure Container Instance (ACI) แต่จะต่างกันตรงที่การจัดการจะเป็นในมุมของการใช้ Web App Service เช่นถ้าอยาก Scaleup ก็ต้องไปเพิ่ม Size ของ App Service Plan
ในเรื่องของค่าใช้จ่ายก็จะแตกต่างจาก Azure Container Instance (ACI) เพราะว่า ใช้รูปแบบ Azure App Service Plan ที่จะเหมาจ่ายรายเดือนตาม Plan ที่เราเลือกครับ ถึงลูกค้าจะใช้งานหรือไม่ได้ใช้งาน Web เราก็จะมีรายจ่ายคงที่ตามเดิมน่ะครับ
Azure Kubernetes Service (AKS)เป็นบริการที่ช่วยให้เราจัดการกับการ ScaleOut Container Images ให้มีหลายๆ Instances ได้อย่างสะดวก (เราเรียก Technology นี้ว่า Container Orchestration) ด้วย Kubernetes (K8s) ที่รันอยู่บน Azure ครับ
จะเป็นบริการที่เราสนใจเฉพาะการพัฒนา Code ที่จะนำมารันเท่านั้น ในส่วนของ Infrastructure นั้น Azure จะดูแลให้
โดยมีส่วนที่ต่างจาก App Service คือเราจะไม่สามารถปรับแต่งอะไรได้มากนัก เพราะทาง Azure จะทำให้อัตโนมัติ เช่นเรื่องของ Scaling
และการคิดเงินจะคิดตามปริมาณการใช้งานจริง ซึ่งจะแตกต่างจาก App Service ที่คิดเหมาจ่ายตาม Package รายเดือน (App Service Plan) เพราะงั้นถ้าระบบมีผู้ใช้งานน้อยค่าใช้จ่ายก็จะถูกลงมากครับ
Azure Functionเป็น Serverless Computing ในรูปแบบ Code-Based ที่รองรับหลายๆภาษาน่ะครับ โดยการทำงานจะขึ้นอยู่กับ Event ที่เราเลือกไว้
Azure Logic Appเป็น Serverless Computing ในรูปแบบ Web-Based Designer ที่เราสามารถเลือกขั้นตอนการทำงานได้ในรูปแบบ Worflow ได้โดยที่เราไม่ต้องเขียน Code นะครับ
ตัวอย่างการใช้งานเช่น เขียนเงื่อนไขให้ส่ง Email ทุกครั้งที่มี Twitter จาก Microsoft
เป็นบริการที่เกี่ยวกับการจัดเก็บข้อมูลที่มาพร้อมกับกับบริการเสริมที่ช่วยให้เราจัดเก็บข้อมูลได้อย่างมีประสิทธิภาพ
ตัวอย่างเช่น
Structured Data เป็นข้อมูลในรูปแบบที่มีโครงสร้าง (Schema) ทำให้ข้อมูลที่จัดเก็บจะมีจำนวน Fields, Properties ที่เท่ากัน
ตัวอย่างเช่นการเก็บข้อมูลในฐานข้อมูล แบบ Relational Database เช่น Microsoft SQL Server, MySql, PostgreSQL
ข้อดีคือทำการสืบค้นข้อมูลได้ง่าย แต่ก็แลกมาด้วยโครงสร้างการจัดเก็บข้อมูลที่ซับซ้อน
Semi-structured Dataจะเป็นโครงสร้างในรูปแบบลำดับขั้น (Hierarchy) ที่ไม่ได้มีโครงสร้างที่ชัดเจนแบบ Table, Row เหมือน Structured Data
ใช้ tags หรือ keys ในการจัดการข้อมูล ตัวอย่างเช่น NoSQL เช่น Redis, Cassandra, MongoDB, Elasticsearch
ข้อดีคือมีความยืดหยุ่นในการปรับเปลี่ยนการจัดเก็บข้อมูล, ทำ Cluster ได้ง่าย แต่ก็แลกมาด้วยการสืบค้นข้อมูลที่ซับซ้อนขึ้น
Unstructured Dataคือข้อมูลที่ไม่มีโครงสร้างทำให้ไม่มีข้อจำกัดในการจัดเก็บข้อมูล ตัวอย่างเช่น Text/PDF/Document/Image/Video File
คือบริการแบบ PaaS ที่ช่วยจัดเก็บข้อมูล Structured Data ด้วย Microsoft SQL Server Database (ขอเรียกย่อๆว่า MSSQL น่ะครับ)
โดยมีบริการเสริมที่ช่วยให้เราดูแลข้อมูลได้อย่างมีประสิทธิภาพ เช่น การเพิ่มลดขนาดในการประมวลผลของ Database Server, การทำ Failover Cluster, การสำรองและกู้คืนข้อมูล (Backup & Recovery)
ที่เทพมากๆคือ Azure มี AI ที่ช่วยตรวจสอบการทำงานของฐานข้อมูลเราและให้คำแนะนำเพิ่มเติมครับ เช่นเราควรจะเพิ่ม Index ให้ Field นี้เป็นต้น
ถ้าลูกค้าที่ใช้ MSSQL อยู่แล้วสามารถที่จะนำฐานข้อมูลขึ้นมาใช้บน Azure SQL Database ได้สะดวกมากครับ แต่จะติดข้อจำกัดบางอย่างที่ทำให้ไม่สามารถเอาขึ้นได้ เช่นข้อจำกัดในการ Join ข้อมูลข้าม Database, การใช้ Link Server และอาจจะต้องพิจารณาบริการอื่นแทนเช่น MSSQL on VM, SQL Managed Instance
ผมมีสรุปบทความตัวอย่างการใช้งาน SQL Database เบื้องต้นกับงานจริงครับ เผื่อเป็นประโยชน์น่ะครับ
medium.com
medium.com
medium.com
คือบริการจัดเก็บข้อมูลแบบ Semi-structured Data โดยมีความสามารถพิเศษคือเป็น Distributed Database ที่กระจายการทำงานไปยังหลายๆ Database ซึ่งทำให้ได้ High Availability เหมาะสำหรับการจัดการข้อมูลแบบ Real-Time มากครับ
บริการจัดเก็บและวิเคราะห์ข้อมูลในระดับ Big Data โดยเฉพาะครับ
คือบริการสำหรับเก็บข้อมูลต่างๆ เช่น ไฟล์รูปภาพ, ไฟล์วีดีโอ, SMB file share ซึ่งจะถูกแบ่งออกเป็น 4 ประเภท คือ BLOB, File Share, Table, Queue
Azure Blob Storageบริการจัดเก็บ Unstructured Data ตัวอย่างเช่น Text / PDF / Document / Image / Video File
ความเทพคือทำ Video Streaming ได้ด้วยน่ะครับเพื่อตอบโจทย์การทำงานแบบ Video-On-Demand Solution
Azure File Storageบริการแบบ PaaS ที่รองรับการ Share File บน Protocal Server Message Block (SMB) ที่ช่วยให้เราเข้าถึง file ได้ผ่านการ Mount Share Drive บน Winidows, Linux, MacOS ครับผม
รองรับการ Encryption ทั้งแบบ At Rest (ข้อมูลที่ถูกจัดเก็บอยู่ใน Storage) และแบบ In Transit (รับส่งข้อมลู) ด้วยครับ
Azure Table Storageเป็นบริการจัดเก็บข้อมูลแบบ Semi-Structured โดยจะจัดเก็บในรูปแบบของ NoSQL แบบ Key-Value (เก็บข้อมูลเป็น Key คู่กับข้อมูลที่เราต้องการจะเก็บนะครับ)
Azure Queue Storageบริการ Message Queue เพื่อช่วยในการทำงานที่ต้องรองรับการรับส่งข้อมูลจำนวนมาก โดยเปลี่ยนมาเป็นการทำงานแบบ Asynchronous โดยจะเอางานที่เข้ามาใหม่จัดเก็บลงใน Queue หลังจากนั้นจะมีขั้นตอนในการดึงงานจากใน Queue เพื่อมาประมวลผลต่อ
เหมาะกับงานที่ไม่ได้ต้องการทันทีทันใด และไม่ได้การเรียงลำดับในการทำงาน เช่น การสร้าง Thumbnails หลังจากที่ User Upload ภาพ
หรือจะเรียกว่า “Azure Managed Disks” ก็ได้ครับ เป็น Virtual Disk ที่เราจะนำไปใช้ใน VMs โดยตรงเพื่อความสะดวกในการจัดการกับ Disk ครับ
ตัวอย่างเช่น OS Disk, Data Disk
Storage Tiers
เป็นชนิดของการความเร็วในการเข้าถึงข้อมูลและราคาที่ใช้จัดเก็บ
Azure จัดเตรียมบริการที่ช่วยให้เราจัดการ Network ทั้งแบบ On Premise และ Cloud ให้ทำงานร่วมกันได้อย่างปลอดภัยและมีประสิทธิภาพครับ เรามาศึกษาบริการที่มีกันครับ : )
บริการฟรีในการจัดการ Network เสมือนให้กับบริการบน Azure เชื่อมต่อกันได้อย่างปลอดภัยครับ
เช่นเมื่อเราสร้าง VMs เราก็จำเป็นต้องสร้าง Virtual Network ขึ้นมาเสมอ และยังสามารถทำ VPN Site-to-Site เพื่อเชื่อม Network ของ On-Premise และ Azure เข้าด้วยกัน
เราจำเป็นต้องกำหนด Subnets ให้กับ Virtual Network ด้วย ซึ่งทาง Azure จะกำหนดเป็นแบบ DHCP เพื่อแจก IP ให้โดยอัตโนมัติ
Network Security Group (NSG)
เป็น Firewall ที่มาพร้อมกับ Virtual Machine ที่เราสร้าง เพื่อจัดการสิทธิในการเปิดปิด Port ถ้าเป็น Windows จะเปิด RDP Port 3389, ถ้าเป็น Linux จะเปิด SSH Port 22 ให้โดยอัตโนมัติ
Load Balancing คือบริการที่ช่วยรองรับจำนวน Request ที่จะเข้ามายัง Servers ของเราได้อย่างเหมาะสม ด้วยการกระจาย Request ไปยังหลายๆ Servers
โดยที่ Servers เหล่านี้ควรทำงานแบบ Stateless เพื่อให้ไม่มี State ถูกจดจำไว้ใน Server ใด Server นึง ทำให้เวลาที่ User คนเดิมถูกโยนไปยัง Server ถัดไปก็สามารถทำงานที่ค้างไว้ต่อได้ตามปรกติ
Azure มีบริการ Load Balancing อยู่หลายรูปแบบดังนี้
Azure Load Balancerทำงานอยู่ใน OSI Model ระดับ 4 (Transport Layer) บน Protocal TCP, UDP ซึ่งเน้นทำหน้าที่ในการรับส่งข้อมูลระหว่าง 2 Endpoint เท่านั้นครับ (Source IP address and port to destination IP address and port)
เหมาะใช้กับ Azure Virtual Machine
เราจะได้ Public IP เพื่อให้ User เข้าถึง Load Balancer ได้จากข้างหน้า ส่วนหลัง Load Balancer เราสามารถเลือกเป็นการสื่อสารผ่าน Private IP ในการเชื่อม Load Balancer ไปยัง Servers อื่นๆ เพื่อความปลอดภัยมากยิ่งขึ้นเพราะไม่สามารถเข้าถึงได้จากภายนอกโดยตรง
Azure Application Gatewayทำงานอยู่ใน OSI Model ระดับ 7 (Application Layer) ที่นำความสามารถในการรับส่งข้อมูลจาก Transport Layer มาทำงานอื่นๆเพิ่มเติม ทำให้ Application Gateway สามารถทำงานได้มากกว่า Azure Load Balancer ตัวอย่างเช่นการทำ Cookie-Based Session Affinity, URL Path-Based Routing, Multisite Hosting
เหมาะใช้กับ Web Application (HTTP/HTTPS) เช่น Azure App Service, ACI, AKS
Azure Traffic Managerบริการจัดการ Network ในกรณีที่ลูกค้าเราอยู่หลายประเทศ เราจะใช้ Traffic Manager ในการเลือก DNS Server ที่อยู่ใกล้กับลูกค้าในประเทศนั้นมากที่สุด เพื่อให้ลูกค้าเข้าถึง Web App ที่เราติดตั้งไว้ในพื้นที่นั้นตัวอย่างเช่น ถ้าผู้ใช้งานมาจากยุโรป ตัว Traffic Manager จะพาผู้ใช้เข้าถึง Web App ของเราที่ติดตั้งในยุโรป แต่ถ้าผู้ใช้งานมาจากเอเชีย ตัว Traffic Manager จะพาผู้ใช้เข้าถึง Web App ของเราที่ติดตั้งในเอเชีย
Azure Content Delivery Network (CDN)บริการกระจายข้อมูลไปยังตำแหน่งที่ใกล้กับผู้ใช้งานมากที่สุดเพื่อลดเวลาในการเข้าถึงข้อมูล ซึ่ง Azure มี CDN ที่รองรับพื้นที่ได้ทั่วโลกเลยครับ ตัวอย่าง Content เช่น Images Files, Document Files
Digital Skill — Azure Fundamentals (ภาษาไทย)
ExamTopics — AZ-900 Exam Actual Questions
Facebook — Data TH.com — Data Science ชิลชิล (ภาษาไทย)
Github — Microsoft Certified Azure Fundamentals (ภาษาไทย)
Medium — Azure AZ-900 Exam Preparation Guide: How to pass in 3 days
Medium — วีธีลงทะเบียนสอบ AZ-900 Online ที่บ้านด้วย Azure Exam Voucher
Medium — AZ-900 รีวิวแนวข้อสอบและวิธีลงสอบที่ศูนย์สอบ
Medium — AZ-900 สรุปละเอียดสุดๆ
Microsoft Learn-Azure Fundamentals
Udemy — Microsoft Azure — Beginner’s Guide + AZ-900 (มีค่าใช้จ่าย)
WhizLabs — AZ-900 (มีค่าใช้จ่าย)
Workshop เล็กๆจาก Microsoft สำหรับ AZ-900 ครับผม
ในบทความนี้เราได้เรียนรู้ Core Services หลักๆที่ Azure มีให้บริการ ที่มาพร้อมกับคุณภาพ,ความปลอดภัย และ รองรับการขยายตัวในอนาคตน่ะครับ เช่น
หวังว่าบทความจะพอแนะนำให้เห็นภาพของบริการต่างๆบน Azure มากขึ้นน่ะครับ
ขอบคุณผู้อ่านมากๆครับนายป้องกัน : )
https://www.tt-ss.net/
14 

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
14 claps
14 
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Written by
Development Manager, Web Developer with ASP.Net, ASP.net Core, Azure and Microsoft Technologies
Web developers with ASP.Net, MSSQL, Azure working in Remote Office 100%
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/plumbersofdatascience/aws-azure-or-gcp-c23f6fe83f45?source=search_post---------385,"There are currently no responses for this story.
Be the first to respond.
I get often asked: What is the best or easiest cloud platform to start with for Data Engineering?
Here are my thoughts on this. A general overview.
Amazon Web Services (AWS) is globally the biggest cloud provider (approx. 50% of the market). This goes for the US and the EU and so on, except China, because they’re not in China. Microsoft Azure is number two on the market.
Google Cloud Platform (GCP) comes in third place regarding market share.
Generally, when you compare the overall tools and functionalities that are on these platforms, it’s most of the time the same. All three named cloud services are almost equivalent. Which one you choose is up to your preferences.
If you work in a company, you should pick the cloud platform they are using. Personally, I like AWS and have worked with AWS. It’s a bit different from working on premise.
When you look at the Azure interface and how you set up stuff it’s a bit more aimed towards companies who have already worked with on-premise. And have employees who are familiar with on-premise skills and how to set up traditional infrastructures. That’s where Azure I think shines a bit more.
If you are looking for a job, do some research on the job ads in your area. What are the requirements in the job descriptions? What are employers looking for? Evaluate and then learn exactly that.
There are high chances that you find a job where you can apply AWS skills — Azure or GCP will be less..
Which cloud provider you should learn also depends on where you are. Look at the regional preferences and make your choice afterwards.
For example, if you’re in the US I would look at AWS as it’s the most used there. Students from Norway say that in Norway it’s just Azure. Almost nobody uses AWS in Norway for instance. Then it makes sense, if you want to stay in Norway and get a job there, that you’ll learn Azure.
The industry in which you are working or aiming to work also influences the cloud service. E. g. Banking, eCommerce or Aeronautics all have special needs and preferences for cloud providers.
You see, it does not matter which is the easiest one to learn, or which one has the best functionality.
You always have to check carefully what cloud provider suits the best for your existing or future job. If you’re starting from scratch and you get some cloud experts I would maybe look into AWS more.
See you later.
Andreas
The Data Engineering Community
33 
33 claps
33 
Written by
Data Engineer and Plumber of Data Science. I write about platform architecture, tools and techniques that are used to build modern data science platforms
The Data Engineering Community, we publish your Data Engineering stories
Written by
Data Engineer and Plumber of Data Science. I write about platform architecture, tools and techniques that are used to build modern data science platforms
The Data Engineering Community, we publish your Data Engineering stories
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@rpradeepmenon/architecting-modern-data-engineering-using-azure-databricks-18686ac1c5ff?source=search_post---------386,"Sign in
There are currently no responses for this story.
Be the first to respond.
Pradeep Menon
Aug 18, 2020·6 min read
There should no doubts in anyone’s mind about how Big Data and AI are fueling the next revolution. Data is the new oil, and AI refines the oil. The questions to ask is the following:
Data Engineering has become vital for any organization that is serious about harnessing data.
This blog illustrates how Azure Databricks strives to modernize and yet simplify data engineering to reduce the data to insight turnover time.
Let us start by understanding the market.
According to IDC, there are three major trends when it comes to Data and AI.
Cloud computing is the primary catalyst to make these changes happen. Cloud enables innovation.
Forbes predicts that, in 2030, the global business value derived from Advanced Analytics & AI will be $15 trillion. This bucket is distributed across smart products, virtual agents, and decision automation.
The potential is immense.
The path to innovation is never easy. It has its road-blocks. Three key challenges prevent organizations from realizing their objectives with Big Data projects are:
By mapping these challenges to the study conducted by TDWI, it is clear that most companies are not satisfied with their current solutions:
New problems strive for new solutions. Addressing these considerations requires a modern data engineering strategy.
Azure Databricks strives to hit the sweet spot. Azure Databricks is the jointly-developed Data and AI service from Databricks and Microsoft with a razor-sharp focus on data engineering and data science.
Let us deep-dive into the key five capabilities of Azure Databricks.
Azure Databricks provides five key capabilities:
Azure Databricks along with Azure’s ecosystem offers a solution that is innovative, scalable, and focuses on value at the right cost.
Now that there is a better understanding of Azure Databricks, let us deep-dive into the Architectural constructs of a modern data engineering platform.
These are the four critical pillars of modern data engineering. Ingest. Store. Prep and Train. Model and Serve. It will look traditional, but the devils are in the details.
Let us drill down into it:
The Architecture is scalable, on-demand, with built-in high availability (HA), and is capable of processing and serving petabytes of data at lightning speed.
The architecture illustrated in the previous section is tried and tested with multiple customers across the globe. Architecture is such that it gives the right levers for cost and functionality control.
There are a few fundamental principles to note in this Architecture:
This Architectural pattern enables data engineers to focus on doing data engineering with a single goal in mind: To reduce data to insight turnover time. Focus less on technicalities. Focus more on functionality.
#Data and #AI Strategist @ Microsoft. Impact driven. Executive-level interpersonal skills. Hands-On. #WorldTraveller. #Blogger
26 
26 claps
26 
#Data and #AI Strategist @ Microsoft. Impact driven. Executive-level interpersonal skills. Hands-On. #WorldTraveller. #Blogger
"
https://medium.com/devops-dudes/amazons-failed-implementation-of-azure-devops-4e68965c96d2?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
Cloud computing is all the rage these days. AWS was way ahead of the curve when they first launched EC2 in August of 2006 to the general public. If you’re reading this right now you’ve probably also seen the #CloudMania at your organization.
"
https://medium.com/open-at-microsoft/the-azure-25-dollar-challenge-2fe77ed18234?source=search_post---------388,"There are currently no responses for this story.
Be the first to respond.
What can you do with $25 in Azure? Ian Philpot, Sr. Technical Evangelist at Microsoft, breaks down all the different things you can do!
Here are a couple of highlights from this list…
Hopefully this paints a pretty clear picture that $25 dollars a month can go a long way.
Original blog post can be found here.
All things open at Microsoft
12 
12 claps
12 
All things open at Microsoft
Written by
All things open at Microsoft. www.microsoft.com/opensource
All things open at Microsoft
"
https://medium.com/@renatogroffe/azure-kubernetes-kubernetes-minha-m%C3%A1quina-roda-xp-setembro-2019-bedd6e71bddf?source=search_post---------389,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Sep 23, 2019·4 min read
No dia 18/09/2019 (quarta-feira) participei como palestrante do Minha Máquina Roda XP, um encontro técnico com profissionais da área de Tecnologia da XP Investimentos em São Paulo-SP e no qual tive a oportunidade de realizar uma apresentação focada no uso do Azure Kubernetes Service na orquestração de containers Docker.
Deixo aqui meus agradecimentos ao Thiago Fonseca (XP Investimentos) e ao Thiago Adriano (XP Investimentos, Microsoft MVP) pelo convite e por todo o apoio para que este evento acontecesse, sendo que tivemos um excelente público para um início de noite: aproximadamente 40 pessoas presentes!
Tem interesse por conhecer mais sobre orquestração de containers Docker com Kubernetes? E combinando a utilização de Kubernetes com tecnologias como Azure DevOps para o deployment automatizado de aplicações? Não deixe então de acompanhar este evento PRESENCIAL e GRATUITO da comunidade DevOps Professionals e que será realizada no auditório do Auditório da TOTVS, localizado na Avenida Braz Leme, 1000 — Santana — São Paulo-SP.
* DevOps + Kubernetes: uma imersão à orquestração e deployment automatizado de containers *
Quando: 08/10/2019 (terça) a partir das 18:45
Palestrantes:- Renato Groffe (Microsoft MVP, MTAC)- Milton Camara Gomes (Microsoft MVP)
Ficou interessado(a)? Faça sua inscrição no link a seguir para garantir sua presença e não deixe de indicar o evento para amigas, amigos e colegas de trabalho.
** Link para inscrições: http://bit.ly/devops-totvs-10-2019
* Teremos sorteio de brindes e um happy hour ao final do evento! *
Os slides da apresentação já estão no SlideShare:
A demonstração prática fez uso da imagem pública renatogroffe/apicontagem-env, que está disponível para utilização a partir do Docker Hub.
Os fontes que serviram de base para a geração desta imagem (uma API REST criada com o ASP.NET Core 2.2) estão no GitHub:
ASP.NET Core 2.2 + .NET Core 2.2 + Docker + Docker Compose + Environment Variables
Já os scripts com comandos para a criação/configuração do cluster Kubernetes e as definições para geração de objetos como Pods, Deployment e Service se encontram no seguinte repositório do GitHub:
Docker + Kubernetes + AKS (Azure Kubernetes Service) + Environment Variables
No webinar a seguir (produzido para a Microsoft) demonstrei a utilização do Azure Kubernetes Service (AKS); a gravação pode ser assistida gratuitamente a partir do seguinte link:
Kubernetes: do Pod ao Deployment Automatizado [Vídeo]
E para concluir deixo ainda como referências os seguintes conteúdos gratuitos (artigos) que produzi para a Microsoft e também para o meu blog:
Orquestração de containers na Nuvem com o Azure Kubernetes Service (AKS) | Microsoft Tech
Azure Kubernetes Services — AKS: referências gratuitas e dicas para solução de problemas comuns
Docker: dicas e truques na utilização de containers — Parte 1
Docker: dicas e truques na utilização de containers — Parte 2
Docker para Desenvolvedores .NET — Guia de Referência
A seguir estão fotos da apresentação:
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
14 
14 
14 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@renatogroffe/visual-studio-2019-dica-r%C3%A1pida-gerenciando-recursos-do-azure-via-cloud-explorer-4b69b331bdfe?source=search_post---------390,"Sign in
There are currently no responses for this story.
Be the first to respond.
Renato Groffe
Aug 12, 2019·4 min read
Desenvolvedores .NET têm no Visual Studio uma ferramenta fundamental na implementação de suas aplicações, com esta IDE oferecendo produtividade e trazendo um grande de número de alternativas que em muito simplificam o processo de construção de novos projetos.
Uma destas opções pode ser extremamente útil para o desenvolvimento de soluções baseadas no Microsoft Azure: trata-se do Cloud Explorer, um recurso presente nas versões 2017 e 2019 do Visual Studio. Tal funcionalidade permite a visualização dos diferentes recursos vinculados a uma conta do Azure, além da possibilidade de executar algumas ações de acordo com o tipo de elemento considerado.
E como o assunto deste post é o Microsoft Azure, deixo aqui um convite para um treinamento…
O Azure na Prática surgiu de necessidades observadas por mim (Renato Groffe) e 2 amigos (os palestrantes e Microsoft MVPs Ericson da Fonseca e Milton Camara Gomes) ao longo de diversos eventos em que os mesmos participaram nos últimos anos.
Constatamos nestes encontros técnicos dúvidas recorrentes sobre qual a melhor maneira de uma empresa migrar para a nuvem, quais serviços utilizar, além de como conduzir este processo de mudança sem custos abusivos e maximizando a produtividade das equipes de TI.
Diante de tudo isso, surgiu a ideia de compartilharmos nossas experiências através de um curso totalmente diferente dos hoje existentes no mercado. Um treinamento com foco totalmente prático e enfatizando os principais recursos do Microsoft Azure, tornando o aprendizado muito mais fácil e dinâmico.
A primeira turma será ao longo do dia 21/09/2019 (um sábado) em São Paulo capital, enfatizando o desenvolvimento de aplicações Web com o Azure. Ficou interessado(a)?
Acesse o link a seguir para obter 20% de desconto e outras informações sobre o treinamento:
http://bit.ly/anp-blog-groffe
Para utilizar o Cloud Explorer será necessário no Visual Studio 2019 acessar o menu View > Cloud Explorer:
Na imagem a seguir é possível observar o Cloud Explorer:
Para exibir os diferentes recursos vinculados a uma conta do Microsoft Azure será necessário efetuar o login com a mesma no Visual Studio:
Com o login realizado aparecerão selecionadas as diferentes subscriptions vinculadas a uma conta. Concluir então este procedimento de configuração acionando o botão Apply:
Os recursos de uma subscription serão exibidos seguindo agrupamentos por tipos de recurso:
Podemos ainda organizar tais elementos com base em Resource Groups do Azure:
Embora não apresente todas as opções disponibilizadas pelo Portal do Azure, ainda assim o Cloud Explorer conta com outras funcionalidades bastante úteis como:
Visual Studio IDE documentation | Microsoft Docs
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
119 
119 
119 
Microsoft Most Valuable Professional (MVP), Multi-Plataform Technical Audience Contributor (MTAC), Software Engineer, Technical Writer and Speaker
"
https://medium.com/@dunith/production-grade-microservices-on-azure-aks-with-ballerina-part-1-the-basics-87b92bef6bdb?source=search_post---------391,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dunith Dhanushka
May 13, 2020·10 min read
In this multipart tutorial series, you will learn how to write a basic Microservice with Ballerina programming language, deploy it to Azure Kubernetes Service (AKS) and make it production-ready with features available in the Azure platform.
The first installment of this series focuses on getting the simplest things done with the bare minimum effort. At the end of this post, you’ll learn the following:
As we dive deep into the series, I’ll show you how to make this microservice production-ready to have features like authentication, auto-scaling, load balancing, monitoring, and integrating with a CI/CD pipeline.
This tutorial assumes a basic understanding of Ballerina programming language. But we are not going to explore all the bells and whistles that come with Ballerina. I’ll limit the scope to the implementation of a basic REST service. For a primer on Ballerina basics, see https://ballerina.io/learn/
To complete this tutorial, you need to install Ballerina on your local machine. Use https://ballerina.io/downloads/ to get it done.
In addition to that, you need a fair amount of knowledge on Microservices, Docker, and Kubernetes along with a local Docker development environment running Linux containers. Docker provides packages that configure Docker on a Mac, Windows, or Linux system.
Finally, you’ll need an account on Azure. It gives you $200 credit to explore Azure for 30 days. If you don’t have an account already, I suggest you register for a one.
I must say that this tutorial is going to be pretty long and full of code snippets. You can try out the code snippets while reading. But for those who need to skip the post and get into the source code, here’s the GitHub repo URL.
github.com
Ever since the Microservices concept conceived, Java-based frameworks like Spring Boot had been the developer’s first choice for writing Microservices. But here I’m going to take a little detour from that and introduce you to Ballerina, an open-source programming language purpose-built for optimizing microservices development.
The major reason I fell in love with Ballerina is it comes with all batteries included for Microservices development — you don’t need to borrow anything from the outside.
As a developer, my justification for Ballerina is you’ll get more things done with less effort.
In this section, let’s create a Ballerina service called TodoService and run it locally.
Open up a text editor of your choice (Visual Studio Code in my case) and create a file called todo_service.bal file inside a directory of your choice. Then add the following code to it.
In the above code, TodoService represents an HTTP service that is bound to /todos context. It listens for incoming HTTP connections with TodosEP endpoint which is bound to port 9090. It has got only one resource method getAll() that returns an in-memory array of Todo objects formatted as a JSON array.
Now we have the basic Ballerina Microservice code in place. Let’s try to run it locally just to make sure that our code works.
Open up a terminal and navigate to the place where you save the todo_service.bal file. Then execute below command in the terminal.
ballerina run todo_service.bal
If you have installed Ballerina on your local machine successfully, the ballerina command will be added to your PATH variable automatically. When you run a Ballerina service, it will spring up a lightweight web server under the covers and bind it to a port specified with the listener configuration. In our case, the port is 9090.
You will see an output similar to below when everything is properly in place.
A GET request to http://localhost:9090/todos will give you the below response in Postman.
In this section, you’ll configure your Azure environment using Azure CLI to deploy above the Todo service.
I’ll be executing almost all the commands related to Azure configurations with Azure CLI. If you haven’t installed it already in your local machine, see this guide to get it done. Note that you need to have Azure CLI version 2.0.53 or later to complete this tutorial. You can verify the version by running az — version command.
In order to point to the Kubernetes cluster in AKS and manage it from the local machine, you are going to need the kubectl command-line tool. If you haven’t installed it already, see this guide to get it done.
Once installed locally, the Azure CLI tool can be accessed with az command. Type the following command in a terminal to log in with your Azure account credentials.
az login
Once you are successfully logged in to your Azure account, let’s create a resource group.
An Azure resource group is a logical group in which Azure resources are deployed and managed. When you create a resource group, you are asked to specify a location. This location is where resource group metadata is stored.
The following example creates a resource group named myResourceGroup in the eastus location.
az group create --name myResourceGroup --location eastus
You’ll get an output like below when your resource group created successfully.
Azure Container Registry (ACR) is a private registry for container images. A private container registry lets you securely build and deploy your applications and custom code.
Let’s create a container registry now so that it’ll be useful in the later parts of this tutorial. The following example creates a container registry named dunithd and attaches it to the resource group that I have created previously.
The registry name must be unique within Azure and contain 5–50 alphanumeric characters. The Basic SKU is a cost-optimized entry point for development purposes that provides a balance of storage and throughput.
Once created, you can refer to your container registry as dunithd.azurecr.io
Now we have a basic Ballerina service that works and we have configured our Azure environment accordingly. Then let’s see how to generate Kubernetes artifacts for the Todo service.
Ballerina offers a convenient way of generating corresponding Kubernetes artifacts automatically with annotations built into the language. The Kubernetes builder extension offers native support for running Ballerina programs on Kubernetes with the use of annotations that you can include as part of your service code. Also, it will take care of the creation of the Docker images, so you don’t need to explicitly create Docker images prior to deployment on Kubernetes.
For more information, see How to Deploy Ballerina Programs in the Cloud.
To add annotations, open the todo_service.bal file and overwrite its content with the below code.
Here, @kubernetes:Deployment is used to specify the Docker image name, which will be created as part of building this service. Notice that I have put the private Docker registry name created in Part 2 in the image name as below.
image : “dunithd.azurecr.io/todo-service:v1”
The @kubernetes:Service {} annotation will create a Kubernetes service that will expose the Ballerina service running on a Pod. For simplicity, I have put LoadBalancer as the service type. Then Azure will automatically provision a load balancer resource for me with an external IP address. I will show you how to configure NGINX as an Ingress controller in a later tutorial.
Now let’s use the below command to build the Todo_service.bal file. This will also create the corresponding Docker image and the Kubernetes artifacts using the Kubernetes annotations that you have configured above.
You can use the docker images command to verify that the Docker image specified in the @kubernetes:Deployment was created.
The Kubernetes artifacts related to your service will be generated in addition to the .jar file. Notice the docker and kubernetes directories created in your working directory.
Now we have Todo service as a Docker image in our local repository. This needs to be pushed to the Azure container registry (ACR) so that our Kubernetes cluster can pull it from there.
In a previous step, we created a container registry in Azure. In order to use it, you must first login. In the Azure CLI, type the below command with the name you have given as the registry name before (dunithd in my case).
az acr login --name dunithd
The command returns a Login Succeeded message once completed.
Use the docker push command to push to the local image to your ACR as follows.
docker push dunithd.azurecr.io/todo-service:v1
This command may take a few minutes to complete.
The following command returns a list of images that have been pushed to your ACR instance
The following command returns the tags for the todo-service image we just pushed.
So far, we have tested the todo-service locally, generated a Docker image, and Kubernetes artifacts out of it and pushed the local image to the Azure container registry. What’s left is to create a Kubernetes cluster in AKS and deploy the generated artifacts into that.
The following command creates a Kubernetes cluster of two nodes named myAKSCluster in the resource group named myResourceGroup which we have created in a previous step.
Here, the AKS cluster needs to access Azure Container Registry (ACR) instance to pull the todo-service:v1 image you pushed earlier. For that, Azure automatically creates an Azure Active Directory service principal and grants the right to pull images from the ACR instance. If needed, there’s an option to use a managed identity instead of a service principal for easier management.
After a few minutes, the deployment completes and returns JSON-formatted information about the AKS deployment.
The following command configures the kubectl tool to connect to your AKS cluster. It fetches credentials for the AKS cluster named myAKSCluster in the myResourceGroup and creates an entry in your ~/.kube directory.
Verify the connectivity to your cluster by running kubectl get nodes command as follows.
Now we are at the final moment of our tutorial. The only thing left is to deploy the Kubernetes artifacts generated in a previous step.
From your working directory where todo_service.bal resides, execute the below command.
If everything goes fine, it should produce an output like below.
Behind the scenes, kubectl has created a deployment and a service for our Todo service. You can verify the service as follows.
Notice the external IP address assigned our todos service. That is because I have set the service type to LoadBalancer when configuring the service annotation.
Now our service is accessible from outside the cluster. Here’s the output I got when I called the todo service with the external IP using Postman.
See, how easy it is to spin up a Kubernetes cluster and deploy your Microservice into that!
As this tutorial is the last part of the series, you may want to delete the AKS cluster. As the Kubernetes nodes run on Azure virtual machines (VMs), they continue to incur charges even if you don’t use the cluster. The following command removes the resource group, container service, and all related resources.
az group delete --name myResourceGroup --yes --no-wait
Well, I guess the tutorial was a bit long. But it covers almost every step that is essential to write a Microservice with Ballerina and getting that deployed into AKS. For an absolute beginner on Ballerina and AKS, this tutorial would be an ideal place to start.
We merely scratched the surface here. When I said “production-grade” in the title, there’s a lot more to come in this series.
Hence, expect the following posts as follow up items to this article in the future.
If there’s any additional topic you want me to discuss in this series, put them as a comment or tweet me.
Editor of Event-driven Utopia(eventdrivenutopia.com). Technologist, Writer, Developer Advocate at StarTree. Event-driven Architecture, DataInMotion
262 
1
262 
262 
1
Editor of Event-driven Utopia(eventdrivenutopia.com). Technologist, Writer, Developer Advocate at StarTree. Event-driven Architecture, DataInMotion
"
https://medium.com/streaming-at-scale-in-azure/serverless-streaming-at-scale-with-azure-sql-304cd2bfd5c2?source=search_post---------392,"There are currently no responses for this story.
Be the first to respond.
Just before Ignite, a very interesting case study done with RXR has been released, where they showcased their IoT solution to bring safety in building during COVID times. It uses Azure SQL to store warm data, allowing it to be served and consumed to all downstream users, from analytical application to mobile clients, dashboards, API and business users.
If you haven’t done yet, you definitely should watch the Ignite recording (the IoT part start at minute 22:59). Not only the architecture presented is super interesting, but also the guest presenting it — Tara Walker — is super entertaining and joyful to listen. Which is not something common in technical sessions. Definitely a bonus!
If you are interested in the details, beside the Ignite recording, take a look also at the related Mechanics video, where things are discussed a bit more deeply.
Implement a Kappa or Lambda architecture on Azure using Event Hubs, Stream Analytics and Azure SQL, to ingest at least 1 Billion message per day on a 16 vCores database
The video reminded me that in my long “to-write” blog post list, I have one exactly on this subject. How to use Azure SQL to create a amazing IoT solution. Well, not only IoT. More correctly how to implement a Kappa or Lambda architecture on Azure using Event Hubs, Stream Analytics and Azure SQL. It’s a very generic architecture that can be easily turned to IoT just by using IoT Hub instead of Event Hubs and it can be used as is if you need, instead, to implement an ingestion and processing architecture for the Gaming industry, for example.
Goal is to create a solution that can ingest and process up to 10K message/secs, which is close to 1 Billion message per day, which is a value that will be more than enough for many use cases and scenario. And if someone needs more, you can just scale up the solution.
This article is quite long. So, if you’re in hurry, or you already know all the technical details on the aforementioned services, or you don’t really care too much about tech stuff right now, you can just go away with the following key points.
If you’re now ready for some tech stuff, let’s get started.
So, let’s see it in detail. As usual, I don’t like to discuss without also having a practical way to share knowledge, so you can find everything ready to be deployed in your Azure subscription here:
github.com
As that would not be enough, I also enjoyed recording a short video to go through the working solution, giving you a glimpse of what you’ll get, without the need for you to spend any credit, if you are not yet ready to do that:
Creating a streaming solution usually means implementing one of two very well know architectures: Kappa or Lambda. They are very close to each other, and it’s safe to say that Kappa is a simplified version of Lambda. Both have a very similar data pipeline:
Event Hubs is probably the easiest way to ingest data at scale in Azure. It is also used behind the scenes by IoT Hub, so everything you learn on Event Hubs, will be applicable to IoT Hub too.
It is very easy to use, but at the beginning some of the concepts can be quite new and not immediate to grasp, so make sure to check out this page to understand all the details: Azure Event Hubs — A big data streaming platform and event ingestion service
Long story short: you want to ingest a massive amount of data in the shortest time possible, and keep doing that for as much as you need. To achieve the scalability you need, a distributed system is required, and so data must be partitioned across several nodes.
In Event Hubs you have to decide how to partition ingested data when you create the service, and you cannot change it later. This is the tricky part. How do you know how many partition you will need? That’s a very complex answer, as it is completely dependent on how fast who will read the ingested data will be able to go.
If you have only one partition and one of the parallel applications that will consume the data is slow, you are creating a bottleneck.
If you have too many partitions, you will need to have a lot of clients reading the data, but if data is not coming in fast enough, you’ll starve your consumers, meaning you are probably wasting your money in running processes that are doing nothing for a big percentage of their CPU time.
So let’s say that you have 10MB/sec of data coming in. If each of your consuming client can process data at 4MB/sec, you probably want 3 of them to work in parallel (with the hypothesis that your data can be perfectly and evenly spread across all partitions), so you will probably want to create at least 3 partitions.
That’s a good starting point, but 3 partitions is not the correct answer. Let’s understand why by making the example a bit more realistic and thus slightly more complex.
Event Hubs let you pick and choose the Partition Key, which is the property whose values will be used to decide in which partition an ingested message will land. All messages with same partition key value, will land in the same partition. Also, if you need to process messages in the order they are received, you must put them in the same partition. If fact, ordering is guaranteed only at partition level.
In our sample we’ll be partitioning by DeviceId, meaning data coming from the same device will land in the same partition. Here’s how the sample data is generated
In Event Hubs the “power” you have available (and that you pay for) is measured in Throughput Units (TU). Each TU guarantees that it will support 1MB/sec or 1000 messages(or events)/sec , whichever came first. If we want to be able to process 10.000 events/sec we need at least 10 TU. Since it’s very unlikely that our workload will be perfectly stable, without any peak here and there, I would go for 12 TU, to have some margin to handle some expected workload spike.
TU can be changed on the fly, increasing on reducing them as you need.
It’s time to decide how many TU and Partitions we need inour sample. We want to be able to reach at least 10K messages/second. TU are not an issue as they can be change on the fly, but deciding how many partitions we need is more challenging. We’ll be using Stream Analytics, and we don’t exactly know how fast it will be able to consume incoming data.
Of course one road is running test to figure out the correct numbers, but we still need to come up with some reasonable numbers also to just to start with such test. Well, a good rule of thumb is the following:
Rule of thumb: create an amount for partitions equal to the number of throughput units you have or you might expect to have in future
For what concern the ingestion part we’re good now. If you want to know more, please take a look at this article: Partitioning in Event Hubs and Kafka, that will go into detail of this topic. Super recommended!
Let’s now move to discuss how to process the data that will be thrown at us, doing it as fast as possible.
Azure Stream Analytics is an amazing serverless streaming processing engine. It is based on the open source Trill framework which source code is available on GitHub and is capable to process a trillion message per day. All without requiring you to manage and maintain the complexity of a extremely scalable distributed solution.
Stream Analytics support a powerful SQL-like declarative language: tell it what you want and it will figure out how to do it, fast.
It also supports a SQL-like language so all you have to do to define how to process your event is to write a SQL query (with the ability to extend it with C# or Javascript) and nothing more. Thanks to SQL simplicity and ability to express what you want opposed to what to do, development efficiency is very high. For example determining for how long an event lasted, for example, is as easy as doing this:
All the complexity of managing the stream of data used as the input, with all its temporal connotations, is done for you, and all you have to tell Stream Analytics is that it should calculate the difference between a start and end event on per user and feature basis. No need to write complex custom stateful aggregation functions or other complex stuff. Let’s keep everything simple and leverage the serverless power and flexibility.
As for any distributed system, the concept of partitioning is key, as it is the backbone of any scale-out approach. In Stream Analytics, since we are getting data from Event Hub or IoT Hub, we can try to use exactly the same partition configuration already defined in those services. If was use the same partition configuration also in Azure SQL, we can achieve what are defined as embarrassingly parallel jobs where there is no interaction between partitions and everything can be processed fully in parallel. Which means: at the fastest speed possible.
Streaming Units (SU) is the unit of scale that you use — and pay for—in Azure Stream Analytics. There is no easy way to understand how many SU you need, as consumption will totally depend on how complex your query is. The recommendation is to start with 6 and then monitor the Resource Utilization to see how much percentage of available SU you are using. If your query partition data using PARTITON BY, SU usage will increase as your are distributing the workload across nodes. This is good, as it means you’ll be able to process more data in the same amount of time . You also want to make sure SU utilization is below 80% as after that your events will be queued, which means you’ll see higher latency. If everything works well, we’ll be able to ingest our target of 10K events/sec (or 600K events/minute as pictured below)
Azure SQL is really a great database for storing hot and warm data of an IoT solution. I know this is quite the opposite of what many thinks. A relational database is rigid, it requires schema-on-write, and on IoT or Log Processing scenarios, the best approach is a schema-on-read instead. Well, Azure SQL actually supports both and more.
With Azure SQL you can do both schema-on-read and schema-on-write, via native JSON support
In fact, beside what just said, there are several reason for this, and I’m sure you are quite surprised to hear that, so, read on:
Describing each one of the listed features, even just at a very high level, would require an article on its own. And of course, such article is available here, if you are interested (and you should!): 10 Reasons why Azure SQL is the Best Database for Developers.
In order to accommodate a realistic scenario where you have some fields that are always present, while some other can vary by time or device, the sample is using the following table to store ingested data
As we really want to create something really close to a real production workload, indexes have been created too:
At the time of writing I’ve been running this sample for weeks and my database is now close to 30TB:
Table is partitioned by PartitionId (which is in turn generated by Event Hubs based on DeviceId) and a query like the following
Takes less then 50 msec to be executed including also the time to send the result to the client. That’s pretty impressive. The result also shows something impressive too:
As you can see, there are two calculated columns QueueTime and ProcessTime that shows, in milliseconds, how much time an event has been waiting in Event Hubs to be picked up by Stream Analytics to be processed, and how much time the same event spent within Stream Analytics before land into Azure SQL. Each event (all the 10K per second) is processed in — overall—less than 300 msec on average. 280msec more precisely.
That is very impressive.
End-to-End ingestion latency is around 300msec
You can also go lower than that using some more specific streaming tool like Apache Flink, if you really need to completely avoid any batching technique to decrease the latency to the minimum possible. But unless you have some very unique and specific requirements, processing events in less than a second is probably more than enough for you.
For Azure SQL, ingesting data at scale is not a particularly complex or demanding job, on the contrary of what can expect. If done well, using bulk load libraries, the process can be extremely efficient. In the sample I have used a small Azure SQL 16 vCore tier to sustain the ingestion of 10K event/secs, using on average 15% of CPU resources on a bit more of 20% of the IO resources.
This means that in theory I could also use an even smaller 8 vCore tier. While that is absolutely true, you have to think of at least three other factors when sizing Azure SQL:
Just as an example, I have stopped Stream Analytics for a few minutes, allowing messages to pile up a bit. As soon as I restarted it, it tried to process messages as fast as possible, in order to empty the queue and return to the ideal situation where latency is less then a second. In order allow Stream Analytics to process data at higher rate, Azure SQL must be able to handle the additional workload too, otherwise it will slow down all the other components in the pipeline.
As expected, Azure SQL handled the additional workload without breaking a sweat.
For all the needed time, Azure SQL was able to ingest almost twice the regular workload, processing more than 1 Million messages per minute. All of this with CPU usage staying well below 15%, and with a relative spike only to the Log IO — something expected as Azure SQL uses a Write-Ahead Log pattern to guarantee ACID properties—which, still, never went over 45%.
Really, really, amazing.
With such configuration — and remember we’re just using a 16vCore tier, but we can scale up to 80 and more — our system can handle something like 1 billion messages a day, with an average processing latency of less then a second.
The deployed solution can handle 1 billion messages a day, with an average processing latency of less then a second.
Partitioning plays a key role also in Azure SQL: as said before, if need to operate on a lot of data concurrently, partitioning is really something you need to take into account.
Partitioning in this case is used to allow concurrent bulk insert into the target table, even if on such table several indexes exists and thus needs to be kept updated.
Table has been partitioned using the PartitionId column, in order to have the processing pipeline completely aligned. The PartitionId value is in fact generated by Event Hub, which partitions data by DeviceId, so that all data coming from the same device will land in the same partition.
Stream Analytics uses the same partitions provided by Event Hub and so it make sense to align Azure SQL partitions to this logic too, to avoid to cross the streams, which we all know is a bad thing to do. Data will move from source to destination in parallel streams providing the performances and the scalability we are looking for.
Table partitioning also allows Azure SQL to update the several indexes existing on the target table without ending in tangled locking, where transactions are waiting for each other with the result of huge negative impact on performances. As long as table and indexes are using the same partitioning strategy everything will move forward without any lock or deadlock problem.
Higher concurrency is not the only perk of a good partitioning strategy. Partitions allow extremely fast data movement between tables. We’ll take advantage of this ability for creating highly compressed column-store indexes soon.
What if you need to run complex analytical queries on the data being ingested? That’s a very common requirement for Near-Real Time Analytics or HTAP (Hybrid Transaction/Analytical Processing) solutions.
As you have noticed, you still have enough resources free to run some complex queries, but what if you have to run many really complex queries, for example to compare average values of month-over-month, on the same table were data is being ingest? Or what if you need to allow many mobile client to access the ingested data, all running small but CPU intensive queries? Risk of resource contention — and thus low performances — becomes real.
That’s when a scale-out approach start to get interesting.
With Azure SQL Hyperscale you can create up to 4 readable-copies of the database, all with their own private set of resources (CPU, memory and local cache), that will give you access to exactly the same data sitting in the primary database, but without interfering with it at all. You can run the most complex query you can imagine on a secondary, and the primary will not even notice it. Ingestion will proceed as usual rate, completely unaffected by the fact that a huge analytical query or many concurrent small queries are hitting the secondary nodes.
Columnstore tables (or index in Azure SQL terms) are just perfect for HTAP and Near Real Time Analytics scenario, as already described times ago here: Get started with Columnstore for real-time operational analytics.
This article is already long enough, so I’ll not get into details here, but I will focus on the fact that using columnstore index as a target of a Stream Analytics workload, may not be the best option, if you are also looking for low latency. To keep latency small, a small batch size needs to be used, but this is against the best practices for columnstore, as it will create a very fragmented index.
To address this issue, we can use a feature offered by partition table. Stream Analytics will land data into a regular partitioned rowstore table. On scheduled intervals a partition will be switched out into a staging table, so that it be loaded into a columnstore table using Azure Data Factory, for example, so that all best practices can be applied to have the highest compression and the minimum fragmentation.
What if everything just described is still not enough? What if you need a scale so extreme that you need to be able to ingest and process something like 400 Billions rows per day? Azure SQL allows you to do that, by using In-Memory, latch-free, tables, as described in this amazing article:
techcommunity.microsoft.com
I guess that, now, even if you have the most demanding workload, you should be covered. If you need even more power…let me know. I’ll be extremely interested in understanding your scenario.
We’re at the end of this long article, where we learned how it is possible with a Kappa (or Lambda) architecture to ingest, process and serve 10K msg/sec using only PaaS services. As we haven’t maxed out any of the resource of our services, we know we can scale to a much higher level. At least twice that goal value, without changing anything and much more than that by increasing resources. With Azure SQL we are just using 16 vCores and it can be scale up to 128. Plenty of space to grow.
Azure SQL is a great database for IoT and HTAP workload
Notes on creating streaming at scale solution in the Azure…
60 
1

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
60 claps
60 
1
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Written by
Data Geek, Storyteller, Developer at heart, now infiltrated in Azure SQL product group to make sure developers voice is heard loud and clear. Heavy Metal fan.
Notes on creating streaming at scale solution in the Azure cloud
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ashish_fagna/introduction-to-microsoft-azure-bot-service-luis-language-understanding-8826d29d013e?source=search_post---------393,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ashish
Apr 13, 2018·3 min read
Introduction:
Microsoft Azure in 2017 launched the General Availability of the Azure Bot service and the language understanding service (LUIS). These are the two amazing cloud AI service for creating conversational AI experiences.
In this article we will see how easy it is to create intelligent and connected Bots. Azure makes creating bots really easy. Azure bot service provides a core component and hosting environment for creating bots including the bot builder SDK for developing bots and the bot connector service to connect the bots to channels. After bot is created, you can add intelligence to the bot with Microsoft Cognitive Services such as Language Understanding Service (LUIS), Vision, Speech and many other capabilities so the bot can see, hear, understand and interact in more natural ways.
With Azure bot service, developers can get started in a few minutes with out of the box templates and reach your audience across the multiple supported channels or provide custom experience in your app or website using web chat.
Few points worth noting about Azure Bot Service:
Now let us explore Azure Bot Service in detail. In order to use Azure Bot Service, sign up on portal.azure.com and subscribe to Azure.
Once you login, you need to click on “New Dashboard” and then you will see “AI+Cognitive Services”. Once you click on this option, you will see the ‘Web App Bot’ listed as one of the featured services.
Once you click “see all” next to “Featured” link, you will see all the AI and Cognitive Services. There you can see under the “Bot Service” subcategory, there are three offerings of the bot service.:
If you already have a bot, it helps to register to the multiple channels (bot interfaces) Azure Bot Service supports. In the article, we will use “Web App Bot”. Once you click on it, you will see a new dialog box displaying information and useful links for ‘Web App Bot’.
Next, Click on “Create” to start the creation process.
Getting Started NodeJS Code:
If you wish to create a Bot using Azure Bot Service,you can refer to this blog post https://docs.microsoft.com/en-us/azure/bot-service/nodejs/bot-builder-nodejs-quickstart
Hope you find this article useful to get started with using Azure bot Service and LUIS.
My Name is Ashish @ashish_fagna. I am a software consultant. LinkedIn profile. If you enjoyed this article, please recommend and share it! Thanks for your time.
You can also contact me on my email ashish [dot] fagna [at] gmail.com
Software Developer. Machine Learning, Artificial Intelligence Learner.
See all (1,892)
75 
4
75 claps
75 
4
Software Developer. Machine Learning, Artificial Intelligence Learner.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/awesome-azure/introduction-to-azure-private-link-andprivate-endpoint-and-private-link-service-a61be184356e?source=search_post---------394,"There are currently no responses for this story.
Be the first to respond.
Introduction to Azure Private Link — What is Azure Private Link?
Private Link enables access to hosted customer and partner services over a private endpoint in your virtual network. It enables a true private connectivity experience between services and virtual networks.
"
https://itnext.io/how-to-get-your-kubernetes-cluster-service-principal-and-use-it-to-access-other-azure-services-637f185a5112?source=search_post---------395,"So, you have a Kubernetes cluster on Azure (AKS) that needs to access other Azure services like Azure Container Registry (ACR)? You can use your AKS cluster service principal for this.
All you need to do is delegate access to the required Azure resources to the service principal. Simply create a role assignment using az role assignment createto do the following:
It looks something like this:
Notice that the --assignee here is nothing but the service principal and you're going to need it.
When you create an AKS cluster in the Azure portal or using the az aks create command from the Azure CLI, Azure can automatically generate a service principal. Alternatively, you can create one your self using az ad sp create-for-rbac --skip-assignment and then use the service principal appId in --service-principal and --client-secret (password) parameters in the az aks create command.
You can use a handy little query in the az aks showcommand to locate the service principal quickly!
This will the service principal appId! You can use it to grant permissions. For e.g. if you want to allow AKS to work with ACR, you can grant the acrpull role:
If you found this article helpful, please like and follow! Happy to get feedback via Twitter or just drop a comment :-)
twitter.com
ITNEXT is a platform for IT developers & software engineers…
94 
94 claps
94 
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Azure Cosmos DB at Microsoft | I like Databases, Go, Kubernetes
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
"
https://medium.com/wortell/detecting-cve-2020-0601-and-other-attempts-to-exploit-known-vulnerabilities-using-azure-sentinel-652fbcc0364c?source=search_post---------396,"There are currently no responses for this story.
Be the first to respond.
Yesterday, Microsoft has released a security update for Windows which includes a fix to a dangerous bug that would allow an attacker to spoof a certificate, making it look like it came from a trusted source.
The vulnerability (CVE-2020–0601) was reported to Microsoft by the NSA. The root cause of this vulnerability is a flawed implementation of the Elliptic Curve Cryptography (ECC) within Microsoft’s code.
Tal Be’ery, Microsoft’s security research manager, wrote an article explaining the root cause of the vulnerability using a Load Bearing analogy. You can find that here.
Watching the logs
Windows has a function for publishing events when an attempted security vulnerability exploit is detected in your user-mode application called CveEventWrite. To my knowledge, the fix for CVE-2020–0601 is the first code to call this API.
After the Windows update is applied, the system will generate event ID 1 in the eventlog after each reboot under Windows Logs/Application when an attempt to exploit a known vulnerability is detected. This event is raised by a User mode process, and more information can be found in the MSRC guidance for this fix (scroll to the bottom of the article to find it).
Azure Sentinel
By default the Log Analytics workspace powering Azure Sentinel will not collect events from the Application eventlog. You can change this by going to the Advanced Settings for your workspace, and adding Application as a source under Data > Windows Event Logs:
There are two ways of going about creating the Rule. Either you set up detection for event ID 1 in general so that you get alerted for any (future) CVE abuse, or you set up specific rules per CVE.
There will be pro’s or con’s to both approach. In this sample we’ll build a specific Rule to detect potential CVE-2020–0601 abuse so that we can map it to the right MITRE tactics and follow specifically.
Alert rule
Pete Bryan, who works at Microsoft’s Threat Intelligence Center (MSTIC), tweeted a sample KQL query that you can use:
I was debating with Olaf Hartong from FalconForce which MITRE TTP’s would be most applicable. These make most sense to us:
That means that we’ll be tagging the rule for Defense Evasion and Initial Access tactics.
AzSentinel powershell
If you want to programmatically push this rule into your Azure Sentinel environment, go have a look at our AzSentinel powershell module. My colleague Pouyan Khabazi built this module to work with alert rules in Azure Sentinel through automation. More information on working with the module can be found here.
Here’s the rule definition you could use with AzSentinel:
Microsoft Defender ATP
If you have Microsoft Defender ATP deployed across your enterprise (which now also supports MacOS), you will get the detection out of the box. The logic was added to MDATP at the same time that Microsoft released the fix:
And because MDATP is now unified with other the other Microsoft ATP products, the detection will also show up in the new Microsoft Threat Protection (MTP):
PRO TIP: Microsoft Defender ATP can also be deployed on servers. For workloads living in Azure: enable Azure Security Center (standard) and the MDATP client will be deployed and configured automatically.
Analyzing your enterprise for CVE-2020–0601 attacks
Today, the Microsoft Defender team also added a dashboard in the Threat Analytics section of MDATP. The dashboard shows you more information about the CVE and whether or not endpoints in your organization were found to have been potentially abused:
However, Microsoft also released the hunting query that you could use to investigate. Here’s the KQL:
Best of suite
In the case that you have MDATP and/or MTP, there is no need to create the Azure Sentinel rule(s). Just enable the Microsoft Defender ATP connector to Azure Sentinel and the alert will be created automatically.
Happy hunting!
— Maarten Goet, MVP & RD
Microsoft Cloud & Enterprise Security
21 
21 claps
21 
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Written by
Microsoft MVP and Microsoft Regional Director.
Microsoft Cloud & Enterprise Security
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://jeremythake.com/using-powershell-to-update-an-azure-active-directory-user-manager-field-5b02fac116d1?source=search_post---------397,"I’m setting up a lot of demo environments right now to highlight typical organizational directories. I wanted to automate this and be able to roll back after demonstrations.
I used Windows 10 for this and ran PowerShell with “Run as Administrator” privileges.
You have to install the PowerShell Module for Azure Active Directory using
Once installed I simply typed:
This then prompted me in a browser to log in with my Azure Active Directory administrator credentials. Basically the same administrator credentials for my Office 365 Tenant I’m using.
Then I tested it worked by using
To get a specific user object I used Get-AzureADUser
Then I could easily see more information about the user by using
You can update attributes for the user object using the PowerShell cmdlet Set-AzureADUser for things like DisplayName etc. But for Manager field there is a special cmdlet called Set-AzureADUserManager. If you wish to remove the manager, you have to use Remove-AzureADUserManager.
For some reason, in the portal.azure.com, you cannot empty the text field and click save on Manager. It always comes back, so I have to use PowerShell if I want to clear this.
Unfortunately, Delve does not reflect this change immediately and you have to wait for a full crawl of Active Directory by the SharePoint User Profiles for this to show up. Unlike on-premises, there is no way to force a full crawl due to the multi-tenant nature of Office 365.
My tech musings
26 
1
26 claps
26 
1
Written by
Microsoft Graph Team, Senior Program Manager at Microsoft.
My tech musings
Written by
Microsoft Graph Team, Senior Program Manager at Microsoft.
My tech musings
"
