story_url,bodyText
https://medium.com/@mendhie/building-real-time-android-chatroom-with-firebase-99a5b51cb4f7?source=search_post---------334,"Sign in
There are currently no responses for this story.
Be the first to respond.
You have 2 free member-only stories left this month. Sign up for Medium and get an extra one
Mendhie Emmanuel
Mar 20, 2019·6 min read
I particularly enjoy working with Firebase because of the ease it brings to the development process. Firebase efficiently handles the backend process associated with authentication, cloud storage, real-time database, and push notification while you focus on developing the app. In this article, I’ll show you how to build a realtime android chatroom with Firebase. Trust me, it will be interesting and straight-to-point.
This article covers the following core areas:
Alright guys, let’s begin.
Create your project on Android Studio, then follow these steps to add Firebase to it.
Now that you have created a Firebase project, add your app to it:
Setting up Firebase authentication, cloud firestore, and cloud storage is quite easy. You click a few buttons, type a few strings and you’re done.
Authentication:
Firebase has up to nine authentication providers (email/password, Facebook, Twitter, Google, etc), however, we will use email/password authentication for this project. Simply enable and save the email/password provider from the authentication menu on Firebase console.
Database:
Storage:
Firebase Storage allows storage and retrieval of user-generated files like images, audio, and video without server-side code. In the Storage section, click the Get Started button and accept the default security rules.
Dependencies:
The last step in the setup process is to add authentication, cloud firestore, and firebase storage dependencies to your app-level build.gradle file. Thereafter, you can add other dependencies you will need in the project. It’s more time-efficient that way.
Signup:
In the SignupActivity, retrieve the user’s email and password from the input fields, then pass them tofirebaseAuth.createUserWithEmailAndPassword() method to create a new user on Firebase.
Within the createUserWithEmailAndPassword() method, verify that the new user was successfully created before calling saving the user’s details to Cloud Firestore.
For this project, I used a HashMap userData containing the user’s details to save the user’s information to Firestore with userID as primary key (Firestore document). Using userID as key allows for easy query.
Login:
During login, Firebase authenticates that the user has signed up and that the login details are correct, else it will throw up an error. To sign in user with email/password, call auth.signInWithEmailAndPassword() method and pass on the user’s email and password for authentication.
When it comes to user-friendliness, simplicity is always the best route to follow. Therefore, our chatroom would have only the basic and most needed features: aRecyclerViewto display messages, EditText to type in messages,FloatingActionButton to send messages and a ProgresBar to display when messages are loading.
Design the list item(s):
This list item contains the views that will be displayed in the RecyclerView. A chatroom’s list item will typically consist of anImageView to display the profile picture, TextView for the username, date/time, and message, and a couple of other features that are important to the users or app functionality. For this project, I created two types: one for messages_in and another for message_out.
Create Message model class
The Message model class represents the properties of a message posted in the forum. The object of this class is what will be stored in the messages collection of your database. Declare the variables in your Message class, and add getters and setters as shown below.
Create FirestoreRecyclerAdapter:
FirestoreRecyclerAdapter binds a Query to theRecyclerView such that whenever messages are added, removed, or changed in the chatroom, these updates are automatically applied to the RecyclerView in real time.
Creating a the adapter involves three steps: First, create an object of RecyclerView.ViewHolder which holds the message items within the RecyclerView. It can be an inner class of the adapter class or a separate class entirely.
The next step is to configure your FirestoreRecyclerAdapter to build FirestoreListOptions.
The last step in creating the adapter is to override onCreateViewHolder(), onBindViewHolder(),and getViewItemType() methods.
The getViewItemType() method sets the item’s view type. A typical chatroom will have two view types: one for current user’s message and the other for messages from other users. The onCreateViewHolder() method is called when the RecyclerView needs a new RecyclerView.ViewHolder while theonBindViewHolder() method binds the corresponding values fromMessage object to the RecyclerView.ViewHolder. The process of binding values to the views is repeated for each message that will be displayed.
Alright. You have finished creating your adapter class, so go over to MainAcivity and set MessageAdapter as the adapter for yourRecyclerView. Finally, attach anOnClickListener to the FloatingActionButton so it will listen for clicks and save the message to Firestore whenever the button is clicked.
Congratulations on completing the article. I hope it has been helpful in building your real-time android chatroom. If you have any queries, do post them in the comments section below. I have added the link to the project github repo below.
github.com
Android Developer
See all (21)
364 
3
364 claps
364 
3
Android Developer
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/automating-cloud-storage-data-classification-dlp-api-and-cloud-function-7546b3763203?source=search_post---------67,"There are currently no responses for this story.
Be the first to respond.
Authors: Priyanka Vergadia, Jenny Brown
“Get Cooking in Cloud” is a blog and video series to help enterprises and developers build business solutions on Google Cloud. In this series we plan on identifying specific topics that developers are looking to architect on Google cloud. Once identified we create a mini series on that topic.
In this miniseries, we will go over the automation of data classification in Google Cloud Storage, for security and organizational purposes.
In this article we will create the Cloud Functions, set up DLP API and test our application for automated data classification.
How to automate the upload and classification of data with Google Cloud Storage.
In the last two blogs [1] [2] we have talked about Dinner Winner — an application that collects recipes from users all over the world, judges them, and then posts the winning recipe.
We know there might be personal information initially tied to these submissions that needs to be removed before anything else can happen, and we want this to be a seamless, automated process.
We saw an architecture of how they can automate and classify the uploaded data by using Cloud Storage, cloud functions, DLP and Pub/Sub.
For Dinner Winner, We need two cloud functions: One that is invoked when an object is uploaded to Cloud Storage and the other that is invoked when a message is received in the Cloud Pub/Sub queue.
Let’s start by creating the cloud function that triggers from the GCS bucket.
Now, we need to create the cloud function that gets invoked when a message is received in the Cloud Pub/Sub Queue.
And now, for the moment of truth. Has our recipe for secure application automation done the trick for Dinner Winner? We need to test to ensure proper automation and detection of private info types in the content.
For the purposes of this exercise, we are defining the sensitive data info types as: name, email address, location and phone number. But you can change it in the code to however you wish for your own use case.
So that’s it! Dinner Winner has their upload and classification automated, and can return to their “gamified” recipe submission with confidence.
If you’re looking to classify data in an automated fashion, you’ve got a small taste of the challenges involved and the ingredients needed. Stay tuned for more articles in the Get Cooking in Cloud series and checkout the references below for more details.
Google Cloud community articles and blogs
114 
114 claps
114 
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@zeepin/zeepin-newsletter-progress-of-the-past-month-cd533606a27f?source=search_post---------398,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zeepin
Aug 17, 2018·3 min read
Zeepin Chain will GO LIVE on August 31.
Zeepin Chain is a decentralized public chain catering for creative industries as well as entertainment industries. To serve the creative industries, it’s dedicated to providing a solution that will create a fair and transparent, efficient landscape in a way to benefit all creatives. In order to revolutionize then entertainment industries as well, Zeepin Chain, is also capable of incorporating various entertainment related assets and systems based on any third party. With the integration of in-system and third-party gaming assets, a free trading market and exchange platform can be developed.
ZEEPIN launching GalaCloud Distributive Encryption Storage Network
A variety of noticeable advantages featuring GalaCloud that can not only clear out entrenched data security issues remaining unsolved in centralized storage system, but also pave the way to innovative economic model and applications. GalaCloud is a distributed encryption storage network, a key component of the Zeepin ecosystem. It provides a distributed storage space and service, which is one of the indispensable infrastructures of Zeepin public chain. Furthermore, it will help Gala coin to reach its greatest extension.
Operation Progress
Zeepin Chain：
1. Zeepin Chain has redesigned its official website to impress the masses as a customized ecosystem for the creative industries to thrive.
2. Zeepin Chain tech white paper has been released, elaborating on the framework and strategy behind Zeepin’s commitment to “enabling the digitization and circulation of assets across the entire creative value chain”.
GalaCloud：
1. GalaCloud official website comes live with introduction about GalaCloud, detailed rules for GalaHub node selection, GalaBox crowdfunding and other first-hand news.
2. GalaCloud white paper was launched, including detailed introduction of a suite of innovative solution to data security and ownership.
3. Zeepin Chain also released “GalaCloud incentive model”in order to incentivize nodes participated in GalaHub selection.
4. The early-bird stage for GalaBox crowdfunding has exceeded its target goal within 24 hours. To reward early ZPT holders, GalaBox will be obtainable for free as long as certain amount of ZPT locked up for 18 months. The second stage is scheduled to begin on August 18th 2PM UTC. In case it will be postponed due to NEP-5 wallet maintenance, we will update you accordingly.
5. Galahub node selection is ongoing. 20% of a total revenue of GalaCloud will be distributed to all GalaHub nodes while Gala equivalent of 10 times ZPT mortgage will be gifted to those nodes as well. Come and get your Gala!
CryptoGalaxy：
1. 382 planets have been released and over 9700 players from 112 countries have participated in CryptoGalaxy within 16-week stretch after its official launch. In addition, more than 5.5 million Gala has been generated;
2. In latest CryptoGalaxy V1.5, wallet function is added that allows ZPT and Gala deposit in user’s favors. A prize pool filled with 150,000 Gala on a daily basis is available for ZPT holders! More ZPT deposited, more Gala rewarded!
dApp Development
ZeeRights：
ZeeRights V1.0.1 has completed with new features.
ZeeRights V1.0.2 has completed product design that includes institutional ID identification, copy registration in bulk, after-sale service and other user-friendly functionalities.
CryptoGalaxy：
Closed-beta tests for CryptoGalaxy beta V1.5 has ended up with impressively positive comments from community testers. It’s been online since the beginning of August.
ZeeWallet：
ZeeWallet is under development for enabling node selection. In updated version, users can expect more intelligent functions allowing swap between TestNet and MainNet as well as staking ZPT to mine Gala.
Stay tuned for more details,
ZEEPIN team
Zeepin:
Website: https://www.zeepin.io/
Telegram: https://t.me/zeepin
News Channel: https://t.me/ZeepinNews
Twitter: https://twitter.com/ZeepinChain
Facebook: https://www.facebook.com/ZeepinChain/
Youtube: https://www.youtube.com/c/Zeepin
SubReddit: https://www.reddit.com/r/ZEEPIN/
Instagram: https://www.instagram.com/zeepinchain/
Linkedin: https://www.linkedin.com/company/zeepin-foundation/
Discord: https://discord.gg/YcPhNXC
Galaxy:
Website: https://cryptogalaxy.one/
Telegram: https://t.me/CryptoGalaxyOne
Twitter: https://twitter.com/TheHubGalaxy
Facebook: https://www.facebook.com/CryptoGalaxyOne/
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
111 
111 
111 
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
"
https://medium.com/@jaychapel/cloud-storage-cost-comparison-aws-vs-azure-vs-google-844dfff3d324?source=search_post---------74,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jay Chapel
Jul 1, 2019·4 min read
Today, we’ll take a brief look at cloud storage cost comparison from the three major cloud service providers. When it comes to finding a solution for your cloud computing needs, it is fair to say that for every business the solutions are based on a case-by-case scenarios — and given the breadth of cloud storage options available, it is certainly true in this case. A few things we’ll briefly touch points on are pricing models, discounts and steps you can take to avoid wasted cloud spend.
The leading cloud service providers have certain fortes and weaknesses that ultimately differentiate each one of them to be the potential solution to support your development infrastructure, operations, and applications. Cloud service providers offer many different cloud pricing points depending on your compute, storage, database, analytics, application and deployment requirements. Additionally, you’d want to consider available services and networks provided to see the full scope of their resource capabilities and governance.
Prices can be subject to the type of hosting option you choose. One example is Relational Database Services (RDS). RDS pricing changes according to which database management system you use, and there are many more services like this to choose from.
More detail, beyond just storage, available in our full cloud pricing comparison.
Although not always the case, AWS is presumed to be the least expensive option available and remains the leader in the cloud computing market. But, Microsoft Azure and Google (GCP) are not far behind, and in recent years they have commanded innovation and market pricing reductions, thus closing gaps to bring them closer to AWS. That been said, being the first in the market gives AWS a great advantage over the competition as they command a large scale of businesses and are able to offer lower prices than the competition. They are well known for attracting more businesses, and in turn, they invest their money back into the cloud by adding more servers to their data centers. Google is closing the gap on AWS as they were the first to cut prices in their pricing model to match AWS’.
Let’s take a look at some of the more popular storage options offered by each of the major three providers.
Amazon Simple Storage Service (S3) is the most durable, highly performant and secure cloud storage service. It manages accounts at every level, scales on-demand and offers insights with built-in analytics.
Amazon Elastic Block Store (EBS) provides block level storage volumes for use with EC2 instances. EBS delivers low-latency and consistent performance scaled to the needs of your application.
Amazon Glacier provides data archiving and long-term back up at a low-cost. It allows you to query data in place and retrieve only the subset of data you need from within an archive.
More about AWS options: https://aws.amazon.com/products/storage/
Google Cloud Storage offers a single API for all storage classes, simplifying development integration and reducing code complexity. Its highly scalable and performant with unlimited object storage.
Google Filestore is a high-performance file storage for applications that require a filesystem interface and a shared filesystem for data.
Google Persistent Disk is a reliable high-performance block storage for virtual machine instances.
Explore Google storage options: https://cloud.google.com/products/storage/
Azure Archive Storage offers a low-cost, durable, and highly available secure cloud storage for rarely accessed data with flexible latency requirements.
Azure Blob Storage is a massively scalable object storage for unstructured data.
Azure Files is a simple, secure and fully managed cloud file sharing storage.
Check this out as well on Azure options: https://docs.microsoft.com/en-us/azure/architecture/aws-professional/services
Comparing cloud storage costs and getting the right solution for your storage use case is important, but don’t forget once you deploy you need to ensure you optimize your solution and cost. It’s important that your organization fully understands how much can be wasted on cloud spend. Over-provisioned, underutilized and idle cloud resources run your cloud bill up and create waste. Always ensure that you are optimizing costs and governing usage by eliminating wasted cloud spend — get started today.
Originally published at www.parkmycloud.com on April 11, 2019.
CEO of ParkMyCloud
See all (317)
27 
27 claps
27 
CEO of ParkMyCloud
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/running-a-static-website-with-hugo-on-google-cloud-storage-f84320f6ab35?source=search_post---------84,"There are currently no responses for this story.
Be the first to respond.
I’ve played a bit with Hugo, the static web site generator written in golang that has been getting a lot of good press lately. At the suggestion of my colleague Warren Runk, I also experimented with hosting the static files generated by Hugo on Google Cloud Storage (GCS). That way there is no need for launching any instances that would serve those files. You can achieve this by using AWS S3 as well of course.  You first need to sign up for a Google Cloud Platform (GCP) account. You get a 30-day free trial with a new account. Once you are logged into the Google Cloud console, you need to create a new project. Let’s call it my-gcs-hugo-project.  You need to also create a bucket in GCS. If you want to serve your site automatically out of this bucket, you need to give the bucket the same name as your site. Let’s assume you call the bucket hugotest.mydomain.com. You will have to verify that you own mydomain.com either by creating a special CNAME in the DNS zone file for mydomain.com pointing to google.com, or by adding a special META tag to the HTML file served at hugotest.mydomain.com (you can achieve the latter by temporarily CNAME-ing hugotest to www.mydomain.com and adding the HEAD tag to the home page for www).   If you need to automate deployments to GCS, it’s a good idea to create a GCP Service Account. Click on the ‘hamburger’ menu in the upper left of the GCP console, then go to Permissions, then Service Accounts. Create a new service account and download its private key in JSON format (the key will be called something like my-gcs-hugo-project-a37b5acd7bc5.json.  Let’s say your service account is called my-gcp-service-account1. The account will automatically be assigned an email address similar to my-gcp-service-account1@my-gcs-hugo-project.iam.gserviceaccount.com.  I wanted to be able to deploy the static files generated by Hugo to GCS using Jenkins. So I followed these steps on the Jenkins server as the user running the Jenkins process (user jenkins in my case):  1) Installed the Google Cloud SDK
2) Copied the service account’s private key my-gcs-hugo-project-a37b5acd7bc5.json to the .ssh directory of the jenkins user.  3) Activated the service account using the gcloud command-line utility (still as user jenkins)
4) Set the current GCP project to my-gcs-hugo-project
5) Configured GCS via the gsutil command-line utility (this may actually be redundant since we already configured the project with gcloud, but I leave it here in case you encounter issues with using just gcloud)
6) Added the service account created above as an Owner for the bucket hugotest.mydomain.com
7) Copied a test file from the local file system of the Jenkins server to the bucket hugotest.mydomain.com (still logged in as user jenkins), then listed all files in the bucket, then removed the test file
8) Created a Jenkins job for uploading all static files for a given website to GCS
Assuming all these static files are checked in to GitHub, the Jenkins job will first check them out, then do something like this (where TARGET is the value selected from a Jenkins multiple-choice dropdown for this job):
The first gsutil command does a recursive copy (cp -r *) of all files to the bucket. This will preserve the directory structure of the website. For testing purposes, the gsutil command also sets the Cache-Control header on all files to private, which tells browsers not to cache the files.
The second gsutil command is executed for each object in the bucket, and it sets the ACL on that object so that the object has Read (R) permissions for allUsers (by default only owners and other specifically assigned users have Read permissions). This is because we want to serve a public website out of our GCS bucket.
At this point, you should be able to hit hugotest.mydomain.com in a browser and see your static site in all its glory.
I’ve only dabbled in Hugo in the last couple of weeks, so these are very introductory-type notes.
Installing Hugo on OSX and creating a new Hugo site
At this point you have a skeleton directory structure created by Hugo (via the hugo new site command) under the directory hugotest.mydomain.com:
(note that we symlinked the themes directory into the hugotest.mydomain.com directory to avoid duplication)
Configuring your Hugo site and choosing a theme
One file you will need to pay a lot of attention to is the site configuration file config.toml. The default content of this file is deceptively simple:
Before you do anything more, you need to decide on a theme for your site. Browse the Hugo Themes page and find something you like. Let’s assume you choose the Casper theme. You will need to become familiar with the customizations that the theme offers. Here are some customizations I made in config.toml, going by the examples on the Casper theme web page:
I left most of the Casper-specific options commented out and only specified a cover image, a logo and a description.
Creating a new page
If you want blog-style posts to appear on your home page, create a new page with Hugo under a directory called post (some themes want this directory to be named post and others want it posts, so check what the theme expects).
Let’s assume you want to create a page caled hello-world.md (I haven’t even mentioned this so far, but Hugo deals by default with Markdown pages, so you will need to brush up a bit on our Markdown skills). You would run:
This creates the post directory under the content directory, creates a file called hello-world.md in content/post, and opens up the file for editing in the editor you specified as the value for newContentEditor in config.toml (vim in my case). The default contents of the md file are specific to the theme you used. For Casper, here is what I get by default:
Now add some content to that file and save it. Note that the draft property is set to false by the Casper theme. Other themes set it to true, in which case it would not be published by Hugo by default. The slug property is set by Casper to “post-title” by default. I changed it to “hello-world”. I also changed the tags list to only contain one tag I called “blog”.
At this point, you can run the hugo command by itself, and it will take the files it finds under content, static, and its other subdirectories, turn them into html/js/css/font files and save it in a directory called public:
That’s quite a number of files and directories created by hugo. Most of it is boilerplate coming from the theme. Our hello-world.md file was turned into a directory called hello-world under public/post, with an index.html file dropped in it. Note that the Casper theme names the hello-world directory after the slug property in the hello-world.md file.
Serving the site locally with Hugo
Hugo makes it very easy to check your site locally. Just run:
Now if you browse to http://localhost:1313 you should see something similar to this:
Not bad for a few minutes of work. For other types of content, such as static pages not displayed on the home page, you can create Markdown files in a pages directory:
Note that the menu property value is “main” in this case. This tells the Casper theme to create a link to this page in the main drop-down menu available on the home page.  If you run hugo server again, you should see something the menu available in the upper right corner, and a link to static1 when you click on the menu:
To deploy your site to GCS, S3 or regular servers, you need to upload the files and directories under the public directory. It’s that simple.
I’ll stop here with my Hugo notes. DigitalOcean has a great tutorial on installing and running Hugo on Ubuntu 14.04.
Originally published at agiletesting.blogspot.com on February 12, 2016.
Google Cloud community articles and blogs
7 
7 claps
7 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevOps, cloud computing, Python and Golang programming, data science, automated testing.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@joeokpus/uploading-images-to-cloudinary-using-multer-and-expressjs-f0b9a4e14c54?source=search_post---------335,"Sign in
There are currently no responses for this story.
Be the first to respond.
okpukoro joe
Nov 21, 2018·7 min read
File upload has become a common feature in every web application, many tutorials you’ll find on the web will only show you how you can upload files to your application’s database and you get stuck when you’re trying to upload your images to a cloud service like Cloudinary and Amazon S3. During the course of this tutorial, we’ll be using Cloudiary for storing our images, we will then save the link to that image to our database instead of the entire image buffer.
In this article, I’ll be showing what Cloudinary and Multer does and how easy it is to work with them whether you’re working on API for a Single Page Application, or building a monolithic application this tutorial got you covered.
Before we proceed, I’ve set up an empty ExpressJS application with support for ES6+ so that we can focus more on the file upload feature, you can access it from here.
Let’s begin by first knowing the use of Cloudinary.
Cloudinary is an end-to-end image management solution for your website and mobile apps. Cloudinary covers everything from image uploads, storage, manipulations, optimizations to delivery.
You can easily upload images to the cloud, automatically perform smart image manipulations without installing any complex software. All your images are then seamlessly delivered through a fast CDN, optimized and using industry best practices.
Cloudinary offers comprehensive APIs and administration capabilities and is easy to integrate with new and existing web and mobile applications.You can learn more about cloudinary here
To get started, create/sign to your Cloudinary account, on your dashboard you should see your account details like this.
On the root directory of your application, there’s a sample.env file that contains all the environment variables we’ll be using for this application. You should see something like this.
Create a .env file on the root directory of your application, copy all the variable names and replace the values with your Cloudinary info.Let’s install the Cloudinary package that will allow us to interface with the Cloudinary API npm install cloudinary.
Now to finish up the Cloudinary setup process, On your server/ directory, create a config/ folder, and a file named cloudinaryConfig.js. This directory will contain the configuration you’ll need to upload files to Cloudinary. When you’re done, your directory should now look like this.
On the cloudinaryConfig.js file, configure the package to use our Cloudinary credentials with the block of code below.
Great!!! Cloudinary has been set up successfully on your application, we can now proceed to upload files with the aid of multer.
npm i multer
Multer is a node.js middleware for handling multipart/form-data, which is primarily used for uploading files. It is written on top of busboy for maximum efficiency.
Why Is Needed? Out of the box, NodeJS doesn’t know how to process any data, for example, we wanted our application to process JSON requests, we had to body-parse, in this case, `multer` makes it possible for an application to accept form-data which allows us to easily transport files from the browser to our server.
www.npmjs.com
Without `multer`, when you’re sending a files to your server, the request body object(req.body) will be empty. Multer allows the data you’re sending from a form to be available and it also creates a req.file object which gives us access to the file buffer we’ve just uploaded from the client-side.
Now let’s configure Multer as a middleware that we can use with any route that needs to upload an image. On the middleware folder, create a multer.js file with the following configuration.
Notice that we’re making use of memory storage instead first writing the file to an upload/ directory, this is because when we deploy our application to Heroku, we may not have the adminitrative privileges to write files to the remote computer which may crash our entire application.
multer.memoeryStorage() tells multer that we’ll save the file to memory first which can be manipulated to the suit any purpose.
multer({ storage }).single('image'); sets the storage option we’ll be using on the application while the .single('image'); specifies the field name multer should go to when it’s looking for the file.
Now it’s all set, let’s try to use multer and Cloudinary to upload an image.
Import multerUploads on your server/index.js and add the following block of code to your server/index.js file
Now open postman on the body section, select forma-data and enter the field name you used in your multer configuration which in our case was image. Your postman should look like this, now click on send.
Notice it logs an empty object on the console.
Now on the fieldname ""image"", change the type textto file just as in the screenshot above, you should have something similar to this.
On the upload route, change `console.log(‘req.body :’, req.body);` to console.log('req.file : ' req.file); go back to postman, and select an image, hit the send button and you should have something like this
Congratulations multer is now working as a middleware, we can go ahead and use it on our entire application, but for the sake of this article, we’ll only call multer on the routes we need it.
But I want to you to notice that the file is coming as buffer because we’re pushing it to memory before we can upload it to Cloudinary, which pose an issue for us since the Cloudinary uploader is either expecting a file path or a string but we’re receiving a buffer from multer.
So let’s convert the buffer to the format that the Cloudinary uploader will understand, we’re going to use thedatauri package.
A data URI is a base64 encoded string that represents a file. Getting the contents of a file as a string means that you can directly embed the data within your HTML or CSS code. When the browser encounters a data URI in your code, it’s able to decode the data and construct the original file. Learn more
npm i datauri to add the package and then update your multer setup to look like this.
On the dataUri function above, we pass the request object and format the buffer and return a string blob.
It’s time to add the Cloudinary uploader to the application, but first let’s make some modification on the configuration function, it’s best we convert it to a middleware so that we don’t have to invoke it everytime we want to use the Cloudinary settings.
Your new Cloudinary config should look be like this.
Notice I added a new package called dotenv it allows us load the settings on our .env file into the node application process.
npm i dotenv , update your server/index.js to look like this.
Open postman again, select a file and click send you should get this message
you can now save the image URL to your database.
Congratulations, you just learn how to upload files to Cloudinary using multer.
I hope this was helpful, please leave your thoughts in the comment section below.
Link to this repo
1.1K 
18
1.1K claps
1.1K 
18
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/using-alibaba-cloud-storage-gateway-for-hybrid-solutions-abfbfbcdd4f3?source=search_post---------126,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Oct 10, 2019·6 min read
Alibaba Cloud Storage Gateway (CSG) is a hybrid cloud solution from Alibaba Cloud that can help customers resolve multiple use cases pertaining to cheaper cloud storage solutions, data backups and data archiving. CSG comes in two flavors — file and block — and two deployment models — online and offline (i.e in Alibaba Cloud and customer premises).
You can find more information and details about Alibaba Cloud CSG in the official documentation.
In this blog post, I’ll show you step-by-step how to create a file gateway and use Alibaba Cloud Object Storage Service (OSS) as the storage backend. This can be useful in scenarios where the customers don’t want to commit to a large onsite storage and instead can use Alibaba Cloud OSS as the storage solution for TBs or PBs worth of data. Though the same outcome can be achieved by an offline deployment as well as online deployment (barring certain limitations highlighted at the end of this post), I’ll use an example of online deployment for this post.
In this solution, Cloud Storage Gateway will be used to expose a folder created in the OSS bucket to the client server as a NFS mount. This mount can be then used to store the data and only the data equivalent to the gateway cache size will be stored locally whereas all the excess data will be available in the OSS. I’ll use an Elastic Compute Service (ECS) instance to mount the NFS share, however the same can be done on an on-premises VM.
In this demo, I’ll use following Alibaba Cloud products:
1. On the Alibaba Cloud Storage Gateway console, create a gateway cluster.
2. Once the cluster is created, create a new gateway in the console.
Click on “Create” button in the right corner. You will see the following screen.
Location — Alibaba Cloud means the online deployment (this is what we are going to see in this example) and On-premise is the offline deployment (i.e in customer’s environment on a VM)
Type — File Gateway (NFS or SMB protocols that we are going to use in this example) and iSCSI Gateway is the block gateway
In the next screen, choose the gateway model. The gateway models vary in terms of the gateway bandwidth. Here you also attach the gateway to a VPC (Virtual Private Cloud) in your environment.
3. Once created you should be able to see the newly created gateway in the list on the gateway console
4. Next step is to create the gateway cache that will be used as the local storage on the client server.
For this, click on the gateway link on the console > click on the cache link in the left navigation menu > click on “create cache” button on the left right corner of the screen.
Chose the size of cache and disk type
5. Next step is to create the NFS share. For this, go the share link in the console and click “create” button
6. On the first screen, select the OSS bucket that you want the NFS share to be mapped to
To be able to map the NFS share to a specific folder in your OSS bucket — After selecting the bucket, make sure you check “Path Prefix” and then type in the exact folder name that you want to map to the bucket. You can also map sub-folders like Test1/test2 in the Path prefix
7. Next, provide the details like share protocol (NFS or SMB) and available cache (that we created in the step#4)
Note that an already mapped cache disk can’t be mapped to another share (they won’t even appear in the drop down). So for every share you create, there must be dedicated cache disk
8. Next is the advanced configurations.
You can leave the advanced settings as default. Check the explanation for each of them before changing the configuration.
9. On the last screen, you will be shown the summary of configuration just made
The new share will appear in the list of shares on the console. You can now manage the share in the console
10. Now perform the following steps on the client Ubuntu ECS server (assuming you already know how to create one). This is the server instance where you want to mount the NFS share
You can see the newly mounted NFS share in the screenshot
To test, create a file on the server inside the NFS share that you just mounted
The same file should appear in the OSS bucket inside the folder that you mapped while creating the share
Please keep in mind the following consideration when adopting a Cloud Storage Gateway based solution:
Author: Rohit Gupta, Solutions Architect
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
50 
50 claps
50 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ipdb-blog/forever-isnt-free-the-cost-of-storage-on-a-blockchain-database-59003f63e01?source=search_post---------205,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Cloud storage services work as follows: You pay a monthly fee up front for a fixed amount of storage space. During the paid time, you can use any amount of storage space up to that limit. When your paid time expires, you have two choices: pay for another month or your files get deleted. Your cloud provider only keeps your files for as long as you keep paying.
Blockchain databases can’t work on this model. A blockchain database must store data indefinitely, so the recurring payment model doesn’t work. Data storage costs must be paid up front, and must cover not just that month but all the months and years to come.
IPDB has developed a sustainable model for the long term storage of data: a one time, up-front payment that covers the cost of indefinite data storage. The payment must be enough to cover the cost of storage and the IPDB Foundation’s operating expenses.
This blog post is a deep dive into the numbers that led to a single per-GB price point — the cost of storing data indefinitely in a blockchain database.
This kind of analysis has been lacking in the hype around blockchain technology. There are many problems that could be addressed with blockchain technology, but without an understanding of what a blockchain solution will cost, it is impossible to say whether economic efficiencies can be achieved. This post is a first step toward understanding which use cases could truly benefit from the application of blockchains.
Before we dive into the model, let’s outline some of our underlying assumptions:
Conservative predictions: As a general rule, we have tried to keep estimates and assumptions very conservative. We would rather have happy surprises than unhappy surprises if our numbers turn out to be off.
Replication: We want to have a replication factor of six, meaning at least six copies of each transaction on six distinct nodes. For extra comfort and security, there will be one additional backup of the entire network.
Transaction Volume: We have estimated the rate of adoption for IPDB. In the first few years we assume adoption to increase exponentially like most technological adoption.
This curve is modelled by:
where t is the number of years away from 2017 i.e. t=1 for 2018.
We chose to model transaction growth using an S-curve because the adoption of similar technologies followed that pattern. In our model, the denominator gives the curve its S-shape. We assume the IPDB will start with approximately 0.37 transactions/sec in 2018, so we include the 16 to shift the curve to start here. The model exhibits a conservative ramp up, with the 1.2 providing the compression of the curve’s growth. These numbers were chosen to model the rapid adoption of successful technologies in the 21st century and the usage we are predicting.
The number of transactions the network can handle can’t grow to infinity, so we provide a limit for the number of transactions per second. The cap is described by the numerator at 1 million transactions every second which is achieved by the model in about 15 years. We also consider limits at 500,000 transactions/sec and 5 million transactions/sec.
Transaction Size:
The maximum BSON document size in MongoDB is 16 MB. Since a block is a document and can contain up to 1000 transactions we assume the soft limit for a single transaction to be 16MB/1000 = 16kB. The size of a single transaction may be anything smaller than this however, so we consider 1.5kB, 7kB and 15kB in our calculations. Given usage of the IPDB Test Network so far, we expect transaction size to trend toward smaller sizes, likely in the 1.5kB to 7kB range.
Time Value of Money: Since we are planning to store data for as long as possible, the majority of the cost of storing that data will be spread out over years. The initial payment will leave the IPDB Foundation with a significant balance that will be invested conservatively. We assume a modest 3% return on that balance, compounded annually.
Inflation: For all our costs, we account for inflation which has historically been around 2%, compounded annually.
Forever: The IPDB plans to store data indefinitely but we only run our calculations to 50 years. We embrace long-term thinking, but even this timeframe is difficult to work with given the pace of technological change.
Let’s start with the money coming into the IPDB Foundation.
1.1 One Time Payment
Users will pay per gigabyte to write data to IPDB. In practice, this will be an up-front fee that allows a certain amount of storage, but for simplicity we will use a flat fee per gigabyte of storage used. This is calculated as the amount of data stored in GB, multiplied by the cost per GB in dollars. There will be no ongoing cost for storing data. The initial fee is for indefinite storage.
1.2 Balance from the Previous Year
The Balance is key to the sustainability of the IPDB financial model. The amount not spent each year can be invested and used in following years to cover the costs of indefinite storage.
where X is the per-GB cost in dollars.
2.1 Storage Costs
The cost of storing data has decreased exponentially as technology improves:
That shows us what a GB of storage will cost in any given year. Now we can calculate our total costs for storage. Each year we have to pay for the new data received in that year and continue to store all data from previous years. So for each year the storage cost is:
2.2. Intercluster Communication Costs
We need to factor in the cost of sending and receiving data. Intracluster communication costs are the costs of transferring data from one node to another within the same cloud network, whereas intercluster costs are for outbound data transfers. During our initial rollout, all nodes will be hosted in the Microsoft Azure cloud for ease of deployment and support. Within Azure, all inbound data transfers are free. Once we are running in 2019 we aim to have approximately 2/3 of all nodes hosted outside the Azure network. As a reference we consider the Azure pricing model given below:
By 2025 we aim to have 50 nodes, with approximately 34 not on Azure. We predict 366,184 GB of new data for 2025, all of which must be sent to each of those 34 external nodes.
In 2025, the first 120,000 GB of data will cost $0.138 per GB so we’ll pay $16,560. Similarly we pay $64,800 for the next 480,000 GB of data, $156,000 for the next 1.2 million GB, $504,000 for the next 4.2 million GB and the $620,613 for the remaining data.
In total this works out to $1,361,973 in intercluster costs for 2025.
To make it easier we will define:
We must also consider that bandwidth costs have been decreasing rapidly since 1997. The literature shows a decrease of 27% annually. It seems safe to assume this trend will continue as we see with higher utilisation rates for existing networks and new fibre coming online. If costs continue to decline 27% each year, by 2025 we should have:
This decrease has a significant effect on intercluster costs over time. In general we have:
In reality, given the large volumes of data transfers predicted, IPDB will be in a position to negotiate wholesale data transfer rates. This model provides an upper limit on how high the price for outbound data could be. Once we also factor in inflation, costs will look like:
2.3. Fixed Costs
So far we’ve only considered the cost of storing and transferring data. What about operational costs like staff, facilities, marketing and outreach, legal and accounting, and other expenses necessary to support IPDB? Unlike physical storage costs, logistical costs (staff, rent, etc.) do not decline but rather increase over time. Staffing costs assume we will grow the team to keep up with the volume of work, and offer wage increases to at least match inflation. Other costs increase to match the needs of the organisation and to account for inflation. We’ve also assumed that some people will not like IPDB or the data stored on it, so we’ve budgeted for legal fees.
At the outset, operational costs are the majority of IPDB’s expenses. These costs become a much smaller percentage of IPDB’s overall expenses as usage increases. Over time, fixed costs per GB decrease significantly. For example, even with the new hires we have budgeted for, the ratio of data stored to staff member salary will increase by factors of over 100.
The ultimate goal is for IPDB to become self-sustaining. This will happen by 2023, according to this model and our assumptions. Until then, we will work to minimize operational costs. Many costs will be covered by BigchainDB. Further operational costs will be funded by grants and donations in this period. The total costs each year are given by:
where F(t) refers to fixed costs.
So what’s the final number?
Financial sustainability is the most important piece of the puzzle. If we set the price too low, even though we can cover the cost of storing data for a long time eventually the number of new transactions each year will wane and yearly revenue will fall below yearly costs. We need to set the price such that investment income on the balance, not new fees, can be used to cover ongoing costs.
That final number is a one time fee of $100 per GB. This allows us to store data indefinitely while covering the cost of operating the IPDB Foundation.
Charging $100 per GB will see us becoming revenue-positive by 2023 with a total shortfall of $3,248,796 that must be recovered through donations or grants. This per-GB price also allows us to break even in the same time scale if our transaction rate is halved and capped at 500,000 transactions per second.
As it scales up, the marginal cost of storing each additional GB falls significantly, allowing IPDB to focus resources on becoming fully decentralized, semi-autonomous internet infrastructure that can store vast amounts of data.
The $100 price point is a maximum because of our conservative estimates. If costs drop faster than expected, we could reduce that price over time. For example, decentralized file storage provided by services like IPFS may prove cheaper than existing cloud options or even self-managed storage, or technological breakthroughs could dramatically reduce costs. But for now, $100 is a safe estimate that provides certainty to people hoping to build on IPDB.
At our $100 price point if we assume a single transaction is of average size (7 kB) how much will it cost an IPDB user to validate and store it for 50 years in IPDB?
That is $0.0007 or 7/100 of a cent for a transaction.
As a comparison, how much would it cost to store the same amount of data for an indefinite period on the Bitcoin or Ethereum blockchains?
Even though it is possible to store data on the Bitcoin blockchain, the Bitcoin protocol was not designed with data storage in mind. However, as blockchain use cases expanded beyond finance, many companies started using the Bitcoin blockchain as a database all the same.
To store data on the Bitcoin blockchain we would enter the data in the OP_RETURN field of Bitcoin transactions. The OP_RETURN field allows a user to send a transaction that doesn’t actually send money to anyone, but allows a small amount of data to be written to the Bitcoin blockchain. Each OP_RETURN output has a maximum size of 80 bytes, and each transaction can have one OP_RETURN output.
To store the same 7KB transaction we have been working with would require 88 OP_RETURN messages. As long as each one is a valid transaction, with a dust fee of 546 satoshis or more, each message will be propagated through the network and mined into a block.
At the current BTC/USD ($2518) exchange rate the dust fee is
As of July 2017, the median Bitcoin transaction fee is about $1.82. So the cost to store 7KB would be:
1GB would need 12,500,000 OP_RETURN messages so would cost approximately $22,766,250. This figure is highly dependent on transaction fees, which have increased dramatically over the past year as Bitcoin has not found a scaling solution. In any event, this is a theoretical exercise and not a proposal to use the Bitcoin blockchain for large-scale data storage.
Transactions in Ethereum work completely differently than in Bitcoin, requiring “gas” to have data processed.
To execute any regular transaction with no embedded data on the Ethereum blockchain uses 21,000 gas. This is the minimum gas limit required.
If you want to include data in your transaction you can do so in one of two ways: by creating a contract, or by sending a message call. Sending a message call allows the user to interact with other accounts or smart contracts without having to create their own contract. It requires the least gas of the two methods, so we’ll send 7KB via a message call.
The gas cost is not just based on how big your data is but also how complex. The most basic data we could send in a 7KB message call would be comprised of only zeroed bytes. If the data included text this would mean the message would have non-zeroed bytes. According to the Ethereum Yellow Paper each zeroed 32 byte costs 4 gas and every non-zero 32 byte word of data requires 68 gas to send so assuming all the bytes are zero provides a minimum gas cost.
7000/32= 219 “32 bit words” so we would need an additional:
That doesn’t seem like much if that was all you wanted to do, but storing the sent data is an additional operation. Every 32 byte word costs 20,000 gas. This is one of the largest gas requirements for any EVM OPCODE, reflecting that this is not a simple operation but one that is being replicated and stored across thousands of nodes. To store all 7KB would be:
Storing and sending 7KB of data requires 4,401,876 units of gas. At the current median gas price (28 Gwei) and ETH/USD exchange rate ($267) this transaction will cost you about $32.91.
In general, the cost to store data on Ethereum works out to approximately 17,500 ETH/GB, or around $4,672,500 at today’s prices.
As noted above, we are not suggesting that large quantities of data — images, videos, audio, other datasets — should be written to Bitcoin or Ethereum. We understand this is not the point.
However, it is important to understand which use cases are economical and which are not. Many of the applications that have been proposed for blockchains — energy markets, music streaming services, IoT, and so on — will require storage of vast quantities of transactional information. This is exactly the kind of data that should be stored within the blockchain database.
In future posts we will explore the economic implications of this. What use cases can IPDB unlock that would be uneconomical on other blockchain databases?
Co-authored with Simon Schwerin and Greg McMullen.
Visuals by Wojciech Hupert (unless noted).
Thanks to Trent McConaghy, Bruce Pon, Troy McConaghy, Tim Daubenschütz, and Simon de la Rouviere for comments and support.
An internet-scale blockchain database for the planet.
1.1K 
6
1.1K claps
1.1K 
6
Written by
Internet Policy Intern at IPDB Foundation.
An internet-scale blockchain database for the planet.
Written by
Internet Policy Intern at IPDB Foundation.
An internet-scale blockchain database for the planet.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.sia.tech/addressing-thoughtful-concerns-about-sias-security-viability-103ccfff5e92?source=search_post---------352,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Following a recent article, a flurry of tweets appeared questioning Sia’s security and viability. A lot of these concerns are more focused, more technical, and aren’t brought up often. But the responses do give a lot of insight into the construction of the Sia network, so I thought I would answer them thoroughly.
1. How vulnerable is Sia to a 51% attack?
Sia is more vulnerable than Bitcoin to a 51% attack. We have fewer mining pools by a significant amount, and less hashrate by a significant amount. That said, our hashrate suggests that there is between 10,000 and 50,000 GPUs actively hashing on our network at all times. A 51% attack is not something you could just perform by pointing your own hardware at it, or even by dumping a bunch of money into an AWS cluster. Your best strategy would be to either bribe the larger mining pools, or find some way to compromise them. Of course, the moment a mining pool starts acting up all of the miners can flee to another pool.
Generally speaking, Sia is safe against 51% attacks. It’s not as strong as Bitcoin. But it’s also a much larger + more heavily hashed blockchain than any that has been successfully attacked for a sustained period of time.
2. The IP addresses of the Sia hosts + nodes are a hit list for hackers. What if someone targets all of the machines on the ecosystem?
Hosts in Sia are expected to be performing their own devops. A concern was expressed that this would not be sufficient, and perhaps that is valid concern. To-date, we have not seen widespread hacks of peer-to-peer ecosystems. When an ecosystem is getting attacked, generally its the protocol that’s being attacked or DDoS’d, and not the individual nodes themselves.
And there’s a good reason for that. Nodes run on a highly diverse set of platforms, running different code stacks and with different vulnerability profiles. Taking out a significant part of the Sia network either requires finding a significant vulnerability directly in the Sia software itself, or else requires using a wide range of exploits to target a large number of highly diverse machines.
Sia adds high redundancy to files to improve security. The default settings are 10-of-30, meaning out of 30 hosts, you only need 10 to survive an attack/disaster/etc. for the file to be retrievable. Hackers trying to target your specific hosts would need a 66% success rate to actually do any damage. And if this attack didn’t all go off at the same time, you’d be able to quickly restore the redundancy.
Overall, I would say that the risk of wide-ranging attacks targeting our hosts is a more significant risk than a 51% attack. But it’s an attack that would require a lot of expertise, a lot of time, and a lot of ready-to-go exploits. Unfortunately, ready-to-go exploits are not actually uncommon today, which definitely amplifies the risk of this type of attack.
3. Do you really expect your host nodes to be able to perform competent SecOps?
That is a good question. Sia expects hosts to be performing their own secops in the standard case. Certainly today we don’t have any certifications or checks that we put hosts through to make sure they are protecting themselves from attack. It’s something we’re looking into but would like to avoid, as it clearly hurts decentralization (getting a certification requires having a central entity that can choose to not certify you).
As Sia continues to grow, we very much expect the vast majority of nodes on Sia to be dedicated machines. This happened with Bitcoin mining. We tried to mirror that template — Bitcoin mining was so brutally effective because there was a very clear relationship between hashrate and profit. No business development needed, no marketing needed, you needed to be good at only one thing to build a successful Bitcoin mining operation (hashrate!). Sia has tried to replicate this with storage. You don’t need to do any marketing, and business development, you don’t need to have a recognized name. You just need a storage offering that is more competitive than the other guys on the network.
We expect most hosts on Sia to be dedicated hosts, which makes SecOps a lot easier. Run a secure operating system, open only exactly the ports you need, run only exactly the software you need. It’s not bulletproof but it’s also much easier than keeping your family computer safe. And, there’s a strong incentive to keep the hosts safe, because they have money on the line. Hosts are putting collateral into contracts that they will lose if their machine is compromised. Hosts need to keep money in their wallets (to provide the collateral in the first place) that they will lose if their machine is compromised.
I ultimately do expect our hosts to be able to have reasonable security. And further, I expect our hosts will be choosing a high diversity of platforms, which means you’ll need a large number of vulnerabilities to do any real damage.
4. I can buy harddrives at $25 / TB, what’s the value add for Sia?
The storage price on Sia today is $1/TB/Mo (that’s after redundancy). This can be compared to $23/TB/Mo on Amazon S3, which is the most comparable service to Sia. Sia is extremely inexpensive for the type of service it offers, and that gives Sia a massive advantage in the market. And in fact, accounting for redundancy, Sia is price competitive with hard drives as well. And there are a lot of elements that drive Sia’s competitiveness.
First, Sia hosts get to enjoy unique advantages that you don’t get to enjoy when you are buying drives for home. Some hosts already had drives laying around that weren’t in use. Some hosts were able to get strong bulk discounts on their storage. Some hosts have access to cheap/free phyical space or electricity, among other advantages that we don’t all get to enjoy. These advantages drive the overall price of storage down.
Second, Sia uses high-scale redundancy which makes the platform substantially more reliable. Probabilistically, a 10-of-30 redundancy scheme is far safer than a 1-of-3 redundancy scheme, provided the reliability of each is higher than 33% (Sia targets 95% reliability for hosts, drives will typically have 98%+ reliability). As the confidence in our software grows, we plan on reducing to as far as a 30-of-50 scheme, which probabilistically is more reliable even than a 1-of-5 redundancy scheme for the same quality of hard-drive, despite being only 1.66x overhead instead of a 5x overhead.
Finally, Sia has all the benefits and more of a cloud. Sia is a very early platform, so not all of the features I’m about to mention are in place, but we expect they will all be there within the next 12 months. The architecture has been explicitly designed to allow for them, we just need more engineers and engineering time:
5. I can buy tape for $5 / TB, what’s the value add for Sia?
Tape requires a lot of hardware, some expertise, can be difficult to work with, and has slow seek / random access times. It’s a completely different class of storage from what Sia currently offers.
That said, tape offerings are actually something that could be done over the Sia network. And I’m guessing that if tape was offered over the Sia network, you would see substantially lower prices for those offerings. Uploads to Sia would not even need to be all tape — you could do the first 1.25x redundancy as fast-storage, and the second 1.25x redundancy on tape as contingency storage. Or you could fully optimize for price and just put it all on tape. And if you are doing that, Sia has all the same advantages as listed in the previous question.
6. Other clouds such as S3 are far more secure, why would I take a risk with Sia?
I don’t believe that you can cleanly assert that clouds such as S3 are more secure. It is true that they have dedicated experts looking over the security of the system, which is something Sia hosts do not have. But it’s also true that they are much juicier targets. When all of the data is on a single system, it becomes much more worthwhile for a dedicated hacker to target that system in depth. The iCloud leaks are a good example of this. A massive treasure trove of illicit photos was distributed resulting from a single vulnerability.
With Sia, data is spread across a large number of nodes. At scale, we’ll be talking about thousands of nodes that are mostly dedicated exclusively to hosting data on Sia. Each host will certainly be more vulnerable than a massive datacenter like iCloud, however the payoff for hacking one will be a lot smaller. Is it easier to hack a single Apple cloud, or hundreds of diverse, widely distributed nodes which are each individually weaker?
It ultimately depends on how much weaker each individual node is. I am guessing that each host will in fact be pretty secure on average, because they will be earning revenue from the machine, will have collateral on the machine, and will be motivated to protect what they own.
7. I have a fire-proof vault at home, a sophisticated backup scheme, with expert-grade security on my system, and tapes in another country that I use to back up my data. It’s very cheap. Why should I switch to Sia?
If you are doing the above, you are probably a security expert and probably well above the competence of our general target market. If you are a business, you are probably paying a team of people to manage your backups and such, so don’t forget to include their salaries in the total-cost-of-ownership. Plus the overheads of needing to deal with things like headhunting + retirement.
The truth is that, with enough expertise, you absolutely can beat the Sia offering. But that requires you to either be an expert, or to hire an expert. And Sia proposes to be cheaper than hiring the expert. If you are a normal person or a normal business, you have other things that you should be focused on and worrying about. Sia hopes to offer you a complete solution that completely obliterates the TCO of other solutions in the space. If you run the numbers, I expect you’ll find that our price ($1/TB/Mo) is hard to beat.
8. I don’t understand why Sia is a cheaper option.
Sia’s fundamental advantage is the marketplace. It’s a simple marketplace where the only thing that matters is efficiency. The cloud storage market today requires a substantial amount of trust, marketing, and business overhead. If you want to start a cloud storage company that is selling data out of your garage, you won’t be able to. And it won’t matter if your garage has superlative security, price, speed, etc. etc., because nobody will trust you. Garages aren’t known for being competent data storage facilities. Enterprises want datacenters, and they don’t just want any datacenter, they want the reliable one, the known one. They want legal paperwork and SLAs and a guarantee that someone can be sued if they screw up. There’s a saying “nobody ever got fired for choosing IBM.” That’s because IBM has a name, a reputation, a massive business arm, and they will take care of you in many ways that extend beyond your data.
All of that is overhead. It’s expensive. Names take a long time to establish, and once they are established they can be cashed in through increased prices. In today’s world, running a cloud storage business requires massive overhead.
Sia changes all of that. The fundamental improvement is that we make it safe to trust some random garage to hold your data. And there are a number of things that we need to do to make this reasonable and secure where it was not previously reasonable.
So we’ve now made it reasonable to trust a random nobody with your data. Because you aren’t merely trusting them, you are trusting that out of 30 nobodies, at least 10 will still be there when you need the file. And you know that if any of the hosts drop out, the ones that drop out will be paying huge financial penalties. And as they do drop off, it’s easy for you to form new contracts and restore the lost redundancy.
This means that hosts and datacenters can sell storage over the Sia network without having a brand name. They don’t need to be a trusted or an audited entity because the specific requirements over their reliability are a lot lower. You don’t need legal SLAs because the blockchain file contracts handle that for you. And you don’t need a marketing arm or business development arm because you are able to easily announce yourself as a host and then compete over an open marketplace. Computers, not humans, are deciding who the best hosts are, and that means that it’s much easier to consider a large number of highly diverse offerings.
Finally, this open market place means that there is absolutely brutal competition. If you aren’t bringing something special to the table, you won’t be able to compete because the competitive hosts are all bringing something special to the table. And as the overall cost of storage gets cheaper, the marketplace automatically, instantly, and mercilessly follows the decline in price, because the new hosts that are able to offer storage at the lower prices will swallow the old hosts that are unable to keep up.
9. Why do you need your own blockchain? Why can’t you use something like the 21.co marketplace?
There are multiple reasons why a separate blockchain makes sense. But the biggest is that Sia’s file contract is very efficient and simple, and it can’t be replicated on the bitcoin blockchain. Sia’s file contracts lock up money (including collateral from the host) for long periods of time, and release the money contingent on the host providing a proof-of-storage. I do not know of another platform with a similar model.
Sia’s file contract has a host prove that they are still storing a file. After the storage proof is provided, the funds are released. The file contract itself contains a Merkle root of the data. The host proves that they are still storing the full data by providing a single 64 byte segment selected randomly from the file and writing a Merkle proof that this 64 byte segment is in fact part of the file.
The random segment is selected by the blockchain, and is derived from the hash of the block immediately preceding the first block where the host is allowed to submit the storage proof. Bitcoin currently has no way for the script to specify the block hash, and even if it did converting the block hash into a random segment indexed from a finitely-sized file is going to be a complex script. And then performing verification of the Merkle proof is another thing that’s not very easily done in Bitcoin script. Maybe we could have done something in Rootstock, except that Sia exists and is usable today, and Rootstock is not.
But there are other issues as well. If we put Sia on the Bitcoin blockchain, we have to compete with the whole bitcoin blockchain for space. Bitcoin fees today are something like $0.20 per transaction, and between the formation of 30 file contracts and the submission of 30 storage proofs for those file contracts (storage proofs being closer to 1kb each for reasonably sized files) we’re talking $10-$20 just to get set up. Sia’s traffic is exclusive to Sia, which means that we don’t have to deal with those levels of fees until we’ve already bootstrapped the network.
In general by using our own blockchain we were able to add a lot of customizations that fit our needs specifically. We don’t particularly subscribe to the idea that there can only be one true blockchain, and we think that the scalability is going to make more sense in the long run if you have different blockchains serving different needs. This of course comes with security tradeoffs, but those are discussed in question 1.
Decentralized storage — Sia, Skynet, and cryptocurrency.
134 
4
134 claps
134 
4
Written by
Decentralized cloud storage. Store your data securely in the cloud without the need to trust any central service. Download at: https://sia.tech/
Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by
Decentralized cloud storage. Store your data securely in the cloud without the need to trust any central service. Download at: https://sia.tech/
Decentralized storage — Sia, Skynet, and cryptocurrency.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ipfscloud/ipfscloud-a-decentralised-cloud-storage-platform-caf000173cd4?source=search_post---------42,"There are currently no responses for this story.
Be the first to respond.
Receive curated Web 3.0 content like this with a summary every day via WhatsApp, Telegram, Discord, or Email.
simpleaswater.com
As the world is moving towards decentralization revolution, a lot of apps(or dapps) are developed every day. A true dapp needs a truely decentralized infrastructure which includes a decentralized content distribution network. A lot of projects are working to create sustainable decentralized content distribution network as we have seen in the below post.
hackernoon.com
One of them is IPFS. I have been working with it for quite a while and recently I decided to create a simple dapp using it.
Let me introduce you to the IpfsCloud. In short, it’s google drive on IPFS.
Here is the link to the github repo: https://github.com/vasa-develop/ipfscloud
Honestly saying I am not able to decide what things should I add to this project. So please check it out and add your suggestions in the comments…
Thanks for reading;)
About the Author
Vaibhav Saini is a Co-Founder of TowardsBlockchain, an MIT Cambridge Innovation Center incubated startup.
He works as Senior blockchain developer and has worked on several blockchain platforms including Ethereum, Quorum, EOS, Nano, Hashgraph, IOTA etc.
He is currently a sophomore at IIT Delhi.
Hold down the clap button if you liked the content! It helps me gain exposure .
Want to learn more? Checkout my previous articles.
medium.com
medium.com
hackernoon.com
hackernoon.com
Clap 50 times and follow me on Twitter: @vasa_develop
An ecosystem, providing Apps, Infrastructure, and Tools…
248 
248 claps
248 
An ecosystem, providing Apps, Infrastructure, and Tools enabling Decentralization.
Written by
Entrepreneur | Co-founder, TowardsBlockChain, an MIT CIC incubated startup | SimpleAsWater, YC 19 | Speaker | https://vaibhavsaini.com
An ecosystem, providing Apps, Infrastructure, and Tools enabling Decentralization.
"
https://medium.com/@partechpartners/why-deep-tech-and-infrastructure-are-sexy-gain-cb11a3290514?source=search_post---------381,"Sign in
There are currently no responses for this story.
Be the first to respond.
Partech
Mar 19, 2019·10 min read
“The Future of Computing” seminar organized by Partech, with Accenture and Microsoft, brought together 70 key members of our portfolio companies and significant figures in the computing industry. The event, which was hosted at the Partech Shaker, saw several key speakers share their insights on the future of deep tech and infrastructure as well as how their specific roles and revolutionary products are changing the industry forever.
Reza Malekzadeh, General Partner in our San Francisco office, began the morning by explaining the increasing opportunity and need for new hardware and deep tech within the world of computing. As the industry becomes ever more interesting and exciting, Reza touched on how “infrastructure and deep tech are finally sexy again”, before briefly introducing the following speakers.
Marc Bousquet, Managing Director and Technology Lead France at Accenture, gave a brief overview of the digital transformation that continues to shake the computing industry and its consequence in modern society. He underlined his day to day relationship with start-ups and the remarkable ways that they push the boundaries of conventional tech. However, Marc stressed a need for a shift in focus to infrastructure rather than the ever-popular building and design of applications.
He continued this by underlining examples of the progressive use of deep tech in the digital world. Firstly, he pointed to how Google and New Balance teamed up during the fashion week in New York to screen the people around the venue to analyse their style, their shoes and consequently define a trend. Furthermore, in a partnership between Accenture and Airbus, they were able to use glasses complete with software that enabled them to know where and how to put in the seats, therefore making the manufacturing process of a plane easier. He shared his excitement for a start-up that looks at a profile on social media and, using AI technology to screen an individual’s photos and habits, gives them a credit score regardless of any prior banking knowledge. Marc concluded by highlighting the new necessity to transform our way of doing and our culture by leveraging our technical skills, confirming how digital transformation is beginning to lead cultural transformation.
Nader Salessi, CEO of NGD Systems, talked about how he and his team have set out to create a “paradigm shift in computational storage”. He explained the forces driving this new movement towards computational storage, identifying principally the continuous growth of data and the physical space and energy required to store it all. “Data is the new oil”, stated Nader, as he underlined the need for a new generation of storage devices. NGD Systems provides this alternative by offering a computational storage solution that implements AI machinery and other deep tech software. Nader mentioned the possible use cases of the product, underlining how it could work in hyperscale data centers used by Microsoft Azure and Amazon Web Services for example, by helping to decrease energy costs and physical footprint while maintaining a high level of functionality. By creating a storage device that can compute and store both simultaneously and locally, they can decrease “the time it takes to process data by 6x, and the energy consumed by 3x”.
Additionally, he underlined how it could be implemented in Modern Distributed “Intelligence Edge”, both acting and storing data locally. The product that they have built has an incredibly high functionality and can incorporate in-storage image classification and image similarity search as well as many other types of AI software. Their “unique, innovative and disruptive technology”, Nader concluded, allows any business to scale up without data storage issues. Also, they have managed to prevent CDN Encryption bottlenecks by providing a computational storage system that decreases the CPU and the user’s TCO as well as lowering the latency.
Benjamin Schilz, Co-Founder of Acorus Networks, began his presentation by underlining how his company has changed the archaic nature of DDoS (Distributed Denial of Service) mitigation. DDoS attacks occur around 10 million times a year, with the aim “to block a system, web service or company network” stated Benjamin. All online services are at risk, from “ecommerce to SaaS” with the attacks becoming increasingly easier to launch and being even more sophisticated and potent. Benjamin used the recent example of a series of DDoS attacks that brought down the whole of Sweden’s train network by targeting their signalling system.
Benjamin underlined how Acorus rethinks the way that we protect these services. His company has taken outdated forms of mitigation, originally done using onsite appliances, in a similar fashion to firewall deployment, and built a worldwide network of scrubbing centers that process the data as close to the source as possible. With new undetectable probe techniques appearing in attacks, strategically timed so they do not appear on graph data, Acorus felt the necessity to build a “performance and security centric” network that protects a company “no matter where”. Benjamin concluded by saying that a business should expect “the best service during an attack, not the best effort”. To combat this, Acorus are “moving DDoS mitigation to the Edge era” with “ongoing deployment” as close to the source as it can get.
Mario Trentini, R&D Director of Product at Mipsology, was next to talk about his team’s product Zebra, a computation for Deep Learning. The French startup was founded four years ago and designs high-performance Deep Learning solutions using FPGAs. Mario began by highlighting the significant growth of machine learning in the last few years. “There are two main steps to machine learning,’’ continued Mario, firstly “Training (or learning)”, involving a data scientist creating neural networks with the ability to learn from existing data, and secondly “inference”, using the results of the training to answer a question. With more complex neural networks being created, powerful CPUs and GPUs are needed to meet their targets for program execution. Mario discussed the decision process behind the choice of using FPGAs to run their Machine Learning software, underlining how they have managed to mask the complexity and design difficulty behind programming them yet still managed to bring the “high performance and flexibility” that makes them stand out from other processing units.
Zebra can fit anywhere: “in data centers, Edge or embedded”. Mario underlined that alongside this universality, it is also incredibly easy to use. It is “fully integrated for DL inference computing” and works with all neural networks. He pointed out several use cases surrounding video and images, demonstrating its practicality in object detection and segmentation. Mario closed his presentation by showing how their performance is increasing fourfold each year, bringing with it a lower price and recurring cost.
Guillaume Delaporte, Co-founder and VP of Customer Success at OpenIO, began by presenting their next gen object storage for cloud and on-premise systems. Emphasizing the need for this, Guillaume referred to the continuously increasing amount of data being produced each year, “applications come and go but data persists”. He continued by explaining that the storage market is split in two, “latency focused” and “capacity focused”, but how there is never any balance between them. “Traditional data solutions cannot keep up”, Guillaume added. He brought up issues such as expensive proprietary hardware and software, a lack of scalability and vendor lock-in.
OpenIo brings a flexible and smart solution to object storage, focusing on bringing clients the ability “to deploy technology in different continents” and “to scale quickly”. “Don’t let storage stunt your growth”, Guillaume remarked. OpenIo have changed conventional storage by building a conscience System (with Dynamic chunks dispatcher). The system, Guillaume explained, allows for Real Time load balancing, a process in which it computes quality scores for each disk and selects the one with the best score, effectively managing storage policy. This process allows for “optimal positioning of the data, flexible scale out/up and high performance”. He underlined how they are also building data platforms to last, around 5–7 years, with good control of your data retention, keeping and protecting important data while automatically deleting trash. Their software “offers an optimized TCO with linear performance growth and continued performance with scale”. Guillaume highlighted the multiple use cases of their products, pointing to a success story with Dailymotion in which they displaced EMC as their storage provider and reduced their TCO by 50%. He concluded by noting the open source nature of their solution and their continually growing customer base of over 30 large clients.
Next to talk was Frédéric Plais, CEO of Platform.sh. He was proud to share their vision for “the grand reunification” of software production. Frédéric explained that “every company is now a software company” and as a consequence you are “expected to deliver software or you will get disrupted by people that do”. However, Frédéric underlined the difficulty behind writing good software, pointing to the problem of staging, merely “a pale copy of production”, and the frequent bottlenecking and expensive cost of the process. “Teams are getting better”, he continued, with “agile methodologies… ways of writing software have become easier”. This led him to share how Platform.sh have unified the development, testing and production of software.
“Platform.sh was built on the idea that the application comes first”, stated Frédéric. He underlined how increasingly nowadays, all the care is about the features of software and the UI but not what is going on behind-the-scenes. Within two years, Platform.sh “were able to clone an application in less than 30 seconds with perfect replication”. Consequently, Frédéric highlighted how the staging process has now become very cheap while each GIT branch still gets their own staging environment. This has led to the “ability to test and deploy with no bottleneck”. Their software brings up to 40% more efficiency for development teams when building an app or a new feature for it. Frédéric stressed how this “leaves more time to spend on the application”, more importantly, “time on what matters”. He concluded by demonstrating a few of their use cases. Their “managed production runtime” has led to companies such as Johnson & Johnson and Arsenal Football Club using their service to deploy with no downtime when it matters the most.
Last to speak was Bernard Ourghanlian, CTO and CSO of Microsoft in Europe. Bernard began the talk by sharing a quote from Marc Weiser who is widely considered the “father of ubiquitous computing”: “The most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinguishable from it”. “Good technology is supposed to disappear”, underlined Bernard as he shared how Microsoft are trying to bring storage closer to the Edge. Referring to a study by Gartner, he highlighted how new IoT technology will decrease data center storage by 10% and how we now need to therefore build a more intelligent and distributed environment.
Microsoft are investing more and more in a global infrastructure, with an aim to put a data center in the ocean, near everyone and with the lowest latency possible. Marc stressed how he and his team “are trying to embrace Cloud and Edge at all levels”, particularly incorporating it with Azure. All of this is with the goal of worldwide vertical integration, “from Edge to Cloud and down”, and “from the smallest device to the largest data center”. The three ideas driving this are “people-centered experience, AI and Ubiquitous Computing” stated Bernard. Sharing more on the topic of AI, he pointed out how Microsoft first implemented AI over 20 years ago to sort span and now aim to “run it everywhere and democratise its use by providing all the building blocks that are needed to execute it at scale”.
Bernard stated that “the idea of having a smartphone to do everything is dumb”. “Devices and software come and go within a single year”, he continued, “the individual should therefore be at the center, not the device”. With our level of interaction with devices getting higher every day, Bernard concluded by stressing the importance of the emerging application patterns of serverless software, artificial intelligence and multi device practicality.
To conclude
The key speakers received a number of questions from the audience. Most of them pertaining to their business models and how they are able to distinguish their product or service from existing heavy weights and market leaders. To learn more about Partech portfolio companies, have a look at Partech’s website.
And the final words go to Reza Malekzadeh:
For the longest time, the world standardized on one of two processor types (x86 or ARM), one of two operating systems (Linux or Windows), one of two relational databases (Oracle or SQL), and so on. But the rise of cloud, new distributed architectures as well as IOT are allowing a new generation of infrastructure and datacenter technologies. The scale of disruption in the technology infra-structure landscape is unprecedented, creating huge opportunities and risks for industry players and their customers. All of these ambitious innovations will require more capital and capacity, but customers in the new IT infrastructure landscape will reward their efforts. Exciting times to be investing in deep technology!
Hungry for more?
You can read Reza’s blogpost on cloud computing.
Any comments? Tell us below or join the conversation on Twitter, and stay up to date with upcoming news.
A global investment platform for tech and digital companies with offices in #SanFrancisco, #Paris, #Berlin & #Dakar
See all (3,912)
257 
257 claps
257 
A global investment platform for tech and digital companies with offices in #SanFrancisco, #Paris, #Berlin & #Dakar
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/sonm/sonm-announces-partnership-with-decentralized-cloud-storage-service-storj-211b6402e812?source=search_post---------17,"There are currently no responses for this story.
Be the first to respond.
Here at SONM, things are really switching into high gear, and this week a major step was taken towards the completion of an important segment of our development objectives.
As was mentioned in the updated version of the roadmap, the SONM infrastructure requires something called “object storage.” Recently, the development team determined that STORJ, the decentralized object storage platform, will serve this purpose nicely, and meets all the requirements to be integrated into SONM.
This week, the teams at SONM and STORJ signed a technical agreement, and both teams are ready to announce their strategic cooperation. This partnership will enable workers and end-buyers to interact and share files directly on the SONM platform.
Together, SONM and STORJ are set to deliver to the community a groundbreaking new type of decentralized, peer-to-peer, service.
SONM (Supercomputer Organized by Network Mining) is a decentralized worldwide fog supercomputer for general purpose computing that ranges from site hosting to scientific calculations. SONM includes an internal marketplace for computing powers.
The really killer feature of SONM is the decentralized and open-source structure. Compared to other cloud services like Amazon, Microsoft, and Google, SONM enables its customers to interact directly with each other on a peer-to-peer basis, thereby creating a fair market of computing powers.
The agreement that SONM has signed with STORJ will enable the uploading and downloading of computing tasks on the system, making the tasks accessible both for the buyers and the workers.
SONM CTO, Igor Lebedev, commented on the integration:
“SONM — STORJ integration will allow SONM consumers to mount data volumes and collect data from containers using STORJ. I think that STORJ is really a stable and easy-to-use solution for our customers, enabling the peer-to-peer interaction that is so critical for SONM.”
STORJ is a peer-to-peer, distributed cloud storage network which implements client-side encryption. This allows users to transfer and share data without being reliant on a third party storage provider. Coming into a partnership with SONM, STORJ cooperates with a peer-to-peer marketplace of computational powers. In the future, SONM is likely to process larger amounts of data and will therefore become a crucial partner for the storage company.
We at SONM are always paying attention to what is happening in the cryptocommunity, and we look forward to frequent cooperation with other innovative companies, like STORJ, in the future. In the meantime, we invite interested individuals to keep posted on SONM development achievements here on the blog and on our other social media channels.
Our website: https://sonm.ioTelegram: https://t.me/sonm_engSlack: https://slack.sonm.io
SONM is a global fog computing platform for general purpose…
759 
3
759 claps
759 
3
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Written by
Global Fog Computing Platform
SONM is a global fog computing platform for general purpose computing from website hosting to scientific calculations. SONM company is an effective way to solve a worldwide problem - creating a multi-purpose decentralized computational power market.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/mediafire/get-the-most-out-of-your-cloud-storage-4dd8dea28f27?source=search_post---------183,"There are currently no responses for this story.
Be the first to respond.
At MediaFire, we’re passionate about online storage. Whether you’re looking to share a huge file online or backup your files to the cloud, MediaFire makes it simple to get the job done.
What you might not know is that MediaFire has several advanced features that can make your life even easier. We’ve put together a list of our top recommendations for getting the most out of your cloud storage.
1. Download our apps to get more free space
MediaFire gives you 11GB of free space, as soon as you sign up, but did you know that you can get and additional 4 gigs of space just for connecting and posting to your twitter and Facebook accounts? You can also receive an extra 4 gigs by downloading and installing our desktop and mobile applications, and can fully max out your free storage (to 50 gigs) by getting your friends to sign up.
To track your free storage progress, visit https://www.mediafire.com/earnspace/
2. Automatically sync your music and videos to the cloud using MediaFire Desktop
Available for OSX and Windows, MediaFire Desktop makes it easy to back up and sync your music and movies to the cloud, so that you can access them from anywhere through any device.
The simplest way to get all of your media into the cloud is through moving your existing iTunes library into your MediaFire Desktop shared folder. This will enable you to automatically sync your iTunes playlists, movies, and music between several computers (at home and at work for example), and will insure that all of your media is automatically protected in the cloud.
To do this, you’ll need to move your library into the MediaFire Desktop folder, then point iTunes towards its new location — hold down the Shift key when launching it on Windows or the Alt/Option key on a Mac to do this. Once your library has uploaded (we recommend setting your computer to do this overnight), you’re ready to set it up on another computer.
Need to use the internet while you are syncing media? MediaFire Desktop uses a smart-uploading speed detector, which will automatically slow down or speed up uploads and downloads based on your bandwidth use.
3. Instantly recover deleted files
Did you accidently delete an important file? Unlike other cloud storage services, files that are deleted on MediaFire are automatically moved to your “trash” folder, where they can be accessed and recovered at any time. To restore a deleted file, login to our web interface and click on the “trash” folder. You can restore individual files, or click the “restore all” menu item to bring any folder back.
4. Share and collaborate on files with friends
MediaFire’s “shared folders” feature makes it east for you to collaborate and share files with friends. To use this powerful feature, when someone sends you a file using MediaFire, simply click the “Follow” button in the top corner. The files will instantly be added to your “following” list in your MediaFire home folder. If you have MediaFire Desktop, any files that you are “following” will be automatically downloaded and synced to your desktop.
5. Easily share large files and folders
MediaFire was originally developed back in 2006 as a simple solution for sharing large files online. We’ve recently added several new features that make sharing files with others online easier than ever, even if the person you’re sharing with doesn’t have a MediaFire account.
With MediaFire Desktop installed, you can you can share any file or folder located in your “MediaFire” folder (located in your favorites list) by right-clicking it and choosing “Share through MediaFire”. This will bring up a menu detailing the different file sharing options available, including sharing through Facebook, Twitter, Google Plus, Pinterest, Tumblr, and even Blogger. You can also add a contact’s name or email address directly to add them as a “follower” to a file, and can even generate a one-time download link for sensitive files.
6. Sync files selectively to your desktop from the cloud
When you first install MediaFire Desktop, you are able to select which files you would like to sync locally to your computer. However, if you change your mind, you can always add or remove which files are synced through the options menu in MediaFire Desktop. Simply select the “sync folder” option in settings to update your selected files.
7. Follow MediaFire on Twitter and Facebook for the latest app and feature news
At MediaFire, we are always looking to launch new and exciting features and apps. We have some exciting features planned over the next year, including automatic photo and video syncing, and several new “top secret” applications and services designed to make your life easier. Make sure to follow us at @MediaFire on Twitter and MediaFire on Facebook for the latest and greatest news!
This is the blog of MediaFire.com
Written by
MediaFire stores all your media and makes it available to you, anytime you want, anywhere you go, on any device you might have.
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
Written by
MediaFire stores all your media and makes it available to you, anytime you want, anywhere you go, on any device you might have.
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/new-gcp-essentials-video-gcp-vs-firebase-part-2-44bb09a20ff3?source=search_post---------253,"There are currently no responses for this story.
Be the first to respond.
In the previous post we discussed how a Firebase project is actually a GCP project under the hood and that data stored in Cloud Storage can be equally accessed via Firebase-powered mobile and web clients as well as from your GCP-hosted applications.
In this post we’ll explore two more products: Cloud Functions and Cloud Firestore :
Cloud Functions
Cloud Functions is really a Google Cloud product, which also happens to be visible from Firebase.
Cloud Functions is a popular implementation of the Function-as-a-Service paradigm where an event in the system (such as an HTTP call or a file uploaded to a bucket) triggers the execution of a small piece of code — a function. Check out the “Serverless Overview” episode for more details on Cloud Functions.
An important architectural difference between the Firebase and GCP approach to Cloud Functions is that Firebase will offer to wrap your functions into callable functions. Such functions can be invoked via the Firebase SDK with user tokens and device instance IDs propagated directly to the function.
Another important difference is the languages supported :
When using Firestore, Node.JS and TypeScript are supported while Go, Python, and Java are also available when using GCP directly.
Before we compare web consoles you can also use command-line interfaces: gcloud or the Firebase CLI. Both come with their own set of functionalities and we recommended that you decide which one works best for you and stick to it.
To deploy functions with Firebase you’ll need to install Firebase command line tools using npm, as deploying from the Firebase console is not possible. You are however able to use an API for strongly-typed handling of events and with the ability to deploy multiple functions at once.
In GCP land you can deploy functions straight from the console, integrating with source repositories or even typing the function source code inline.
A function created within a Firebase project can also be managed using Cloud Console. “Detailed usage stats” in Firebase will take you to that same function, only in Cloud Console where you’ll get additional features such as monitoring graphs, a tab to test the function, and the ability to set features such as retry-on-failure, memory allocation, or timeouts.
Cloud Firestore
Cloud Firestore is Google’s state-of-the-art NoSQL document database. It is schemaless and can store documents containing attributes in a hierarchy of collections. It’s fast, it supports transactions, it’s highly available, it’s strongly consistent, and it does not require managing any kind of infrastructure.
The Native mode of Cloud Firestore can automatically scale to millions of concurrent clients but what makes it even more special is the fact that it offers near-real-time notifications. This is what enables synchronization of data across devices. With the built-in offline support, you can access and make changes to your data, and those changes will be synced to the cloud when the client comes back online.
Data stored in Cloud Firestore can be accessed using Firebase SDKs and of course you can use either console to view and edit data and indexes, and to monitor database access usage.
With Cloud Firestore you can query the database directly from your mobile or web clients without the need to set up an intermediary server to manage access to your data. This is why the Firebase console has an additional tab for security access Rules.
These rules can range from fairly simple to quite sophisticated and typically use a combination of the user’s identity, the new document content (data sent), and the existing data stored in Firestore. If you are a Firebase user shipping an app that accesses Firestore, then you really should use Firebase Authentication and think carefully about your security access rules.
On the GCP side, there are no equivalent security rules as you typically access Firestore using a service account. Generally speaking, server-side code is considered trusted as opposed to client code from mobile apps.
Finally, supported Firestore SDK languages include Python, Node.js, Java, C# (.NET), Go, PHP, and Ruby with Firebase mobile SDKs adding Web, Android, and iOS support. These mobile Firebase SDKs include local caching as a unique feature to help implement the offline capability mentioned earlier.
So, whether you’re a GCP developer interested in building mobile and web apps with Firebase or a Firebase user trying to grow your usage of GCP services, you should now have a good sense of what’s possible.
Check out also Doug’s posts on these topics :
Google Cloud community articles and blogs
52 
52 claps
52 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/get-a-single-one-csv-file-with-bigquery-export-956d2a147886?source=search_post---------384,"There are currently no responses for this story.
Be the first to respond.
Distributed computing is the key to process big data at scale. All data processing systems use clusters of VMs: Hadoop, Dataflow, and, of course, BigQuery.Dozens or hundreds of processes can run at the same time and perform parallel operations to speed up the process.
The BigQuery export statement is a great example: with a SQL statement, you can spawn hundreds of slots (i.e. slice of VM) to export the data from BigQuery. But because the export is performed in parallel by hundreds of slots, the export generates hundreds of files.
I already described how to leverage BigQuery to handle, merge, clean CSV files with the tradeoff of several CSV files as output; Or how to export the data from BigQuery and load them in Cloud SQL, with an iteration through all the exported CSV files by BigQuery. And an issue has been open on my GitHub repository with a great optimisation proposition:
Instead of importing a lot of small files into Cloud SQL, why not merging all the files and importing only one in Cloud SQL?
How to export data from BigQuery in a single one CSV File?
Let’s solve that challenge
Cloud Storage stores binary object (Blob) of any type. When a export is performed by BigQuery, Cloud Storage is used to stored those exported files.
Cloud Storage is very efficient (and affordable) to store blobs, but its processing capacities are limited:
That’s roughly all!
The latest bullet points, Compose, isn’t well known because it’s not available through the console. It allows you to compose (to append) up to 32 files into a single one.
But this feature is limited to 32 files per API call, and when you have hundreds of files, you need to loop over that command.
How to automatically loop and compose hundreds of files?
Cloud Workflow allows to declare through a YAML file a sequence of API calls with controls on the flows (iteration, conditions, retries,…).
Therefore a Workflow is perfect to browse all the exported files by BigQuery and to append them in a single one CSV file.
Let’s define that workflow!
The first step is to query BigQuery with the export statement. The format used is CSV, because it’s easy to merge CSV by appending the files (without headers).
The BigQuery job query connector is used to perform that query.The service account that runs the Workflow must have the role “BigQuery data viewer” on the data to query and “BigQuery Job User” on the project to be able to run a query job.
The header=false is important to avoid header addition in each file, and then issue when appending files.
Cloud Storage is the core service of that workflow. Many different APIs, and features, are used.
In that section, the service account that runs the Workflow must have the Storage Object Admin (viewer + creator), to be allowed to perform the following operations.
As output, I want a new file with all the exported data in it. I don’t want to change the file generated by BigQuery, to keep an integrity in the BigQuery process, and to re-run the compose operation in case of issue.
So, the idea is to create an initial file in which all the other CSVs will be appended. For now (December 2021), there is an issue in the connector, the team is on it. Therefore, I use the Cloud Storage JSON API directly
That file can be empty (my case, body: is empty) but you can add a header manually if you need a header in your use case.
The other usage of Cloud Storage is the file listing to get all the exported files. The good news is that the BigQuery export statement requires a Cloud Storage file prefix to store the exported file.Cloud Storage can only filter on file prefix.
We simply have to reuse the same one!
Let’s use the Cloud Storage object list connector for that
I will discuss the maxResults and the pageToken parameter, in the optimization section
Now that we have listed all the files, we need to iterate over them and when we have accumulate 32 files (the Compose limit), or reach the end of the file list, we can run a Cloud Storage compose operation.
The iteration is based on the list of files returned by the Cloud Storage list object. We append the file names in a list, and when the list reaches 32 elements, a compose operation is triggered.
The list isn’t empty, the first file is the desired output file, all the other files are appended to it.
The last and the most important, the compose operation. Here, we are using the connector and appending all the files in the destinationObject.
As you can see, the destinationObject is “special”. We must transform it to avoid incompatible characters. Here the / is replaced by %2F, the URL encoding equivalent. You could have a similar replacement to perform with space character or <>.The team is working on new encoding functions to do that automatically.
You can find the full code in my GitHub repository and follow the instructions in the README.md file to deploy and run the workflow.
In the workflow YAML definition, I included some optimization and issue fixes.
Firstly, when the process is too quick between the end of the BigQuery export and the compose operation, the files aren’t found on Cloud Storage (I don’t know why). I added a sleep timer of 1 second to fix that latency issue
Then to optimize (i.e. minimize) the number of calls to the Compose API, I list the objects by multiple of 31. Why 31? Because the limit is 32 files to compose, and we have to take the final output file in consideration. Therefore only 31 new files to compose with the final output file.
But why 62? Not 93, 124 or more? Here comes a limit of Workflow. There is a limit on the variable size. Today it’s 64kb, soon 256Kb, but you can’t store too many results in a variable without crashing your execution.
The latest issue is the prevention of too much iteration depth. if you recursively call a sub-workflow, you can get an execution RecursionError.The solution is to perform the check and the iteration outside the sub-workflow itself and to call it again until the end of the recursion (still pages to process)
Cloud Workflow is a wonderful tool to easily orchestrate API calls. You can combine products without having code to deploy, applications to write or complex architecture to set up. You can solve challenges that previously took much more effort to achieve.
The product is not yet perfect, and several developer features are still in the roadmap, but it already offers elegant solutions, it evolves very quickly and it is already Generally Available (GA). Have a try on it!
Google Cloud community articles and blogs
16 
16 claps
16 
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jasonbyrne/if-you-go-into-the-regular-google-cloud-storage-console-like-this-https-console-cloud-google-com-7f27150f4fd?source=search_post---------168,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jason Byrne
Jun 1, 2017·1 min read
Jerry Gag
If you go into the regular Google Cloud Storage Console like this:https://console.cloud.google.com/storage/browser
It will list all of your buckets and there is a vertical … icon in the right side. When you click that a menu drops down to edit bucket permissions. I think Google has changed some things since I wrote this though because I no longer see “Edit object default permissions” in that dropdown.
CTO @ Echelon Fitness. Founder of MileSplit. Entrepreneur, technology executive, historian, writer, Christian, family man, and track & field fan.
See all (195)
CTO @ Echelon Fitness. Founder of MileSplit. Entrepreneur, technology executive, historian, writer, Christian, family man, and track & field fan.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/future-vision/the-birth-of-an-object-on-aws-813124fa7a80?source=search_post---------350,"There are currently no responses for this story.
Be the first to respond.
Just like users, objects are first class citizens in the AWS world. They’re given a name at birth, a tag to identify any specific ethnicity (groups in AWS), they have different versions of themselves, and they have a life with different phases which they can transition through with time!
In this article, we’ll talk about Naming, Tagging, and Versioning, and we’ll talk about Lifecycle…
"
https://medium.com/@tiwari_nitish/enterprise-grade-cloud-storage-with-nginx-plus-and-minio-e708fb4cb3e8?source=search_post---------81,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
Jan 30, 2017·5 min read
In this post, I explain how to use NGINX and NGINX Plus as a reverse proxy and load balancer for Minio servers.
Almost all applications need storage, but different apps need and use storage in particular ways. Take for example, a document store: it might not need to serve frequent read requests when small, but needs to scale as time progresses. Another application, such as an image gallery, needs to both satisfy requests quickly and scale with time.
These nuances make storage setup tough. However, everything is not gloomy — with the advent of object storage as the default way to store unstructured data, HTTP has become the default communication mode, standardizing how applications communicate with storage.
Still, the question remains: How do you build an object storage setup that is tailored for your application requirements, but still flexible?
Because object storage involves HTTP servers and clients, it makes sense to have a versatile web server like NGINX Plus in front to handle HTTP traffic. A lightweight object storage server such as Minio can be used to provide scalable storage at the backend. The flexibility of such a system is the key to creating an enterprise‑grade service.
With NGINX Plus, administrators can not only load balance incoming traffic — they can cache, throttle, terminate SSL/TLS, and even filter the traffic based on various parameters. Minio, on the other hand, offers a lightweight object storage server that is compatible with Amazon S3.
Minio is best suited for storing unstructured data such as photos, videos, log files, backups, and VM and container images. Minio server is light enough to be bundled with the application stack, similar to Node.js, Redis, and MySQL. Minio also supports a distributed mode, letting you pool multiple drives — even on different machines — into a single object storage server.
In this post we’ll explore some of the features of NGINX Plus in various use cases and learn how to combine them with Minio to set up a production‑grade, highly scalable, highly available, and stable object storage system.
NGINX Plus is well known as a reverse proxy server. But why does one need a reverse proxy for Minio? Let’s look at some of the use cases:
NGINX Plus reverse proxies client traffic by passing requests to a backend server, which is listening on the URL specified by the proxy_pass directive. In the following configuration snippet, a standalone Minio instance is running on localhost, so it’s available at http://localhost:9000. All requests coming on port 80 to the top‑level directory (/) at www.example.com are passed to Minio. NGINX Plus explicitly sets the Host header to its value in the original request.
If you have multiple Minio servers, load balance traffic among them by listing them in an upstream configuration block and referencing the upstream group in the proxy_pass directive:
For more details about setting up NGINX or NGINX Plus as a proxy for Minio, see the Minio documentation.
With HTTPS now becoming the default protocol for much web traffic, it makes sense to deploy an HTTPS server, rather than simply an HTTP server, for Minio. It’s fairly easy to set up NGINX Plus as an HTTPS server. You need a SSL/TLS certificate to get started, Let’s Encrypt provides free SSL/TLS certificates and integrates with NGINX Plus.
The next step is to edit the NGINX Plus configuration file. Here you need to specify the ssl parameter to the listen directive in the server block, then specify the files containing the server certificate and private key:
For more information about SSL/TLS termination, see the NGINX Plus Admin Guide.
Object storage servers are not known for their speed, but that doesn’t have to mean slow responses to clients. When you enable caching on the NGINX Plus server, it saves frequently accessed data which it can return to the client immediately without having to forward the request to the backend server.
Here is how it works. The NGINX Plus web cache sits in between a client and Minio, saving a copy of each requested content file. When a client requests content that is stored in the cache, NGINX returns it directly, without contacting Minio. This both improves response time to the client and reduces the load on the Minio server.
You set up the NGINX Plus cache for Minio with the proxy_cache_path and proxy_cache directives. The proxy_cache_path directive sets the location and configuration of the cache, and the proxy_cache directive activates it. For details, see A Guide to Caching with NGINX and NGINX Plus.
Sometimes you need to throttle requests for business or security reasons. With NGINX Plus, you can limit the available bandwidth, number of requests, or number of connections.
To throttle bandwidth, use the limit_rate directive. This example limits download speed to 200 KB per second:
For request throttling, use the limit_req and limit_req_zone directives, as in this example which limits each unique IP address to 10 requests per second while allowing for bursts of 20 requests.
To throttle the number of connections, use the limit_conn and limit_conn_zone directives. This example limits each unique IP address to 5 simultaneous connections.
For more details, see the NGINX Plus Admin Guide.
In this post we demonstrated the use of several NGINX Plus features for load balancing in — particular, for load balancing in front of a Minio object storage server. The combination of NGINX Plus and Minio allows you to set up a flexible object storage server tailored for your application requirements.
This post was originally published on Nginx Blog.
We hangout on Slack, meet us here: slack.minio.io
While you’re at it, help us understand your use case and how we can help you better! Fill out our best of Minio deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
See all (472)
19 
1
19 claps
19 
1
Minio.io
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/exporting-data-from-storage-to-memorystore-using-cloud-dataflow-5d37287139e7?source=search_post---------366,"There are currently no responses for this story.
Be the first to respond.
Recently, I got a chance to do an R&D on a requirement where I would need to read files stored in a Cloud Storage bucket, which would be processed and transformed in the desired format and stored in an in-memory data store, i.e., Memorystore for faster access. Well, honestly it took several days to figure out the correct approach before finding the correct technologies to implement this.
"
https://medium.com/@atul-sharma-94062/how-to-use-cloud-firestore-with-flutter-e6f9e8821b27?source=search_post---------329,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Atul Sharma
Jun 20, 2019·5 min read
In one of my previous blogs i had discussed about using Firebase realtime database with flutter. Now Firebase comes up with a more advanced version of it as Cloud Firestore. I will show you step by steps how we can use Cloud Firestore in Flutter.
Cloud Firestore
Cloud Firestore is also a NoSQL Database like realtime database, but the difference is in the structure of data storage. Realtime database stores data in JSON tree. In Cloud firestore we are having Collection. A collection can have any number of Documents, these Documents contains data. Cloud Firestore scales automatically, its current scaling limit is around 1 million concurrent connections and approx. 10,000 writes operations /second.
https://console.firebase.google.com
2. Add two apps one for android and other for IOS
iOS
Android
On firebase console from the left menu select Database option to create Cloud Firestore database.
Select security rules for database, for our demo purpose let us select test mode, it ill make the public database, any one can read and write to this database.
2. Open ios/Runner.xcworkspace. Save GoogleService-info.plist in Runner folder
Similarly open Android Studio and put google-services.json file in the app module of android project.
3. In your IDE or editor, open the file pubspec.yaml. Add dependency for cloud_firestore and save the file.
4. From the project directory, run the following command.
flutter packages get
2. Create databaseReference object to work with database.
3. Create a screen with buttons for CRUD operations.
In Firestore, data resides in document. Collections contains these document. In our case we will create a collection named “books” and add documents to this collection, than add data in document.
1. On click of “Create Record” button, createRecord() method is invoked.
2. In createRecord(), we create two demo records in database.
when we use collection(“books”).document(“1”) a document is created in collection named “books” and using setData() we have created data in that document.
when we use collection(“books”) .add(), it has done the same but the document name is randomly generated by firestore as we have not specified.
In order to view the data, we have to retrieve all documents or particular document of that collection. For retrieving particular document we have to specify document id.
3. Output on console is
It updates description of document id 1 to ‘Head First Flutter’
2. It deletes document 1 from the collection named books.
See all (11)
1.7K 
8
1.7K claps
1.7K 
8
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@highwayman1991/cloud-computing-in-a-modern-world-distributed-cloud-computing-by-hivenet-f6f2f96294d5?source=search_post---------378,"Sign in
There are currently no responses for this story.
Be the first to respond.
Алексей Филатов
Sep 19, 2019·4 min read
The Internet and information technology play a great role in modern human’s life. Today we can observe a new era in the development of social media where we deal with a variety of electronic devices, big storage capacities and high speed of information response. There is an important innovative in this world of new technologies which is called cloud computing. It can bring absolutely new advantages to people from different professional groups.
Cloud computing represents a system which serves as an internet-based information centre. The customers can get an access to all kind of files and software safely through some different devices. This opportunity doesn’t depend on the location of customers: they can use it from any place on Earth where it possible to have an access to Wi-Fi.
Cloud computing is an excellent instrument for organizations and individuals seeking an easy way to store and access media from one device to another. It enables people to run software programs without installing and develop and test programs without necessarily having servers. And it has a wide list of advantages for business projects. Let’s consider them in a more detailed way.
There are several positive features in the usage of cloud computing in business. Here are some of them.
Global access. Thanks to a remote server, employees will be able to share files and calendars with ease. Such opportunities help to improve planning and time management within your company.
Comfortable data sharing. Thanks to a remote cluster of servers, you won’t get lost important information. If there will be mistakes with your concrete files or all of your data, it will be able to be sent to another device.
Economic solutions. Cloud computing allows you to avoid additional expenses for licensing fees, data storage costs, payments for software updates and salary for special managers.
Safety and security. Databases in cloud computing are difficult to penetrate for hackers. And developers strive to find new techniques to protect the system from hackers better.
Large storage. In comparison with a personal computer, cloud computing gives your company an opportunity to store much more information.
Device independence. You can get an access to cloud computing software everywhere and anywhere. You just need to have a device and an access to Wi-Fi.
HiveNet represents a Distributed Cloud Computing Network. It connects devices from different countries in order to perform valuable computing tasks. HiveNet helps customers and computer owners to come into contacts with each other.
HiveNet is responsible for task assignment and validation and fair payment distribution. Computer owners offer computing power to customers and receive payments from them. Let’s see what advantages different sides in this project have.
Computer owners. Usual people can earn during using computer. Some profit will be got for the fulfillment of simple tasks by your computer’s power. You just need a device with an internet connection. So, this is a real perspective way to get passive income that doesn’t require special abilities from you. And you can get profit even when you sleep.
Customers. HiveNet allows to use already available computers, so there is no need for companies to spend money on new devices, housing, etc. That’s why this instrument is much cheaper in comparison with usual cloud computing.
Crypto traders. People from this group will have nice short- and long-term opportunities thanks to using HiveCoin. These tokens are used for the payments for computing power within the HiveNet.
Nature. Thanks to the increase of the usage of available devices, there will be no need to build many new ones. That’s why there will be less electronic garbage. And it will be better for environment.
The project team consists of specialists in different fields. You can get to know more about them here.
Presale of tokens will be started on 24th September, so you will have a chance to purchase perspective HiveCoins for good value. You will be able to buy tokens for the cost 0.06 USD/HNT from 24th September to 7th October. Then the price will have risen gradually. With the start of IEO / public sale, tokens will be sold for the cost 0.08 USD/HNT.
Here are the links to resources where you will be able to know more about the project:
Website: https://www.hivenet.cloud/
White Paper and Light Paper: https://www.hivenet.cloud/resources
Facebook: https://www.facebook.com/HiveNetCloud
Twitter: https://twitter.com/HiveNetCloud
LinkedIn: https://www.linkedin.com/company/hivenetcloud
Medium: https://medium.com/official-hivenet-blog
Youtube: https://www.youtube.com/c/HiveNet
Telegram: https://t.me/hivenet
The article was written and published by Arcadio707 (https://bitcointalk.org/index.php?action=profile;u=2426003)
Crypto enthusiast
See all (1,369)
213 
213 claps
213 
Crypto enthusiast
About
Write
Help
Legal
Get the Medium app
"
https://blog.sia.tech/the-sia-alerts-system-be-more-informed-about-your-cloud-storage-9d101f62fe6e?source=search_post---------77,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Sia is a next-gen cloud storage platform that lets you store your files online with ultimate privacy, security, and speed. Learn more at sia.tech.
One of the new features coming in Sia v1.4.2 is the Alert System, a new set of tools that allow Sia to report irregularities during runtime — in other words, it will tell you when you have a problem. It will also tell you what the problem is, how severe the problem is, and (if Sia can figure this much out) what caused the problem.
Here’s a quick example to illustrate the point: Sia needs your wallet to be unlocked periodically so it can access your funds to perform maintenance like repairing files and renewing contracts. If it can’t do this, Sia might eventually lose access to the data you’ve uploaded because you’ve stopped paying for it. It’s an issue that me and my team have seen quite a few times in confused emails from users.
With alerts, Sia can now tell you that your wallet is locked and contracts can’t be renewed, instead of you finding out when your file health drops, or later.
If you’re into Gitlab and want to check out the birth of the Alert System, see core dev Chris’ first MR for it here.
Part of my job is to assist with and manage Sia’s support channels. Back in July of 2019, Sia lead dev David asked me to come up with a list of things that siad should be able to tell the user automatically. The list ended up being the types of things that we see a lot in support, and would be helpful for a user to get smacked in the face with instead of having to either figure out on their own, or email in so we can tell them.
Danger, one of our Sia support reps, and I ended up putting together a list of what we thought were good starters for this — the types of things that are good to know as soon as they happen, before they might become problems or emergencies.
The dev team took this as a starting point and began developing a series of alerts for different modules in Sia. This has evolved and expanded into a wide net of alerts that cover every part of Sia, from wallet to renting and everything in between. These alerts should allow you to be far more nimble when using Sia, and address issues before they expand into something that could impact your files or host.
Because Sia can now provide more insight into potential issues, this is something that can be expanded on by developers of apps that are Built With Sia.
Sia is decentralized, and we take that belief seriously. As a platform, you should remain in control of how you use Sia, what it does, and how it does it. This means that, while Sia might know there’s a problem, and can now tell you about that problem because of Alerts, there aren’t many times when it can fix the problem for you. Doing so could violate the trustless ethos that Sia is built on. For example, Sia might know when it’s out of date, and tell you that that’s a problem because you’re now a version behind, but it can’t auto-update because that removes your control.
This isn’t so much of a problem for third party devs, whose apps inherently take some control away from users in return for convenience. It lets you identify issues, present them to the user, and deal with them in creative ways.
Alerts register when they’re triggered, and then unregister when whatever caused them is fixed. This makes for easy tracking with the new API endpoint /daemon/alerts. This returns the following for each error that is registered:
cause | stringCause is the cause for the information contained in msg if known.msg | stringMsg contains information about an issue.module | stringModule is the module which caused the alert.severity | stringSeverity is either “warning”, “error”, or “critical” where “error” might be alack of internet access and “critical” would be a lack of funds and contractsthat are about to expire due to that.
Here’s an example return that you might get when you call for the current alerts that Sia is experiencing.
cause: wallet is locked, msg: user’s contracts need to be renewed but a locked wallet prevents renewal, module: contractor,severity: warning,
The great thing about the Alert System is that, as a user, you don’t need to do much. If you’re an advanced user, just call the /alerts endpoint or use siac alerts in the command line. You’ll see any currently registered alerts and can take the appropriate action, or at least get pointed in the right direction.
The alerts aren’t yet implemented into Sia-UI, at least not in a GUI-friendly way yet. You can still go the Terminal in Sia-UI and type siac alerts to find them, but in the future Sia-UI will auto-report them to you. The beautiful user interface of Sia-UI is the perfect home for important updates about the status of your Sia node.
The implementation of the Alerts System is an important step for Sia — one that allows Sia to stay decentralized while giving it essential self-monitoring tools. The amazing third-party developers in our community will be able to make use of this to enable a better experience for their users.
This is a feature that will serve enterprise-level use well, and something that can be expanded on easily in the future. As Sia grows (we recently hit an all-time high of 734 TB stored on the network), this feature provides a great way for the platform to remain agile, provide important info, and creates a path for third-party devs to come up with some really clever solutions.
Download Sia v1.4.2 (v1.4.2.1 coming soon) to experience the Alerts System in action. It’s accessible via the CLI for Sia, or the Terminal built-in to Sia-UI. This new feature expands Sia’s capabilities and is accessible to all users.
Chris’ code that created the Alert System: !3769
Important alerts for renters: !3855
New gateway alerts: !3874
PJ adds hosting alerts: !3903
Tbenz adds the siac alerts command: !3858
David and Matt address low funds: !3754 !4013
steve@sia.tech
steve#4381 on Discord
Decentralized storage — Sia, Skynet, and cryptocurrency.
22 
Thanks to Matthew Sevey. 
22 claps
22 
Written by
Head of Support for Sia
Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by
Head of Support for Sia
Decentralized storage — Sia, Skynet, and cryptocurrency.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://debugger.medium.com/i-joined-google-one-and-i-bet-you-will-too-9ffc8eca9324?source=search_post---------211,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lance Ulanoff
Sep 28, 2021·3 min read
I fought and fought. In truth, I deleted and deleted in a futile attempt to maintain enough free Google Cloud Storage space to continue receiving Gmail.
They were a continued source of stress, the messages informing me that I’d eaten up 97% of my Gmail storage space, and I’d been seeing them for at least a…
"
https://medium.com/google-cloud/how-to-write-to-a-single-shard-on-google-cloud-storage-efficiently-using-cloud-dataflow-and-cloud-3aeef1732325?source=search_post---------20,"There are currently no responses for this story.
Be the first to respond.
In Apache Beam, when you write out a text file to a blob store using:
You will get a number of output shards, like this:
The exact number depends on your dataset, number of workers, etc. Yet, many times, you will want exactly one output file because the software you are using wants just one file.
The easy way out is to tell Beam to write out only one shard:
However, this is very inefficient. The power of a distributed system comes in being able to completely parallelize the work and having a single sink will slow down your Beam pipeline.
I recently faced this problem, and solved it using Cloud Functions (code in GitHub).
In your Beam pipeline, specify a number of shards. The “compose” feature of Google Cloud Storage that we are going to use has a limit of 32 files currently, so you don’t want your pipeline producing more than that. At the same time, for efficiency sake, you’d like to have more shards than the number of workers in your Dataflow job:
When this is done, you will have a bunch of files. How do you concatenate them?
The naive way to concatenate a bunch of files on GCS is to download them to a VM, concatenate them using Unix `cat` and upload them. Don’t do that!
Google Cloud Storage supports a nifty feature called “compose”: it lets you compose a blob out of up to 32 source blobs. You can do this from the command line by doing:
No downloading or local storage needed! You can also delete the shards after the compose to avoid clutter. So, an efficient way to create single output file is to run the Dataflow job, tell it to produce up to 32 shards, and then concatenate the output using the compose functionality.
Well, you could run the Dataflow job, wait for it to finish and then invoke the compose command. But why do that when you can set up a Cloud Function to do it for you automatically whenever the sharded files show up in the bucket?
I went to the GCP web console, navigated to the Cloud Function console, created a function to trigger on Create events in my bucket, chose Python 3.7 as my preferred language and then typed in the following Python function in main.py:
I also checked the box that said ‘retry’ on failure and specified the `google-cloud-storage` package in requirements.txt.
What does the code above do? It verifies that it was triggered on a file that has the prefix my Dataflow pipeline was writing to and that this is the last shard.
Now, the last shard is not necessarily the last one to get written out. There could be a race condition, or one of the earlier files could be very large. So, I do a bit of defensive programming and make sure that all 10 shards that I expect exist. I can do that by constructing a Blob with the expected filename and check that it exists. If it doesn’t exist, I simply throw an exception. Recall that we set up the function so that it will be retried in 60s if this is the case.
Once I verify that all the shards exist, I can use the API call to compose the blobs into one large blob, and delete the individual blobs.
So, now, when I run my Dataflow pipeline, it produces 10 blobs. As soon as the 10th blob hits Google Cloud Storage, the Cloud Function runs and concatenates all 10 blobs into a single one.
Me? I can go get tea while the job runs and the function watches the job.
Google Cloud community articles and blogs
243 
5
243 claps
243 
5
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@fortelabs/thank-you-for-your-heartfelt-message-charlotte-cd336237e772?source=search_post---------283,"Sign in
There are currently no responses for this story.
Be the first to respond.
Tiago Forte
Jan 19, 2016·4 min read
Charlotte Crowther
Thank you for your heartfelt message Charlotte. One of the main reasons I write these things is to create a wavelet that others can ride with me. People think writing an “expert” post is the end of a learning journey, but it’s actually the beginning. I get to have lots of diverse people critiquing and adding to my experiments.
I see some ppl looking earnestly for a single solution to all their digital storage needs. But it can’t be done. Each and every storage tool has major pluses and minuses, and the pluses are valuable enough that I believe it’s worth investing the time to deal with the minuses.
For every file I store, I try to think what is the very best medium to retrieve it at the moment of greatest need? Because that moment is when time is of the essence. This boils down to large files (such as videos, design assets, photos) in Dropbox, office docs in Google Docs, and random media and text snippets (i.e. notes) in Evernote. Virtually everything I have to store goes in those 3 locations.
This means that, even when working on a single project, you will be rapidly switching between tools. Many assume this is a drawback, but I see it as a great strength. You get to see information in different formats, in different contexts, in different media — all good for creativity.
The key to making it work is using a single, universal organizational scheme across ALL tools. I use only 4 top-level categories: 1 Projects, 2 Areas, 3 Resources, 4 Archives. Projects are the very few currently active projects, which change weekly. Areas are longer term responsibilities like Health, Finances, and Business Dev’t that don’t pertain to a single project. Resources are general topics of interest like Web Design, Recipes, Stock Photos, etc. And Archives are all the completed, inactive projects.
I replicate these 4 categories and the folders within them across the 3 tools I mentioned earlier, plus my task manager. I recommend doing so down to the exact wording, punctuation, and capitalization, so that as you switch between tools your brain doesn’t have to make any cognitive leaps. The biggest challenge in doing this is to define your projects and areas very, very clearly, apart from any tool, instead of creating random projects differently in each tool. You can create new projects ad hoc during day to day work, but periodically need to “sync” them.
The good news is, you probably already have the right info in the right tool already. For ex, large files in GDrive instead of Evernote. All that’s required is defining your Active Projects and Areas of Responsibility, creating them identically in every tool, and then moving files into this new scheme. You will find something interesting: You will spend the most time in 1 Projects, but it will have the fewest folders and notes. Just as it should be, to preserve your attention. You will spend less time in 2 Areas but it will have more info, even less in 3 Resources with even more info, and very little time in 4 Archives, and it will have the great majority of info. Doesn’t it make sense that the great majority of info is non-actionable at any given moment, and therefore should be as removed (archived) as possible from your active projects? While still keeping it just in case of course.
2. Reminders
I do use lots of different kinds of reminders, but it’s difficult to lay out a system for it. Basically, it comes down to whether I can leave something in a place that I know I will come across it at the right time, and if I don’t, it’s no big deal.
For ex, for blog posts, as soon as an idea starts percolating in my mind, I create a note for it in the Blog notebook. Any random idea or content I come across, I stick it in there. When I feel I’ve reached a critical mass of content to pull together into a post, all the research is there waiting for me.
It’s all digital versions of the ultimate reminder hack: leaving something in front of the door to be sure you don’t forget it.
3. Social media
It’s part of my daily checklist to “do the rounds” of social media. This means opening tabs for the sites I want to check, and responding to anything I have to.
But crucially, I don’t consider this work, and I don’t start my time-tracking until I actually finish that and do real work. Since this is usually the start of the workday and I’m feeling energetic and optimistic, this creates a pressure to keep it to an absolute minimum.
Trello is cool for certain projects, but I find that I don’t need everything to be that visual and tactile. It’s almost too skeuomorphic. Text is faster, cleaner, more compressed, and more universal, so I’ll stick to it unless I need to switch mediums for creative purposes.
You’ll understand why I don’t and wouldn’t use Streak or Mail Conductor if you read my post on Inbox Zero ;)
Founder of productivity consultancy/training firm Forte Labs (fortelabs.co), editor of members-only publication Praxis (praxis.fortelabs.co)
See all (659)
2 
2 claps
2 
Founder of productivity consultancy/training firm Forte Labs (fortelabs.co), editor of members-only publication Praxis (praxis.fortelabs.co)
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nderground-net/the-source-code-for-nderground-is-stored-in-a-private-cloud-repository-66d4c212946b?source=search_post---------313,"Sign in
There are currently no responses for this story.
Be the first to respond.
nderground
Feb 24, 2018·1 min read
Ramesh Jayavaram
The source code for nderground is stored in a private cloud repository. One of the reasons that I like the cloud solution, compared to local storage, is that if anything happens to my local systems, the source code is still safe and available.
Modern cloud storage backs data up across multiple geographic regions, in multiple data centers, on redundant disks. In effect this means that the data is never lost.
Having a local device store critical source code is fraught with hazard. The justification given here is that “hackers may get you” in the cloud.
This plays on a common misconception that cloud systems are less secure than local systems. In fact, cloud systems can often be constructed so they have higher security than local systems.
No system can be built so that it is “hacker-proof”. However, by storing code in the cloud it guarantees that critical source code cannot be lost to fire, flood or physical theft.
nderground was a social network designed for privacy. nderground.net never took off and has been shut down. See topstonesoftware.com and bearcave.com.
nderground was a social network designed for privacy. nderground.net never took off and has been shut down. See topstonesoftware.com and bearcave.com.
"
https://medium.com/@openbom/solid-edge-dropbox-collaboration-and-openbom-cloud-storage-integration-7c991c0ead8e?source=search_post---------147,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Oct 16, 2017·3 min read
Collaboration is a big deal for users. Your favorite CAD system running on your desktop is likely to run into the painful problem of sharing data and collaborating. Many customers have asked us how to solve it. For the last couple of years, Solid Edge developers took the direction of providing cloud file storage to help users collaborate. Built-in Solid Edge data management supports sharing data on popular cloud -file storage platforms such as Dropbox, Google Drive, etc. Check this Siemens article, Built-in Solid Edge data management.
The Design Manager utility enables you to review and edit properties of multiple files, and to perform revision and release operations on Solid Edge parts, assemblies and drawings. And you can backup, share, and synchronize your Solid Edge files using popular cloud based file sharing software like Dropbox, OneDrive, Google Drive and Box.
More information can be found on this Siemens press release from last year, here.
New cloud capabilities provide faster and more flexible deployment options, improved user access and collaboration across remote designs teams. Licenses and user preferences can be stored on the cloud enabling users to access their personal environment anytime and anywhere. Solid Edge ST9 also adds data storage options via cloud-enabled vaulting so users can store and share design data in a controlled manner with external suppliers and customers, using popular software like Dropbox, OneDrive® software, Google Drive™ online storage service and Box. While this allows users to efficiently work online, the option to work locally, i.e. offline, is always available.
At OpenBOM we are also thinking of the value of cloud-based file collaboration. To facilitate this cloud storage based collaboration, we extended OpenBOM with the ability to define reference properties with links to Dropbox, Google drive and other cloud storage tools. Check you this article to learn more, Connecting cloud storage to Bill of Materials in OpenBOM
If you’re using Solid Edge and sharing data using Dropbox, you can create a direct reference to a file from OpenBOM. It can be quite useful and provide direct access to the file for people you’re sharing your design with such as contractors and suppliers. Although, you can do it already, we found this manual process a bit cumbersome. So, we are planning some future enhancements to our Solid Edge integration which will automatically share 3D files to Dropbox and other cloud storage tools. We are looking for people using Solid Edge to share their experience of working with cloud storage.
In the coming week, we will attend Solid Edge University and Developer Days in Cincinnati. More information about the event can be found, here. If you plan to be there, let us know; we would love to discuss how OpenBOM can help you.
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
"
https://blog.sia.tech/things-you-can-do-with-sia-82baec01d0ac?source=search_post---------236,"Sia is fundamentally a cloud storage platform. And, when compared to other types of cloud storage, it has the following advantages:
Sia makes the most sense for write-seldom, read-often data, however it is also a good substitute for most things that you would put on Amazon Glacier or Amazon S3. Here are some examples of things that work very well for Sia:
More creative ideas:
Sia, by Nebulous Inc., is a blockchain-based decentralized cloud storage platform.
Decentralized storage — Sia, Skynet, and cryptocurrency.
11 
2
11 claps
11 
2
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://medium.com/google-cloud/gcs-kms-and-wrapped-secrets-e5bde6b0c859?source=search_post---------232,"There are currently no responses for this story.
Be the first to respond.
Simple procedure to encrypt and save a secret uisng Google Cloud Platforms Key Mangement System (KMS) and Cloud Storage (GCS). THis is one possible way to save a secret file (or any file) securely in an ACL-protected cloud storage bucket and on top of that, encrypt it with a key only select identities can decrypt.
This procedure is an implementation of some of the guidlines descrbed in the following articles:
Possible usecase is to save a secret (JSON certificate file, API Key, etc) in as a wrapped (encrypted) filed on GCS that is protected by ACL and which can only get decrypted by Cloud KMS
What that basically describes is a way to encrypt a secret and save it in a GCS. The code sample here is in golang which does the following:
→Initializes KMS and GCS clients
→Remove all ACLs on that object except for the OWNER and principal that will read the object (SERVICE_ACCOUNT_A)
→Sets KMS KeyRing IAM permissions on the principal (SERVICE_ACCOUNT_A) to use the decryption Key
→Encrypts the secret.
→Uploads the secret to GCS
→Initializes client as another user (SERVICE_ACCOUNT_A)
→Use SERVICE_ACCOUNT_A to access KMS and the encrypted object in GCS
→Decrypt secret using KMS.
→If the encrypted file is a service_account.json file, read it in and initialize a client
Note: if CLoud KMS supports remote user-specified key encryption directly (i.,e a key you upload first and KMS uses that specific key), the procedures outlined here will need be modified.
This is the principal that will have access to the secret
First setup the golang environment:
and then execute the script as shown below in the same shell as you previously setup the env-vars!
This saves and encrypted form of a file in a GCS bucket as you (using applicaiton default/gcloud credentials) and then, as SVC_ACCOUNT_A, decodes it using KMS:
Sample output:
What that demonstrates is taking your plain text secret, using kms to encrypt the file, then upload the encrypted file to GCS…then finally remove ACL access to that file in GCS. This means only the OWNER and principal you set the ACL to read and use KMS (for that key) can read+decrypt it.
You can also enable data access logging to show who used a given key to decrypt a secret.
for example, to enable key data access logs, run
then edit /tmp/policy.json (remember to specify a new etag and your svc account!)
Finally apply the new policy:
After this if you run the KMS-GCS script below, you should see a similar audit log describing who used the key (of course this is just the KMS key details; once you secret is in possession of the client app, all bets are off!)
Here are somescreenshots from the GCP console for KSM and the object ACLs enforced:
______
The primary example here is pretty straightforward: we’re using a KMS system to encrypt the secret outright. An alternative to this is one more layer of indirection with layered security. That is, you can use “Envelope Encryption”. What you are doing there is generating an encryption key locally (it can be anything), then using that key to symmetrically encrypt your secret, then ask KMS to encrypt your _symmetric key_ that you just generated. After that, you can forget about the paintext symmetric key. What you are left with is your secret that is now encrypted with a key you just generated locally and the key you just generated itself encrypted by one in KMS. You can save both together because both are encrypted. If someone needs to decrypt the data, they will take the encrypted symmetric key, ask KMS to decrypt it, then use that decrypted form to decode the original secret.
In certain cases, the key you generate locally is not cryptographically strong (i.,e not enough randomness, etc). What you can do there is ‘strengthen’ it cryptograpically by running the orignial key you generated through a KDF many times. Think of it as a hash of hash of hash of your original key. Once you have this derived key, you can then encrypt your data. If someone needs to decrypt it, they’d take your original key, then run it through the same KDF protocol (its one way repeatable), then end up with the same derived key…use that to decrypt your secret as normal
GCS already has a built in capability to encrypt objects on upload and download. That is, GCS will always encrypt an object at rest using either Google’s own key or a key you provide with each request or a key that you specify that is stored by Google in its KMS. THese various options are described here:
However, the subtle difference between the procedure described in this article and the mode where you use GCS+CSK with KMS backend is that mere access to the GCS object allows you to see the object since Google does the decryption using your KMS keys for you. To put it another way, GCS+CSK with KMS allows you to define a key k1, allow GCS’s own internal service account access to encrypt/decrypt with k1. Then you say, “any object uploaded/downloaded to this bucket will use key k1”. If you save a secret with this mechanism, all a user needs to is access to the object to get the secret. The procedure outlined in this artilce requires two spaprate access controls: access to the object (GCS) and the _callers_ (your) access to the KMS secret. I’ll provide a sample of GCS-CSK (with KMS) here:
Set IAM permissions to GCS’s default svc account for the KMS key
The encrypted file is now labled as CSK backed:
Google Cloud community articles and blogs
38 
1
38 claps
38 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@alibaba-cloud/five-measures-to-make-alibaba-cloud-storage-more-secure-bf0dda20ccb9?source=search_post---------156,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 16, 2020·7 min read
By Alibaba Cloud Storage
On June 1, 2017, the Cybersecurity Law of the People’s Republic of China was officially implemented, marking an important milestone in China’s establishment of strict network governance guidelines. On May 13, 2019, the “Baseline for Classified Protection of Cybersecurity” and other relevant national standards were officially released, opening the era of Classified Protection 2.0. Additionally, the “Measures for Data Security Management (Draft for Comments)”, “Regulation on the Protection of Children’s Personal Information Online”, “Guideline for Internet Personal Information Security Protection”, “Measures for Security Assessment of Cross-Border Data Transfer of Personal Information and Important Data”, and “Cryptography Law of the People’s Republic of China” were released in quick succession. The General Data Protection Regulation (GDPR) issued by the European Union in 2018 is currently the most stringent and detailed law designed to protect the security of user data. Starting in January 2020, the California Consumer Privacy Act (CCPA) came into force. This act regulates all business activities that involve data related to California residents.
With the introduction of laws and regulations around the world, data security is becoming increasingly important. In data collection, application, and storage procedures, data security protection and privacy protection are no longer optional.
Constant attacks by hackers and ransomware also pose major challenges to data security. According to the “Ransomware Virus Analysis Report for February” released by 360 Security Brain, there are many new ransomware additions. There are familiar names like GlobeImposter, Phobos, and Cryosis as well as new names like HackedSecret and Makop. All of these ransomware launched violent attacks against various systems.
Software vendors and cloud service providers must constantly enhance their ransomware detection capabilities to prevent the reinfection of backup data. At the same time, ransomware defense technology needs to move from detecting and alerting against attacks to identifying malicious code before intrusion to protect the security of backup data.
Protecting data security against human error is an aspect of data security that is often overlooked. Inside enterprises, such human errors include the improper selection of technologies, weak security awareness, lack of backup and disaster recovery planning, and improper process configurations, such as improper permissions or privileged users. Manual operations can result in service crashes or the deletion of core databases, which can permanently damage the operational capabilities and competitiveness of an enterprise.
Data sharing platforms based on a centralized data exchange mechanism have the problems of complex processes, high costs, and low efficiency in personal privacy protection. Therefore, data security needs to be empowered by new digital technologies. For example, artificial intelligence (AI) technology is required to implement more efficient data security management, help ensure the secure use of data at large scale, and advance the digital transformation of the economy and society.
Data access permissions specify when, how, and by whom data can be obtained. Access management is an effective way to protect data.
Alibaba Cloud storage products support standard directory and file-specific operations in file systems, read, write, and execute permissions for users and groups, Virtual Private Cloud (VPC) mount points, and classic network mount points. These products also support allowing only Elastic Compute Service (ECS) instances to access file systems in the same VPC or under the same account. At the same time, our storage products provide the permission group function, which can add permission group rules through a whitelist. This allows only the specified IP addresses or network segments to access the file system. In addition, users can apply different levels of fine-grained access permissions to different IP addresses or network segments. These methods implement effective permission control for data access and make data more secure.
As the most common data security method, data encryption can be performed by the source, intermediate device, or transmission channel. The main problems in data encryption are how to store and use keys properly.
At the storage stage, Alibaba Cloud Object Storage Service (OSS) provides the read many feature, which allows users to prevent the tampering with or deletion of data stored in the cloud. The data encryption feature of OSS supports both client-side encryption and server-side encryption. OSS can use key hosting services and user-defined key methods for encryption, greatly improving data security and compliance.
At the same time, Alibaba Cloud provides services for transparent internal operations, which allow users to view and control operation logs on the cloud platform, increasing users’ trust in the security of Alibaba Cloud.
In the data transmission phase, Alibaba Cloud Network Attached Storage (NAS) provides encryption in transit and encryption at rest functions to guarantee data security during transmission through managed keys and private key encryption. Alibaba Cloud Disk is integrated with Alibaba Cloud Key Management Service (KMS) to allow users to encrypt disks and ensure data privacy and autonomy. Users can encrypt their data by using the one-click cloud disk encryption function.
For cloud users, it is very important to know who accessed what data at what time and what operations they performed. In particular, when a security event occurs, users must be able to quickly locate the source of the event by querying operation records, determine whether the event is resulting from an internal or external attack, and address the event immediately.
To this end, Alibaba Cloud provides operation log storage services for cloud users. Data owners can enable access logging for their data in the Alibaba Cloud Storage console. After access logging is enabled, access logs are automatically generated on an hourly basis, named according to predefined naming rules, and then written to a specified location. Users can analyze these log files directly by using the Alibaba Cloud Data Lake solution or by building a Spark cluster. Users can also set lifecycle management rules to convert the storage class of log files to Archive for long-term archiving.
The real-time log query function combines storage with log services. This feature allows users to query relevant access logs in the console and helps users perform data access operation audits, compile access statistics, backtrack abnormal activities, and locate problems.
To ensure the security of users’ log data, all HTTP requests to the log service API must undergo security verification. The verification process is based on the Alibaba Cloud Access Key mechanism and completed by using the symmetric encryption algorithm. This feature prevents log tampering and comprehensively improves the security and compliance of Log Service.
In terms of security for enterprise data, ransomware is one of the most dangerous cyberattack forms. After data is encrypted by ransomware, it is difficult to decrypt unless you pay the ransom. Data center faults, natural disasters, and accidental deletion may also incur data security risks and business interruptions. The intrinsic resource advantages of the cloud provide the conditions that make it possible to solve these problems.
Alibaba Cloud provides storage products with multi-version functions that allow users to retain and restore previous file versions and customize the retention period. In the event of data loss or damage caused by ransomware or human errors, data can be restored by rolling back to a previous version. This ensures that user businesses are not affected.
In June 2018, Alibaba Cloud officially released Hybrid Backup Recovery (HBR) and Hybrid Disaster Recovery (HDR), the first cloud-native products in China. HBR and HDR provide cloud backup and disaster recovery protection and allow users to deploy disaster recovery solutions in minutes. Alibaba Cloud released a storage service that allows users to deploy three availability zones in the same city, which was also a first in the Chinese market. This service can meet enterprise users’ need for zero data loss and uninterrupted business in the case of data center-level disasters.
Compared with the construction of offline, local disaster recovery data centers, OSS zone-redundant storage provides a 99.95% availability service-level-agreement (SLA) guarantee, 99.9999999999% reliability, and one-click deployment of cloud-based zone disaster recovery services. By incorporating cross-region replication capabilities, OSS can provide complete disaster recovery services at the data center, region, and cross-region levels.
AI security technology for protecting data privacy has developed in the field of distributed computing and information security, providing a new computing model for collaborative network computing. This approach uses multiple technologies to protect data security, including secure multi-party computing, differential privacy, dynamic encryption, and encrypted search and computing. By leveraging AI and related technologies, Alibaba Cloud storage products are evolving to be more automated and intelligent in the security and governance of stored data. At the same time, the Alibaba Cloud Storage team will work with DAMO Academy to explore ways to reduce trust costs and financial costs and give full play to the value of data.
Currently, Alibaba Cloud provides storage services to millions of customers in government, Internet, finance, healthcare, and education. Our storage services are deployed on a global scale with a capacity exceeding 100 EB. Alibaba Cloud is committed to providing users with stable, secure, reliable, and easy-to-use storage services.
In the future, we will continue to focus on customer needs and refine our storage technology and products to create more value for customers. We aim to allow customers to enjoy the technical and service benefits made possible by cloud computing.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/the-coffeelicious/too-bad-some-things-can-t-be-moved-to-the-cloud-cartoon-9e59de5bdeb7?source=search_post---------239,"There are currently no responses for this story.
Be the first to respond.
MIT , where I work, gives all of its students, faculty, and staff access to free, unlimited cloud storage through Dropbox. It’s a great perk but, unfortunately, won’t solve all of your problems. Oh well.
Home to some of the best stories on medium.
12 
12 claps
12 
Home to some of the best stories on medium. Look around, relax and enjoy one with a sip of coffee.
Written by
Cartoonist. Former Dave Letterman joke writer. Yinzer. Dad of two girls (one a T1Der). Beer drinker. Pizza lover. He/him.
Home to some of the best stories on medium. Look around, relax and enjoy one with a sip of coffee.
"
https://medium.com/@imrenagi/writing-date-partitioned-files-into-google-cloud-storage-with-cloud-dataflow-50ee1d5c03ed?source=search_post---------57,"Sign in
There are currently no responses for this story.
Be the first to respond.
Imre Nagi
Jun 13, 2018·3 min read
Writing Date Partitioned Files Into Google Cloud Storage With Cloud Dataflow
If you read my previous article about Serverless Lambda Architecture With GCP, you will know that I was trying to store all events to google cloud storage by using dataflow. I intended to store it on date-partitioned basis, but it failed..
My goal was to group a collection of files into a date partitioned folder. What does it mean? It means, all streams arrived at a time should be stored in a folder named as yyyy/mm/ddof that time. For instance, If I would like to store a file namedworldcup.json collected on 25th May 2018 , to gs://imre-poc bucket, the file should end up stored on this path: gs://imre-poc/2018/05/25/worldcup.json
Here is the approached that I used with Dataflow API:
With the code above, I was trying to use .to() method of TextIO by parsing the predefined string as the parameter. Once the code run on the cloud, I can see the files immediately stored on gs://bucket/rsvp-stream/2018/05/25/file-xxxxxx.json . But, can spot the bug? If you realized, I use .to() method to set the prefix of the file, but this approach didnt work as expected when the day is changed. When the day changes into 26th May 2018, all of the streams were still written into the same bucket gs://bucket/rsvp-stream/2018/05/25/ .
So, I started digging about writing dynamic file name to dataflow and I found out that extending FileBasedSink.FileNamePolicy would solve my problem. Extending FileNamePolicy requires us to implement to methods: windowedFilename() and unwindowedFileName() . We only need to implement windowedFilename() and throw exception in another function because we can guarantee that the data stored to GCS are always bounded data (your use case might be different than mine!)
With the new implementation above, I have private function named filePrefixForWindow() which will construct a the folder path of the files every time they are written to GCS. Then, instead of passing the file name directly to .to function of TextIO, I’m trying to only pass the bucket name to .to() method and pass an instance of DatePartitionedNamePolicy into .withFilename() . Remember that, you also need to use .withWindowedWrites() to make sure that the writing will always be done in windowed time.
Finally as expected, now the result will be stored on different date partitioned folder in my GCS bucket.
I will be happy to share all of my knowledge related to Google Cloud Technology specifically for its Big Data Technology. Let me know if you are interested to learn more or just want to discuss something.
Google Developer Expert, Cloud Platform Engineer @gojek
141 
3
141 
141 
3
Google Developer Expert, Cloud Platform Engineer @gojek
"
https://sonyreconsidered.com/ps3-firmware-3-6-brings-cloud-storage-online-game-saving-cf92ade2f72e?source=search_post---------195,"When the PSN (PlayStation Network) entered it’s usual scheduled maintenance time, no one would have guessed that Sony was taking down the service to beef up the PS3 offering for PlayStation Plus users and giving them one of the holly grail features that people have been wanting, saving games to the cloud.
When PS3 firmware 3.6 goes live tomorrow, PlayStation Plus users will be able to back up game save data to the cloud. This new feature not only will ensure that your files are saved off location, in case your hard drive were to go south, but users will be able to access these files from any other PS3, simply by logging into their PSN account.
The new online storage feature allows gamers to store up to 150MB of game save data and a maximum of 1000 data files per PSN account.
Saves games to the cloud will happen automatically for PlayStation Plus users and while most games current available will work with this online storage method, all future games are required to have cloud saving compatibility. I’ve been a long time PlayStation Plus users, having written a piece why I believe every PlayStation 3 user should have PlayStation Plus. Now, I can finally get another PS3. I’ve long wanted a secondary PlayStation 3, leaving the current one in the living room while having a machine in the bedroom as well. But the idea that I cannot continue my career in Gran Turismo 5 or the levels I’ve completed in LittleBigPlanet 2 always held me back and copying saved files back and forth from each machine via a USB drive seemed too tedious and bound for disaster. With Cloud Storage, I’m well on my way towards ordering another PlayStation 3. Press release after the jump.
PLAYSTATION®NETWORK BRINGS ONLINE STORAGE FOR GAME SAVES EXCLUSIVELY TO PLAYSTATION®PLUS SUBSCRIBERS
March 9, 2011 — Sony Computer Entertainment America LLC (SCEA) today announced that online storage for game saves will be available exclusively to PlayStation®Plus subscribers, allowing gamers to back up their save data to the cloud. Online storage for game saves will be available with system software update (v3.60) for the PlayStation®3 (PS3™) system, coming on March 10, 2011 and will be automatically enabled for PlayStation Plus subscribers once users install the update.
With the new feature, PlayStation Plus subscribers can store up to 150MB of PS3 game save data and a maximum of 1000 data files per PlayStation®Network account. “Copy-prohibited save data” is also supported, and all previously backed-up save data may be restored once per 24-hour period. When the new feature launches, most PS3 titles will be compatible with online storage for game saves, and moving forward new titles will have the capability to offer the storage option.
Online storage for game saves ensures that data stored by PlayStation Plus subscribers is more secure than ever and is an integral feature for gamers who wish to access their data on other PS3 systems. Users can check their available/total space at any time via the XMB™ (XrossMediaBar). Users who wish to utilize the feature immediately following the system software update (v3.60), can do so with these instructions:
To back-up:1. Select [Game] > [Save Data Utility].2. Focus on the save data you wish to back up.3. Open up the option menu using “triangle” and select [Copy].4. Select [Online Storage] as the destination of back up.
To copy:1. Select [Game] > [Save Data Utility] > [Online Storage].2. Focus on the save data you wish to copy.3. Open up an option menu using “triangle” and select [Copy].
PlayStation Plus is available for a yearly fee of $49.99, and for a limited time, gamers will also receive 3 free bonus months when signing up for a one-year subscription. Gamers can also subscribe to the 3-month option for $17.99.
[Via PlayStationBlog]
Commentary, analysis, and insight on Sony and PlayStation / Formerly SonyRumors /
Written by
 alumni | journalist and content creator | part 🇩🇪, full petrol head | lover of all things Marvel | creator of @sonyrumors | #fuckcancer
Daily commentary, analysis, and insight on Sony, PlayStation, Sony Pictures and various other divisions and their place within the greater tech and gaming space.
Written by
 alumni | journalist and content creator | part 🇩🇪, full petrol head | lover of all things Marvel | creator of @sonyrumors | #fuckcancer
Daily commentary, analysis, and insight on Sony, PlayStation, Sony Pictures and various other divisions and their place within the greater tech and gaming space.
"
https://medium.com/@wickr/cloud-security-10-ways-to-better-protect-data-in-the-cloud-d0933fd8988d?source=search_post---------297,"Sign in
There are currently no responses for this story.
Be the first to respond.
Wickr
Sep 22, 2020·5 min read
According to 451 Research, 90% of organizations are using cloud storage and services. Cisco projects that, by the end of 2021, cloud data centers will represent 95% of all data center traffic. Chances are good that your organization is using the cloud for some or all of your data storage.
The question is: How secure is data stored in the cloud, and what can you do to improve your organization’s cloud security?
Cloud storage services store data in “the cloud,” a collection of servers connected by and to the Internet. Instead of storing data on local servers, companies upload that data to cloud servers where it can be accessed by multiple users from multiple locations.
Data stored in the cloud needs to be at least as secure as data stored on-premises. Just as cloud storage is an outsourced solution for storing digital data, cloud security is an outsourced solution for securing that data.
Cloud security involves several technologies and processes designed to ensure that only authorized users have access to the specific data stored in the cloud. These best practices include file-level encryption, advanced firewalls, intrusion detection, and physical security that provides 24/7 monitoring of cloud data centers.
As popular as cloud storage has become, not every company is completely comfortable with the concept. According to LogicMonitor, two-thirds of IT professionals see security as the biggest concern in their adoption of cloud computing. To improve cloud security, your company can implement the measures listed here.
Cloud security is not perfect. According to the McAfee Cloud Adoption and Risk Report, the average organization registers 12.2 incidents of unauthorized cloud access each month.
If there is even the slightest chance of your company’s most secure data being compromised in the cloud, don’t put it there. If you have data that could seriously damage your organization if stolen, you should store it on an ultra-secure local server instead of exposing it to the world via the Internet.
What would you do if the data you stored in the cloud got corrupted or accidentally deleted? The possibility of data loss is why it’s wise to create local backups of all data you store in the cloud. If something happens to your cloud data, you have a replacement copy nearby.
According to a LastPass report, 59% of people use the same passwords across multiple accounts. If left to their own devices, your employees are likely to create a single easy-to-remember password that they use for all of their accounts — including access to your organization’s cloud data. Easy-to-remember passwords are easy to hack, so you should require your employees to use stronger passwords that are unique to your cloud access. They should also be required to change their passwords regularly.
Multi-factor authentication (MFA) improves on password concerns by requiring employees to input a code sent to their phone or computer in addition to using a normal password. Microsoft reports that MFA stops 99.9% of automated hacking attempts.
Not every employee needs access to everything your company stores in the cloud. You can limit your exposure by limiting access to only those employees that need it. This means applying rigid access controls based on actual need, not on position or location.
What happens if cloud data is accessed by an unauthorized third party? Nothing — if that data has been encrypted. By encrypting all data before it’s sent to the cloud, anyone accessing that data without the appropriate decryption key won’t be able to read it.
When remote workers access cloud data from their personal computers, tablets, and smartphones, security suffers. These personal devices are often less secure than work-supplied devices, which makes them attractive penetration points for hackers. According to the Verizon Mobile Security Index, 39% of organizations have experienced a security compromise caused by the poor security of mobile devices. The best defense is to secure all connected devices, corporate and personal, with strong anti-malware tools.
Don’t assume that your cloud provider’s security is performing as promised. Scholarly research has long proven that security can (and often does) decay over time. Do periodic testing to see if your provider’s security systems — as well as your own — are still working as they should.
Human negligence can defeat even the toughest security systems. According to IBM’s Cost of a Data Breach Report, 24% of data breaches are caused by some sort of human error. Moreover, MediaPRO found that 7 out of 10 employees lack the training required for basic cybersecurity awareness.
Make sure your employees are properly trained on proper security techniques, including how to recognize phishing scams. Regular training sessions can also help keep security top of mind.
Not all cloud storage providers are equal, especially where cloud security is concerned. Look for a provider that offers strong, state-of-the-art cloud security, including file-level encryption and sophisticated credential management tools. You should also compare your existing service to others in the industry once a year or so, just to make sure they’re keeping up with the latest developments.
Ensuring strong cloud security takes work. You can’t solely rely on your cloud provider to carry the burden — your company’s IT staff and employees have to do their part, as well. It’s a constant but necessary challenge.As part of your cloud security efforts, consider employing a secure cloud-based communications platform, such as Wickr Pro. Wicker Pro utilizes end-to-end encryption to secure all cloud-based communications, including text, voice, and video messaging and collaboration.
Originally published at https://wickr.com on September 22, 2020.
Secure Ephemeral Communications. Built for the enterprise. End-to-end encrypted messaging. Secure rooms. Peer-to-peer encrypted file sharing. Multi-platform.
1 
1 
1 
Secure Ephemeral Communications. Built for the enterprise. End-to-end encrypted messaging. Secure rooms. Peer-to-peer encrypted file sharing. Multi-platform.
"
https://netflixtechblog.com/byte-down-making-netflixs-data-infrastructure-cost-effective-fee7b3235032?source=search_post---------328,"By Torio Risianto, Bhargavi Reddy, Tanvi Sahni, Andrew Park
At Netflix, we invest heavily in our data infrastructure which is composed of dozens of data platforms, hundreds of data producers and consumers, and petabytes of data.
At many other organizations, an effective way to manage data infrastructure costs is to set budgets and other heavy guardrails to limit spending. However, due to the highly distributed nature of our data infrastructure and our emphasis on freedom and responsibility, those processes are counter-cultural and ineffective.
Our efficiency approach, therefore, is to provide cost transparency and place the efficiency context as close to the decision-makers as possible. Our highest leverage tool is a custom dashboard that serves as a feedback loop to data producers and consumers — it is the single holistic source of truth for cost and usage trends for Netflix’s data users. This post details our approach and lessons learned in creating our data efficiency dashboard.
Netflix’s data platforms can be broadly classified as data at rest and data in motion systems. Data at rest stores such as S3 Data Warehouse, Cassandra, Elasticsearch, etc. physically store data and the infrastructure cost is primarily attributed to storage. Data in motion systems such as Keystone, Mantis, Spark, Flink, etc. contribute to data infrastructure compute costs associated with processing transient data. Each data platform contains thousands of distinct data objects (i.e. resources), which are often owned by various teams and data users.
To get a unified view of cost for each team, we need to be able to aggregate costs across all these platforms but also, retaining the ability to break it down by a meaningful resource unit (table, index, column family, job, etc).
As the source of truth for cost data, AWS billing is categorized by service (EC2, S3, etc) and can be allocated to various platforms based on AWS tags. However, this granularity is not sufficient to provide visibility into infrastructure costs by data resource and/or team. We have used the following approach to further allocate these costs:
EC2-based platforms: Determine bottleneck metrics for the platform, namely CPU, memory, storage, IO, throughput, or a combination. For example, Kafka data streams are typically network bound, whereas spark jobs are typically CPU and memory bound. Next, we identified the consumption of bottleneck metrics per data resource using Atlas, platform logs, and various REST APIs. Cost is allocated based on the consumption of bottleneck metrics per resource (e.g., % CPU utilization for spark jobs). The detailed calculation logic for platforms can vary depending on their architecture. The following is an example of cost attributions for jobs running in a CPU-bound compute platform:
S3-based platforms: We use AWS’s S3 Inventory (which has object level granularity) in order to map each S3 prefix to the corresponding data resource (e.g. hive table). We then translate storage bytes per data resource to cost based on S3 storage prices from AWS billing data.
We use a druid-backed custom dashboard to relay cost context to teams. The primary target audiences for our cost data are the engineering and data science teams as they have the best context to take action based on such information. In addition, we provide cost context at a higher level for engineering leaders. Depending on the use case, the cost can be grouped based on the data resource hierarchy or org hierarchy. Both snapshots and time-series views are available.
Note: The following snippets containing costs, comparable business metrics, and job titles do not represent actual data and are for ILLUSTRATIVE purposes only.
In select scenarios where the engineering investment is worthwhile, we go beyond providing transparency and provide optimization recommendations. Since data storage has a lot of usage and cost momentum (i.e. save-and-forget build-up), we automated the analysis that determines the optimal duration of storage (TTL) based on data usage patterns. So far, we have enabled TTL recommendations for our S3 big data warehouse tables.
Our big data warehouse allows individual owners of tables to choose the length of retention. Based on these retention values, data stored in date- partitioned S3 tables are cleaned up by a data janitor process which drops partitions older than the TTL value on a daily basis. Historically most data owners did not have a good way of understanding usage patterns in order to decide optimal TTL.
The largest S3 storage cost comes from transactional tables, which are typically partitioned by date. Using S3 access logs and S3 prefix-to-table-partition mapping, we are able to determine which date partitions are accessed on any given day. Next, we look at access(read/write) activities in the last 180 days and identify the max lookback days. This maximum value of lookback days determines the ideal TTL of a given table. In addition, we calculate the potential annual savings that can be realized (based on today’s storage level) based on the optimal TTL.
From the dashboard, data owners can look at the detailed access patterns, recommended vs. current TTL values, as well as the potential savings.
Checking data costs should not be part of any engineering team’s daily job, especially those with insignificant data costs. To that regard, we invested in email push notifications to increase data cost awareness among teams with significant data usage. Similarly, we send automated TTL recommendations only for tables with material cost-saving potentials. Currently, these emails are sent monthly.
What is a resource? What is the complete set of data resources we own?These questions form the primary building blocks of cost efficiency and allocation. We are extracting metadata for a myriad of platforms across in-motion and at-rest systems as described earlier. Different platforms store their resource metadata in different ways. To address this, Netflix is building a metadata store called the Netflix Data Catalog (NDC). NDC enables easier data access and discovery to support data management requirements for both existing and new data. We use the NDC as the starting point for cost calculations. Having a federated metadata store ensures that we have a universally understood and accepted concept of defining what resources exist and which resources are owned by individual teams.
Time trends carry a much higher maintenance burden than point-in-time snapshots. In the case of data inconsistencies and latencies in ingestion, showing a consistent view over time is often challenging. Specifically, we dealt with the following two challenges:
When faced with a myriad of data platforms with a highly distributed, decentralized data user base, consolidating usage and cost context to create feedback loops via dashboards provide great leverage in tackling efficiency. When reasonable, creating automated recommendations to further reduce the efficiency burden is warranted — in our case, there was high ROI in data warehouse table retention recommendations. So far, these dashboards and TTL recommendations have contributed to over a 10% decrease in our data warehouse storage footprint.
In the future, we plan to further push data efficiency by using different storage classes for resources based on usage patterns as well as identifying and aggressively deleting upstream and downstream dependencies of unused data resources.
Interested in working with large scale data? Platform Data Science & Engineering is hiring!
Learn about Netflix’s world class engineering efforts…
1.2K 
1
1.2K claps
1.2K 
1
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
Written by
Learn more about how Netflix designs, builds, and operates our systems and engineering organizations
Learn about Netflix’s world class engineering efforts, company culture, product developments and more.
"
https://medium.com/google-cloud/security-advice-for-gcs-buckets-6a6e9825d49f?source=search_post---------252,"There are currently no responses for this story.
Be the first to respond.
Almost every non-trivial website will have storage needs. As a Google Developer Advocate, I’ve been using Google Cloud Storage and talking to customers about their issues and difficulties with GCS. Security has come up a couple of times in these discussions. While I can’t tell you how to make your buckets and objects 100% secure, I can give you some basic advice that is pretty universally applicable. The vast majority of this information came from a week of research into the security of GCS buckets in a typical use case that I did in early December.
I know it seems obvious that you shouldn’t share passwords, service accounts, API keys, or other secrets. Yet, leaked credentials are a common source of security issues. The accepted best practice is that each individual and each application should have their own credentials. This makes it easy to remove one person’s permissions without affecting any of the other users of the buckets. It also gives you visibility into who is doing what with your audit logging. But just assigning everyone an account isn’t sufficient. There have been high profile leaks because one engineer’s credentials got leaked. Ensure that credentials are getting stored securely enough for your use case. A piece of paper under your keyboard tray isn’t secure, but it is a lot more secure than checking credentials (or dotfiles) into a public source repository. And if you use password managers or check credentials into source then the credentials are only secure as the password manager or source repository.
It wasn’t obvious to me when I started storing data in the cloud that bucket names are often discoverable by anyone. Complicating this is that in GCS and other cloud storage providers bucket names must be global unique. Under these constraints it feels like a good idea to put your project, company, application, or even personal credentials into the bucket name. I used to do this since it was easy to come up with a unique bucket name that had “aja-“ as a prefix. Once I realized that a sufficiently motivated individual could probe my buckets I took all personal information out of bucket names. Depending on you access the contents of a bucket the object name may also appear in URLs, and therefore in browser or firewall logs. Again, this is another good reason to not put secrets into object names. In the official storage best practices doc Google Cloud recommends using guids for bucket names if you need a lot of buckets.
My last tip is to regularly audit permissions and ACLs. I’ve found misconfigured buckets when I double check permissions on my project. Maybe you were setting up the proof of concept and made your bucket globally readable while you sorted out the rest of the application. Most of us do stuff like that occasionally, but it becomes a problem if that bucket lives on into production with the same permissions. With GCS most projects use IAM permissions, but you may need to use ACLs lists if you need to have different permissions for objects in the same bucket. When I was digging into GCS permissions, I discovered that I had made incorrect assumptions about what different scopes meant. For example, I had assumed the “AllAuthenticatedUsers” scope meant all the users of my cloud project. When I read the documentation, I realized that it meant anyone with a Google account. If I want to limit access to just collaborators on my project I can do that with other scopes. Of course, many projects have valid reasons for making a bucket globally readable. Perhaps you are using the bucket to store assets for your web application, or maybe you are using a bucket to store a scientific data set that you are sharing with others. Just make sure that the permissions are appropriate for your use case and set a reminder to audit them every so often to ensure that your use case hasn’t changed.  This blog post only covers a few points. Before you take your app to production, I recommend reading more information on best practices for Google Cloud Storage on the Google Cloud site. That document is kept up to date, unlike a blog post.
01/17/18
Originally published at www.thagomizer.com on January 17, 2018.
Google Cloud community articles and blogs
8 
1
8 claps
8 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Rubyist, Data Nerd, Lazy Dev, Stegosaurus. Cloud Developer Advocate @ Google. Disclaimer: My opinions are my own. *RAWR*
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.helium.com/introducing-helium-channels-47f38b45aa42?source=search_post---------237,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Recently we made the product decision to wind down the cloud storage component of the Helium offering. The Helium Cloud was a way for IoT developers to store and retrieve data from a Helium-managed storage layer. While we had a lot of users relying on this during prototyping, we often ran into the same question when a customer was operationalizing their application or service: “Can you just send the data directly to my cloud?” This is not an absurd request. For many good reasons most large organizations have standardized on one (or several) cloud environments, so us asking them to add another layer of storage was neither convenient nor was it adding too much value.
So, in response to this usage pattern, we’re introducing Helium Channels. A Helium Channel is a pre-built, secure connection from Helium-powered devices straight through to your preferred cloud (or clouds). Put another way, Channels are a convenience layer between sensors and your organizations’s cloud of choice. A Channel can connect to public clouds — AWS IoT, Azure IoT, Google Cloud Platform, etc — or a private cloud. Or both in parallel.
Channels are made possible by three pieces of the Helium infrastructure working together:
Though they each do it slightly differently, the primary feature set for the IoT offerings across the three major Cloud vendors — AWS IoT, Azure IoT, and Google Cloud IoT Core — revolves around enabling users to manage IoT devices at scale in a secure way. AWS IoT, for example, relies heavily on the AWS IoT Gateway. Authenticated device traffic is routed through one of these Gateways, and from there, a AWS Thing Registry exposes an API for actual device management. After you’ve completed this integration, you can then move device data to additional AWS cloud services for processing, storage, analysis, etc.
Unfortunately these integrations are still too onerous for your typical developer. We see it with our customers who are using these very products in production. Even if you manage to master the nuances of the Azure IoT Hub (or do you need the Azure Event Hub?), you’re still on the hook to build X.509 certificate-based authentication across all your devices.
Helium Channels erase all this complexity for the IoT developer. Helium has done the work to handle device creation and ensure security on your behalf, all abstracted behind a simple UI.
When a user creates a Channel for a Cloud provider in the Helium Dashboard, here’s what happens behind the scenes:
That’s it. The entire process takes a matter of seconds. And it’s all transparent to the user.
It’s important to be conscious of the need for every device to have an individual, hardware-authenticated identity. It’s the only way to be sure that data came from a given device, and that data can be sent securely over the air back down to it. Some other platforms combine all individual devices into a single entity when routing data to cloud providers. This is critically flawed, as it makes it almost impossible to have a verifiable root of trust through the communication chain, to send cloud-to-device data, and make use of cloud-provided “device shadow” or “twin” functionality.
As outlined above, Channels are an optimized convenience layer between your devices and the cloud. Integrating with the security schemes specifically is where we have done extra work to ensure that IoT developers aren’t wasting any time implementing requirements that, while absolutely necessary, can certainly be painful to do right.
With that in mind, at a very high level, here are the recommended device security mechanisms for each of the IoT cloud vendors and how Helium implements them in our Channels architecture:
(We’ll be publishing more specifics on each of these Channel security implementations in the near future on the Helium Developer Site.)
Another common requirement from enterprises building device-based applications is the ability to route data to multiple clouds in parallel. With Channels, this is easy. Once your sensor data is generated at the edge and sent to the Helium routing infrastructure, you can configure any number of Channels that will route data in parallel on your behalf.
We’ve built the first set of Channels based on demand and feedback from customers and the community. The list of supported Channels available in Helium Dashboard is:
In addition to the dedicated IoT Cloud Products, we’re also providing generic, protocol-level Channels for the following:
All of these are configurable with just a few clicks in Helium Dashboard and will require no tuning save for a few bits of account and platform details for the respective IoT Cloud products and protocol endpoints. We think this is extremely powerful.
If you only remember three things about Channels make it the following:
We’re rolling out access to the revamped Helium Dashboard as we speak. To get access, you’ll need the latest Helium Hardware. The Helium Starter Kit will give you full access to the platform for just $79 and is the easiest way to prototype and end-to-end IoT application.
Enjoy And if you have any questions about Channels, hardware, or anything else Helium, join us in chat.helium.com or get in touch with our team.
Building the world’s first decentralized wireless network
11 
11 claps
11 
Written by
VP of Business Development at @helium; Basho OG. - mark@helium.com
Building the world’s first decentralized wireless network
Written by
VP of Business Development at @helium; Basho OG. - mark@helium.com
Building the world’s first decentralized wireless network
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@openbom/connecting-cloud-storage-to-bill-of-materials-in-openbom-6771b3676c9d?source=search_post---------145,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Sep 8, 2017·2 min read
The preview of our new functionality, connecting cloud storage and keeping files connected to a BOM resonated among our users. Thank you for your feedback and sharing your ideas. Cloud file storage is mainstream and a secure way to store different types of files. In product development and manufacturing, these files types could be CAD, specifications PDFs, production instructions pictures, graphic resources and so on.
The coming version of openBoM will allow users to connect to Google Drive within their BOMs. And immediately following that release, the next one will include support for Dropbox and most likely Microsoft OneDrive, which has been asked for by significant number of our users.
We also are considering to support a connection to specialized CAD file cloud collaboration storages. If you use or have preference for a particular one, please let me now. Examples include GrabCAD and Autodesk A360. Maybe you’re using Onshape to store CAD files in the cloud. Let us know if that’s important to you as well.
Here are few typical use cases to use connected cloud storage with openBOM:
1 — Connect to CAD files in native and sharable format (PDF, JT, etc.)
2 — Connect to Part specification in contractor or supplier catalogs
3 — Connect to documents or pictures to help with installation
We are actively thinking of future applications of how to support our connection to cloud storage for CAD integrations. One idea is to generate a PDF or other neutral CAD data content at the time an initial BOM (part list or multi-level BOM) is created. What do you think?
Conclusion. We are looking forward expanding and supporting a variety of scenarios with cloud storage. This seems to be demanded and widely used by many of you. Please share with me your use case stories. I’d love to hear more and think how can we support them.
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mhausenblas/minio-amazon-s3-compatible-object-storage-under-your-control-4d975ec4d5f1?source=search_post---------353,"Sign in
Michael Hausenblas
Dec 27, 2016·2 min read
TL;DR: we’ve been working with the awesome folks from Minio to make it available on DC/OS. I walk you through the setup and some basic usage.
Minio is an Amazon S3 compatible object storage offering you can run wherever you like. It comes with the familiar abstraction of buckets and files in it and is secure by default.
To install Minio in DC/OS, have a look at the Minio usage examples if you like, but the gist is:
After that, Minio is running:
Next, I installed mc—the Minio command line client—and carried out some basic operations against Minio running in my DC/OS cluster:
Minio has a minimalistic yet beautiful UI:
Last but not least I want you to see what Minio’s resource footprint looks like:
That’s it! Give it a try and let us know what we can improve here and we’ll work together with our friends from Minio on it.
Solution Engineering Lead in the AWS open source observability service team
44 
Some rights reserved

44 claps
44 
Solution Engineering Lead in the AWS open source observability service team
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/hosting-a-static-website-on-google-cloud-using-google-cloud-storage-ddebcdcc8d5b?source=search_post---------33,"There are currently no responses for this story.
Be the first to respond.
In this mini series we are covering, how to create websites on Google Cloud. This is the second article in the series.
Let’s say you’re a small company looking to set up a basic web presence. You don’t expect a lot of traffic, and don’t want to pay a lot to get up and online. So, read on to find how to start small by deploying a static website.
Static websites are a good option for sites like blogs — where the page rarely changes after it has been published, or where there isn’t any dynamically-generated content. Static web pages can contain client-side technologies such as HTML, CSS, and JavaScript. They cannot contain dynamic content such as server-side scripts, like PHP.
Before we begin, we need to make sure we are properly set up in Google Cloud Console. Ensure the following:
Now that we’ve covered the logistics, there are four steps to set up a static web app.
First step is to connect your domain to cloud storage. To do that, create a CNAME record that points to c.storage.googleapis.com. Your domain registration service can help with this.
www.example.com CNAME c.storage.googleapis.com.
Second step is to create a Google cloud storage bucket. To do this, browse to the google cloud console, click on cloud storage, and click create bucket. Make sure the bucket name matches the CNAME created for your domain. In this case, the storage bucket should be named www.example.com
Third step is to upload the files you want your website to serve. We can do this in one of two ways.
First, by directly uploading files using the google cloud console.
Second, is using gsutil command line, which is great for when you have an existing website.
gsutil rsync -R local-dir gs://www.example.com
Once the files are uploaded, we need to make sure they are properly shared for access. You can either make all files in your bucket publicly accessible, or you can set individual objects to be accessible through your website. Generally, making all files in your bucket accessible is easier and faster.
At this point, we essentially have a functioning static website but it is recommend to assign an index page suffix and a custom error page to guide the users better.
This helps in scenarios when say you have no file named apple in your bucket www.example.com. In this situation, if a user requests the URL http://www.example.com/apple, Cloud Storage attempts to serve the file www.example.com/apple/index.html. If that file also doesn’t exist, Cloud Storage returns an error page with 404 response.
You can learn more about setting this up here.
The last step is to test our static website. Verify that content is served from the bucket by requesting the domain name in a browser.
One important thing to note is that GCS ONLY supports HTTP. In order to serve HTTPS, and get all that security goodness, we need to either using direct URIs using a CNAME redirect, or migrate to using a load balancer as a front-end as shown in the picture.
In this article we learned how to start small and deployed a static web application on Google Cloud platform. We used Google cloud storage to host the static content and pointed the Domain name to the storage bucket.
Google Cloud community articles and blogs
205 
2
205 claps
205 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@alibaba-cloud/how-important-is-caching-to-a-website-bfed7dd739ec?source=search_post---------311,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 4, 2017·1 min read
Most people use cloud storage apps, such as Object Storage Service (OSS), for images or other large files. However, cloud storage is not intrinsically limited to a single use case but merely provides a platform for us to solve a wide range of problems. In this article, we will explain how to use Alibaba Cloud OSS to solve the problem of insufficient HTML cache space.
Caching is crucial for websites and can solve most complex performance issues. There are many types of cache, the most common ones being the memory cache and the disk cache (page cache). Memory cache is quicker and is typically used to store critical and hot content, but it cannot be used to store large amounts of data. Disk cache on the other hand is much slower because it employs disk I/O. However, the disk cache can be used for performing complex database queries, processing business logic, and accelerating page loading.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@thelogocreative/4-reasons-why-cloud-storage-is-a-necessity-in-2020-b47a5e537227?source=search_post---------176,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Logo Creative™ ✏
Oct 19, 2020·5 min read
It is impossible to have missed the meteoric rise of cloud-based services, and the use of them goes even beyond the IT industry. In this article we share 4 Reasons Why Cloud Storage Is a Necessity in 2020.
Not so long ago, people had very little idea of what “cloud” meant in the context of the online world, and storing and exchanging files was still predominantly conducted on different social networks or through the use of storage drives.
However, in today’s digital world, cloud storage has proven itself to be the number one storage and information exchange solution.
To help anyone who is still unclear about the huge advantages that cloud-based storage provides, here are 4 reasons why cloud storage is a necessity in 2020.
Everybody understands the importance of top security online. In the digital world, people are all too aware of the threats to their digital security.
However, far too few spend the effort to take the initiative to prevent security breaches from happening.
Experts from BackBlaze.com explain that a lot of the time, people protect their local storage on systems that are not totally secure, or even worse, they use drives that can fail at any moment.
A system crash or virus attack can have extremely detrimental consequences on your files but synchronizing your files with a cloud storage service, especially one with a tight-knit ecosystem, will help to keep them safe.
All you have to do is follow the cloud provider’s instructions on setup and file transfer, and everything else will be handled on your behalf.
Unlimited cloud storage is significantly more affordable than having to buy and maintain a lot of hard drive storage space in the long run.
Hard drive technology is not static, and every few months, businesses have to upgrade their drives to the latest-and-greatest versions in order to take advantage of all the features that they bring.
There is also the risk of breaking physical storage devices and having to replace both the device and the files.
With cloud storage, the only expense that you will have to pay for is for the space you need, and cloud providers offer very attractive and affordable deals for their users.
For text documents and smartphone files, platforms will allow you 2GB of storage space for free — and that is more than enough for those types of data.
If your work demands more space, and you want to be able to use the platform on different devices — unlimited space offers from providers usually start from around $99 per year.
Being the pinnacle of security and affordability, cloud storage is also an ideal method for instant data exchange.
Instead of having to keep your hardware active online on a 24/7 basis, using a CDN-based service like cloud storage allows you and anyone else with permission to access files from anywhere at any time.
This makes cloud storage an ideal tool for both distant and in-house work.
Whether you are a freelancer who works from home or someone who works in a traditional office setting, you benefit from all the cloud advantages and access and exchange data more effectively.
In this way, cooperation between different people in an organization can become more efficient, leading to a more united workflow and better overall results.
The importance of staying prepared for unfortunate incidents cannot be stressed enough.
While online risks like system breakdowns or virus attacks can be reduced with quality cybersecurity, anyone can lose or accidentally break a USB stick or hard drive.
The loss of private content can be a huge problem for both individuals and businesses.
Using cloud storage as a depository for your files to ensure that you will be able to access them in case your devices get broken or lost is the ideal way to backup your important data.
Remember to keep everything updated by turning the auto-update settings on, and both your device and your cloud backup will have an extra level of protection.
Cloud storage is very effective, especially in the digital information age that we are living in right now.
It has made various other storage solutions essentially obsolete, and this is because it provides so many totally unique benefits.
Whether you are looking to get involved in big data or simply trying to keep your work files in order, cloud storage is a must-have for everybody in 2020.
With the protection and services of a quality cloud service, you will be able to rest assured that all of your important data is secure, accessible, and protected.
We hope you have enjoyed these 4 Reasons Why Cloud Storage Is a Necessity in 2020.
If you would like more personal tips, advice, insights, and access to our community threads and other goodies join me in our community.
You can comment directly on the posts and have a discussion with Andrew, the Founder of The Logo Creative.
*TIP — We recommend Skillshare to learn online. There are tons of classes for everything including graphic design, web design, marketing, branding and business related courses. Get a free trial with our link and you won’t regret it Trust us!
Originally published at The Logo Creative | International Logo Design & Branding Studio.
Award Winning International Brand Identity Design Studio: Creating Adaptable Visual Identities that are Memorable & Timeless. Working With Clients Globally.
See all (220)
Award Winning International Brand Identity Design Studio: Creating Adaptable Visual Identities that are Memorable & Timeless. Working With Clients Globally.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/crustnetwork/a-brief-introduction-to-crato-the-cloud-storage-solution-of-the-new-era-ed3c0b95502?source=search_post---------161,"There are currently no responses for this story.
Be the first to respond.
Crato is a decentralized distributed cloud storage solution based on blockchain, provides Decentralized Storage service based on IPFS and Crust.
Crato focuses on providing more efficient space for cloud storage infrastructure and a more open expansion environment for cloud storage applications. Crato is committed to becoming a powerful middle platform in distributed cloud storage network architecture and enabling a new storage ecosystem in the era of blockchain
What is IPFS?
IPFS (Inter Planetary File System) a P2P storage network that every two nodes can exchange their data via DHT (Distributed Hash Table) protocol. In IPFS network a file is divided into numbers of blocks each has a unique CID (Content Identity). A file block contains its children’s CIDs. It can search and get the blocks by its children’s CIDs via DHT. This process runs recursively until a whole file downloaded.
The Advantage of IPFS
IPFS’s will replace the current used HTTP protocol to access the Web. People use IPFS and content with Hash value to access web pages instead of the existing HTTP and domain name method. As long as there is a web page on the IPFS network, no matter where the node is moved to, the web page can be accessed. This makes the web content exist longer and effectively.
IPFS can also be used as a CDN because its thermal data is copied all over the world.
What is Crust?
Crust implements the incentive layer protocol for decentralized storage. It is adaptable to multiple storage layer protocols such as IPFS, and provides support for the application layer. Crust’s architecture also has the capability of supporting a decentralized computing layer and building a decentralized cloud ecosystem.
The relationship between Crust and IPFS is quite similar to the relationship between incentive layer and storage layer. Filecoin protocols encourage nodes in the network to provide storage capacity through incentives, which IPFS can exactly provide this kind of storage ability.
Crato — Easy, Stable, Economic!
Crato provides Decentralized Storage service based on IPFS protocol and Crust.
Carto prevents the potential problems of the centralized storage server.
Crato provides optional encrypting service to secure the sensitive data.
Carto provides basic competence and the solutions under certain circumstances, and reaches a win-win situation for our users and partners.
Functions of Crato
1. To provide optional encrypting service to secure the privacy and safety of the data
2. To make sure a stable storage on IPFS
3. To help the customers to avoid the risk of service price fluctuation
4. To provide user-friendly SDK
How Crato Works?
1. We provide storage service to our customer by using Centralized Storage + Decentralized Storage. Customers don’t have to pay close attention to IPFS and the chain. Users just need to upload the data and the centralized server will assign tasks.
2. We use multiple ways to encrypt the sensitive data. Users can operate the encrypting procedure on their own side. Even though centralized server is used, the encryption key of the files will not be stored directly.
An Early Preview of Crato
pan.nashcloud.cn
Contacts
The Crato Team
Email: CratoCloud@outlook.com
Decentralized Cloud Blockchain Technology
2 
2 claps
2 
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/what-happens-when-you-upload-a-file-to-a-decentralized-network-54199d241014?source=search_post---------279,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Aug 15, 2019·5 min read
When people first hear about how decentralized cloud storage works, it sounds like a joke. Common questions include:
Let me tell you, it’s real and it’s spectacular!
When files are uploaded to the Storj network, the first thing that happens is the Uplink, which runs on the local machine uploading the files to the network, determines if the file is small enough to be stored as an inline segment. I will discuss the difference between inline and remote segments later in this post but for the rest of this section assume we are referring to a remote segment. The Uplink then sends an upload request to the Satellite, which includes the Uplink’s API key so that the Satellite can validate if that Uplink is authorized to upload data to the project and bucket that it requested. If there are no issues with the authorization, the Satellite then compiles some information to send back to the Uplink.
This information includes a list of storage nodes (where the data can be stored) and an order limit for the Uplink. The list of storage nodes will include nodes with varying levels of reputation, as well as the information that the Uplink needs in order to establish a connection with them such as IP/port. Check out our blog about reputation to read more about what type of criteria determines whether a node is reliable or not, and how the network disqualifies poorly behaved nodes. The Uplink will use the order limit as proof that the Satellite authorized its uploads when the Uplink establishes connections with storage nodes. Order limits are like an empty tank that the Satellite gives the Uplink. As the Uplink uploads more data over time, that empty tank will begin to fill. Once it reaches its limit, the Uplink won’t be able to upload any more, because it has reached the amount that was agreed upon with the Satellite. While this seems like a lot of back and forth, this all happens in microseconds.
At this point, the Uplink has been given the green light to store data on the network. From the order limit and other info shared by the Satellite, It creates orders for each of the storage nodes. The storage nodes will later send these orders to the Satellite.
Next, the Uplink begins breaking down the file into segments, then stripes, after which it erasure codes each stripe to produce erasure shares. The Uplink then concatenates the erasure shares into pieces. This may seem like overkill, but it’s actually a key process of the network because it enables buffered file streaming by allowing the downloading, decrypting and recompiling of pieces of files, rather than waiting for the entire file to be downloaded first. These pieces are what is uploaded to storage nodes, and at this time the network begins to send all the pieces of the file to each storage node, in parallel. Because a 1 TB file might be uploaded to a thousand different nodes, the process goes very quickly as there is generally not a bottleneck on the storage node side.
When the storage nodes receive the pieces, they hash them and send the hash of each piece back to the Uplink in a signed message. With the message signed by storage nodes, the Satellite can later audit both Uplinks and storage nodes for honest behavior.
The Uplink hashes all of the hashes it received from the storage nodes and then sends them to the Satellite, which stores a pointer to the file in its database. The pointerDB contains information about which storage nodes are storing which pieces. That way when the Uplink needs to download the file, the Satellite can tell it which storage nodes are holding the pieces it needs to download.
Before the Uplink first contacts the Satellite, it must determine how big the file is and how big its pieces would be. In some instances, files might be so small that the metadata to store them (which keeps track of where the file’s pieces are located and the corresponding NodeIDs) would actually be larger than the file itself. If this is the case, the Uplink and Satellite would determine that the file should actually be stored inline on the Satellite, rather than distributed across a network of storage nodes. This maximizes the efficiency of costs, and we don’t foresee this occurring very regularly. These inline segments are also encrypted client-side for security and privacy. Remote segments are those that will be erasure encoded and distributed across the network. A remote segment is larger than the metadata required to keep track of its bookkeeping.
As storage nodes go offline, the Satellite is responsible for “repairing” those missing pieces on the network. To execute a repair, the Satellite determines how many pieces a file has on the network and replenishes those that are missing if they fall under a threshold. This ensures that the file’s durability is maintained. If you want to learn more about our repair system, you can check out this blog.
When a user decides they need to download a file, the network begins the short process of downloading and assembling a file from its various pieces. The Uplink contacts the Satellite to begin the process of downloading the file. The Satellite returns data about the file’s pieces, specifically a list of nodes, with IP and port addresses, storing pieces and other metadata that helps the Uplink in the file download process. The Satellite returns a list of nodes that the Uplink may choose to download from.
Once the Uplink receives the data it needs from the Satellite, it reaches out to the storage nodes storing file pieces and begins downloading them in parallel. If an Uplink needs 10 file pieces to rebuild a file, it will actually download 12 or 13 pieces, to accelerate the file download. That way the file can be rebuilt once the fastest 10 nodes complete the delivery of their corresponding file pieces. Once the needed file pieces are downloaded, the Uplink cancels any remaining downloads, reconstitutes the file from its pieces, unencrypts the file, and serves it up.
All of these steps may seem super complex, but they happen in microseconds, faster than what one would experience on a legacy, centralized cloud storage solution. Building decentralized systems is definitely not the easiest challenge to solve, which is why our team is committing so many resources to engineer the network. We are very passionate about building decentralized/distributed systems that are not only easy-to-use but better in nearly every way when compared to their centralized counterparts. From uploading your first file to building a web-scale application on top of the network, we hope you enjoy your journey down Decentralization Drive.
By Brandon Iglesias, Storj Labs
Originally published at https://storj.io on Apr 23, 2019
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
See all (1,648)
3 
3 claps
3 
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/google-cloud-platform-and-flutter-mini-course-released-9193efebb8b0?source=search_post---------348,NA
https://medium.com/micro/launching-m3o-space-435e2bb158ac?source=search_post---------323,"There are currently no responses for this story.
Be the first to respond.
Today we’re announcing M3O Space aka infinite cloud storage.
To get started just head to M3O, signup and check out the Space service.
The space API is effectively what we think of as CRUD for infinite cloud storage. It’s backed by S3 and provides one place to put all your images, files, videos, etc. But why not just use S3 directly? Personally we found the S3 api at this point to be pretty cumbersome. It’s a building block for infrastructure but maybe not how we should directly interact with a storage API today.
Here’s how you’d create an object in space:
A public url will then be returned like:
https://space.m3ocontent.com/micro/3db66283-55b5-4e6e-9c83-de9e53959db0/images/file.jpg
To read small files directly though the API
To create private objects just specify visibility=private at time of creation, then download it directly through the API.
And that’s it. Signup for free at m3o.com/register or join the community on Discord to learn more.
Thanks for reading!
Programming the real world
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
Written by
Working on Micro
Consume public APIs as simpler programmable building blocks for a 10x developer experience. Signup for free 👉 https://m3o.com/register
"
https://medium.com/pcmag-access/what-is-icloud-apples-cloud-based-subscription-service-explained-b88dfd5a98b7?source=search_post---------221,"There are currently no responses for this story.
Be the first to respond.
ICloud+ kicks in extra Apple cloud storage as well as security and privacy features. Here’s how it works.
By Lance Whitney
ICloud has long been Apple’s online service for backing up, syncing, and sharing your online files. With the release of iOS/ iPadOS 15, Apple has unveiled an expansive upgrade to iCloud’s paid subscription, dubbed iCloud+, that adds exclusive features aimed at enhancing your security and privacy online.
These additions include Private Relay, Hide My Email, Custom Email Domain, and HomeKit Secure Video. The plan is accessible from your iPhone, iPad, Apple Watch, Mac, and Apple TV, and can be shared with as many as five family members.
Apple’s use of the term iCloud to describe a variety of apps and services is confusing. So what’s the difference? First, iCloud is Apple’s standard file backup and syncing service. You’re able to back up and synchronize your photos, email, contacts, calendars, notes, reminders, messages, and other content online. The basic version of iCloud offers 5GB of online storage for free.
iCloud+, meanwhile, is the paid upgrade that becomes available once you update to iOS 15 or iPadOS 15. It includes tiers of 50GB, 200GB, and 2TB of iCloud storage, plus several enhanced features that are not accessible through the free 5GB plan.
Apple’s online storage site is called iCloud Drive, which is where your backed up and synced content resides. You can also upload, download, and share files directly with iCloud Drive.
The biggest new addition to iCloud is Private Relay, Apple’s take on a VPN aimed at keeping your internet activities private and protected when using Safari. However, it differs from traditional VPN services by replacing your IP address with one from a range of anonymous addresses based on your general region. Your internet traffic is sent through two separate hops, or relays, by two separate companies, so no single entity (not even Apple) can monitor your online activity.
Hide My Email is an improved version of the existing Sign in with Apple feature that lets you use an anonymous Apple ID to register with certain apps and websites. Whereas Sign in with Apple works only with supported apps and websites, Hide My Email allows you to use a random email address for any website or online form to cut down on spam.
Designed for anyone who already has their own personal domain name, Custom Email Domain allows you to send and receive iCloud Mail using your own domain name, instead of an icloud.com address. iCloud Mail supports as many as five personal domains with up to three email addresses per domain.
iCloud+ also has more options for those who use HomeKit Secure Video. Depending on your subscription, you get 50GB of iCloud storage and support for recording video from one HomeKit Secure Video camera, 200GB of storage and support for up to five HomeKit Secure Video cameras, or 2TB of iCloud storage and support for unlimited HomeKit Secure Video cameras.
Don’t know what subscription tier you have? After upgrading to iOS/iPadOS 15, you can check your plan or upgrade to a paid version on your iPhone or iPad. Open Settings > [your name] > iCloud. If the Storage section at the top says iCloud+, you’re good to go. If it says iCloud, then you’re on the free plan.
To upgrade to a paid subscription, tap Manage Storage, then select Change Storage Plan. Choose one of the available three plans—50GB for 99 cents a month, 200GB for $2.99 a month, or 2TB for $9.99 a month—then select Upgrade to iCloud+ to activate the new plan. iCloud+ is also included with all three Apple One subscriptions.
You can enable iCloud Private Relay under Settings > [your name] > iCloud > Private Relay (Beta). Turn on the switch next to Private Relay (Beta). From there, tap the IP Address Location option.
There are two choices. Tap Maintain General Location to use an IP address based on your overall location so that you can still see local content when you use Safari. Choose Country and Time Zone to use an address based on a much larger and broader location derived from your country and time zone.
You can now browse the web on Safari. If a site supports Private Relay, everything should work normally. Websites that do not may show content for the wrong region, add an extra step for logging in, or not work at all.
If you run into trouble with Private Relay, or want to turn it off, disable the feature completely or just for certain networks. For turning it off, go back to Settings > [your name] > iCloud > Private Relay (Beta). Turn off the switch next to Private Relay (Beta).
To turn it off for certain Wi-Fi networks, go to Settings > Wi-Fi and tap the More button next to a specific Wi-Fi network. Turn off the switch next to iCloud Private Relay.
You can also turn off Private Relay for any cellular connection. Go to Settings > Cellular > Cellular Data Options. Turn off the switch next iCloud Private Relay.
To use the Hide My Email feature, go to Settings > [your name] > iCloud > Hide My Email. You can also go to your iCloud settings page and select the Manage button for Hide My Email.
The Hide My Email page shows any random addresses you have already used with the Sign in with Apple feature. Tap an address that you wish to use again as a way to hide your real email address, then confirm that it is forwarding to your actual email address.
You can also tap Create New Address to generate a new email. A new random address will automatically be generated with the icloud.com domain name. If you’re okay with the address, tap Continue. Otherwise, choose Use Different Address and another email will be created.
Once an address is selected, you can then enter a label to help you remember this address. Tap Done at the All Set screen. Now the next time you need to create an online account or fill out a form, you can enter this random address.
Any emails sent to this account will be forwarded to your actual address. But the good news is that you can deactivate the address if you’re receiving too much spam or other unwanted messages. Just go back to Settings > [your name] > iCloud > Hide My Email. Select the random address the mail is being forward from, then tap Deactivate Email address. Tap Deactivate to confirm your request.
You can set up a custom email domain from the iCloud settings page. Select the Manage button under Custom Email Domain. A window pops up asking whether you want to use the domain only for you or for your entire family.
Next, enter the domain name you want to use with iCloud then follow the required steps. You must enter the email addresses that you use with the domain.
The records with your domain registrar must also be updated, then verify that your domain and addresses have been set up with iCloud mail.
Your iCloud+ subscription and benefits can be shared with other family members. On your iPhone or iPad, go to Settings > [your name] > iCloud > Manage Storage > Share with Family.
Tap Share Storage Plan at the next screen, then select Send Invitation to compose a message that will be sent out to your family members. As each person accepts the invitation, they are added to your iCloud family.
Originally published at https://www.pcmag.com.
PC Magazine: redefining technology news and reviews since…
102 
102 claps
102 
PC Magazine: redefining technology news and reviews since 1982.
Written by

PC Magazine: redefining technology news and reviews since 1982.
"
https://medium.com/@alibaba-cloud/how-to-install-cloud-storage-gateway-cc20b147fbc1?source=search_post---------159,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 2, 2019·5 min read
By Oliver Zhang, Solution Architect
Cloud Storage Gateway (CSG) is a gateway that connects on-premises software applications with Alibaba Cloud data storage to provide seamless integration. CSG combines customers’ existing storage applications and workloads with virtual devices. These virtual machines are compatible with industry-standard storage protocols and can be deployed to customers’ on-premises environment and in the cloud. With CSG, customers can easily access Alibaba Cloud data storage and computing services.
This article will discuss the hands on install of Cloud Storage Gateway in customer’s in-house IT environment.
Available CSG Storage Types:
There are 2 types of storage gateway and 3 protocols available. File type CSG supports NFSv3, NFSv4 and SMB. Block type CSG supports iSCSI protocol.
Create a new gateway cluster by clicking on the Create Gateway Cluster button. Give it a name and description (optional). For our example, we will be calling the cluster csg2.
In the csg2 Gateway Cluster, create a new gateway by pressing on the Create Gateway button.
We will name our gateway csgtest. Since we are performing this installation on the customer’s end, select “On-premise” for Location.
Choose your desired image based on your business requirements.
Make sure to click “Download Certificate” to activate CSG.
Once downloaded, click “Completed” to proceed. You should see your new gateway in the console.
The method and file for installation vary with hypervisors. You can obtain the installation file from Alibaba Cloud Customer Service. You can learn more about the various installation methods on this link: https://www.alibabacloud.com/help/doc-detail/70277.htm
After installing CSG, access the CLI console to configure the network.
Select Configure the Network in the console.
Enter your static IP address.
Configure the DNS.
Configure the NTP server by selecting Use NTP server.
If no NTP server is available, Alibaba Cloud NTP server is used by default.
Give the following access to the RAM user on the target bucket.
Initialize the CSG via https://csg_ip
Cloud Storage Gateway is an ideal product to use when a customer wants to move large amount of data into cloud or out of cloud. It can extend customer’s on-premises storage into cloud and have unlimited capacity. Cloud Storage Gateway is not suitable for concurrent read and write applications, and is not for transaction systems and databases.
Reference:https://www.alibabacloud.com/blog/how-to-install-cloud-storage-gateway_594317?spm=a2c41.12451477.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@zeepin/galabox-crowdfunding-galahub-selection-start-today-dont-miss-the-opportunity-f7ac6200afee?source=search_post---------363,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zeepin
Aug 8, 2018·5 min read
At Zeepin Foundation, we’re developing a distributed new economy for creative industries.
It might sound abstract, but over the course of developing our chain and dApps, we’ve managed to design glimpses into this new economy. It’s swiftly becoming a reality soon to be explored by Zeepiners.
This article will provide a way for those who hold both large and small amounts of ZPT to benefit from our discoveries, so if you are one — make sure to read to the end.
We actively work on real solutions and products, making us one of the few Blockchain companies that have something to show to our community.
So our vision propels us and our industry forward. But in order to develop it, we had to dig to the very foundations of what an economy even is, digital or otherwise.
The Future of Data
We started from a blank slate, completely re-imagined the established way of doing things. Our way of thinking is simple: «What’s in the way of the creators currently and how can we help?».
This is what most of our dApps will boil down to, and we decided to announce GalaCloud among all others. We believe that a distributed storage network based on GalaBox and GalaHub nodes will be crucial for the new economy.
There is a good reason for this.
GalaCloud provides a way of handling the lifeblood of any digital ecosystem: data. If creators aren’t in control of their own data, no amount of decentralization is ever going to solve the problems that plague the industry.
Current cloud storage services can’t ever be in line with our mission, because they’re controlled by third-party systems and organizations. This means that until we solve the problem of centralized data storage, service providers will be the real owners of everything uploaded to the cloud.
GalaCloud is crucial for Zeepin Chain and our vision of a better economic environment for the creative industries.
You can read more about GalaCloud here: https://bit.ly/2OxVCGY
Because we understand the importance of this decision, we decided to put the fate of the system into the hands of our community. This is why we want Zeepiners to decide how GalaCloud is going to look like by voting with their ZPTs.
All we can do is make our case and provide an amazing value proposition to early adopters.
GalaBox Crowdfunding Campaign
GalaBoxes are the backbone of the GalaCloud storage system, basically its operation nodes. They’re encrypted storage devices that store shards of information uploaded by users. GalaBoxes can back up files and pick up instructions from the rest of the system. They provide rewards in Gala for successfully reading or writing data.
GalaBoxes are crucial for smooth operation of GalaCloud and make up the main pillars for the new distributed data storage system. This is why we decided to make them available for early adopters among Zeepiners and other interested parties.
GalaBoxes come free, if users agree to lock their ZPTs in storage for 18 months.
We have a total of 10,000 GalaBoxes to distribute through the first stage of crowdfunding.
So we’re asking ZPT holders who share our vision and understand the value of this device to vote with your tokens. This will both help us create a better product and secure your place in the GalaBox distribution process.
All GalaBoxes will be sent to their new owners during Q1 2019. This is one of the announcements we’ve been preparing for in the past few months, and we hope Zeepiners are as excited as we are.
In fact, we’re so happy with this idea that we decided to do a secret, early distribution phase.
August 8th 10 pm GMT+8 only for 24 hours, there’s early Bird offer for the GalaBox.
During these 24 hours, you only lock up 4999 ZPTs for a GalaBox.
The rest of GalaBox distribution will happen in three phases:
- Phase 1: Aug 12th — 15th. Price: 5250 ZPTs locked in storage for 18 months.
- Phase 2: Aug 16th — 20th. Price: 5500 ZPTs locked in storage for 18 months.
- Phase 3: Aug 21st-25th Price: 5750 ZPTs locked in storage for 18 months.
But only for 24 hours, you only lock up 4999 ZPTs for a GalaBox.
Don’t make this commitment unless you really do want to receive a GalaBox. This offer is for dedicated Zeepiners — those who share our vision for products with real world use cases.
To take advantage, simply commit your ZPTs by following instructions on the GalaCloud website that will be released when campaign starts.
GalaBoxes are a crucial part of GalaCloud service and function as main nodes for distributing and storing data, but you might be wondering where they get signals from and how. Let’s explore this.
GalaHub Selection Process
GalaHub nodes are like dispatch centers of GalaCloud. They configure storage space and enable searching for addresses, make sure all devices offer high-performance bandwidth. If GalaBoxes are the backbone of the network, then GalaHub nodes are the sensory organs — eyes, ears, etc.
They help coordinate the data storage network and help us reach the goal of creating a new digital economy by working as network servers.
This, once again, is where Zeepin community comes in.
We want to create an honest and fair solution to the data storage control mechanisms in the current system. This can only be done with the support of our current and future users.
GalaHub presents an opportunity to be a big part of the GalaCloud system and become one of such dispatchers, helping direct streams of data in the distributed network. You could set up one of the servers and receive substantial returns in Gala tokens when the application is in use.
Much like with GalaBoxes, participation only requires locking up ZPTs in storage for a period of 18 months. The minimum amount for GalaHub operators is 400,000 ZPT in storage.
49 participants will be selected between August 8th and Aug 25th to become GalaHub operators and support the distributed network. This is a serious commitment, which is why in return we promise significant returns in Gala and continuous earning potential with GalaCloud.
Starting as much as 10 times the amount of your ZPT investment in Gala.
You can find more details and apply through GalaCloud website that will be launched once campaign starts.
As always, we’re looking for dedicated Zeepiners inspired to create a new economy for the creative industries. Hopefully how it looks like became much clearer after this glimpse behind the scenes.
Data ownership happens to be our priority right now, to create a stable and reliable ecosystem but we have much more to come. You brought us closer to realizing this vision than we ever were, and we look forward to being in touch with you every day.
We have a lot to learn from you, Zeepiners.
Thank you for all the support so far, this is just the beginning.
ZEEPIN Team
Zeepin:
Website: https://www.zeepin.io/
Telegram: https://t.me/zeepin
News Channel: https://t.me/ZeepinNews
Twitter: https://twitter.com/ZeepinChain
Facebook: https://www.facebook.com/ZeepinChain/
Youtube: https://www.youtube.com/c/Zeepin
SubReddit: https://www.reddit.com/r/ZEEPIN/
Instagram: https://www.instagram.com/zeepinchain/
Linkedin: https://www.linkedin.com/company/zeepin-foundation/
Discord: https://discord.gg/YcPhNXC
Galaxy:
Website: https://cryptogalaxy.one/
Telegram: https://t.me/CryptoGalaxyOne
Twitter: https://twitter.com/TheHubGalaxy
Facebook: https://www.facebook.com/CryptoGalaxyOne/
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
359 
359 
359 
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
"
https://medium.com/@marylholden/in-cloud-storage-c8265990e3cd?source=search_post---------174,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mary L. Holden
Aug 30, 2014·2 min read
What’s in your Cloud?
In the bucket
of your wish list
lies the fable
of what you’ve missed.
Dreams in storage
too elastic
to be kept in
pails of plastic.
Have a penchant?
Learn something new?
Send it upward
then pull and do!
Desires rise!
You’ll get the gist
with head in clouds
of see and mist.
NOTE:
My friends have heard me talk about the fact that I have a Cloud List. It’s what most people refer to as a Bucket List. My desire is to have people start storing their unrealized dreams in a Cloud List.
The theoretical cloud is a much better place to keep your list of desires. It is safer than a bucket, and much more romantic! A bucket is also something you can use in an emergency leak or throw-up situation. It holds mop water or other kinds of throw-away or recycle-able things.
You don’t see buckets all that often, but you do see clouds in the sky, and when you see them they can remind you of what you’re keeping in a vapor state. Vapor states are mutable—they can evaporate or turn to rain or snow. And, they are interesting things to imagine—when you want to realize a dream, see yourself stretching into the sky, reaching into a cloud and pulling out a ticket that will actualize and allow your dream come true.
What’s on your Cloud List?
Since I’ve had my dreams in the clouds, I have been able to realize a ski lesson, a dance lesson and a few other things I prefer to write about by hand, and only in a journal. There are a whole lot of other things on my Cloud List—some are profound, some are pedestrian and all are possible poem material!
A constantly evaporating editor and writer. Believer in medium since 2013 when they made me wait for an invitation….
1 
1 
1 
A constantly evaporating editor and writer. Believer in medium since 2013 when they made me wait for an invitation….
"
https://medium.com/@openbom/what-cloud-storage-do-you-want-to-use-with-openbom-160a46390038?source=search_post---------153,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Nov 18, 2017·4 min read
Today, I want to talk about cloud storage. In the past few months we’ve had many requests from users, not only to create bill of materials based on engineering information from CAD system, but also, to include CAD files and geometry information in a neutral format such as 3D PDF, STEP among others. You may have had a chance to read my recent post about our work to make CAD and PDF files exported by the new Solidworks integration, SOLIDWORKS OpenBOM plug-in early availability of CAD and PDF file sharing.
This raised an interesting question I want to share with you: What cloud storage do engineers prefer to use for storing CAD and neutral geometry files? Cloud storage is not a new topic in engineering software. Moreover, I’ve published a few article about cloud storage on Beyond PLM. For example, you can read this one: Dropbox reality for engineers.
The SearchCIO blog presented the picture above, seems like ages ago, in 2012, showing Dropbox usage by department. In it, notice that 34% of engineers five years ago already used Dropbox. Another article from 2012 discusses the advantages of special cloud data storage and my discussion with GrabCAD. Navigate here to read more. GrabCAD has been acquired since then. Solidworks xDrive is still not available, but other systems such as Autodesk A360 are moving forward and providing a valuable opportunity to store CAD files with Viewer option integrated into storage. Read more here and here.
Another interesting opportunity to store CAD files is Onshape. Read my earlier blog, Onshape quietly developed Google Drive for CAD, here and another similar article, here. There is a number of smaller vendors developing specialized CAD file sharing systems such as Kenesto and Team Platform. Some CAD vendors such as Solid Edge went on to create a special partnership with Dropbox. Read more about these things here, Thinking Outside the Cloud Storage Box.
Getting back to mainstream cloud storage vendors, the CloudRAIL report Cloud Storage 2017 brings a few interesting data points. Read more, here. Here are two pictures that might shock you:
So, getting back to our OpenBOM strategy… At OpenBOM we specialize in BOM and data management. Therefore, we are strategically partnering with other vendors to integrate different cloud storages for the benefit of our customers. OpenBOM can be integrated with multiple cloud storages and we are going gradually introduce cloud storage options to our users.
You can integrated cloud storage in the BOM view:
And the new CAD plug-ins introduce a new option to store 3D CAD files and auto generate neutral formats and store it to cloud storage:
Conclusion. OpenBOM is integrated with cloud storages on different levels. Together with other vendors we are planning to provide reliable and scalable solution to manage and share Bill of Materials and connect it to cloud storage based requirements and preferences of customer. Let me know what you think. Would this be helpful to you?
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Want to learn more about PLM? Check out my Beyond PLM blog and PLM Book website.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
"
https://blog.sia.tech/cloud-storage-for-2-tb-mo-8a34043e93bb?source=search_post---------6,"One of the frequent claims of the Sia network is that over the long term, storage will be cheaper than $2 / TB / Mo, assuming that storage economics do not change. Though we’ve claimed this many times, we’ve never published a detailed model explaining where this number comes from. Until now.
For the purposes of this model, we are going to be assuming an endgame where Sia has substantially outgrown all of the latent / unused storage in the world, where the only way Sia can continue to grow is by having new datacenters established for the sole purpose of profiting from selling data to the Sia network. You can follow along with the math using this spreadsheet.
One of the key ways that the Sia network distinguishes itself from traditional cloud storage is its datacenter architecture and requirements. The Sia network only expects hosts to have a 95–98% uptime. Despite this, the Sia network is able to achieve 99.9999% uptime for files. This is because each piece of data on the Sia network is stored on many hosts, requiring only a subset of them to be online in order for the file to remain available.
Today, data is typically stored on the Sia network with a 10-of-30 redundancy scheme. This means that there’s a 3x overhead, and that as long as any 10 hosts are online, the file itself is still retrievable. Once the Sia network is more mature, we will likely be switching to 64-of-96 hosts.
If we assume that the 30 hosts go offline independently, and each host has a 95% chance of being online over a given time interval, the equation to determine the probability that a file is unavailable looks like this, giving a result of 10^-19, or 18 nines of uptime. Practically speaking host failures are not truly independent and you have to account for black swan situations like world war three. The true reality is that no system actually hits 18 nines of uptime (nor 11 nines of reliability).
Amazingly, even though 64-of-96 is only 1.5x redundancy, exactly half of the total copies of 10-of-30, the uptime equation has a nearly identical result — 17 nines of uptime.
In Sia’s endgame, 1.5x redundancy is the number that is most likely to be used in production contexts, so that is the number we are going to be using when we model the long term cost of storage.
An efficient storage rig is going to need the following parts:
What’s going here is we’re buying 32 HDDs, and then finding a low cost way to put them together. This is a technique that was championed by the Bitcoin mining industry, with unprecedented levels of corner cutting, bitcoin mining farms were able to get prices to be absurdly low. We can employ the same strategies here. With 95% uptime requirements, we don’t need to splurge on expensive parts like a full computer tower. The mobo selected above supports 48 gbps in data transfer, which means we still end up with over 1gbps per HDD even though we are doing a ton of splitting. It also turns out that 32 HDDs only consume 200w, so the 750w PSU we picked is more than sufficient.
In total, we’re spending $4945 to get a rig with 192 TB, or about $26/TB. And this is all paying consumer prices. This rig was assembled using consumer parts, when buying a large number of rigs at scale, a datacenter should be able to get much better pricing. So we will be using a rig cost of $4500 in our spreadsheet.
Often times for datacenter buildout, you budget around $1mm per megawatt in costs, or about $1 per watt. These rigs are going to consume about 400w each (6 watts per HDD, 65 for the CPU, 70 for RAM and mobo, and then loss from the PSU and datacenter PUE). In total, that means budgeting about $500 in capex for datacenter buildout.
Another cost that needs to be considered is networking equipment. If we’re assuming that you can fit about 8 machines per rack, and you need $2,000 of networking equipment in the datacenter per rack, you get $250 per rig.
Finally, everything is going to have to be assembled and constructed. Including screws and such, each rig has in the ballpark of 400 parts. That means about 2 hours of labor per rig. We’ll call that $50, bringing our total buildout expenses to $800 per rig.
The whole purpose of understanding capex for the purposes of a profitable datacenter build is to model depreciation. We need to understand how much value we are losing on our hardware every year that the datacenter is in operation. Hard drives last on average about 7 years. Hard drives also go down in price over time, meaning that you lose value a little faster than hard drives break. For this example, we are going to assume a 15% depreciation rate on our storage rigs, and a 5% depreciation on our build-out. That means our total depreciation costs are $785 per year.
Investors putting money into datacenter buildout are going to expect to make more revenue than just operational costs and depreciation costs. I think a reasonable profit expectation for a datacenter built to support a mature Sia ecosystem is around 10%, because the risk of building storage for a mature platform should be relatively low. Earlier in the life of the Sia network, risk may be higher, therefore profit requirements may be higher. At 10% profit expectation, the rigs will need to earn $570 per year in profit to be considered a good investment.
Our rig takes up about 2 square feet. The shelf that I linked should actually be able to hold about 3 of these rigs, and should be able to stack 3 or more of these shelves together. If you account for leaving space for airflow and aisles, you end up with about 1 square foot per rig, meaning a 2,000 square foot datacenter can hold 200,000 TB. The cost of rent is going to be negligible compared to other expenses, and is therefore excluded. Electricity aside, utilities should be approximately negligible as well.
If we assume a PSU efficiency of 93% (this is easily attained at datacenters) and a PUE of 1.4 (generally attainable for this type of setup), you get about 400w per rig. Mining farms often have electricity costs at low as 4 cents, but Sia datacenters are restricted by the fact that they need to have access to good network connections, so we will budget 10 cents per kilowatt hour for electricity. At 500 watts per rig, this comes out to $450 per year in electricity expenses.
On Sia, bandwidth is priced separately, renters pay for upload bandwidth and download bandwidth independently from storage, which means that we can exclude all ISP costs from this equation. Those expenses are covered as renters utilize the bandwidth.
Typical datacenters often have multiple redundant power agreements, backup power setups, batteries, etc, so that they can maintain 99.99% uptime. They will also have technical staff at the datacenter 24/7 to react quickly to any failures or outages. None of these things are required when your uptime target is 95%, and therefore this is a huge set of costs we can ignore when designing Sia datacenters.
Sia datacenters will however need at least a bit of maintenance. For a 32 HDD system, you expect about 5 drives to fail per year. This takes time to repair and you will need on-site staff (just not 24/7). To account for these costs, we will budget $50 per year per rig.
All told, we are at about $1850 per year in expected revenue. This combines the depreciation costs, the electricity costs, the utility costs, and the profit requirements.
A mature datacenter should be able to maintain a utilization of 90% or more by purchasing equipment on an as-needed basis. Assuming that a datacenter does achieve 90% utilization, a 192 TB storage rig is going to need to earn all of its revenue on only 172 TB.
Something that is unique to the Sia network is collateral lockup. When hosting someone’s data, you need to lock up collateral to retain that data. When storing 172 TB of data, a host should expect to have about 6 months worth of revenue locked up. At a 10% profit expectation, that comes down to losing about 1 month worth of revenue to capital expenses per year on actively used storage. So really, we should only be counting 11 months of revenue, as that 12th month is going to pay for our collateral lockup expenses. We compute the collateral expenses as a number of months lost because it can be computed independently of other values this way and simplifies the math.
The final expense is the siafund fee. All storage on the network incurs a roughly 10% fee, which means the renter is going to be paying more than the host is earning.
With a total revenue requirement of about $1750, an effective utilization of about 172 TB, and an effective earnings period of 11 months, we compute that the host needs to be earning about 90 cents per TB per month in revenue for an investment to make sense. Accounting for siafund fees, the renter needs to pay about $1.00 per TB per month. And then accounting for 1.5x redundancy, the final cost of redundant storage is about $1.50 per TB per month.
This is possible because the Sia network is designed so that datacenters can take shortcuts while building. The traditional model for the cloud assumes that datacenters need to be ultra high reliability, and often also assumes certain performance requirements on the internal network and servers of the datacenter. Sia is much more relaxed, everything needs to happen on the order of milliseconds as opposed to microseconds, and is perfectly comfortable with datacenters and storage rigs that have over a day of downtime per month on average.
More to the point, as a whole the Sia network has been carefully designed to be optimal as a decentralized system. Many of the fundamental design choices for Sia had to be different to achieve decentralization, and we have been able to leverage the strengths and weaknesses of these requirements to create something entirely new, and ultimately far superior to the traditional cloud.
Try out Sia’s personal storage solution at https://sia.tech or Sia’s content publishing and distribution solution at https://siasky.net
Decentralized storage — Sia, Skynet, and cryptocurrency.
499 
3
499 claps
499 
3
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://blog.spently.com/this-tax-season-act-with-reason-digital-receipts-and-cloud-storage-3004b656cdcd?source=search_post---------164,"They say that April showers bring May flowers, which is great way to stay positive during the rainy transitional period that segues into summer. However, in addition to high precipitation, the month of April is also tax season. Now depending on how organized you are, filing your T2’s and T4’s could be a black cloud unto itself. For many small businesses and customers, tax season is a dreaded time of year, but it doesn’t have to be. Current technology has made it easier than ever to have all your important financial documents in order.
Since you’re on our blog, you are probably already aware of the convenience and efficiency of Spently. If not, let’s do a quick recap on the beauty of digital receipts:
Now, building on that last point, there are many available applications out there that allow you as a business or regular consumer to better organize digital receipts. For example, our friends over at Wave will soon be launching a beta product to get your receipts into Wave via mobile, web or email. Wave is an amazing cloud software set up that allows you to manage your accounting and payroll with ease.
Another option for small businesses are the smart folks over at 10Sheet. They currently offer a highly reputable bookkeeping service that does all the nitty-gritty for you. If you’re looking to use cloud bookkeeping without the hassle or responsibility, these are your people.
If you’re not a small business and are feeling particularly daunted by the state of your financial records, Lemon Wallet is an excellent resource to store all of your transaction information. Better yet, it’s a mobile application, and can be accessed on the go, wherever you happen to be.
The above are our personal choices for keeping your records in order, but if you wanted to shop around, there are a variety of options that will lighten your load. At the end of the day, that’s the whole point of what we at Spently, and our friends around the Internet, try to do: make your life a little easier, and your business a little more efficient.
Digital receipts and cloud storage is the one two punch for keeping organized. Whether you’re a start-up, small business or average consumer, it’s the simplest, two ingredient recipe for head ache alleviation. So instead of spending this upcoming long weekend sorting through piles of paper, you could spend it like I do: going on a chocolate bender and waking up Monday unsure of where I am. That’s normal, right?
Happy Easter!
Spently Blog
Written by
Turn transactional emails into marketing opportunities with upsells, discounts, and follow-up emails after every purchase. // Available for @Shopify
Spently Blog
Written by
Turn transactional emails into marketing opportunities with upsells, discounts, and follow-up emails after every purchase. // Available for @Shopify
Spently Blog
"
https://medium.com/google-cloud/google-cloud-storage-signedurl-resumable-upload-with-curl-74f99e41f0a2?source=search_post---------43,"There are currently no responses for this story.
Be the first to respond.
A couple days ago a colleague asked if its possible to use Google Cloud Storage Signed URL with Resumable Uploads.
SignedURLs are pretty useful in that they allow an application to issue a time-limited URL that a customer can use to upload or download a file in Cloud Storage (GCS) without needed to login.
That is, your application can simply give a self-contained URL to a user and he/she can use that URL alone without logging into upload or download a given object.
However, what if the file/object to upload is large or your network connection is flaky. Well, in GCS you can use Resumable Uploads as described here for GCS XML and JSON APIs and their corresponding libraries.
One problem though…GCS SignedURL only works with the XML endpoint and the libraries to perform the resumable upload only speaks to the JSON endpoint.
What to do? You can ofcourse mint a signedURL and reply the protocol as shown here….its certainly very tricky to do this but this article simply shows the mechanism. Hopefully, there will be library support for this within the GCS library set as they do now for the JSON API.
I would advise against implementing the protocol…there are many cases you need to consider like parallel download and handling all the appropriate retry logic….for reference, i’ve provided a link to Gmail attachment download page.
Anyway, here is the raw protocol using curl
2. Generate the signed URL and exchange it for the upload location URL.
The java and golang source for the samples here is shown at the end of the article. For java, you’ll need to crate a service account JSON file while for golang, a .p12 which you will need to convert to PEM. You will also need to grant the service account access to the bucket+object in question.
3. Submit an empty POST request with the added HEADER (x-googe-resumable:start) to get the location URL Use the signedURL from the previous step
4. Just set the Location value in an environment variable for later use (enclose the value with quotes)
5. Now start the upload and interrupt it after maybe 5 or 10 seconds to simulate network failure (i.,e click ^C to interrupt curl). If you do not interrupt it, the upload should work as normal…but we want to show
6. Now find out how much got transferred.
Remember to set the content-range header:
7. Create a temp file to transfer with the remaining bytes
response header shows that we transmitted 9699328 bytes so we have to transmit the reamaining bits…so lets create a file with that starting with the _next_ byte in the file 9699327 +1 = 9699328
8) Upload the remaining
9) Verify partial transfer:
Thats it!, we’ve uploaded the file completely by hand.
The remaining is for extra credit and if you want to generate a signedURL with canonical headers.
Appendix
The following code samples in Java and Golang issues a SignedURL with the resumable headers baked into it already.
Note: SignedURLs issued by google-cloud Java currently does not support setting Canonical Headers (see issue#2000)…which means you have to create a signUrl manually as shown below:
google-cloud goland does allow for setting the canonical header in the request:
developers.google.com
Google Cloud community articles and blogs
117 
2
117 claps
117 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/google-cloud-storage-tutorial-part-1-aee81f9d3247?source=search_post---------35,"There are currently no responses for this story.
Be the first to respond.
During the past few days, I’ve been diving into Google Cloud Storage (GCS). You might assume that this is a boring aspect of Google Cloud Platform because cloud storage has been around for awhile, but I found some pleasant surprises in my exploration. Let’s start with a brief overview:
Below is a simple example of storing and retrieving files. First we need to create a bucket — the basic container used to store files. Since all bucket names share the global name space, it needs to be unique across all of Google Cloud. You’ll see why this is the case in a later post when we start sharing files publicly via http. For this example, I’ll use a bucket named “gwbucket”. When referencing a bucket you need to specify the URL — e.g. gs://[bucketname].
Nothing too exciting yet, but you quickly see that the commands are familiar. You can add “-r” to most commands to make it recursive, as expected. Detailed help is available for all commands at https://cloud.google.com/storage/docs/gsutil or from the command line with gsutil help cp
Here’s another example using rsync and versioning:
Learn more about object versioningLearn more about the -m option
Note: The above examples use the default bucket class of “standard” and the default bucket location of “US”. To learn about other options, see these docs.
I used the last example above (except for the versioning line) to backup my 120,000+ image library to Google Cloud Storage. Anytime I add/modify/delete images, I simply repeat the gsutil rsync.
(Be very careful using the -d option on rsync as it deletes any files from the destination that have been removed from the source. I suggest using the <strong>-n</strong> option if you have any doubts. The -n option causes rsync to run in “dry run” mode, i.e., just outputting what would be copied or deleted without actually doing any copying/deleting .)
In my next post, I’ll show how to set up bucket object lifecycle management to configure automatic object deletion when it reaches a certain age, or to simply keep the last n versions of a specific object. This becomes a key feature when doing regular backups such as archiving log files each night, etc. Then I’ll show you how to share objects publicly via HTTP and how to utilize Google’s worldwide edge caching to provide very fast downloading of your files.
Google Cloud community articles and blogs
81 
2
81 claps
81 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Developer Relations — Living in San Francisco https://gregsramblings.com | https://cloud.google.com | http://instagram.com/gregsimages
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/a-lightweight-way-to-check-the-iam-permissions-granted-to-your-cloud-platform-cloud-storage-buckets-803dd86ce9ca?source=search_post---------79,"There are currently no responses for this story.
Be the first to respond.
Understanding the permissions you’ve granted to your Cloud Storage buckets is important so you can avoid inadvertently granting too broad permissions so please take time out to understand IAM roles as they pertain to Cloud Storage by reading this .
The Cloud Storage IAM best practices are listed here but to validate that you haven’t made a mistake in setting up permissions a lightweight check can be useful .
That’s what this post is all about . For a framework approach you may want to check out the extensible Forseti Security ( I’ll revisit that in depth at some point)
You can quickly check permissions on a bucket by utilising a very underrated feature that GCP has available that allows you to test API’s in the browser.
To do this you need to use two of the API pages:
Buckets List api docs- this will give you a list of all the buckets in a project if you are authenticated and have as a minimum the roles/viewer permission granted on the project
Now you have a list of buckets you can obtain the permissions against the identified buckets by using the buckets:getiampolicy api docs . For this page as minimum you need the storage.buckets.getIamPolicy permission on the project containing the bucket
The permissions on the bucket is returned as JSON ( yeah!) which for one of my buckets ( suitably anonymised output)looked like this:
That’s a bit tedious though ( as it’s separate http request for each bucket ) and as I like to automate stuff and use the easy to use Client libraries I wrote a small python script that you can use as the basis to automatically check the permissions on the buckets in your project(s). It also gives you a list of the projects you have access to as well ( just because really).
It writes the permissions for each bucket to a text file but it can easily be extended to write the values to a CloudSQL table or BigQuery. ( I tend to split the list project function out into a separate script and use the projectID’s that outputs to set the GOOGLE_CLOUD_PROJECT env variable then call a script that does the bucket checking for that project)
The script writes a single line for each permission against a bucket in the format :
If the same role is granted to additional groups or users ( members in GCP IAM terminology) that entry is included “, “ separated as shown above.
An example ( suitably anonymised output) for one of my buckets looked like :
Please note that the output is sensitive so please ensure that you take appropriate steps to take care in how you manage the data and who you give access to it.
You can find my base python script here ( Yes I know no tests but it’s a really simple script & I wanted to turn this around before I got back to work after my week off) .
The script was Inspired by Ric Harvey( @ric_harvey) who posted to github a handy s3 permission checker. Thanks Ric !
Mine doesn’t give you as pretty an output as Ric’s though it’s really designed for you to fork and do something more than just write to a text file
Google Cloud community articles and blogs
18 
18 claps
18 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Chocolate addict - I have it under control really I do. I do stuff involving cloudy tech. Tweets my own so only me to blame, except for retweets.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/pcmag-access/google-photos-kills-free-unlimited-storage-here-are-7-top-alternatives-f65b13fc24c9?source=search_post---------388,"There are currently no responses for this story.
Be the first to respond.
Free unlimited photo and video storage from Google? That deal is over, as of June 1. Here are your best options for storing photos if you don’t want to pay Google.
By Michael Muchmore
Many people jumped on the Google Photos bandwagon when the search giant announced free, unlimited photo uploads three years ago…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/blockonomist/siacoin-sc-surged-more-than-70-in-a-week-why-does-nobody-care-about-it-718d2e0a8c33?source=search_post---------272,"There are currently no responses for this story.
Be the first to respond.
Sia is a blockchain-based decentralized and distributed cloud storage platform whose native token is SiaCoin (SC). The current market price of SiaCoin is 0.0455, with a market capitalization of nearly $2.6 billion.
The platform is quite trustless and secure, where users can lease their unused storage space. It is the medium to pay for…
"
https://blog.realworldfullstack.io/real-world-app-part-17-cloud-storage-with-firebase-and-angular-d3d2c9f5f27c?source=search_post---------65,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Adding features: Bulk upload and User profile settings using firebase cloud storage
This is Part 17 of our Real World App series. For other parts, see links at the bottom of this post or go to https://blog.realworldfullstack.io/. The app is currently under development and can be viewed at https://bitwiser.io and the github repo can be found here.
In the previous post, we migrated our data from Firebase realtime database (rtdb) to firestore.
In this part, we’ll add a couple of features to the application. The bulk upload feature will allow select users upload question/answer content in bulk instead of doing it one at a time. We’ll also add an initial version of user profile settings.
The bulk upload feature will allow users (with appropriate role) to upload a csv file with Question and Answers in a batch. The file will get saved into the firebase storage and the data will be imported into the unpublished questions. The admin will then be able to verify them, approve/reject or send it back to be revised.
The instructions to upload are shown below and the material stepper control will be used for uploading the file. We’re restricting the file to have only one category at a time. If a user wants to upload Qs for multiple categories, they must do it one file at a time.
The user profile allows the user their basic profile info. The user profile will also need firebase storage to store avatar images of the user.
Cloud Storage stores your files in a Google Cloud Storage bucket, making them accessible through both Firebase and Google Cloud. This allows you the flexibility to upload and download files from mobile clients via the Firebase SDKs, and do server-side processing such as image filtering or video transcoding using Google Cloud Platform.
The firebase storage rules works similar to the firestore database. Refer to the documentation for an understanding the security rules - https://firebase.google.com/docs/storage/security/
We’ll start initially by allowing all authenticated users to read/write, but later setup so the users can only see their files -
For final rules for this part see here
As always, we will upgrade our components to the latest versions.
NOTE: The support for firebase storage sdk was introduced in version 5.0.0-rc.6
This part has large amount of code change, but a most of it is wiring up our components, store and services that we’ve seen enough times in this series. The complete source for this part is here.
We’ll simply focus on code related to storage and file uploads.
Make sure to include the “AngularFireStorageModule” in our core.module.ts
The following function in question.service.ts uploads the file to the storage. The file is created under the users uid node, so we can use the security rules appropriately). It also has a unique id as part of it’s name, that will be be used to refer it back from the questions saved in the firestore db.
The bulk-upload.component incorporates the logic for parsing out the csv, validating it and displaying the questions back to the user before submitting it. Partial code below -
For uploading the avatar for the user profile, we’ll use the ngrx-image-cropper library. This will allow for cropping and adjusting the aspect ration of the uploaded image.
As with bulk upload, we will use the firebase storage to store user images on the cloud storage.
See profile-settings.component.ts for complete code.
We need to adjust our rules so that the bulk uploaded files can be created by an authenticated user but restrict read only to the creator. On the other hand, the profile avatar image can be read by everyone.
The admins will have permissions to everything. Here are the rules -
Note the use of hard-coded admin uids for now. We’ll change this over the next parts to use the admin role instead of the uids.
Complete source code for this part here
In this part, we added much needed features of bulk upload and user profile using firebase storage for uploaded files. It’s missing the role assignment and some UI changes that we’ll cover in the next part of the series.
Revisiting ngrx and cleanup UI for bulk upload
If you enjoyed this article please recommend and share and feel free to provide your feedback on the comments section.
Series summary and index: Part X
Full Stack development of real world applications
175 
175 claps
175 
Written by

Full Stack development of real world applications
Written by

Full Stack development of real world applications
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/why-do-you-rob-banks-because-that-is-where-the-money-is-w-sutton-5182488b387a?source=search_post---------385,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Mar 16, 2016·1 min read
Ashley Madison is toast (http://techcrunch.com/2015/08/19/ashley-madison-data-dumped/).
Due to a lack of security measures, the trust and discretion that AM shared with its 33 million members has been lost forever. The personal details of infidelities hit the dark web today where hackers will surely begin personal blackmail campaigns and trigger countless divorces.
For a company that booked $115 million in revenues and pre-tax profits of $55 million in 2014, with a potential Unicorn IPO on the horizon (http://www.forbes.com/sites/adamtanner/2015/01/21/profiting-from-cheating/), the stakes could not have been higher.
How could this have been avoided? Simply. Remove the honey pot of data with Storj.io: https://www.youtube.com/watch?v=vl3bUzfn2lg
With Storj, AM’s data would have not only been encrypted end-to-end, but sharded into millions of mini files and then widely distributed across thousands of nodes, such that no hacker or employee could possibly access this information without multi-sig encryption codes.
Remove the honey pot and greatly diminish the risk of hacking. It is counter-intuitive that your data is more safe in thousands of individual hard drives spread all over the world than AWS/Dropbox/Box/dedicated company servers, but it is also true.
John Quinn, Co-founder of Storj Labs
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
13 
13 
13 
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
"
https://medium.com/hackernoon/should-you-trust-the-cloud-to-securely-store-computer-files-701b0f885d94?source=search_post---------215,"There are currently no responses for this story.
Be the first to respond.
By John L. Miller, Worked on several distributed and cloud storage services. PhD in distributed system. Originally published on Quora.
Files stored in reliable cloud services are some of the most secure files you can have, provided you have good passwords. Google, Microsoft, and Amazon all provide reliable cloud services for consumer file storage.
What makes them safe?
Personally speaking, my home machines have lost hundreds of gigabytes of data (video, audio, and some important stuff) to hard disk failures. I’ve NEVER lost ANY data I put in cloud services.
There are two provisos:
Some cloud storage has versioning of files to help you recover from accidental deletes and overwrites.
Overall the cloud is a great place to securely store data.
By John L. Miller, Worked on several distributed and cloud storage services. PhD in distributed system. Originally published on Quora.
For more trending tech answers from Quora, visit HackerNoon.com/quora.
#BlackLivesMatter
169 
6
169 claps
169 
6
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
A place to share knowledge and better understand the world.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/linedevth/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87%E0%B8%A3%E0%B8%B0%E0%B8%9A%E0%B8%9A%E0%B9%81%E0%B8%8A%E0%B8%A3%E0%B9%8C%E0%B8%A3%E0%B8%B9%E0%B8%9B%E0%B8%88%E0%B8%B2%E0%B8%81-cloud-storage-for-firebase-%E0%B9%83%E0%B8%AB%E0%B9%89-liff-%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%87%E0%B9%89%E0%B8%AD-database-5ee946b7291f?source=search_post---------49,"There are currently no responses for this story.
Be the first to respond.
เดิมทีการดึงไฟล์ทั้งหมดจาก Cloud Storage for Firebase นั้น นักพัฒนาจำเป็นจะต้องเก็บ URL ของไฟล์ไว้ใน database แล้วจึง query ข้อมูล URL ออกมาใช้งาน
แต่ก็มีบาง use case ที่นักพัฒนาต้องการเพียงแค่อัพโหลดไฟล์ไปเก็บไว้ แล้วดึงไฟล์ทั้งหมดออกมาใช้ โดยปราศจากการ query ข้อมูลจาก database
นั่นจึงเป็นที่มาของบทความนี้ที่จะมาเฉลยวิธีการ list ไฟล์ทั้งหมดจาก path ที่เราต้องการออกมา ซึ่งถือเป็นภาคต่อของบทความก่อนหน้า และผมแนะนำให้ผู้ที่ยังไม่ได้อ่าน ให้อ่านก่อน เพื่อที่จะสามารถประติดประต่อระบบที่เรากำลังจะสร้างให้สมบูรณ์
medium.com
ถึงตรงนี้ คิดว่าทุกคนน่าจะมี LINE Chatbot สำหรับการอัพโหลดและ resize รูปกันเรียบร้อยแล้ว ถัดไปเราจะมาสร้าง LIFF ที่ให้ดึงข้อมูลรูปทั้งหมดจาก Cloud Storage for Firebase มาแสดง พร้อมทั้งเพิ่มฟีเจอร์ให้ผู้ใช้สามารถแชร์รูปไปยังเพื่อนๆ หรือกลุ่มต่างๆได้ โดยขั้นตอนในการพัฒนามีดังนี้
ขั้นตอนนี้ใครที่ยังไม่มี LIFF app ให้ทำตามบทความด้านล่างนี้ในข้อที่ 2 และ 3 ก่อน แต่สำหรับใครที่มี LIFF app แล้วก็ให้ข้ามไปลุยต่อในข้อถัดไปได้เลย
medium.com
เริ่มจากเข้าไปที่ Firebase Console แล้วเลือกโปรเจคเป้าหมาย(โปรเจคเดียวกับที่ใช้ในบทความก่อนหน้านี้)
เมื่อเราเข้ามาในโปรเจคเป้าหมายแล้ว ให้เรากดปุ่ม Add app ที่หน้าแรก
ตั้งชื่อให้ Web app ของเรา แถมถ้าใครจะใช้ Firebase Hosting ก็สามารถซีตุ๊ป (setup) จากตรงนี้ได้เลย
เมื่อกดปุ่ม Register app แล้วก็จะเจอหน้าแสดง SDK Configuration แบบนี้
copy โค้ดดังกล่าวออกมา แล้วแก้ไขโค้ดนิดนึง โดยตัวอย่างในบทความนี้จะไม่ได้ใช้ Google Analytics for Firebase แถมเราจะใช้ Cloud Storage for Firebase อีกด้วย
ดังนั้น ให้แก้ไขชื่อ SDK จาก firebase-analytics.js ไปเป็น firebase-storage.js แทน และลบ firebase.analytics() ออก
หลังจากเราได้โค้ดสำหรับการ config มาแล้ว ก็ให้ copy วางลงใน template ที่เตรียมไว้ตามตัวอย่างนี้
ถัดไปคือส่วนที่สำคัญที่สุดของบทความนี้ละ ให้เราสร้างฟังก์ชัน getImageList() เพื่อดึงรูปทั้งหมดจาก path ที่ต้องการใน Cloud Storage for Firebase แล้วไปแสดงผล
จุดสังเกตจากโค้ดตัวอย่างนี้คือ จะเห็นว่ามีการเรียก ${profile.userId} ที่เป็นส่วนหนึ่งของ path เนื่องจากบทความก่อนหน้านี้เราได้อัพโหลดรูปเข้าไปตาม userId นั่นเอง และยังมีฟังก์ชัน shareMsg() อีก ซึ่งทั้งสองจะเกี่ยวข้องกับขั้นตอนที่ 3
จุดเริ่มขบวนของการพัฒนา LIFF ก็คือการ import ตัว LIFF SDK เข้ามาก่อน จากนั้นก็สร้างและเรียกใช้ฟังก์ชัน main() ตามตัวอย่างด้านล่างนี้
จุดสังเกต คือ LIFF-ID ซึ่งจะเป็น ID ของ LIFF ที่ได้จากขั้นตอนที่ 1 นอกจากนั้น LIFF app ตัวนี้จะรองรับการเปิดทั้งในแอป LINE และ external browser
เพื่อให้ผู้ใช้สามารถกดที่รูปเพื่อแชร์ไปยังเพื่อนหรือกลุ่มที่ต้องการได้ เราก็จะสร้างฟังก์ชันชื่อ shareMsg() ขึ้นมา โดยรับ parameter 2 ตัวด้วยกันคือ ลำดับของรูป และ thumbnail URL
จุดสังเกต คือ originalUrl ซึ่งจะมาจากการ replace ค่าบางอย่างใน URL เป็นค่าว่าง ซึ่งมันเป็น pattern ที่มาจากการตั้งค่าใน Firebase Extensions จากบทความที่แล้วนั่นเอง
ก่อนจะไป deploy กันเรามาดู source code ฉบับสมบูรณ์สักหน่อยว่าทำตาม แล้วมันได้หน้าตามาประมาณแบบนี้มั้ย
จุดสังเกต คือ SDK URL ที่ไม่เหมือนกับที่ระบุไว้ในข้อ 2 ทั้งนี้ก็เพราะผมจะใช้ Firebase Hosting จึงสามารถใช้ dynamic SDK ได้(อ่านเพิ่มเติมใน เกร็ดความรู้ หัวข้อถัดไป)
เนื่องจาก Web URL ที่จะนำมาใช้กับ LIFF จะต้องรองรับ HTTPS โดยคุณจะเลือกใช้ host ที่ไหนก็ได้ แต่ถ้าใครยังไม่มี host ผมขอแนะนำ 🔥Firebase Hosting เนื่องจากฟรี SSL, CDN ทั่วโลก, พื้นที่ 1GB และมันง่ายมว๊ากในการใช้งาน ถ้าสนใจลองศึกษาบทความด้านล่างนี้ได้
medium.com
ให้เรานำ URL ที่ได้จากการ deploy ไปอัพเดท Endpoint URL ใน Channel และเปลี่ยนสถานะของ LIFF app ที่เราสร้างจาก Developing เป็น Published เพื่อให้ผู้ใช้ทุกคนสามารถเปิดใช้งานได้ โดยให้ทำตามข้อ 4.6 ของบทความนี้ได้เลย
medium.com
เราสามารถเข้าไปดู SDK Configuration ได้ตลอดเวลา โดยเข้าไปที่ Project settings เลือก tab ชื่อ General จากนั้นให้เราคลิกเลือก web app ที่เราสร้างไว้ จะเจอส่วนที่เป็น config ทางด้านขวาตามรูปนี้
จากรูปเราจะเห็นตัวเลือกทางด้านขวา 3 ตัวเลือกได้แก่
บทความนี้ถือเป็นภาคต่อและภาคจบ ของการสร้างระบบ Gallery ไว้ใช้ส่วนตัวตั้งแต่ สร้าง LINE Chatbot มาเพื่อรับอัพโหลดรูป, ย่อรูป, ดึงรูปทั้งหมดมาแสดงใน LIFF และแชร์รูปที่เก็บไว้ให้เพื่อนหรือกลุ่มด้วย Share Target Picker หวังว่าเนื้อหาทั้ง 2 บทนี้จะช่วยให้คนที่สนใจเกี่ยวกับการจัดการรูปภาพใน Chatbot และ LIFF นำไปต่อยอดได้ง่ายๆนะครับ
ก่อนจากกันไปขอฝาก publication ทั้ง LINE Developers Thailand และ Firebase Thailand ให้ช่วยติดตามกันด้วยนะครับ สำหรับวันนี้ ลากันไปก่อน แล้วพบกันใหม่บทความหน้าครับ
Deliver world-class developer experiences and learning through LINE API
46 
1
Thanks to Tan Warit. 
46 claps
46 
1
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Closing the distance. Our mission is to bring people, information and services closer together
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Closing the distance. Our mission is to bring people, information and services closer together
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@markstarlin/cloud-storage-worries-f15790f1f814?source=search_post---------86,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mark Starlin
Mar 30, 2018·1 min read
I have apps that are continually asking me if I want to store my data in the cloud.
But I keep thinking, What if an airplane flies through the cloud?
More wit:
medium.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/chainsafe-systems/true-data-ownership-your-keys-your-files-your-cids-dcfb0b2d50c?source=search_post---------273,"There are currently no responses for this story.
Be the first to respond.
ChainSafe Files provides highly private, highly secure cloud storage that allows users to maintain full control over their data. Our product leverages IPFS and Filecoin to give users all the benefits of distributed storage and blockchain technology. Files abstracts away the complexities of these systems to deliver a simple, user-friendly experience. In this way, users can enjoy easy access to our non-custodial data storage solution that is more private, more resilient, and more decentralized than traditional cloud storage options. Try it today here!
Co-authored by Timothy Hao Chi Ho
What is data ownership and why is it important?
Modern capitalist society is based around the concept of private property. If you buy a house, you can prove that you own it and everyone else in society acknowledges your ownership of that property. If you invent something new, you can apply for a patent for ownership over that idea. If you write a book, you can publish it and earn revenue from its sale and you can sue if someone else tries to publish your book as their own.
On the other hand, if you write a tweet, or post a photo on Instagram, who owns that material? What about all the data about your online shopping history or even your banking transactions? All of those activities generate digital data that powers one of the largest industries in the world today. However, unlike other aspects of our society, most of the data that people produce and store on the internet is not owned by them. Once created, it becomes the property of the company where that data was stored, posted, or filtered through. Some of the largest companies in the world have prospered by appropriating the data of their users and selling it for marketing and machine learning at astronomical profits.
Most of the value generated online is captured by large companies who exploit their users’ data, thereby removing a user’s agency over the very data they created. At ChainSafe, we believe that users of the internet should be able to maintain control over their data. Files provides data ownership to users through complete end-to-end encryption of all uploaded data. Files are encrypted on the client side, throughout transit and at rest. This ensures that even though IPFS and Filecoin are public networks, all of your data remains yours and yours alone. Another aspect of data ownership is freedom of retrieval. Files gives users full control over where and how their data is hosted and retrieved. More on this below!
IPFS and Filecoin are distributed file storage networks that allow users to store their data online without fear of it being compromised or exploited. ChainSafe Files leverages both of these networks, abstracts away the complexities of the underlying storage layers, and provides users with a simplified front-end that does all the “non-evil” things you could want from a cloud storage solution:
Unlike traditional Web2 cloud storage, ChainSafe Files doesn’t:
Thibaut’s article on Files’ infrastructure does an excellent job explaining the architecture and how it all jives together. In essence, when users upload files using our system, we take these files, encrypt them so only you can read the contents of the files, before pinning it to IPFS as a caching layer. For long term storage, we then also upload these files to the incentivized Filecoin network to ensure multiple levels of redundancy.
In this sense, should an IPFS node or Filecoin miner go down, there will always be several backups available from which to restore your files.
For most users, this level of privacy, redundancy and data ownership likely satisfies their requirements. However, at ChainSafe we understand that some will in fact want even stronger guarantees that they will be able to access their files.
What if the Files UI or back end goes down? 😱
All files uploaded via our front-end UI are given content identifiers (CIDs). In IPFS’ content-based addressing, information is stored based on its content, not on its location like in the web2 world. To easily allow you to download, decrypt, and access all your files, we have built a special command-line interface (CLI) tool so that you can get to your files even without our interface. Talk about full-service!
The CLI tool can be found in this repo: https://github.com/ChainSafe/files-cli
To use the CLI tool, users are required to have recorded the CIDs for their files in advance. Each files’ CID can be found in the “Info” section of the options menu (ellipsis button on each line item).
For folks with very high data ownership requirements, we recommend recording your CIDs periodically, or even at the time of upload for each file.
Once all CIDs have been recorded down, simply follow the instructions in the README to download the files directly from IPFS. Currently, the tool only works for files pinned to IPFS due to the expense of retrieval on Filecoin. However, all files uploaded through our product are backed up multiple times on both IPFS and Filecoin, so users should have no issue retrieving their files from IPFS using the CLI tool.
We built this tool to eliminate ourselves as a single point of failure for data retrieval and to deliver on our promise of giving users full control and ownership over the data they upload to IPFS and Filecoin. If there is enough demand, we would consider improving the tool to allow for automatic downloads of CIDs and other features that may be requested.
We rely on user feedback to help make files better so we would love to hear your thoughts! What do you like about Files? Is there anything you would change? What feature would you like to see in the future? Tell us what you think in our Files power user Telegram group, or on GitHub. Also feel free to reach out on Twitter and head over to our Discord channel for product support and Q&A.
Help shape the future of Files by scheduling a user interview with our product manager Colin Schwarz. The user interviews are short 15 minute calls that aim to gleam insights into your current problems and solutions, and how Files might help. Schedule a call today!
Want to work with ChainSafe? Come join us! Check out the careers section of our website with our open positions, and get in touch with us at careers@chainsafe.io if you are interested!
Many thanks to Timothy Hao Chi Ho for co-authoring this article and to Thibaut Sardan for building the CLI tool and for editing.
building the infrastructure for web 3.0
102 
102 claps
102 
building the infrastructure for web 3.0
Written by
Helping to build Web3 for a better world. Project Manager and Technical Writer at ChainSafe.
building the infrastructure for web 3.0
"
https://medium.com/@matryer/introducing-stow-cloud-storage-abstraction-package-for-go-20cf2928d93c?source=search_post---------13,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mat Ryer
Nov 8, 2016·4 min read
Stow is an open-source (Apache licensed) abstraction on top of Amazon S3, Microsoft Azure Blob Store, Google Cloud Storage, Openstack Swift and more. We hope Stow will become the de facto library for interacting with such storage providers in Go.
Stow allows you to interact with various cloud storage providers via a simple single API.
GrayMeta’s MetaFarm product harvests metadata from all kinds of files, making that data queryable and searchable, regardless of where that content lives.
When it came time to consume content from the cloud, we noticed that most providers had a similar (although not exactly identical) approach to storing content; files (or items) were stored in buckets (or containers).
Stow abstracts these concepts and provides a single API through which you can seamlessly interact with various providers.
The main three objects in Stow are:
The Stow implementations are built using other open-source packages, so it certainly stands on the shoulders of giants.
Implementations are referred to by a kind string (like “s3”, “azure”, etc.) and an object providing implementation specific configuration values (such as credentials, the S3 region, Google Cloud project ID, etc.)
First, you import Stow and any implementations you wish to support:
The underscore indicates that you do not intend to use the package in your code. Importing it is enough, as the implementation packages register themselves with Stow during initialization.
Next, we can create (or load from a database) the kind and config data, and Dial the location:
You can see that during runtime it is trivial to pass in different kindand configuration values.
Once you have dialled the Location, you can walk the Containers:
The stow.WalkContainers function is like filepath.Walk, except that it walks the Containers at that Location. In Amazon S3 each container is a Bucket.
Once you have found the container you’re interested in, you can walk the items in a similar way using the stow.Walk function:
The 100 argument represents the number of items to get per-page, and where we pass in stow.NoPrefix, you can optionally specify a prefix to filter the items being walked.
Once you have found a stow.Item that you are interested in, you can stream its contents by first calling the Open method and reading from the returned io.ReadCloser (remembering to close the reader):
If you want to write a new item into a Container, you can do so using the container.Put method passing in an io.Reader for the contents along with the size.
Stow implements paging via cursors; you make a request, in the response (along with the items) is a cursor string, which can be passed into future calls to get subsequent pages of data.
You call methods that accept cursors by first passing in stow.CursorStart, which indicates the first item/page. The method will, as one of its return arguments, provide a new cursor which you can pass into subsequent calls to the same method.
When stow.IsCursorEnd(cursor) returns true, you have reached the end of the set.
The walker functions use cursors under the hood, so that’s a good place to look if you want to see a real example of how to use them.
We have been succesfully running Stow in production for a few months.
Stow is ready to use. Please vendor the dependency just in case we make any API changes before the official release.
An official version 1 release will happen early next year after the community has had time to digest it and contribute feedback, ideas, bug reports, etc.
Founder at MachineBox.io — Gopher, developer, speaker, author — BitBar app https://getbitbar.com — Author of Go Programming Blueprints
See all (267)
193 
1
193 claps
193 
1
Founder at MachineBox.io — Gopher, developer, speaker, author — BitBar app https://getbitbar.com — Author of Go Programming Blueprints
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cloud-believers/adobes-sensei-ai-platform-and-video-marketing-df833310df9d?source=search_post---------295,"There are currently no responses for this story.
Be the first to respond.
The use of big data, cloud storage, algorithm advancement, and elastic computing have all played a role in the machine learning and artificial intelligence revolution. According to the internet, video traffic will account for 82% of all consumer Internet traffic in the world. Because the importance of video on the Internet continues to increase, companies are eager to find tools that facilitate their daily…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/slack-developer-blog/meet-slack-at-twilio-signal-241a8db0cf44?source=search_post---------235,"There are currently no responses for this story.
Be the first to respond.
In today’s workplace ‘real-time’ and ‘access anywhere’ are the expected standards of communication. As technologies like cloud storage, virtual conferencing, SMS, and AI-powered bots become increasingly the norm, businesses need to keep pace by offering new ways to connect with everyone.
One company we admire in this category is Twilio. They provide easy tools to build for our modern communication landscape while their APIs let businesses quickly connect existing apps to every phone on earth. They’re also beloved by developers, and we look up to the way they’ve encouraged the world to look at developers as an entire, stand alone market.
Twilio’s annual developer conference, Twilio SIGNAL, begins tomorrow and we’re sending a lot of amazing team members from Slack to participate. There will be Slack representatives from our growth, platform, engineering, and developer relations teams in attendance. You can visit our booth or see us on stage at the following sessions:
Allison Craig on Developing Good Slack Apps and Custom Integrations
Don Goodman-Wilson on Combining Slack Bots with the Twilio API
Merci Grace giving a lightning talk on the Future of Messaging
We’re not just talking — this is Twilio Signal, so we’ll be showing lots of live code. We’ll demo a custom bot bridging the worlds of SMS and Slack messages. To get an idea of what’s involved, Twilio’s own Slack bot tutorial lays out the basics of how to let users send and receive Slack messages over SMS.
If you’re attending SIGNAL this year, make sure to stop by our booth and say hello! We’ll be the ones with emoji cookies.
Where you make work happen
14 
14 claps
14 
Written by
Tips to integrate with Slack APIs to make your work life simpler, more pleasant and more productive — whether for your internal team or millions of Slack users.
Where you make work happen
Written by
Tips to integrate with Slack APIs to make your work life simpler, more pleasant and more productive — whether for your internal team or millions of Slack users.
Where you make work happen
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@openbom/how-to-create-bom-for-solidworks-design-with-files-automatically-uploaded-to-onshape-cloud-storage-19c9f4733cbb?source=search_post---------151,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Nov 28, 2017·4 min read
Manufacturing companies are distributed in both location and time. Nothing happens in at single place or time these days. The number of contract manufacturing companies being used is growing. From large OEMs to very small manufacturing shops, everyone is looking how to optimize cost and find the right talent to work with. Therefore, everyone in the manufacturing industry is looking how to share data in an efficient way as well as getting updates on information that is changed by other people.
As efficient as desktop CAD systems are, they are probably achieving at the top of their performance these days; read more, here, specially when it comes to the technology related to cloud and data sharing. Systems like Solidworks might be perfect for design on a powerful desktop workstation, but when it comes to sharing files and coordinating the activities of a distributed team, these products don’t help much and engineers default to emails and cloud storage.
At OpenBOM we paid special attention at how to develop technology that can be both easy to use and supports the demand for data sharing and collaboration. Talking to many users, we found that companies want not only to share BOMs (which is obviously important), but gain access to files. This is how we got to the point of enhancing our OpenBOM CAD integrations with the ability to share CAD files and automatically convert files in the right neutral formats (Eg. 3D PDF). Read more, here.
As we move forward with this functionality, we learned about the different preferences engineers and manufacturing companies have. They are already using different cloud systems and OpenBOM will now integrate with other cloud storages such as Google, OneDrive, Box, etc. Read more, here. However, we also think that integration with specialized 3D Cloud storage platforms can give some additional advantages to our customers. There are a few examples of such specialized cloud storage systems, e.g.. Autodesk Fusion Team (aka A360), GrabCAD, Kenesto, and Onshape. Specialized CAD file management platforms have their advantages such as 3D CAD file viewing and special publishing and conversion tools. Read more, here. Some of these platforms, e.g., Onshape, have the means to translate and edit CAD files.
The cost of poor collaboration is huge. Why create competitive disadvantage? Based on the recent e-book provided by Solidworks, here, and research by Tech-Clarity provides an idea of the cost of poor collaboration. My favorite quote from the Solidworks e-books is the following:
While collaboration is important, it is also abstract, so it’s difficult to measure its cost. However, when engineering and manufacturing operate in silos, problems arise and there are bottlenecks. Figure 4 shows the business cost of poor collaboration:
The very nature of releasing design data to manufacturing requires collaboration. Engineering and manufacturing must be able to share and exchange design information. Usually that data needs to be imported and repaired, which is not only tedious and time-consuming, but the translation process can introduce errors. By integrating design and manufacturing systems, the teams can share design information seamlessly and avoid the excess costs, delays, and quality issues that arise with poor collaboration.
In an earlier blog, SOLIDWORKS OpenBOM plug-in early availability of CAD and PDF file sharing, we demo’d how the coming OpenBOM plug-in shares Solidworks data using cloud providers such as Dropbox. In today’s video, I’d like to show how the same OpenBOM Solidworks plug-in can share data using specialized 3D CAD platform such as Onshape and create a cloud BOM with links directly connected to Onshape storage.
Conclusion. For engineers using Solidworks, OpenBOM provides a way to easily create BOMs and share them in amongst teams which include contractors, suppliers and other vendors. The new functionality of the OpenBOM plug-in to connect with cloud storage providers creates even more value by connecting data and linking it to the right design files, transparently moving it to cloud storage. Based on feedback we received from early users, this is a most useful enhancement for manufacturing companies distributed around the globe.
So, what do you think? The OpenBOM Solidworks and Autodesk Inventor plug-ins will be available in the next OpenBOM release. However, if you want to try it now, please let me know via email (oleg @ openbom dot com)
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Want to learn more about PLM? Check out my Beyond PLM blog and PLM Book website.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
"
https://blog.upthere.com/data-center-networking-redundancy-implementation-for-cloud-storage-service-3bd26e3abfae?source=search_post---------66,
https://medium.com/google-cloud/how-to-run-containerized-workloads-in-gce-vm-7ae99d5e0975?source=search_post---------397,"There are currently no responses for this story.
Be the first to respond.
While the idea of a serverless platform and long running workloads does seem somewhat “unnatural” at first, smart people are already working on that (looking at you @Knative community). In the meantime, a simple approach is sometimes all you may need.
In this post I will illustrating how to use Google Compute Engine (GCE) container execution option to run variable duration jobs. This approach supports custom VMs, GPU/TPU accelerators and VPC networks… so may be handy alternative to other compute options on GCP. I’ll also demo how to auto-terminate the created VM on container completion, so you won’t have to pay for idle VM time.
Finally, in this example I’ll parses small gzip file from Google Cloud Storage (GCS), but since this approach is not limited by client timeouts you can use it to do pretty much anything… transformations on bigger files, lightweight ETL, or media format encoding. You can even combine it with GCP Task Queue for more complex pipelines.
If you don’t have one already, create new GCP project and configuring Google Cloud SDK.
To start, clone this repo, and navigate into that directory:
Also, to streamline the subsequent commands, we are going to export a few variables:
To execute this sample you will need a GCP service account. You can do that either in UI or using gcloud SDK.
We also will have to assign this account the necessary IAM roles:
Finally, to enable our demo to impersonate that account, we will need to provision a key for that account which will be saved in your home directory. You should protect that key or just delete it after this demo.
The unit of workload in this example is container image. To create an image from the source code in this demo, first vendor all the go dependancies:
And then submit the image build request to Cloud Build:
If successful, you should see a confirmation with the fully qualified URI to the created image.
Now to start a new VM and configure it to run the above built image. This sample will use gzipped CSV file (100-Sales-Records.csv.gz) located in publicly available GCS bucket (long-running-job-src-files). If you want you can replace these with your own files.
There is no way right now to upload files in the same command, so right after, you need copy the service account key created above to the new VM. The container is set to restart on failure so it will automatically use the key when it becomes available in the mounted volume attached to our VM.
Once the VM has started you can monitor the logs output from the container to Stackdriver. To do that we need to first capture the VM instance Id:
Once we have captured the VM instance ID we can now query the Stackdriver for logs output by the container:
Note, this command will print only the logs that are output by the user code in the container (the logger inside prefixes all log entries with “[LRJ]”). You can see the complete list of log entries by removing thejsonPayload.message:""[LRJ]"" filter.
After the container exists, the VM will automatically shutdown but the logs should still be still available in Stackdriver for forensic analyses.
I hope you find this helpful addition to your GCP compute toolbox. You can find the source code along with scripts for easier execution at https://github.com/mchmarny/long-running-job
Google Cloud community articles and blogs
12 
1
12 claps
12 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Engineering Leader @Apple, Xoogler, Gooner, still hungry for more
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@krishnan/portable-apis-retaining-the-flexibility-to-innovate-with-s3-api-58ecab82eb71?source=search_post---------375,"Sign in
There are currently no responses for this story.
Be the first to respond.
Krish
Nov 27, 2017·4 min read
I am here in Las Vegas to attend AWS re:invent this year, and I have been thinking about how AWS S3, their object storage service, has gained traction over the years. The power of AWS S3 lies in its API and how easy it is for developers to use it in their applications. The simplicity of the API and seamless scalability offered by the service opened up some interesting use cases, helping users in today’s world of ever-increasing data storage and consumption. As an industry observer and researcher who sees patterns in how technology market evolves, I got excited about the role Minio can play in the market. In this post and a series of posts in the coming weeks and months, I want to highlight various use cases with Minio in helping organizations retain their flexibility to innovate to meet the Modern Enterprise needs.
As technology evolves rapidly, enterprises are forced to embrace a Continuous Innovation strategy which emphasizes on enabling the developers to meet the business needs rapidly. This requires a loosely coupled architectural approach and Portable APIs becomes critical in retaining the flexibility to innovate in a Multi Cloud world. The abstractions offered by the portable APIs allow applications to run on any cloud or on-premises without having to rewrite the applications.
Minio is the cloud native object storage for the multi cloud era object with Amazon S3 compatible API. This allows developers to use Minio API for object storage and make it work across multiple clouds. This flexibility allows organizations to break down data silos while empowering their developers to take advantage of various services offered by different cloud platforms or providers. Since this is AWS re:invent week, I want to talk about using Minio with AWS S3 in the context of a modern enterprise.
Let us consider a typical hybrid cloud use case where an enterprise uses AWS as an extension to their on-premise private cloud. Minio helps them retain the flexibility to innovate rapidly as well as reduce costs with the caching feature. In this section, I will highlight how modern enterprises can use Minio as a part of their cloud strategy.
Minio is S3 compatible. By writing your applications to talk to Minio API, you retain the flexibility to seamlessly move the application between AWS cloud and On-premise private cloud. This abstraction can also be leveraged to deploy the apps in any other cloud while still maintaining the compatibility with AWS S3.
Typically, using an abstraction over a native service requires compromises over a least common set of features across various clouds. But Minio takes a different path and helps users have an S3 like developer experience on any cloud including on-premise private cloud. This allows developers to pick any environment needed for their applications without worrying about how and where their objects are stored.
The other use case I want to highlight involves using Minio servers at the edge of the private cloud while using AWS as an extension. With this set up (highlighted in the image below), applications can leverage the data stored in AWS S3 while providing a seamless user experience to the end users.
This takes into account the caching feature offered by Minio. Minio caching works are similar to data caching with Minio servers sitting at the edge of the private cloud. The application talks to the Minio server which caches the data locally while a background process moves older data to AWS S3. This helps deliver data faster to the applications without being impacted by any bottlenecks on the path to the S3 service or a sub optimal response from the S3 service.
Some of the advantages with Minio on the edge of the data center talking to AWS S3 are:
Object storage is going to be an important part of modern enterprise puzzle. The key to innovation lies in the flexibility to use the right set of services for modern applications. Minio is one such portable API which allows organizations to use any cloud or on-premise private clouds for their application needs. Even in a hybrid environment involving a private cloud and AWS, using Minio offers significant advantage than tying into the native storage APIs.
Disclosure: Minio is a client of Rishidot Research
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
See all (529)
38 
1

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
38 claps
38 
1
Future Asteroid Farmer, Analyst, Modern Enterprise, Startup Dude, Ex-Red Hatter, Rishidot Research, Modern Enterprise Podcast, and a random walker
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/luxor/sia-decentralized-cloud-storage-7de576542497?source=search_post---------78,"There are currently no responses for this story.
Be the first to respond.
We are bullish on Sia’s business model and that’s why we at Luxor Mining have dedicated one of our mining pools to supporting the network. In addition, we have been contributing back a percentage of our mining pool fees to the community to help it develop further. To check out more about Luxor’s Sia Mining Pool click here.
Sia is a decentralized data storage platform powered by blockchain technology. Sia connects people “hosts” that have underutilized hard drive capacity with customers “renters” in search of data storage. In essence, Sia is the Uber of data storage.
The blockchain technology delivers a competitive advantage for Sia, making it more secure and lower cost than traditional cloud storage providers. Sia has created a marketplace for data that provides higher latency, more throughput, less downtime and more security.
Below is a breakdown of what we think makes Sia such a valuable service:
One of the most important purchase criteria for renters is the price of data storage. Currently, renters purchase data storage from a number of providers such as Amazon, Microsoft, Dropbox, Google, etc. These services offer prices that are far above the price of Sia. On average, Sia’s storage costs 70% less than existing cloud storage providers.
Sia is able to offer such a low price point due to their low costs. This is fueled by blockchain technology and competition on the network.
It’s no secret that blockchain technology has the potential to dramatically reduce costs. That is why big companies such as Google and Goldman Sachs (amongst many others) have invested in the technology. By providing a cloud storage marketplace on the cloud, Sia is able to dramatically reduce its costs, which results in a lower price point for consumers.
The business model has built in competition between data storage hosts which reduces the price even further. On the network hosts show their geography, speed, latency, price and uptime. Renters have the ability to select the hosts that are best suited for their situation. This competition causes downward pressure on price and upward pressure on quality.
Arguably more important than price is data security. Renters need to be certain that their data will not be lost, tampered with or stolen.
This is a problem for the existing cloud storage providers. The threat of data being destroyed is more probable than the public is led to believe. For example Amazon only has a few data storage centers in the world. This means that if these were all destroyed then so to would the data. In the unlikely case that America was going to be attacked, targeting data centers would be an effective place to start. Even if all the facilities are not attacked, a simple outage can cause other companies to go with it. For example when Amazon experienced an outage last year in one of its data centers a number of companies also temporarily went down including Slack and Business Insider. It’s dangerous to rely so much on one company and a few data centers. An additional (although less likely) threat to existing cloud storage providers is stolen or tampered data.The public is becoming increasingly suspicious of large tech companies and hence is worried about their data. Cyber security attacks from third-parties are also a threat even with technology companies investing heavily to prevent it.
Ultimately Sia delivers a strong solution to data security issues. Sia is more secure than any of the existing providers as no one event can trigger a loss of data. The security is built from the following concepts:
Sia uses the Reed-Solomon Redundancy to ensure the security of data. Each piece of data is stored on 30 devices located around the world. Only 10 of the 30 devices need to be online in order to store the file. If every host has a 90% reliability (which is lower than the actual reliability) than the chance of losing the data is 1 in 1,000,000,000,000,000. Because these 30 servers are located around the world and on different servers the chances of lost data are extremely low. No random event can trigger a loss of data.
Sia encrypts and distributes renters’ files across a decentralized network. The files are encrypted on the renters machine. Each separate piece of data has a separate password so it can’t be deduplicated. Renters control their private encryption keys and have ownership of their data. No outside third party can access or control renters data, unlike traditional cloud storage providers.
The File Contract is an application of Sia that is build off of Smart Contract technology. Basically, the hosts do not get paid until they prove (at the end of the period) that they did hold on to the data. When the contract is formed both the renter and the host puts money into the smart contract. The reason the host also puts money into the contract is to provide additional incentive to fulfill their duties in the contract. If they break the contract they will not only lose out on revenue but also their deposit. The blockchain enforces the smart contract automatically, meaning there is no need for trust of a third-party or between the host and renter.
The way the data is stored is by using the Merkle Tree method. Each file is split into 64 Byte pieces, then each piece is hashed, and then the adjacent hashes are combined until one piece remains. The final hash in the Merkle root. The host must prove that they have one of the 64 Byte segment of the data (randomly selected). This ensures that the host does in fact have the data.
To hold hosts accountable they are monitored often to make sure they are indeed providing the services they advertised. They are checked for uptime, latency, throughput and price. Overall, hosts are around 90–95% reliable.
Sia provides a unique solution to a problem facing many users of cloud storage. Under the existing industry renters pay a high price point for a service with questionable security. With Sia renters are able to store their data for cheap and with better security.
Ultimately, we expect this business to to thrive and hope to continue to support it in the future!
For additional information on Sia check out their Whitepaper here.
Questions about Luxor?
Follow us on Twitter and tweet to us! We’re happy to help: https://twitter.com/LuxorTechTeam
We’re also on Discord. Ping us there at https://discord.gg/5qPRbDS
Luxor has spent the past 2 years building North America’s…
215 
215 claps
215 
Written by
Luxor is a Bitcoin mining pool and full-stack crypto mining company. The financialization of hashrate starts with us. https://www.luxor.tech/
Luxor has spent the past 2 years building North America’s largest Bitcoin & Altcoin mining pools. Through mining thousands of blocks, we’ve gained a deep understanding in extracting and valuing hashrate.
Written by
Luxor is a Bitcoin mining pool and full-stack crypto mining company. The financialization of hashrate starts with us. https://www.luxor.tech/
Luxor has spent the past 2 years building North America’s largest Bitcoin & Altcoin mining pools. Through mining thousands of blocks, we’ve gained a deep understanding in extracting and valuing hashrate.
"
https://medium.com/iotatangle/welcome-tsvi-sabo-to-the-iota-foundation-caf9587f1397?source=search_post---------206,"There are currently no responses for this story.
Be the first to respond.
Tsvi Sabo has several years as a software developer, most recently at Ericsson VSPP as cloud-storage developer and a few years as a statistical analyst in a consulting company.
With curiosity for technology and mathematics he is constantly seeking to learn more about anything pertaining to these fields. Recently he began to develop a keen interest for crypto-currency and Distributed Ledger Technologies in general and learned about their underlying concepts, architecture and algorithms.
He is very excited about DLT and consider it an inevitable part of our technological future, and therefore want to contribute with his talent and dedication.
Tsvi has B.Sc. in industrial and management engineering from Ben-Gurion University and professional training in computer science
Shortly after I began to show interest in crypto-currency andblockchain technology IOTA caught my attention, I read their white paper and the model impressed me.I could not ignore its advantages,it seemed as if it offers a more generic consensus algorithm because there are no participant types(miners/buyers).Also it is mathematically proven (within the model assumptions) that expected transaction time is shortening with the rate of incoming transactions increase,not to mention it is scalable and quantum proof, it scored so many points at once..
The white-paper has really thorough and elegant analysis of the model and is an academic document with significant implications.The team approach seemed so creative and ahead of time that instead of riding the blockchain wave it is more like they created a new one.IOTA is the perfect solution for the need of IOT transactions mechanism which soon is expected to grow in a very non-linear fashion
Tsvi has shown great promise in the last weeks working with us. Particularly his deep experience and expertise with low level language development will be of great assistance for IoT focused development as well as IOTA’s C/C++ initiatives. We are excited to his skillset in our dev army. Give him a warm welcome!
Official IOTA Foundation blog
2.6K 
2
2.6K claps
2.6K 
2
Written by
Founder of IOTA
Official IOTA Foundation blog
Written by
Founder of IOTA
Official IOTA Foundation blog
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebasethailand/%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81-security-rules-%E0%B9%83%E0%B8%99-firebase-storage-723b859e4240?source=search_post---------53,"There are currently no responses for this story.
Be the first to respond.
สิ่งหนึ่งที่สำคัญและมีความซับซ้อนในการพัฒนา Software ก็คือเรื่องของ “ความปลอดภัย” บทความนี้จะพูดถึงความปลอดภัยในการเข้าถึงและการตรวจสอบไฟล์ ที่ Cloud Storage for Firebase Security Rules ได้เตรียมมาให้เราสามารถจัดการได้ด้วย code เพียงไม่กี่บรรทัด และไม่ต้องเขียนโค้ดในฝั่ง server-side เลย
การกำหนด Rules ใน Storage Security Rules จะต้องเริ่มต้นด้วย service (service firebase.storage) ชั้นต่อมาคือ Firebase Storage bucket (match /b/<your-firebase-storage-bucket>/o) และชั้นต่อๆมาก็จะเป็น rules อื่นๆตามแต่ละ path ใน bucket ละครับ
1. allow เป็น rule ที่ใช้กำหนดการเข้าถึงและตรวจสอบไฟล์ทั้ง read และ write โดยเราจะสามารถใช้ if ในการกำหนดเงื่อนไข
2. match เป็น rule ที่ใช้กำหนด path จะมีทั้งแบบตรงกันเป๊ะ, แบบซ้อนกัน และแบบ Wildcard โดยสามารถมีได้หลาย rule ใน 1 path
นอกจากนั้นก็ยังมี request ที่จะให้ข้อมูลสำหรับเงื่อนไขต่างๆ เช่น request.auth ที่สามารถเช็คสิทธิ์ในการเข้าถึง หรือจะเป็น request.resource.size ที่สามารถเช็คขนาดไฟล์ คลิกเพื่อดูรายการของ request และ resource ทั้งหมด
การจำกัดการเข้าถึงไฟล์ของ Cloud Storage for Firebase นั้นจะใช้ Firebase Authentication มาเป็นตัวกำหนดสิทธิ์ ซึ่งเมื่อผู้ใช้ได้ผ่านการ authen มาแล้ว ตัวแปร request.auth ใน Storage Security Rules จะสามารถดึงค่า UID(request.auth.uid) และข้อมูลผู้ใช้ทั้งหมดผ่าน Token(request.auth.token) ของผู้ใช้ได้ แต่หากไม่ผ่านการ authen มา ตัวแปร request.auth จะมีค่าเป็น null
คุณสามารถกำหนดการเข้าถึงใน แต่ละไฟล์, แต่ละ path หรือทั้งหมด ได้ทั้งแบบ read และ write ผ่าน Firebase Console โดยเลือกเข้าไปที่เมนู Storage และเลือก tab RULES ตามภาพ
ตัวอย่างการจำกัดการเข้าถึงแบบต่างๆ1. DEFAULT แบบที่จะต้อง authen ก่อนจึงจะ write ได้
2. PUBLIC แบบที่สามารถ read และ write ได้โดยใครก็ได้ (ไม่ต้อง authen)
3. USER แบบที่สามารถเจาะจงผู้ใช้ให้สามารถ write ไฟล์ใน path ของผู้ใช้เอง
4. PRIVATE แบบที่สามารถ read ได้เท่านั้น
Cloud Storage for Firebase Security Rules สามารถตรวจสอบไฟล์ได้ ไม่ว่าจะเป็น ชื่อไฟล์, path ของไฟล์ หรือแม้กระทั่ง metadata ของไฟล์ได้ตัวอย่าง ที่ path /images จะอนุญาตให้อัพโหลดเฉพาะรูปภาพ ที่มีขนาดเล็กกว่า 5MB
การ Publish security rules แต่ละครั้งอาจใช้เวลา 1–5 นาที จึงจะอัพเดทให้เป็นผล
บทความเรื่อง Cloud Storage for Firebase Security Rules นี้ก็สามารถเอาไปประยุกต์ใช้ได้ทั้งกับ Android, iOS และ Web ซึ่งไม่ยากเลยช่ายมะที่จะควบคุม Storage ของคุณให้มีความปลอดภัย ส่วนบทความเรื่อง Firebase Realtime Database Secuiry Rules จะตามมาอีกแน่นอน โปรดติดตามตอนต่อไป สำหรับวันนี้ขอลาไปก่อน ราตรีสวัสดิ์ พี่น้องชาวไทย
Let you learn and share your Firebase experiences with each…
105 
4
105 claps
105 
4
Let you learn and share your Firebase experiences with each other.
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
"
https://medium.com/@yannmjl/hi-tom-dadb2dd2ac0a?source=search_post---------321,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yann Mulonda
Aug 30, 2020·1 min read
Tom Schreck
Hi Tom,
I haven't published something related to Cloud Storage and serverless function but Will do in the near future.
But you wanna set up continuous integrity and continuous delivery on your app project workflow; You might also wanna check out my other article: How to deploy Angular App to Firebase Hosting using GitHub Actions (https://blog.bitsrc.io/github-actions-how-to-deploy-angular-app-to-firebase-hosting-89a93f9c4fe1)
CIO @ITOT Africa | Lead Senior Site Reliability Engineer @ICF󠁧󠁢󠁳󠁣󠁴 | ""Learning is experience; everything else is just information!”
CIO @ITOT Africa | Lead Senior Site Reliability Engineer @ICF󠁧󠁢󠁳󠁣󠁴 | ""Learning is experience; everything else is just information!”
"
https://medium.com/@giuliano/building-an-ai-powered-digital-asset-manager-with-box-84ebd7cdfc3a?source=search_post---------265,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giuliano Iacobelli
Dec 29, 2016·4 min read
Cloud storage solutions like Box are used to organize content of all kind. Sometime it’s about documents sometimes is about media like images or videos.
Keeping your content organized and searchable turns out to be painful if not done in a smart way, especially for content like images. What if you can automatically add metadata and/or tags to the any image you upload on Box and move them in the right folder with zero manual effort?
We worked with the Box team and came up with this handy blueprint that will show you how to integrate Clarifai, an AI service for analyzing images and videos, to your Box account to make image organization effortless.
Note: this blueprint will work on Box accounts at the Business+ level or higher, since it relies on direct download capability and metadata templates.
Before we get started let’s create on Box two folders that our Application will use to organize the content.
Now let’s get a free API Key on Clarifai. Create an account on https://developer.clarifai.com/signup/ then go to Applications and create a new one
Once the application is created copy the Client ID and Client Secret, we’re going to need them in a couple of minutes.
This app is available as a Blueprint, a pre-built template to help you get started with proven integration solutions. To get started click here or on the button below:
You’ll be prompted to pick a name for your project and then a wizard will start. After that Stamplay will prompt you to:
Once you have connected the two services click on Next.
Now it’s time to fill the blanks of the workflow, you’ll be first asked to select the folder where you’re going to upload your images, that’s why we created the upload folder for.
Start typing “upload” in the dropdown and select the folder once it appears. If it doesn’t it depends by Box timings for indexing new content and make it available for search, so if this is the case you’ll need to wait a few minutes and try again.
Once the upload folder is selected and you see that the green check below the dropdown you can move to the following step.
The next step will ask us for the Id of the main folder for this app so we have to provide the Id of the folder “Stamplay DAM”. The Id can be found on the address bar of your browser when you’re into it as you can see below:
The same Id needs to be pasted in the Create Folder step, as this is where our images will be organized and where this application will create subfolders to organize pictures.
Last step is simply to configure and email address where we want to receive notification in case our app is having problems while processing a picture. Type your email address inside the To: field, and a random address in the From field.
The very last step is to initialize a Box Metadata template by clicking on the link provided. In case your Box is running on a plan that doesn’t support Metadata drop us a note and we can help you setting this app to only use Box Tags.
You’re good to go now. Just start uploading pictures in the upload folder and you’ll start seeing your content organized.
At Stamplay we make it easy for people to automate processes and create high value integrations by tying together APIs. If you need help to connect your apps or have an API that you want to make easy to connect with tweet us at @stamplay and/or join our Slack organization.
Enjoy!
Co-Founder @Stamplay, Lego™ for APIs. Enabling people to unleash the power of APIs. A #500STRONG Software Engineer in love with web products. Hip Hop addicted.
See all (2,485)
6 
6 claps
6 
Co-Founder @Stamplay, Lego™ for APIs. Enabling people to unleash the power of APIs. A #500STRONG Software Engineer in love with web products. Hip Hop addicted.
About
Write
Help
Legal
Get the Medium app
"
https://expertise.jetruby.com/firebase-cloud-storage-how-to-and-use-case-dd9810fe041b?source=search_post---------4,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Firebase Cloud Storage is a modern technology that allows for storing and managing various media content generated by mobile app users.
One of its biggest advantages is reliability. Firebase SDK for Cloud Storage works regardless of the network quality. in other words, if a file stops uploading because of slow Internet connection, the process will be automatically restarted. For regular users, this feature saves minutes of waiting.
From a security viewpoint, Firebase Cloud Storage is doing great. Firebase SDK for Cloud Storage can be integrated with Firebase Authentication which provides a simple and user-friendly mechanism of authentication for developers. It allows you to use a declarative security model to enable access based on the name of a file, its size, content type, and other kinds of metadata.
Firebase Cloud Storage SDK also uses a declarative data protection language which allows you to manage the access to the files in the storage and make them public or private.
In plain English, Cloud Storage takes full care of protecting the files you upload.
First, you need to register your application in Firebase Console. When that’s done, add the generated file “google-services.json” to the app.
Go to the “Rules” section and enable the ability to read from and write into the storage. In future, you should provide access only to authorized users.
firebase.google.com
Now you need to create Storage. All the files will be stored in Bucket and organized in the form of a tree structure (just like the file system on a hard drive). To get access to a specific file, you need to create a link to it that can be used for uploading or editing the metadata and removing the files.
Now let’s connect Google Services to build.gradle(Project):
After that, we need to connect Storage SDK to build.gradle(Module):
In the app, we’ll pick am image and send it to Storage. To pick an image, we’ll use Intent. The Uri of the image will be stored in onActivityResult.
Now let’s create Firebase Storage:
To upload the file to Firebase Storage, we need to create a link to the file “img/fileUri”. This will create a child object “child(“img/${fileUri.lastPathSegment}”)” in the Storage. The “img” folder is where the file “fileUri.lastPathSegment” will be uploaded.
We can also create the metadata for the file:
Let’s create a Task that will upload the image with the metadata to Storage putFile(fileUri, metadata):
To listen on a state of the file upload, we need to add OnProgressListener:
Also, to listen on the state of successful upload or upload failure, we need to add OnCompleteListener, OnFailureListener:
Now we need to make sure the file has been created in the storage:
and it has all the necessary metadata:
Now it’s time to create a link to the file in the storage to upload it to the device. Another thing that we’ll need is calling the getFile() method:
To delete the file, we can use the delete() method:
Firebase Cloud Storage is a powerful yet easy-to-use tool for storing all sorts of objects. With its help, you can store images, audio, video, and other types of user content.
Firebase Cloud Storage allows you to smoothly switch from an app prototype to a full-fledged product thanks to being highly-scalable and the ability to process exabytes of data.
If you want to have a more detailed look at the capabilities of Firebase Cloud Storage, you can refer to the official documentation by this link:
https://firebase.google.com/docs/storage
We are a team of web and mobile development jedi.
1.1K 
1
1.1K claps
1.1K 
1
Written by
JetRuby is a Digital Agency that doesn’t stop moving. We expound on subjects as varied as developing a mobile app and through to disruptive technologies.
We are a team of web and mobile development jedi. We believe sharing knowledge pushes the industry forward and creates communication bridges. Our technical expertise includes Ruby on Rails, ReactJS, Elixir, Swift, Kotlin, React Native, and many more.
Written by
JetRuby is a Digital Agency that doesn’t stop moving. We expound on subjects as varied as developing a mobile app and through to disruptive technologies.
We are a team of web and mobile development jedi. We believe sharing knowledge pushes the industry forward and creates communication bridges. Our technical expertise includes Ruby on Rails, ReactJS, Elixir, Swift, Kotlin, React Native, and many more.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@openbom/connected-cloud-storage-and-bom-digitalization-645906cb7347?source=search_post---------155,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Dec 12, 2017·3 min read
Unless you’ve been living under a rock you’ve heard the term “digitization” and “digital transformation”. These terms are everywhere. Our lives are very much connected to digital environments. “Old-school” ways to conduct business are dying or already dead. New working methods enabled by the “digitalization” of everything are here. Today, it’s all about how to use the internet and digital infrastructure to become more efficient.
“Digitization” methods are about change. They are about how to move away from paper, how to optimize communication, and about how to transfer information with ease and share it with people. This is why I’m excited about the new digital transformation OpenBOM brings to users working with traditional desktop CAD systems. Those CAD systems are the bread and butter of everyday workflows. But when it comes to collaboration and communication, files are hard to share digitally. The availability of multiple cloud file storages are brought into play these days, however, files on the cloud doesn’t solve communication problems on its own. It is about the context and other related information of parts and products that promotes effective communication.
We just made available our first set of our CAD plug-ins that enable BOM rich content (such as CAD files) to be uploaded to the cloud storage of your choice. The first two supported cloud storages are Dropbox and Onshape. Our choice was deliberate: Dropbox is a commonly used cloud file storage platform used by many in manufacturing. Onshape is a special 3D CAD platforms. We believe have good reasons to be used thus we wanted both cloud storage platforms support first.
Later this month we are coming with additional file storage systems such as Google Drive, OneDrive, Autodesk 360 and maybe Box.com. Take a look at these two videos each demonstrating this functionality for Solidworks. The first video is with Dropbox. The second, with Onshape.
Solidworks and Dropbox cloud storage:
Solidworks and Onshape cloud storage:
Conclusion. We are helping to digitize traditional file-based environments for manufacturing companies. At OpenBOM, we believe it is all about Bill of Materials with rich content connected and available to users when they share their BOMs containing valuable product information. We don’t wish to dictate what cloud storage to use, rather, we’re looking to support any cloud storage system that might be useful to our users. We are also considering expanding partnership with more cloud storage providers — so stay tuned!!
And tell me what you think? This is just a first shot; more updates are coming. What would you like to see?
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Want to learn more about PLM? Check out my Beyond PLM blog and PLM Book website.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
"
https://medium.com/tech-news-and-opinions/please-sir-may-i-have-some-more-bytes-3cb61a7eef76?source=search_post---------314,"There are currently no responses for this story.
Be the first to respond.
I've been using cloud storage since the days of emailing documents to myself. It honestly never made sense to me to store a document on a memory stick and then have to carry that physical object with me everywhere I went. Perhaps it’s just because I inevitably forget everything important every time I go anywhere. Maybe it’s because I liked not waiting for driver software to instal every time I plugged a flash drive into a new computer.
I've been using Google Drive since its days as Google Docs and in that time it had upgraded from the 5GB starting amount to 15 GB, combining storage with Gmail. The real beauty is that this is free storage; I don't have to pay anything to keep my data in the cloud. Google even stopped counting any documents made using its services against the storage limit.
I picked up Evernote as a way to store digital versions of my class notebooks, because tablets and styluses have still not advanced to the point where they make sense in my note-taking workflow (I’d love to just type everything out, but I take a lot of math-based classes, and typing math is difficult under good circumstances). Evernote’s philosophy on storage is that they will store and allow access to as much as you want/need for free, only limiting your upload amount to 60MB per month. This is quite reasonable and very useful, especially for my use case.
I started using Mailbox for iOS, though, and because they were bought out by Dropbox, it became much easier to handle my attachments through that service than through Drive or any other cloud based storage solution. The advantage over Google Drive is that it is more robust as a backup/storage service than Drive out of the box, most likely due to Google focusing more on office productivity than just storage. Dropbox offers 2GB of storage for free, upgradable to 16GB through free methods.
The one thing that I dislike about Dropbox is that it lacks a modern idea of what free cloud storage should be. Dropbox offers 2GB free for an initial account, and then offers various ways to get trivial amounts of additional free storage (250MB for completing their tutorial, 125MB for linking with Facebook, 1GB for linking with Mailbox, etc). The real way to get additional free space is through other people signing up through your referral link, but even through this method, you only gain 500MB at a time. 500MB! This requires you convincing someone that you know to download dropbox onto their computer, and you only get 500MB!
At one time, this system of memory allocation made sense, and it worked. In a time when flash drives barely held 1GB and were expensive, and when other cloud storage services didn’t exist, setting the free tier at 2GB worked. But it’s now an outdated model. Why would I use Dropbox when Google Drive gives me 7.5x the amount of free storage that Dropbox does out of the box? That doesn't even account for the fact that Drive offers essentially infinite storage if you're willing to convert documents into Google’s formats. What advantage does Dropbox still offer?
It gets especially ridiculous with Dropbox’s newest effort, an app called Carousel, which aims to be the home for all of your automatically backed up photos. Yes, they offer 3GB additional storage for free, but another service, Flickr, offers a free terabyte of storage for photos, and the app handles automatic back-up as well. 1TB! 1TB just for photos, not eating up the other storage that you have in other services. That’s 204x the amount of storage that Dropbox gives you for free for using Carousel (in addition to the 2GB you get for just using Dropbox). Even if you have the maximum amount of possible free storage, Flickr still offers 64x the amount of storage. For free.
The other thing that Dropbox’s system of allocating free space does is it causes users searching for more space to spam their friends, family, and social media with their referral links (mine, by the way, is here) in a desperate attempt to gain miniscule amounts of extra space. It’s annoying, and it’s not something that we should have to do in 2014.
So what does it all mean? What’s the point of this post? No storage solution is perfect, and you should weigh the pros and cons of all of your options before you make a decision (but applies to everything in life, doesn't it?). But, I think that from a consumer’s perspective, established companies in the market are going to have to change their game if they intend to stay competetive.
Included in this collection are posts about news and my…
Written by
University student, engineer, blogger, audiophile, lacrosse player, wikipedia author, headphone addict, aspiring vlogger.
Included in this collection are posts about news and my opinions regarding technology, from mobile to desktop, from hardware to software. 
Written by
University student, engineer, blogger, audiophile, lacrosse player, wikipedia author, headphone addict, aspiring vlogger.
Included in this collection are posts about news and my opinions regarding technology, from mobile to desktop, from hardware to software. 
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@bethgsanders/cloud-storage-how-to-store-files-with-dropbox-e4338e4f735a?source=search_post---------137,"Sign in
There are currently no responses for this story.
Be the first to respond.
Beth Gramling Sanders
Jan 23, 2021·6 min read
Cloud storage is an important safety measure for your critical files, and most cloud storage apps provide file sharing capability. My favorite tool is Dropbox, which I’ve been using for years. I first posted about Dropbox back in 2016, when this post was originally written. It is the easiest to use of all of the cloud storage apps I’ve used (and that’s quite a few).
"
https://medium.com/google-cloud/google-cloud-storage-what-bucket-class-for-the-best-performance-5c847ac8f9f2?source=search_post---------28,"There are currently no responses for this story.
Be the first to respond.
One of the most common performance questions we get with respect to Google Cloud Storage is related to “What type of bucket I should use for the best performance?”
Well, to help figure that out, let’s describe the buckets, and run some tests to figure out the best situations.
GCS asks you to create “buckets” in order to place your assets, where the first question is what type of service bucket you want:
Regional, Multiregional, nearline or cold-storage.
Nearline and Coldline are not intended for high-performance systems, so we’ll ignore those right now, and look at the performance of the Regional and Multiregional buckets.
First off, let’s clarify some terminology:
There’s a common scenario where you want to have some client, or on-prem system uploading data to a cloud environment to do compute work, and then returning the information back to the client. Or, in sysadmin terms : “You want your VMs close to your data source to maximize throughput”
Regional GCS buckets guarantee all data in this bucket lies in the specified region, for this exact reason.
Now, to be fair, you don’t have fine grained control what subregion in the region your data is in; over time it can migrate and move. As such write latency is replicated to 2 locations, where there’s different metadata listings for each location it’s copied for fault tolerance. This can cause a remote-round-trip sync write to all the other meta data locations on a write (due to strong read-after-write consistency).
This means Regional buckets are great for data processing since their physical distance is fairly tight, and the overhead of write consistency is low.
Multiregional Storage, on the other hand, guarantees 2 replicates which are geo diverse (100 miles apart) which can get better remote latency and availability.
More importantly, is that multiregional heavily leverages Edge caching and CDNs to provide the content to the end users.
All this redundancy and caching means that Multiregional comes with overhead to sync and ensure consistency between geo-diverse areas. As such, it’s much better for write-once-read-many scenarios. This means frequently accessed (e.g. “hot” objects) around the world, such as website content, streaming videos, gaming or mobile applications.
To provide real numbers, we set up a test: upload a 2MB file to a bunch of regional and multiregional buckets, and then fetch that asset (with caching disabled) from a VM in us-west1.
This data appears to show that multiregional buckets perform significantly better for cross-the-ocean fetches, however the details are a bit more nuanced than that.
Looking back at the logs, the reason that the Multiregion buckets are performing better in those scenarios, is that the data was duplicated to region (of the multi-region) which provided a better access point (and lower latency) to our fetching client. (To confirm this, I ran the same exact test between us-west1 and europe-west1 directly, and got about ~175ms.)
What these tests show us is that there’s no specific performance difference of the classification of the buckets themselves. Rather the performance is dominated by the latency of physical distance between the client and the cloud storage bucket.
As such, we get a handy little rule here:
Google Cloud community articles and blogs
154 
1
154 claps
154 
1
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@julsimon/i3-instances-and-nvme-booya-57fbf452a1c1?source=search_post---------399,"Sign in
There are currently no responses for this story.
Be the first to respond.
Julien Simon
Sep 1, 2017·2 min read
In a previous post, I used an i2.8xlarge instance (32 vCPUs, 244GB RAM, 8x800GB local SSD storage, 10Gb networking) to build FreeBSD in 17 minutes. Pretty good, but in the never ending quest for performance, I decided to try again with an i3.8xlarge instance, as the I3 family is now supported by the latest FreeBSD 11.1 AMI released at the beginning of August. Kudos to Colin Percival for maintaining the FreeBSD AMIs!
The I3 family is more than a generation bump. Along with the FPGA instances (F1 family), they’re the first to support non-volatile memory express (NVMe) SSD instance store volumes, which deliver a massive performance upgrade compared to traditional SSDs. Thanks to this new technology, the I3 family can deliver up to 3.3 million IOPS at a 4 KB block and up to 16 GB/second of sequential disk throughput.
Along with a CPU upgrade (from Ivy Bridge to Broadwell), we should definitely see a nice improvement in our build time. Let’s get to work.
We’ll start by launching an i3.16xlarge instance (64 vCPUs, 488GB RAM, 8x1900GB local NVMe storage, 20Gb networking), with the latest FreeBSD 11.1 AMI (ami-ab56bed2 in eu-west-1).
A few minutes later, we can connect to the instance. Let’s take a look at the NVMe devices.
Now, let’s create two ZFS pools with four disks each: one for /usr/src, one for /usr/obj.
OK, now let’s fetch and extract the FreeBSD distribution.
Let’s build. Ready… set… go!
And the result is… 10 minutes and 54 seconds.
This is a 35% speedup compared to the 17 minutes we got on the i2.8xlarge. Granted, the i3.16xlarge has more cores, but given the structure of the FreeBSD build, they’re hardly ever put to work. Amdahl’s Law strikes again.
I’m guessing that most of the improvement does come from the crazy-fast NVMe storage system. To confirm this, let’s run the same test using memory disks.
No improvement at all. This leads me to think that CPU is now the bottleneck. Indeed, the quest never ends. Now, EC2 team, could be please get NVMe on the upcoming C5 Skylake-based instances? Please please please? :D
Thanks for reading and happy testing.
Chief Evangelist, Hugging Face (https://huggingface.co)
16 
16 
16 
Chief Evangelist, Hugging Face (https://huggingface.co)
"
https://medium.com/@alibaba-cloud/implementing-a-cross-region-and-hybrid-cloud-file-sharing-service-using-cloud-storage-gateway-and-c584a5836677?source=search_post---------120,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Feb 19, 2019·7 min read
By Johnson Chiang, Solutions Architect
This article will demonstrate a file sharing service solution for file sharing cross region, between on-premise IT environment and public cloud.
The main components of this solution architecture include Cloud Storage Gateway (CSG) and Object Storage Service (OSS) as the backend to offer following considerable benefits:
Such architecture would be especially useful to achieve network resilience when transferring file in- and out- China to international locations. You will also walk through how to implement an example to fulfill the file sharing needs for China region’s and international region’s consumers.
What You Will Build
For demonstrating the cross-region and hybrid-cloud file sharing scenario, we will build following topology which consists an online CSG file gateway in China side, an on-premises CSG file gateway in overseas side, and an OSS bucket to be the backend storage of both file gateways. For OSS bucket, we choose Hong Kong region which is geographically closest to both China and our data center in Taiwan.
The following steps gives an outline of the overall flow of the implementation:
Let’s us now look at the detailed steps of the implementation.
First, create an OSS bucket, with a preferred name like oss-file-sharing. The OSS bucket will be associated with two File Gateway instances to mount in our file sharing architecture. In this example, we chose Hong Kong region which would work best for the File Gateway and client in Taiwan IDC.
This section guides you through sample tasks to deploy an online CSG instance and create the File Gateway; for more information, please see https://www.alibabacloud.com/help/doc-detail/87408.htm.Create a Gateway Cluster (for example, CSG-Cluster-BJ)
Create a Gateway instance (for example, named CSG-BJ)
Create a Cache for the running Gateway.
Create a NFS share by using gateway management console:a
Create a Cache (for example, Cache_b) in Caches tab
Create a Resource (for example, BackendOSS) in Cloud Resources tab
In About, check the information and take a note of the assigned intranet IP address (192.168.1.131 as shown below in our CSG instance)
This section covers the steps of operating an ECS-hosted Linux client to mount the remote NFS mount point shared by the CSG File Gateway. And then we write some files to the mounted directory and verify the newly written files are synchronized to the OSS bucket.Log on to an ECS server in the same VPC and VSwitch (VPC-BJ and VSwitch-BJ) with the CSG instance created in Step 2 (CSG-BJ).
On the ECS, mount the remote NFS-shared directory (for example, /share1nfs). And then copy files (for example file1, file2, file3) to the mounted directory.
Check the OSS console and see that the files have been transferred from the CSG File Gateway in Beijing to the OSS bucket oss-file-sharing in Hong Kong.
Now, deploy an on-premises CSG instance and a File Gateway by following instructions at How to Install Cloud Storage Gateway. Alternatively, you can opt to deploy an online CSG instance in another Alibaba Cloud region by following instructions at Step 2.
You will need to log on to the CSG web console to configure the required settings: Caches, Cloud Resources, and CIFS settings. Following shows the configurations of the CIFS share for example:
Notice that we select Sync Mode for Data Access Mode and Yes for Enable Remote Sync.
Above screenshot shows that a SMB User named user2 is added for the SMB Share.
Next, you operate a Windows system, which is in an intranet with the CSG instance created at Step 4., to mount the CIFS share (for example, \\192.168.5.36\share2) as network drive. You can now see those 3 files under the mounted directory which maps to the backend OSS bucket (namely oss-file-sharing).
So far, you have completed the architecture where 2 cross-region clients being able to access NAS filesystem, shared by a near-client CSG. Now you can further test to do write and read operations using either client, and the CSG will transparently synchronize and map the its file structure to the one backend OSS bucket.
Featured by CSG file gateway and OSS, we can now implement a simple and effective architecture for sharing and distribution of files. In addition, the NAS-based filesystem reserves the convention usage of file management user interface.
Given the base of cross region and hybrid (online and on-premises) architecture, now you will be able to make tweaks to adapt to variations of scenarios.
To learn more about CSG, refer to https://www.alibabacloud.com/product/sgw.
You can refer to the following documentation for additional information.
Reference:https://www.alibabacloud.com/blog/implementing-a-cross-region-and-hybrid-cloud-file-sharing-service-using-cloud-storage-gateway-and-oss_594450?spm=a2c41.12560012.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
9 
9 claps
9 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Child_of_Humans/in-a-world-with-wifi-cloud-storage-overnight-delivery-call-centers-on-the-other-side-of-the-f3b5372ae327?source=search_post---------109,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nabeelah Patel
Sep 18, 2018·1 min read
AJ Jones
In a world with WiFi, cloud storage, overnight delivery, call centers on the other side of the world.. I guess we need to rethink the olden days idea of city hubs like London and New York
These cities should be abandoned by small and medium firms. We don’t need to spend six hours commuting that’s like a hamster stuck in a wheel.
I would rather resign and live with just a tent in a forest than spend six hours in a car.
Different priorities I guess.
Millennial female;optimistic, nature loving, vegetable eating artist. Do spreadsheets for fun,and have a 50 000 word draft in need of editing.
See all (906)
8 
1
8 claps
8 
1
Millennial female;optimistic, nature loving, vegetable eating artist. Do spreadsheets for fun,and have a 50 000 word draft in need of editing.
About
Write
Help
Legal
Get the Medium app
"
https://artplusmarketing.com/cloud-storage-analysis-box-vs-google-drive-vs-dropbox-eb5646727553?source=search_post---------73,"The purpose of an h1 or heading on the homepage is identifying your business to the next visitor. I don’t have a particular interest in cloud storage, but where to store information is a universal annoyance that allows us to effectively dive into the essence of copy (i.e. words on webpage).
Since Box is overtly targeting enterprise — unlike Google Drive & Dropbox — each word in their H1 or primary heading is entirely capitals, and there is no use of the word “yours” because in the enterprise world, nothing you make is yours. Rather, it’s “Your Work,” as Box clarifies. As in, you’re responsible for the files, but can’t “share them with anyone.” This targeting also explains what follows the first line break; Box wants (co-workers to) “Come Together,” whereas Dropbox wants (consumer to be assured) “you do,” and Google is the incumbent that doesn’t need ‘trendy techniques’ like line breaks in the H1 heading, or the use of an H2 subhead altogether. Meanwhile as Dropbox wants consumers and enterprises they contradict themselves with with the use of “works” and “you.” I’m 100% onboard with this contradiction — it implies you the consumer can get the experience of enterprise, and ultimately creates an honesty of we-want-small-customers-but-really-want-big-customers.
In the subhead (Google, just kick it while I dive in), Box starts with “Simple” while Dropbox starts with “Get.” The H2 subhead is all about re-assurance. Box is trying to get influencers of the enterprise software purchasing decision to repeat the double adjectives they start with: “Simple, secure.” Seriously, are B2B homepages designed with this purpose? Yes, B2B homepages exist to help employees make arguments to their bosses… But starting your subhead with two adjectives, what is this, a press release during the dotcom bubble? J/k maybe it’s working for ‘em…. Before I get too far into any of this, any of that, or any copy for that matter, lets discuss the triple use of “any” in Dropbox’s H2 subhead. They want the consumer to know, if they have “any” use case for cloud storage, Dropbox will be able to handle it.
Cloud storage is about where your files go. Box hits the nail on the head with “Where,” Google is — relatively verbosely — about “A safe place for,” and Dropbox forgets place until the subhead (where again to remind you, it will work “from anywhere”).
Lastly, don’t forget what the verb choice says about the company’s objective. Box wants to be where “Your Work Comes Together,” Dropbox actively “works the way you do,” and Google, again the incumbent, wants to just (be). There is no verb for Google’s H1 heading. Once you own ‘search,’ I imagine your choose adjacent product offerings via drinking games because what’s the difference in adoption rate? I’m jus’ playin, the bigger the corporation the more money to gained and lost for every delta in the conversion. And that’s why Google’s keyword is “safe.” If someone is going to store “all your files,” they better create a feeling of “safe…” or corporate togetherness…. or you doing you….
Box goes with the horizontal, “Watch Demo” vs. “Try Now;” Dropbox goes with the vertical “Sign up for free” vs. “or sign in;” and Google Drive, on the other hand, does not have an above the fold call to action. Google just believes after seeing their brand, their product name and their identifier, the visitor will gladly take another action to get to a call to action. That’s a choice in itself, and it’s an easier choice to make if you’re Google and can casually drive all search traffic from one of our most common verbs to your cloud storage product. When it comes to vertical vs. horizontal, know that Box is not disappointed if you “Watch the Demo.” Most read left to right and while this first button does not have the blue background, its the first call to action read and if you click it then watch the video, you are more likely to pay Box money.
Without a readily accessible “sign in” button, Box is catering toward leads and Google is assuming you’re already logged in, whereas Dropbox is the only one allowing users who are logged in to reach the log in screen in one click. So, it’s different takes on how to treat who is already paying you. Ultimately — and this is where the fun of competition comes into play — the primary call to action is free usage with the likelihood that you will pay later when the opportunity cost to switch is inconvenient.
Cloud storage, the homepage is about grabbing that lead, adding value, and then the provider can change the price of storage whenever it is convenient. Or so my pessimism says. It all depends on brand trust. Time to think about a different place for my messaging.
That was a fun forty-five minutes. I don’t really have time to scroll down, and neither do you. Talk later,
@DavidSmooke
We publish creators.
11 
2
11 claps
11 
2
Written by
https://www.davidsmooke.net/ https://hackernoon.com/ Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant,
We publish creators. Why they make. How they see. What they do. Everyday is the creators' perspectives. This is the corporate blog of @AMI.
Written by
https://www.davidsmooke.net/ https://hackernoon.com/ Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant,
We publish creators. Why they make. How they see. What they do. Everyday is the creators' perspectives. This is the corporate blog of @AMI.
"
https://medium.com/@alexellisuk/introducing-webhooks-for-minio-e2c3ad26deb2?source=search_post---------354,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alex Ellis
Feb 7, 2017·3 min read
Minio provides integration with a range of backend systems which enables you to build a complete solution for your projects. The team has recently integrated a pull request from the community to add Webhook support and wanted to tell you a bit about it.
First we will look at what Webhooks are, some examples of what they can connect with, then run through an example end-to-end of how to configure a Thumbnail generator with Minio.
Webhooks are the de facto standard for API communication over the Internet. In technical terms a webhook is where a server application sends a notifcation to another system as a HTTP POST with a JSON payload. To consume Minio webhooks in your own application you just need to create a HTTP/s endpoint and edit your ~/.minio/config.json file.
In this post we will to highlight a use-case for automatically creating thumbnails in near-realtime without the need for polling.
Other Webhook uses with Minio:
Benefits of webhooks
Up until recently the only way to get events from Minio over HTTP was by polling which means creating a long-running task on your computer to checks for new messages in a loop. Polling has its benefits in that it’s easy to setup and doesn’t require an HTTP endpoint, but it can also be wasteful with resources.
Well designed APIs allow their users to receive a stream of events in real-time as they happen and now Minio officially supports: NATS, Kafka, Redis, PostgreSQL and webhooks (HTTP/s). For more on the range of notifcations check out the docs: Minio Notification docs.
Use-case: Thumbnail generator
Before we go ahead and explore how to configure your webhook endpoint let’s talk about a common use-case.
Imagine that you have a team uploading photos to your server in full-resolution, but that these need to be resized to thumbnails for an online catalog. Minio Developer Harsha has put together an example project in Node.js called Thumbnailer. It listens for S3 put events with a file-extension of “.jpg” and then fetches, resizes and re-uploads the associated file.
github.com
1. Install Thumbnailer
Open a Terminal then create two new buckets for the code and install “thumbnailer”. (You will also need git and nodejs)
Before starting the thumbnailer code we need to edit the config file at config/default.json. Add the configuration for your server and then start the code.
2. Start Thumbnailer
3. Configure a webhook ARN for your Minio Server
The server component of Minio is configured through a file at ~/.minio/config.json — here you can configure one of the supported queues or notifcation systems. We will edit this file and add our HTTP/s endpoint, but first make sure that you are running the latest version of Minio server and client.
Edit your ~/.minio/config.json file and add the following section:
The endpoint can be set to any site and TCP port on your network or the Internet with either an HTTP or HTTPs scheme. The endpoint needs to be live and reachable when you re/start your Minio server.
4. See it in action
Set-up a notifcation for the webhook ARN:
In a separate Terminal upload a .jpg photo to the images bucket on your Minio instance:
Wait a few moments, then check the bucket’s contents with mc ls — you will see a thumbnail appear in the images-processed bucket.
Over to you
You may already consume a number of webhooks in your application. Minio now enables you to get notifcations in near real-time about activity on your S3 buckets.
You can test out Minio in no time with Docker:
Head over to the project homepage for more instructions. And if you’d like to know more or just get connected, please join the team on the Slack community below:
Written for Minio by Alex Ellis, Docker Captain
CNCF Ambassador. OpenFaaS & Inlets founder — https://www.alexellis.io
63 
1
Thanks to Harshavardhana. 
63 
63 
1
CNCF Ambassador. OpenFaaS & Inlets founder — https://www.alexellis.io
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/hybrid-cloud-storage-cross-cloud-disaster-recovery-bdc73859ffe8?source=search_post---------170,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 8, 2018·14 min read
tion configuration option, and decide whether or not to encrypt the backed-up data. Hybrid Backup Recovery uses the AES256 cryptographic algorithm to ensure absolute security for data stored in the disaster recovery gateway. You can decide whether or not to turn on the encryption based on the features of your business. I will use the default Off status and then submit.
Backup encryption configuration
After submitting the configuration, you will be directed to the Hybrid Backup Recovery gateway management home page. On the home page, you can not only easily configure backup and recovery jobs, but also intuitively check and monitor the real-time status of each job. Now, the status of the disaster recovery gateway changes to Running normally.
Disaster tolerance gateway home page
Let’s get back to cloud vendor T’s console. We can create two virtual machines based on the image files of the Hybrid Backup Recovery controller and gateway provided by Hybrid Backup Recovery: the host of the controller runs on the Windows 2012 R2 64-bit system, 2-core CPU, 4 GB MEM, and uses a 128 GB SSD cloud disk as the system disk; while the gateway host runs on the Ubuntu 14.04 64-bit system, 4-core CPU, 8 GB MEM, and uses a 128 GB SSD cloud disk as the system disk, and a 500 GB SSD cloud disk as the data disk. For procedure of importing the image file and creating a cloud host based on the image file, refer to the relevant steps in “Cloud Storage Gateway-based cross-cloud replication”. The cloud hosts we created are as follows:
Disaster recovery gateway internally deployed by cloud vendor T
After completing the deployment, you will be able to directly enter the IP address of your controller host (Windows host) in your browser, then you will see the same disaster recovery gateway interface as we configured earlier. The steps are similar, and you need to provide your Alibaba Cloud account’s AK, the account and password of the gateway, etc. This part is omitted here.
After completing the configuration, enter again, and you will be able to see the logon interface of the disaster recovery gateway (offline) deployed internally by cloud vendor T. Logon steps for the online gateway and the offline gateway created on Alibaba Cloud are different. Alibaba Cloud console has already integrated the auto-logon function. You can log on to the disaster recovery gateway without entering your username and password. Enter the username and password to log on.
Disaster recovery gateway logon interface
After logging on, you will see exactly the same disaster recovery gateway home page as that of the online version.
Home page of the disaster recovery gateway deploy internally by cloud vendor T
Next, connect the disaster recovery gateway to the Oracle server, and back up the database host to the cloud disaster recovery warehouse using the disaster recovery gateway. First, let’s take a look at the information and content of data tables in the Oracle database. Using the following query, we can see there are 971 user tables in the Oracle database, as well as the size of each data file. Pay attention to the host name of the Oracle server. We found that the host names of some cloud vendors may start with numbers, which may cause a failure when creating ECS instances while failing over to Alibaba Cloud ECS. Alibaba Cloud’s cloud host name must comply with the following rules: must contain [2, 128] English or Chinese characters; must start with a upper/lower case letter or a Chinese character; must not start with http:// and https://; and may contain numbers, half-width colons, underscores (_) or hyphens (-).
Total number of Oracle data tables
Oracle data file size
Before backing up the Oracle host, execute the precheck script to check whether the Oracle host configuration meets the preconditions for backup. You can modify and configure the environment based on the execution results of the script, to meet preconditions for Hybrid Backup Recovery backup.
precheck script execution
precheck execution results
Let’s back up this Oracle host. Return to the management home page of cloud vendor T’s disaster recovery gateway. Choose Back up.
Disaster recovery gateway backup
On the Protect Server page, choose the host type. Then enter the server IP address and the server’s username and password. In this case, we chose Windows server. Note that the server IP address should be the intranet IP address of the Oracle host. This provides high-bandwidth intranet for the backup and recovery traffic.
Configuration of Protect Server
After clicking Submit, we can check the progress by choosing Monitoring > Jobs. It takes a few minutes to compete a newly added job. You can click Job details, or the i icon, in the Operation bar to view job details.
Add the Protect Server job
Detailed information of the successfully added job is shown as follows. The status of each step is OK. If the job fails, an error message will be displayed on this page. You can handle the error at your discretion, or seek help from Hybrid Backup Recovery engineers.
Job details
Close Job details. We can view the newly added Oracle server to be protected on the Backup page. We can see that all of this host’s information has been retrieved by the disaster recovery gateway, such as the disk data and the Oracle database data. We can perform different backup operations based on this information.
Oracle host disk information
Oracle host database data
Before starting the backup, we must create a backup policy in Backup Policies. The most important role of the backup policy is to define the backup schedule. In addition, Enable cloud replication must be selected. Otherwise, the backup data will only be stored locally at the gateway, and will not be uploaded to the cloud disaster recovery warehouse. The number of days of Local copy and Cloud replication indicates the number of days that the data will be kept locally and on the cloud. Click Save.
Creating a backup policy
After saving the policy, go to the Backup page. Click Plan (floppy disk icon) to create a backup plan. We need to bind a backup policy for a backup plan. Let’s use the policy that we created in the previous step. Then click Save. We can combine different backup policies, for example combining full backup and incremental/differential backup to make a reasonable backup plan that has a small RTO. We suggest you perform a full backup every week, and appropriately perform incremental or differential backup everyday based on the characteristics of your applications. Backup latency interval defines the time interval between two backups. If a backup operation is still in progress when we start a new backup, the new backup operation will be skipped.
Backup policy combination
Creating a plan
After submitting the plan, we can click the Run Now icon to start the first backup.
Run Now
The backup type for the first backup is Full, which means full backup. After we click Submit, the full backup job will be executed.
Click Submit to immediately run the backup job
We can go to Monitoring > Jobs to view the execution details of the backup job.
Backup details
We can also see that the backup job is running in the Dashboard.
Dashboard information
Shortly after the backup starts, data transfer will start. Now, go to Task Manager > Performance of the Oracle host to view the speed of uploading the backup data to the gateway. As shown in the following picture, the upload speed is about 800 Mbps, or 100 MB/s. This matches the read performance ofDisk 1 (D drive) (105 MB/s).
Oracle host upload performance
A few moments later, we can see from the following backup job details that backing up 148.4 GB of data took 36 minutes and 29 seconds, and the average speed is 69.42 MB/s.
Backup job details
Return to the Dashboard. We can see it from the storage information that the 148.4 GB raw backup data occupies 130.36 GB space in local storage. That’s because the Hybrid Backup Recovery gateway has removed duplicate data during the upload process. Deduplication not only saves the gateway storage space and improves the backup efficiency, but also improves the efficiency of uploading the backup data to Alibaba Cloud via the Internet.
Backup space occupation
Now, you must have noticed that there is still no data in Cloud Storage. In fact, after the backup data has been completely uploaded to the disaster recovery gateway, the corresponding Upload job (to the Cloud Storage) is automatically created and executed.
Uploading backup data to the cloud disaster recovery warehouse
Click the upload job, then we will be redirected to the upload jobs list. You can also view the upload progress and details here.
Upload job
From the Monitoring tab of the cloud host, we can view the performance of upload data from the backup gateway to Alibaba Cloud’s cloud disaster recovery warehouse. Currently, the instantaneous upload speed is 100 Mbps, or 12.5 MB/s.
Upload performance
After the upload is complete, we can view the uploaded data and the total duration of the upload in Job details. This upload took 3 hours 11 minutes and 20 seconds, and the average upload speed was 13.24 MB/s.
Upload job details
Now, the dashboard shows one successful upload and information relevant to the occupied cloud storage space.
Successful upload and cloud storage space
You can log on to Alibaba Cloud console, and go to the Hybrid Backup Recovery page. On the Overview page, we can see one protected server and one protected database. This was automatically recognized by Alibaba Cloud’s disaster recovery gateway after we uploaded the backup data to the cloud disaster recovery warehouse.
Hybrid Backup Recovery summary page
Go to the Disaster Tolerance Center > Protected Servers, to view the detailed information of the protected host.
Protected server
Open Alibaba Cloud’s disaster recovery gateway, we can see that Cloud Storage shows the same storage size as the gateway of cloud vendor T. So far, the link that successively connects cloud vendor T’s Oracle server, cloud vendor T’s disaster recovery gateway, Alibaba Cloud’s disaster recovery warehouse, and Alibaba Cloud’s disaster recovery gateway has been established. If you want to fail over the protected server from the disaster recovery warehouse to Alibaba Cloud’s ECS host, you can directly use Alibaba Cloud’s disaster recovery gateway; if you want to fail back the protected server to cloud vendor T’s Oracle server, you can simply use the disaster recovery gateway deployed internally by cloud vendor T. Notes: First, the local storage is 2.69 TB. You must remember that when we create the Alibaba Cloud disaster recovery gateway, a MC host and a Store host will be automatically created. The store host is provided with three 1 TB cloud disks. The 2.69 TB space comes from these three cloud disks. Secondly, the local storage space is not used. That’s because Alibaba Cloud’s disaster recovery gateway did not actually pull the protected data from the cloud disaster recovery warehouse to local storage.
Alibaba Cloud disaster recovery gateway’s storage space
Now, we choose to fail over the protected data from the cloud disaster recovery warehouse to an Alibaba Cloud ECS instance. This is implemented on Alibaba Cloud’s disaster recovery gateway page. Go to the Server Recovery page. We can find the protected Windows server in the cloud disaster recovery warehouse, which is the Oracle server of cloud vendor T. Attention: Font colors of backups on the list can be used to differentiate different backup statuses. For example, the font color of this backup is blue, it indicates that the backup data is on the cloud. Specifically, the blue color indicates that the backup data is only stored in the cloud disaster recovery warehouse; the green color indicates that the data is stored both locally and in the cloud disaster recovery warehouse; while the black color indicates that the backup data is only stored locally.
Server Recovery page
Click the Recovery icon in the Operation bar of the Server Recovery page. There are some fields to be entered on the Recovery Configuration page, such as the instance type, disk type, and VPC network. Because the Oracle server we created is 4-core CPU and 8GB MEM, we will choose ecs.hfc5.xlarge. Click Submit after verifying all fields have been correctly entered. Note that if cloud vendor T’s private IP address is different from Alibaba Cloud’s VPC IP address, we need to select Enter private IP address and specify the private IP address.
Recovery configuration page
After submitting the recovery job, we can find this recovery job by choosing Monitoring > Jobs. Likewise, we can view the recovery job details.
Recovery configuration page
After submitting the recovery job, we can find this recovery job by choosing Monitoring > Jobs. Likewise, we can view the recovery job details.
Recovery job details
From the above job details, we can see that the recovery has already been completed. Now, the Hybrid Backup Recovery gateway will automatically create an ECS instance in the current region, which is Shanghai in this case. The content of the ECS instance will be identical to that of the protected Oracle host at the source end.
ECS host after recovery
Log on to this ECS host, which is a Windows host, to view the Oracle data files. The data files are identical to those at the source end, including the metadata such as file modification time. You can even check the files’ MD5 values to verify.
Recovered Oracle data files
Go to the Oracle server to check the table count, and see if it is the same as that of the source end. There are 156 tables, too. Of course, you can proceed with your business in the disaster recovery ECS host, and fail back from the disaster recovery ECS host to cloud vendor T’s Oracle server after the disaster is recovered. This is indicated as Step 6 in the architecture diagram. The operation is similar and will not be demonstrated here.
The number of recovered Oracle tables
The backup scenario (backing up cloud vendor T’s entire Oracle server to cloud vendor T’s disaster recovery gateway, and then to Alibaba Cloud’s disaster recovery warehouse) and the recovery scenario (recovering from the cloud disaster recovery warehouse to Alibaba Cloud’s disaster recovery gateway, and automatically creating an ECS host that is identical to the Oracle host of cloud vendor T) perfectly verifies that Hybrid Backup Recovery achieves backup and recovery of applications and the entire host in cross-cloud/multi-cloud scenarios. The entire process is very simple. Hybrid Backup Recovery supports backing up and recovering the entire server and mainstream enterprise applications, such as all Oracle (support for Oracle RAC will be added shortly) and SQL Server versions. In addition, the advanced compression and deduplication service saves network bandwidth and space occupation during the backup process; the cloud disaster recovery warehouse supports resource distribution based actual needs and unlimited elastic scaling; the disaster recovery ECS instances generate no fees after they are shut down — minimizing your costs from multiple aspects. We have turned the expensive traditional disaster tolerance solution into an out-of-the-box service with 0 threshold, achieving Alibaba Cloud’s goal to benefit all small and medium sized enterprises using cloud computing technologies.
We suggest you go to Alibaba Cloud console to activate Hybrid Backup Recovery, and use Alibaba Cloud’s advanced technologies to protect your business.
We can aggregate Hybrid Cloud Storage Array, Cloud Storage Gateway, and Hybrid Backup Recovery into a cross-cloud disaster recovery (multi-cloud disaster tolerance/recovery) solution, providing a complete set of efficient cross-cloud disaster tolerance backup solutions for customers of third party cloud vendors. In the order of high to low RTOs, you can choose Hybrid Cloud Backup, Hybrid Cloud Storage Gateway, and Hybrid Backup Recovery based on your needs. Hybrid Cloud Backup and Hybrid Cloud Storage Gateway only support backing up and recovering files; while Hybrid Backup Recovery supports backing up and recovering files, applications, and the entire server.
Of course, we need to deploy some hosts and applications on a third-party cloud. The solution that delivers the lower RTO requires higher cost. However, as you can tell from the previous steps, the operation is still very simple, and the cost is far lower than that of traditional solutions. In order to maximize the backup performance and minimize your costs, we suggest that you configure the highest bandwidth and use the pay-as-you-go payment model for your server’s network if it’s deployed on a third-party cloud for cross-cloud disaster tolerance backup. These allow data from the source end to quickly reach the target end of backup, without significant cost increases for cross-cloud disaster tolerance backup.
Diagram of hybrid cloud storage-based cross-cloud disaster tolerance backup
To learn more about Hybrid Backup Recovery, visit www.alibabacloud.com/product/hbr
Reference:https://www.alibabacloud.com/blog/hybrid-cloud-storage%3A-cross-cloud-disaster-recovery_594133?spm=a2c41.12245015.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/we-distribute/got-zot-mike-macgirvin-45287601ff19?source=search_post---------347,"There are currently no responses for this story.
Be the first to respond.
Mike Macgirvin is a personal friend, and a force to be reckoned with. Living on a farm in the hills of Australia, he has quietly spent his days carving out an elaborate system from scratch, doing the bulk of the work himself.
Over the past decade in particular, he has pushed the boundaries of a next-generation communication platform, restlessly experimenting and taking notes, often inventing his own solutions to complex problems.
This interview is the first part of our interview series, Faces of the Federation.
I’m a software professional currently living in Australia. I’ve created a number of open source projects (most notably Friendica and Hubzilla) and been involved in the related fields of electronic communications and software development for around 40 years.
I spent my younger professional career fixing machines and contraptions for the semiconductor industry. We had machines fabricating things at atomic scales on semi-automated or fully-automated production lines. I spent a lot of time fixing robotic wafer handlers and high-vacuum equipment and writing custom software to automate how all these machines interacted together to reliably reproduce nano-scale devices. This got me deeply involved in communication theory and what infrastructure it took to make devices (any devices) interact.
In the early 80s, I created and federated several BBS apps (dialup bulletin board services) with some of the emerging network protocols. We provided (over analog phone lines) a lot of the services that online social spaces provide today (messages, forums, mail, games, media downloads, etc.) , and federated messages with FidoNet, Bitnet and ARPAnet. I also wrote some email clients and forum software for NASA to link some of the contractors involved in a space physics satellite launch.
Eventually I ended up running Unix servers at Stanford and became an in-house expert in email technologies — just prior to the explosion of the World-Wide-Web. I developed IMAP servers and clients and developed a couple of open source email applications which integrated multiple protocols, filtered views, and end-to-end encyption. This ultimately led to several years as a lead developer building messaging systems for a little internet startup called Netscape. Then I went on to get sucked into Sun/iPlanet and AOL and managed dev teams building groupware and large scale communications servers.
That was not the first protocol I wrote. I was on several messaging standards ad hoc committees and was directly involved in standards and spec review for LDAP mail delivery, IMAPS (SSL IMAP), IMAP4REV1, and authored a half dozen drafts for things like email alias management services and special purpose IMAP extensions.
There’s nothing magic about a protocol. It’s basically just a gentleman’s agreement about how to implement something. There are a number of levels or grades of protocols from simple in-house conventions all the way to internet specifications. The higher quality protocols have some interesting characteristics. Most importantly, these are intended as actual technical blueprints so that if two independent developers in isolated labs follow the specifications accurately, their implementations should interact together perfectly. This is an important concept.
The level of specification needed to produce this higher quality protocol is a double-edged sword. If you specify things too rigidly, projects using this protocol cannot grow or extend beyond the limits and restrictions you have specified. If you do not specify the implementation rules tightly enough, you will end up with competing products or projects that can both claim to implement the specification, yet are unable to interoperate at a basic level.
“I discovered that in 2010 there was no decentralized solution on the ‘free web’ to replace Facebook — in even the most basic sense. “
In early 2010, I left Facebook because I was increasingly concerned about the directions they were taking related to personal privacy. At that time I was looking for decentralized alternatives, as my feeling was that centralization and increased reliance on single providers was a dangerous trend. I discovered that in 2010 there was no decentralized solution on the “free web” to replace Facebook — in even the most basic sense.
There was Status.Net (a Twitter workalike), and some kids in New York were building a “Facebook killer” (Diaspora) but were doing it in private and had no prior experience with communications technologies. There was also a project called Appleseed and another called 6d; but both were barely useable and neither offered anything remotely resembling the interactivity of the Facebook experience.
“There’s nothing magic about a protocol. It’s basically just a gentleman’s agreement about how to implement something.”
I looked at the so-called “Open Stack” that Status.Net used, but it offered absolutely nothing in the way of privacy. There weren’t any existing internet protocols that specifically dealt with this kind of a use-case; sharing “web resources” privately over a decentralized architecture. So I created DFRN to fill in some of these missing pieces:
DFRN (Distributed Friends and Relations Network) provided some simple but workable solutions in all of these areas. It is still being used in Friendica, but the technology is pretty out-dated now and some parts are clumsy. I believe I did a fair job on the specification and an independent developer should be able to federate with it based entirely on reading the spec.
There are two primary components. The first is an authenticated “pipe” that we create between Alice and Bob. Alice can prove that Bob is at one end and Bob can prove that Alice is at the other end. Then you can put most anything you want in that pipe and send it between Bob and Alice. We initially sent XML ActivityStreams, because in its day it was a pretty good model for a wide array of social content.
The other thing I did was spend a long time on the “wall-to-wall” problem and media access, and realized they were essentially the same problem. My server needs to know who is looking at it, even if the viewer doesn’t have an account on my server. This resulted in development of something I call “magic-auth”. You visit Bob’s web page and provide a claim that says “I’m Alice” and Bob’s server goes out and contacts Alice’s server and does a little dance to see if this is in fact Alice. Then it can let Alice write on Bob’s wall or see his private videos. The crypto behind it was based on essentially the same mechanism that “password-less SSH” uses; but modified for the web.
Within a month of the first release, I had implemented federation with StatusNet (“OStatus”, although we couldn’t send private messages to that network), and also we added RSS feeds and email in our social stream. Later we added Twitter and Facebook and Google and had a pretty awesome social experience.
It sucked memory like a pig, but dang, if you were on the internet — I could see you in my stream and in most cases could comment on your posts and you’d see it on your own service.
At the time, OStatus was a decent specification, despite its complete lack of privacy and spam potential. Technically I only had issues with implementing salmon magic signatures, because that specification changed and some projects were using the older spec while I implemented the newer one. Once we all got on the same page, federation between us and Status.Net was pretty good.
But that was then. GNU-Social and Mastodon’s current versions of OStatus are almost unrecognizable and have diverged a long way both from the original written spec and from each other. If you implement the OStatus spec faithfully today based purely on the written specification, you will likely be unable to communicate with either project without a lot of additional effort.
A very basic high level description of the Diaspora protocol appeared in the Diaspora wiki in 2011. Initially, I had problems implementing even basic communications. The protocol wasn’t very well defined, but more importantly, the specification was incorrect, and therefore barely useful. The published description was completely different than what I saw “on the wire”.
I looked at the source code and examined packets for several months before I finally was able to put together what they were *actually* sending. Once I figured out how the protocol really worked, I communicated with Ilya (who was more or less in charge of protocol stuff for Diaspora). I explained what I found and pointed out a number of mistakes that were made. He was very cooperative and within a week or two he fixed most of the protocol issues which had surfaced.
I found several more protocol issues (some of catastrophic severity), but most of my time was spent smoothing over “policy” issues. These things aren’t even discussed in the protocol specifications, but if you don’t adopt the same set of policies you can’t interact. For example what characters are allowed in a username, and do you link hashtags on the sending side or the receiving side? These seem like trivial matters but they can really stuff things up for a service which does them differently.
Unfortunately, a few months after the initial federation module was completed Ilya’s life was cut short; and from that point forward I’ve found the Diaspora developers in general to be very difficult to work with. There is a more recent version of the protocol, released just this last year. This version has a useful specification. There are some bits lacking, but I was able to read the spec and come up with an independent implementation which interoperates.
My complaints with the current Diaspora protocol are simply that the protocol is too restrictive to express the kinds of data and relationships that my projects need to express, and the media privacy support is somewhat weak.
By 2012 it was apparent that unseating Facebook with a decentralised free web project was not going to happen. Anybody who appeared on the scene with a new “social network” was immediately compared/contrasted with Facebook and then laughed at. There were a lot of these and most have vanished as quickly as they appeared.
Subsequently we stopped focusing on “social networking” as a project mission. We concentrated on providing a range of privacy respecting services that were decentralised, yet highly integrated. Our focus now is more towards content management, cloud services, and groupware than social networking. You can use the same interface to share a wiki with your basketball team as you can to share private videos with your girlfriend.
We added blogging features, WebDAV, CalDAV and CardDAV, and also a range of content management tools so you can build a company website using the same interface you use to provide customer assistance. You can have loyalty coupons and only those who have the coupon can see your special offers page.
The first big thing I did was solidify the “magic-auth” protocol. In Friendica it was somewhat limited in what it could do. In Zot and Hubzilla magic-auth is really the core of the service.
Access control and privacy can be attached to any resource on your server and restricted to viewers from across the web — no matter what server they have their account on. Your private videos are only visible to select people; and they don’t need an account on your server to view them. Authentication is invisible and you aren’t asked for passwords or tokens. It just happens as you browse your social stream and visit the websites you find therein.
The second important thing we did is provide “nomadic identity”, which is also built into the protocol. In 2010–2012, the free web lost *hundreds of thousands* of early adopters because we had no way to easily migrate from server to server; and lots of early server administrators closed down with little or no warning. This set the free web back at least five years, because you couldn’t trust your account and identity and friendships and content to exist tomorrow. Most of the other free web projects decided that this problem should be solved by import/export tools (which we’re still waiting for in some cases).
I saw an even bigger problem. Twitter at the time was over capacity and often would be shut down for hours or a few days. What if you didn’t really want to permanently move to another server, but you just wanted to post something and stay in touch with friends/family when your server was having a bad day? This was the impetus for nomadic identity. You could take a thumbdrive and load it into any other server; and your identity is intact and you still have all your friends. Then we allowed you to “clone” your identity so you could have these backup accounts available at any time you needed them. Then we started syncing stuff between your clones so that on server ‘A’ you still have the same exact content and friends that you do on server ‘B’. They’re clones. You can post from either. If one shuts down forever, no big deal. If it has a cert issue that takes 24 hours to fix, no big deal. Your online life can continue, uninterrupted — no matter what happens to individual servers. As far as quality protocol specifications, Zot is at the low end. It started off as an experimental specification and I haven’t seen much interest from other projects in working together on anything — especially something that was invented by a different project. Subsequently Zot is currently only being used in a handful of forked projects. (There was an alternate Zot implementation in Java early on but that project has long been abandoned.) There is enough documentation available that somebody could implement the basic services independently; but might have issues with some of the more esoteric communications like key change notifications and clone sync packets for datasets which they don’t support locally.
We’re currently working on the first major protocol upgrade in 5 years (titled ‘Zot VI’). We will still be able to interact just fine with older zot servers, but most of the services and APIs have been streamlined and separated so that other projects can implement individual pieces of the protocol without requiring them to implement the whole ball of wax including nomadic identity, magic-auth, and messaging. You can write a plugin for NextCloud or WordPress or Diaspora and access Hubzilla private data (or offer private data of your own to share with others) using a Python script, or add just the communications module to your own service without forcing you to implement nomadic identity.
“The way I look at it is that the free web is like family. Everybody has a dysfunctional family.”
We’re also working on federated forums which work seamlessly across all the federated protocols. Currently only ActivityPub has a problem with third-party federation. Diaspora is slightly problematic but I believe we can make it work.
The communications layer will probably add ActivityStreams-JSON as a possible data serialization. When work began on Zot in 2012, this specification was incomplete and changing daily. Hence we used our own internal data format that is mostly compatible, but uses some slightly different naming conventions. Internally it’s just a different driver and Zot can support any number of alternate serializations — subject to the availability of a driver for the chosen format.
At one point, we were close to abandoning social network federation completely (most other services aren’t compatible with either nomadic identity or magic-auth; and those represent our core features/strengths). Then several months ago I upgraded all the federation modules and started working on ActivityPub integration. The way I look at it is that the free web is like family. Everybody has a dysfunctional family. You have black sheep and relatives you really just want to strangle sometimes. Thanksgiving dinner always turns into a shitfight. They’re all fundamentalist Christians and you’re more Zen Buddhist. You can’t carry on a conversation without arguing about who has the more successful career or chastising cousin Harry for his drug use.
But when you get right down to it — none of this matters. They’re family. We’re all in this together. That’s how it is with the free web, even if some projects like to think that they are the only ones that matter. Everybody matters. Each of our projects brings a unique value proposition to the table, and provides a different set of solutions and decentralised services. You can’t ignore any of them or leave any of them behind. We’re one family and we’re all busy creating something incredible. If you look at only one member of this family, you might be disappointed in the range of services that are being offered. You’re probably missing out completely on what the rest of the family is doing. Together we’re all creating a new and improved social web. There are some awesome projects tackling completely different aspects of decentralisation and offering completely different services. If we could all work together we could probably conquer the world — though that’s unlikely to happen any time soon. The first step is just to all sit down at Thanksgiving dinner without killing each other.
This is why I’ve gone back to providing and improving interoperability with other protocols and projects — even if they don’t work perfectly with our core features and services. Hubzilla is currently alone when it comes to being able to see most everybody in the free web family from one place. Friendica isn’t far behind, and I think postActive and Pleroma might get there in the coming months. Then we might have a chance to do something useful with our crazy creations.
As far as protocols go, ActivityPub is a poor quality specification. This is unfortunate for a protocol with such high expectations, coming from an organization whose only function is to produce working specifications for the web. The places where we needed guidance to produce interoperable products (such as privacy and encryption and signatures and even allowed message content) were left undefined or poorly defined.
Many of the things that were specified with any precision turned out to be things that are critical to interoperation and cross-federation of existing web services and the specification restricts the ability for them to interact in some fundamental ways. If the protocol solved any existing problems there might be a good reason for projects to adopt it, but it doesn’t solve any problems that we don’t already have better solutions for.
“While it doesn’t solve any specific problems for me, I enjoy being able to interact with the rest of the free web no matter what protocol they use.”
I was asked early on in the protocol development what specific use cases I’d like to see covered. I answered with several use cases involving nomadic identity and cross-domain decentralized access control. I was met with “Right. That isn’t going to happen. Come up with some realistic use cases.” The point is, I already have solutions for my use cases. If my customers’ needs don’t matter in the design of a protocol and somebody is soliciting my project as a “customer” of that protocol, I can only suggest that this is a complete marketing failure on their part. At the end of the day, I need to provide solutions for the needs of my customers. The same thing applies to every other free web project.
“Eugen is a clever developer and I respect him, but he simply does not have the experience and insights into creating global protocol decisions that might potentially affect billions of people and will work equally well for Twitter clones and newspaper publishers and bloggers.”
While it doesn’t solve any specific problems for me, I enjoy being able to interact with the rest of the free web no matter what protocol they use. So I implemented ActivityPub as a cross-federation service according to the published spec, at about the same time as Eugen implemented it for Mastodon. We both claim to follow the spec in the areas that matter, yet interoperation in some cases cannot be easily achieved without breaking interoperability with some other service.
The W3C/ActivityPub editors decided that since Mastodon has more potential “users” than any other ActivityPub adopters, Mastodon’s implementation is being praised as the gold standard for interoperability.
Therefore everybody else must bend to the way Mastodon does things, and this shuts off about half of the existing free social web from being able to interoperate over ActivityPub. Mastodon uses some pretty aggressive HTML filtering and has made other implementation decisions which are now being forced on everybody else.
Eugen is a clever developer and I respect him, but he simply does not have the experience and insights into creating global protocol decisions that might potentially affect billions of people and will work equally well for Twitter clones and newspaper publishers and bloggers.
“People on different projects tend to refuse to listen to anybody outside their chosen project, or treat them as an enemy, without looking at what the others bring to the table and what core strengths other projects provide and figuring out how to work with them.”
I’m not saying I do either, but the W3C editors haven’t provided a level playing field and I truly believe the specification is now worthless as a unifying force for the free web. We’re probably stuck with supporting multiple competing protocols for some time (years) into the future. This is OK — it is what it is, but any opportunity for free web unification using a common stack has probably been lost. Ironically, I believe this was ActivityPub’s primary goal, and that makes the specifications which restrict the ability to federate seamlessly with other services flawed — critically.
People on different projects tend to refuse to listen to anybody outside their chosen project, or treat them as an enemy, without looking at what the others bring to the table and what core strengths other projects provide and figuring out how to work with them.
As a result, every project re-implements their own incompatible solutions to every federation problem and ridicules any other solutions that others have provided without so much as logging into the service and having a look at how it works. They believe their own project is “special” and someday the masses of the internet will leave the walled gardens and come crawling to their awesome project, begging to use their awesome services. So far even the largest wave of migrations hasn’t come close to 0.05% of the large service populations, and many of those folks tried the free web once and quickly went back to the walled gardens.
Many times I’ve heard things like “I’d use the free web if they had a working events system and forums”. There are projects in the free web that offer these things, but many people on Mastodon and Diaspora do not know this; and so not only do those services not retain the member, but the free web services which actually have these services don’t get a chance to offer their solutions. The person goes away thinking that the free web doesn’t have enough basic features for general use; when in fact only specific projects may be unable to meet their needs.
Because several projects now federate to varying degrees, your choice of software isn’t all that important; just as it doesn’t really matter whether you read your email with K9 or Thunderbird or Outlook.
We have some very diverse projects with even more diverse profile spaces and even when you consider the fact that we’re all speaking completely different languages and protocols, we still have managed to create a space where all can co-exist and to some extent share socially across project boundaries. But we still need to work together, because there can be no special snowflakes. We are *all* the free web. We all have compelling features which may appeal to different groups of people.
Two things. Nomadic identity because it shows how we can offer resilience in decentralized services, and (ironically) the first iteration of [Friendica’s] Diaspora protocol. It was bloody hard to reverse engineer and disassemble their crypto packets, and this challenged me in ways I haven’t been challenged before or since.
I’m fortunate to be living in a beautiful and isolated part of the world and I can sit for hours and watch the night sky, or the fog lifting over the Blue Mountains Wilderness (this is literally across the street); or just watch the kangaroos and cows as they go about their lives. Also playing guitar, and spending quiet time with my lovely wife, family, and friends.
Reporting on decentralization and the free web.
256 
6
Some rights reserved

256 claps
256 
6
Reporting on decentralization and the free web.
Written by
Editor of WeDistribute. Obsessed with Free Software and Decentralization. Also makes things, sometimes with Elixir.
Reporting on decentralization and the free web.
"
https://blog.sia.tech/why-blockchains-are-the-future-of-cloud-storage-91f0b48cfce9?source=search_post---------18,"Bitcoins and blockchain tech are — without a doubt — the future of finance. Our money currently flows through a slow combination of outdated digital and physical infrastructure. There is widespread belief that blockchains, which create trust without the need for a trusted third party, will overhaul these antiquated systems.
But what about cloud storage?
Before companies like Amazon offered cloud services, storage infrastructure was very much decentralized. Companies hosted their own servers in their own offices. This allowed for more control, but also resulted in higher costs. Companies needed in-house expertise to set up and maintain costly servers and drives. They also needed to invest in off-site backups for redundancy.
It’s no surprise that most organizations have switched to the cloud. At about $25 per terabyte per month, services like Amazon S3 offer incredible value and, by replicating data across multiple data centers, offer reliable uptime and redundancy.
This convenience has blinded us from the downsides. When using a cloud service, we put a large amount of trust in third parties. We trust these third parties to secure our most sensitive, most private data. This data is typically unencrypted. Moreover, costs seem low — but that’s because we have no good point of reference.
We trust these third parties to secure our most sensitive, most private data.
I see huge parallels between existing cloud storage infrastructure and existing financial infrastructure. Both rely on trusted third parties. Both could be more efficient and more affordable. Both will be replaced with blockchains.
A blockchain enables the creation of a decentralized, distributed storage marketplace. The most successful marketplaces identify under-utilized resources and match them with large nascent demand. Uber, for example, matched under-utilized automobiles with consumers demanding better transportation options.
On a blockchain storage marketplace, hosts sell their surplus storage capacity and renters purchase this surplus capacity and upload files. Payments take place over the blockchain. Files are encrypted, broken up into fragments, and intelligently distributed across dozens of nodes in dozens of countries.
Using a blockchain enables a few things that were previously impossible:
With these advantages, I believe blockchain storage marketplaces will ultimately replace existing cloud storage infrastructure by Amazon, Microsoft, Google, and others. We are headed toward a decentralized web. There’s much to be excited for!
Sia, by Nebulous Inc., is a blockchain-based decentralized cloud storage platform. Zach works as Head of Operations for Nebulous.
Decentralized storage — Sia, Skynet, and cryptocurrency.
180 
5
180 claps
180 
5
Written by
Nebulous COO, building Sia and Obelisk
Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by
Nebulous COO, building Sia and Obelisk
Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://medium.com/@alibaba-cloud/a-strategic-take-on-cloud-storage-solutions-part-2-optimizing-77e60d09593f?source=search_post---------116,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 30, 2020·5 min read
By Shantanu Kaushik
In Part 1 of this article series on Cloud Storage Solutions by Alibaba Cloud and how to utilize them by requirements, we discussed how important optimizing your cloud storage solution is. We also laid out some of the key scenarios and explained the deep integration of Alibaba Cloud services associated with these scenarios.
In this article, we will discuss the Block Storage service and the different useful usage scenarios. We will also discuss optimizing these storage solutions to achieve more cost-effectiveness with your storage service. In Part 3 (the final article) of the series, we will talk about data durability and the implementation of security practices by gathering important storage metrics.
Alibaba Cloud Block Storage is a storage solution that is attached with the Elastic Compute Service (ECS) instances. It offers high-performance with random read/write operations that can accelerate up to 1,000,000 Input/Output Operations Per Second (IOPS). Block Storage offers low latency and enables automatic replication for storage volumes within the same region to account for disaster recovery scenarios, including hardware failure.
Block Storage works to achieve better business continuity and to provide stability during crucial workload scenarios. Block Storage allows you to create a file system for devices using block storage to achieve data persistence. It supports data encryption with a centralized Key Management System (KMS) for a no-fuss encryption scenario.
Block Storage enables the automated scaling of disks without the need to restart the ECS instance. When deploying an enterprise-level instance, the block storage offers input and output isolation to separate bandwidth assigned to input and output operations. This accounts for a highly-stable and consistent performance.
Alibaba Cloud Block Storage is based on the latest generation of software and hardware methodologies. The networking architecture and the enhanced SSD drives offer a tremendous performance boost over the previous generation storage solutions. ESSDs are the most suited storage type for relational database systems deployed over Alibaba Cloud ECS instances.
When it comes to handling databases, the Block Storage provides a low-latency random access feature, and the ESSDs ensure that your read/write performance stays maximized throughout the database access cycle. ESSDs are deployed in a distributed storage architecture to enhance the experience when working with databases.
Alibaba Cloud Block Storage provides seamless support for NoSQL Databases. The diagram below shows the compatible architecture:
Block Storage provides excellent support, uninterrupted performance scaling, and encryption for data at rest when stored in ESSDs. Block Storage can scale your capacity and manage it without having to disrupt the ECS services with your enterprise deployment.
Any enterprise-level application software requires the lowest downtime possible with data backup and recovery that caters to disaster recovery scenarios. When it comes to reliability, the Alibaba Cloud Block Storage provides high reliability and data persistence features for better business continuity and secure data access in key workloads.
Storage architecture needs to justify the ratio between the size of data and its associated price. Storage needs to be optimized continuously for you to operate optimally. Make it a habit to go through this cycle at different intervals to keep the service operational without putting in massive resources for optimization that might get incurred if you optimize at larger than acceptable intervals.
Some key points to remember:
Object Storage Service (OSS)
Optimizing is one of the essential practices that must be followed by cloud storage solutions. Alibaba Cloud OSS offers numerous management features that will enable you to optimize the storage cost and performance:
OSS Select
OSS Select helps you to better optimize your storage solution by optimizing storage and data retrieval costs. OSS Select allows you to minimize the usage of compute resources for scanning and filtering data in objects. It achieves this by allowing the use of simple SQL statements to retrieve objects.
Bucket Inventory
It allows you to have an overview of objects stored in your buckets. It scans the buckets for objects and generates an inventory for you to consider. This report includes the status of objects, including metadata and encryption status. This helps to speed up the workflows and other big-data tasks.
Data and Storage form the backbone of any enterprise; better data optimization equals more productivity for your business application performance. Maintaining business continuity and data durability is among the essential practices to follow when dealing with cloud storage. The continuous evaluation of the data changes is a highly recommended practice to follow.
In Part 3 of this article series, we will give some ideas to keep in mind while optimizing the Block Storage service. We will discuss the importance of monitoring your storage solution to gather metrics to better your system over time. We will also talk about security practices with your storage solution and how to make sure that data durability is maintained throughout the data lifecycle. In the end, we will answer some FAQs on storage optimization, business continuity, and security.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
2 
2 
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://blog.angular.io/file-uploads-come-to-angularfire-6842352b3b47?source=search_post---------204,"Cloud Storage for Firebase is a serverless way to upload and download binary files straight from the browser. As of today, it’s officially supported by AngularFire. Adding AngularFire to your app allows you to easily and securely manage a Cloud Storage bucket. All without a line of server code.
Get ready, uploading files from your Angular app just got a lot easier.
If you’re not familiar with AngularFire, it’s the official Angular library for Firebase. AngularFire combines the power of Angular, Firebase, and RxJS to act as your serverless backend. It includes modules for the Realtime Database, Firebase Authentication, Cloud Firestore, and today it supports Cloud Storage.
Adding AngularFire to your project is easy. Install Firebase and AngularFire from npm:
Then add it to your NgModule:
Inject the AngularFireStorage module into your component.
Now you’re ready to manage your files from Cloud Storage without a server of your own.
The primary way of uploading files on the web is through the <input type=""file""> tag.
This tag fires a (change) event when the user selects a file. It even allows you to restrict the user from uploading undesired formats with the accept attribute (beware that this is only a client restriction, we’ll cover server restrictions later). Event bindings in Angular make it easy to handle a (change) event and send the file to Cloud Storage.
The sample above creates an AngularFireStorageReference with a randomly generated Id. This reference controls a path in your Cloud Storage bucket. Creating a reference doesn’t initiate an upload, but it allows to start one as well as delete the file saved at that location. Calling .put() on a reference with a Blob beings the upload to Cloud Storage.
You may have noticed that creating a reference and calling .put() is the longer way of calling afStorage.upload(path, blog) shown above. Under the hood the afStorage.upload() method creates a reference, calls .put(), and returns the AngularFireUploadTask for you.
The AngularFireUploadTask is how you’ll monitor the upload progress.
The web platform provides an easy and accessible way of displaying the progress of any given task. This is done through the aptly named <progress> element.
The progress element’s value attribute is easy to control with Angular’s property binding. AngularFire provides an upload observable that you can pipe in the new value as it changes.
The .snapshotChanges() method on an AngularFireUploadTask returns an object with helpful metadata about the upload. Properties such as the totalBytesTransferred, totalBytes in the upload, any metadata provided, the state of the upload, and the downloadURL of the file once uploaded. Using this metadata you can update the UI to show the progress.
Most of the time your UI will display the upload percentage and download url. Since this a common task, we made this a bit easier with two helpful methods: .percentageChanges() and .downloadURL().
Instead of calculating the changes yourself, the .percentageChanges() observable does the work for you. The .downloadURL() observable emits the download URL string once the upload is completed. This simplifies binding this information to your UI.
The best part of these observable methods is that you don’t need to worry about complex mapping or multiple .subscribe() calls in your component code. Just bind them to the async pipe and your users are watching their file upload progress.
But what if your user changes their mind? What if they want to pause their upload because they’re no longer on a WiFi network? Will they be able to resume when they want? What if they want to cancel the upload all together?
A user should be able to pause, resume, or cancel an upload in progress. If or when their upload completes they should be able to delete it as well.
An AngularFireUploadTask contains the following appropriately named methods: .pause(), .resume() , and .cancel() . In the sample above, the component stores the task as an instance property. This allows you to call it from your template:
Ideally you’ll want to update your UI so the user knows the state of their upload. The .snapshotChanges() observable emits this information through the state property.
When the user taps “Pause”, the 'paused' state emits from the observable. When the user taps “Resume”, the 'running' state emits. And of course, when the “Pause” button is tapped, the 'paused' state emits.
The template above will disable each button depending on the state. A user can cancel a 'running' upload, but they can’t click “Resume” unless the state is 'paused' .
Binding the UI to the upload state simplifies your templates because you don’t have to manage stateful properties on your component like: isPaused, isUploading, and isFinished. The state emits from the observable and you bind the expression in your template.
Downloading files from Cloud Storage is real simple with AngularFireStorage. The .getDownloadURL() method on an AngularFireStorageReference returns an observable of a download URL.
Combining the .getDownloadURL() method with the *ngIf directive and async pipe allows you to display the image once it’s downloaded.
Just because uploading and downloading files is easy, it doesn’t mean we can’t secure them from unauthorized access.
Cloud Storage for Firebase comes with a server security rule set. These security rules ensures the proper file formats are uploaded and only the right users have access to the right files.
The <input type=""file"" access="".png,.jpg""> tag restricts file uploads to .png and .jpg. However, this a client only restriction. It has nothing to do with what Cloud Storage will accept. To secure your storage bucket you need to write rules that only allow images to be uploaded at the /images path.
The rules above will let anyone read from the /images path, but only upload a file if it has an 'image/.*' content type (.png, .jpg, .jpeg, etc…) and is less than 5MB.
Get started by checking out our Github repo and the official documentation. AngularFire and the Cloud Storage SDK are each open source projects. If you want to get involved in the project make sure to check out our open issue or file one yourself!
The latest news and tips from the Angular team
2.3K 
5
2.3K claps
2.3K 
5
Written by
Developer Advocate at Google. Working on the Firebases.
The latest news and tips from the Angular team
Written by
Developer Advocate at Google. Working on the Firebases.
The latest news and tips from the Angular team
"
https://medium.com/@hpolatyuruk/let-me-give-you-an-example-1ea68d17a422?source=search_post---------299,"Sign in
There are currently no responses for this story.
Be the first to respond.
Huseyin Polat Yuruk
May 22, 2019·1 min read
György Pusker
Let me give you an example. Let’s say you have software that has only one purpose: managing emails. And for that purpose, sending and receiving emails are two essential features to your project. So they are in the “must-have” category for you.
However, when you are coding you start guessing “What if customers need to save email attachments automatically to their cloud storage (Dropbox, google drive, etc.)” You are developing an email management software. This is not essential. Don’t implement something based on your assumptions.
First, you have to implement essential parts, then you have to listen to your users. You need to collect feedback from them. If they request that feature as well, then you can consider implementing it. If there is a few feedback, don’t take action immediately. You have to have enough amount of feedback to move this feature into your “must-have” category.
Software developer • startups • Write on programming • Simple rule: Move forward • Medium TR: hpolatyuruk_tr • Blog: https://huseyinpolatyuruk.com
See all (67)
1 
1 clap
1 
Software developer • startups • Write on programming • Simple rule: Move forward • Medium TR: hpolatyuruk_tr • Blog: https://huseyinpolatyuruk.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@iamgique/aws-s3-%E0%B8%84%E0%B8%B7%E0%B8%AD%E0%B8%AD%E0%B8%B0%E0%B9%84%E0%B8%A3-%E0%B8%89%E0%B8%9A%E0%B8%B1%E0%B8%9A-%E0%B9%80%E0%B8%82%E0%B9%89%E0%B8%B2%E0%B9%83%E0%B8%88%E0%B8%87%E0%B9%88%E0%B8%B2%E0%B8%A2%E0%B9%83%E0%B8%99-2-%E0%B8%99%E0%B8%B2%E0%B8%97%E0%B8%B5-648012fd7268?source=search_post---------213,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sakul Montha
Apr 14, 2018·2 min read
AWS S3 (Amazon Simple Storage) เป็น Cloud Storage หรือ Object store ตัวนึง ที่ถูกสร้างขึ้นมาเพื่อ จัดเก็บวัตถุ หรือ ข้อมูลใด ๆ สามารถนำข้อมูลมาวิเคราะห์ และ สามารถเข้าถึงข้อมูลนี้ได้จากทุกที่ ไม่ว่าจะเก็บ website, mobile app หรือพวก data ต่าง ๆ ที่ต้องการ ทาง AWS กล่าวว่ามี durability ถึง 99.999999999% จะบอก 100% ก็คงกลัวว่าจะดูเยอะไป ซึ่งในปัจจุบัน ก็มีผู้เลือกใช้บริการจาก AWS S3 มากกว่า 1,000,000 applications ซึ่งอยู่ในทุก ๆ อุตสาหกรรม นอกจากเรื่อง storage แล้ว มันก็ยังสามารถ integrate กับบรรดา thirth-party ต่าง ๆ ได้ อีกทั้งยังสามารถ integrate กับ service อื่น ๆ ที่อยู่ใน AWS ได้ด้วย เข่น การทำชื่อ domain โดยการต่อกับ AWS route53 หรือ เอาไปต่อทั่วโลกกับ CloudFront ส่วนค่าใช้จ่าย จ่ายตามที่ใช้เท่านั้น
อย่างที่ได้เกริ่นไปข้างต้น ตัว AWS S3 มี Durability หรือ ความน่าเชื่อถือที่สูงมาก สามารถ scale ได้ และ มีความปลอดภัยสำหรับ เก็บข้อมูลสำคัญของคุณได้
การเก็บข้อมูลแบบ Data lakes เป็นการเก็บข้อมูลที่โครสร้าง หรือ ที่ไม่มีมีโครงสร้าง ก็ได้ ไม่ว่าคุณจะเก็บ data แบบไหน ไม่ว่าจะเกี่ยวกับการเงิน หรือ เกี่ยวกับจำพวกข้อมูลเภสัช หรือ รูปภาพ ภาพถ่ายวิดีโอ ทาง AWS S3 ก็สามารถใช้เป็นข้อมูล ในการทำ data analytics ได้
Use Cases ของผู้เขียนที่เคยใช้ ก็มีอยู่พอสมควร เช่น การเก็บ static webpage, js, css, image, config file โดยให้ access ผ่าน CloudFront มีการเก็บ data บางอย่าง มีทั้งที่สำคัญ และ ไม่สำคัญ ตรงไหนที่เป็น sensitive data หรือ ข้อมูลที่ไม่ต้องการให้คนภายนอกเห็น ก็จะตั้งเป็น private สามารถเข้าถึง bucket ได้เฉพาะที่ถูกกำหนดไว้เท่านั้น อันไหนที่ต้องการให้ลูกค้า access ได้ก็จะ set public (by default permission ที่ถูก set ไว้ตอนสร้าง Bucket จะเป็น Private)มีการใช้การเก็บ version ของ website รวมถึง service ที่เรา pack เอาไว้ deploy ทำให้สามารถเลือกได้เลยว่าต้องการจะ deploy version ไหน (เอาไว้กรณีมีการ rollback 555)
ยังมี use cases อีกหลายตัว ที่ไม่ได้เขียนในบทความนี้ ไม่ว่าจะเป็น Hybrid Cloud Storage, Cloud-Native application data หรือ Disaster recovery และ ยังมี Cases Studies ของ Netflix, AIR BNB, THOMSON REUTERS ซึ่งท่านสามารถ อ่านต่อได้จาก https://aws.amazon.com/s3
AWS S3 เป็น Cloud Storage ที่ได้รับความนิยมเป็นอย่างมาก เหมาะกับการใช้เก็บข้อมูลต่าง ๆ ที่เป็น static - สามารถทำเป็น static webpage ได้- สามารถจัดเก็บข้อมูลได้เยอะมากกกกก ก.ไก่ล้านตัว- สามารถกำหนด Permission ในการเข้าถึงได้- สามารถจัดการ  — Lifecycle ใช้ในการดู วงจรการทำงานของ Bucket นั้น — Replication ใช้ในการ backup data ของเรา,  — Analytics ใช้ในการวิเคราะห์ ข้อมูล สามารถกรองข้อมูลที่ต้องการวิเคราะห์ได้ — Metrics มีตาราง metrics ให้ดูจำพวก (Storage, Request, Data transfer) — Inventory สามารถจัดการข้อมูลต่าง หรือ วัตถุต่าง ๆ ได้- สามารถกำหนด Location หรือ Region ในการสร้าง Bucket ได้- มีความน่าเชื่อถือ ถึง 99.999999999% มั่นใจได้ว่า งานของคุณจะไม่หาย ไม่ล่ม ยกเว้นคุณไปกด delete มันคามือ (ส่วนตัวมันยังไม่เคยพัง หรือ หาย ถ้าไม่กดลบเอง)- AWS S3 pay per use จ่ายตามที่ใช้ ค่าใช้จ่าย ท่านดูต่อได้ที่ https://aws.amazon.com/s3/pricing
ยิ่งเขียน ยิ่งเหมือนอวย เอาเป็นว่า ของมันดีครับหวังว่าท่านผู้อ่าน จะรู้จักกับ AWS S3 กันมากขึ้นขอจบบทความไปแต่เพียงเท่านี้ ขอบคุณครับ
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
172 
172 
172 
Technology Advocacy Manager, a man who’s falling in love with the galaxy.
"
https://medium.com/@dennis.alund/yes-but-that-would-be-a-whole-different-story-that-would-require-some-more-configurations-to-be-d516c18d0a67?source=search_post---------302,"Sign in
There are currently no responses for this story.
Be the first to respond.
Dennis Alund
Dec 28, 2018·1 min read
Milind Mevada
Yes, but that would be a whole different story that would require some more configurations to be allowed to send email of your (or some other account’s) behalf.
You might consider though to do something simpler such as upload the binary to a cloud storage such as Google Drive, Dropbox or something. Because email protocol generally has a size limit of ~20MB for attachemnts, which might be a risk of being a problem for Android builds.
Google Developer Expert for Firebase, nerd and passionate problem solver | Founder of Kumpul coworking space in Bali
2 
2 
2 
Google Developer Expert for Firebase, nerd and passionate problem solver | Founder of Kumpul coworking space in Bali
"
https://medium.com/swlh/aws-simple-storage-service-s3-cd251fc661b6?source=search_post---------342,"There are currently no responses for this story.
Be the first to respond.
S3 is Amazon’s Object Storage Service. It is a highly durable, scalable, and fast, secure storage system that is highly available via a web interface for you and me to upload and download any amount of data from anywhere in the world.
That’s a lot of marketing speak. Let’s break this down.
What is Object Storage?Object storage is where every file is an object as opposed to a file…
"
https://medium.com/@tiwari_nitish/embracing-the-multi-cloud-era-74cc2580f75d?source=search_post---------396,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
Sep 21, 2017·5 min read
During the last few years, we have seen a cloud revolution of sorts. The term Cloud is now a genuine, production grade, application deployment strategy, from being a computing buzzword, few years back. Almost everyone is shifting to the Cloud, whether a private cloud, confined to single organization, or public cloud, provided on shared infrastructure.
An important result of this revolution is the number of cloud providers coming up —there are cloud providers of all sizes. Not only that, three of the biggest tech companies in the world, Google, Microsoft, and Amazon are now competing on the Cloud battlefront.
Competition of cloud providers is good for the consumers, it makes sure there is no monopoly, keeps prices reasonable, and providers on their toes. As this space grows, new use cases come up and new strategies are derived. But, there is a growing trend seen in the cloud ecosystem.
Instead of a depending on a single cloud — public or private, organizations are looking to diverge their cloud stack.
Recent study by Microsoft and 451 research says, more than one third of organizations want to work with 4 or more cloud vendors.
This is akin to not putting all your eggs in one basket, but it needs more reasoning than that to be considered the ideal way to deploy applications. So, let us analyze the multi-cloud trend and understand why it is the best way to leverage cloud technologies to achieve a truly reliable, cost-effective deployment.
Multi-cloud means just that, leveraging multiple cloud providers and services instead of just one. It means, deploying applications in a fashion that you don’t get locked into one vendor, rather, having the freedom to mix and match solutions best suited to solve specific problems.
Why would someone want to setup and manage multiple clouds? Isn’t managing one cloud difficult in itself? The answer lies in the benefits
Cost effective — Multi-cloud deployments turn out to be cost effective from the get-go.
Let us try to understand this with an example, say vendor A offers cloud storage at $0.023 per GB and requests at $0.005 per 1000 requests. Storage for 200 TB with 3 million requests will cost around $4500 for a month. Vendor B offers compute nodes with attached 24TB drives at $1095 per month.
If all of the 200 TB of your data is not accessed very frequently (hot data), there is a chance for cost optimization here. You can deploy an object storage like Minio on 2 compute nodes from vendor B, giving you 72 TB of hot storage, with almost no cap on number of requests, (cost ~$2000). While rest of the cold data can be in vendor A cold storage (cost $0.01/GB, approx $1500 per month for 150 TB). This means multi-cloud approach leads you to savings of around ~$1000 per month in this example.
Resilient applications — Irrespective of claims from vendors, things may and almost certainly go wrong at some point in time. A multi-cloud approach means having applications based on more than one vendor support. So, even if there is a downtime from one vendor, application is flexible enough to quickly move load to another vendor and nullify the effects of downtime.
Using the best solutions available for specific workloads — Certain vendors are good in certain areas. For example, certain vendors provide great bare metal options while others may be good at containerized work loads. Multi-cloud approach allows you to choose the best solution for your kind of workloads. For example, legacy apps can be easily deployed on bare metal, while modern cloud native applications can be deployed on the containerized platforms.
Flexibility — Flexibility is the cornerstone of multi-cloud strategy. Apart for reasons mentioned above, there may be several other scenarios that require a resource flexible methodology. Say, government restrictions on certain types of data, enterprise internal policies regarding data on public cloud, or something else.
In all such cases, the flexibility and agility of multi-cloud approach will actually be your best bet to build a scalable, reliable, cloud infrastructure that adheres to regulations and is cost effective.
While applications are generally stateless and can move around quite easily, data appears difficult to move around. This causes certain compromises while application deployment on a multi-cloud setup.
Lets see how to avoid data gravity in a multi-cloud architecture
REST APIs, based on S3 protocol — A standard protocol to access unstructured data from your application, makes sure there is a clearly defined way to access data instead of each application in your stack doing its own thing.
With wide adoption of the S3 protocol, you have many options to actually use S3 protocol without actually being tied to the AWS ecosystem. This gives you the best of both worlds, using the most popular API for data, without vendor lock in.
All your clouds talk S3 — Different clouds vendors or even your private cloud may have different interfaces for data access. Minio runs on all such platforms and makes them looks like S3 — whether you run it on JBODs, NAS, or even other providers like Azure blob storage, Minio unifies the API, allowing you to build a real world multi-cloud setup.
Tools to move data around easily — An agile data storage system needs a powerful command line utility that can move data around, easily and reliably. Minio provides minio client aka mc for this purpose.
Use mc to download, upload, or delete data from buckets. You can also mirror, watch buckets. mc can be easily used with shell scripts to create powerful scripts for data management.
In this post, we saw how multi-cloud is the new paradigm shift defining the way enterprises are levering cloud technologies. We also tried to understand the various reasons driving this trend and why it actually makes sense to get on the multi-cloud boat. Finally we saw how data gravity can hinder a proper multi-cloud setup and the possible ways to mitigate data storage and accessibility issues with Minio.
While you’re at it, help us understand your use case and how we can help you better! Fill out our best of Minio deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
28 
28 
28 
Minio.io
"
https://medium.com/@markpapadakis/the-case-against-online-backup-services-82e8e16d7c77?source=search_post---------304,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mark Papadakis
Sep 8, 2014·2 min read
I was thinking about this topic for some time now and it’s time to export those thoughts into a blog post.
Online consumer-level backup companies are going to have an increasingly harder time justifying their service offerings to their customers. A few reasons:
Why bother?
Seeking Knowledge 24x7. Head of R&D at Phaistos Networks | Simple is Beautiful — https://markpapadakis.com/
3 
3 
3 
Seeking Knowledge 24x7. Head of R&D at Phaistos Networks | Simple is Beautiful — https://markpapadakis.com/
"
https://medium.com/meet-lima/finding-the-right-hard-drive-for-lima-cb79da5bca5d?source=search_post---------364,"There are currently no responses for this story.
Be the first to respond.
Lima needs a hard drive to work. That’s where all your files will be stored, safely at home. A hard drive is a data storage device used for storing and retrieving your digital information.
If you’re getting a new one, you will have to consider its capacity, speed, format, and dimensions.
No ideas about that? Sounds like we are going to spend a few minutes together then! Here are a few tips to make your choice easier.
First and very important thing, your hard drive should be bigger than the storage spaces of all your devices combined. Lima will ultimately turn into your reference memory, so do not hesitate to think big! Once you have it, you have it forever.
For example, your computer’s drive is 512GB, your smartphone is 8GB, and your tablet stores 32GB; then you have to get at least a 552GB hard drive. Got it?
Note that your hard drive needs can be empty or filled with files, it will work the same way.
For a regular person who does a little bit of everything, like downloading some music and films, shooting a few videos of his kids, taking holiday pictures, we would recommend at least 500GB. You will save money in the long run and keep a quiet mind as far as your storage grows.
If you are a heavy media consumer, spending hours downloading digital music, and building your own high-def movie library, then you should directly go to the terabyte territory.
The bigger the files, the bigger the space you will need. Like you wouldn’t build the same garage for a Hummer truck or a mini cooper. Makes sense right?
Remember that physical storage is cheaper than cloud storage. You will spend 3 to 5 cents per GB and you will own your hard drive forever. For a cloud like Dropbox, you will spend 12 cents/GB per year and 24 cents/GB for 2 years, which will be way more expensive on the long run.
Renting VS owning, you know the deal.
And obviously with physical storage, you know where your files are: at your place. Nobody can mine them.
Same as far as using it with the Lima! Lima supports both. You can choose a self-powered USB hard drive with a 2.0 port. If you prefer a 3.0 port, it will work the same way. So, it’s up to you! Know also that you don’t need to go for a SDD, as Lima will not need that much of speed to work best.
Lima supports the following files systems : NTFS, HFS+, Ext4. If you buy a hard drive with a different file system, be aware that you can format it (do not worry, it is an easy procedure, only thing you need to do is to plug your hard drive to your computer and follow the instructions).
Up to you too! If you get one that is not powered, Lima will power it for you. Though, if you’re looking for a storage capacity bigger than to 2TB, we recommend to get a powered hard drive.
Did we help you make up your mind about hard drives?
Hope so!
Now go get it online on Amazon or Best Buy. Or old style shopping if you prefer, you will find your happiness in your favorite tech store ;)
The first Cloud that respects your privacy.
106 
13
106 claps
106 
13
Written by
Access your files from all your devices. Keep those files safe at home. Just the way it should be.
The first Cloud that respects your privacy.
Written by
Access your files from all your devices. Keep those files safe at home. Just the way it should be.
The first Cloud that respects your privacy.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebasethailand/cloud-storage-for-firebase-%E0%B9%80%E0%B8%88%E0%B9%89%E0%B8%B2%E0%B8%82%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%94%E0%B8%B5%E0%B8%A2%E0%B8%A7%E0%B8%81%E0%B8%B1%E0%B8%9A-firebase-storage-%E0%B9%84%E0%B8%87%E0%B8%88%E0%B8%B0%E0%B9%83%E0%B8%84%E0%B8%A3%E0%B8%AB%E0%B8%A5%E0%B8%B0-3d88171383bd?source=search_post---------45,"There are currently no responses for this story.
Be the first to respond.
จากงาน Google Cloud Next 2017 ที่ผ่านมา จะเห็นได้ว่าทีม Firebase และทีม Google Cloud Platform (GCP) ได้ทำงานใกล้ชิดกันมากขึ้น ดูจากชื่อบริการที่เปิดใหม่อย่าง Cloud Functions for Firebase ที่ดึงบริการ Cloud Functions ใน GCP มาใช้งานร่วมกับ Firebase หรือการเปลี่ยนชื่อ Firebase Storage เป็น Cloud Storage for Firebase นั่นทำให้ผมเชื่อว่าน่าจะมีบริการอื่นๆใน GCP มาร่วมกับ Firebase เพิ่มขึ้นอีกในอนาคตเป็นแน่ จับตาดูไว้เลย
ก่อนจะไปต่อใครยังไม่รู้จักบริการ Firebase Storage ก็แนะนำให้เข้าไปอ่านที่บทความนี้ก่อนนะครับ
developers.ascendcorp.com
ส่วนคนที่รู้จักกันแล้ว ก็มาดูกันว่า Cloud Storage for Firebase นั้นมันมีอะไร ที่ทำให้คุณนั้นต้องอยากใช้…เกาะสมุย(ไม่เกี่ยว)
ขอโฟกัสไปที่งาน Google Cloud Next ก่อน ที่ได้มีการเปิดตัว Storage Class ออกมา 4 แบบ ตามภาพด้านล่างนี้
โดยปกติเวลาเราจะเก็บไฟล์ไว้ที่ Firebase Storage ไฟล์จะถูกเก็บไว้ที่ US เท่านั้น แต่จากนี้เราจะสามารถเลือก Region และความถี่ในการเข้าถึงไฟล์ที่เราต้องการเก็บได้ แล้วมันดีอย่างไร มาดูรายละเอียดของแต่ละประเภทของ class กัน
เป็นการเก็บไฟล์ที่ region ต่างๆไว้ตั้งแต่ 2 region เป็นต้นไป แบบนี้เหมาะมากกับไฟล์ที่มีการเข้าถึงที่บ่อย เช่น ไฟล์วิดีโอ ภาพ เป็นต้น ซึ่งปัจจุบัน Multi-regional ก็มีอยู่ 3 region หลักตามภาพด้านล่าง
เป็นการเก็บไฟล์ไว้ภายใน region ที่เลือกเท่านั้น โดยไฟล์ที่เก็บก็มีการเข้าถึงบ่อยเช่นกัน เช่น ข้อมูลสำหรับทำ analytics หรือจะเป็นข้อมูลที่เราคิดว่าจะให้บริการในเฉพาะ region นั้น
เป็นการเก็บไฟล์ไว้ภายใน region ที่เราเลือกเท่านั้น โดยไฟล์ที่เก็บมีการเข้าถึงไม่บ่อยอย่างเดือนละครั้ง เช่น ข้อมูล backup
เป็นการเก็บไฟล์ไว้ภายใน region ที่เราเลือกเท่านั้น โดยไฟล์ที่เก็บมีการเข้าถึงน้อยมากอย่าง 3 เดือนครั้ง เช่นไฟล์ที่เป็น archive, ไฟล์ log ที่เกี่ยวข้องทางกฎหมาย หรือไฟล์พวกที่เป็น disaster recovery
คำถามคือ ประโยชน์ของการเลือกเก็บไฟล์ตาม class ต่างๆได้มันดีอย่างไร ลองดูภาพด้านล่างนี้
จากรูปให้เราโฟกัสไปที่ Price/GB-mo กับ Ops Fee จะเห็นว่าราคามันสวนทางกัน เช่น ถ้าเราเก็บไฟล์ที่ Multi-regional ค่าใช้จ่ายในเก็บจะสูงกว่าประเภทอื่น แต่ค่าใช้จ่ายในเรื่อง transaction การใช้งาน upload, download จะถูกกว่าแบบอื่น
ดังนั้นหากเราเลือก class เก็บที่เหมาะสมแล้ว เราก็จะบริหารค่าใช้จ่ายได้ดีมากขึ้นไปด้วยนั่นเอง ส่วนเรื่องของ Latency ทีม GCP เขาเคลมว่าแต่ละ class นั้นเท่ากันนะจ๊ะ
คราวนี้กลับมาโฟกัสที่ฝั่งของ Firebase บ้าง ณ วันนี้ที่ผมเขียนบทความอยู่นี้ หากใครใช้ Blaze Plan ของ Firebase อยู่ เมื่อเข้าไปเมนู Storage จะเห็นตุ่ม 3 ตุ่มด้านบนขวา กดตุ่มมาจะเจอเมนูที่ให้ เพิ่ม bucket ได้ ในส่วนนี้เราสามารถตั้งชื่อ bucket ได้ เลือกประเภทของ region, เลือกความถี่ในการเข้าถึงไฟล์ ซึ่งเมื่อเลือกเรียบร้อยเราก็จะได้ค่าของ storage class มาให้เห็นละ
เรื่องต่อมาก็คือการ import ไฟล์จาก Google Cloud Storage (GCS) มาใช้งานใน Firebase เรื่องนี้ผมว่าหลายคนคงเคยประสบปัญหานี้ เช่น เมื่อก่อนเราเก็บไฟล์ทั้งหมดไว้บน GCS วันดีคืนดี ทาง business ต้องการให้เอาไฟล์เหล่านี้มาแสดงบนมือถือ หรือทำงานร่วมกับ Firebase ถ้าเป็นคุณเจอโจทย์นี้เข้าไปจะทำไงดี ถ้าไฟล์มีน้อยก็คงไม่มีปัญหา อัพโหลดใหม่ก็ได้ แต่ถ้ามีเยอะ อันนี้ไม่ง่ายแน่เลย
แต่ไม่ต้องกังวลใจอีกต่อไป เพราะวันนี้ Cloud Storage for Firebase มีบริการดีๆมาช่วยเราเชื่อมต่อไฟล์จาก GCS มาใช้งานร่วมกับ Firebase ได้เพียงไม่กี่คลิกเท่านั้น
ตัวอย่างผมมี bucket ใน GCS ชื่อ jirawatee-gcs ซึ่งมีไฟล์ 1 ไฟล์ดังรูปด้านล่าง
จากนั้นเมื่อผมต้องการเชื่อม bucket ใน GCS เข้ากับ Firebase ก็แค่เข้าไปที่ Console จากนั้น เลือกเมนู Storage แล้วคลิกตุ่ม 3 ตุ่มขวามือ จากเห็นเมนู add bucket ให้กดตรงนั้นเบาๆ คราวนี้จะมีหน้าต่างมาให้ เลือก Import existing Google Cloud Storage buckets แล้วก็เลือก bucket ของเรา อย่างของผมก็คือ jirawatee-gcs
เมื่อ import มาแล้ว ทุกคนก็น่าจะเจอหน้าต่างแบบนี้ ให้ปฏิบัติตามขั้นตอนเพื่อที่จะ grant สิทธิ์ให้เราสามารถจัดการไฟล์จาก GCS ได้ ซึ่งทำครั้งเดียวในแต่ละ bucket นะครับ ขั้นตอนคือไปดาวน์โหลด Cloud Tools มา แล้วทำตาม step 1–6 จากลิงค์นี้ https://cloud.google.com/sdk/docs/ เมื่อเสร็จแล้วคุณจะอยู่ใน folder bin ให้ run คำสั่งในการ grant
ถ้าคุณทำสำเร็จ สิ่งแรกที่พิสูจน์ว่าสำเร็จได้คือ เมื่อคุณเข้าไปดูรูปใน bucket ท่ีมาจาก GCS จะต้องแสดงได้
และสำหรับโค้ดในการระบุ bucket นี่ก็ง่ายเลย แค่ระบุ bucket ที่ต้องการ ตอน getInstance แบบนี้
ยังไม่พอๆ ในเมื่อไฟล์ของคุณมาเชื่อมต่อกับ Firebase ได้แล้ว คุณสามารถที่จะปกป้องไฟล์ของคุณด้วย Firebase Security Rules ได้ด้วย เช่น
รายละเอียดเพิ่มเติมเรื่อง Firebase Secuiry Rules สำหรับ Storage ตามอ่านได้ที่นี่
developers.ascendcorp.com
เรื่องสุดท้ายก็คือการ integrate กับ Cloud Functions for Firebase นั่นเอง สำหรับเรื่องนี้เล่าให้ฟังเป็นน้ำจิ้มไปก่อนว่า เมื่อไฟล์ใน Storage มีการเปลี่ยนแปลง มันจะมี Listerner วิ่งไปบอก Cloud Functions for Firebase ให้ทำงานอะไรสักอย่างได้ เช่น resize ภาพ เป็นต้น ซึ่งจะขอเล่าในบทความ Cloud Functions for Firebase ต่อไป
สำหรับบทความนี้ ผู้เขียนได้บินข้ามน้ำข้ามทะเลมาเขียนถึงประเทศสหรัฐอเมริกา เลยทีเดียว เนื่องจากได้รับเชิญมางาน Android O-MG และด้วย time zone ที่ต่างกันมาก บอกตรงๆ นอนไม่หลับ เลยได้โอกาสเขียนพอดี ก็หวังว่าจะมีประโยชน์กับ Firebase Developers ที่ใช้ Firebase Storage อ๊ะ อ๊ะ ต้องเรียกว่า Cloud Storage for Firebase สิ ถึงจะถูก ตอนนี้ 06:40 ที่นี่ละ ขอตัวไปอาบน้ำ เตรียมตัวไปงานก่อน แล้วพบกันใหม่กับบทความถัดไปครับ
Let you learn and share your Firebase experiences with each…
38 
38 claps
38 
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/firebase-developers/how-to-upload-image-from-uiimagepickercontroller-to-cloud-storage-for-firebase-bad90f80d6a7?source=search_post---------93,"There are currently no responses for this story.
Be the first to respond.
Cloud Storage for Firebase is a great place for us to store users’ files. Uploading and downloading files to and from Cloud Storage is very simple and straightforward: Firebase provides a set of SDK for iOS, Android, Web, C++ and Unity developers to manage Cloud Storage for Firebase in an easy way. And it is easy for developers to…
"
https://medium.com/google-cloud/google-cloud-storage-exploder-2-9870d41fcee3?source=search_post---------98,"There are currently no responses for this story.
Be the first to respond.
Gophers and Explosions!?
Reference: Hallmark Caddyshack Gopher Ornament
Not quite but I’m still alright…
Almost one year ago, I wrote a proof-of-concept Node.JS Cloud Function (is that the correct singular form?) that unzipped files triggered by uploaded to GCS: exploder.
With the availability of Golang Cloud Functions Alpha (early access) and inspired by JBD post (link), herewith a simple (single-threaded) rewrite in Golang.
And:
Then, find yourself a zipped file, upload it to ${BUCKET_SRC}, slurp coffee (red wine, depending on the time of day), then check ${BUCKET_DST}:
Or using Cloud Console’s Storage Browser:
And:
And then, slightly more involved:
And:
Since JBD wrote about using OpenCensus, I thought I’d revisit this story, the code and add a stat for the count of unzipped files (link) and a trace for the GCS object creates.
And some results though I’m perplexed by the apparent low rate :-(
The trace was more problematic for me to get working but I think it was because I was not reliably vgo build && go mod vendor between deployments. So, please, bear that in mind as you develop code.
The coding changes are trivial.
The Golang runtime for Cloud Functions is a compelling (albeit overdue) addition to the set of runtimes. One noteworthy benefit with Golang over Node.JS and Python is that your Functions may be multi-threaded. This example is a mostly direct transliteration of the Node.JS example. A good evolution of this would be make this code multi-threaded (and async).
That’s all!
Google Cloud community articles and blogs
11 
No rights reserved
 by the author.
11 claps
11 
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/devplanet/part-3-introduction-to-docker-volumes-2ccec8effc98?source=search_post---------387,"There are currently no responses for this story.
Be the first to respond.
Docker and a lot of tools in the cloud native world are aimed primarily at highly distributed stateless applications. Stateless apps are easier to manage at scale and the general approach to building modern applications that needs to persist data is to have the data storage service such as database servers managed and hosted separately from the application. This decoupling allows us to manage deployments easily and the app is often connected to external storage services via service URIs.
However, for practical reasons, sometimes we need to store data permanently in docker containers. A simple example is a small scale webapp which stores its data in an embedded SQLite database.
When you run a container, every data it generates or stores only lives as long as the container. The moment the container is removed, the data is lost. To enable permanent data storage, docker has a feature called, “Volumes”.
Docker volumes act like an external hard disk you can attach to your container.
In this tutorial, we shall go through how to use docker volumes to persist data for a simple web app using SQLite as its database.
Yeah! You read that right, we are creating a review site for Avengers Endgame. This site would allow users submit reviews and view submitted reviews. All the data would be stored in an SQLite database and persisted using docker volumes.
You will find this example in the DevPlanet repo
To build this example, follow the steps below.
Step 1: Clone DevPlanet
Step 2: CD to the Endgamereview Directory
These app is made up of a number of files, all of which you will find in the folder stated above.
Below is the app.py
As seen from DATA_PATH = “/database”, we are storing the sqlite database file in a directory named /database within our container.
Dockerfile
The Dockerfile is same as in part 1 of this series.
Step 3: Build
Step 4: Run With A Volume Attached
In the above, -v datavolume:/database maps a docker volume named datavolume to the /database directory within the container. The name of the volume can be anything you choose, the first time you run this, the volume with the given name would be automatically created. All data you write to the /database directory would be stored in this volume, ensuring that anytime you run your image with the same volume name attached, all data previously written to the /database directory is permanently persisted.
Visit localhost:80 on your system
After submitting a review, you should see the reviews page appear like below
To demonstrate the persistence enabled by docker volumes, we shall remove the container named endgame above and start it again.
Now run it again,
Visit localhost/reviews and you would see the reviews you previously submitted.
You should try running these without the volume attached and you would see how the reviews clears out on every run.
It is generally a good idea to have a master directory within your container and map a single volume to these master directory. With these, you can create multiple sub directories for different purposes and have all of them in one volume. However, if you so desire, you can map multiple volumes to multiple directories within your container. Below is an example mapping a database and a log directory to two volumes.
While volumes are automatically created the first time you use them, you can also explicitly create new volumes using the command below.
You can view the list of all previously created docker volumes using
You can permanently delete a volume along with all of its stored data
Docker volumes offer a simple way to build stateful containerized applications. Volumes can be shared across multiple containers and even multiple hosts. In this tutorial, we applied docker volumes to persist data for a SQLite based application. In more practical scenarios, you will need docker volumes to store application logs, host containerized database servers etc. In a typical deployment scenario, you can have a set of containers running your main application and another set of containers running a database server. On docker hub, you will find ready to run docker images for almost any database server you dream of using, all of them use docker volumes to persist their data and often involve multiple volume mappings.
In the next tutorial, we shall focus on deploying docker containers in production using docker compose.
Tutorials on DevOps, Cloud Native Computing, GitOps and…
106 
Some rights reserved

106 claps
106 
Written by
Software Engineer at Microsoft | Creator of TorchFusion (https://github.com/johnolafenwa/TorchFusion)
Tutorials on DevOps, Cloud Native Computing, GitOps and Programming
Written by
Software Engineer at Microsoft | Creator of TorchFusion (https://github.com/johnolafenwa/TorchFusion)
Tutorials on DevOps, Cloud Native Computing, GitOps and Programming
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/linode-cube/ceph-block-storage-for-the-21st-century-b59f3b62d4ca?source=search_post---------349,"There are currently no responses for this story.
Be the first to respond.
Storage used to be so simple. You had a Single Large Expensive Drive (SLED) and you stored all your data on it.
Then, we moved on to redundant arrays of inexpensive disks (RAID), and things got more complex. But, it was still pretty easy. Unless you were using Small Computer System Interface (SCSI). I still get the heebie-jeebies thinking about chaining SCSI drives together.
But, even as hard drives were replaced by solid-state drives (SSD), physical drives couldn’t keep up with modern server data needs, never mind those of clouds and containers. This is where Ceph, and software-defined storage (SDS) have stepped in.
Ceph is an open-source SDS system. It’s designed to run on commercial off-the-shelf (COTS) hardware. From the user’s viewpoint, you’re not concerned about the hardware — whether it’s cache-enabled hard-drives or RAID SSDs.
There are two ways to create this kind of SDM. One way is with a Distributed File Systems (DFS), which is basically the files and directories you’ve been using for years. The only real difference between DFS and conventional storage is that instead of storing files on a single drive or array of drives, they’re stored across drives on multiple servers.
The other method, which Ceph uses, is in an Object Store, where each piece of data is stored in a flat, non-hierarchical namespace and identified by an arbitrary, unique identifier. File details, its metadata, are stored along with the data itself.
Ceph stores your data on a Ceph Block Device (CBD). This is a virtual drive, which can be attached to bare-metal or virtual machine (VM) Linux-based servers. To manage its storage, Ceph uses the Ceph Reliable Autonomic Distributed Object Store (RADOS), which facilitates block storage capabilities such as snapshots and replication.
Within RADOS, an object is THE unit of storage. In turn, objects are stored in object pools. Each pool has a name (e.g., “foo”) and forms a distinct object namespace. Each pool also defines how the object is stored, designates a replication level (2x, 3x, etc.), and delineates a mapping rule, describing how replicas should be distributed across the storage cluster. (For example, each replica should live in a separate rack.)
Finally, the Ceph storage cluster is comprised of object storage daemons/devices (OSDs). This cluster can store multiple pools and makes Ceph so scalable. You can start with little more storage than you have on your desktop and surge up to petabytes.
While these things are all neat, what makes most people — and anyone who cares about the bottom-line — excited about Ceph is that it makes storage much more affordable.
You can access Ceph storage multiple ways.
If you’re just using block storage, you can use the RADOS Gateway, which is an object storage interface built on top of Librados to provide applications with a RESTful gateway to Ceph Storage Clusters. (In general, Ceph Object Storage supports two interfaces. These are Amazon Simple Storage Service (S3) and OpenStack Swift Representational State Transfer (REST)-based application programming interfaces (APIs). Ceph also has its own native API.)
Ceph also includes two other access modes. The first of these is as file storage. This uses the Portable Operating System Interface (POSIX)-compliant Ceph file system (CephFS). To users this looks like a DFS. You can even use such old-fashioned storage access means as Network File System (NFS) with CephFS.
You can also mount Ceph as a block device. In this mode, Ceph automatically stripes and replicates the data across the cluster. Ceph’s RADOS Block Device (RBD) also integrates with Linux’s built-in Kernel Virtual Machines (KVMs). This enables you to deploy Ceph’s storage to KVMs running on your Ceph clients.
Setting up Ceph isn’t too difficult. You should keep in mind that to use Ceph efficiently you’ll need ample system resources. For example, Ceph OSD nodes defaults to a replica factor of three. This means for every 1 Terabyte (TB) of objects, you need 3 TBs of capacity. Ceph also recommends that your OSD data, OSD journal and OS reside on separate disks. In other words, adding in a swap drive, you’ll need four physical disks.
Why should you use Ceph? To quote Red Hat, Ceph’s owner, “Efficient, agile, and massively scalable, Ceph significantly lowers the cost of storing enterprise information in the cloud and helps you manage exponential data growth, so you can focus on making your data available.”
Ceph is also very flexible. Whether you want to access storage as blocks, files, or objects, its there for you. In today’s world where we need quick, reliable access to huge data stores, not to mention Big Data, SDS programs like Ceph are as necessary now as RAID was back in the day.
Please feel free to share below any comments or insights about your experience with or questions about using Ceph, block storage or Linode’s block storage beta. And if you found this blog useful, please consider sharing it through social media.
About the blogger: Steven J. Vaughan-Nichols is a veteran IT journalist whose estimable work can be found on a host of channels, including ZDNet.com, PC Magazine, InfoWorld, ComputerWorld, Linux Today and eWEEK. Steven’s IT expertise comes without parallel — he has even been a Jeopardy! clue. And while his views and cloud situations are solely his and don’t necessarily reflect those of Linode, we are grateful for his contributions. He can be followed on Twitter (@sjvn).
We’re covering everything from tech news and industry…
99 
4
99 claps
99 
4
Written by
Cloud Hosting for You. Sign up today and take control of your own server! Contact us via ticket or email for all support inquiries: https://www.linode.com/contact
We’re covering everything from tech news and industry happenings to event recaps and general tips.
Written by
Cloud Hosting for You. Sign up today and take control of your own server! Contact us via ticket or email for all support inquiries: https://www.linode.com/contact
We’re covering everything from tech news and industry happenings to event recaps and general tips.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/best-software/best-file-hosting-sites-d5d7e6beae6c?source=search_post---------332,"There are currently no responses for this story.
Be the first to respond.
File hosting services are used for storing or creating backups of your personal files. If you want to keep certain documents safe or lack storage space on your computer, a file hosting site is definitely your best bet.
As these services are dealing with private files, it’s crucial that you choose one that is reliable, safe, and secure. There’s a lot of different file hosting services available, all of which have their highlights and drawbacks. To help you pick the right file hosting site for you, I’ve looked extensively at a variety of different services.
Out of all the filehost services I’ve reviewed, this one is my overall recommendation.
MEGA ensures all of your files are protected due to its fantastic end-to-end encryption. It offers standard file storage, syncing, and sharing, but also has a few other great tools, like a chat function and Versioning. This lets you store multiple versions of the same file, which is then replaced with a previous version if it’s deleted.
This file hosting platform is ideal for users looking for an affordable option.
OneDrive is a good choice if you’re looking for an inexpensive file hosting platform. Like with the other services, you get the expected file sharing, storage, and syncing features. The best part about this hosting site is its great collaboration tools.
The file hosting platform in this platform is my personal favorite as it has a lot of impressive features.
Despite being targeted for businesses, Box is a good for storing your files and sharing your files. It has great integrations tools and excellent customer service. It also offers one of the best free trials out of all the platforms I’ve reviewed. With this free version, you get 10GB files storage and a maximum of 250MB of files size.
File hosting sites are pretty much how they sound. They’re an online hosting service that hosts user files. You upload files such as images, documents, and videos to the site after a username and password, or another type of authentication is provided.
File hosting sites are useful for personal storage or backup of files. You can share your files publicly or keep them password-protected so only you can access them.
Alternatively, file syncing/sharing services let you make folders on any one of your computers or mobile devices, which is then synchronized so it appears identical no matter which device is used to look at it. Any files put inside these folders can normally be accessed via a website or mobile app, which can then be shared with other users to look at.
For content providers who regularly run into bandwidth congestion, some file hosting services offer static or cached content. However, this isn’t normally necessary unless you have a large internet presence and require a higher bandwidth than usual.
Depending on what you want to get out of a file hosting platform, some hosting services might be more beneficial to your specific needs.
As a standard, you want the platform to be safe and reliable with good security features. A lot of hosting platforms have different price plans that offer extra file storage and file size, as well as priority support, quicker upload speeds, etc.
Certain platforms offer a trial period with free file hosting when you create an account, which is handy for testing out the service to check it’s a good fit for you. While free file hosting isn’t a necessity, it does allow you to try out the platform’s features and interface before you purchase it.
If you need to upload a lot of files or those with a large file size, then you need to make sure the hosting platform you use is able to accommodate your requirements. Additionally, if you want integrations with other platforms, then file hosting services that offer them are essential.
When choosing a file host platform, your budget obviously comes into play. The majority of cloud based services shouldn’t break the bank, but some more higher tiered plans that offer extra features can increase the overall cost of the platform.
These extra features might include unlimited file storage space, fast upload speeds, improved security, and the ability to have a bigger file size for each document. If you’re not too concerned about these aspects or don’t need to upload a lot of files, then a basic plan should be sufficient.
Some file hosting services let you create an account that’s completely free, which gives you the opportunity to test out the platform. However, a cloud storage free plan normally has limited features or very little file storage space.
The platforms available to access the file hosting service is another factor you need to keep in mind. While you can use cloud storage services via your web browser, some might also offer desktop apps and mobile apps for more convenience.
You should also check whether the available apps are compatible with your computer or mobile device to make sure you can actually use the service properly!
A key component of any online file hosting platform is great customer service. When you encounter an issue or have a question about a product, you want a response or solution as quickly as possible. When companies fail on customer support, it can understandably be frustrating.
To make sure any problems that you potentially run into are resolved swiftly, make sure you use file hosting site services with high-quality customer support and quick response times.
Additionally, it’s a good idea to check what types of customer support the file hosting site offers. The majority provide email support, but some might also offer live chat and phone services.
File hosting services with simple interfaces that are easy to use will save you time and frustration. It’s never fun when you’re faced with confusing and complicated menus, which can make file upload, file sharing, or accessing any other features difficult.
Additionally, you want the service to run quickly and smoothly so you don’t have to wait a long time to share and upload files. Any buggy or laggy file hosting services should be avoided, especially as the main role of the site is to make sure your files are saved and protected.
MEGA is an excellent end-to-end encrypted (private) file hosting platform that protects you and your personal files. This protection also extends to the people who are given access to your files, as the site makes sure only users you approve can view your documents.
In addition to file sharing features and cloud storage, MEGA allows you to chat with users you’ve added on the site. This chat feature uses encryption to make sure your conversations are protected. While not a make-or-break factor when it comes to choosing a hosting site, it’s a nice touch.
A feature called Versioning allows you to store multiple versions of one file which is replaced with a previous one if you or someone else makes an error or accidentally deletes it, Additionally, if you sync or upload a file that has an identical name to a existing file in the same location, then a new version of it is made. These files can be identified by a clock symbol.
A nice feature about MEGA is that it lets you preview your images or videos. However, you are unable to do this with documents. Unfortunately, MEGA currently doesn’t provide integrations with G Suite and Microsoft Office.
With this in mind, MEGA isn’t the best when it comes to collaboration as it doesn’t have editing features or integrations. You can only access your documents through a web browser.
MEGA offers a few different pricing plans, including an unlimited free trial version for those looking for free cloud storage. However, with the free file hosting plan, you’re limited to just 15GB of storage.
Excluding the free cloud storage option, the cheapest MEGA plan is Pro Lite. This version costs $5.69 per month and offers 200GB storage and 1TB file transfer.
MEGA states that their most popular plan is Pro I, which is priced at $11.38 each month. It offers 2TB file transfer and 1000GB storage.
The two most expensive plans are Pro II and Pro III The former costs $22.78 per month, while the latter costs $29.78 a month. Pro II offers 3TB file transfers and 4000GB storage, and Pro III provides 16TB file transfer and 8000GB storage.
MEGA’s desktop client is fairly easy to navigate and looks appealing. It has a sync folder and a system tray symbol that allows you to manage your sync and get into your settings. You can download it on Windows, Linux, and macOS.
The web interface for this file hosting platform is quick and simple to use. The cloud drive menu lets you search through files and folders either on the left hand side or through the ‘recents’ page. This menu also allows you to look at your contacts, open the chat feature, go back to your dashboard, and any uploaded files that have been shared with you.
MEGA has a strong customer support. It offers an FAQ and a message form which allows you contact an advisor. You can expect to receive a response very quickly, but Pro users are given priority support.
Pros
Cons
Microsoft’s OneDrive offers cloud based storage and file syncing for PCs, Macs, Androids, iOS, and even Xbox consoles. It isn’t too expensive and offers a lot of app integrations, which includes Microsoft Office.
However, OneDrive has run into a bit of trouble regarding privacy in the past (they were once connected to the PRISM project!). Since then, they have improved their security, but it’s still not on par with some other file hosting sites.
The highlight of OneDrive is its productivity and collaboration tools. Being a Microsoft product, it can connect to other Microsoft services like Outlook, Skype, and Office Online.
The web app for OneDrive has Skype embedded, which allows you to use it to chat and make calls.
You are able to view, edit, and share files from Office when using OneDrive, which is handy. The cloud storage service automatically stores images and videos on devices you upload to a computer that has OneDrive installed.
The photo preview options are quite extensive. You can add images to albums, put them in slideshow and play it, add effects, rotate them, view them in their original size, and more.
OneDrive offers a few different price plans for home use, which includes a free storage option. With this version, you can create a free account and access 5GB of free storage. You don’t get any other features with the free plan, unfortunately.
Aside from the free account plan, the least expensive choice is the OneDrive 100GB. It only costs $1.99 per month but you’re limited to just 100GB of file storage, with no extra features.
At $6.99 per month or $69.99 per year, the Office 365 Personal plan gives you 1TB of file storage, as well as Office apps, productivity tools, advanced security options, and more.
Finally, the Office 365 Home plan is priced at $9.99 each month or $99.99 every year With this cloud storage plan, you get 6TB file storage online for up to 6 users. You also get all the features available on the Office 365 Personal plan.
Currently, OneDrive offers a free account trial for Office 365 Home, which is ideal if you want to test out the service. The free trial lasts 30 days, so you can upload documents, download files, share download links and use the service for a fairly long period of time to see if it’s the right cloud storage site for you.
The desktop client for this file host service is compatible on Windows and MacOS, but not Linux. It works fairly well, but looks a little outdated as the design is very basic and bland.
The web browser for this file hosting site, on the other hand, looks great. It’s a breeze to navigate through and doesn’t lag. You won’t lose countless hours figuring out how to use it due to its simple but effective design.
The file hosting service also offers a mobile app that you can download on iOS and Android for when you need to access your files on the go.
Customer service for OneDrive is a strong point. There’s a bunch of help topics that you can look through to learn more about the hosting service’s different features, as well as training pages and videos.
If you still need help, you can use the community forum to get assistance or contact a Microsoft advisor via email. On average, the response time for someone to get back to you shouldn’t take long too long.
Pros
Cons
Box is more suited to businesses and companies, but it works pretty well for home use. It offers a lot of features, including the standard file storage, file sharing and syncing, etc.
File versioning is available in Box, but it’s limited to specific plans. In the free file version, you only get one version. The Pro plan, however, saves up to ten versions of the file you’re working on.
If you accidentally delete any of your files, you can recover them within 30 days. Although, if you wait longer than this, then they are deleted permanently. In the Business and Enterprise plans, moderators are able to change this.
One of the best features of Box is that it has a lot of tool integrations, so you can use third-party apps with the service.
Box Sync is the desktop version of the platform, which you can download on Windows and macOs. It’s easy to use and essentially works as a virtual drive. Alternatively, you can download Box Drive which is a newer version of Box Sync.
In Box Drive, you don’t need to store files on your hard drive, which gives you more space. However, the program caches your files before it deletes them.
If you don’t want to download Box Sync or Box Drive, then you can also access the file hosting service via your web browser. There’s also a mobile app for iOS and Android users that you can download so you can access your uploaded files and share them via download links on the go.
Box doesn’t currently offer end-to-end encryption or client-side encryption, so it’s not as secure as some other file sharing services. However, it does provide two-factor verification.
Box offers a few different price plans, including a starter edition that’s free. If you’re looking for a free file hosting platform, then Box might be worth looking at.
The Starter free plan gives your 10GB of file upload storage and caps your maximum file size to 250MB. Compared to some other platforms, this is a fair amount of features for a free version.
If you don’t need a lot of storage or don’t need a bigger file size, Box’s free storage option is great.
Box‘s Business Starter plan only costs $5 per month and is suitable for 3 to 10 users. It gives you 100GB storage, 2GB file upload size, and Microsoft Office 365 integrations. You also get mobile, sync, and share capabilities.
The Business plan is priced at $15 per month and is suitable for a minimum of 3 users. It comes with all the features of previous tiers, as well as unlimited storage, advanced security, more customization, 5GB file size, and much more.
Box Enterprise’s price is not currently specified and requires you to contact the company to find out more information. It is targeted for large businesses that require premium content management.
Another highlight of Box is their fantastic customer service and community. The community forum is very active and helpful, which is great for finding out information about the program.
If you require the assistance of an advisor, Box also offers live chat support. As long as you message someone during working hours, you can expect to receive a response to your query within minutes.
Pros
Cons
After looking at a few different platforms for uploading and storing files, I think the clear winner is MEGA. It has fantastic end-to-end encryption that makes sure all of your files are protected.
It offers a good range of features, including a chat function and Versioning, which adds an extra layer of security to your files. If you accidentally make a mistake or delete a file, it’s replaced by a previous version.
However, MEGA doesn’t have the best editing features or integrations.
If MEGA doesn’t seem right for you, then either OneDrive or Box are two other good alternatives. OneDrive and Box both aren’t as secure as MEGA, but they do have integration tools.
OneDrive is very budget-friendly as it’s cheapest price plan is only $1.99 per month. It also has a free version you can download, but you’re limited to 5GB of storage for your files.
OneDrive is available to download as a mobile app and desktop client, but the latter has a plain and outdated design.
Box is more suited to businesses, but it has a good range of features for home use. You can download a few different apps of Box, including a mobile app. It has a lot of integration tools and has great customer support. Aside from a free version, the cheapest price plan for Box is $5 per month.
This post first appeared on medium.com/best-software
Educated reviews and oppinions on the best software and…
96 
1
96 claps
96 
1
Educated reviews and oppinions on the best software and online tools available right now!
Written by
Software developer, hacker, creator and lover of technology. Author of medium.com/best-software
Educated reviews and oppinions on the best software and online tools available right now!
"
https://medium.com/foundations/cloud-storage-costing-and-pricing-93e95e231d60?source=search_post---------182,"There are currently no responses for this story.
Be the first to respond.
I’ve been doing some cloud-related (cloudy?) thinking as part of my work on the FleSSR project over the last couple of days, ultimately with the aim of delivering a piece on business models for cloud services (one of the project deliverables) but initially just looking at the costs of storage in the cloud (Amazon, Dropbox and Rackspace) and the costs of building cloud storage in-house.
The result is a couple of posts on the FleSSR project blog and a Google spreadsheet. Please have a read. I’m keen to get feedback!
So, what can we conclude? Looking at the cost per TB per year, the Dropbox and Rackspace prices are pretty much flat (i.e. the same irrespective of how much data is being stored) at around £1530/TB/year and £1220/TB/year respectively (though, as noted above, the Dropbox prices are only applicable for 50GB and 100GB). Amazon’s pricing is cheaper, particularly so for large amounts of data (anything over 100TB data where the price starts dipping below £1000/TB/year) but never reaches the kind of baseline figures I’ve seen others quote for Amazon storage alone (i.e. without network costs) of around £450/TB/year. (My lowest estimate is around £510/TB/year for 500PB data but, as mentioned above, this estimate is probably unrealistic for other reasons.)
Superficially, these prices seem quite high — they are certainly higher than I was expecting. What is interesting is whether they can be matched or beaten by academic providers (such as Eduserv) and/or in-house institutional provision, and if so by how much?
In the second post I try to identify a ‘shopping list’ of things that would need to be paid for if one were to build a cloud storage infrastructure oneself, partly as a simple reminder that setting up this kind of service isn’t just about buying some kit — there are all sort of costs that need to be met (some up-front and some on an ongoing basis):
I don’t go as far as identifying specific costs (in terms of amounts of money) because doing so is subject to all kinds of variables. However, the list itself is intended to help think about costs when considering things like whether to outsource to the cloud or not. I’m hoping that this will prove useful to people but if you think I’ve got things majorly (or even a little bit) wrong, please shout.
Originally published at efoundations.typepad.com on December 10, 2010.
A blog by andypowe11
A blog by andypowe11
Written by
Cloud CTO, Jisc
A blog by andypowe11
"
https://medium.com/@jgmac1106/this-assumes-that-memories-main-hard-drives-resides-in-the-brain-42811ad3ea4c?source=search_post---------322,"Sign in
There are currently no responses for this story.
Be the first to respond.
Greg McVerry
Jan 6, 2016·1 min read
This assumes that memories main hard drive resides in the brain.
I do not hold this true. We have always had cloud storage.
Memory is something to be remembered but this data is stored in culture, in tools, and surroundings. Its the OG Cloud.
Does the mind play some part. No doubt. I spend way too much time inside my own to not hold this true.
I think basically we are agreeing. The digital world changes brain behavior in the chemical sense. I do not think we have evolved entirely new neural networks or patterns of neural activity. Maybe I am wrong.
I also believe that those who are in charge of delivering notifications actively research how the brain or, as you state more importantly the behavior, reacts to each ping and vibration. They probably do both,
It is not so much a matter of changing the brain but rather taking advantage of.
I am a researcher and teacher educator at Southern Connecticut State University. Focus on literacy and technology.
I am a researcher and teacher educator at Southern Connecticut State University. Focus on literacy and technology.
"
https://architecht.io/backblaze-ceo-on-the-economics-of-cloud-storage-and-the-business-value-of-blogging-3bf081aa286b?source=search_post---------92,"Cloud storage provider Backblaze turned 10 years old on April 20, and just days after it cut download prices on its B2 service by 60 percent. During the past decade, Backblaze has made a name for itself by building and open sourcing high-density, low-cost storage “pods” and, more recently, releasing detailed analyses on reliability of the consumer hard drives that fill those pods. Today, it’s running more than 70,000 drives and storing more than 300 petabytes of customer data.
In this episode of the ARCHITECHT Show, Backblaze co-founder and CEO Gleb Budman tackles a wide range of topics, ranging from the economics of cloud storage to the benefits that come from blogging about what you’re building. He also shares his opinion on the pros and cons of raising lots of capital versus bootstrapping it. Backblaze raised its only round of venture capital, $5.3 million, in 2012. (For more on that angle, read this recent BusinessInsider interview with Budman.)
Keep reading for highlights from the interview with Budman, and scroll to the bottom (or click here) for links to listen to the podcast pretty much everywhere else you might want to.
This week’s podcast sponsored by:
In the news segment, co-host Barb Darrow (Fortune) and I discuss Oracle-vs.-AWS on data center scale, the past week’s container acquisitions and DockerCon, and the rush for cloud providers to tackle offensive content with AI APIs.
Here are the highlights from the interview with, but it’s well worth listening to the whole thing if you’re at all interested in the costs of storing data, the process of open sourcing hardware, and the cost-benefit analysis of when to make certain engineering investments or roll out new services.
“The ‘a-ha’ moment didn’t require technology, in this case. Dropping the cost of storage required a tremendous amount of tech. It required a decade of all of us focusing on squeezing a penny here and a penny there out of the entire cloud storage stack. That was hard.
“… The cost of bandwidth isn’t that expensive, it’s just that you buy it in a way that’s flat. It’s a question of usage. What we didn’t know was how spiky the usage would be. What we’ve learned is that, across a reasonably sized base, the bandwidth usage isn’t that crazy spiky.”
architecht.io
“It’s had tremendous value for us and it’s something [on which], as you can guess, we spend a tremendous amount of time, resources [and] energy on putting together and putting out there. And in addition to it being something which just feels good for us — which is, we want to to contribute back — it is also something that provides us business value.
“We have hired various people because they were readers of our blog, and then contacted us and said, ‘Hey, I love your openness. I’d like to work for a company that’s open like this. Do you guys have a job?’ … We absolutely have customers that have come to us and said, ‘I’ve been reading your blog for a while, now I have a need for either backup or storage … so that’s why I chose you.’”
“On the flash and SSD side of things, I would love to use it. When we started this company and started building our pods, we said, ‘OK, clearly SSD is too expensive today. But lithography should advance faster than spinning drives, and within maybe 2 years they will cross over and it will make sense financially to use SSD.’
“And we are sitting here 10 years later, and I’m currently saying, ‘Maybe in 2 years it will make sense for us to use them.’”
architecht.io
“There are constantly these articles that we all love internally, but are obviously years and decades away — you know, storing data in salt, storing data in DNA. … But at this point, spinning hard drives — if what you’re looking for is dense, real-time storage — they’re just hard to beat.”
“We do buy in bulk and we do try to drive a reasonably hard bargain on the drives that we get. The interesting thing is that the drive companies aren’t selling software. They don’t have 90 percent margin that you can take out of what they offer. There’s some margin for them, they make a profit, but whether you buy 1 drive or 1 million drives, you’re not going to get them at half off.”
architecht.io
“I can’t say that I never think about the what-ifs and sometimes fantasize about, ‘OK, if we had a hundred million dollars in the bank, think of all the things we would do and all the people we would hire.’ …
“At the same time, I love the fact that we can set our own way. When we chose to raise funding, it was a ‘We chose to raise funding’; it wasn’t a ‘If we don’t raise funding, we will die.’ That is an incredibly comforting and luxurious position.
…
“I love that we have built a company where the people here are excited and happy to be working here, that they’re not only chasing the valuation and only chasing the unicorn status. That they’re focused on, ‘Am I building a product and a service that I think is good?’ and ‘Do I enjoy showing up to the office every day?’ Those two things make me just gloriously happy.”
architecht.io
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
4 
4 claps
4 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@macsources/lima-personal-cloud-storage-review-2737925d9923?source=search_post---------179,"Sign in
There are currently no responses for this story.
Be the first to respond.
MacSources
Apr 5, 2016·8 min read
It never fails, I am constantly juggling space on my iPhone. I moved from the 32 Gb iPhone 5 to the 64 Gb iPhone 6S Plus, thinking that this would be adequate storage space. I bet you saw that coming, 64Gb is still not enough storage space. I believe when the iPhone 7 releases, I will purchase the 128 Gb model. Walking around CES 2016 in Las Vegas, Nevada, I had the pleasure to meet the team from Lima Technology Inc. I had no idea what this company was about and I had never heard of this technology. Lima is the first technology to unify all your content across devices. I had been using Dropbox to dump data to, and then offloading the data to a personal 1Tb drive when Dropbox was full. This was time consuming, it was “me time” intensive and I forgot to do this regularly. The end result was a full Dropbox and a full phone and the dread of needing to find something that I could delete.
The team at Lima has developed an amazing technology. They had the device to display at their booth at Tech West in the Sands Expo, Level 2 of the Venetian Hotel/Casino. CES was amazing and I truly am looking forward to 2017. This device is a hardware adaptor, a very tiny device by the way, that connects the Internet (through your router) to an external hard drive. The beauty of the device is the link to the Internet and their app, which allows you to have exactly the same files on your smartphone and your computer or tablet. This works cross platform, regardless of the OS/capacity.
Packaging: The Lima arrives in a 5.5″ X 5.5″ X 2 1/8″ box. The outer box is a slip cover white with lime green ends. The cover is sleek, clean and displays the Lima Unit (not in actual size). I am a big fan of the knot/infinity symbol that they have chosen as it is very eye catching. Examining the back of the box, the product provides a thought provoking question: “What if all your devices had the same files?” That question really summarizes this device very well. It shows a single picture with a computer, phone, tablet on that picture. “Each of your devices has different content. Install Lima at home, and they will all contain exactly the same thing.” The packaging clearly tells you what to expect from the device, which is appreciated. You now never have to move files between devices again. It is important to note here that the Lima has no internal storage capabilities. This is not a hard drive. It is a hardware adaptor that turns your hard drive into a personal cloud storage.
Slide the slip cover off of the internal box and open the front cover like a book. The internal box does not have a picture of the Lima but does have the same infinity sign with the logo on a white background. All sides of this box are lime green except for the white top. Opening the lid of the box your eyes are immediately drawn to the included earphones. These are included by the company as a visual analogy, a gift to show the strength/benefit of the technology. The idea, no matter where you are, no matter what device you are using, you can enjoy your entire music library with the included headphones. There is a pull strap that will lift the headphone flap away and you are now introduced to the Lima. The device is small 2 13/16″ long 1 1/2 inch wide and 1 1/16 inch thick. It is safely nestled in formed plastic. Lifting this away, there is another compartment below that houses the power brick for the LIMa and an Ethernet cable.
Installation: This may be one of the easiest installs that I have ever been involved with. There is no included instruction manual. You do not need one. If you look on the backside of the headphone flap, you have a 3 step process. Step 1, plug the Ethernet cable into your router and plug the other end of the Ethernet cable into the Lima unit. Step 2, plug the power from the Lima into the wall outlet. Step 3 Go to http://install.meetlima.com on your computer and follow the online steps. Step 4 plug the Lima into an external hard drive via USB to USB cable. It is seriously that simple and it is ready to go. They use some fun colorful graphics during the waiting periods.
The computer setup is just as easy as the hardware installation. You can choose your language, choose where you want Lima to store the database. Review and accept the terms and then create the account. You can now move files from your computer to the hard-drive (iTunes library) and videos/movies etc. You are now ready to download the app on your iPhone, iPad, Windows phone, Android phone and share all of the files on the hard drive across all platforms.
I paired my Lima with a Toshiba 1 TB external hard drive. This was a pairing that worked flawlessly. It is important to note that there are a few requirements for the attached hard drive. First, it must be a USB 2.0 or 3.0 connection. You need to have at least as much room as you have on your computer with a maximum size capability of 7 Tb. The hard drive must be “EMPTY” when installing LIma. It does not work on already full drives, at least not yet according to their site. Eventually, you may be able to add multiple hard drives to the Lima through a powered HUB. This feature is currently not available per the website. I would love this feature.
LIma is designed to be a personal cloud and really is not meant for access by multiple people. Having said that, my wife has installed the app on her Galaxy S5 and I have it on my iPhone. This way, as we take photos and videos, they are stored in an additional place. You can choose to have this occur over cellular data or not. For data purposes, we only have the app share the photos while on WiFi. I have recently installed a Synology Diskstation DS416j to provide some RAID protection of my data. The Lima installation was immensely easier than installation of the DS416j. Both systems back-up my photos/files etc. The cost is significantly less for the Lima, the ease of install was significantly less for the Lima and the needed tech knowledge is significantly less for the Lima.
I personally feel that the reach of the Lima is going to be huge. This device can be used by everyone, from every ability level to serve as a personal cloud storage. My wife is more likely to lose her phone than the hard drive in my home is to fail. Here is where Dropbox or online storage could potentially be more safe for storage. If there is a fire in my home, I may lose the phone, the DS416J and the Lima in one fell swoop. Unfortunately, there is no complete safety. What I really like about the Lima is the installation and the ease of use. It really was the easiest tech install that I have been involved with so far. The storage machine above was very complicated and daunting for someone new to the home server/storage world.
You can learn more about the device from meetlima.com. Here you can buy the device, read reviews/blogs about the device and you can review How-To’s, FAQ, Troubleshooting. You can choose from multiple colors to include the green (my favorite), blue, yellow, purple, pink. At a sub $100 dollar price point and a $20 reduction for additional devices, everyone should pick one of these as a means of data access/storage. The only real issue/complaint that I have is regarding the “Movies” aspect of the device. I moved one of my iTunes videos from iTunes onto the Toshiba hard drive. I have been unable to get this to play on my iPhone or iPad Air 2 (M4V file type). I suspect that this is likely due to the Apple DRM of the file and not due to any limitation of Lima. The unfortunate part is, I have bought these movies in Blu-Ray, DVD and digital format and now I don’t have the access to the digital wherever I want it. I have searched the website and I have seen that you can put movies onto the hard drive and access them through the Lima software/app. So, I may look into trying Handbrake or trying PavTube M4V to remove the DRM. I have a DRM free copy of Guardians of the Galaxy and guess what, this worked amazingly well. There was some mild lag initially but then it played without any skips. I was able to stream the movie from both my iPad and iPhone. With a little work you are able to use this device as a storage point for your movies. This does work well, but they cannot be from iTunes. I also added a few songs from my iTunes library to the LIma. These worked just fine, and played well without any issues. All you have to do is copy the folder in your iTunes library (under music) and paste it or drag it into the music folder of Lima.
Videos recorded with on my phone are safely stored and easily playable on my phone and my wife’s phone. This is quite amazing. I wish that there was a larger capacity than 7 TB and I wish it would work with RAID/home NAS storage systems. This would be a great add on to a multiple bay hard drive system, or as stated above a multiple USB powered hub.
This device is rated at 5/5 stars. I would rate it higher if I could. Just so you are aware, the more files you have on your smart device, the longer this transfer process will take. I had a substantial amount of photos/videos on my phone and it took about 45 minutes to completely transfer.
Bonus: Included Ear bud Headphones. These are a set of single flange ear buds. There is no listed left and right. This does not matter much but is uncommon to not have an L/R listed. These are stereo style earbuds, meaning there is the possibility of different sound output to each earbud. It is appreciated that they included the set of single flange ear buds, however I never can get them to fit my ears. I prefer tree style or triple flange as they tend to fit my ear canal much better. I like the lime color of the ear buds and the flat cable is unique as well. I was actually impressed with the sound output of these earbuds. Incredibly surprised actually. For a set of free/included earbuds they are far superior to the sound I expected to get from them. I listed to my usual repertoire for testing headphones “Bohemian Rhapsody,” Queen, Eagles “Hotel Calfornia,” Far and Away soundtrack and to “Why so Serious” Dark Knight Rises Joker Theme.” These speakers alone seem to be on par with the $20 dollar headsets that you can get at local retailers. The base was immensely better than I expected. I had to turn down the sound on my iPad as the sound was too loud. I have not had to do that in a while. I am not certain how they will hold up with time. However for a set of included free headsets the company has provided a quality product. I really am impressed with Lima and with their product. This could have detracted from their product overall. Yet they included a rather good set of earbuds. BUY FROM AMAZON
To learn more, visit meetlima.com. Find Lima on Facebook and Twitter.
Originally published at macsources.com on April 5, 2016.
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
"
https://saasholic.com/cloud-storage-security-c2ac1e1e9c3b?source=search_post---------101,"Most entrepreneurs around the world are adopting cloud storage due to its scalability, accessibility, and less IT overhead, making it cheaper. Despite its convenience, the security of cloud storage is now a concern in many establishments as employees have access to organizations data from anywhere and anytime. Staff can also use any device to access the data.
In comparison to locally implemented hardware, cloud storage is way cheaper hence cost-effective. However, storing confidential and sensitive files their exposes your business to new risks. This is because the cloud is out of the safeguard limits you would implement within your premises for data protection. In other words, it is out of your control.
The inception of the IoT (Internet of Thing) and connected office technology has increased the dependency of industries reliance on cloud technology regardless of the security risks. There is a higher risk of unintended leakage or compromise with more devices connecting to the internet.
The biggest challenge threatening cloud security is the use of cloud storage and sharing of files that do not meet security standards or are not approved by the IT department. Your employees can compromise the security of sensitive company data intentionally or out of ignorance.
Fortunately, there are ways to improve your data security even when using the cloud. Here are some ten solutions.
Keep off sensitive data from the cloud or virtual space as no cloud storage guarantees 100 percent security. Organizations that opt for cloud storage services risk more threats than those that store their files locally. Remove all sensitive data that you intend to store in the cloud. Restrict sensitive data to storage within your controls.
Among your IT department roles is managing multiple accounts of the employees, which makes it hard to develop a foolproof security framework for your enterprise. However, strong passwords developed and managed through the password management tools will help. Strong passwords entail a mixture of letters, numbers, and symbols in no particular order. Change your passwords regularly and whenever an employee departs as your code of ethics no longer binds them.
End-users should create passwords that are hard to guess but easy for them to remember. Consider software services that create and store passwords if you wish to keep track of many passwords. Do not store them in a computer, or open places and ensure that you remember the master passwords you create.
Using multi-factor authentication is safer to prevent the hacking, misplacement, and compromising of passwords. Multi-factor authentication requires another factor to verify identification besides your username and passcode. The third factor may be a voice analysis, unique code, or fingerprint, that only the user has access to, separately generated. All these are effective in the reinforcement of data security as they keep off intruders.
Consider encrypting data at the source, in transit and all the way. Cybersecurity analysts confirm that encrypting data at the source is safest. Ensure to manage the key yourself. Use end to end encryption when transporting data to reinforce security, though data on transit is secured by the advent of SDN with virtualization of the network. Keep all your interaction with your CSP’s server over SSL transmission for security.
Encryption ensures that you comply with contractual obligations, regulatory requirements to handle sensitive data, and privacy policies for data at rest. When you store data in cloud storage disks, encrypt it. Also, ensure that you encrypt the encryption keys with regular rotated master keys. The CSP should provide level field encryption and specify the fields you wish to encrypt (like CFP, SSN, credit number among others).
The CSP you adopt should use outstanding incident and vulnerability response tools. Solutions from the response tool need to support automated security assessments fully to test for the weakness of the system. It also shortens the time between crucial security audits. Scans are performed on schedule or demand.
You should enjoy accurate role-based access control (RBAC) features which allow set up for user-specific data editing permissions and access. Ideally, the system should permit access to exceptional grained, control-based, and enforced duty segregation within an establishment. This helps maintain compliance with internal and external data security standards like COBIT and HITRUST frameworks.
3 Type II or SOC 2: Helps in the regulation in regulatory compliance oversight, internal risk management processes, and vendor management programs. SOC 3 or SOC 2 confirms that a software service like CSP is specifically designed and rigorously managed to give the highest security level.
PCI DSS: A SaaS provider undergoes a detailed audit to ensure that sensitive data is processed, transmitted, and stored in a fully secure and protected manner to get this certification. The all-around security standards include software design, policies, network architecture, procedures, security management, among other critical protective measures.
Define and enforce a clear data deletion policy with your clients. At the end of the clients’ data retention period, the data is programmatically deleted as the contract defines, which results in more storage space. It also prevents unauthorized access of data.
Always remember that cloud storage (or sync) does not substitute back up. Whenever data is deleted from the cloud end, it is removed from the local machine as well. Most cloud services do not give excellent revision histories for files that are synchronized when booting. Use online back up to protect against data loss. Here, multiple data backups, including those offsite, are crucial. Online backup services often update your data, complete with granular revisions. Store your data and make sure it is encrypted in a third-party data center.
Educating employees on the risks associated with cloud adoption is crucial. The awareness should be in addition to the implementation of stringent security solutions that protect your data from unauthorized access and enforcement of cloud security policies. It is also crucial to teach them the need to protect their passwords, by secure storage and endpoint services. Employees should void sharing passwords or writing it carelessly.
As more companies undertake cloud adoption, cloud storage security is becoming a priority in IT architecture and information security strategies. Companies are now more aware of the need to protect their data as they enable their staff to enjoy the performance and flexibility of the cloud. Just as new threats on data security emerge, establishments must be vigilant to keep their files secure.
You will share cloud storage responsibility with your CSP, Ergo. The CSP is responsible for the implementation of baseline protections like authentication, encryption, and data access controls processed on the platforms. You will eventually supplement the encryption on your end with reinforced security to tighten access to sensitive information and bolster cloud data protection.
© 2022 SaaSholic. All rights reserved.
"
https://medium.com/@saascribe/storage-wars-microsoft-strikes-back-4b9905d3299b?source=search_post---------294,"Sign in
There are currently no responses for this story.
Be the first to respond.
SaaScribe
Apr 24, 2015·4 min read
For those of us who are following the on-going price war in the cloud storage market with interest, it is abundantly clear that it has become a classic race to the bottom with Dropbox, Google, Box, and Microsoft apparently in free fall towards unlimited storage at a nominal monthly cost. So is this a zero-sum game or does somebody stand to win?
Imagining for a minute that I’m a business IT Manager and I recognise that my “prosumer” users are using a wide array of personal storage solutions for their confidential, business-related information, and hoping to put some manners on these users and their data to appease my CEO who is freaking out about the potential PR disaster that awaits him if some of our customers’ data were to be leaked, I decide to take a look at my options:
ProviderStorage CapacityPrice/user/month Dropbox for Business1TB (min. 5 users)€12 BoxUnlimited (min. 3 users)€12 Google DriveUnlimited (> 5 users) 1TB ( €8 OneDrive for Business1TB (1–300 users as part of O365 Business Essentials)€3.80
The first thing to note here is capacity at either 1TB or unlimited from all the major providers — do my users even need that much space? My experience tell me not so straight away the differentiation between 1TB and limited loses its impact, particularly given the price difference.
Next up is usability, ease of migration, and generally how happy will my users be to start using this solution? Well, a lot of them are using Dropbox and Google’s consumer solutions already so those should be a pretty easy switchovers, but then I’m sure they’re all pretty intuitive and, again, my CEO is going to be more worried about price.
So that leaves us with price, and it looks as though Microsoft’s OneDrive for Business is best value for money — €3.80 for 1TB of storage (and didn’t I read that it was increasing to unlimited later this year?) with Office Online, a 50GB Exchange Online Mailbox, Lync, SharePoint, and all those other Microsoft products? How do they do it?
Good question, friendly IT Manager — how do they do it? Well, for established software giants like Microsoft and Google, it’s really a just a matter of cross-subsidising revenue streams and offering super low cost storage to get users on to their platform. Storage is a Trojan Horse — it’s like supermarkets selling milk and bread below cost to get customers in the door in the hope that will buy some steak and wine while they’re here and enjoying the experience. If we can extend the clumsy analogy a little further, the steak and wine for Microsoft and Google is their cloud productivity platforms, Office 365 and Google Apps respectively, which both will aim to upsell their storage customers to as soon as they have them on board.
There are a couple of gambles taking place in this high stake game of storage poker. The first is the relatively low-risk assumption that the majority of customers won’t get anywhere close to using 1TB of storage so this is really just an above-the-line proposition to entice the customer on board. The second is the more risky assumption that one’s storage platform is such a fantastically sticky product that it makes the productivity platform with which integrates an obvious choice for the user. The fact that Microsoft have recently integrated their Office 365 platform with Dropbox for Business highlights this fact even more — storage has become commoditised and productivity is where the real value is perceived to lie.
The cross-subsidising of storage is all well and good for Microsoft with their legacy software revenue and Google with their advertising revenue, but what about the like of Dropbox and Box for whom storage is their number one product? Will their slick user experience and established share of the consumer storage market be enough to keep them afloat against the big boys who seem determined to price them out of the market? It is unlikely that Dropbox or Box could develop a productivity platform to rival Office 365 in any meaningful way — even Google has struggled to do so, which may explain their reluctance to reduce their storage pricing to quite the extent Microsoft has — so it is a smart move by Dropbox to hitch its wagon to the rising star of Office 365? Or is this the first sign of a potential acquisition that would see Dropbox’s 300 million users and, in my humble opinion, superior storage solution being subsumed into Microsoft’s portfolio? One thing is for certain — this particular war will not be without casualty.
by Michael Cullen @michaelcullen87
Originally published at saascribe.com on April 23, 2015.
Digital Magazine for SaaS Professionals
1 
1 
1 
Digital Magazine for SaaS Professionals
"
https://medium.datadriveninvestor.com/what-makes-snowflake-different-from-other-dbms-as-a-cloud-based-software-3f21457cd78f?source=search_post---------357,"In recent years, Snowflake DB has become more and more popular because of its creative features. A lot of companies who are leaders in their business domain have started their journey with Snowflake, such as Sony, Logitech and Electronic Arts. So, what are these “features” that make Snowflake such attractive and competitive? In this article, I will pick up some…
"
https://medium.com/@tiwari_nitish/minio-benchmarks-with-cosbench-81704a8f0178?source=search_post---------358,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
Jan 16, 2018·4 min read
Minio is an AWS S3 compatible object storage server, that can be deployed on a variety of hardware and software platforms. Consistent S3 API with freedom of deployment on a variety of platforms makes Minio an ideal cloud storage platform.
The versatility of Minio server makes it tricky to gauge performance since several factors decide the overall performance. These factors include storage drives in use, available network bandwidth, available RAM and available processing power. These details may differ per deployment and user, making it impossible to state a universal performance number that holds correct for everyone.
To make things straightforward, we chose fastest NVMe storage drives available from cloud providers to test Minio server performance. Here are the details of the configuration used for benchmarking:
Here is the NVMe disk I/O performance using 10GiB files (measured using Linux dd command)
We tested with object sizes used are 256 KiB, 1 MiB, 5 MiB, 10 MiB and 32 MiB — each with parallel threads ranging from 128 to 2048.
Write operations
Single node deployment write bandwidth usage peaks at around 2.79 GiB/sec with 1024 parallel threads attempting to write 32 MiB objects.
The write throughput peaks at 4875 ops/sec with 128 threads trying to write 256 KiB objects each.
Read operations
Read operations were benchmarked with similar number of parallel works and object sizes. Minio single node deployment read bandwidth peaks at around 2.51 GiB/sec with 512 parallel threads attempting to read 32 MiB objects.
Minio single node deployment read throughput peaks at around 9544 ops/sec with 512 to 2048 parallel threads attempting to read 256 KiB objects.
Next let's take a look at Minio distributed erasure code mode. The setup includes 10 server instances and 10 client machines. Object sizes are 256 KiB, 1 MiB, 5 MiB, 10 MiB, 32 MiB and 64 MiB — each with number of parallel threads ranging from 128 to 8192.
Write operations
Distributed erasure setup write bandwidth usage peaks at around 8.8 GiB/sec with 64 MiB objects being written by 2048 parallel threads.
Write throughputs peak at around 1200 ops/sec for 256 KiB objects being written by 8192 parallel threads.
Read operations
With only read operations, bandwidth usage peaks at 8.17 GiB/sec with 10 MiB objects being written by 8192 parallel threads.
Read throughput peaks at 1556 ops/sec with 256 KiB objects being written by 128 parallel threads. Same operation when done by 8192 threads reach 1552 ops/sec.
COSBench configuration files used for benchmarking are available in our benchmarks repository.
While you’re at it, help us understand your use case and how we can help you better! Fill out our best of Minio deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
See all (472)
90 
2
Thanks to Frank Wessels. 
90 claps
90 
2
Minio.io
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tiwari_nitish/build-aws-s3-compatible-cloud-storage-on-gcp-with-minio-and-kubernetes-159cc99caea8?source=search_post---------70,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
Jan 30, 2017·5 min read
Applications today generate more data than ever, and this upward trend is expected to keep up in foreseeable future. How do you handle this ever growing storage requirement of your application? A storage solution that can run where your application runs and can scale with it in an automated manner, is the way to go. Add multi-tenant capabilities and it becomes near perfect!
Minio provides a reliable, lightweight object storage service. Running it on an orchestration platform like Kubernetes, adds automated storage mapping and multi-tenant capabilities. Such setup has a clear separation of concerns — one of most important parameters for scalability. Also, it can be very easy to find and isolate errors in such a setup.
Minio running on orchestration platforms like Kubernetes is a perfect solution for growing storage needs.
In this post, we’ll see how to build AWS S3 compatible object storage server on Google Cloud Platform with Minio and Kubernetes. We’ll also see how you can scale this setup for a multi-tenant environment.
Minio is a lightweight, AWS S3 compatible object storage server. It is best suited for storing unstructured data such as photos, videos, log files, backups, VM and container images. Size of an object can range from a few KBs to a maximum of 5TB. Salient Minio features include
For readers not aware of Kubernetes terminology, I’ll quickly go through all the terms used in this post.
Pod: A pod is the smallest unit of computing in Kubernetes. It is a group of containers running in shared context.
ReplicaSets: A ReplicaSet ensures a specific number of pod replicas are always up and running. While ReplicaSets are independent entities, they are mainly used by Deployments as a mechanism to orchestrate pod creation, deletion and updates.
Deployment: A deployment can be thought of as an abstraction containing Pods and ReplicaSet.
Service: A service defines a logical set of Pods and a policy by which to access them. The set of Pods targeted by a Service is determined by a Label Selector (defined in Service’s yaml file).
Persistent Volumes: A Persistent Volume (PV) is a piece of networked storage in the cluster with storage specific details abstracted away.
Persistent Volume Claims: A Persistent Volume Claim (PVC) is a request for storage by an application/pod.
To get started you’ll need a Kubernetes cluster running on Google Compute Engine (GCE). Follow these detailed steps on setting up a Kubernetes cluster on GCE.
With persistent volumes (PV) and persistent volume claims (PVC) — Kubernetes makes it very easy to abstract away physical storage details from your application. You can just create PVs with the physical storage in your cluster and then let your application ask for storage it needs via PVCs. As a storage request is made via PVC, Kubernetes maps it to actual storage (PVs) automatically.
Let’s explore this further in Google Compute Engine context. GCE has disks that serve as physical storage for your compute nodes. On Kubernetes, you can create PVs that use these disks as the backbone physical storage.
Later, as you deploy Minio on the Kubernetes cluster, you can create PVCs to request for storage that you need for that particular Minio instance. Kubernetes automatically binds matching PV to PVC. This is known as static binding in Kubernetes world, and yes, there is dynamic binding too, but we’ll skip that for now. Read more about binding here.
Now that you’ve a clear picture of how things work, lets start with creating a GCE disk.
This creates a disk named disk1 with a size of 10GiB. Now create a PV based on the GCE Disk we just created.
Download and save the file as minio-gce-pv.yaml. You can then create a persistent volume using the command:
A deployment encapsulates replica sets and pods — so, if a pod goes down, replica set makes sure another pod comes up automatically. This way you won’t need to bother about pod failures and will have a stable Minio service available.
But before creating the deployment, we need to create a persistent volume claim (PVC) to request storage for the Minio instance. As explained above, Kubernetes looks out for PVs matching the PVC request in the cluster and binds it to the PVC automatically.
This automation can come in very handy if you need a large scale multi-tenant environment with varying storage requirements. You can spin up a Minio deployment (with a PVC requesting appropriate storage therein), per tenant. Kubernetes automatically binds the PVCs to PVs. This way you have a multi-tenant, stable, S3 compatible object storage server at your command!
Here is how you can create a PVC and a single pod deployment running Minio Docker image.
Download and save the file as minio-standalone-deployment.yaml. Notice that we create PVC first and then the deployment uses it as its volume. You can then deploy Minio using the command:
Now that you have a Minio deployment running, you may either want to access it internally (within the cluster) or expose it as a Service onto an external (outside of your cluster, maybe public internet) IP address, depending on your use case.
You can achieve this using Services. There are 3 major service types — default type is ClusterIP, which exposes a service to connection from inside the cluster. NodePort and LoadBalancer are two types that expose services to external traffic. Read more about services here.
Below yaml file configures a LoadBalancer service for your Minio deployment.
Download and save this file as minio-service.yaml and run the command —
The IP address where the service is served generally takes a couple of minutes to be created after the above command is run. You can check the IP using —
Once you have the IP address available, you can access Minio via the address
http://<Service_IP_Address>:9000/
Access Key and Secret Key remain the same as the environment variables set in minio-standalone-deployment.yaml.
Note that LoadBalancer will work only if the underlying cloud provider supports external load balancing.
Kubernetes comes bundled with a neat dashboard. You can easily track your Minio pod’s memory, CPU usage and many other metrics via the dashboard.
To access the dashboard, execute the command —
Access the URL mentioned against kubernetes-dashboard. Here is how my dashboard looks
Need help? We hangout on Slack. Join us!
While you’re at it, help us understand your use case and how we can help you better! Fill out our best of Minio deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
16 
16 
16 
Minio.io
"
https://medium.com/bb-tutorials-and-thoughts/building-a-react-static-website-with-gcp-cloud-storage-1d8a5299c431?source=search_post---------76,"There are currently no responses for this story.
Be the first to respond.
There are a number of ways you can build a website with React such as Java with React, NodeJS with React, NGINX serving React, etc. For the single-page applications, all you need to do is to load the initial index.html. Once you load the index.html the React library kicks in and do the rest of the…
"
https://medium.com/@storjproject/what-is-decentralized-cloud-storage-3a530f1552?source=search_post---------54,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Oct 17, 2020·5 min read
Tardigrade is the world’s first enterprise-grade, decentralized cloud storage service. Through decentralization, Tardigrade is more secure, more performant, more affordable, and more private by default than centralized cloud providers.
What exactly is decentralized cloud storage? On the user’s end, it operates exactly the same as traditional cloud storage options like Amazon S3. But, instead of your files being stored in a big data center that’s vulnerable to outages and attacks, your information is stored on thousands of distributed Nodes all across the globe.
First off, we aren’t going to get super technical here. This is an overview of how it works, so if you really want to dig into the technical specifications of Tardigrade (the nuts and bolts stuff), you can check out our documentation. Here’s also a cool diagram to reference.
As previously mentioned, with traditional cloud storage, all of your data resides in one large data center. This often leads to downtime and outages when one of these facilities goes offline. With decentralized cloud storage, we have a large, distributed network comprised of thousands of Nodes across the globe that are independently owned and operated which store data on our behalf. A Node is simply a hard drive or a storage device someone owns privately. We pay all of our Node Operators to store files for our clients, and we compensate them for their bandwidth. Think of it like this: You have a 10 TB hard drive, and you’re only using 1 TB. You could sign up to be a Node Operator and we would store pieces of our clients’ files on your hard drive utilizing your unused space. Depending on how many files you store and how many times that data needed to be retrieved, we’d compensate you accordingly.
The real issue with centralized providers like Amazon S3 is all of your data resides in huge data centers. If a part of Amazon’s network goes down, you won’t be able to access your data at best, and at worst, your data could be permanently lost or damaged. Large data centers are also vulnerable to hackers, as we’ve seen time and time again. With decentralized cloud storage, end-to-end encryption is standard on every file-each file is encrypted on a user’s computer before it’s uploaded, broken into pieces, and then spread out to uncorrelated Nodes across our network. Only you have access to the encryption keys, making it virtually impossible for your data to be compromised or stolen.
Plus, centralized cloud storage costs a lot more than our decentralized network. Large data centers cost a ton of money and take a lot of resources to operate. The fact we don’t have to spend money operating a data center, but rather use individual, privately owned devices, means we pass those savings onto our customers.
We’ve never lost a single file on the Tardigrade network due to our decentralized architecture. Tardigrade has 99.99999999% file durability, and since we split each file into 80 pieces, and we only need 30 pieces to reconstitute a file, it would take 51 nodes going offline at the same time for your file to be lost. Complete files are fully retrieved at lightning speed by downloading the fastest 30 of 80 pieces. If you know how torrenting works, it’s the same concept. There’s no central point of failure, ensuring your data is always available. Since each file uploaded to Tardigrade is split into 80 pieces, encrypted, then stored on 80 different Nodes, one Node going offline won’t impact any files stored on any particular Node.
The real beauty of the decentralized architecture lies in the fact that a Node Operator doesn’t know what files are stored on their Node. Even if a Node operator wanted to access your files, they only have a small shard or piece of that file. They would have to track down at least 30 other Nodes to reconstitute a file, and all of those files are also encrypted. It’s virtually impossible to compromise a file.
Storj is what we like to call “trustless.” What does this mean? It means you don’t have to place your trust in any single organization, process, or system to keep the network running, and you don’t have to worry about your data because we couldn’t access it even if we wanted to. Tardigrade is private and secure by default, and files are encrypted end-to-end before they’re ever uploaded to our network, ensuring no one can access data without authorization.
A file on Tardigrade is almost impossible to access without the proper keys or permissions. Because everything is encrypted locally, your data is literally in your hands, and no one else’s. After files are encrypted, they get split into smaller fragments that are completely indistinguishable from each other. A typical file gets split into 80 pieces, of which any 30 can be accessed to reconstitute the file. Each of the 80 pieces is on a different drive, with different operators, power supplies, networks, geographies, etc. For example, there are currently 171 million files on our Tardigrade service. To compromise a single file, the hacker would first have to locate 30 of its pieces among the 171 million on the network, creating a needle in a haystack scenario. Then, they would have to decrypt the file, which is extremely difficult-if not impossible-without the encryption key. Then, the hacker would have to do all this again to access the next file.
So, there you have it, the decentralized cloud in a nutshell. If you’re interested in becoming a Node Operator, please visit www.storj.io. And, if you’re interested in trying Tardigrade for yourself, head on over to www.tardigrade.io.
Originally published at https://storj.io.
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
See all (1,648)
119 
1
119 claps
119 
1
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/a-strategic-take-on-cloud-storage-solutions-part-3-block-storage-optimization-and-comparison-873b1d0bb410?source=search_post---------173,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 5, 2021·4 min read
By Shantanu Kaushik
In the previous article of this series, we discussed how important and essential it is to optimize your cloud storage solution. We discussed the optimization scenarios associated with Object Storage Service (OSS). We also talked about some of the key scenarios and Alibaba Cloud services associated with Alibaba Cloud OSS and defined the Block Storage solution.
In this article, we discuss optimizing the Alibaba Cloud Block Storage Service. We will also compare the storage solutions from Alibaba Cloud and give you a detailed overview to select the solution best suited for you.
The first step to optimizing the Block Storage is to monitor and identify the cloud-based disk drives that are less used, overused, or not attached at all. The Alibaba Cloud Block Storage attaches and pre-configures specific storage for the system to use for read and write operations. The billing model only charges for what is utilized.
Adjusting the storage by optimizing less used and overused disks to create a balance of storage usage will lessen the load on the Elastic Compute Service (ECS) instance and reduce costs.
The Cloud disks that are not attached or that are not being used for storage operations can lead to a great optimization scenario. It will reduce security threats and will account for a great cost-cutting scenario. In some cases, even after the ECS instance is terminated, the attached cloud disk might not automatically get detached and deleted. This scenario will keep incurring costs, so it is imperative that you keep a close check on such scenarios and manually delete the unused and unattached disks.
Snapshots are an important piece in the whole cloud storage scenario. Over time, a lot of snapshots can get accumulated, especially if you have configured a policy to take snapshots on a daily or weekly basis. It is advisable to delete the large number of snapshots that hold no value. Alternatively, you can set another policy to automatically delete the snapshots older than a certain date.
If you are using the enhanced SSDs (ESSDs), you can increase the performance level of the disk online to adjust the requirements. These requirements can be performance and capacity. For your overused cloud disk, you can scale up or down based on whether you wish to increase or decrease the capacity of the disk. This can lead to a perfectly optimized usage scenario and reduce the cost associated with Elastic Block Storage.
Let’s list out a few features for Apsara File Storage NAS, Object Storage Service, and the Elastic Block Storage from Alibaba Cloud. Below is a representation of the traditional application and internet application-based share scenario, where Elastic Block Storage, Apsara File Storage NAS, and Object Storage Service (OSS) are being used.
When it comes to extracting the most out of your storage service, optimizing is the way to go. Storage optimization is a practice where you continuously evaluate the changes in your storage requirements. These changes could be a result of an expansion or a strategical move from one business type to another. Therefore, it calls for strategic planning and selection of a storage solution.
Selecting the storage that you wish you use for your deployment scenario solely depends on which usage scenarios you are working with. A typical setup might call for different storage types, but creating a service that checks most of the boxes that you have strategized for is the best practice to follow.
When it comes to the Object Storage Service (OSS), you can configure lifecycle rules to automatically optimize and manage your storage solution by converting data storage types based on the frequency of usage. With Block Storage, you can monitor and adjust your storage based on how your disks are being used (over, under, or unused.) Deleting old snapshots and unused ECS instance cloud disks are also essential practices to follow.
In the end, the most important practice to follow is to set up a maintenance plan for your storage that can monitor and optimize your cloud storage and adjust it depending on your usage to cut unnecessary costs.
1. Security and Monitoring Practices with Alibaba Cloud Storage Solutions
We will outline how Access Control and the Encryption Service works with the cloud-based storage solutions from Alibaba Cloud.
2. Apsara File Storage NAS — What and How?
We will discuss the complete architecture and usage scenarios with the Apsara File Storage NAS solution from Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/readwrite/going-to-cloud-storage-practical-strategies-to-match-your-iot-business-needs-b22d3b793e2?source=search_post---------158,"There are currently no responses for this story.
Be the first to respond.
IoT business models hinge on data analysis. From responding to emergency situations to discerning patterns in vast troves of historical data, a variety of approaches to derive insights are being made possible by IoT businesses. Despite the critical role that data plays in creating new business value, enterprises often do not have robust strategies to manage the data.
Data growth has been explosive in recent years with 63% of enterprises managing 50PB or more. Compounding the data management challenge is the frenetic pace of data growth annually — sometimes as high as 40–50%. This growth is creating a massive challenge in managing costs and effectively managing the data. In recent surveys, 51% of businesses admit their backup infrastructure (ex: tape backup) cannot keep up with the data growth and find that migration of data is painful. Lack of an effective back up and growth management strategy can be very expensive. Downtime costs expensive range between $50K-$5M per hour and can lead to incalculable reputation damages. More importantly the opportunity costs when data is unrecoverable or unavailable for use for analytics insights can lead to significant competitive disadvantages.
Before an effective data management strategy is devised, it is useful to understand how the data collected will be used. In a typical scenario (figure 1), sensors and control devices at the edge of the network continuously collect data and gateways transmit the data to on-premise or public clouds. The collected data needs to be quickly assessed and categorized in terms of frequency of access. For example, critical alarm data will be frequently accessed to ensure fixes are in place and site level actions are performed, while log/routine data collected for regulatory compliance need to be archived and will be accessed only by exception.
In recent times, with the emergence of machine learning and AI algorithms, archived data- often termed “cold data” — has become invaluable to train algorithms and devise new insights. Data management stakeholders must balance the current and future utility of the data collected and assess what data will be in “cold” storage and which will be readily available.
The type of data being collected is also an important factor in determining a storage strategy. Here, the IoT application and industry play a big role. Table 1 captures a few scenarios of data collection in four industry verticals and the kind of insights that can be driven by the data.
Data storage paradigms in the form of blobs for unstructured and binary data, data lakes for big data analytics, files for sharing, tables for schema-less NoSQL data etc. are all important to account for, as decisions on data strategy are made.
If the application generates mostly binary and unstructured data, finding the most effective way to save, archive and backup data blobs becomes critical. Similarly, if analytics are correlating structured and unstructured data, having the right approach for data lakes is needed.
Another key element in determining a data management strategy is the rate of data acquisition and consequently the rate of growth of the collected data. Highly frequent small-size data additions (ex: text data) can be as problematic as sporadic large volume data additions. Further, HD image data feeds or graphics data may be particularly expensive to store and constantly archive. Trading off the business need with the frequency of data collection, the amount of historical data needed, and the cost of data storage is essential to arrive at a sustainable data management strategy.
With the nature of data and use of data determined, security of the data becomes forefront. Due to the value and compliance aspects of data becoming prominent, it is essential to ensure that the data is secure. Whether choosing a DIY option or a commercial option, securing data is paramount as unauthorized access to the data can have costly consequences as witnessed by several high-profile breaches at FedEx, Target, etc. Similarly, not losing the data when breakdowns or outages happen is also crucial. Regular tests of backup and recovery process is, of course, best practice.
Perhaps the most important consideration in the cloud-era is the decision to host the data on-premises or in a public cloud. While advantages exist for both only a true accounting of the complexity, control premiums, the total cost of ownership and any inherent legacy switching costs can yield an objective assessment to drive decisions. On-premises infrastructure gives organizations more control but requires skills, resources, and strong processes for success. More important, the breadth of capability required to run a modern enterprise datacenter can be significant. Facility management, electrical and power infrastructure, IT procurement, management personnel and 24/7 support for users can be costly. Nevertheless, there may be situations where infrastructure control, security or proprietary considerations can tilt the economics in favor of on-premise infrastructure.
Off-premise public clouds or co-located Infrastructure-as-a-Service (IaaS) models treat compute storage as service that organizations can procure on demand. This creates great flexibility and focuses IT resources on the value-add and takes away the hassle of managing infrastructure. Robust regimes for back-up, restore and on-demand scalability provide a growing enterprise with scalable data management infrastructure.
Increasingly, hybrid environments that combine on-premise and public cloud infrastructure are popular as they overcome stakeholder concerns related to control and proprietary issues and offer flexibility and on-demand scale. Stakeholders also like the security and backup options that cloud vendors provide.
For an effective decision, a thorough total cost of ownership exercise is invaluable. Outlining the various aspects of infrastructure management, opportunity costs related to scale and flexibility are important as stakeholders evaluate options.
An effective IoT Cloud data storage strategy requires consideration of the following elements
1. How the collected data will be used and how frequently the data will be accessed
2. The type, nature, volume, and velocity of data being collected
3. How the data will be secured, backed-up and recovered
4. The most efficient storage strategy: on-premise, cloud or hybrid infrastructure
5. The most effective ways to scale storage as needs inevitably will change
6. The Total Cost of Ownership (TCO)
Originally published at readwrite.com on November 6, 2018.
ReadWrite is the leading media platform dedicated to IoT…
Written by
The latest #news, analysis, and conversation on the #InternetOfThings
ReadWrite is the leading media platform dedicated to IoT and the Connected World. We work with the industry's top technologies, thinkers, and companies to tell the stories that drive this world forward.
Written by
The latest #news, analysis, and conversation on the #InternetOfThings
ReadWrite is the leading media platform dedicated to IoT and the Connected World. We work with the industry's top technologies, thinkers, and companies to tell the stories that drive this world forward.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/s-c-a-l-e/databases-at-box-when-scale-collaboration-and-enterprise-users-collide-42b93e3e9b51?source=search_post---------230,"There are currently no responses for this story.
Be the first to respond.
Tamar Bercovici has helped cloud storage and collaboration vendor Box grow from a small company running a single MySQL server into a public company managing a massive data layer that serves some of the world’s largest companies. In 2014, BusinessInsider named her one of the most powerful female engineers in the tech world.
In this recent interview with SCALE, Bercovici discusses how Box has built its data architecture, the unique challenges of managing data for a company like Box and why it can be so hard for large, innovative companies to adopt new technologies.
SCALE: You’ve been at Box for a few years now. Can you walk through the evolution of data infrastructure there?
TAMAR BERCOVICI: I’ve been at Box for almost four and a half years now, and there’s definitely been a lot of change, both in terms of the scope of technical challenges that we need to deal with and also how our architecture has evolved to handle that. I find it interesting to look at the way the team has evolved as well.
When I joined Box, I joined as an engineer on the backend engineering team, which was pretty small — I think we were around six people at the time, total. Somewhat randomly, my first project ended up being on optimizing queries on our database. We were seeing some bottlenecks in our replication mode, which meant that we had more users who were doing more operations on the site and that was starting to take its toll because we were basically running off of a single MySQL database at the time. We had obviously had replicas and failovers and things like that, but pretty much at any given time all our traffic was being served from a single host.
The first thing I did was structure some low-hanging fruit optimization queries that could easily be made better, and look for things we didn’t even need in the first place. After that, we bought ourselves a little headroom.
We also upgraded our hardware a bit, so we did a little bit of vertical scaling, and then we took a step back and said that we have to find a more scalable infrastructure. We had some interesting things we wanted to push on the business side, so we were expecting our user activity to grow more drastically and we felt that we needed to move to a more stable architecture to be able to support that.
We looked at a lot of different options and ultimately decided to basically stay with MySQL, but shard our largest and fastest growing tables. So we embarked on a pretty long project — I think once we started doing the design work and planning, through to having it fully rolled out and completed, it was probably around nine months.
Since then we’ve sharded some additional tables and made some other optimizations in our infrastructure. Sometime after completing that first big project, we also decided that it was time to form a team that was focused specifically on our data access infrastructure now that it was a bit more complicated. We formed a team around that, which I began to manage and that’s grown a bit since then.
We solved our first on-fire scalability bottlenecks, but now it’s really about taking a step back and figuring out what are the core infrastructure components that we need to be able to build and support and, in a lot of cases, pulling those out of our web-app code base, basically. If you want to do something that’s more client-agnostic that can also support some of the new services and projects that we’ve begun to ramp up, you don’t want to be baked into the one big first client use case that you had. So we’re in the process of building out better abstraction layers around our systems and then also evolving the architecture as we continue on that path.
“We predominantly use either MySQL or HBase for persistent storage, and then on the caching tier we use memcached and Redis.”
You were able to scale Box as a whole on MySQL, but do you look at new databases or data systems as you build around the new use cases or features you mentioned?
We’ve evolved the suite of technologies that we use. We have a bunch of stuff that we use on the analytics and data warehousing side, which is not my main area of focus, but we use Hadoop in that area and we have been for a while. On the core metadata side, we still have the majority of our objects stored in the MySQL — users and enterprises and files and folders, all of those are in sharded MySQL.
We’re also using HBase for a few key use cases. We have a really interesting piece of infrastructure that’s basically a guaranteed-delivery message bus that’s implemented on top of HBase as a persistence layer. We use that to host the stream of events that happen in a user’s account.
That’s something that’s available through our third-party API, so a bunch of our external developers use that to basically build any type of event stream or event-triggered behavior into their applications, but then we also use it internally to drive our desktop application, our updates tab, notifications and things like that.
We also use HBase in our Notes product. We store versions of the notes that people are collaboratively editing, and there’s a part of that that get stored in HBase. We also use that as part of our storage service — we’re actually in the process of migrating the metadata from the files themselves from MySQL to HBase.
We predominantly use either MySQL or HBase for persistent storage, and then on the caching tier we use memcached and Redis.
“Ultimately, after a pretty involved analysis, we figured out that Kafka wasn’t going to be quite good for our use cases.”
Is the message bus something that Box built internally?
We did build it ourselves. This is something that we probably built three years ago or so. This is the first time that we used HBase in our stack and so was it a little bit of an experiment.
Part of the reason that we liked HBase was because it gave us a really good guarantee of persistence. The fact that you can configure it so that any write that you do to the system has a minimum number of replicas, and then if one of the nodes goes it’ll rebalance and get back to that level of guarantee on its own, made it a good fit from that perspective. It also fit the type of access pattern that we needed to do message queues, which is basically doing range reads to pick up the latest messages in the stream.
We’re actually in the process right now of pulling it out and making the message bus more generic. Right now, it really is sort of an entire system that’s just goes around implementing those user event streams that I described before, but really there’s a more generic concept of a guaranteed-delivery message bus that could be used for a bunch of other use cases.
Before embarking on that project, we did a reevaluation of whether we should keep it as a system in HBase or if we should use a different queuing technology. One of our lead contenders to switch to was Kafka. We used Kafka on the analytics side of the house to collect all of our log data. But, ultimately, after a pretty involved analysis, we figured out that Kafka wasn’t going to be quite good for our use cases.
First off, it does have different persistence guarantees, but actually that wasn’t the key differentiator for us in this case. It was more that Kafka is tuned to having a relatively small number of very high-volume queues, and for us we actually wanted a dynamically large set. In our current use case, we basically have a queue per user.
We also need to be able to support this pretty fancy failover mechanic, so that if I were reading my stream of events and we need to failover in the middle to another datacenter, we can basically keep the correct position in the queue and make sure that we haven’t missed any events.
If you think of a log collection situation, it’s OK if eventually you’ve got all the data, but you’re not trying to do it one by one — it’s more about looking at the full picture of what’s going on. Whereas here, there’s really a sequence and so we had to build some sophistication into our system around how we do those failovers, and we felt that we got basically better infrastructure from HBase to be able to do that.
What are some unique challenges of doing data systems at Box given the number of users and those types of work they’re actually doing? And does this change as the company grows or even goes public?
Well, I don’t know if the IPO in and of itself changed anything in terms of our database requirements, but I think there are a few key themes that are a little bit unique. The first is just that because we’re selling to enterprise customers, their expectations in terms of how we handle their data, or how we handle various flows around permission, access control, security and things like that are very important for us to get right.
So, for us, things like consistency and security and auditability are very, very important. Hence, we need to build a message bus that absolutely guarantees it never loses anything because we can then use it as an audit log, for example. Especially with us selling to businesses and various types of regulated industries that are doing pretty interesting things with our system, we have to make sure that we’re building a system that can support that.
From the data layer perspective, it’s really about building the right types of tools that the other developers at Box can leverage to build these kind of enterprise-grade features, so supporting things like transactions, consistent reads from read replicas, making sure that our cache fails over in a consistent way, or making sure that the messages that you put in the message bus will be guaranteed to delivered. Things like that all enable the other developers at Box to build the type of features that we need to sell to these enterprises.
I think the other interesting aspect that’s more specific to our data architecture is the collaboration model that we have. A huge aspect of Box is the fact that not only do you have access to your data from everywhere, and you have all these cool things like search, but it’s also about the people that you collaborate with. A lot of our customers are using us for use cases where they’re working with some number of people on a project and using the content as the basis for that. In some cases, those people might be outside of their enterprise, so it’s an external collaboration use case, which is again a hugely important and valuable flow for us from a business perspective.
What it means on the data side is that when we decided to partition our data, we did go for the natural partitioning based on enterprise, but we still had to be able to support these scenarios where you can actually interact with data that’s on a different shard or new data from multiples shards at one time. So we’ve definitely had to support a broader range of use cases in our sharding architecture that has to do with the way our collaboration model works.
“It’s a large-scale distributed system, so we can’t have it all. You always need to be smart around where you place your tradeoffs.”
Is there a limit to how consistent or how fast stuff has to happen? How real-time is real-time-enough for most users?
It’s a large-scale distributed system, so we can’t have it all. You always need to be smart around where you place your tradeoffs. For example, in that system I described before, we guarantee that every message will be in it, but we don’t actually guarantee that the order of the messages will be the same order as the updates that happened in the first place. So, for example, that’s where you have to choose which constraints you’re going to keep versus which constraints you’re going to relax.
Then I think also, in our data model, we always seek to look for what operations do we have to make strictly transactional and make sure that they are consistent to anyone who’s reading the result of our operation at the same time, versus spaces where you can have a lag in propagating and updating. You have to optimize differently in different use cases.
For Box, or presumably for any company trying to do a file storage and collaboration play, what’s the biggest challenge that you face?
I have a few that I can choose from. I mean, definitely just the scale overall is a thing. Some of our customers, they upload huge volumes of data and then … that obviously impacts our file storage layer. But then also on the database layer, because you need to be tracking some amount of metadata for every one of those files. The size and scope of our metadata store is growing rapidly and continues to grow rapidly, so that means that we’ve had to invest and continue to invest in sharding architectures and then the management layer for that sharded architecture, et cetera.
An interesting wrinkle in that story, and something that I think makes our data model more complicated, is the fact that you have collaboration between users and multiple enterprises. Any way that you partition the data, they’re always going to need to potentially access data from multiple shards. That means our data access layer needs to be able to handle these use cases.
In fact, we even have just one tier of access where we might need to move data between shards in response to user actions. If I move one of my files into one of your folders, and we happen to be on different shards, then I’m actually going to need to take the metadata associated with that file and move it to a different production database that could be on a different server. That’s a dataflow that we need to be able to support.
“It’s almost always the case that you can’t just take something and drop it into your infrastructure and be done with it. There’s almost always some sort of customization layer or custom configuration of something that you need to build around it so that it actually supports your use case.”
Another angle that’s definitely challenging for us is selling to larger and larger customers that have their own unique flows. It just presents a higher degree of variability in terms of what we need to be able to support.
One of the core aspects of our product is that we have a platform layer, so we have an API layer that’s able to provide some functionality that we then expose to our web, mobile and desktop clients. That could be used for anything from just developing a third-party enterprise application on top of our platform, through to custom integrations that our customers sometimes build. You have customers that build custom integrations, and now you have some automated process uploading content into their account, and you can all of a sudden need to support higher rates of uploads, or higher rates of modifications, or wait conditions between multiple threads that are trying to do this at the same time.
We’ve definitely had to engage with some of our newer paying customers on the flows that they’re going to make so that we can support them in a good way.
Are those types of things unique to a business like Box? Can you think of other industries where you might run into the same challenges?
We have some patterns that you see elsewhere, but sometimes I think the combination is unique. We are an enterprise company, and I think that any enterprise company has this challenge of large deployments versus small deployments and making sure that what they’re building works.
But then we also have this collaborative access, which is a lot more similar to a social company data model. Specifically, like a Facebook or a LinkedIn. But those guys, on the other hand, maybe they don’t have the same variability between users. Also, they have the celebrity scenario that we don’t need to account for. If Taylor Swift opens a Box account, probably that’s going to be private.
Correct me if I’m wrong, but it sounds like the ethos at Box is we’re going to build stuff only if absolutely necessary, right?
I think at the end of the day, you want to be smart about where you’re spending your resources. We’re huge believers in open source. We love using open source technology. We love open sourcing stuff that we ourselves have built, so we’ve had a nice push on that front in the past couple years.
If there’s a tool that’s developed by the community that’s used by companies that are at a larger scale than us, for example, where it’s been battle-tested, then that can be a really good core component to put in.
I think what’s interesting, though, is that it’s almost always the case that you can’t just take something and drop it into your infrastructure and be done with it. There’s almost always some sort of customization layer or custom configuration of something that you need to build around it so that it actually supports your use case. I think that’s actually one of the most interesting aspects of building a system for scale, is that it’s really more about optimizing than about finding a general solution.
We all know that there is no general solution that works in all cases in all ways once you’ve distributed your system, so you have to look at your use case and basically find what’s valuable to you. What are the constraints that you absolutely have to adhere to, versus what are things that you can maybe relax or exploit? I think when you look at the infrastructure that companies end up with, they have this mix of using open source technologies, but then building different abstraction layers around them or connecting them in different ways.
Like in our case, HBase and MySQL are super-core components of our infrastructure, but we don’t really just access them directly. We have something around them that solves the specific problem for us. Even LinkedIn, I believe one of their internal data source that they developed, Espresso, is actually based on MySQL and Lucene. So they’re also using, occasionally at least, open source projects but then building a different abstraction layer around them.
“You have to figure out what’s going to be valuable to the developer building on top of that infrastructure and make sure that you support those use cases well.”
When you look around the database ecosystem right now, are there things that really excite you? Even if you’re not going to replace MySQL or HBase at Box, are there things that make you say, “Wow I would like to work with this!” or “This is really promising!”?
We try to keep up most with open source projects that are going on and also some of the database companies. I find it interesting to see what kinds of directions people are going in.
At the end of the day, when you’re building an infrastructure layer, the more you try to support, the more complex it’s going to be. Therefore it’s going to be harder to scale and harder to maintain. So in some sense, you’re striving for simplicity. On the other hand, anything that your infrastructure does not support … any developer that’s building on top of that will have to deal with that challenge.
Either it’s something that you really don’t need, it’s a case or a type of functionality that you don’t need in your company, or you’re going to have 10 different dev teams that implement their own version of secondary indexes. You have to figure out what’s going to be valuable to the developer building on top of that infrastructure and make sure that you support those use cases well.
There’s a lot of different data stores popping up in the wild, but I don’t know that there’s a one that solves all problems. There’s a scenario where you need a little bit of a relational data model but you still want a flexible data store. There’s a lot of people attempting to go in that direction, but I don’t think anyone’s solved it in a meaningful fashion.
“If you’re going to be the biggest deployment of that database technology, you’re basically going to be finding all of their kinks and all of their bugs. And some of them definitely only show up under a load or in specific types of conditions.”
If there was a pull away from MySQL, there seems to be a pull back toward it right now. Does the fact that people know a technology and it’s proven hinder adoption of some new possibly great thing?
It definitely is a really difficult area to break into because it’s not just about what you know — although that is definitely a part of it — but it’s also when you’re choosing a new database technology, you want it to be battle-tested. I think that’s why some of the more successful open source projects are ones that have basically been deployed or developed at companies that have large scale. You know that if a company significantly larger than you has successfully used this technology in their production environment, then probably you’re going to be fine.
Whereas, if you choose something that’s a lot newer and potentially more innovative … if you’re going to be the biggest deployment of that database technology, you’re basically going to be finding all of their kinks and all of their bugs. And some of them definitely only show up under a load or in specific types of conditions. It puts you in a position of being dependent on how quickly they’re able to address it or how quickly the community is able to address if it’s an open source project.
We definitely bias very, very heavily toward open source with all the technologies that we choose to use in-house. We prefer a project where they’ve been doing it in production at large scale, there’s a lot of activity in the community and you can see that there’s progress being made. That’s an important feature to us.
“People who are paying Oracle anyway, they might as well be paying them for a different or more newer style technology, for sure.”
Can you think of a proprietary data store in the past few years that you’d seriously consider using?
I find it interesting to see what companies that are focusing all their efforts just on developing the new, interesting ways of doing databases are coming up with.
But again, we have a pretty strong bias against most of these guys because usually they’re newer, so we would be one of the larger, if not the largest user, which is just a huge liability for us. With any small company, you’re sort of at the mercy of their ability to keep up with your scale or their ability to address any problems that come up.
Then also, of course, if they get acquired or go out of business or anything like that … Open source — especially if you have a few strong companies that are committed to contributing to the project that continue to develop it in some sense — has sort of a stability aspect to it that’s a little bit difficult for vendors to replicate.
When you look at the vendors, I feel like a lot of them are basically trying to understand this themselves. Some of them built open source projects and then sell the services. I think they’re also trying to figure out what the best way is to break in, because it’s sort of counterintuitive. Databases are actually very hard and very difficult and it makes sense to focus on developing technology, but it’s interesting to see if there’s something very strong that comes out of the proprietary model in that field.
I haven’t seen anything yet that we’re seriously considering moving to.
Is there anything holding back from a company like Oracle from just rolling every new popular feature that comes along into its database, and then living on in perpetuity because it’s adding the new things its existing customers are asking for?
It’s actually easier to sell databases to companies that have less of a growth trajectory, where they have scale but it’s a little bit more stable. They can have someone set a solution to support the scale they’re at, and that shouldn’t change in a super drastic way over time. So people who are paying Oracle anyway, they might as well be paying them for a different or more newer style technology, for sure.
For a company like us, it’s really hard to commit to a vendor because we would be committing to its path. We have the current state that we’re in right now. That’s going to be different six months in the future and six years in the future, and all of the points in between. That’s why my team’s usually busy evolving our own infrastructure and adding in the features and capabilities that we need to keep up.
What’s next in computing, told by the people behind the…
16 
1
16 claps
16 
1
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
What’s next in computing, told by the people behind the software
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
What’s next in computing, told by the people behind the software
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@graymiller/ok-this-is-too-pretty-a-gauntlet-not-to-take-up-794cfab1a14c?source=search_post---------276,"Sign in
There are currently no responses for this story.
Be the first to respond.
Gray Miller
Nov 18, 2015·1 min read
Jake Knapp
Ok, this is too pretty a gauntlet not to take up. I confess I didn’t quite go so far as you in terms of absolute simplicity, but I did take off everything with a “rapid” stream — Twitter, tumblr, Instagram, Pinterest — leaving only “long format” digital media. Things like Medium, Kindle, scribd.
I also left anything that was creative-iMovie, writing apps, drawing apps- and also my cloud storage stuff (Dropbox, Simplenote) so that I can still write as needed.
The goal is to channel my attention — try for deeper thoughts, more nuanced responses rather than reactions. Also to encourage my overactive brain to produce and create rather than consume. It’s Moore’s “Care of the Soul” taken into my phone.
Yep, I’ll give it a week. Then we’ll see. Thanks for the challenge!
Gray is a former Marine dancer grandpa visualist who writes to help adults figure out what they want to be when they grow up.
6 
6 
6 
Gray is a former Marine dancer grandpa visualist who writes to help adults figure out what they want to be when they grow up.
"
https://medium.com/@Egnyte/5-simple-steps-to-understanding-your-data-analytics-93c301bfb4bf?source=search_post---------393,"Sign in
There are currently no responses for this story.
Be the first to respond.
Egnyte
May 30, 2017·3 min read
By Isabelle Guis
It’s not surprising that IT leaders face mounting pressure to support new business initiatives with real-time analytics. Your organization may even be tempted to make large investments in hardware, software, or talent to tackle the challenges. But before taking any initiatives, implement the five actions below to not only tackle but embrace these challenges.
1. Classify All The Data Analytics You Have
Take the opportunity of a change in your IT infrastructure that will force you to look at your data to tag and classify it. For instance, migrating to the cloud, upgrading on-premises infrastructure or changing cloud providers is a good triggering event.
· Who captures the analytics?
· Where is it?
· Do you retrieve it from your data center or via API?
2. Identify Valuable Information
Knowing the location of your valuable data is just as critical as knowing who the decision makers are in your organization. Make sure to group your data (at least logically) around business objectives, for instance the data need for a new customer service or a new regulation, such as the GPDR (General Data Protection Regulation) where there will be no grace period.
3. Group Data Around Business Objectives
Always keep your data organized around business objectives so they support your strategic activities, not by type, storage location, owner etc. For example, group them around:
· Business forecast
· Infrastructure optimization
· New product launch
*IDC# US42388517 (March 2017) Source: IDC Customer Insights and Analysis IT and Communications Survey, 2016, N=3528
4. Get Actionable Insights
Start with a business objective in mind and track back to the set of data analytics needed and focus on this to get the right information you need to act. For instance, if you have a Governance and Policy Management strategy, determine the actions you will have to take and then the associated data insights:
· Actions: Encrypt PII data, set policies to access intellectual property content, eliminate unnecessary copied of sensitive data.
· Insights: where the PII, IP and redundant copies are, who has access to them, when were they last accessed, etc.
5. Continuous Improvement Towards Your Business and IT Transformation
Keep up with the data analytics discipline and make it part of your DNA. Assess if it:
· Is needed for one project vs. group needs · Is outsourced vs. in house data · Should be integrated with an app or standalone
· Provides insights for users to decide or is part of a fully automated decision accessed etc.
With the growing data challenges comes many opportunities and this is the time for IT leaders to step up and expand their responsibilities as they prepare their company for Digital Transformation.
Originally published at www.egnyte.com on May 30, 2017.
Connect better. Protect better. Do more together.
See all (717)
Connect better. Protect better. Do more together.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/relationship-building-over-zoom-1239c211aa59?source=search_post---------288,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Jul 21, 2020·6 min read
Storj Labs decided early on that Tardigrade.io, the leading globally-distributed cloud storage platform should be built by an equally distributed team. As a result, Storj now has workers spread across 24 cities in 13 countries. While we definitely know a thing or two about effectively managing remote teams, it doesn’t come without its challenges. Despite remote work becoming more popular and widely adopted — especially in a post-COVID-19 world — there are significant trade-offs, both for the company and the employee.
In Buffer’s 2019 State Of Remote Work report, loneliness and collaboration ranked among the top three struggles for remote workers. This seems like a no-brainer, right? The basic definition of “remote” refers to faraway isolation, so feelings of alienation are expected. Regardless, the research and data on interpersonal connection at work are widespread and abundant — see here, here, and here — and it all comes to the same conclusion: social interaction is a key contributor to overall well-being, which directly impacts employee engagement and productivity.
While remote work can never perfectly simulate in-office life, it’s important for companies to be intentional about making remote work feel less, well, remote. Many companies — Storj included — host a few company retreats each year in which all employees are brought together for face-to-face collaborative work and recreation. While these retreats are powerful and invigorating, they can’t be expected to make up for the other 48+ weeks of isolation.
The best (and most simple) answer for building relationships and increasing engagement among remote employees is to leverage (and get creative with) what the company already has on hand: video conferencing tools. The aim of this blog is to provide some concrete ideas for how your team can best use video conferencing to improve employee engagement and connection.
A great place to start is to simply make video conferencing the consistent norm for your team. This won’t replace email or chat-based communication, but defaulting to more face-to-face interaction, albeit mediated, can break up the isolation and create opportunities for connection and casual conversation. While it might be difficult to enforce as a policy, perhaps try to make an unspoken rule of taking the first few minutes of a meeting to chit chat with your colleagues. As always, be respectful of time, but don’t take an opportunity to laugh and unwind for granted. It’s striking to see just how much of a pick-me-up a light-hearted conversation can be.
With all this being said, it’s important to remember that video meetings feel very different than in-person meetings and can have some unintentional effects. As such, it’s well worth your time to observe and gauge the effectiveness of your remote meetings and adjust accordingly.
When it comes down to it, setting aside a specific time to connect with other remote colleagues is one of the better things to do. Whereas in-office life is replete with opportunities to chat and recreate with co-workers, remote work easily creates an environment of “all hands on keyboard, all the time.” Intentionally blocking off time to set projects aside and socialize is a great way to boost this necessary interaction. Due to the direct impact on employee satisfaction and overall productivity, it would be wise for leadership at the highest levels to proactively support and encourage such e-gatherings.
Right now you’re probably saying “OK, but what should we actually do during those times?” The ultimate goal is to provide several opportunities each week for people to connect, not impose more demands on them. As such, prioritize low friction over perfect attendance.
Here are a few suggestions:
Virtual Social/Happy Hours
Storj Labs has recently rolled out a weekly “Storjlings Social Hour” on Friday afternoons. There is nothing structured or formulated during these meetings — people simply show up and enjoy the chat! There isn’t anything sacred about holding these meetings on Friday, so do what works best for your team. For example, you might find out that people would prefer to have a mid-week get-together to get them over the hump and finish the week strong. There’s also no reason to not hold multiple social hours throughout the week.
You’ll find that sometimes you and others might contribute significantly to the conversation, while at other times you keep on mute and just enjoy the banter. Regardless of the participation level, the underlying idea is to set projects aside for a bit and enjoy the respite. For many, cheerful voices and the opportunity to laugh are deeply therapeutic and rejuvenating. Though our Social Hours are actually only a half-hour long, the spike in energy they offer is incredible.
Ever-Present Water Cooler
One idea that’s worth trying is having a video conference meeting in the calendar that’s open all day. Instead of a dedicated time slot for people to gather, the “water cooler conference” provides an opportunity for folks to pop in at random times of the day when they need a moment to rest their brains. If you’re wanting to mimic the spontaneity of in-office chit-chat and respite, this is a great option.
While it might add an extra step, it could be worth creating a Slack (or another chat program) channel to alert when folks are at the water cooler. In a similar fashion, Storj has a channel called #weasley-clock to alert the team when someone is stepping away from their computer to grab lunch or run an errand. As such, a water cooler channel wouldn’t be a stretch since such open and transparent communication is central to our culture.
Game Time
The appropriateness of playing games at work is still a matter for debate, but a host of research and data shows games are great for relieving stress and building camaraderie. Any executive would agree managing stress loads and increasing cohesion among employees are great objectives for the company; not only are they good for employee satisfaction and morale, but they can also have a direct impact on productivity (read: the bottom line). As such, why not break down some of the traditional assumptions about work and make things a bit more jovial?
The internet provides ample opportunities to play games with people all over the world. Some obvious examples are the online multiplayer functionality through the leading game consoles in addition to popular PC-based services like Steam and Stadia. You can also play several classic board games online as well. While these are great options, they require everyone to own the same consoles, software, and games. As such, these methods won’t serve as universal options for everyone. Luckily, there’s another option.
It has become much easier (and popular) to play games through video conference tools. In fact, some video chat programs like Houseparty come with built-in games! Because these tools provide a face-to-face element, party games are often the most common. Jackbox, for instance, offers a selection of “party packs” that each contain a suite of fun and engaging games. There are also some great digital escape rooms like this one for Harry Potter fans, and the Random Trivia Generator has become a go-to for trivia lovers.
There are so many different options available for folks who want to connect with others through online games. As with anything, it comes with some trial and error and the occasional technical difficulty. While any video conference tool would work just fine, Zoom’s screen and audio sharing capabilities are a bit more optimized for video games. Also, don’t forget that you’re not confined to video or web-based games. Since everyone has a camera and mic, “in-person” party games like charades are possible as well. Some (brave) folks have even hosted karaoke and talent shows! The options are endless.
As stated before, the main goal here is to increase employee satisfaction and engagement by providing remote workers with opportunities to connect and build relationships with one another. However, it’s important to remember games and social hours aren’t enough on their own. Instead, they should be supplemental actions woven into a greater culture of engagement and inclusion. While it’s always good to start somewhere, we encourage businesses and leaders to focus primarily on building a firm foundation of transparent communication, employee empowerment, and purpose.
Storj Labs is excited about the future of work, specifically the increased adoption of remote teams. We’re also excited about tearing down walls and barriers at work that hinder people from building impactful and fulfilling relationships. We would love to hear what your team is doing to bring your teams closer together.
This blog is a part of a series on remote work. In this series, we share examples of how we address remote work at Storj Labs, where the majority of our team works remotely. To view the rest of the blogs in this series, visit: https://storj.io/blog/categories/remote-work.
By Ben Mikell on Remote Work
Originally published at https://storj.io on July 20, 2020
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
1 
1 
1 
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
"
https://medium.com/@yanwei-liu/googld-cloud-storage-introduction-920a367ca1d1?source=search_post---------141,"Sign in
There are currently no responses for this story.
Be the first to respond.
Yanwei Liu
Feb 24, 2020·2 min read
#20200321更新：在NVIDIA Jetson Nano學習筆記（五）：即時影像分類系統(PiCamera+OpenCV+TensorFlow Lite+Firebase) 一文當中，用到了Firebase Storage。我發現：Firebase Storage與Cloud Storage似乎是一樣的，也因此API指令幾乎共用，該文章中，也有關於Firebase的Docs連結，有興趣的讀者可以參考看看。
原文：
在Google Cloud使用筆記(六)：使用 Cloud AutoML Vision訓練影像辨識模型(Tensorflow Lite)中，曾提到過Cloud Storage，不過我們並沒有深入地去探討詳細內容。本文透過Google官方的互動式教學後，將心得轉換成文字來進行講解。
我個人將Cloud Storage定義成一個可以安全、可設定讀取權限的資料存放空間。
我們可以像是平常在下載檔案一樣，直接透過Cloud Storage的檔案連結，把資料下載回來。
值區bucket，有點像是一個專屬的資料夾，只存放我們指定的檔案
步驟
先在Dashboard最左方找尋Storage，直接點擊進去即可
主要分為以下四個步驟
其中，我們可以自行(或者批次)針對不同的檔案編輯讀取權限、進行下載，並與他人分享。
再搭配上Python或是其他API，就能進行更多不同的檔案操作。
官方文檔
Python API：https://googleapis.dev/python/storage/latest/client.html
Machine Learning | Deep Learning | https://linktr.ee/yanwei
Machine Learning | Deep Learning | https://linktr.ee/yanwei
"
https://medium.com/pcmag-access/samsung-wants-to-be-the-general-electric-of-the-iot-era-6bb905b864e2?source=search_post---------274,"There are currently no responses for this story.
Be the first to respond.
From gadget production to cloud storage to an Internet of Things (IoT) operating system, Samsung wants to own connected devices.
By Juan Martinez
When you think of Samsung, you probably picture smartphones, TVs, and maybe even washing machines and semiconductors. You wouldn’t associate the South Korean company with light switches, door locks, and delivery trucks. But that’s all about to change.
Samsung recently pledged to invest more than $1.2 billion in start-ups and research and development (R&D) for the Internet of Things (IoT) and connected devices. A clear business opportunity exists for technology manufacturers that can stake a claim in the IoT. Market research firm IDC expects the IoT market to be worth $1.7 trillion by 2020. Much of this revenue will be driven by consumers, with each of us expected to own 50 connected devices — our small personal claim in a world expected to produce more than 30 billion connected devices.
Practically speaking, this means sensors on delivery trucks will be able to collect real-time data to detect when the trucks might be at risk of failing. Hospitals will be able to install sensors on beds to correlate patient recovery times with the angle at which the bed is reclined. Sensors on utility poles will be able to create a network that can detect where the majority of utility pole failures will occur. In all of these instances, Samsung could, theoretically, deliver the sensors or the operating system (OS) on which the data is processed, the cloud in which the data is stored, or the security measures that keep the data from being pilfered by hackers.
But Samsung wants to own the action, not just participate in it. Like IoT competitors Intel and Qualcomm, Samsung has heavily invested in microchips and processors. As devices have gotten smaller, so have the chips. As the internet has become more connected, so have the devices that interact with one another. What Samsung is attempting with its massive investment is to create both the devices that power the IoT and the ecosystem in which those devices communicate.
But perhaps more interestingly, Samsung could also create the delivery trucks, the hospital beds, the utility poles, and any other product that lends itself to connectivity. Samsung has been coy about whether or not it intends to reach beyond gadget, chip, cloud, and OS development to create things such as light bulbs, light switches, and door locks. “There’s ample opportunity across the board,” said Curtis Sasaki, VP of Ecosystems at Samsung. “We don’t even have to decide.”
Sasaki did discuss the possibility of an exclusive Samsung deployment across a single, connected office building. “If you think of office buildings, you literally have thousands of light switches, plugs, and robust energy management controls,” he said. “A single deployment in a building is a lot of products and devices.”
Think of the business potential. To take advantage of this, Samsung has taken a four-point approach to owning the IoT. It has already begun building the OS, the storage, the hardware, and the security that will provide the foundation for a global IoT. “By 2020, 100 percent of Samsung products will be connected,” said Sasaki. “But we’re going further than just hardware, operating system, security, and cloud. We also want an ecosystem of partners.”
This means working with other technology companies, auto manufacturers, industrial designers, and even municipalities to help lay the foundation for the IoT. “The IoT space won’t be a single company dominating because the space is so diverse,” said Sasaki. “We have a lot of consumerization products but our semi-conductor products are also in Dell products. In both cases, we’re pretty excited.”
If in 2016 you think of Samsung as an Apple competitor, by 2020 it might be more appropriate to consider it in the same vein as General Electric. General Electric started as a company that produced electric lamps, but then it began to produce the energy to power the lamps. Next, it began investing in businesses that would use the energy it produced to run their products. It quickly became a leader in electric product development, creating things such as the X-ray machine, the electric locomotive, electric kitchen appliances, and even the toaster oven. The company has registered more than 67,500 patents.
Similarly, Samsung is slowly dipping its toe into the IoT waters one stage at a time. In 2014, it acquired SmartThings, a home automation company focused on connecting home appliances to smartphones and computers. In 2015, it unveiled Artik, its open-source end-to-end IoT platform. In April 2016, it teased its unnamed IoT OS. Last month, it acquired Joyent, a server and data rental company; a few weeks ago, it announced the $1.2 billion investment. In concert, all five actions clearly indicate Samsung’s mission to conquer IoT — from the devices we use to the system on which they run to the cloud on which the data they produce is stored.
But, even if Samsung plunges head-on in an attempt to conquer the IoT from every angle, the company knows it’s a real long shot. This is why it’s hedging a bit by focusing on partnerships. “From a Samsung perspective, it would be a shame on us if we can’t get all of our products to work together seamlessly,” said Sasaki. “But most people don’t have a 100-percent Samsung home. That’s where the interoperability [with other manufacturers’ devices] matters. Unless we do this in an open way, we may explode the entire adoption into smart homes. Better to get consumers educated, have products interoperate [with other manufacturers’ devices], and create a seamless experience between Samsung products.”
Samsung will divide the $1.2 billion it invests between internal projects and external start-ups with which it can partner to advance IoT development. But the company also has a cash pile of more than $60 billion it can use to research an incursion into gadgets, fixtures, automobiles, and anything else driving data into the IoT.
Read more: “Connecting Everything to the Internet: What Could Go Wrong?”
Originally published at www.pcmag.com.
PC Magazine: redefining technology news and reviews since…
3 
3 claps
3 
Written by

PC Magazine: redefining technology news and reviews since 1982.
Written by

PC Magazine: redefining technology news and reviews since 1982.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@timtech4u/deploying-a-gcp-virtual-machine-instance-with-a-startup-script-fe5431f16e66?source=search_post---------371,"Sign in
There are currently no responses for this story.
Be the first to respond.
Timothy
Feb 20, 2019·3 min read
This article guides us through deploying Virtual Machines (VM) Instances with a public accessible remote start-up script.
This would be helpful if you need to run a few commands on startup of your virtual machines. For example, update/install packages, perform a cron job etc...Also, you can always update the remote script contents whenever, without ever having to modify your instance’s configurations.
We’ll be hosting our startup script on Google Cloud Storage, feel free to use whatever you prefer, all we’ll need at the end is a publicly accessible URL to your startup script.
Login to Google Cloud Console , on the Navigation Menu, Click on Storage and then Browser. Let’s go ahead and create a bucket…
Enter a Unique Bucket Name and click on Create. Once done, we can now upload files to our bucket. Uploaded files are called Buckets.
I’ve successfully uploaded my startup script, however, I still need to make the script publicly accessible.
Click on the Permissions tab and Enable Object-level permissions.
We’ll now be able to Edit Permissions on our objects.
To make our Object public, Click on Edit Permissions and a New Item:User: EntityName: allUsersAccess: Reader
We’ll head over to Compute Engine > VM Instances and click on Create InstancesGo ahead and leave all default for the VM Instance, Click on the dropdown for Management, security, disks, networking, sole tenancy. In the Metadata section, we’ll add our startup-script-url
Remember to change the value of startup-script-url to your startup script URL.
Go ahead and Create your virtual machine. 🕺🕺🕺
Another option to add startup scripts to VM instances is to add the contents of your script to Automation > StartUp Script (pasting the contents of your script works too).
If you’d like a faster way of doing this, the GCloud command-line tool does this in just one line:
Thanks! Looking forward to writing more tips about Google Cloud Platform 😉😉
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
See all (664)
76 
1
76 claps
76 
1
Software / DevOps Engineer | Google Developer Expert for Cloud | https://timtech4u.dev
About
Write
Help
Legal
Get the Medium app
"
https://regarding365.com/6-tips-to-improve-cloud-storage-user-adoption-78eb9bad1f67?source=search_post---------149,"The way we organize files is intensely personal. Folders have given us a way to group files in almost countless ways. Folders are familiar. Their boundaries are easy to understand. Permissions applied to the folder and anything in the folder inherits those permissions. Sub-folders are easy to create. Folders are simple to copy or move. Someone new to computing can easily learn file management because of this containerized concept. It translates to real-world experience with containers we use every day to store and organize objects.
The trouble with moving to cloud-based storage is that each platform has its own spin on how they think file storage should operate. Their folders may use the same features, such as supporting sub-folders or permissions. But there are differences between the platforms, sometimes subtle and sometimes not so subtle…
We should not expect everyone in our organizations to be able to quickly learn and understand the new ways of working with cloud-based files. It will require a gradual introduction of concepts and clear value propositions for people to change their habits. We must also be mindful of the pace and pressures of work that will always reduce the rate at which new habits and concepts are learned and adopted.
Thankfully, we can meet our people half way. We can bridge the gap between working with files in Files Explorer and in the Cloud. Here are 6 essential skills to improve adoption of working with files in the Cloud.
Often when someone creates a new document, they either start from a blank template, a template created by your organization. Or they open a similar document and make a copy to alter and adjust. They can continue to work on that document for some time before they have the presence of mind to save it somewhere. When they do, it’s usually one of two places: Locally on their Desktop or Documents folder, or into their Home drive. This is not going to be an easy habit to change, which leads to the first two tips…
For the purposes of this blog post, I’m going to use Microsoft 365 cloud-based file storage in my examples.
The Home drive is usually mapped to Files Explorer. It can be given a familiar letter like “H” so people can easily associate it with “Home.” The first significant change is to move the contents of Home drives into OneDrive for Business and synchronize it to the owner’s desktop. Don’t underestimate how impactful this first change can be. You’re changing the icon of where they will find their files. It’s no longer a yellow ‘manilla folder’ icon. It doesn’t have the familiar “H” drive letter. It will appear in a different position on their folder view pane in Files Explorer.
But this is also the first and most important change to understand and adopt.
• To quickly reduce the anxiety, show people their familiar files and folder structure still in tact within OneDrive for Business. • Show them that the OneDrive icon operates in the same way their Home drive did. The folders can be expanded. New folders and sub-folders can be created. Files can be dragged around to shift them.
When people see their familiar files and folders, there’s an immediate understanding that they haven’t lost their files.
Files on Demand is the latest and best tool within Windows 10, for encouraging adoption of working with cloud-based files. When OneDrive for Business is synced with a person’s Desktop, it creates placeholder icons for each file and folder in the cloud. They immediately recognize their files and folders.
In a file migration project where Home drives are being moved to OneDrive for Business, the files have often been moved in batches on behalf of the end user. This means that when they begin to use their OneDrive for the first time, files are likely to still be based in the cloud.
• Show your people how to recognize when a file is still in the Cloud and that the simple act of opening the file, synchronizes it onto their device. • If there is a file or folder that they often reference, show them how to ‘always keep [it] on their device.’
• Lastly, show your people how to ‘free up space’ on their device and make the file a placeholder, so they can retrieve it later from the Cloud when it’s needed again.
Practicing these three skills will begin to build up an understanding of how to work with cloud-based files. By leveraging the on-demand syncing of OneDrive, an individual learns they are in control of when they retrieve a file from the Cloud, what is stored on their device and for how long.
Some of our people have learned what they know about file management on a computer through the features and guidance built into Windows. They sign into their computers and save all their documents to the ‘Documents’ folder that opens by default when saving a new document. When we are short on time, even those who understand that files should be stored on the network, save their files to their Desktop folder or Documents folder for convenience. Smart IT departments have redirected these common folders to a network share to ensure they are backed up and accessible from different devices.
OneDrive for Business introduced ‘Known Folder Move’. When active, files saved to local folders like ‘Documents’, ‘Photos’ and ‘Desktop’ will be synchronized to the Cloud. A local synchronized copy will still be available, even without an internet connection. This supports our people who are still learning to deliberately save files into the OneDrive folder in Files Explorer. They haven’t formed the habit yet. But even when they continue to save files to their ‘Desktop’ or ‘Documents’ folder, the files will be synchronized to the Cloud for them.
Make the time to speak with these people and show them that they are working with their documents in the Cloud, without any conscious effort. But then show them the real value they can take advantage of.
• Ask if you can set up the OneDrive app and a couple of the Office apps on their mobile. Connect OneDrive to their work account and show them the files that are synced to the Cloud. • Open the most recent Word document that they saved to the ‘Desktop’ or ‘Documents’ folder. Show them that they can read and review that same document from their mobile.
• Open Office.com in a web browser and highlight the ‘Recent’ documents list on the landing page. Open that same Word document and make some changes from the web browser.
Seeing the files saved to common places are available in a web browser and from a mobile, really hits home the value of syncing files to the Cloud. You’re not asking your people to change their file saving habits right away. But you can show them where to find them in the Cloud, if they ever had the need to read, edit or share them.
The number one file sharing platform on the planet is email. It has maintained that title for years because it’s so easy to attach a file and send it to the people you address in the email. When you need to find that file later and the conversation associated with it, email search has become very powerful. Type in a few keywords, a person you think you sent the file to and an approximate date and you will usually find the file you’re looking for.
But mailboxes were never designed to be file storage and retrieval facilities. What keeps us locked into this habit is the value of the conversation attached to the file.
However, we soon experience trouble when we need to maintain version control and keep track of collaborator’s responses. Some of our organizations still run a strict limit on the size of our mailboxes and limit the size of the files we send.
Your next drawcard for using cloud-based file storage is the simple file sharing. Our people can feel they are sending the file, when it’s really a link and invitation to view or edit the file.
Outlook and OneDrive have done a superb job in this area, by guiding our people towards cloud-collaborative behavior. When they click to attach a file, the recent files are offered to attach. If the file is one that has been synchronized with OneDrive, Outlook will offer to ‘Share [a] link’ or ‘Attach as [a] copy.’ Sharing a link makes the file still appear to be attached. However, it sets the permissions to the sharing default chosen by your organization (such as, ‘Anyone can edit’) and attaches a link for the recipient to open.
IT savvy people know the value in this. But make sure you point it out to those who are on this learning path to working with files in the Cloud.
• Show them how to attach links to files rather than attaching the full file. • Encourage them to experience co-authoring by opening the document with the recipient while on a phone call. Review the document together and make changes while discussing the content of the document.
As our people grow in confidence working with cloud-based files, introduce them to synchronizing their project folders from their team sites.
• Open the document library of a project they are currently working on and sync it. • Show the same placeholders and Files on Demand behavior can be used with collaborative file repositories.
Even when our organization makes a choice to standardize on a single productivity platform like Microsoft 365, we have external contacts that use other platforms. There are a lot to choose from. We shouldn’t expect our people to learn to use them all. Use a file syncing service like SkySync to connect and sync all the common cloud file platforms. This reduces the learning for our people who struggle with change and multiple ways to share and collaborate on files. It means that ultimately, they can continue to use OneDrive for Business and sync the files shared by their external contacts.
The tips in this post are just a starting point for improving adoption of cloud-based file productivity and collaboration. The main thing to keep in mind is that you build progressively on the skills, starting with what is familiar. Communicate the value of each feature using real-world scenarios that solve meaningful problems for our people. They will soon become experts.
Originally posted at Syncopathlabs, Oct 29, 2018.
Thoughts, opinions, discoveries and tips regarding…
Written by
Embedding #MeaningfulActiveUsage | http://youtube.com/ModernWorkplaceChange | http://messagecenter.show | @REgarding365
Thoughts, opinions, discoveries and tips regarding Microsoft 365, from enthusiasts who make it their business to share them.
Written by
Embedding #MeaningfulActiveUsage | http://youtube.com/ModernWorkplaceChange | http://messagecenter.show | @REgarding365
Thoughts, opinions, discoveries and tips regarding Microsoft 365, from enthusiasts who make it their business to share them.
"
https://medium.com/digital-leaders-uk/the-transition-from-pen-and-paper-to-technology-and-cloud-storage-saves-time-and-money-f957ef46fa4?source=search_post---------175,"There are currently no responses for this story.
Be the first to respond.
Written by Scott Hollinshead, Operations and Governance Manager at Street Soccer Scotland
Street Soccer Scotland is a non-profit organisation that changes the lives of young people and adults from a range of socially disadvantaged backgrounds. We deliver our services in Aberdeen, Dundee, Edinburgh and Glasgow. We aim to provide hope and inspiration to our players. Our inclusive programmes and sessions bring players together, without labels or stigma and offer a chance to grow, meet new people and access support and mentorship when needed.
Since 2009, where Street Soccer Scotland was first founded, the processes in place were very dated, especially around collecting and storing information. However, progressively, we have transitioned from pen and paper, isolated email accounts and paper records to where we are today; with a CRM system in place, mobile form technology (Prontoforms) and online cloud storage. We also have a collaborative and secure email service which is now backed up and can be used on numerous formats.
In 2017 alone, we transitioned all disclaimers, sessional registers, cancellation forms, incident reports to digital formats with all disclaimer records automatically syncing with our CRM system (Salesforce) to give coaches more time for building relationships and face to face interactions with the players. We decided to improve productivity and free up time in the team’s very busy diaries, so we moved to several digital systems that automate certain pieces of work to reduce work pressures and to also maximise time available to work with players.
With the increasing demand for reporting and growing the presence of Street Soccer Scotland through our networks, it was essential that we moved with the digital age, to cope with the demands of the organisation, but also to ensure that we keep pushing the standards of our services to all players, supporters and partners. The plan was to eradicate stress from the process, increase time spent with the players to work to their needs and collectively increase productivity across the whole team.
We were at our limits responding to requests on the old system, so motivation to change and improve was not only significant but vital.
Firstly, we needed to address what the main issues were for the whole organisation to see if there were any common themes across all the regions. This was completed using the ‘Measuring Up’ tool that was part of the Fit for Impact Programme delivered by Sported. The results from this report outlined the ‘headline’ tasks that needed addressing in the first instance but also gained endorsement from the team.
Liaising with different organisations and companies that deliver similar work to Street Soccer Scotland, gave a good understanding of what can be achieved in an ideal world to demands and learning curves that other sectors have experienced. This added knowledge and direction was then applied and tailored to the organisation’s structure and working environment.
We have seen a significant change not only with time, but also with attitude when completing ‘paper work’. We have estimated that to complete a sessional register on paper would have taken a coach 43 hours of their time per year. Currently, we have been able to save 30 hours of admin time (per coach per year) on just one document utilising the Prontoforms system. We have received excellent feedback from the whole of the coaching team, as the new system has eradicated any technical support that was needed with excel spreadsheets, whilst providing an increased output of data, required for review, reports and internal use.
We have since moved away from paper and pen for the following reports: incident reports, cancellation forms, disclaimers, absence leave requests, event tracker and training logs for staff and volunteers. We have also added additional tracking information on volunteer names, hours and coach feedback for every session that is delivered.
Utilising Salesforce, we have been able to automate the information from online disclaimers direct to Salesforce which again removes admin time and effort from the coaches when logging a new Street Soccer Scotland player.
The ability to report in real time, create dashboards and receive in depth analysis on each piece of data that we record now gives us very accurate and timely reports with data that we can pro-actively work with.
It is always difficult to change the ‘normal’ process of day-to-day working habits, as with implementing any change within an organisation. We have varying levels of computer literacy within the organisation, so certain changes were resisted. To try and reduce any stress or problems with the digital change we have always tried to explain why the changes are needed but also to give support with training to increase their knowledge and understanding.
This article was originally published here and was reposted with permission.
Originally published at digileaders.com on February 5, 2018.
Thoughts on leadership, strategy and digital transformation…
Digital Leaders is a global initiative that has created a shared professional space for senior leadership from different sectors promoting effective, long-term digital transformation.  Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Written by
Informing and inspiring innovative digital transformation digileaders.com
Thoughts on leadership, strategy and digital transformation across all sectors. Articles first published on the Digital Leaders blog at digileaders.com
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/minimizing-business-downtime-with-alibaba-cloud-hybrid-disaster-recovery-solution-ccf8b3fb190e?source=search_post---------307,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 27, 2018·11 min read
By Alibaba Cloud Storage Team
At The Computing Conference 2018 in Hangzhou, Alibaba Cloud presented its Hybrid Disaster Recovery (HDR) and Hybrid Backup Recovery (HBR) solution by demonstrating a full disaster recovery process on stage. The five-minute HDR demonstration presented the end-to-end process of enterprise application disaster recovery with Alibaba Cloud’s second-level recovery point objective (RPO) and minute-level recovery time objective (RTO), covering the core steps of a typical cloud disaster recovery scenario.
We all know that downtime of key business brings great losses to enterprises. Traditional self-built disaster recovery solutions are costly and require complex operation and maintenance (O&M), so high-performance cloud disaster recovery services are becoming the preferential choice for enterprises to ensure business continuity. The HDR solution for key business fully demonstrates the real-time disaster recovery replication of an accounting system running on a local server to Alibaba Cloud, and quickly recovers the business on Alibaba Cloud after the downtime.
The entire demonstration was divided into three phases:
In the digital economy era, data is growing exponentially. In just a few years, the amount of data has jumped from the TB level to the PB or even ZB level.
According to surveys conducted by research institutions, the total amount of data exceeded 15.2 ZB in 2017, with a year-on-year increase of 35.7%. The total amount of global data will reach 19.4 ZB by 2018. In the next few years, the global data will grow at a rate of more than 25% per year. The global data amount is expected to hit 50 ZB by 2020.
Undeniably, data is the core of digital operations and data security determines the survival of enterprises.
In August 2018, an international cloud service provider reported data leakage because its sales staff did not follow the specifications for a bucket.
In July 2018, a serious failure was exposed on a Chinese cloud platform. The failure directly led to the loss of all data of a startup company, which then faced an unprecedented shutdown crisis.
On May 12, 2017, the WannaCry worm incident caused chaos across the world. ATMs in banks were out of order, computers in gas stations broke down, and students’ theses were encrypted.
In January 2017, the O&M personnel of a code hosting platform mistook the production environment for a test environment during multi-terminal switching and accidentally deleted the database of the production environment.
In November 2014, a financial payment company experienced a system failure, causing nearly 400 million duplicated receipts.
According to IDC statistics, 55% of the companies that experienced disasters in the past decade collapsed right away. In the remaining 45%, 29% also went bankrupt within two years due to data loss. Only 16% survived.
According to a Gartner report, two-fifths of the companies that experienced system downtime due to major disasters never resumed operations again, and one-third of the remaining companies went bankrupt within two years.
In this context, enterprises urgently need to strengthen data protection.
The hybrid cloud backup and disaster recovery solution is the first choice for enterprises’ digital transformation. In the past, the traditional backup and disaster recovery solution was based on a disaster recovery center built with an architectural system similar to a production center. Although this solution meets the data backup and replication needs of the production center, it poses many challenges to enterprises because of its inconvenient and long implementation period, expensive devices, and complex O&M.
Compared to the traditional backup and disaster recovery solution, the hybrid cloud backup and disaster recovery solution features high efficiency, high availability, high cost effectiveness, and no O&M. This modern solution helps customers securely and efficiently back up files, databases, virtual machines (VMs), and even the whole system, locally or on the cloud. In addition, the application server system backed up on the cloud can work as a virtual server, which delivers the required RPO and RTO to ensure business continuity and disaster recovery on the cloud.
Hybrid Backup Recovery (HBR) is an easy-to-use and cost-effective online backup service. This service helps customers back up data from desktops, servers, or VMs to a backup repository on Alibaba Cloud, which ensures secure and efficient cloud storage, backup, and management of customer data.
A customer requires data protection in different IT environments, including physical machines and virtual platforms in a local Internet data center (IDC), ECS servers on Alibaba Cloud, and servers on other public cloud platforms.
The RPO and RTO are not demanding, but need to ensure data security and recoverability.
In this scenario, HBR has the following advantages:
A customer has established multiple branches in different cities, provinces, and regions. Each branch has data to be backed up.
Backup devices need to be deployed in each physical IDC, resulting in scattered and complex management of devices. It is also difficult to ensure successful data backup.
Based on the multi-region disaster recovery requirements, the customer hopes that a copy of the backup data can be stored in another region to prevent data or services from becoming unavailable due to regional failures.
In this scenario, HBR has the following advantages:
Different from HBR, HDR provides “Cloud + Local” dual backup and cloud disaster recovery for enterprise applications. HDR protects servers, files, and applications. The disaster recovery server is deployed in the local IDC to quickly recover local data. At the same time, the backup data is synchronized to the cloud disaster recovery repository for disaster recovery on the cloud. If the IDC has a failure, HDR is able to recover the business server on the cloud to guarantee business continuity. HDR can also be used for disaster recovery drills or data analysis. To protect data in a big data cluster architecture, Alibaba Cloud has released the first big data backup and disaster recovery solution on the public cloud.
A customer needs to replicate the production environment in real time to ensure that the business can be quickly taken over on the cloud if a business failure occurs in the local IDC.
The business system is built based on a complete set of servers, including database servers, file servers, and application servers.
In this scenario, HDR has the following advantages:
A customer needs to replicate the production environment locally to ensure quick recovery of data in the local IDC if the data is deleted by mistake or a disk failure occurs.
The customer also requires remote disaster recovery for the local backup data to ensure that business can be quickly restored in the disaster recovery center if disasters occur in the local IDC, shortening the business interruption time.
The business system is built based on a complete set of servers, including database servers, file servers, and application servers.
In this scenario, HDR has the following advantages:
A customer has deployed a Hadoop big data cluster, which contains hundreds of TB of data, in the local IDC. If a remote disaster recovery cluster of the same size is built, a lot of resources are idle and the costs are high.
The customer’s required RPO is close to zero, which cannot be satisfied by the traditional DistCp solution.
In this scenario, HDR has the following advantages:
HBR and HDR support multi-level RPO and RTO to meet different business disaster recovery requirements.
HBR and HDR ensure real-time replication of the core business, regular disaster recovery of key business, and regular backup of common business.
No hardware or gateway device is required.
The backup space is ready for use upon purchase and easy to expand.
The deduplication ratio of up to 30:1 allows 30 copies to occupy the original space of only one copy.
The permanent incremental technology prevents redundant data from being uploaded repeatedly and delivers an extremely fast speed for access to the cloud without private lines.
The amount of data is calculated and charged based on the cloud storage space instead of the source backup amount, minimizing backup costs.
The infrastructure of public cloud platforms reduces more than 70% of costs compared to the traditional disaster recovery center.
The data is backed up in multiple versions. A full copy is stored at each time point.
The cloud storage reliability is 99.9999999999%.
The multi-availability zone (AZ) backup repository for disaster recovery relies on the support of multiple zones on the cloud platform to protect data in multiple copies.
The configurable cross-region remote disaster recovery mode prevents data loss caused by major regional failures.
The whole system is backed up and recovered. Customers can recover the same business system on the cloud without the need to change applications or IP addresses.
When the whole system is on the cloud, the data of its offline IDC can reflow, which helps to restore applications to the original status after the offline IDC is recovered.
OS: Windows and Linux
System platform: VMware, Hyper-V, and physical servers
Database: SQL Server, Oracle, and other databases
Reference:https://www.alibabacloud.com/blog/minimizing-business-downtime-with-alibaba-cloud-hybrid-disaster-recovery-solution_594200?spm=a2c41.12325799.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@nutanix/dynamically-increase-aws-ebs-capacity-on-the-go-now-with-new-elastic-volumes-68b9caa5b148?source=search_post---------382,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Feb 17, 2017·4 min read
Say goodbye to scheduling downtime while modifying Elastic Block Storage (EBS) volumes. No more bottlenecks. Modify these EBS volumes on-the-go. Here’s why: AWS announces new feature to its EBS portfolio, called Elastic Volumes, which will help you automate changes to your EBS workloads without going offline or impacting your operations. Plus, grow your volume, change your IOPS, or change your volume types too, as your requirements evolve. All without the need for scheduling downtime. And with today’s 24×7 operating models, it is more important than ever to have no room for that downtime.
EBS workloads are known to optimize capacity, performance, or cost by allowing you to increase volume size, adjust performance, and change volume type as and when the need arises. Primarily, due to its dynamic nature and the ability to offer persistence high-performance block storage for AWS EC2.
Prior to the launch of Elastic Volumes, you had to schedule a downtime to that end, perform several steps like create a snapshot, restore it to a new volume, and attach this snapshot to a EC2 instance as and when your data volume grows.
Now, with the launch of Elastic Volumes, AWS has simplified the process of modifying EBS volumes drastically. You can also use CloudWatch or CloudFormation, along with AWS Lambda, to automate EBS volume modifications, without any down time.
AWS, in one of its blogs, says that Elastic Volumes reduce the amount of work and planning needed when managing space for EC2 instances. Instead of a traditional provisioning cycle that can take weeks or months, you can make changes to your storage infrastructure instantaneously, with a simple API call.
Essentially with AWS Elastic Volumes, as per AWS, you can:
It’s very simple to configure:
Image Source: Amazon Web Services
Image Source: Amazon Web Services
Image Source: Amazon Web Services
While the new feature helps increase capacity, tune performance, and change volume types on-the-fly, without disruption, and with single-click, it comes with certain restrictions:
With the launch of Elastic Volumes, AWS EBS is now more elastic. The best part, you can change an EBS volume’s size or performance characteristics when it’s still attached to and in use by an EC2 instance.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
21 
21 
21 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/@imgix/imgix-google-cloud-storage-more-source-options-8318c3766a28?source=search_post---------124,"Sign in
There are currently no responses for this story.
Be the first to respond.
imgix
Oct 18, 2017·2 min read
One of imgix’s core values is giving our customers flexibility in their image storage. We believe you shouldn’t have to move your images onto another storage system to take advantage of our imaging infrastructure, which is why we offer multiple ways to connect to them where they already live.
Our existing Amazon S3, Web Folder, and Web Proxy solutions cover a wide swath of our customers’ needs, and we’ve now added support for the Google Cloud Storage™ service as well, a much-requested option.
Setting up a Source with Google Cloud Storage works similarly to Amazon S3; simply generate your access key and secret key on the Google side, then plug in those values, the bucket name, and an optional prefix to set up the Source and start serving images. And because you’re providing your credentials, you won’t need to make the images public.
For more detailed setup instructions, see the Google Cloud Storage Setup Guide or watch the video below.
Real-time image processing and delivery for developers.
See all (129)
1 
1 clap
1 
Real-time image processing and delivery for developers.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@vrypan/deepfreeze-io-cloud-storage-for-the-rest-of-your-stuff-730eef12861d?source=search_post---------150,"Sign in
There are currently no responses for this story.
Be the first to respond.
Panayotis Vryonis
Jul 21, 2014·2 min read
If you use Dropbox, Box, Google Drive or OneDrive to store and sync your stuff between your computers, your smartphone and your tablet, you know: There are files you don’t want to be synced between all your devices, and you don’t want to pay a premium to store in the cloud. These are the backup copies, the files from old projects or past jobs, the stuff you rarely use.
Deepfreeze.io is cloud storage for these rarely used files, “the rest” of your stuff. It’s reliable (based on Amazon S3/Glacier), integrated with your primary cloud storage and extremely cheap.
Deepfreeze.io introduces the concept of cold storage to the broader consumer audience, by removing the technical and pricing complexities usually associated with it: The annual cost is $15 per 100GB, and Beta testers get 100GB for one year free. The only limitation of cold storage: Your files are not immediately available, but require up to four hours to “defrost” —a limitation usually worth dealing with for files rarely if ever accessed.
We don’t want only to be defined by low pricing: Delivering high value is equally important to us. It’s easy to offer ridiculous amounts of storage quotas by introducing limitations that make it unlikely for most users to fill it up. In contrast, we’ve added features that will help our users take advantage of the space they buy.
For example, Dropbox users can easily move gigabytes of files to and from Deepfreeze.io without being limited by the speed of their Internet connection. Transfers take place on the server side, where speeds are much faster. We are very happy to see users “freeze” tens of gigabytes in just a couple of hours after they sign up, because this means they get value from what we offer them.
There is a good number of great and mature cloud storage services and we are not trying to replace them. We want to be the “secondary” cloud storage, where users will save the files they may not need for months or years: old projects, old presentations, archival material, research data, backups.
Our Mac application allows up to 100GB single file uploads, and archives can be downloaded as a .zip file from a browser or be restored directly to Dropbox. We are already testing our Windows application and we hope to be able to release it in the next couple of days. Box and Google Drive integrations are planed for later this year.
Deepfreeze.io is currently available by invitation only. Users that get an invitation while we are in Beta, will also get 100GB free space for one year.
For more information visit https://www.deepfreeze.io/
Dad, geek, entrepreneur. My home is www.vrypan.net, my blog is http://blog.vrypan.net/
See all (998)
2 
2 claps
2 
Dad, geek, entrepreneur. My home is www.vrypan.net, my blog is http://blog.vrypan.net/
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/crustnetwork/crust-and-3commas-announce-partnership-795ae3aa6d7e?source=search_post---------291,"There are currently no responses for this story.
Be the first to respond.
We are pleased to announce that Crust Network and 3Commas will collaborate together to offer decentralized cloud storage solutions to 3Commas users worldwide.
3Commas will offer decentralized cloud storage solutions to users to back up their important information, including trading data.
Crust Network is excited about the cooperation and looks forward to exploring new and exciting ways that cloud storage solutions can be deployed in the trading industry.
3Commas Technologies OU, O/A 3commas.io, was founded in Tallinn, Estonia in 2017 by Yuriy Sorokin, Egor Razumovskii and Mike Goryunov to address major pain points in trading crypto across multiple exchanges. Since its inception, 3Commas has become the leading non-custodial trade automation platform in crypto and now services over 100,000 users on all major exchanges with aggregate monthly trading volumes in excess of $2B. Whether you are a novice or experienced trader, 3Commas offers a complete suite of manual or fully automated bots and trading strategies as well as risk and portfolio management solutions, all underpinned by a vast, low-latency cloud infrastructure.
Twitter: https://twitter.com/3commas_io
Telegram: https://t.me/commas
Website: http://3commas.io/
Crust implements the incentive layer protocol for decentralized storage. It is adaptable to multiple storage layer protocols such as IPFS, and provides support for the application layer. Crust’s architecture also has the capability of supporting a decentralized computing layer and building a decentralized cloud ecosystem.
At present, public testnet Maxwell CC2 is live, and welcomes everyone to join for testing. Crust Network successively joined Substrate Builders Program and Web3.0 Bootcamp, and also obtained a Web3 Foundation Grant.
Twitter: @CrustNetwork
Telegram: https://t.me/CrustNetwork
Website: https://crust.network/
Decentralized Cloud Blockchain Technology
36 claps
36 
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
"
https://medium.com/google-cloud/google-cloud-storage-large-object-upload-speeds-7339751eaa24?source=search_post---------30,"There are currently no responses for this story.
Be the first to respond.
Top highlight
Medic7 was seeing a problem uploading their large video files. Their clients would make a video of some genetic test, and then upload it, and it was taking forever. They needed help.
To get a sense of their problem, I uploaded a bunch of 100MB 200MB and 500MB files to see the performance. You can see in the graph below, that the current upload performance seems to cap out at about 200M object size.
So if we’re uploading a bunch of 4 gig files (say, video editing) we need a better plan.
The answer to this problem smacks you in the face while using gsutil. Any time you try to upload a “large file” you’ll see the following message.
Breaking this down, gsutil can automatically use object composition to perform uploads in parallel for large, local files being uploaded to Google Cloud Storage. This process works by splitting a large file will into component pieces that are uploaded in parallel and then composed in the cloud (and the temporary components finally deleted).
You can enable this by setting the `parallel_composite_upload_threshold` option on gsutil (or, updating your .boto file, like the console output suggests)
gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp ./localbigfile gs://your-bucket
Where `localbigfile` is is a file larger than 150 MiB. This will divide up your data into chunks ~150MiB and upload them in parallel, increasing upload performance. (Note, there’s some restrictions on the # of chunks that can be used. Refer to the docs for more information)
Here’s a graph showing 100 instances of uploading a 300MB file regular, and with composite.
Using parallel composite uploads presents a tradeoff between upload performance and download configuration: If you enable parallel composite uploads your uploads will run faster, but if you’d like to fetch the object using gsutil (or other python apps), then the client will need to install a compiled crcmod (see gsutil help crcmod) in order to download the file properly.
To be clear, this restriction for crcmod is temporary, and mostly there to protect the integrity of the data and ensure you your client doesn’t end up freaking out that things might look different. (CRC values and HTTP ETAG headers might show some difference.)
However, if this doesn’t work for your setup, you’ve got three options:
1) Maybe turn it off? Modify the `check_hashes` option of your config files to disable this step. NOTE: It is strongly recommended that you not disable integrity checks. Doing so could allow data corruption to go undetected during uploading/downloading.
2) Don’t use gsutil? To be clear, this isn’t an endorsed recommendation. However if you download the composite object using cURL, wget or http, then the fetch will work (you get the composited object). However, it’s strongly advised to still do crc checking, it’s just your responsibility to do it now.
3) Machine-in-the-middle? Another way to reduce this problem is to use a cloud-side instance to download the file (since crcmod can be installed there), and then re-upload it to a bucket in it’s entirety. To be clear, this takes time, and is more expensive (in terms of transaction costs), however this completely removes the crcmod restriction, and it might be a net-win, time wise, since GCP can easily get ~16 Gbits / sec upload speed from an internal VM.
For Medic7, putting CRCmod on each of their internal clients was not an issue, since uploaded videos had to be fast, and were then processed internally before being moved to another GCS bucket for distribution, so the machine-in-the-middle approach was almost de facto for them. The use of composite objects resulted in a 50% performance improvement for their clients, which is pretty great!
Google Cloud community articles and blogs
169 
2
169 claps
169 
2
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/how-alibaba-clouds-storage-technology-is-supporting-online-education-20e964b756ad?source=search_post---------166,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Apr 9, 2020·3 min read
In order to win this inevitable battle and fight against COVID-19, we must work together and share our experiences around the world. Join us in the fight against the outbreak through the Global MediXchange for Combating COVID-19 (GMCC) program. Apply now at https://covid-19.alibabacloud.com/
By Alibaba Cloud Storage team
At the beginning of 2020, the outbreak of the coronavirus disease (COVID-19) tremendously impacted the economy and people’s lives. After the outbreak of the epidemic, educational institutions issued notices to extend the holiday and postpone school opening. On February 5, the National Development and Reform Commission (NDRC), Ministry of Human Resources and Social Security (MOHRSS), Ministry of Industry and Information Technology (MIIT), and All-China Federation of Trade Unions (ACFTU) jointly issued the Notice for Promotion of Online Training Courses During the Epidemic. To fully respond to the demand to “keep learning while classes are suspended”, many online educational institutions and emerging educational platforms introduced the “online distance learning” model and offered free courses. Online education has become an important driving force for fighting the epidemic and restoring the economy.
Alibaba Cloud, Asia’s leading cloud computing company, and one of the world’s top 3 cloud computing companies, is committed to leveraging its technical strengths to help global educational institutions move to cloud education.
In addition to offering discounts for its products, Alibaba Cloud provides customized scenario-based solutions for online educational institutions. Alibaba Cloud offers comprehensive solutions that include the Object Storage Service (OSS) based full-process ecosystem, automatic management based on the data lifecycle, and a reliable and stable system architecture to help online educational institutions with scenarios such as courseware sharing and storage, course recording and playback, and intelligent course quality monitoring. Both the online education and video conferencing scenarios have demanding network requirements. The OSS-based global acceleration network built by Alibaba Cloud improves the efficiency of cross-province and cross-ocean data access. It also supports upload and download acceleration and non-static content acceleration, providing enhanced network elasticity.
Under the objective influence of the epidemic and new standards and requirements of the state-issued “Education Informatization 2.0” plan, many educational institutions now have new informatization needs. Based on the infrastructural feature of cloud computing, Alibaba Cloud can help educational institutions avoid redundant investment in businesses, improve resource utilization, and reduce development and O&M costs. The unique self-service feature is what makes cloud an infrastructure. Enterprises and organizations, such as educational institutions, can use remote video and file sharing to implement remote work. Meanwhile, they can use cloud computing functions as needed, such as real-time communication monitoring, data analysis and summarization, and elastic computing.
By cooperating and pulling all sectors of society to work together, we have the determination, confidence, and ability to win this war to prevent and control the epidemic. Here, we would like to pay tribute to the frontline medical workers and to every warrior fighting the epidemic. Wind might extinguish candles, but it makes fires burn stronger and brighter. Let us each be a fire that burns and dances in the wind.
While continuing to wage war against the worldwide outbreak, Alibaba Cloud will play its part and will do all it can to help others in their battles with the coronavirus. Learn how we can support your business continuity at https://www.alibabacloud.com/campaign/fight-coronavirus-covid-19
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@openbom/heads-up-how-to-put-data-at-the-center-using-openbom-and-autodesk-forge-cloud-storage-f4019332e3e4?source=search_post---------157,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Jan 6, 2018·3 min read
In a world of diversified organizations the question of collaboration in the same value chain or outside is critical. Modern manufacturing companies need to collaborate more and more. Companies need to share both content and files. Currently information lives in databases and multiple files. Some information is specific to CAD systems and some information is CAD neutral.
File exchange seems to be a simplest approach, but it is very fast becoming gridlock. People are lost in multiple files, emails and spreadsheets. What can be a winning approach to collaborate in the future? One option is to develop special data collaboration platforms capable of serving multiple companies and individuals. This is why we founded OpenBOM, to provide a new digital collaboration platform for engineers, contractors, manufacturing companies, and suppliers, to work together “on the same BOM” (or data, to be frank).
However, what about files? In a world full of cloud file storages, there are many companies capable of providing services to store files in the cloud. One size doesn’t fit all. You can check my earlier blog on Beyond PLM — Where engineers will store CAD files in 10 years? With the high diversity of cloud storages available, the OpenBOM strategy is to provide seamless integration with different cloud storage providers, and also highlight what the value is to use one of them. Check our earlier blogs about integration with Google Drive, Dropbox, OneDrive and Box.com.
Today I want to give you a heads up to another type of cloud storage provided by Autodesk: Autodesk A360 (Autodesk Forge platform). The value of the Autodesk platform is to serve as a storage and collaboration platform for 3D Content. According to Autodesk SVP Amy Bunszel and CEO Andrew Anagnost, “Data is in the center”. Check out my blog from AU2017 last month, here.
The OpenBOM cloud CAD plug-ins are now connected to the Autodesk Forge cloud seamlessly by uploading CAD and neutral geometry files to Autodesk cloud and connecting them contextually to online bill of materials and related data in OpenBOM.
Watch the short video below, it provides an idea of how powerful this feature is. Any CAD system (and we practically support all CAD systems, some of them out of the box and some of them as service offering) can use OpenBOM to create collaborative BOM and upload CAD data to Autodesk Forge platform for viewing and 3D collaboration.
Conclusion. Put data in the center. By combining OpenBOM and Autodesk Forge you can leverage both aspects of collaboration — data and 3D. Let me know what you think? Are you using Autodesk cloud today? I’d love to talk to you and learn more.
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Want to learn more about PLM? Check out my Beyond PLM blog and PLM Book website.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
See all (479)
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/tutorial-on-how-to-use-clamav-to-scan-files-uploaded-to-google-cloud-storage-gcs-b37777360210?source=search_post---------75,"There are currently no responses for this story.
Be the first to respond.
Tutorial on how to use ClamAV to scan files uploaded to Google Cloud Storage (GCS).
GCS does not have any built in capability to scan or do any other type of preprocessing on its files and relies on other services to perform these steps. In this tutorial, we will process a file that gets uploaded to GCS for viruses, malware, etc using ClamAV.
The basic flow outlined here is:
(ClamAV periodically requests updated virus definitions using Cloud NAT for external communication.)
github.com
note: we are creating a custom subnet 192.168.1.0/24 used exclusively for the ClamAV cluster. You can use any network but this setup sets up a custom range that can be isolated easily.
Setup NAT on the custom subnet where each ClamAV instance can request updated virus definitions.
Note the IP range 10.132.0.0/28: this is the range that traffic from GCF will appear to originate from to the custom GCE network we just setup.
These firewall rules are important
A couple notes about the member VMs in this instance group. These VMs are small (g1), have NO public IP addresses and have NO GCP service accounts. We do this to limit its privileges. Also note that in the firewall rules for the custom subnet, we did not even define ssh access (port 22).
In my case it is
Export the IP as a variable:
Now that we have defined all the components, upload a sample test file and a file with a known test virus that ClamAV can detect
Once uploaded, you should see the the files getting processed: OK means nothing found..and quite vaguely FOUND means a virus is found in that file.
This sample uses a golang library shim for clamv. For attribution and license, please see https://github.com/dutchcoders/go-clamd.
Google Cloud community articles and blogs
21 
1
21 claps
21 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@davidbyttow/google-cloud-storage-vs-amazon-s3-f05ee3435eb0?source=search_post---------64,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Byttow
Aug 30, 2013·3 min read
TL;DR: Google Cloud Storage didn’t work.
Here’s the problem: I have a web service and I want to serve “important” static content as quickly as possible. That means Javascript and CSS.
The solution is quite simple and well-known:
So far so good. The only problem remaining to be solved is where to host the files. It’s best to use some form of edge cache (location closest to your users as possible). This day and age, we can stand on the shoulders of giants and develop on both Google and Amazon’s infrastructure as a service. Notably, Google Cloud Platform and Amazon Web Services.
My goal was to integrate the upload of my JS and CSS binaries to the storage system as part of my release script (which is in Gradle btw).
Given I knew exactly what I needed to do, I decided this would be a great opportunity to try both and document my experience.
First, I worked at Google for five years. I know many of the individuals that work on this product. They’re all smart and awesome. This is in no way a reflection of them but instead what I believe is a lack of leadership in place to tie all of the new and legacy components together into a cohesive product.
I didn’t really care about pricing as they’re both comparable and I’m serving small files so storage and network costs are negligible.
But oh dear god, the flow for setting up Google Cloud Storage was so utterly and completely broken. Even though my application is on AppEngine, the two simply do not play nice together. I couldn’t use the app ID from my AppEngine application as my project ID for GCS and so needed to setup an entirely different account with its own billing. Of course, that didn’t work. Once I signed up for billing, I naturally tried to setup a storage bucket but was greeted by the following opaque error message:
The account for this specified project is disabled.
What? There was nothing left for me to do here. No amount of searching around helped either. I was out of luck.
Naturally I tried this several times, each time greeted with the following beautiful UI.
Who felt this was good enough? 4 hours later, I gave up.
I setup a new AWS account and signed up for S3 in under 5 minutes. 15 minutes later, I had created a storage bucket, decided on my directory structure and had uploaded test files via a command line script. 30 minutes later, it was completely integrated into my build environment thanks to this Gradle plugin.
And that’s it. Next step is to put CloudFront in front of S3 and I can move on with life. And while I’m certain GCS would have suited my needs perfectly, the ease of setup and simplicity in AWS far outweighed any marginal price or performance gains I may see from GCS, plus I can always try and switch to it later. There’s not much else I have to say here.
Love or hate this article? Let me know @davidbyttow.
Engineer by trade, artist at heart
25 
1
25 
25 
1
Engineer by trade, artist at heart
"
https://betterprogramming.pub/streaming-images-videos-to-google-cloud-storage-with-node-js-express-and-multer-%EF%B8%8F-7455c60f310?source=search_post---------8,"Sign in
Nick Parsons
Jan 22, 2019·2 min read
Note: I use a config.js file for holding configuration values for simplicity. I highly recommend using dotenv over this approach.
"
https://webeconoscenza.gigicogo.it/la-gestione-delle-foto-ai-tempi-del-cloud-844e3ba4d8ae?source=search_post---------271,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Il cloud storage è maturo e i fotografi (quelli dilettanti come il sottoscritto) ne sono molto felici.
Premetto, in questo articolo tratterò della mia personale esperienza che continua e si evolve osservando i vari servizi che nascono, crescono, a volte muoiono, e che non vuole assolutamente fissare degli obblighi, dei must comportamentali, proprio perchè tutto è beta e tutto è ancora abbastanza precario.
Ognuno è libero di pensarla diversamente su autorialità, diritti e condivisione. Ovviamente vale la solita raccomandazione: LEGGETE BENE I T.O.S. di ogni servizio prima di aderirvi e consegnare ad esso le vostre produzioni.
Era il lontano 2012 quando scrivevo un articolo analogo. Ora le cose son molto cambiate, dunque partiamo dall’inizio.
Ho due macchine fotografiche. Una Canon EOS 450D con due obbiettivi e un iPhone 7. Inorriditi? Si lo so, chiamare l’iPhone macchina fotografica è un po’ provocatorio, ma è indubbio che con esso scatto dieci volte più foto che con la vecchia reflex.
La prima ha quasi 10 anni e un grande difetto per i miei gusti: non ha il GPS e dunque non riesco a localizzare le foto. Ok, son maniaco e poi vediamo come risolvo questo drammatico problema.
Dunque, prima raccomandazione la nomenclatura dei file. L’iphone propone nativamente il formato: 2016–11–01 12:00:00.jpg mentre la mia reflex Canon non va oltre la numerazione sequenziale, ovvero: IMG_8878.JPG, e questo mi crea problemi di omologazione e coerenza.
Salvataggio immediato nel cloud.
Ci son diverse opzioni possibili per l’iPhone perchè quasi tutti i servizi principali (Box, Dropbox, Onedrive, iCloud, ecc.) garantiscono la funzionalità ‘camera upload’ che automaticamente salva una copia di ciò che abbiamo salvato anche nel cloud. A dire il vero lo fanno anche servizi specifici come Flickr o Google Photo, ma ci torniamo dopo.
Quindi, la prima cosa che faccio quando scatto con l’iPhone è attivare anche uno di questi servizi per garantirmi un salvataggio quasi immediato. In effetti attivo la preferenza ‘wi-fi only’ in modo da non esaurire il traffico 4g.
E con la reflex? Semplice, appena disponibile un pc collegato alla rete, scarico la SD card anche sul cloud service che ho scelto. Attualmente utilizzo Dropbox che diventa quindi il primo punto di raccolta delle mie foto prima di iniziare il lavoro di post produzione.
Da ciò rimane esclusa ogni foto che viene condivisa real-time sui social. In effetti le foto prettamente sociali, quelle che hanno il preciso scopo di creare engagement e reaction, spesso non rappresentano un interesse particolare nella post produzione e successiva raccolta e gestione.Per capirsi, un selfie dalla spiaggia va bene così com’è, lo condividi su Twitter, Instagram o Facebook e finisce li con il codazzo di like, commenti, ecc.
Instagram meriterebbe un discorso a parte. Non lo uso molto, ma le foto che carico in quel social sono spesso frutto di una post-produzione immediata, basata sui filtri di Snapseed o di Instagram e gestite direttamente dallo smartphone. La foto originale va su Dropbox, mentre quella di Instagram, filtrata e comunque cambiata rispetto all’originale rimane li per sempre, e non ne faccio copie altrove: https://www.instagram.com/gigicogo/
La post produzione di base
Qualche giorno dopo aver scattato e caricato le mie foto su Dropbox, mi siedo alla scrivania e utilizzo il PC per analizzare ciò che ho scattato e che risiede ora su Dropbox.
Apro dunque il folder sincronizzato sul PC e mi trovo le mie foto, siano esse quelle scattate con l’iPhone che quelle scattate con la reflex.
Per prima cosa do una scorsa veloce in modo da eliminare quelle meno interessanti che cancello dunque per sempre.
E ora passiamo al software e iniziamo da Lightroom. Si, Lightroom è il primo SW che uso e che mi serve per importare le foto da Dropbox dentro il suo sistema. Va detto da subito che il pregio di Lightroom è quello di non essere distruttivo, per cui le operazioni di post produzione avvengono sul software ma non inficiano sugli originali scattati.
Cosa faccio con Lightroom? Innanzitutto raddrizzo le foto. Spesso la linea del mare è storta. I palazzi sono sbilenchi, e le persone pure. Dunque utilizzo tutte le funzioni utili per rendere la foto più simmetrica, in verticale, orizzontale, frontale, laterale, ecc.
Lightroom mi permette anche di giocare con le regolazioni di base come il bilanciamento del bianco, le luci, le ombre, la saturazione, ecc. Difficilmente uso i suoi preset. Non mi interessano, anche se ce ne sono di incredibilmente potenti.
Dunque questo è il primo passaggio e adesso le foto sono pronte per essere esportate in una cartella locale. Da Lightroom, con la funzione esporta, creo una copia delle foto scattate su cartella del disco locale, alla massima qualità.
A questo punto non cancello ancora le foto dalla cartella di Dropbox e opero sul nuovo set di foto che Lightroom ha generato direttamente sul disco fisso del mio PC. Da questa cartella importo tutto sul SW nativo per Mac, ovvero Foto di MacOS.
Perchè lo faccio? Perchè mi permette di gestire la ricerca direttamente dal sistema operativo, l’eventuale condivisione su iCloud o Streaming foto (funzioni che trovo utilissime per guardare le foto dalla Apple tv) e soprattutto perchè mi permette di geo-taggare le foto scattate con la reflex e di cambiare il nome del file di tutte le foto prodotte con la Canon.
Come vedete dall’immagine qui sopra, è su Foto per MacOS che tutte le mie foto diventano finalmente omologhe nella nomenclatura: 2016–11–01 12:00:00.jpg, azione che faccio manualmente inserendo quella semantica come titolo della foto e che mi aiuta sempre a capire quando le stesse sono state scattate.
L’ultima attenzione che dedico su Foto di MacOS è quella di creare degli album cronologici. L’unico approccio agli album, infatti, è quello di aggregare le foto per periodo di scatto. Generalmente gli album prendono il nome di: ‘2016 Ottobre-Varie’, piuttosto che di ‘2016 Agosto-Madrid’.
Fatto questo e controllato per bene che tutte le foto siano nell’album giusto, cancello le foto da Dropbox, dal rullino dell’iPhone e dalla scheda SD della reflex. Direi che ho già due copie, una su Foto e una sulla cartella generata da Lightroom che, per ora, rimane temporanea fino al passaggio che più avanti descriverò.
Foto di MacOS mi permette di effettuare diverse operazioni native di post-produzione ma, soprattutto, permette di lanciare app terze direttamente dalla sua interfaccia, come ad esempio Photoshop, Aurora HDR, ecc.Qui trovate la pagina dell’assistenza Apple su come configurarle: https://support.apple.com/it-it/HT205245
Detto questo, lascio immaginare che le modifiche distruttive le faccio con SW di terze parti (eliminazione di particolari, viraggio HDR, filtri e altre diavolerie che provano a rendere la foto artistica).
Uno dei vantaggi di Foto per MacOS è che in ogni momento con la funzione ‘ripristina originale’ si può ottenere la foto di partenza, e questo non è banale perchè significa che nel suo database conserva sempre due foto, una come scattata e una modificata.
La catalogazione
Arriviamo dunque alla catalogazione definitiva e alla massima cura nel salvataggio di più copie.
Ogni album di Foto mi permette infine di esportarne l’intero contenuto ovunque ma, soprattutto, ricordo che a questo punto ogni lavoro svolto sino a qui ha prodotto un file coerente con la nomenclatura, metadatato, geolocalizzato e post prodotto artisticamente.
Eseguo dunque la funzione esperta su una nuova cartella del mio Hard Disk, stando attento a includere il nome del titolo come nome file di output.
Ottengo così un nuovo file, ma definitivamente utile alla catalogazione e successiva condivisione. Ma in che cartella lo salvo?
Qui scatta un’altra follia. Avendo la fortuna di possedere un’account molto capiente di One Drive, mi assicuro di salvare questi file definitivi su una cartella automaticamente sincronizzata con il servizio di cloud storage di Microsoft.
Cosa ottengo in questo modo? Esattamente 5 copie. Ovvero, la prima e la seconda nel data base di Foto per MacOS, la seconda nel mio drive esterno da 2 Terabyte gestito con Time Machine, la quarta nella directory One Drive del disco fisso (esclusa da Time Machine) e la quinta sui sever remoti di Microsoft. Può bastare?
Direi di si, ma ora scatta la follia definitiva. Non perdetevi l’ultima parte.A proposito, a questo punto cancello i file dalla cartella buffer dove li avevo originariamente esportati da Lightroom.
La condivisione nel cloud
Tutto ciò che ho fatto sinora mi garantisce una certa sicurezza personale. Ma ora vorrei anche far vedere le foto agli amici o ai parenti. Magari sulle varie device, piuttosto che via web, sulla Smart Tv, ecc.
Bene, nulla di più semplice. Ogni cartella viene totalmente replicata su Google foto (privato) e Flickr (pubblico): https://www.flickr.com/people/gigicogo/
Può bastare? Si, direi di si. Sono quasi soddisfatto. Anche perchè ciò significa che altre due copie di tutte le mie foto sono sul cloud. Su Flickr in formato originale e su Google Foto in alta qualità.
La condivisione artistica
Lo so, sono un dilettante, ma comunque mi piace bullarmi di qualche scatto venuto discretamente bene, e allora ho scelto altri modi per esporli.
Il primo, quello originale, è un self hosted che ormai mi convince poco. Una landing page con rimando ai vari portfolio nei social specializzati:
www.gigicogo.it
Gli altri tre sono servizi collettivi gestiti che, per ora, sfrutto solo per la potenzialità della community che ci sta dietro.
Il primo è 500px: https://500px.com/gigicogo
e il secondo YouPic: https://youpic.com/photographer/gigicogo/
e il terzo Pixiset: https://gigicogo.pixieset.com/venezia/
In aggiunta a questi tre, va segnalato Viewbug: https://www.viewbug.com/member/gianluigicogo che in origine è pensato come ambiente per concorsi fotografici.
Sono servizi di comunità dove è possibile confrontarsi con altri dilettanti e qualche professionista. Un posto dove condividere ciò che è bello e bullarsi delle votazioni, condivisioni e commenti.
Ora sto cercando un posto (managed e hosted) dove pubblicare un blog fotografico. YouPic, Pixiset e anche 500px a modo loro offrono una funzione simile ma sto cercando qualcosa di più awesome.
Esporre i propri lavori è bello, da soddisfazione e io non sono geloso se qualcuno li scarica, li usa, li diffonde.
Che ne pensate?
Blog personale di Gianluigi Cogo
3 
1
3 claps
3 
1
Written by
https://www.gigicogo.it
Blog personale di Gianluigi Cogo
Written by
https://www.gigicogo.it
Blog personale di Gianluigi Cogo
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@HerbertRSim/what-are-decentralized-cloud-storage-platforms-19bc1b0e5f03?source=search_post---------40,"Sign in
There are currently no responses for this story.
Be the first to respond.
Herbert R. Sim
Feb 10, 2018·1 min read
Even the most passionate blockchain enthusiast probably understands by now that blockchain technology isn’t a magic wand to solve every problem. However, the technology still holds great potential in a lot of areas, and cloud storage services are one of the most prominent.
Both consumers and businesses have become heavily dependent on cloud storage providers. As we are storing more…
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/build-your-own-in-home-cloud-storage-part-2-8fe86e9bd5bf?source=search_post---------38,"Sign in
There are currently no responses for this story.
Be the first to respond.
Randal Kamradt Sr
May 29, 2020·10 min read
In my article “Build Your Own In-Home Cloud Storage (Part 1)”, I built a basic network-attached storage system (NAS) based on GlusterFS and NFS Ganesha. In subsequent articles, I showed how to use the new storage solution in your home Kubernetes cluster. But the computer I used as a storage…
"
https://medium.com/mediafire/mediafire-beats-competition-in-best-cloud-storage-provider-reviews-dafd3ce3f93d?source=search_post---------131,"There are currently no responses for this story.
Be the first to respond.
MediaFire named a 2016 Top Rated Cloud Storage Provider by users on TrustRadius besting the likes of Amazon, Apple, Box, and Microsoft. A big “Thank You” to you, our users, who helped recognize our work to bring you an exceptional cloud storage service.
TrustRadius is one of the most trusted review sites for business technology. They collect unbiased and substantive reviews that are authenticated and vetted by their research team before publication. We’re honored that so many of our users have taken the time to submit such thoughtful and positive reviews of our work.
Look forward to much more to come, including some major updates to our website and services!
This is the blog of MediaFire.com
1 
1 clap
1 
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
Written by
MediaFire stores all your media and makes it available to you, anytime you want, anywhere you go, on any device you might have.
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
"
https://towardsdatascience.com/bigquery-data-structure-in-google-how-to-get-started-with-cloud-storage-b6146fcad1d1?source=search_post---------32,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Marie Sharapa
Jun 30, 2020·9 min read
Google BigQuery is a cloud storage service that allows you to collect all your data in one system and easily analyze it…
About
Write
Help
Legal
Get the Medium app
"
https://blog.sia.tech/sia-technical-update-and-roadmap-8f0fc42eb815?source=search_post---------331,"Hard for me to believe this, but we’ve been working on Sia now for more than 4 years. Even harder for me to believe, of the 4 major competitors that existed in 2014 (Sia, Storj, Filecoin, Maidsafe), only one has successfully launched a fully decentralized platform for cloud storage. That was us, and we pulled it off in 2015.
That said, you’d have a difficult time convincing anyone that what we released in 2015 was a finished product. We’ve come a long way since then, and we have a long way to go.
Today, Sia is able to store multiple terabytes of data from a single client, achieve sustained speeds above 100mbps for both upload and download, and can manage tens of thousands of files. And, in v1.3.3 we will be releasing official API support for video streaming from the Sia platform.
In the near road ahead, we will be focusing in the short term on the accuracy of renter spending reports. Today, the broad reporting is outright incorrect, and it appears that the per-contract reporting is also incorrect, often overlooking spending. We’ve fixed some of this already, but are performing a full audit of the contractor spending reporting (in addition to expanding the test suite to more extensively cover reporting) to make sure that accounting is correct from v1.3.4 forward (some of the fixes will not be ready in time for v1.3.3).
Another thing that we are focusing on is crash-resilience of the contractor. Crash resilience ends up being extremely difficult to achieve for decentralized networks, especially ones as complicated as Sia. The core problem is consumer hard drives. If a computer suddenly loses power or is turned off abruptly, data may only be partially written to disk, causing corruption. To make matters worse, the hard drive and operating system will actually report to the code that a write is complete even if the data is not safe from a sudden loss of power. Sia is currently very good in this regard — most everything that happens on Sia is resilient to unexpected crashes, however we need to get the final testing in place for the contractor.
The primary goal for v1.3.4 is going to be file backups. Today on Sia, if the host machine is corrupted, all of the data gets corrupted as well. A developer (myself or my cofounder) would be able to take an old backup of the metadata and recover the files, but it would be a lot of manual, highly expert work. We’re going to be changing that in v1.3.4, making it possible to back up your metadata such that you can recover your files in the event that the host machine crashes, removing the single point of failure that currently exists for most people.
As many people are aware, file backups and file sharing on Sia are actually roughly the same thing. You can think of filesharing as giving a file backup to a friend. Or you can think of a file backup as sharing a file with yourself. We will be focused on making the endpoints for backups in v1.3.4, and then providing basic support for file sharing in v1.3.5. Our target for v1.3.6 is a recursive metadata compression technique that should make full system backups small enough to be put on the blockchain itself. We can encrypt those backups with your seed and put them on the blockchain such that you will be able to recover your data using nothing but your wallet seed. This will require making infrequent transactions on-chain for backups, and also means that backups will not be continuous, however it’s a big step forward.
The changes to the file format to enable file backups, file sharing, and seed-based file recovery will also lead to greater file scalability for the renter, increasing the renter’s capabilities from tens of thousands of files to millions of files. These changes should also substantially increase the total amount of data that the renter is capable of supporting.
In the very long term, we do know how to make seed based file recovery continuous. It’s… a huge pain and a massive engineering effort. I’m guessing it’s going to be more work than creating the entirety of the Sia platform to date. That said, we know how to do it, and we know how to make it reasonably efficient. That would mean that at any point, you could log into any computer, install Sia, and gain access to your full, up-to-date data. This is something that is definitely on our roadmap, but not something we are likely to complete in 2018 or even 2019.
We have hired a full time developer who will be working on the UI starting in the next month. UI development has been all but frozen over the past year due to our primary UI developer leaving, however this gap in our team has been filled! Expect good things moving forward.
Within the broader cryptocurrency community, we’re going to start reaching out to more projects such as RNDR and Blockstack so that Sia may be the file backbone for the cryptocurrency space. The biggest thing holding back many projects today is filesharing and backups, and we will be enabling both of those in the coming months.
One other huge focus for us over the next few months is going to be an on-chain scalability solution that I’ve been working on quietly over the past year. I’ve called the solution microchains, and I believe we can leverage it to gain 10,000x on-chain scalability for Sia, among a myriad of other benefits. I’ll be releasing regular blogposts explaining microchains and exploring the various advantages they have over traditional blockchains. If all goes well, we hope to be writing some microchains based code by the end of the summer.
Overall, 2018 is going to be a busy year for us. As always, we’re looking at what we believe is the most long-term critical elements for the platform, and building Sia to be a platform that’s likely to still be in use 50 years from now.
Decentralized storage — Sia, Skynet, and cryptocurrency.
1.7K 
5
Thanks to Steve. 
1.7K claps
1.7K 
5
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://medium.com/@nabeelxy/categorizing-cloud-storage-services-by-their-encryption-options-14ca041d9607?source=search_post---------148,"Sign in
There are currently no responses for this story.
Be the first to respond.
Mohamed Nabeel
Dec 13, 2016·3 min read
I have been analyzing cloud storage services for a quite sometime now. I would like to summarize some findings in this post.
In the market, there are three types of encrypted storage solutions available in the cloud — server side encryption, client-side encryption (zero knowledge) and proxy encryption. We go through them one by one here.
Let’s first focus on the cloud storage service providers themselves. Most of the storage service providers support server side encryption. How does it work? It is quite simple, the storage service provider manages keys and encrypts the data they store in their cloud. In other words, this provides data-at-rest encryption. What does such encryption buy you? Assuming that the keys are stored separately from the encrypted data, if an attacker gains access to servers containing encrypted data, they won’t be able to see your data. However, since all keys are controlled by the cloud service provider, this solution does not provide privacy guarantees with respect your data. In theory, cloud service provider can still see your data. Further, data stays cleartext during processing and user access times. Why do many cloud storage service providers support this type of encryption? We believe it is mainly to meet the compliance requirements and as we all know compliant does not mean secure. Can we do better than simply having data-at-rest encryption? Yes, that is where client-side encryption (zero knowledge storage) comes into play. Simply putting, your data is encrypted BEFORE it reaches the storage service provider and YOU manage the encryption keys. Thus, the storage service provider sees only the encrypted data. While zero knowledge storage seems beneficial for users in general, many major storage service providers such as DropBox, Box, Google Drive, and OneDrive do not support it. Ever wondered why? There could be many reasons, but some of the key reasons could be loss of functionality (compression, de-duplication, search, etc.), lack of support from the eco-system (support by operating systems), safety reasons (e.g. recovering lost client-side keys) and additional computational and communication costs involved. Nevertheless, there is a growth in the offerings of zero knowledge storage services.
In order to bridge the gap created by major storage service providers, there are many vendors who offer proxy encryption solutions. The idea is that you install a proxy between your data and the cloud storage service. The proxy transparently encrypts and decrypts data to and from the cloud storage service. While such solutions achieve the objectives of client side encryption, they are still not very popular. One possible key reason could be lack of usability.
Encryption in this space is sure to expand in the future, but there are many unknowns as to how this expansion will happen. Where will the storage services go in the future in terms of encryption offerings? How can we overcome the drawbacks of client side encryption solutions to provide same level of service as server side encryption? Will major storage service providers ever support client side encryption?
Cyber Security Researcher | Machine Learning | Crypto for everyone!
See all (78)
Cyber Security Researcher | Machine Learning | Crypto for everyone!
About
Write
Help
Legal
Get the Medium app
"
https://blog.sia.tech/siastream-is-the-new-home-for-all-your-media-dd51ac7b06ec?source=search_post---------231,"It’s the most efficient cloud storage platform, capable of streaming to all major media players.
We’ve just released SiaStream, a brand new app from Nebulous. It’s a way to store your media in the cloud and get it to your favorite streaming app like Plex, Kodi, Emby, or Jellyfin. It can stream in beautiful 4k 60 fps. Buffer times are incredibly low. Most of the time it feels like you’re watching it right from your home setup, except it’s backed up across the world and highly secure.
It’s the first app to use a global network of storage providers to keep costs down and performance high. You can store your files on SiaStream either as a backup for your main media library or as your primary location to replace your home server.
The bandwidth costs associated with frequently accessing files have traditionally forced cloud storage out of the equation for media servers. That’s why you see so many people with extensive media libraries running their own private servers, typically setup in their homes. And even though hard drives come cheap these days, we still believe that cloud storage can be a great option for your media.
Enter SiaStream. SiaStream can store your media library in the cloud and stream directly to your favorite media player. It can store up to 35 TB per instance and has a base price $3.99* for each TB. That price includes both storage and bandwidth, one of the most expensive factors when it comes to accessing cloud data. It can do this, and keep it secure and private in a way that no other solution can. Because SiaStream uses the Sia marketplace, storage prices can sometimes temporarily exceed $3.99/TB/month.
This is also the first official use of the new Sia fee manager, where app developers can charge a fee for their apps built on Sia or Skynet. SiaStream charges a user fee that equals the difference between what the user pays for storage, and $3.99. For example, if you end up paying $2.99 for your storage, $1.00 is taken as a fee. This fee goes straight to Nebulous. If storage costs were to go above $3.99 due to marketplace fluctuation, no fee would be charged.
SiaStream is built on Sia, a blockchain-based cloud storage network. Sia is completely decentralized and stores your files in locations all over the world, not in huge server farms like how Amazon or Google do it. There are no sign ups and no trusted third parties — just you, and the people who store your data.
Sia has thousands of active users and a network that has been online for over five years. The files that are stored on Sia are split up and encrypted, and can never be viewed by anyone but you. The exception of course being media files uploaded to SiaStream when accessed by you or others connected to your media server.
SiaStream helps mount Sia as a drive on your computer using FUSE, and everything you upload can stream directly to your media player of choice, like Plex or Kodi.
It’s ok if you’ve never heard of Sia, blockchains, or cryptocurrency at all. Our goal with SiaStream is to break down those barriers and make it as accessible as possible. You can read more about the technical side of things in this great blog post from Chris, one of our core devs.
We have a simple on-boarding process and common-sense explanations for features that are, behind the scenes, pretty complex. You can download the app and get set up in a matter of minutes. And we’ve got a wonderful support center with guides and answers to your questions.
You can head over to siastream.tech to get it now.
Decentralized storage — Sia, Skynet, and cryptocurrency.
169 
Thanks to Manasi Vora. 
169 claps
169 
Written by
Head of Support for Sia
Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by
Head of Support for Sia
Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://medium.com/hackernoon/architecting-a-highly-available-and-scalable-wordpress-using-docker-swarm-traefik-glusterfs-a69186e9f0e?source=search_post---------337,"There are currently no responses for this story.
Be the first to respond.
This content is part of / inspired by one of our online courses/training. We are offering up to 80% OFF on these materials, during the Black Friday 2019.
You can receive your discount here.
Docker is a powerful tool, however learning how to use it the right way could take a long time especially with the rapidly growing ecosystem of containers which could be confusing, that is why I had the idea to start writing Painless Docker.
Painless Docker is a complete and detailed guide (for beginners and intermediate levels) to create, deploy, optimize, secure, trace, debug, log,orchestrate & monitor Docker and Docker clusters in order to create a high quality microservices applications.
This article is more detailed in the bonus chapters of Painless Docker Book.
Using Docker, Docker Swarm, Amazon RDS (Aurora) + EC2, GlusterFS & Traefik, we are going to create a highly available and scalable WordPress cluster.
I am using Amazon EC2 machines but you can use you prefered infrastructure.
Start by creating two (EC2) machines in two different availability zones.
In this tutorial, I am using:
The first machine will be the manager of the Swarm cluster and the second one will be the worker.
This is just an example but it depends on how available you want your cluster to be, you may create 3 managers (or more) for more availability.
The manager will have a public IP because it will receive all of the ingoing requests and redirect them to Docker that will handle the internal load balancing between the containers living in the manager (same machine in this case) and the containers living in the worker.
Don’t forget to add an Elastic Block Store to each machine.
We should have two instances:
Using lsbsk, we can verify that the EBS is attached to our machine:
On each machine, create our filesystem:
GlusterFS is a scale-out network-attached storage file system.
Using common off-the-shelf hardware, you can create large, distributed storage solutions for media streaming, data analysis, and other data- and bandwidth-intensive tasks.
GlusterFS was developed originally by Gluster, Inc. and then by Red Hat, Inc., as a result of Red Hat acquiring Gluster in 2011.
Let’s start by installing GlusterFS:
If you are using another OS or another Linux distribution, adapt this command to your need.
Notice: I created a machine with no public IP (the worker), this machine will not be able to reach Internet and install GlusterFS unless you setup an Amazon NAT instance. If you are just making some tests or not familiar with AWS, create a machine with public IP or create an EIP then assign it to the machine.
Create a mount target (brick):
Notice: A brick is a directory on an underlying disk filesystem. If one of the bricks goes down, there is a hardware failure to make the data available.
Use fstab file to permanently mount the EBS to the new brick:
Now use mount /dev/xvdb in order to apply the modifications that we added to the fstab file.
Under each /glusterfs/bricks/booksfordevops.com mount point a directory used for GlusterFS volume:
Let’s add these lines to the /etc/hosts/ file on each server , we will need this in a following step:
node1 is the manager and node2 is our worker. In order to test this configuration, you can execute: ping node2 from node1:
From the manager (node1), type the following command to establish Gluster cluster nodes trust relationship:
You should have peer probe: success as an output, otherwise check your firewall settings or tail your logs.
Now we have a working GlusterFS trusted pool.
A GlusterFS storage pool is a trusted network of storage servers (node1 and node2 in our case). When we started the first server, the storage pool consisted of that server alone. When we added additional node2 storage server to the storage pool (using the probe command from the node1 storage server that is already trusted), we created a trusted storage pool of 2 servers.
Now, from the manager server, we should create a two-way mirror volume that we will call booksfordevops-com-wordpress using:
You will get this output:
You can see the volume if you type gluster volume list
Now you should start it using gluster volume start booksfordevops_com-wordpress:
If everything is ok, you will get a similar output to this:
Our volume should be healthy but you can check its status using gluster volume status:
Now that the GlusterFS server is set, we need to setup the client side. Let’s create the directory (in each node) to be used by the client in each node of our cluster:
We need to mount a shared directory on node2 from the node1 and the same directory on node1 from node2.
On node1, add this line at the end of /etc/fstab file:
On node2, add this line at the end of /etc/fstab file:
Then on both hosts, type mount -a.
The next step is to install Docker on both hosts:
Then initialize the Swarm cluster:
Execute the last command on the manager and you will get a command to execute on the worker:
If everything is ok, the worker will join the cluster:
In this tutorial, we want to create a Wordpress blog, we can host the Mysql or the MariaDB database in our GlusterFS trusted pool but in my specefic case I created an Aurora database.
We are going to host Wordpress files in the storage pool we created, that points to /data/booksfordevops_com-wordpress.
This is the Docker Compose v3 file that we are going to deploy:
Notice: For security reasons, do not put your docker-compose.yml file in the same directory as your Wordpress files in `/data/booksfordevops_com-wordpress, it will be publicly accessible.
In order to deploy our website, we should execute this command:
We can use a similar command to the following one in order to deploy the Wordpress app:
But putting all together in a Docker Compose v3 file is a more organised way to deploy our app.
You may have some premission problems with your Wordpress fresh installation, you will need to execute these commands:
In both servers, you can now notice that Wordpress files are mounted from the service containers to the host volume:
If you check the created brick on each server, you will find the same files:
At this step, we have a working Wordpress installation that you can reach using the IP address of the manager and the port 8000.
Træfɪk is a HTTP reverse proxy and load balancer made to deploy microservices and supports Docker and Docker Swarm (and other backends like Mesos/Marathon, Consul, Etcd, Zookeeper, BoltDB, Amazon ECS and Rest APIs).
Let’s create a service to run Traefik on the manager ( --constraint=node.role==manager ). The reverse proxy will run on a separate network ( --network traefik-net).
In order to make our Wordpress website work with Traefik we are going to update its docker-compose file and add it to the same Traefik network traefik-net. I added also some labels related to Traefik like port and frontend.rule:
Update the deployment using
docker stack deploy --compose-file=docker-compose.yml booksfordevops_com
You can check if the configured domain is accessible using a simple curl, in my case:
You can also go to the health dashboard in order to see things like the response time and the status codes of our application.
We saw how to build a highly available Wordpress website, where storage and computing are distributed into two different regions.
Within each EC2 machine we can scale Wordpress to more than one container and have another level of resilience.
Our reverse proxy can check the health of each container and manage to redirect traffic to the working ones.
We used modern tools and technologies like:
Our website is working, you can subscribe and wait for Books For DevOps release.
This article is part of Painless Docker Book: Unlock The Power Of Docker & Its Ecosystem.
Painless Docker is a practical guide to master Docker and its ecosystem based on real world examples.
Painless Docker tends to be a complete and detailed guide to create, deploy, optimize, secure, trace, debug, log, orchestrate & monitor Docker and Docker clusters. Through this book you will learn how to use Docker in development and production environments and the DevOps pipeline between them in order to build a modern microservices applications.
If you resonated with this article, please subscribe to DevOpsLinks : An Online Community Of Diverse & Passionate DevOps, SysAdmins & Developers From All Over The World.
You can find me on Twitter, Clarity or my blog and you can also check my books: SaltStack For DevOps & The Jumpstart Up.
If you liked this post, please recommend and share it to your followers.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
268 
2
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
268 claps
268 
2
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/memory-leak/a-to-z-alphabets-cloud-software-contributions-6f660383587?source=search_post---------390,"There are currently no responses for this story.
Be the first to respond.
This week is Google Next, Google’s three-day cloud technology event. For years Google has provided leadership around cloud native infrastructure, often leading by example with its own internal solutions that are either open sourced or end up inspiring other projects.
Since Google has managed complex distributed systems and run Linux containers for more than ten years, their internal tools and open source projects provide a good indicator of the challenges businesses face when they become cloud-native at scale. Google’s ability to foretell the needs of the broader community are exemplified by Kubernetes, one of the most successful open source projects, and publicly traded companies built around technology inspired by Google.
In light of Google’s tremendous impact on cloud and open source innovation in our industry, we roundup a list of projects we hear about most frequently: Bazel, Borgmon, BigTable, Dapper, GFS, Go, gRPC, Istio, Kubernetes, MapReduce, Spanner, Spinnaker, TensorFlow, and Vitess.
We know there are more! Please let us know what other projects you are using and what makes you excited about them. We are excited to speak with entrepreneurs who find motivation from Google’s work as their eyes are on the future.
Bazel: Bazel is a build system with advanced local and distributed caching and parallel execution to accelerate builds. It can build and test Java, C++, Android, iOS, Go, amongst other languages. It is used by numerous companies like Asana, Braintree, Etsy, Pinterest, and Redfin and has achieved over 9,500 on Github. Alternatives include Twitter Pants and Gradle, which has seen accelerated adoption over the past year.
Borgmon: Borgmon is the internal monitoring system Google built in tandem with its own cluster scheduling system, Borg. Borgmon helped inspire Soundcloud engineers to build Prometheus, a popular open source time-series monitoring tool that is often used in tandem with Kubernetes. According to a survey published by “The New Stack” last fall, 63% of those surveyed used Prometheus as a monitoring tool in their Kubernetes clusters, significantly higher than second place Heapster that saw 40% penetration (respondents were able to choose multiple tools).
BigTable: In 2004, Google began to develop BigTable, a distributed storage system for managing structured data at a very large size. BigTable is a highly distributed share nothing architecture that supported web indexing, Google Earth, and Google Finance. BigTable inspired HBase and Cassandra, which DataStax built a company around. DataStax is close to unicorn-status with its most recent post-money valuation at ~$967 million. DataStax underscores that Google-influenced solutions can grow into large businesses.
Dapper: Modern applications are constructed from various services developed by different teams and run across multiple separate facilities. Google observed the challenges of identifying performance issues in these disparate environments and built Dapper, a large scale distributed tracing solution that could identify performance issues and provide root-cause analysis. Dapper could handle high throughput, analyzing 2 billion transactions per second. Dapper inspired open tracing, a vendor neutral open standard for distributed tracing that joined CNCF in October 2016, and LightStep, an early-stage startup providing next-generation APM leveraging tracing to diagnose anomalies across all components of today’s software applications. LightStep exemplifies how internal Google solutions have inspired commercial offerings.
Google File System (GFS): GFS is a scalable distributed file system for large distributed data-intensive applications that runs on commodity hardware. Four observations inspired GFS: 1) component failures were becoming the norm so constant monitoring, error detection, fault tolerance, and automatic recovery were integral; 2) multi-GB files were becoming more common so I/O operation and block sizes had to be revisited; 3) most files were mutated by appending new data rather than overwriting existing data; and 4) well-defined semantics for multiple clients that concurrently append to the same file were crucial. Google published the GFS paper in 2003 and inspired HDFS created by Doug Cutting and Mike Cafarella in 2005. Cloudera and Hortonworks went on to commercialize HDFS and are now publicly traded companies with a market cap of ~$2.2B and ~$1.6B, respectively.
Go language: Since Go was released as an open source project on November 10, 2009, we’ve seen the rapid rise of the language and its community of Gophers. Go was built from the ground up and is one of the few languages created when computers had multiple cores. Go is a statically typed language that is compiled to machine code. Programmers note Go’s benefits include its garbage collection, simplicity, and concurrency. Because Go leverages “goroutines” and channels that are more efficient than threads, it works well for writing backend systems like Vitess and Kubernetes.
According to GitHub’s 2017 “Octoverse,” Go is the ninth most popular language, beating out C. The report stated that of the top 10 languages Go was the fastest growing language on GitHub in 2017 with 52% YoY growth. The 2017 Go Survey found that the percentage of developers using Go for work rose to 67% of respondents, up from 62% in the 2016 survey.
Note: Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.
gRPC: gRPC is a modern open source Remote Procedure Call (RPC) using protocol buffers, a mechanism for serializing structured data. Used internally for 15 years and known as Stubby, gRPC can replace traditional protocols in client/server and web apps. Like traditional RPC systems, gRPC defines a service, specifying the methods that can be called remotely with their parameters and return types. In gRPC a client application can directly call methods on a server application hosted on a different machine as if it was a local object.
gRPC was designed for internal communication between services as compared to REST that was optimized for North-South communication. gRPC can be combined with GraphQL and has support for C++, Java, Python, Go, C#, and Ruby. We’ve seen gRPC used in both container and non-container environments, and it is fundamental to Istio. Probably the most widely known open source project that uses gRPC is CoreOS’s key/value store etcd. Google contributors note they will be focused on improving gRPC’s performance and decreasing its latency in 2018.
Istio: Launched in May 24, 2017, by Google, IBM, and Lyft, Istio is an open-source platform for managing, securing, and monitoring microservices. We’ve previously written about the excitement around service meshes and an in-depth overview of the market can be found here.
Kubernetes: Kubernetes builds on Google’s experience with its own internal cluster scheduling systems, Borg and Omega. Over the past year we’ve seen Kubernetes become the default orchestrator for containers and have heard there are over 30 distributions. We expect the number of distributions to increase over the year as the project continues to gain traction.
We believe the community’s coalescence around Kubernetes allows startups to avoid using resources to build integrations with many orchestrators, so they can focus on building value-added solutions on top. Additionally, standardization allows the best technology solution to win the sales opportunity versus the solution with the specific integration the customer needs.
MapReduce: During the same time as GFS arose MapReduce, a programming model and an associated implementation for processing and generating large data sets. MapReduce inspired Hadoop MapReduce and became part of the Hadoop stack offered by Cloudera and Hortonworks.
TensorFlow: Probably the most well-known Machine Learning (ML) framework, TensorFlow is an open source library for numerical computation and that can work across a variety of hardware platforms from CPUs to TPUs. TensorFlow stemmed from Disbelief, a propriety ML system based on deep learning networks. Originally developed by researchers on the Google Brain team, TensorFlow was released under the Apache 2.0 open source license on November 9, 2015. We’ve written extensively about ML Workflows and published a landscape of solutions, including TensorFlow.
Spanner: Google created BigTable in the early 2000s but found that data with complex, evolving schemas, or that required strong consistency in the presence of wide-area replication weren’t a good fit for the system. In turn, Google build Spanner a scalable, multi-version, globally distributed, and synchronously-replicated database that could be queried using SQL. Inspired by Spanner, ex-Google employees Spencer Kimball, Peter Mattis, and Ben Darnell went on to build Cockroach Labs, which offers an open source cloud-native distributed SQL database, often termed “NewSQL”. CockroachDB has almost reached fourteen thousand stars on GitHub.
Spinnaker: Spinnaker is an open source multi-cloud Continuous Delivery (CD) platform. Martin Fowler defined CD as “a software development discipline where you build software in such a way that the software can be released into production at any time.” While Netflix started Spinnaker, Google joined the project in 2014 and helped launch it to the community in November, 2015. Armory.io offers a version of hosted Spinnaker-as-a-Service.
Vitess: Vitess is a database orchestration system for MySQL that leverages sharding for horizontal scaling. It supports autonomic failover, replication, and rolling upgrades. Vitess runs best in a containerized environment and is cloud agnostic.
Tumblr, HubSpot, Twitter, Facebook, amongst others have implemented and committed to the project. Vitess has recently been voted to be an incubating project in the CNCF. It was the second storage project voted in after Rook. Some members of the Vitess team have gone on to form PlanetScale Data, an early-stage NewSQL start-up based on Vitess.
VC Astasia Myers’ perspectives on distributed computing…
17 
1
17 claps
17 
1
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Written by
Founding Partner, Enterprise @ Quiet Capital, previously Investor @ Redpoint Ventures
VC Astasia Myers’ perspectives on distributed computing, cloud-infrastructure, developer tools, open source and security.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@PokitDok/how-electronic-health-records-influence-the-cost-of-healthcare-13a4a2d35e0d?source=search_post---------268,"Sign in
There are currently no responses for this story.
Be the first to respond.
PokitDok
Apr 22, 2015·4 min read
In this day of managing our lives on smart phones, tablets and online cloud storage, the idea of electronic health records (EHRs) just makes sense.
The government has nudged doctors and medical facilities to get on board with the HITECH (Health Information Technology for Economic and Clinical Health) Act of 2009, which calls for full scale use of EHRs by the end of this year. By offering financial incentives via its meaningful use program through 2014, doctors stand to make money (of course countered by what they spent on the software) by implementing EHR systems. Starting this year, physicians treating Medicare patients will actually be financially penalized if they do not integrate with an EHR system.
How then do electronic medical records affect patients? For the most part, the results are positive.
Influencing the cost and utilization of care
While the cost charged to patients for care probably won’t go down as a direct result of electronic medical records, patients may indeed pay less because they stand to undergo fewer tests. If lab and diagnostic radiology results are available through a shared system- as they would be by implementing an EHR -, providers will be able to find the results they need without ordering additional, duplicative exams. That’s also obviously — way more convenient for patients, who won’t need to waste time on additional, unecessary testing.
Patients may feel the pinch with duplicate exams if they’re using multiple care providers who aren’t part of the same physician network, since it’s harder to coordinate care without shared records. While this was standard operating procedure in the past, it’s less so now. Patients in today’s world want a seamless healthcare experience. Cut the friction, as we say.
Increasing patient safety and quality of care
The goal of healthcare systems, we should be able to agree, is to increase patient safety and quality of care. Electronic medical records can help accomplish this goal in a number of ways.
The downside to EHRs
On the doctors’ side, not everyone is happy about the use of EHR systems. First of all, they aren’t cheap. According to a 2013 Medical Economics survey, 77% of the largest practices said they spent upwards of $200,000 on their software systems. These offices said that electronic records haven’t made their practices more efficient, but rather, the expense of acquiring, implementing and maintaining the system, plus training staff, has increased the provider burden. In addition, doctors are lowering their personal efficiency as they take extra time to input patient information into the online record, instead of their former method of jotting down notes on a paper chart.
Similarly, some doctors in the Medical Economics survey didn’t feel that patient care and physician/hospital coordination had seen the higher quality results they expected by this point. As with any major system change, this shift in business model is a work-in-progress that requires ongoing investment and assessment to yield long term return for everyone. It will be interesting to watch the EHR space — and the players in it (GE recently announced at HiMSS that they will no longer be pursuing the business) as they continue to grow and evolve with time. Undeniably, EHRs are an inevitable staple in the future of health; it’s really just a matter of building the foundation (which is where we come in) to make massive cost saving and increased efficiency in health possible.
PokitDok is a cloud-based platform that streamlines healthcare transactions, from improving patient engagement to submitting claims more efficiently. #healthIT
5 
1
5 
5 
1
PokitDok is a cloud-based platform that streamlines healthcare transactions, from improving patient engagement to submitting claims more efficiently. #healthIT
"
https://medium.com/ipfscloud/ipfscloud-tech-update-2-upload-using-url-7fc45eb8aa7f?source=search_post---------380,"There are currently no responses for this story.
Be the first to respond.
Receive curated Web 3.0 content like this with a summary every day via WhatsApp, Telegram, Discord, or Email.
simpleaswater.com
As mentioned in our previous post, one of the features that was proposed in the Roadmap was Uploading files using URL.(ipfscloud.store)
medium.com
And now it’s here…
You can do a lot of stuff using URL uploader.
Here is a small demonstration of how you can upload simple websites in a go.
Here is a small demo website.
The website is can be accessed here: http://18.204.88.53:3001/uploads/index.html
Paste the above URL to the URL uploader as shown below.
Now hit the upload button.
You will get a link like below:
Your may have to add “Access-Control-Allow-Origin” header to your website host.
If you are interested in being our very first users and want to check out the app before it hits the playstore then sign-up for our beta testing program here: https://goo.gl/forms/Z9g3BVZrISV1gW8M2
Follow our updates here:
Thanks for reading;)
About the Author
Vaibhav Saini is a Co-Founder of TowardsBlockchain, an MIT Cambridge Innovation Center incubated startup.
He works as Senior blockchain developer and has worked on several blockchain platforms including Ethereum, Quorum, EOS, Nano, Hashgraph, IOTA etc.
He is currently a sophomore at IIT Delhi.
Hold down the clap button if you liked the content! It helps me gain exposure .
Want to learn more? Checkout my previous articles.
medium.com
medium.com
hackernoon.com
hackernoon.com
Clap 50 times and follow me on Twitter: @vasa_develop
An ecosystem, providing Apps, Infrastructure, and Tools…
35 
35 claps
35 
Written by
Entrepreneur | Co-founder, TowardsBlockChain, an MIT CIC incubated startup | SimpleAsWater, YC 19 | Speaker | https://vaibhavsaini.com
An ecosystem, providing Apps, Infrastructure, and Tools enabling Decentralization.
Written by
Entrepreneur | Co-founder, TowardsBlockChain, an MIT CIC incubated startup | SimpleAsWater, YC 19 | Speaker | https://vaibhavsaini.com
An ecosystem, providing Apps, Infrastructure, and Tools enabling Decentralization.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/ipg-media-lab/google-imports-hard-drive-data-to-cloud-storage-333c6adefc77?source=search_post---------194,"There are currently no responses for this story.
Be the first to respond.
Google today added a new service to its Cloud Storage that will allow developers to send their hard drives to Google to import very large data sets that would otherwise be too expensive and time consuming to undertake alone. Google is charging a flat fee of $80 per hard drive. Google claims that this process of uploading data to a Cloud Storage bucket is both faster and less expensive than transferring data over the Internet. Amazon already has a similar service, and also charges $80 for a hard drive — but Amazon also charges a per-hour fee for uploading data.
The media futures agency of IPG Mediabrands
The media futures agency of IPG Mediabrands
Written by
Keeping brands ahead of the digital curve. An @IPGMediabrands company.
The media futures agency of IPG Mediabrands
"
https://rom.feria.name/using-cloud-storage-privately-5693d3ad26b9?source=search_post---------119,"Most of us have our own favorite cloud storage services. In my case, I prefer using iCloud for my documents and photos. However, iCloud is not like the other online storage solutions that you can easily access outside of the Apple ecosystem. For those instances, I have multiple options — a web hosting provider that I use, a Nextcloud instance, a virtual private server (VPS), Google Drive and Microsoft OneDrive (both provided under the academic license). However, since I do not trust these cloud storage services, it is imperative that I encrypt all my data.
"
https://medium.com/box-tech-blog/box-graph-how-we-built-a-spontaneous-social-network-cda3b88b3486?source=search_post---------355,"There are currently no responses for this story.
Be the first to respond.
by Sesh Jalagam, Senior Staff Engineer
By now it’s no secret that the amount of content that is being created and shared is increasing exponentially. To help extract value from all of that content, we introduced Box Skills, a framework that brings machine learning innovations to content stored in Box. For example, video intelligence, one of the initial Box Skills, provides transcription, topic detection, and facial recognition for individual videos. But that’s just one file at a time.
We’ve also been developing a way to better understand your content at scale, to give you insight into the relationships between content and people. When users collaborate with each other to accomplish their work, these collaborations can be short-lived or last for months or longer. This activity implicitly forms a spontaneous social network within your enterprise. Unlike user-managed social networks, this spontaneity requires a real-time distributed graph that computes the network.
Enter Box Graph, our own machine learning model that actively interprets and then maps how your organization works. Box Graph understands the relationships between content and other content, content and people, and people-and-people. Graph continuously learns from everything you do in Box: from the shared links you generate, to the partners you collaborate with.
Below visual depicts user-to-user relations that are formed over a period within an enterprise.
From the graph we can gain various insights:
One can use these valuable insights to serve relevant content to users and help them accomplish their job, help admins and team members to get a global picture of everything that is happening within their enterprise, and suggest relevant collaborators.
Every day millions of users engage with content across various enterprises to accomplish their work. Each interaction contributes to a bigger picture of how work is being done in that organization. We can look at the user interactions (comments, views, modifications, etc) with the content to understand how users are collaborating, this forms the basis for the Box Graph.
In the above example:
This creates the implicit user-to-user graph, as derived from the explicit user-to-file graph
There can be many different types of these implicit networks that can exist across files, locations, tasks and other entities.
In our model, the thickness of the edge in the graph represents the strength of the edge, given by a score. If users are collaborating closely they have higher edge score. When recommending files, we recommend the top contributions of the top collaborators for a user (in the above example Sue and Bob are Joe top collaborators). To accomplish this, the graph should be able to:
We built a horizontally scalable, versioned graph system that can host realtime scoring formulas called Score Keeper Service (SKS) to maintain user to user and user to file relations. We asynchronously update the graph from user events using a high throughput , low latency, persistent queuing system called Message Queue Service (MQS is box built guaranteed delivery queue with isolation and QoS support).
SKS is built on top of HBase, with notion of channels (name space separation), each channel can host one or more graphs. Formula determines how graphs are updated with events and how to retrieve top edges for given vertex.
SKS stores scores in Scoring Table that allows for low latency updates of the edges. Ranking Table maintains the secondary index of score, to provide low latency retrieval of topN edges for given vertex.
Following diagram depicts the key design for scoring and ranking table.
Score Row Key|<-sourceType-><-source-><-channel-><-targetType->|← — — target — — — ->||← — — — — — — — — — — — — — — — 16 Byte hash — — — — — →|← 16 Byte hash — ->|
Rank Row Key|<-targetType->|← — — score — — ->|<- timestamp ->|← target — — — ->||← — — — — — — — — — — — — — 16 Byte hash — — — — — — ->|<-2x8 Byte Long->|<-8 Byte Long->|<-16 Byte hash ->|
Where SourceType is the source vertex type and TargetType is the target vertex type of the edge. Above design ensures that ScoreRowKey and RankRowKey will have same prefix, increases chances of collocating RankRows with ScoreRows, hashing reduces hot spotting and provides channel level separation of graphs.
High throughput score updates of edges are accomplished by:
Time slicing allows us to look at snapshots of the graphs as they were at a given moment in time. With a simple count Formula and a snapshot per day, one can ask the graph questions like how many views happened in the past 7 days. Or much more complex questions like: show the top users I was collaborating a month ago on this project. Time slicing of the graph is accomplished by leveraging hbase versioning and ensuring KEEP_DELETED_CELLSis configured to true on HBase.
One of the key requirements for formulas to be realtime is that they are incremental in nature and could be represented as an accumulation function.
Recency vs FrequencyThe underlying theory behind the Box Graph is our own version of collaborative filtering. In our version, we think of our users as described by a vector of file interactions. An entry of a user vector measures the strength of that user’s interaction with the corresponding file. We quantify that strength as a sum over all the user’s interactions with a file as depicted by below graph.
The sum is taken over all the observed user-events with that file (all of the times the user opens, edits, comments, shares, etc), t is the current time, wᵢ is the weight associated to the event (for example, to emphasize that editing signals a stronger relationship to a file than just opening it) and tᵢ is the time of the event.
The parameter λ lets us control the time-decay tradeoff between recency and frequency. When λ is 0, then the formula reduces to counting the number of interactions of the user with the file. When λ is very large (and negative), then only the most recent event will contribute anything meaningful to the resulting score. To be able to incrementally update the score in realtime, we use the LogSumExp trick.
Cosine similarity
Each user can be described by a vector with an entry for each file
To compute the strength of the relationship between any two users, we can use a common measurement of vector distance, cosine similarity.
Two users are considered closest collaborators if they have the same affinity scores for the same files, in which case their cosine similarity is 1. If they have never worked on any common files, then their cosine similarity is 0. This can be expressed as
Now, we can infer the strength of any two users by observing the file interactions, and the strength of that relationship naturally time-decays and updates as more events are observed.
In practice, we must decide how frequently to update all the scores. We can choose to update all the scores (user-file as well as user-user) on some intervals via a large batch computation, we can choose to update all the scores on each event, we can make some approximation, or we can use some combination of the these strategies. To update user length incrementally for each user file event along with LogSumExp we use LogDiffExp trick.
In order to capture the real-time nature of collaboration, we’ve chosen to update the scores on each event. Every time a user interacts with a file, that entry in the user vector changes, and that user’s relationship with every other user also changes. Because of the number of users Box customers have, that amount of per-event computation is not feasible. Instead, on each event, we come up with an approximation by updating the user-user scores for only a subset of the other users. Since most users have a very weak relationship to almost all other users, by judiciously choosing which scores to update, the result is a pretty good approximation of the full update. We store log value of the numerator and denominator of cosine similarity, and with few math simplifications we can incrementally update.
A/B testingTo enable A/B testing, we need to ensure that we can maintain multiple graphs with different formulas. Following depicts the setup needed for this. Primary queue can update graph with Formula A and secondary queue can update graphs with Formula B. This setup can also be used to warm up Formula B before releasing while Formula A is live.
BacktestingThere are a few choices that need to be made to implement this algorithm — we have to choose the decay rate (λ) the event weights (wᵢ) and decide how to implement our user-user update rule. We use both backtesting and A/B-testing to help us understand the impact of the parameters or algorithms. Backtesting allows us experiment with different formulas and different parameters. It gives us a mechanism to test formulas that might be much more complex than collaborative filtering and get a sense of how far behind other strategies our current choices might be.
One challenge specific to Box is that when we backtest how Graph could be used in various products to service files to users (for example, in Feed), we have to take file permissions into account — was the user allowed to see a specific file on the day we’re backtesting? File permissions at Box are fairly complex, and far too large to be denormalized. To overcome this problem and scale backtesting out to many enterprise customers, we cache a minimal amount of historical data we need to derive permissions, and developed a scheme to “batch compute” permissions on top of testing results in order to have a more accurate view of how the product will perform.
Because it’s using offline data, we can never be sure that gains made in offline testing will hold up in the product, but it has proven to be directionally relevant. In contrast, A/B testing gives us much finer grained control over what we observe. We can deploy the system with different choices of parameters and measure how users interact with features built on top of the graph. This helps us understand which parameter values are working the best and which tests to run next.
People swarm together to accomplish their work, to capture this spontaneous nature we needed realtime graph. We created high throughput realtime Box Graph by using data structures optimized for low latency retrieval of top edges and updates of edges. Taking advantage of few math tricks and processing optimizations we created formulas for collaborative filtering allowing us to compute recommendations in realtime. With backtesting and A/B testing we are able to fine tune the formula parameters for various unique use cases at Box.
Stories, projects and more from Box Engineering
104 
104 claps
104 
Written by
A new way to work. ⁣Questions about your account? @BoxSupport has your back!
Stories, projects and more from Box Engineering
Written by
A new way to work. ⁣Questions about your account? @BoxSupport has your back!
Stories, projects and more from Box Engineering
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/backup-firestore-data-to-storage-bucket-on-a-schedule-in-gcp-7d910b2dea70?source=search_post---------229,"There are currently no responses for this story.
Be the first to respond.
In this article we are going to use four features of Google Cloud Platform (in four simple steps) to backup our Firestore database to a storage bucket.
"
https://medium.com/@tiwari_nitish/minio-the-zfs-of-cloud-storage-54a73cdc3ee1?source=search_post---------37,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
Mar 19, 2018·4 min read
ZFS is best known for abstracting away the physical storage device boundaries by pooling them together. ZFS completely removed the need to manually handle physical storage or worry about their individual capacities. ZFS is also a pioneer in its ability to detect data corruption and recover if data redundancy is available.
However, as we already discussed, traditional filesystems can’t handle modern application requirements. Applications now need ways to handle unstructured data programmatically while they have very low tolerance for data loss or security lapses.
And clearly this is difficult to achieve using current filesystems. But, what if ZFS evolved into a cloud storage system? What would it look like? I am quite sure it would look something like what Minio is today. What makes me say this? Well, there are a lot of things. I will cover all those in this post.
Silent data corruption or bit rot is a serious problem faced by disk drives where data stored therein can get corrupt without the user even getting to know about it. This can happen because of things like (but not limited to) current spikes, bugs in disk firmware, phantom writes, misdirected reads/writes, DMA parity errors between the array and server memory, driver errors, and accidental overwrites.
ZFS was probably the first open source file system to provide protection against these data corruption issues. ZFS uses a Fletcher-based checksum or a SHA-256 hash throughout the file system tree, with each block of data checksummed and the value saved in the pointer to that block — rather than at the actual block itself. This checksumming continues all the way up the file system’s data hierarchy to the root node.
Now, since ZFS stores the checksum of each block in its parent block pointer, the entire pool self-validates when a block is accessed, regardless of whether it is data or metadata. The validation happens when checksum is calculated and compared with the stored checksum value of what it should be.
On similar lines, Minio provides comprehensive protection against data corruption and bit rot issues, albeit with faster hashing algorithm and an extensive data recovery mechanism based on erasure coding.
To protect against bit rot, Minio verifies the integrity of a data chunk read back from disk by computing the hash of the chunk and comparing this to the initially computed hash (at the time of storing the data). Only in the case both hashes are identical it is guaranteed that the data has not been altered in any way. In case of any difference between the hashes the block itself is discarded and has to be reconstructed from the other data and parity chunks. We’ll come back to reconstruction in a bit, let's first understand hashing in Minio.
Minio uses an optimized implementation of the Highway hashing algorithm — highwayhash, written completely in Go. It can achieve hashing speeds over to 10 GB/sec on a single core on Intel CPUs.
As you can imagine, hashing has to run on almost every data read (to verify) and data write (to save initial hash), so it is important that hashing is fast and accurate. That is why we invested time and energy in highwayhash, a perfect fit for such use cases. You can read more about it in this blog post.
An erasure code is an error correction code, which transforms a message of k symbols into a longer message (code word) with n symbols such that the original message can be recovered from a subset of the n symbols.
Minio uses Reed-Solomon code to strip objects into n/2 data and n/2 parity blocks. This means that in a 12 drive setup, an object is sharded across as 6 data and 6 parity blocks. Even if you lose as many as 5 ((n/2)–1) drive, be it parity or data, you can still reconstruct the data reliably from the remaining drives.
How does this compare to RAID?
Erasure code protects data from multiple drives failure unlike RAID or replication. For example, RAID 6 can protect against 2 drive failure whereas with Minio erasure code you can lose as many as half of the drives and still recover your data.
Further, Minio’s erasure code is at object level as each object is individually encoded with a high parity count. So, it can heal one object at a time. While, for RAID, healing can only be performed at volume level which translates into huge down time.
Additionally, Minio’s erasure coded backend is designed for operational efficiency and takes full advantage of hardware acceleration whenever available.
Another key aspect of ZFS is the ability to scale with your needs. ZFS is potentially scalable to zettabytes of storage, considering only software aspects. Minio on the other hand scales in a multi-tenant manner, where each tenant’s data is stored on a separate Minio instance. This decouples the scale to physical limits of the software. Whether you have 100s or 1000s or millions of tenants — as long as there is a single Minio instance catering to each tenant, the deployment is simple and can be scaled easily.
Not only such setup allows for easy maintainability (only few tenants down at a given time), it also keeps the deployment simple and easy to scale up and down.
However, if you prefer large deployments, Minio recently introduced large bucket support that lets you launch petabyte scale deployments from the get go. Read more here.
While you’re at it, help us understand your use case and how we can help you better! Fill out our best of Minio deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
116 
116 
116 
Minio.io
"
https://medium.com/swlh/cloud-storage-is-broken-heres-how-i-built-my-own-90b94a0fe180?source=search_post---------39,"There are currently no responses for this story.
Be the first to respond.
We’re all familiar with cloud storage services, such as Google Drive, One Drive and Amazon Drive (basically pick any word + “Drive” and you stand a chance of guessing one). In case you’re not, cloud storage (in this case primarily for individuals) is a way of storing your files and documents online. This has a plethora of advantages, such as being able to access your files…
"
https://medium.com/@alibaba-cloud/hybrid-cloud-storage-cross-cloud-backup-38a5f048db82?source=search_post---------163,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 9, 2018·10 min read
11.11 The Biggest Deals of the Year. 40% OFF on selected cloud servers with a free 100 GB data transfer! Click here to learn more.
Cloud storage has become the go-to solution for enterprise storage applications because of its reliability and security. Statistics show that AWS, Microsoft Azure, and Alibaba Cloud revenues have increased by 45.9%, 61%, and 126%, respectively. According to Gartner, IaaS continues to be the most promising growth field, with a projected growth of 28% in the next 5 years. However, major public cloud providers are not immune to accidents and disasters, which can lead to downtime in services.
In February 2017, an engineer at AWS accidentally entered an incorrect command line while trying to debug an S3 storage system in the data center located in Virginia, causing four hours of downtime. This affected many enterprise platforms including Slack, Quora, and Trello. In September, another storage accident occurred in this region (East US).
In March 2017, Microsoft Azure public cloud storage encountered availability issues for more than eight hours, during which a portion of customers located in eastern US were influenced.
In June 2018, an operation error during Alibaba Cloud maintenance caused some customers to encounter issues when they tried to access the console on the official Alibaba Cloud website and use some products.
In August 2018, Tencent Cloud, lost production data stored by several start-up companies due to a silent error resulting from a hard drive hardware bug.
Almost all major cloud providers have had similar production incidents. Does this indicate that public clouds are insecure?
Backup and disaster tolerance products/solutions are still the battlefield of traditional service providers. They provide rich products that cover a wide range of fields. Other cloud providers have relatively small input and output in this industry. In our opinion, traditional backup and disaster tolerance products have two problems:
For public cloud users, the backup and disaster tolerance ecosystems on clouds are incomplete. Even when backup and disaster tolerance software from traditional service providers are successfully deployed, it is hard to integrate them with existing resources on clouds for seamless monitoring and maintenance. Additionally, non-cloud-native backup and disaster tolerance may pose potential risks to users. Even though some backup and disaster tolerance products have been integrated into public clouds, traditional service providers may still fail to provide immediate response and support due to frequent releases and upgrades of products and features provided by public cloud providers. Therefore, users may not be able to take advantage of new features and performance improvements immediately when they are available. Finally, traditional service providers cannot implement internal coordination among various products while cloud providers can. Private cloud or hybrid cloud users also face the same problems.
Traditional backup and disaster tolerance products still target the ecosystem of traditional servers and storage. The lump-sum input in deploying one or more devices and designing solutions, both for the pay-by-node and pay-by-capacity payment models, is very costly for small and medium enterprises. The maintenance cost can even be higher than the initial input when device warranties or authorization expires.
Users’ concerns and problems are our responsibilities. In addition to improving reliability of individual products and providing maintenance guarantees, each public cloud provider has the obligation to provide cost-effective, easy-to-use, and efficient disaster tolerance solutions. More public cloud users means stronger disaster recovery needs. Hybrid Backup Recovery, Cloud Storage Gateway, and Hybrid Disaster Recovery from Alibaba Cloud’s hybrid cloud storage team can provide users with perfect disaster tolerance solutions. These products are solutions for customers needing hybrid cloud disaster recovery from local IDCs to Alibaba Cloud or cross-cloud disaster recovery (multi-cloud disaster recovery) from other cloud providers to Alibaba Cloud. This article mainly shows how the three products from Alibaba Cloud’s hybrid cloud storage team react to the cross-cloud disaster recovery (multi-cloud disaster recovery) scenario.
For users, the architecture of Hybrid Backup Recovery is very simple: clients and cloud backup warehouses. Clients are installed on hosts that need to be backed up, and the unlimited space in cloud backup warehouses are used to store backup data. For users, clients and cloud backup warehouses have the many-to-one relationship. Multiple clients and one cloud backup warehouse are connected by using public network or leased lines.
Diagram — Hybrid cloud backup cross-cloud backup architecture
This section uses two backups and one recovery example to show how Hybrid Backup Recovery backs up files on users’ hosts, backs up incremental data, and recovers user data. The section will give you an intuitive understanding of Hybrid Backup Recovery.
To give an end-to-end demonstration, we apply for a virtual machine from another domestically famous cloud provider T. The virtual machine will be used to simulate a user server. The configuration of this cloud host is shown as follows: dual-core 4 GB RAM, 50 GB system disks, 100 GB data disks, 1.5 Gbit/s Intranet bandwidth and 50Mbit/s public network bandwidth, and 64-bit CentOS 7.4; the cloud host is located in Shanghai.
Server configuration
The 100 data disks contain 33 GB data files and 13 GB server logs.
Content in the server data disks
Data files in databases
Log files
Log on to Alibaba Cloud Console, go to the Hybrid Backup Recovery page, enable the service, and create backups. Note: We recommend that you select a Hybrid Backup Recovery region that is the same as or close to the region of the backup source. So, here we also choose the “China (Shanghai)” region.
Select a region and enable Hybrid Backup Recovery
Select a region and create a backup
After creating the backup and the backup warehouse, we need to download clients and certificates. Clients will be uploaded to and installed on the backup source end, which is the cloud host that we created previously.
Complete the creation and download the client and certificate
Upload the downloaded client software to the backup source (the cloud host), decompress and install the software.
Upload and install the backup client
After the installation, open http:// host public network IP>:8011 in a browser. Note that port 8011 may be opened in the cloud host’s security group. Users need to edit the security group policy to open TCP port 8011. The backup client registration page pops up. Users need to enter the downloaded certificate (the key used to register and connect the backup source and the backup warehouse), the AK information of Alibaba Cloud accounts and users’ own client logon passwords. Because the backup client and the cloud backup warehouse are connected over a public network, we select Classic as the network type.
Backup client registration page
After registration, users can see the backup client page. This page is the portal where users create backups and recover data. We start with creating an immediate backup (an immediate backup can be perceive as one single backup that runs only once; planned backup is the user-defined schedule where backups are made regularly). Here we select Back Up Now to back up the “/server_dir” directory only once.
Create backups
When required information is submitted, the backup starts immediately. In the backup client page, users can see backup progress and other related information.
Backup progress
While a backup is in progress, many users may wonder if the backup influences the services on the backup source end. We can check CPU, memory, and network usage by using the resource monitoring information on the backup source end. As we can see, when the backup starts, CPU load doesn’t increase, and memory usage is increased by about 400 MB. The server resource usage hasn’t increased much. The network bandwidth quickly rises to the fullest, which indicates the high performance of Hybrid Backup Recovery (Note: The cloud host has only one network adapter, so Intranet traffic and public network traffic are the same. We may as well perceive this as the cloud provider’s design).
Cloud host resource usage at backup
Next, add a new directory that includes 13 GB files to the “server_dir” directory, and set a traffic limit on the backup task in the “Traffic Control” page: 24-hour traffic limit with the maximum speed of 2 MB/s. Remember to click Add to validate the traffic policy.
Add 13 GB files
Backup traffic limit
After settings are submitted, the backup task starts. We can see that the total data to be backed up is 57 GB, and very soon the progress bar displays 79%, with the speed exceeding 1.5 GB/s. This is because that the backup source directory includes 45 GB of files that have already been backed up. Hybrid Backup Recovery quickly identifies different content between two backups by using efficient comparison algorithms, and backs up added and modified files to cloud, improving backup efficiency.
Incremental backup
You might have a question as to whether the 1.56 GB/s backup speed is being restricted by the traffic limit. Let’s check if the traffic limit works. By checking resource monitoring information on the cloud host and the nload output on the host, we can easily find out that 16Mbit/s equals the 2 MB/s outbound traffic speed. CPU usage experiences a short high load period due to the computing resource consumption in the process of incremental file comparison.
Resource monitoring on the cloud host
nload output of the host
After the two backups are completed, in the Hybrid Backup Recovery page on Alibaba Cloud Console, we can clearly see the backup overview: two successful backups, total source data and data that actually uses the backup warehouse. The ratio of the original data size to the actual usage represents the compression-to-deduplication ratio. Alibaba Cloud Hybrid Backup Recovery implements highly efficient compression and deduplication algorithms with the maximum ratio of 1:30, considerably saving bandwidth consumption and backup warehouse space consumption at the time of backup.
Backup warehouse info
Finally, we will show that how Hybrid Backup Recovery recovers files across clouds (recovering files from the backup warehouse to hosts of other cloud providers). We delete both the “db_file” directory and the “server_log” directory to simulate a user data loss scenario.
Delete files
After going back to the Recover page of Hybrid Backup Recovery, we can see the two successful backup records and related information.
Hybrid Backup Recovery Recover page
Click the Recover button after the latest backup record to recover data. In the Data Recovery pop-up page, we can specify backup files that are to be recovered and destination directories to which these backup files are to be recovered. This is easy to understand. Note: Many users may enter a destination path that is the same as the backup folder and check “All files”. In this case, another “sever_dir” directory will be created under “/sever_dir/” in the actual recovery process. This doesn’t influence the recovery operation, but we need to move directories after recovery is completed.
Simple and flexible recovery policy
After clicking Submit, we can see the Data Recovery page. This page shows recovery performance, data size and number of files. Recovery performance is better than backup performance. The reason may be that the cloud provider T has a lager upper limit on the write bandwidth. We can see that recovery performance is very good. The write performance of the 100 GB cloud disks may experience a bottleneck.
Data Recovery
Similarly, users can verify the file recovery speed by checking network traffic on the cloud host.
Recovery performance
After recovery is completed, users will see the success status in the recovery page of the client.
Recovery completed
Log on to the cloud host, and we can see that the two deleted directories have been recovered. The metadata is also completely recovered.
File directories after recovery
The three preceding examples clearly show users how Hybrid Backup Recovery efficiently backs up and recovers files on demand and on schedule in hybrid cloud backup or multiple cloud scenarios. One-click client installation, registration, and backup (batch processing) support file backup scenarios on multiple cloud hosts, which is very convenient for enterprise users to protect multiple cloud hosts.
The Hybrid Backup Recovery client currently supports all versions of Windows (32 bit and 64 bit) and popular releases of Linux (32 bit and 64 bit). Support for MacOS is coming soon.
To learn more about Hybrid Backup Recovery, visit www.alibabacloud.com/product/hbr
Reference:https://www.alibabacloud.com/blog/hybrid-cloud-storage%3A-cross-cloud-backup_594134?spm=a2c41.12245042.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@dleib/i-use-1password-too-and-while-i-like-the-tool-as-b82b255a4ca6?source=search_post---------261,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Leibowitz
·Feb 19, 2021
The reason I endorse 1Password is that the company has shown genuine care about its users over the years and is incredibly aggressive with updating its apps for new features on devices soon after their release, like Touch ID unlocking, Windows Hello, and biometrics on mobile devices.
Owen Williams
I use 1Password too, and while I like the tool..as a 7 year old user, you may recall when it was not a SaaS product with a subscription fee.
I still have the one-time fee product that they used to offer, which I happily paid for. It syncs across devices using my own cloud storage keys, not the 1Password servers.
So when 1Password decided that they would retire that option for a monthly usage a few years ago, I wondered of the compelling reason to switch. There was none. And thankfully, the grandfathered product (though with no more updates), still works.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
26 claps
26 
2
Breaker of treadmills. Contributions in XBOX Mag, Forbes, CNN, OneZero & industry rags. @ retail, CPG, health/wellness, education, culture & tech.
About
Write
Help
Legal
Get the Medium app
"
https://blog.oceanprotocol.com/filecoin-storage-integrated-with-ocean-protocol-by-protofire-896dd03a1f04?source=search_post---------362,"Ocean Protocol now boasts Filecoin’s decentralized cloud storage capabilities. The proof of concept aims to deliver multiple customizable storage options and reduce expenditures on a data provider. The integration was completed by Ocean service partner Protofire, who received a grant from Filecoin for the work.
In Ocean Protocol, it is possible to tailor a storage strategy — in terms of software redundancy, the speed of data retrieval, etc. — for each data marketplace. Users can either self-host data or store it on the IPFS protocol. However, there were certain limitations across data management and configuration of storage resources. In addition, relying on third-party services capable of extending management and configuration options would result in data centralization.
Having a successful history of collaboration, Ocean Protocol partnered with Protofire to implement a storage strategy that would promote a broader range of customizable options in a decentralized way.
“The Filecoin dev grants program gives teams like Protofire the opportunity to build valuable integrations across the Web3 landscape. We’re thrilled they’ve undertaken building an integration between Ocean and IPFS/Filecoin to enable users to seamlessly store their data on a decentralized network.” — Jonathan Victor, Product and Business Development at Filecoin.
To achieve the set goals, Protofire integrated the Filecoin storage to the Ocean Protocol marketplace. Now, data providers can enjoy a greater variety of configurational options across data replication and extraction, geographic distribution of access rights, as well as the availability and size of the data stored. In addition, data publishers no longer need to rely on an intermediary service to manage storage resources. Meanwhile, data consumers are able to cut costs on a data provider.
“We believe in the power of data, decentralized data markets, and open-source technology. That is why we have already chosen Ocean Protocol as a project to partner with. Filecoin became an obvious solution when we evaluated adding it as an additional storage strategy to better suit users’ needs in dataset size, redundancy, speed of retrieval, and cost. This POC is just the beginning for Protofire. More contributions to Filecoin coming soon. Stay tuned!” — Manuel Garcia, CTO at Protofire.
“Decentralized storage is fundamental to giving people control over their data and assets. We’re pleased to have Protofire build the integration with Filecoin storage into Ocean, thankful to Filecoin for sponsoring this grant, and are happy to celebrate the growing ecosystem of tools available for developers to build Web3 DApps with Ocean Protocol.” — Bruce Pon, Founder of Ocean Protocol
To deliver even more decentralized experience, it is planned to enable payments for storage usage via a single digital wallet.
For more information, please check out the project’s GitHub repository.
Ocean Protocol is a decentralized solution for building data marketplaces with a strong focus on security. Connecting data providers and consumers, the protocol enables traceability, transparency, and trust for all the parties involved. Preventing vendor lock-in, Ocean Protocol allows for adding value to shared services and having control over data assets.
Filecoin is a decentralized marketplace for data storage and retrieval. Filecoin allows users to make use of the benefits of distributed cloud storage (privacy, high bandwidth, no central authority or point of failure) for storing and retrieving files.
Follow Ocean Protocol on Twitter, Telegram, LinkedIn, Reddit, GitHub & Newsletter for project updates and announcements. And chat directly with other developers on Gitter.
A New Data Economy
307 
307 claps
307 
Written by
A Decentralized Data Exchange Protocol to Unlock Data for AI | oceanprotocol.com | #ANewDataEconomy
A New Data Economy
Written by
A Decentralized Data Exchange Protocol to Unlock Data for AI | oceanprotocol.com | #ANewDataEconomy
A New Data Economy
"
https://blog.coinbase.com/backup-your-private-keys-on-google-drive-and-icloud-with-coinbase-wallet-3c3f3fdc86dc?source=search_post---------212,"Starting today, you can now backup an encrypted version of your Coinbase Wallet’s private keys to your personal cloud storage accounts, using either Google Drive or iCloud.
This new feature provides a safeguard for users, helping them avoid losing their funds if they lose their device or misplace their private keys.
Explore the open financial system with peace of mind
Since we launched Coinbase Wallet (formerly Toshi), our users have experienced the full power of an open financial system — storing their own funds and accessing them anywhere in the world.
But with that power comes great responsibility. The private keys generated and stored on your mobile device are the only way to access your funds on the blockchain. Owners of ‘user-controlled wallets’ like Coinbase Wallet sometimes lose their devices or fail to backup their 12 word recovery phrase in a safe place, thus losing their funds forever.
Now, with cloud backup, we give you the ability to store an encrypted copy of your recovery phrase on your personal cloud account. You will only have to remember a password, that you decide, in order to recover your funds. If you lose your device or get signed out of the app, you can easily regain access to your funds with the combination of your personal cloud account (iCloud or Google Drive) and your password.
Your backup is encrypted with AES-256-GCM encryption and accessible only by the Coinbase Wallet mobile app. The backup can only be decrypted using your password.*
Coinbase will not have access to your password or funds at any time, preserving your privacy and control. Your cloud backup provider will also not have access to your funds, as only you know the password that decrypts your encrypted recovery phrase.
The cloud backup feature currently supports iCloud on iOS devices and Google Drive on Android devices. We intend to add support for other cloud services in the future.
Cloud backup is optional and you have to opt-in to activate it. You will still have the option to view your recovery phrase and back it up manually as before. Because we offer cloud backup as a convenience, we recommend that users also backup their passphrase manually after activating the cloud backup service. For additional security, remember to activate Two-Factor Authentication on your personal Google or iCloud accounts to make those accounts harder for attackers to compromise.
How to activate cloud backup
When you update your Coinbase Wallet app to the latest version in the next few days, you will start to receive notifications to backup your private key to the cloud.
You can also enroll in cloud backup at any time from the Settings menu, by tapping on ‘Recovery Phrase’ and following the prompts.
We’re also introducing periodic notifications to remind you to backup your recovery phrase to the cloud or manually if you haven’t done so already.
Restoring your wallet
To restore your wallet on a new device or from a logged out state, just tap ‘I already have a Wallet’, enter the password you used to backup, and voila, your account is recovered! Note that your phone must be logged into the same Google account (Android) or iCloud account (iOS) that you originally used to backup your wallet.
Finally, if you juggle multiple wallets, no problem! Our cloud backup service supports multiple wallets so you can easily switch between accounts.
If you have only ever used the primary Coinbase app (or Coinbase.com), we encourage you to check out Coinbase Wallet. The Coinbase Wallet app is Coinbase’s user-controlled cryptocurrency wallet. With Coinbase.com, you can buy crypto and Coinbase stores it (along with your private keys) for you; with Coinbase Wallet, you store your own crypto (safeguarded by a private key that only you know). To learn more, visit our website.
We hope you enjoy using Coinbase Wallet. Your feedback helps us make Wallet better for everyone. You can reach us at wallet.support@coinbase.com.
Unless otherwise noted, all images provided herein are by Coinbase.
*Updated to include details on backup encryption.
Learn about working at Coinbase…
504 
13
504 claps
504 
13
Written by
Building Coinbase Wallet
Learn about working at Coinbase: https://www.coinbase.com/careers
Written by
Building Coinbase Wallet
Learn about working at Coinbase: https://www.coinbase.com/careers
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://itnext.io/upload-files-to-cloud-storage-from-wordpress-e8acc8ce70cd?source=search_post---------99,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
If you are interested in the Integrate Firebase PRO version, read the full updated documentation: https://firebase-wordpress-docs.readthedocs.io/
Demo: https://wordpress.dalenguyen.me/
Upload files from WordPress to Cloud Storage is similar to Save Data from WordPress to Firebase. Everything can be done through Contact Form 7. However, you can design your custom form to suit your needs.
Enable Cloud Storage
In the general tab, you have to add the Storage Bucket from your firebase project, then check the option for Storage under Firebase Services.
Upload Files to Cloud Storage
This is an example of Contact Form 7 from uploading images to Cloud Storage and save the path to firestore.
After submitting the file, it will be uploaded to Cloud Storage under wpImages/som-random-id path. The image name will be prefixed with a timestamp.
ITNEXT is a platform for IT developers & software engineers…
19 
19 claps
19 
Written by
Full Stack Developer / JavaScript Enthusiast/ http://dalenguyen.me
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Written by
Full Stack Developer / JavaScript Enthusiast/ http://dalenguyen.me
ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-storage-re-defined-alibaba-cloud-oss-part-1-f4c288f86f21?source=search_post---------177,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 28, 2020·5 min read
By Shantanu Kaushik
Cloud computing has redefined the way data works. The processing and delivery of data have been revolutionized over time by implementing different algorithms and assessments based on real-life challenges.
In this ever-changing industry, data storage needs have also changed considerably. It is not only about data as text, but it is also about data request-to-delivery of image and video, among others. Alibaba Cloud developed the Object Storage Service (OSS) to offer a reliable, secure, and cost-effective solution.
The Object Storage Service (OSS) has default support for Rest APIs. This enables users to store and access data from practically anywhere without using the console associated with OSS. On top of that, Alibaba Cloud guarantees 99.9999999999 % durability and 99.995% availability in the first year of operation.
The Object Storage Service was built for large data operations, such as storage and delivery. It has been modeled to work with API Gateway (API operations) and SDKs right out of the box. Objects have been well defined within the OSS framework to support different types of storage needs. If you need to migrate your storage to and from Alibaba Cloud, the OSS migration tool has been designed to work for almost every scenario.
The Object Storage Service can be easily managed using the web-based console. You can use the console to fully control your storage deployment scenarios. Alternatively, you can manage the Object Storage Service using REST APIs and software development kits using various programming languages.
The container that is used to store data within the OSS is called a bucket. Every object within the OSS is stored in a bucket and allows different attributes to be associated with different objects. Various types of buckets can be created to store data, depending on the storage class and requirements. Some of the attributes are region, ACL, and storage class.
Different storage classes are assigned to different objects or different types. As an example, let’s take up the Object Storage Service (OSS) IA storage class. This storage class is suitable to store long-lived data, but the access frequency of this data should average to once or twice per month. Similarly, the Object Storage Service Archive Storage is suitable for long-term and infrequently accessed data. On the contrary, the standard storage class provides high-performance, reliable, and highly available object storage services with support for frequent data access.
Objects are files stored within the OSS. Object has its own metadata, key, and data. The metadata of an object defines the attributes associated with an object, its size, its creation, and modification time. The key of an object is the unique name of the particular object in a bucket.
While deploying using the Object Storage Service, you can choose which data center you wish to use. In the case of multi-region deployment, regions are the physical location of the data center you select to work with or the data center that is used to cater to a specific region.
The endpoint is the domain name used to access the Object Storage Service (OSS). Different regions may have different endpoints that are used for external services provided by the OSS using HTTP REST APIs.
AccessKey pair is a combination of AccessKey ID and AccessKey Secret. It is used for identity verification and encryption services within the Object Storage Service. The AccessKey ID is used to identify a user and the AccessKey Secret is used for encrypting a signature string.
Built with the industry-leading Alibaba Cloud Elastic Compute Service (ECS), E-MapReduce is a big data processing system based on Apache Hadoop and Apache Spark that is primarily used for data analysis and processing.
Alibaba Cloud CDN stores the content as a cache and delivers it on-demand. Alibaba Cloud CDN stores the cached version on different edge nodes that are located in different regions for faster content delivery.
As the name suggests, IMG is a service that can be used to format, convert, crop, resize, and perform various other image processing operations on images stored with the Object Storage Service.
ApsaraVideo is an intelligent media processing service that can process audio and video files into a number of formats to support different devices and platforms. It uses multi-model analysis based on large amounts of data.
Based on Ruby, the OSS-Emulator is a very lightweight emulator used for testing and debugging. It simulates the OSS environment and provides support for the same APIs OSS.
It can be used to migrate TB-grade data volumes. It supports distributed deployment and can leverage multiple servers for simultaneous data migration. As the name suggests, you can import data to the OSS using this tool from any third-party source.
This automated tool can be used to generate permission policies for different OSS modules. These permissions include Identity and Access related policies that can be added as custom policies in Resource and Access Management (RAM).
The Object Storage Service makes use of this tool to attach a bucket to the local FS (File System.) You can use OSSFS to perform access and share operations with the objects. It also supports the partial upload functionality for large objects with MD5 verification for data integrity.
The OSSbrowser provides a graphical object management functionality. Using this tool, you can easily navigate and browse objects. You can also use this tool to upload and download objects with the resume functionality. Like other OSS tools, the OSSbrowser tool can be configured with RAM to create authorization policies and permission handling.
The Alibaba Cloud Object Storage Service (OSS) is a secure, available, and highly reliable storage platform that can be customized for your needs. The strong product line-up from Alibaba Cloud backs the OSS to make it a strong contender for any cloud storage needs.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/how-to-import-data-from-a-spreadsheet-into-firebase-cec55119910?source=search_post---------223,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nuha Khaled
Mar 6, 2021·6 min read
Databases usually have data that are gathered by data entry operators. It is always faster to let the data entry operator use a tool that everyone knows (e.g. Google spreadsheets) than to give them a personalized way to inject data into your database. This…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@ikai/my-last-company-does-data-operations-for-people-with-cloud-based-datasets-aa39b225a268?source=search_post---------293,"Sign in
There are currently no responses for this story.
Be the first to respond.
Ikai Lan 藍奕凱
May 25, 2016·1 min read
Felipe Hoffa
My last company does data operations for people with cloud based datasets. We found that once a client got their data into cloud storage*, BigQuery outperformed just about Redshift on every single equivalent query on non trivial data sizes. We also found that long-lived Redshift clusters started to degrade in performance if we constantly dropped and rebuilt tables; someone from Amazon just recommended we rebuild regularly. Since we had written scripts to do this, it wasn’t particularly costly, but it isn’t quite as zero-maintenance as it first seemed. Note that there’s one very real strength of Redshift that was left out of the original article: PostgreSQL compatibility. Anything you have that can connect to a PostgreSQL database can connect to Redshift, including GUIs, dashboards, etc, so you don’t have to write any new code just to do this.
I mentioned earlier(*) that the trick is moving your data into cloud storage. If your data is already in S3, it can be costly to move it into Google Cloud Storage (needed for BigQuery), and vice versa.
Disclosures:
I work on computer stuff at a company that does computer stuff too.
52 
52 
52 
I work on computer stuff at a company that does computer stuff too.
"
https://uxplanet.org/6-benefits-of-using-cloud-storage-in-design-process-66be54d3632f?source=search_post---------103,"If you’re a designer, you probably have a lot of work that needs storage. While you can store all your files on the local machine, this approach isn’t perfect for three reasons:
Over the last few years, many designers have been steadily moving away from local systems to the cloud solutions for storing their data. Dropbox provides powerful cloud storage for all your files. In this article, I want to describe a few benefits of integrating Dropbox into the daily design process.
When you’re working with teams, finding the right file can be a hard deal. In many cases, you have to search for the file in a few different places, but even after you find it, you wonder whether this is the latest version of it or not.
To prevent a situation like this, every time you start a new project, create a folder for the project in the cloud. The way you organize a folder should match your project needs. Here is an example of a directory structure you can use:
This folder should be shared with the entire product team so everyone can have access to the information. This approach is beneficial when new members join a team in the middle of the design process, and you need to onboard them quickly. New team members will have access to the latest resources right from day one.
The ability to sync your files across numerous devices and have access to them everywhere you go gives you a superpower. No matter what device you choose to use, you can always be sure that you have the latest data. Here are just a few benefits cloud storage brings to designers:
What happens with your data when you lose your device, or someone stole it? If you store your data on a local hard drive and it’s not encrypted, you can’t guarantee that it won’t be copied. But if you use Dropbox for your valuable data, you don’t need to worry about it. You can clear all the files and folders from a selected device remotely using Dropbox.
From time to time, designers face a situation when they delete important files by accident.If you accidentally delete an important file in a Dropbox folder, don’t worry — Dropbox will automatically store copies of your deleted files and folders for 30 days, and even longer for Dropbox Business users.
Overwriting a file with a new version and being not happy with it is another typical case for creatives. Dropbox also covers this scenario. It has built-in version control and saves version history for all types of files. Version history does not take up any of your available storage quota.
Designers produce a lot of files daily. And when designers run out of space, they face a dilemma — what files are important and should stay and what files are not important and can be deleted. The process of evaluating files can be a time consuming, and it becomes really painful when designers have to do it regularly. Cloud storage saves the day. By sending your files in the cloud, you can save your local hard drive space. Dropbox offers a feature called Smart Sync. This feature helps you free up space on your hard drive and keeps files safe in the cloud — easily accessible from your desktop when you need them. It also has a range of plans that will offer you the right amount of data storage — no matter how much you need.
Every design process begins with customers. The only way to create a great design is to engage clients in your design process from the very beginning. Dropbox makes it easier to share the work with clients and collect feedback from them. Every time you want to get feedback on your work, just copy the link with your asset and sent it to your client. Your clients won’t need to install apps/plugins to preview a design.
Making the design process more efficient is one of the top goals that many product teams have. Everybody wants to create products in the shortest possible amount of time without sacrificing quality. This has a lot to do with the tools designers use. Cloud storage allows designers to work without limits — it becomes much easier to exchange ideas, collect feedback, and share projects.
This is a sponsored post for Dropbox. All opinions are my own. Dropbox is not affiliated with nor endorses any other products or services mentioned.
UX Planet is a one-stop resource for everything related to…
4 
4 claps
4 
Written by
http://uxplanet.org
UX Planet is a one-stop resource for everything related to user experience.
Written by
http://uxplanet.org
UX Planet is a one-stop resource for everything related to user experience.
"
https://medium.com/hackernoon/another-way-to-find-max-partitions-36005590a2a5?source=search_post---------214,"There are currently no responses for this story.
Be the first to respond.
You’re organizing a hackathon and decide to give free cloud storage as prizes to the winners. For the prize fund, you’ve got 1024 GB of cloud space. You would be giving these gigabytes with the condition that a higher place in the hackathon gets a larger amount of space. Since you want to make as many participants happy as possible, you want to find the maximum number of places for which you’ll be awarding the prizes. That means, if you had just 8 GB available, you’d be having total 3 positions — the winner gets 5 GB, the runner up gets 2 GB and the person who came third gets 1 GB (another variation is possible — 4, 3 and 1 GBs, but the number of positions is still 3 for 8 GB).
So how do you solve this? Note that (as demonstrated in the above example) there are multiple distributions possible for a given number of positions (let’s call this number p). Indeed, this boils down to representing a number in terms of the sum of distinct smaller numbers in such a way that there are as many of these numbers as possible. For 8 gigs, we could’ve chosen the form 8 = 7 + 1 or 8 = 5 + 3, but these wouldn’t have been optimum, since 8 can be expressed as a sum of more than just a couple of numbers — as in 8 = 5 + 2 + 1. A mathematical concept which can add convenience to solving this problem is that of partitions — to quote Wikipedia, a partition of a positive integer n, also called an integer partition, is a way of writing n as a sum of positive integers. So in our case we simply want to calculate the partition of 512 which has as many numbers as possible. Let’s call this a max partition for the sake of the conversation.
In computer science, this problem falls into a certain class of problems whose solutions use greedy algorithms — procedures that make the locally optimal choice at each stage of the solution with the hope of finding a global optimum.
The “greedy” approach to solving our example is as follows: doesn’t it seem natural to start with 1 as the first summand? All that remains then is to express 7 as a max partition and add 1 to it. But now expressing 7 as a max partition has a constraint — we cannot use 1. So we use 2 and move on to represent 8 - (1+2) = 5 as a max partition. Again, for that, we can’t use 1 and 2. Neither can we use 3 or 4 because then we’ll end up using 2 and 1 again, respectively. Thus, we represent 5 as just itself and we’re done — we now have our max partition as 8 = 1 + 2 + 5. It can be shown easily that this final condition arises when the number we originally wanted to pop out (here 3) is atleast half the remaining number (here 5). I leave that part for you to figure out.
So, to put this strategy more formally — consider that we initially have two numbers n= 8 and l = 1. If n ≤ 2l, we simply represent n as itself, otherwise we pop out l and then solve the subproblem of representing n - l as a max partition such that each number in the partition is at least l+1. The value of l for this subproblem is 1 greater than that of the original problem. So for our example (n, l) of representing 8 as a max partition, we first pop out 1 and then solve the subproblem (n-l, l+1), i.e (7, 2). For this subproblem, we pop out 2 and then solve the subproblem (7-2, 2+1), that is (5, 3). Now, since 5 ≤ 2x3, we just pop out 5 and we’re done. We now just sum up the popped out numbers to get 8 = 1 + 2 + 5.
Since we’ve articulated the strategy more formally now, it’s easy to come up with a working program to solve our problem. Here’s a straightforward implementation in Python 3:
But wait — I’ve got another approach. Perhaps better. I noticed that numbers which can be represented as the sum of first n natural numbers are special. Aren’t they already in max-partition form when they show off their identity?! For example, isn’t 6 already in max-partition form when written as 6 = 1 + 2 + 3? Isn’t 10 already in max-partition form when written as 10 = 1 + 2 + 3 + 4? Let’s call such numbers “senior numbers” for the sake of this conversation. This insight forms the basis for my algorithm.
Here’s how we proceed: if the number n whose max-partition we have to find is already a senior number, we simply represent it in it’s n = 1 + 2 + 3 + … + k form. If it’s not a senior number, we still find a k which is just large enough to make the sum s = 1 + 2 + 3 + … + k greater than n (by large enough I mean k can make 1+ 2 + 3 + … + k greater than n, but cannot do the same for 1 + 2 + 3 + … + k-1). Since k is just large enough for s to be greater than n, s-n is going to be less than k. So s-n is going to be a number among 1, 2, 3, …, k-1. What if we “pluck out” s-n from the sum of first k natural numbers? That would give us s-(s-n), which is nothing but n!
By the way, the last character of the above paragraph isn’t for a factorial 🙂. Let’s visualize the idea we’ve learned so far. Usually, Ferrers diagrams are used to visualise partitions, but for our purposes, I found my custom visualization more convenient: in the below tree, the top node is the number whose max partition we wish to evaluate. The leaves are the numbers in the max-partition representation, that, of course, sum up to give the number on the top node. For a senior number, everything’s good:
For a number that’s not senior, we cut the appropriate branch so that the number on the leaf which was connected to the top node via that branch isn’t added. Consider the case of 9:
And below is the tree for 8. Notice that we’re simply finding the number k. For 8 and 9, it’s 4. Since the sum of first 4 natural numbers is 10 we first draw the tree for 10 and then replace 10 in the top node with the number we want — here it’s 8. We then cut of the branch connecting the top node to the number 10 – 8 = 2. For 9, that number was 10 - 9 = 1.
The max partition then is simply the sum of the remaining leaves. I hope you understand my algorithm by now.
One subtlety that’s remaining to be uncovered is the method to find out the “just large enough k”. But it’s a pretty straightforward calculation. The sum of first k natural numbers is n = k * (k+1) / 2. After solving this equation for a positive k, we get k = (√(1 + 8*n) – 1) / 2. Since k is going to be fractional if n wasn’t a senior number, we take the ceiling of it. That makes k large enough.
I think by now I’ve articulated this algorithm clearly. We can thus proceed to programming the solution. Here’s another straightforward implementation in Python 3:
If we do some analysis on both the algorithms, we discover that both of them run in linear time, that is O(n). However, the invisible constant hidden in O(n) is perhaps much less for optimal_summands() than for max_partition().
I did some simple checks in Python to see which method is quicker and the latter one turned out to execute more than thrice as fast as the former. I used Python’s timeit module to time both the algorithms, and here’s an instance of one of my checks on the Python interpreter:
I’ve often observed that knowing some mathematical facts allows one to develop a better algorithm, or atleast develop an algorithm faster and with more intuition. Mathematical insights can often dramatically improve the runtime of one’s programs. Math and computer science — especially the study of algorithms, are great friends!
If you know about some other factors which make the latter program work faster, please let me know in the comments. At the end, perhaps greed isn’t always good, but math is. 😀
By the way, you can split those 1024 gigabytes as:
1024 = 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 12 + 13 + 14 + 15 + 16 + 17 + 18 + 19 + 20 + 21 + 22 + 23 + 24 + 25 + 26 + 27 + 28 + 29 + 30 + 31 + 32 + 33 + 34 + 35 + 36 + 37 + 38 + 39 + 40 + 41 + 42 + 43 + 44 + 45
(Notice the missing number? Hint: it’s the sum of first 45 natural numbers - 1024.)
Of course, then you’ve got to have more than 45 participants in your hackathon! 🙂
P.S: Should I write an ArXiv paper for this?
#BlackLivesMatter
268 
268 claps
268 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Passionate. Pythonist. Perfectionist.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@alibaba-cloud/a-strategic-take-on-cloud-storage-solutions-part-1-the-overview-487bdd4d8f94?source=search_post---------186,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 30, 2020·6 min read
By Shantanu Kaushik
Cloud storage is among the prime movers of the data industry. Data storage requirements are rising by the day, with more businesses and enterprises shifting to the cloud. Alibaba Cloud has made sure the cloud storage products they offer can cater to multiple audiences and scenarios of the modern cloud.
Whether it is the Object Storage Service, the Apsara File Storage NAS, Storage Capacity Units (SCUs), or the Block Storage, every product caters to a specific industry type and is based on requirement assessment. No matter which storage service you use, optimization of your service is highly recommended. One must differentiate between the local storage on your computer and the cloud storage.
Alibaba Cloud has flexible storage solution options. The Object Storage Service practically offers unlimited storage. Choosing not to optimize your storage service can incur additional costs. Old snapshots, storage volumes, or archived data blocks should be removed or replaced with synced and updated data for an efficient storage solution.
In this article, we will discuss how you can assess and optimize your cloud storage solution.
Choosing the right solution for your business is important. Every solution has unique advantages and features to support a particular enterprise type. There are different factors to consider before selecting a storage solution:
It is essential to determine how sensitive data will be handled on the cloud storage and what measures can be taken to ensure the protection of sensitive data. Data durability has to be the prime focus while opting for a particular product. Measures have to be taken to ensure there are no gaps in sensitive data, and this data is not prone to malicious access or any accidental modifications.
It is imperative to have a fair idea of your data size. This will enable you to manage the data capacity of your cloud storage product. Storage capacity correlates with costs. Even though OSS practically offers unlimited storage, the costs will depend on the amount of data that is being traversed.
The Input/Output operations per second are an essential factor to consider while opting for an online storage service. Alibaba Cloud’s storage lineup offers different IOPS and throughput. You can easily estimate what your requirements are and opt for a suitable product.
Enterprises deploy and use multiple applications. These applications may have different requirements for data access frequency and response time. Alibaba Cloud Storage Solutions are designed to cater to the needs of any organization.
If you are storing data for extended periods, you have to make sure you optimize your storage needs and selection based on what type of data you storing. If you are storing regulated data or data that is critical for your organizational operations, you need to select a product that ensures relatable security services to protect such data. Alibaba Cloud OSS has a lineup of secure product integration to help you with such scenarios.
Alibaba Cloud has a lineup of storage solutions. The lineup consists of Storage Capacity Unit, Object Storage Service, Apsara File Storage NAS, and Elastic Block Storage.
Alibaba Cloud OSS is a highly-elastic, secure, and highly-reliable cloud storage solution for storing unstructured data. OSS has three classes of storage, Standard, Infrequent Access (IA), and Archive Type Storage. The Object Storage Service (OSS) offers the most durable and highly-available storage solution and is more cost-effective compared to other providers. The storage cost depends on the frequency of data access; the “colder” the data gets, the cheaper the solution.
Storage Capacity Units are a subscription type storage solution that offers a model to club the storage solutions within the same region. When compared to the disks purchased with ECS, SCUs offers a more flexible approach towards cost-sensitive storage subscriptions.
Storage Capacity Units (SCUs) offer three different subscription models. The standard model is the typical blue-green deployment model. In this model, the services are automatically deployed and updated depending on which subscription you choose. Microservices architecture is the basis of this model, and storage can be optimized depending on the microservices usage or deployment scenarios.
Storage Capacity Units (SCUs) offer a better model for DevOps deployment at different stages. Whether it is R&D, testing, or the deployment phase, the storage resource provided will share and enable a project storage model that can incorporate functionality from different storage products from Alibaba while simultaneously decreasing the associated costs. The ability to modify the resources depending on which phase of DevOps deployment you are going through is a storage solution that can be adjusted for cost-effectiveness.
Storage Capacity Units enable you to create and release resources for multiple projects based on the lifecycle of the product. The subscription model will ensure that you have cost-cutting scenarios at the highest level. With mobile gaming, the ECS instances used by the service are released, and new instances are purchased when the mobile game lifecycle changes. This calls for the most effective SCU deployment model.
Apsara File Storage NAS is a cloud-based service that is used for the storage of ECS instances, E-HPC nodes, compute nodes, and nodes for the Container Service for Kubernetes (ACK). The Alibaba Cloud network assisted storage solution has a distributed file system that supports NFS and SMB protocols.
Apsara File Storage NAS offers shared access, high reliability, elastic scalability, and high performance. It supports extreme NAS, NAS performance, NAS capacity, and infrequent access modes.
NAS offers many benefits:
In Part 2 of this article series, we will continue explaining Block Storage and the optimization scenarios for all the storage products from the Alibaba Storage solution lineup. We will also talk about security and monitoring solutions that apply to the storage solutions.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/mediafire/cloud-storage-services-compared-9fe1e586498a?source=search_post---------172,"There are currently no responses for this story.
Be the first to respond.
When it comes to selecting a cloud storage provider, we understand that there are many options to choose from on the market. In order to make things easier, we decided to compare MediaFire’s storage pricing and plans with our top competitors, including Dropbox, Google Drive, iCloud, Microsoft OneDrive, Bitcasa, and the Western Digital MyCloud home storage system.
As you can see from our findings below, our results are conclusive. Whether you are looking at signing up for a free account, or a paid plan, MediaFire gives you the most storage on the market for the lowest price. The pricing difference between MediaFire and our competitors is even more substantial when you look at the cost per gigabyte over a year long period.
How can MediaFire afford to offer this pricing? Simple, unlike many of our competitors, we own and operate all of our own infrastructure. This allows us to offer the most competitive cloud pricing on the market, combined with our award-winning storage and syncing applications.
For users looking to purchase cloud storage, the choice is clear: MediaFire offers the most bang for your buck.
This is the blog of MediaFire.com
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
Written by
MediaFire stores all your media and makes it available to you, anytime you want, anywhere you go, on any device you might have.
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
"
https://medium.com/@alibaba-cloud/storage-system-design-analysis-factors-affecting-nvme-ssd-performance-1-51c1de19e1d?source=search_post---------266,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 22, 2019·9 min read
By Alibaba Cloud Storage Team
The performance of non-volatile memory express solid-state drives (NVMe SSDs) is sometimes elusive. Therefore, we need to take a look inside the mysterious SSD, and analyze the factors that affect SSD performance from multiple perspectives. We also need to think about how to use storage software to optimize NVMe SSD performance to promote the use of flash memory in IDCs. This article briefly introduces SSDs, analyzes NVMe SSDs from the perspective of factors that influence performance, and then provides some thoughts on flash memory design.
The storage industry has undergone earth-shaking changes in recent years. Semiconductor storage has appeared on the scene. Semiconductor storage media have natural advantages over traditional disk storage media. They are much better than traditional disks in terms of reliability, performance, and power consumption. NVMe SSD is a commonly used semiconductor storage medium today. It uses the PCIe interface to interact with the host, which greatly improves the read/write performance and unleashes the performance of the storage medium itself. Usually, NVMe SSDs use the NAND flash storage medium internally for data storage. The medium itself has issues such as read/write asymmetry and short service life. To resolve the preceding issues, NVMe SSDs use the Flash Translation Layer (FTL) internally to provide the same interface and usage mode for upper-layer apps as a common disk does.
As shown in the above figure, the development of semiconductor storage media has greatly improved the I/O performance of computer systems. There has always been a tricky performance scissors gap between the magnetic medium-based data storage disk and the CPU. With the evolution and innovation of storage media, this performance scissors gap will disappear. From the perspective of the entire system, the I/O performance bottleneck is shifting from the backend disk to the CPU and network. As shown in the following figure, at 4 KB access granularity, the random read IOPS and write IOPS of an NVMe SSD are about 5,000 times and 1,000+ times faster than a 15k rpm HDD respectively. With the further development of non-volatile storage media, semiconductor storage media will have even better performance and I/O QoS capabilities.
The storage media revolution has brought about possibilities of storage system performance improvement, and also many challenges for the design of storage systems. Disk-oriented storage systems are no longer suitable for new storage media. We need to redesign a more rational storage software stack for new storage media to unleash their performance and avoid the new issues that they bring. Reconstructing storage software stacks and storage systems for new storage media has been a hot topic in the storage field in recent years.
When designing an NVMe SSD-based storage system, we must first be familiar with the features of NVMe SSDs, and understand the factors that affect SSD performance. In the design process, we need to use software to optimize the storage system based on the SSD features.
At present, mainstream NVMe SSDs use NAND flash as the storage medium. In recent years, the NAND flash technology has developed rapidly, mainly in two directions — increasing the storage density through 3D stacking and increasing the number of bits per cell. 3D NAND flash has become the standard for SSDs. Currently, all mainstream SSDs use this technology. NAND flash can store three bits per cell, which is commonly known as triple-level cell (TLC) NAND flash. This year, the storage density of a single cell increased by 33% to store four bits, evolving to quad-level cell (QLC) NAND flash. The continuous evolution of NAND flash has driven the increasing storage density of SSDs. As of today, a single 3.5-inch SSD can have a capacity of up to 128 TB, which is much greater than that of a common disk. The following figure shows the development and evolution process of NAND flash technology in recent years.
As can be seen from the above figure, some new non-volatile memory technologies emerged in the evolution process of NAND flash. Intel has already launched Apache Pass (AEP) memory. It is expected that the future will be the era of semiconductor storage where non-volatile memory and flash memory coexist.
From the perspective of interfaces, NVMe SSDs are not much different from common disks. They are both standard block devices in the Linux environment. Because NVMe SSDs use the latest NVMe protocol, the software stack for NVMe SSDs has been significantly simplified. A big difference between traditional SATA/SAS and NVMe is that NVMe introduces a multi-queue mechanism as shown in the following figure.
What is multi-queue technology? Data interaction between a host (x86 server) and an SSD is implemented using the “producer-consumer” queue. The Advanced Host Controller Interface (AHCI) protocol defines only one interactive queue. Accordingly, data interaction between a host and an HDD can only be implemented through one queue. This restriction also applies to data interaction between a multi-core CPU and an HDD. In the age of disk storage, one queue is sufficient because disks are slow devices. All CPU cores use one shared queue to interact with the disk, which may lead to resource contention between different CPU cores. However, the overhead introduced by such resource contention is negligible compared to disk performance. In addition, the single-queue model is advantageous in that it provides one I/O scheduler for one queue to optimize the I/O order of requests.
Compared to a disk, a semiconductor storage medium has much better performance. The AHCI protocol no longer applies, and the original assumptions have ceased to exist. In this context, the NVMe protocol came into being. The NVMe protocol replaces the AHCI protocol. The software-level processing commands have also been redefined, and the SCSI/ATA command set is no longer used. In the NVMe era, peripherals are closer to the CPU. The connection-oriented storage communication network like serial-attached SCSI (SAS) is no longer needed. Unlike previous protocols such as AHCI and SAS, the NVMe protocol is a simplified protocol specification for new storage media. With the introduction of this protocol, the storage peripherals are connected to the CPU local bus, greatly improving performance. In addition, a multi-queue design is used between the host and SSD to adapt to the development trend of multi-core CPUs. This allows each CPU core to use an independent hardware queue pair to interact with the SSD.
From a software perspective, each CPU core can create a queue pair to interact with the SSD. A queue pair consists of a submission queue and a completion queue. The CPU places commands into a submission queue, and the SSD places completions into the associated completion queue. The SSD hardware and host driver software control the head and tail pointers of queues to complete the data interaction.
Compared to a disk, the biggest change in NVMe SSDs is that the storage medium itself has changed. Currently, NVMe SSDs generally use 3D NAND flash as the storage medium. NAND flash consists of an array of internal memory cells, and uses a floating gate or a charge trap to store charges. It maintains the storage status of data based on the amount of stored charges. Because of the capacitance effect, wear, aging, and operating voltage disturbance, NAND flash inherently has a charge leakage issue that may cause stored data to change. Therefore, in essence, NAND flash is an unreliable medium because it is prone to bit flips. SSDs turn the unreliable NAND flash into a reliable data storage medium through controllers and firmware.
To build reliable storage on such an unreliable medium, a lot of work has been done inside the SSD. At the hardware level, SSDs need to resolve the frequently occurring bit flips through the error-correcting code (ECC) hardware unit. Each time data is stored, the ECC hardware unit calculates the ECC for the stored data. When reading data, the ECC hardware unit restores the corrupted bit data based on the ECC. Integrated inside the SSD controller, the ECC hardware unit represents the capabilities of the SSD controller. In the era of MLC storage, the Bose-Chaudhuri-Hocquenghem (BCH) codec technology can correct 100 bit flips in 4 KB data. In the era of TLC storage, however, the number of bit flips greatly increases. We need to use the low-density parity-check (LDPC) codec technology that has a higher error correction capability. We can use LDPC hard-decision decoding to restore data even when there are up to 550 bit flips in 4 KB data. The following figure compares the capabilities of LDPC hard-decision decoding, BCH, and LDPC soft-decision decoding. We can see that LDPC soft-decision decoding has a stronger error correction capability. Therefore, it is usually used if hard-decision decoding fails. The shortcoming of LDPC soft-decision decoding is that it increases the I/O latency.
At the software level, an FTL is designed inside an SSD. The design idea of the FTL is similar to that of the log-structured file system. The FTL records data by appending records to a log file. It uses the LBA-to-PBA address mapping table to record the data organization method. One of the biggest issues with a log-structured system is garbage collection (GC). The high I/O performance of NAND flash is compromised when it is used in the SSD because of the GC issue. There is also a serious I/O QoS issue faced by the standard NVMe SSDs currently. SSDs use the FTL to resolve the issue that NAND flash cannot perform in-place write, use the wear leveling algorithm to resolve the issue of uneven wear of NAND flash, use the data retention algorithm to resolve the issue of long-term charge leakage of NAND flash, and use data migration to resolve the issue of read diatribe. The FTL is the core technology for the large-scale application of NAND flash, and is an important component of SSDs.
NAND flash itself has many concurrent units. As shown in the above figure, a NAND flash chip consists of multiple targets, each containing multiple dies. Each die is an independent storage unit, which consists of multiple planes. Multiple planes share the same operation bus and can be combined into one unit for multi-plane concurrent operations. Each plane consists of several blocks. A block is an erase unit, and the size of a block determines the GC granularity at the SSD software level. Each block consists of multiple pages. A page is the smallest write (programming) unit, which is usually 16 KB in size. The SSD internal software (firmware) needs to make full use of these concurrent units to build a high-performance storage drive.
A common NVMe SSD has a simple physical hardware structure that consists of a large number of NAND flash memories. These NAND flash memories are controlled by the SSD controller system-on-chip (SoC). The FTL software runs inside the SoC and uses a multi-queue PCIe bus to interact with the host. To improve performance, enterprise-oriented SSDs require on-board Dynamic Random Access Memory (DRAM). DRAMs are used to cache data for better write performance, and to cache FTL mapping tables. Enterprise-oriented SSDs usually use flat mapping to improve performance, which requires more memory usage (0.1%). The memory capacity limits the development of large-capacity NVMe SSDs. A practical method to resolve this issue is to increase the sector size. A standard NVMe SSD uses 4 KB sectors. To further expand the capacity of an NVMe SSD, some vendors have used 16 KB sectors. The popularity of 16 KB sectors will accelerate the promotion of large-capacity NVMe SSDs.
Continue reading Part 2 to learn more about NVMe SSD Performance Factors and Analysis.
Reference:https://www.alibabacloud.com/blog/storage-system-design-analysis-factors-affecting-nvme-ssd-performance-1_594375?spm=a2c41.12516620.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
21 
21 claps
21 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041?source=search_post---------2,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Philipp Lies
Jul 10, 2019·3 min read
Google Colab is amazing for doing small experiments with python and machine learning. But accessing data can be tricky, especially if you need large data such as images, audio, or video files. The easiest approach is storing the data in your Google Drive and accessing it from Colab, but Google Drive tends to produce timeouts when you have a large amount of files in one folder.
More robust and scalable is Google Cloud Storage, where you can also more easily share the data with colleagues. But unfortunately there is no native way to transfer data from Google Drive to Google Cloud Storage without having to download and upload it again. However, with Google Colab we can transfer files quite easily.
Mounting your own Google Drive is fairly easy. Just import the drive tools and run the mount command. You will be asked to authenticate using a token that you create using Google Auth API. After you pasted the token your drive is mounted to the given path.
Next we need to create a Google Cloud Storage project. Go to the Resource Manager and create a new project.
After the project is created (and you need to have billing enabled, as the storage will cost you a few cents per month) click on the menu in the upper right corner and select Storage (somewhere way down the menu). Next you need to create a bucket for the data.
The name of the bucket must be globally unique, so not only for your account but for all accounts. Just be creative ;-). There you can also estimate the cost for the bucket, which is around 0.60 EUR per month for 10 GB with 10,000 uploads and 1,000,000 downloads per month.
Once your bucket is set up you can connect Colab to GCS using Google Auth API and gsutil. First you need to authenticate yourself in the same way you did for Google Drive, then you need to set your project ID before you can access your bucket(s). The project ID is shown in the Resource Manager or the URL when you manage your buckets.
This will connect to your project and list all buckets. Next you can copy data from or to GCS using gsutil cp command. Note that the content of your Google Drive is not under /content/drive directly, but in the subfolder My Drive. If you copy more than a few files, use the -m option for gsutil, as it will enable multi-threading and speed up the copy process significantly.
That’s it. Now the process is running and you can check from time to time if it’s completed. I created a Colab notebook with the example code given here: https://colab.research.google.com/drive/1Xc8E8mKC4MBvQ6Sw6akd_X5Z1cmHSNca
Further information can be found in the Colab documentation here and the gsutil documentation here.
For future projects just authenticate the Colab notebook and transfer the files from the bucket to the local file system. Then you can run all experiments on the local copy.
Machine learning and neuroscience | Coding python (and recently JS and Kotlin) | Building apps you love
See all (8)
745 
19
745 claps
745 
19
Machine learning and neuroscience | Coding python (and recently JS and Kotlin) | Building apps you love
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jboitnott/best-cloud-storage-providers-for-2020-1d09f5d68c1f?source=search_post---------160,"Sign in
There are currently no responses for this story.
Be the first to respond.
John Boitnott
Sep 25, 2020·4 min read
Online file storage is a must-have for any business or entrepreneur. With cloud backup, you can avoid file loss and restore your device if it’s ever damaged. Cloud storage also makes collaborating with team members easier since they can access files remotely. Plus, remote access means you can access files anywhere and on any device.
"
https://medium.com/google-cloud/use-cases-and-a-few-different-ways-to-get-files-into-google-cloud-storage-c8dce8f4f25a?source=search_post---------22,"There are currently no responses for this story.
Be the first to respond.
Including AppEngine with Firebase Resumable Uploads
For this article I will break down down a few different ways to interact with Google Cloud Storage (GCS). The GCP docs state the following ways to upload your data: via the UI, via the gsutil CLI tool, or via JSON API in various languages. I’ll go over a few specific use cases and approaches for the ingress options to GCS below.
1. upload form on Google App Engine (GAE) using the JSON api Use case: public upload portal (small files)2. upload form with firebase on GAE using the JSON api Use case: public upload portal (large files), uploads within mobile app3. gsutil command line integrated with scripts or schedulers like cron Use case: backups/archive, integration with scripts, migrations4. S3 / GCP compatible file management programs such as Cyberduck Use case: cloud storage management via desktop, migrations5. Cloud function (GCF) Use case: Integration, changes in buckets, HTTP requests 6. Cloud console Use case: cloud storage management via desktop, migrations
You can launch a small nodejs app on GAE for accepting smaller files directly to GCS ~20MB pretty easily. I started with the nodejs GCS sample for GAE on the GCP github account here.
This is a nice solution for integrating uploads around 20MB. Just remember the nginx servers behind GAE have a file upload limit. So if you try and upload something say around 50MB, you’ll receive an nginx error: ☹️
You can try and upload the file size limit in the js file but still the web servers behind GAE will have a limit for file uploads. So, if you plan to create an upload form on App Engine, be sure to have a file size limitation in your UI.
Since the previous example only works for smaller files, I wondered how can we solve for uploading larger files say 100MB or 1GB? I started with the nodejs app engine storage example here.
After attempting to use resumable uploads in GCS API with TUS and failing I enlisted help from my friend Nathan @ www.incline.digital to help with another approach.
With the help of Nathan we integrated resumable uploads with firebase SDK. Code can be found here https://github.com/mkahn5/gcloud-resumable-uploads.
Reference: https://firebase.google.com/docs/storage/web/upload-files
While not very elegant with no status bar or anything fancy this solution does work for uploading large files from the web. 🙌🏻
gsutil makes it easy to copy files to and from cloud storage buckets
Just make sure you have the google cloud sdk on your workstation or remote server (https://cloud.google.com/sdk/downloads), set project and authenticate and thats it.
More details here.
gsutil makes it just easy to automate backup of directories, sync changes in directories, backup database dumps, and easily integrate with apps or schedulers for scripted file uploads to GCS.
Below is the rsync cron I have for my cloud storage bucket and the html files on my blog. This way I have consistency between my GCS bucket and my GCE instances if I decide to upload a file via www or via GCS UI.
Enjoy an client ftp type experience with Cyberduck on MacOS for GCS.
Cyberduck has very nice oauth integration for connecting to the GCS API built into the interface.
After authenticating with oauth you can browse all of your buckets and upload to them via the cyberduck app. Nice option to have for moving many directories or folders into multiple buckets.
More info on CyberDuck here.
You can also configure a Google Cloud Function (GCF) to upload files to GCS from a remote or local location. This tutorial below is just for uploading files in a directory to GCS. Run the cloud function and it zips a local directory files and puts the zip into the GCS stage bucket.
Try the tutorial:https://cloud.google.com/functions/docs/tutorials/storage
You can also use cloud functions created to display bucket logs. Below shows a file uploaded via my public upload form and deleted via the console ui. This could be handy for pub/sub notifications or for reporting.
Cloud Functions can come in handy for background tasks like regular maintenance from events on your GCP infrastructure or from activity on HTTP applications. Check out the how-to guides for writing and deploying cloud functions here.
The UI works well for GCS administration. GCP even has a transfer service for files on S3 buckets on AWS or other s3 buckets elsewhere. One thing that is lacking in the portal currently would be object lifecycle management. This is nice for automated archiving to coldline cheaper object storage for infrequently accessed files or files over a certain age in buckets. For now you can only modify object lifecycle via gsutil or via API. Like most GCP features they start at the function/API level then make their way into that portal (the way it should be IMO) and I’m fine with that. I expect object lifecycle rules to be implemented into the GCP portal at some point in the future. 😃
In summary I’ve used a few GCP samples and tutorials that are available to display to different ways to get files onto GCS. GCS is flexible with many ingress options that can be integrated into systems or applications quite easily! In 2017 the use cases for object storage are abundant and GCP makes it easy to send and receive files in GCS.
Leave a comment for any interesting use cases for GCS that I may have missed or that we should explore. Thanks!
Check my blog for more updates.
Google Cloud community articles and blogs
154 
5
154 claps
154 
5
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@zeepin/how-to-become-a-galahub-node-in-galacloud-network-2fee427e49b6?source=search_post---------374,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zeepin
Aug 4, 2018·3 min read
What is GalaHub?
The distributed storage system provided by GalaCloud is composed of multiple GalaHub nodes and GalaBox nodes.
For the introduction of the GalaCloud, click on the link: https://medium.com/@zeepin/galacloud-distributive-encrypted-data-storage-network-by-zeepin-16caac88be22
GalaHub describes itself as a network server provided by applicants to configure suitable storage space and offer high-performance bandwidth. GalaHub also sees itself as a device to search addresses and as a dispatcher. During GalaHub’s communication with GalaBox, it can obtain key information of Heartbeat of GalaBox, collect service from client side and dispatch the API service.
Daily incentives
Users need to pay Gala for storing files in GalaCloud and 20% of Gala earned from this will be distributed to all GalaHub nodes. The revenue of each node will be determined based on the ammount of mortgaged ZPT. As a reward for early adopters, Zeepin will also provide an additional compensation to GalaHub nodes in the first year of operating.
In the first year, Zeepin Foundation shall compensate GalaHub nodes with Gala equivalents that are 10 times the amount of ZPT mortgage, which is capped at 500,000,000 Gala and will be distributed on a weekly basis. The start of distribution is scheduled forNovember this year.
For more details, see the Zeepin GalaCloud economic paper: https://www.zeepin.io/about
Once the selection begins, we will provide a simple interface to calculate the position on the incentive curve depending on the mortgaged amount of ZPT.
Become a GalaHub node
Join the GalaBox crowdfunding and GalaHub selection: https://galacloud.io
If you hold ZPT and wish to join GalaHub cluster, you need to submit a mortgage application through the GalaHub participation page. In order to enter the pool you need to mortgage at least 400,000 ZPT (which will be locked for 18 months). At first, 49 GalaHub nodes will be recruited and ranking will be done based on the number of mortgaged ZPT and geographical location. The community management comission will require candidates in the pool to submit relevant information to verify their authenticity. If selected you need to provide a server with suitable storage space and high-performance bandwidth in November 2018 . There will be no charge if the applicant is not selected.
If selected, the ammount of mortgaged ZPT will be send to a designated contract and be locked for 18 months. Before being selected as the GalaHub node, the applicant can increase the number of mortgaged ZPT or cancel the application but cannot reduce the number of mortgaged ZPT or his application will be cancelled.
GalaCloud plans to recruit 49 GalaHub nodes. The application will open on August 8th, 2018 and close on August 30th, 2018.
At that time, the ZEEPIN community team will release the GalaHub campaign link, so stay tuned.
For more detailed information about GalaCloud please refer to GalaCloud White paper: https://www.zeepin.io/about
Zeepin:
Website: https://www.zeepin.io/
Telegram: https://t.me/zeepin
News Channel: https://t.me/ZeepinNews
Twitter: https://twitter.com/ZeepinChain
Facebook: https://www.facebook.com/ZeepinChain/
Youtube: https://www.youtube.com/c/Zeepin
SubReddit: https://www.reddit.com/r/ZEEPIN/
Instagram: https://www.instagram.com/zeepinchain/
Linkedin: https://www.linkedin.com/company/zeepin-foundation/
Discord: https://discord.gg/YcPhNXC
Galaxy:
Website: https://cryptogalaxy.one/
Telegram: https://t.me/CryptoGalaxyOne
Twitter: https://twitter.com/TheHubGalaxy
Facebook: https://www.facebook.com/CryptoGalaxyOne/
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
218 
218 
218 
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
"
https://medium.com/@alibaba-cloud/storage-system-design-analysis-factors-affecting-nvme-ssd-performance-2-cfcc1be3ece?source=search_post---------263,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 22, 2019·17 min read
By Alibaba Cloud Storage Team
In Part 1 of the article series, we briefly discussed the evolution of storage media and then discussed in depth the technology and hardware of non-volatile memory express solid-state drives (NVMe SSDs). In Part 2, we will be analyzing the performance factors for NVMe SSDs.
The performance data provided by the NVMe SSD vendor specification is perfect. The performance comparison between the NVMe SSD and a common disk shows that NVMe SSD performance is indeed much better than disk performance. In the actual application, however, NVMe SSD performance may not be as good as expected, and it seems that the performance is not stable. The rules that make perfect NVMe SSD performance have not been found. Compared to a common disk, SSD performance is affected by many factors. To analyze SSD performance factors, we need to first understand the main parts of an SSD. As shown in the following figure, an SSD includes the following parts: host CPU, PCIe interconnection bandwidth, SSD controller and FTL software, backend NAND flash bandwidth, and NAND flash medium. The main factors affecting SSD performance can be divided into hardware, software, and other objective factors.
NAND flash
Different types of NAND flash have different performance. For example, the performance of SLC is better than that of MLC, and the performance of MLC is better than that of TLC. SSDs employing different types of NAND flash have different performance.
The count of backend channels (CE count) and bus frequency
The count of backend channels determines the count of concurrent NAND flash memories. Different SSD controllers support different channel counts. Therefore, the SSD controller type determines the backend throughput bandwidth of an SSD. The bus frequency of NAND flash channels also determines the access performance.
Processing capability of the SSD controller
The SSD controller runs complex FTL processing logic to convert read and write requests of logical blocks into NAND flash read and write requests. The requirements on the processing capability of the SSD controller are not high when data is read from or written to large blocks, but extremely high when data is read from or written to small blocks. The processing capability of the SSD controller is prone to be a performance bottleneck for the entire I/O system.
Architecture of the SSD controller
Generally, the SSD controller uses the symmetric multiprocessing (SMP) or massively parallel processing (MPP) architecture. Early controllers usually use the MPP architecture. In the MPP architecture, multiple small processors interconnect through the internal high-speed bus and communicate through the hardware message queue. Memory resources are used as separate peripherals shared by all processors. This architecture is similar to a distributed system based on message communication. The MPP architecture has a great advantage in performance, but its programming is complex. The performance scalability of the SMP architecture depends on software and its programming is as simple as that on an x86 server. The controller architecture affects the overall SSD performance. In the SSD design, we need to select different types of SSD controllers based on design goals.
Memory capacity
For better performance, the resource mapping table inside the SSD resides in the memory. The memory space occupied by the table is 0.1% of the SSD capacity. Insufficient memory capacity may cause mapping table swap-in and swap-out issues, affecting performance.
PCIe throughput bandwidth
The PCIe frontend bandwidth reflects the frontend throughput of the SSD. Currently, the NVMe SSD uses the x4 lane access mode and the maximum bandwidth allowed is 3 Gbit/s. The frontend PCIe tends to be a performance bottleneck when the backend NAND flash bandwidth and processor capability are sufficient. The NAND flash has high read performance. The read performance of an SSD is greatly limited by the PCIe bus. Therefore, the PCIe 4.0 standard needs to be rapidly advanced.
Temperature
If the NAND flash runs at full speed, a large amount of heat dissipation occurs. When the temperature rises to a certain value, the system becomes abnormal. To resolve this issue, the SSD has a temperature control system. This system monitors the temperature and adjusts SSD performance accordingly to keep the system temperature within a specified threshold. Essentially, SSD performance is compromised to reduce the temperature. Excessive high ambient temperature affects SSD performance and triggers the internal temperature control system to adjust SSD performance.
Service life
The probability of bit errors rises as the NAND flash is continuously erased and reused, resulting in the deterioration of SSD I/O performance.
Data layout
The data layout design needs to take into account concurrent units in the NAND flash, that is, how to convert I/O operations into concurrent operations of the NAND flash. For example, data layout is arranged on a multi-channel page by means of data interleaving. In this way, the sequential bandwidth can be optimized.
Dispatching during GC and wear leveling
Operations such as GC, wear leveling, and data retention result in a large amount of NAND flash backend traffic. The backend traffic directly reflects the write amplification factor of the SSD and the backend bandwidth consumption. The backend traffic generated can also be called background traffic, which directly affects the frontend performance. Therefore, the background traffic and user traffic need to be properly dispatched to achieve optimal I/O performance for users.
Overprovisioning (OP)
To resolve issues such as damaged blocks and GC, some spare resources are reserved inside the SSD. These reserved resources are also called overprovisioned resources (OP resources). More OP resources indicate less data moved during GC on average and less background traffic, which reduces the write amplification and improves user I/O performance. On the contrary, if fewer OP resources are reserved, the performance deteriorates and the write amplification increases. When the capacity of SSDs is small, a larger OP value is often specified to prolong the service life of an SSD.
Bit error
The following mechanisms are used inside the SSD to correct bit errors generated by the NAND flash: ECC, read retry, soft LDPC, and RAIN. When the bit error rate increases, software processing overhead also increases. However, if the bit error rate is controlled within a certain range, the errors can be corrected by the hardware. When software is involved in bit error correction, performance overhead becomes greater.
FTL algorithm
The FTL algorithm affects SSD performance. The FTL design and implementation vary depending on the application purpose of an SSD. Enterprise-oriented SSDs usually apply flat mapping and cache mapping tables consuming much memory space to ensure high performance. Consumer-oriented SSDs usually apply layered storage through metadata swap-in and swap-out based on pSLC+TLC to reduce cost. They can also cache metadata in the host memory space to reduce cost. These two methods, however, both affect performance.
I/O scheduling algorithm
The NAND flash has severe performance asymmetry, with millisecond-level erase and program latency and microsecond-level read latency. Consequently, how to schedule erase, program, and read operations needs to be considered during SSD backend design. In addition, the frontend I/O and backend I/O also need to be considered, and optimal performance can be achieved through I/O scheduling. In the I/O scheduling process, it is also necessary to take advantage of NAND flash features, such as program suspension, to achieve optimal SSD frontend I/O performance.
Driver software
The driver software runs on the host in kernel mode or user mode. In kernel mode, the software consumes more CPU resources and encounters frequent context switching and interruptions, so the performance is low. In user mode, the software usually uses the I/O polling mode to avoid context switching to fully improve the CPU efficiency and overall I/O performance.
I/O pattern
The I/O pattern affects the GC data layout inside the SSD, which indirectly affects the amount of data moved during the GC process and determines the backend traffic. The full sequential I/O pattern is the friendliest pattern for GC in the SSD. When an SSD uses the full sequential I/O pattern, the write amplification is close to 1 and the SSD achieves optimal performance. When an SSD uses the small block-based random I/O pattern, more data records are moved during GC and performance is significantly reduced. In practical applications, the I/O pattern needs to be optimized through the local file system to achieve optimal performance.
SSD use time
After being used for a long time, the wear of the NAND flash inside the SSD becomes worse and the bit error rate increases. There is a complete bit error recovery mechanism inside the SSD, which involves both hardware and software. When the bit error rate reaches a certain degree, the hardware mechanism fails, and software (firmware) needs to be used to restore flipped bits. However, the software mechanism causes great latency, which affects SSD performance. In some cases, SSD performance may deteriorate after the SSD is powered off for a period because a charge leakage fault has occurred on the SSD NAND flash during this period. As a result, the bit error rate increases. SSD performance is time dependent and it is essentially related to the bit error rate of the NAND flash.
Ambient temperature
The SSD has a negative feedback mechanism designed to restrict the temperature within a proper range. Based on the detected temperature, this mechanism adjusts the backend bandwidth of the NAND flash to reduce the SSD temperature. If the mechanism starts to work, the NAND flash backend bandwidth is limited, which affects I/O performance of the frontend application.
The following sections describe the impact of GC and I/O patterns on SSD performance from a software perspective.
The SSD has a very heavy software layer inside. This layer is used to resolve NAND flash issues and it records data in log-structured mode. However, the log-structured mode introduces the Garbage Collection (GC) issue. For frontend business, GC traffic is background noise. GC traffic does not exist all the time, so SSD performance greatly fluctuates. When the SSD is empty, its performance is optimal. After the SSD is used for a period, its performance greatly deteriorates. In this process, GC plays an important role. The specification released with an enterprise-oriented SSD provides the steady-state performance indicators of the SSD. During the performance test, SSD aging needs to be pre-processed. During the pre-processing, sequential data is written to the full SSD and then random data is written to the SSD twice. After the pre-processing, a random read/write test is conducted on the SSD to obtain the steady-state performance values provided by the specification. The steady-state performance values can be considered the minimum performance values of the SSD.
The above figure shows the performance comparison of multiple vendors’ SSDs in the empty and steady statuses. The difference between the performance in the steady status and empty status is huge. In the steady status, GC inside the SSD is performed at full speed, which occupies a lot of NAND flash backend bandwidth. The ratio of the background traffic to the frontend traffic reflects the write amplification factor of the SSD. The larger the write amplification factor, the more bandwidth the background traffic occupies and the worse the SSD frontend performance is. The write amplification factor is affected by many factors, such as OP and application I/O pattern. A better application I/O pattern reduces the write amplification factor and background noise, and improves the performance of frontend business. For example, if data is written into an SSD in full sequential mode, the write amplification factor is close to 1 and data traffic generated by GC is little (almost no background traffic generated). The backend bandwidth is almost fully occupied by business data traffic, so SSD performance is perfect.
GC is an important factor affecting SSD performance. In addition, GC leads to increasing write amplification and shortens the SSD service life. From a software perspective, we can optimize the I/O pattern to optimize the SSD internal GC to further improve SSD performance and prolong the service life. We should apply this optimization method to the next-generation of cheaper QLC SSDs so they can fully meet the actual business application requirements.
The I/O pattern has a serious impact on SSD performance mainly in the following aspects:
When designing a flash memory system, we need to consider the impact of the I/O pattern on system performance. We can optimize system performance through software optimization. In practical use, the full sequential I/O pattern is seldom used except for writing logs. We will be convinced that the sequential pattern can optimize SSD performance if we have an in-depth understanding of the SSD. Once we know the implementation logic inside an SSD, we can try to use an appropriate method to realize sequential write as much as possible to achieve optimal SSD performance.
The SSD records data in log-structured mode. The concurrently written I/O data is aggregated into large data blocks in chronological order. The large data blocks are then written into the NAND flash as page stripes. Multiple page stripes are written into the same GC unit (chunk or superblock). When a GC unit is full, it enters the sealed mode (read-only). Then, a new GC unit is allocated for writing subsequent data. If multiple applications concurrently write random data into an SSD in this mode, the data of these applications is interleaved and stored in the same GC unit. As shown in the following figure, the data lifecycle varies according to the application. When a GC unit needs to be recycled, a large amount of data is migrated, which causes write amplification and affects SSD performance and service life.
As above mentioned, the data records of different applications may be interleaved and stored in the same GC unit. This issue is essentially due to the interleaved storage of data of different hot and cold levels. For GC, it is best to store cold or hot data of the same level in the same GC unit. For this reason, Samsung launched the multi-stream SSD, which allows data from different applications to be stored in different stream units (GC units), thereby improving GC efficiency and reducing write amplification. The multi-stream SSD uses an explicit design that requires changes to the SSD interfaces and applications. We can achieve a similar effect by writing large blocks in the sequential I/O pattern. Suppose there is only one thread in the SSD and all applications use large data blocks to write data. In a time segment, only one application writes data into the SSD. In this way, the data stored in a GC unit becomes sequential and regular. As shown in the following figure, the cold or hot data of the same level is stored in the same GC unit after the above mentioned method is used. The data migrated during GC is greatly reduced, and the background traffic is reduced accordingly.
In practical use, the above I/O pattern is difficult to use because applications seldom generate extremely large grained requests. For this reason, we can use the high-performance Optane memory as the write cache of the SSD when designing the storage system. Write requests from different frontend applications are first written into the Optane persistent medium and aggregated into large data blocks in the medium. The large data blocks are then written into the SSD. In this way, sequential write can be achieved as much as possible to improve SSD performance and service life.
As shown in the following figure, the NAND flash has a serious read/write asymmetry. The block erase and page program latency can be much longer than the page read time. If the read operation conflicts with the erase or program operation on the same flash channel, the read operation is affected by the erase or program operation. This is an important factor affecting the read performance when the read and write operations are mixed.
In practical use, it is often found that the application test results do not match the values provided by the SSD specification, and the actual values are smaller than the values provided by the specification. The performance indicator values provided by the specification are usually calculated in the case of pure read or pure write operations. When the read and write operations are mixed, the actual values are significantly different from the values provided by the specification.
Test results show that there is a large performance gap between different SSDs when the read and write operations are mixed. When SSDs are in the steady status and applications use the random read pattern, you can find that the SSDs have different anti-disturb capabilities when a small amount of data is transferred in sequential write. In the case of write disturb, the read performance of some SSDs drops dramatically, the latency rises rapidly, and the QoS cannot be guaranteed. The following figure shows the test results of two different SSDs in the same situations. The results show that different SSDs have different anti-disturb capabilities. The read performance of SSDs that have a strong anti-write disturb capability does not drop dramatically.
Why do some SSDs have a strong anti-write disturb capability? The mystery lies in the I/O scheduler inside the SSD. The I/O scheduler schedules the write, read, and erase requests. SSDs using different scheduler algorithms have different anti-disturb capabilities. At present, many NAND flash memories support the program or erase suspension function. In the I/O scheduling process, the Suspension command can be used to suspend the program or erase requests so that read requests can be scheduled first. This reduces the read request latency to improve the read performance.
The read-write conflict is an important factor affecting the I/O QoS within SSDs. The SSD QoS can be improved through optimization of the I/O scheduler, but it is still not possible to optimize the SSD QoS through cooperation with the storage software. To achieve optimal SSD performance and QoS, we need to use open-channel technology. Open-channel is actually just a method of hierarchically dividing hardware and software. Generally speaking, the logic inside the SSD can be divided into the physical resource management layer for NAND resources and the resource mapping layer oriented to the data layout. Physical resource management can be placed inside the SSD because it is closely related to the NAND flash. The traditional NVMe SSD needs to expose the standard block device interface, so the resource mapping layer needs to be implemented inside the SSD. From an end-to-end perspective, the resource mapping layer can be combined with the storage software layer. For this purpose, the resource mapping layer is stripped from the inside of the SSD and integrated into the storage software layer. Once the resource mapping layer is stripped from the inside of the SSD, a new SSD interface needs to be defined. Open-channel is one of these interface modes.
We have done a lot of researches on SSD QoS in Apsara Distributed File System and put forward the concept of Object SSD, which is a new SSD interface mode. We use the object mode to read data from and write data into an SSD. Each object uses the append write operation mode. In this mode, the interface can be seamlessly integrated with Apsara Distributed File System. With Object SSD, a lot of work within the SSD is simplified, I/O scheduling is more flexible, and the storage software cooperates with the SSD to optimize I/O performance and maximize the QoS.
The data traffic inside the SSD is divided into two categories: frontend user data traffic and internal background data traffic. The frontend user data traffic and the background data traffic are aggregated into the NAND flash backend traffic. When there is no background data traffic, the NAND flash bandwidth is fully occupied by the user data traffic. In this case, SSD performance is optimal. When the SSD has a large write amplification, it produces a large amount of background data traffic, which occupies the NAND flash bandwidth, resulting in deterioration of the frontend user I/O performance. To stabilize the frontend I/O performance, the scheduler inside the SSD balances the frontend and background data traffic to ensure frontend performance consistency. The proportion of background data traffic reflects the write amplification factor of the SSD. From the perspective of the NAND flash bandwidth occupation, we can analyze the performance of the SSD in the steady status.
Suppose the write amplification factor is WA, the total bandwidth occupied by sequential write is B, and the user data write traffic (random write traffic) is U. Then, the background data traffic caused by GC write amplification is: (WA — 1) x U
Both read and write operations occupy bandwidth. Therefore, the total bandwidth occupied is calculated as follows:
B = 2 x (WA — 1) x U + U
Then, we can get:
U = B/[2 x (WA — 1) + 1] = B/(2 x WA — 1)
The above formula expresses the relationship between the frontend user data traffic, the total NAND flash bandwidth, and the write amplification factor.
According to the specification, the sequential write bandwidth of Intel P4500 is 1.9 Gbit/s. Based on the above formula, the frontend user data traffic in random access mode is: 1900/(2 x 4–1) = 270 Mbit/s. The IOPS is 67 KB. The result is consistent with that provided by the specification.
The following figure shows the comparison between the random write latency of Intel P4500 and Samsung PM963 and the values derived from the formula. The results are consistent.
Based on this, we can infer that the random write performance is determined by both the SSD backend bandwidth and the write amplification factor. Therefore, from the perspective of storage software, we can optimize the I/O pattern to reduce the write amplification factor and improve the random write performance of the SSD.
Flash storage technology is rapidly developing. Centered on flash storage, the flash media, SSD controller, storage system software, and storage hardware platform are all evolving. The value that flash memory brings to data storage is obvious. The use of flash memory in IDCs is a trend. NVMe SSD performance is affected by many factors. At the software level, we can optimize the I/O pattern to improve the performance of the SSD and the overall storage system.
Reference:https://www.alibabacloud.com/blog/storage-system-design-analysis-factors-affecting-nvme-ssd-performance-2_594376?spm=a2c41.12516652.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
4 
4 claps
4 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/fortknoxster/fortknoxster-encrypted-cloud-storage-is-live-get-your-private-cloud-storage-now-9f63881311c3?source=search_post---------11,"There are currently no responses for this story.
Be the first to respond.
We are very proud and excited to announce the release of FortKnoxster’s end-to-end encrypted cloud storage.
A lot of effort has been put into the design, to reach a high level of simplicity, following well-known design standards like “drag & drop”, “up- and download”, folders hierarchy and more, to easily organize and manage your files.
As always, we are proud to have hidden the complexity of the end-to-end encryption in a way that is completely transparent for our users.
Our military-grade encrypted cloud storage allows you to keep your files, photos and other documents encrypted from the moment they leave your device (end-to-end encryption), protecting and safeguarding your data at all times. Please read more about our encryption design in our whitepaper.
Unlike inbox and chat attachments, the FortKnoxster’s end-to-end encrypted cloud storage allows very large files of any types to be stored and accessed anywhere.
Soon to be released and extending the functionality of our cloud storage, is the ability to share files and folders, with friends, family, teams, business partners and others, in the most secure/private manner possible and easily manage user read and write permissions.
Users will have a limit of 1GB of FREE storage for all our features, including the FortKnoxster’s end-to-end encrypted cloud storage, inbox, chat, and any other features.
To extend the storage limit and unlock premium features, users will be able to upgrade their account and extend their storage limits using $FKX tokens. This feature will be available very soon.
This release, together with file sharing and our utility token as payment, will position FortKnoxster as one of the few in the world, to have a fully operative in-app token ecosystem.
Signup to start protecting your files now.
Download the FortKnoxster apps:
FortKnoxster for the Web
FortKnoxster for iOS
FortKnoxster for Android
Got Questions?
Join our public channels on Telegram and follow us on Bitcointalk, Twitter, Facebook, Reddit, YouTube, Medium and our website. You can also always contact us directly by email at contact@fortknoxster.com.
What happens in FortKnoxster — stays in FortKnoxster.
FortKnoxster is a cyber-security company offering secure and private communications for all.
1K 
1K claps
1K 
ABOUT US FortKnoxster is founded by skilled entrepreneurs and cyber-security experts, with an extensive experience in the field of online security and cyberdefence.
Written by
FortKnoxster is a cyber-security company offering secure and private communications for all.
ABOUT US FortKnoxster is founded by skilled entrepreneurs and cyber-security experts, with an extensive experience in the field of online security and cyberdefence.
"
https://medium.com/codechai/firebase-cloud-storage-and-flutter-fa2e91663b95?source=search_post---------9,"Sign in
There are currently no responses for this story.
Be the first to respond.
Aseem Wangoo
Feb 3, 2019·4 min read
Firebase, Cloud Storage and Flutter
All in one Flutter Resource: https://flatteredwithflutter.com/uploading-files-in-firebase-using-flutter/
Uploading….a tireless work, anyone does at any time.Why not me????
"
https://medium.com/pinknode/revolutionizing-decentralized-cloud-storage-with-our-esteemed-partner-crust-network-b1721ac6cb26?source=search_post---------113,"There are currently no responses for this story.
Be the first to respond.
The common role in building out Polkadot’s infrastructure has brought about Pinknode’s latest partnership with Crust Network, the most promising decentralized cloud storage network for Web 3.0.
About Crust’s game-changing decentralized cloud storage
Crust Network utilizes a unique combination of GPoS (Guaranteed Proof of Stake) and MPoW (Meaningful Proof of Work) to create a decentralized cloud storage market for a cloud ecosystem that prioritizes individual data sovereignty and privacy. Crust Network also supports multiple storage layer protocols, such as the Interplanetary File System (IPFS), which allows storage interfaces to be exposed to the application layer.
Crust Network’s innovation in the realm of decentralized storage has caught the attention of multiple prominent platforms, with Uniswap integrating Crust to host its frontend, and Polkadot choosing them to host the Polkadot.js extension.
Union of Infrastructure Layers
Pinknode will be supporting Crust Network by running multiple nodes to provide a secure and reliable middleware layer that caters to the needs of Crust Network’s enterprise users. API endpoints provided by Pinknode will allow wallets, exchanges, and decentralized apps (dApps) to integrate with Crust Network’s protocol seamlessly.
“Pinknode is glad to be joining hands with Crust Network to facilitate the development of Web 3.0 technology. As a pioneer in the realm of decentralized cloud storage on Polkadot, Crust brings with it an essential piece that promotes data privacy and ownership which is vital for the ecosystem. With world-class teams such as Crust Network building unique solutions on the Polkadot network, we’re confident that we will achieve new heights in the blockchain space.”
-Eric Poh, Co-Founder of Pinknode
As decentralized cloud storage continues to grow in use cases and popularity, the infrastructure that supports its system will be more crucial than ever and Pinknode is positioned to play a critical role in this impending growth.
About Pinknode
Pinknode is a Polkadot-only Infrastructure-as-a-Service project with the goal of accelerating adoption and empowering innovators to create on the most promising Web 3.0 meta protocol. By providing API endpoints through a streamlined onboarding process and maintaining secure, reliable and scalable node infrastructure, Pinknode helps innovators jumpstart development quickly and accelerates product life cycles.
If you want to know more about Pinknode, check out our website, follow us on Twitter, and join our Telegram to get in on the conversation.
The Gateway to Polkadot Ecosystem
100 
100 claps
100 
Pinknode empowers developers by providing node-as-a-service solutions, removing an entire layer of inefficiencies and complexities, and accelerating your product life cycle.
Written by
Leading infrastructure on Polkadot
Pinknode empowers developers by providing node-as-a-service solutions, removing an entire layer of inefficiencies and complexities, and accelerating your product life cycle.
"
https://medium.com/cloudflare-blog/logpush-the-easy-way-to-get-your-logs-to-your-cloud-storage-dbdbe92ee685?source=search_post---------162,"There are currently no responses for this story.
Be the first to respond.
by Filipp Nisenzoun
Today, we’re excited to announce a new way to get your logs: Logpush, a tool for uploading your logs to your cloud storage provider, such as Amazon S3 or Google Cloud Storage. It’s now available in Early Access for Enterprise domains.
We first explained Cloudflare’s logging functionality almost six years ago. Since then, the number of domains on our network has grown by ten times. We’ve continued giving our Enterprise customers the ability to download logs using a REST API, which has gotten a large number of functional and technical updates. We’ve also been paying attention to how our customers’ needs have evolved, especially as we protect and accelerate increasingly larger domains. This led to the development of Logpush.
Cloudflare works by being an intermediary between our customers’ websites, applications, and devices, and their end-users or potential attackers. As part of providing our service, we create a record of each request that goes through our network. These records (or request logs) have detailed information regarding the connecting client, our actions — including whether the request was served by the cache or blocked by our firewall — and the response from the origin web server. For Enterprise customers who request these logs, we save them for up to a week and make them available for download.Although some of our customers download their logs only when they need to investigate a problem or question, others download them regularly by writing scripts using our “Logpull” API. They may then combine their logs with data from other parts of their infrastructure, such as their application servers or marketing tracking tools. This process allows them to create analytics to see what is happening across all of their platforms; debug issues with their Cloudflare configuration or their own systems; and monitor traffic and make adjustments to improve security or performance. In fact, many download the logs and then upload them to a few common cloud services that have become popular for storage and performing analysis.
We were glad to learn that our logs were proving so useful, but having each customer write their own script simply to download and then upload them to the same few places seemed really inefficient. Couldn’t we just do that on their behalf?
So that’s the basic idea behind Logpush: rather than writing a script to repeatedly download logs using our Logpull API, simply tell us once where to send them, and we’ll push them there for you. You’ll get the exact same logs either way. If you’re already using Logpull, we’ve made transitioning to Logpush easy by keeping all the functionality the same: you can select the fields you want to receive; change the time format for timestamp fields; and even get a randomly-sampled percentage of logs. When setting up a push job using the Logpush API, you can directly copy all of the previous options you had set in Logpull using the `logpull_options` parameter. We also provide a Logpush UI in our Analytics tab that walks you through the setup for a domain. Full documentation on all of our logging products is available in our new developer documentation section.
Logpush currently works with Amazon S3 and Google Cloud Storage, two of the most popular cloud storage providers. As you may already know, we’re big proponents of working with many cloud services, so more popular destinations are coming soon. Want to help us decide where to push next? Take this survey. Interested in helping us build? We’re hiring Systems Engineers with an interest in data in San Francisco, London, and Austin.
Originally published at blog.cloudflare.com on February 25, 2019.
Select highlights from blog.cloudflare.com
Select highlights from blog.cloudflare.com
Written by
The Official Account of Cloudflare
Select highlights from blog.cloudflare.com
"
https://medium.com/google-cloud/migrating-a-cpanel-web-hosting-server-to-google-cloud-acdcf7c95768?source=search_post---------244,"There are currently no responses for this story.
Be the first to respond.
Using Google Cloud Storage, Google Compute Engine, and Cloud DNS to host static and dynamic websites from an old cPanel hosting server.
In this article I will show how to move static websites and dynamic applications on a cPanel server to Google Cloud Platform (GCP). My goal is to reduce my personal hosting cost, move sites to a secure, updated environment, and migrate away from my current Linode cPanel VPS to GCP.
Since migrating away from cPanel is typically a manual task I will focus on domain public_html directories and manually migrate any DBs by hand. At this step I’ll determine what staying and going. On my source cPanel server I will look for all active domains.
Now is a great time to start thinking about shutting down sites or what you can archive and discontinue. In my case none of these sites are generating any income but they are sites I’ve had for years and I am not ready to get rid of them yet.
This exercise helps you prioritize and organize your migration. I’m taking stock on what purpose the sites serve, determine those I would like to retain, and where they best fit on GCP. I used a template like the one below to help me walk through and plan out my migration:
I will be using the Google Cloud SDK to transfer files securely so first up I’ll need to install the SDK on my source cPanel server. Installing the Google Cloud SDK.
I had a few issues with my out of date cPanel server and the gcloud SDK install. Since gcloud SDK requires Python 2.7 or 3+ and I had 2.6, I had to fix the following:
Outdated python (2.6)Fix: Install python27
Unable to gcloud auth login — Issues with sqlite
Fix: install sqlite and recompile python2.7
My python2.7 in located in /usr/local/bin
Set env variable to let cloud SDK know where Python 2.7 is located
After fixing the above points I was able to successfully gcloud auth login and gcloud init to authenticate to GCP and select the project I want to use for this migration.
Since I am only transferring 6 sites from my Linode cPanel server I can easily examine each home dir to see what i’m working with. I’ll start by checking how big the public_html directory is and if there is anything I can clear out to make the transfer quicker.
Here I am looking for anything that is GBs+ and may take a while to transfer.
For every site that we plan to host on a Google Cloud Storage bucket I’ll need to verify domain ownership by modifying the DNS zone. If you do not verify your domain before creating a domain named bucket you’ll receive the following error:
Follow the docs here Domain-named bucket verification. You can either do this ahead or during your migration. If you have never done this before you’ll need access to the location where DNS is hosted for your domain to add a TXT record or you’ll need to upload a file to the root directory.
Make sure you are an owner of the site in Webmaster central if you plan to use domains with Google Services. After you verify your domain with a HTML file upload or TXT file DNS zone edit you can then use the domain with Google services such as Cloud Storage.
If you are using nameservers (maybe currently pointing to a cPanel server) you will need to change DNS server settings from Custom to registrar servers so you can modify the DNS zone or modify the zone on the hosting server.
Google Cloud Storage buckets are a great solution for static basic HTML websites because its low cost and scales with no effort. Just note that you are moving hosting websites on a web server to being served by Google Cloud Storage. A few things I noticed that are different:
Issue: No HTTPS support
Solution: Cloud Storage only supports HTTP via CNAME. If you want to serve you content via HTTPS you will need to use a load balancer or use Firebase hosting instead of Cloud Storage.
Issue: Web servers can serve directory contents without an index page, example: www.website.com/files/ would return a page from the web server listing all of the files in /files/.
Example, one of these:
Cloud Storage does not support public directory listing.
Solution: All of the directory list scripts I have found are in PHP which will not work using GCS for hosting. Use the Cloud Storage browser instead to browse your files.
Solution 2 : Use Cloud Storage Fuse to mount your bucket as a file system to browse your files. Now you can view/upload/download files in your bucket locally instead of via a web server directory.
PHP and dynamic sites will need to be hosted on Google Cloud services with a web server.
Starting the migration with one of your lower priority not critical domains will get you familiar with the transfer process. Since 5 out of my 6 domains I plan to land on Google Cloud Storage I will go ahead and test moving a static site first. I’ll use this doc Hosting a static website.
Depending on the criticality of your site you may want to setup the cloud storage bucket and move files first. In an enterprise / business setting you would likely move files first. I will follow the docs for this walkthrough since the sites I am moving are low priority.
Create a CNAME at the registrar DNS zone for your domain to point to c.storage.googleapis.com.
In my case for one of my domains:Hostname: www.jordanaandmike.usRecord type: CNAMEAddress: c.storage.googleapis.com.
Here is how my DNS zone looks after the CNAME update, URL redirect (for root domain), and the domain ownership TXT record addition for verification.
Note: the www host CNAME and the URL Redirect are required for your domain to resolve via www.yourdomain.com (CNAME) and yourdomain.com (URL Redirect). If you want the root domain to resolve (non www, just domain.com) make sure you set a root domain alias, sometimes referred to as an ANAME or ALIAS. More on root domain and CNAME in this lovely article from Dominic Fraser here.
Assuming you have already verified your domain name you can create a domain-named bucket. If not, follow the domain ownership verification docs here.
This part sets your index page file and what is served when someone reaches the domain name. Without this gsutil web set command only a xml file will be returned when accessing the domain. We’ll also set 404 page for when a page is not found.
https://toolbox.googleapps.com/apps/dig/#CNAME/
Check another workstation / location if you are not seeing changes on your end. Dont panic just give it time.
Read through the static website examples and tips for additional configuration options.
Note the Cloud Storage caches files so if you are making rapid changes on an html page, it may take some time for those changes to reflect.
Use the cPanel script /script/removeacct username to remove the account on the source server once the domain resolves to Cloud Storage.
I will repeat steps 1–9 for my other domains that are planned for Google Cloud Storage.
So my goal is to migrate my cPanel server to Google Cloud for the lowest cost possible. For my php low traffic site I will transfer to a web server and mysql server on a Compute Engine instance. I could host the DB on other services such as Cloud SQL, but I am looking for the lowest cost possible.
I will be using GCS as my staging area for my transfer. So, I will transfer from source -> GCS -> destination.
Note that I am not creating a domain-named bucket (as I did for static sites) here as I am just transferring to GCS as a middle tier between my transfer. I could have used SFTP or transferred another way, but since I was already using GCS I figure I will continue. Plus this will also give me a backup of the site that I can put on the cold-line storage tier and pay pennies for a year.
Backup source DB and copy to staging bucket
I will use the LAMP stack Google click to deploy image as it will have everything I need pre-configured and ready to go. This is a Google click to deploy image from the marketplace so I can trust it.
The httpd root directory is /var/www/html/ in this debian image so I will copy files here.
Make sure to add a * at the end of your bucket name to copy the files to your source directory. If you leave off the * it will create a directory with the bucket name and you’ll have to move all files and directories by hand.
The mysql root password for the LAMP Google click to deploy image can be found in Deployment Managers deployments in the Google Cloud Console.
So my source server had php5.4 and my destination php7. The site I want to move will not work on php7 and a lot of changes would be needed to get it to work. I’m a decent sys admin but not a decent php dev. I’m now at a crossroads — I could modify my website code for php7 or downgrade the default php version on my Debian click to deploy LAMP server and get php5 up and running. Since I am trying to do this in the shortest amount of time and cost possible I will run php5.6 on my destination server even though it is EOL. I am aware of the risks of doing this and doing it to get my site up and running asap.
I followed this Cloudwafer article on how to run multiple php versions on Debian.
After downgrading php on my destination server I was still unable to get php pages to load.
Checking the apache2 error log in /var/log/apache2/error.log and in GCP logging I kept seeing segmentation faults come through whenever trying to load a php page:
After searching segmentation faults in php are typically related to a php module. So I decided it was best to compare the php modules and php.ini setting in my source and destination.
I did a quick compare of the php -m output from my source and destination in Google sheets:
Using phpdismod I removed all of the modules that were not on my source server:
After making sure that the php -m matches on my source and destination I continued to monitor the error_log and troubleshoot. I was about to try going back to php7 with this article and modifying my applications code then came across this error when trying to disable php5.6:
After modifying my modules I may have forgotten to reload apache2 configuration. The error messages reminded me to reload apache2. After a systemctl reload apache2 my php5.6 configuration was working for my php application. Horray. Heh!
This is a great time to take a backup (snapshot) of my working instance configuration. This way if there are any issues with this server I can restore this last working version.
While we are creating a backup lets setup alerting. Since this site is hosted on a single server and not redundant with a GLB or managed instance group if it goes down I’d like to know.
Cloud DNS costs me about $0.21 cents per zone with my existing DNS zone so I will point my domain name to Cloud DNS and setup the zone to point to my Compute Engine instance via an a record.
.htpasswd
My 2015 php site has an admin area that uses a .passwd file for authentication. By only migrating the public_html directory I missed this. So I needed to recreate for my admin area to work.
Directory ownership
My application creates files with user inputs. During my migration all of my application files were created owned by root. In order for my application to work I needed to change two directories to be owned by www-data
My cost at Linode was: $38.50/month
Additional IPv4 Address MK_Personal (144500) 2020–05–01 04:00:00 2020–06–01 03:59:59 0.0015 $1.00 $0.00 $1.00Linode 6GB MK_Personal (144500) 2020–05–01 04:00:00 2020–06–01 03:59:59 0.05 $30.00 $0.00 $30.00Backup Service Linode 6GB (pending upgrade) MK_Personal (144500) 2020–05–01 04:00:00 2020–06–01 03:59:59 0.012 $7.50 $0.00 $7.50
I decided not to renew my cPanel license which was $20/month for 5 domains.
So to run these few websites on a VPS with cPanel I was paying around $58/month.
I will update this article in a month to compare costs with a full month running on GCP. I estimate it will be around $40/month or about $20/month less.
Thanks for reading!
Google Cloud community articles and blogs
16 
1
16 claps
16 
1
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Customer Engineer, Google Cloud. All views and opinions are my own. @mkahn5
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/announcing-early-access-for-tardigrade-c209d41a0b47?source=search_post---------290,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Mar 2, 2020·5 min read
Today our team is thrilled to announce our Tardigrade decentralized cloud storage service is finally ready for production workloads — we can’t wait for you to try it out. We’re welcoming the first paying customers to experience the advantages of decentralized cloud storage with an early access release (RC 1.0). We expect a complete production launch this quarter, at which time we’ll remove the waitlist and open user registration to all.
With this release, Tardigrade users can expect:
If you haven’t yet, sign up now to get your credit before time runs out! If you’ve already joined the waitlist, check your inbox for an email with details on how to get started. If you’re already a Tardigrade user, first, thank you very much and second, your account will be credited with 1TB of storage and bandwidth after you add a STORJ token balance or a credit card. Users who have both a credit card and a STORJ token balance will first see their STORJ token balance charged, with their credit card as the secondary method of payment. Even after users exhaust their credits, they’ll still pay less than half the price of traditional cloud storage for their usage.
Over the past six months, our team has quietly been gathering feedback from customers pilots and POCs, as well as data from network stress tests. We’re confident our first initial partners and customers, as well as users who are joining from the waitlist, will have a positive experience trying out Tardigrade. As an extra measure to ensure the quality of that experience, we’re being extremely vigilant in balancing the network in terms of supply and demand. Over the coming weeks, we’ll continue to gather data and feedback from our initial customers. Once we’re fully confident we can scale that quality experience to meet the anticipated demand, we’ll announce general availability, remove the waitlist, and allow anyone to sign up to experience Tardigrade first-hand.
Between now and production not much will change in the system, other than a significant increase in data to be uploaded by our first paying customers.
We’ve previously talked about the qualification gates we use to ensure the product is ready to support our customers’ cloud storage needs and deliver a solid user experience, both of which are critical to drive adoption of the platform. We established these gates to guarantee that we delivered not only an enterprise-grade technology solution but also a world-class customer experience. Since establishing these launch gates, we’ve continuously monitored and reported upon our progress. At every major milestone, we’ve evaluated our progress toward the gates, and the applicability and validity of those gates. As part of this new early access phase, we’ve added an additional qualification gate, which is to deliver two weeks of success with real-world users (and their data).
To date, we’ve welcomed 10,000+ Tardigrade waitlist members to the platform. There is more than 4 PB (4,000 TB) of data stored with 20 PB of available capacity. Thousands of developers have created accounts, projects, and uploaded data to the network. In addition to these developers, we’ve been working with a number of large, notable partners and customers who have large-scale use cases to prove that decentralized cloud storage is the right solution for their needs. We’ll be sharing more about these customer’s use cases in the coming months.
During this early access phase, we’ll provide an extra level of support to early customers and developers, gather further feedback, and continue to refine the onboarding process. We’re doing this so that when we actually move forward with mass adoption, we’ll be ready — and so will Tardigrade.
Once we announce general availability, users will be able to sign up directly on Tardigrade.io, after which they’ll receive immediate confirmation for their account. During early access, we’ll be sending out invites once a week to continue to throttle new user registrations.
As mentioned before, those users that do have access will have their limits raised to 1TB for both bandwidth and storage after they add a method of payment. During general availability, limits will start at 5GB once a credit card is added, and limits will increase from there. So sign up now to reserve your higher limit for our production launch.
Our Storage Node Operators won’t experience much of a difference between early access and general availability other than a steady increase in the amount of customer data and bandwidth utilization on the network. We have a lot of upcoming features planned for the storage nodes including SNO board enhancements, configuration tools and improvements to graceful exit . We have some exciting news to share about our team building out features for Storage Node Operators on the Q1 2020 Town Hall, so make sure to tune in.
After our general availability release, we’ll share more information about our 2020 roadmap, but we’ve been very impressed by the amount of feedback we’ve received through the ideas portal and on the forum — we’ve actually incorporated many of the suggestions into our plans. If you have additional suggestions, please share them. We review every single suggestion. You can also see other ideas that have been submitted and the status of suggestions.
We want to give a HUGE thank you to our community of amazing Tardigrade users and Storage Node Operators for all your contributions to the network. We literally couldn’t build the decentralized future without you and your efforts.
By John Gleeson and JT Olio on Business
Originally published at https://storj.io on January 30, 2020
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
50 
50 
50 
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
"
https://medium.com/curious/which-cloud-storage-service-will-most-boost-your-productivity-739b8fb39b59?source=search_post---------85,"There are currently no responses for this story.
Be the first to respond.
Note: I have no links to any of the suppliers mentioned.
Online storage once was about gigabytes for bucks, but — oh look — service providers are throwing in…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/uploading-resizing-and-serving-images-with-google-cloud-platform-ca9631a2c556?source=search_post---------333,"There are currently no responses for this story.
Be the first to respond.
More and more applications today require their users to upload photos. From simple things like updating a profile photo, to more complicated services like Snapchat where you share a bunch of them. These applications are running on all kind of devices with different resolutions and at different network conditions. In order to do it right we have to deliver the images as fast as possible, with the best available quality, taking into account the targeted device and screen resolution.
One option is to create an image service yourself, where you can upload the images, store them and possibly resize them in few different sizes. Unfortunately, doing so is usually very costly in terms of CPU, storage, bandwidth and can end up very pricy. It is also quite a complicated task and many things can go wrong.
By using Google App Engine and Google Cloud Storage though, you can easily achieve this seemingly difficult task by using their API. Start by completing a simple tutorial on how to upload files into the cloud and read the rest if you want to see it in action to understand why it’s one of the coolest things ever.
App Engine API has a very useful function to extract a magic URL for serving the images when uploaded into the Cloud Storage:
get_serving_url()
Returns a URL that serves the image in a format that allows dynamic resizing and cropping, so you don’t need to store different image sizes on the server. Images are served with low latency from a highly optimized, cookieless infrastructure.
In practice it’s the same infrastructure that Google is using for their own services like Google Photos. The magic URLs usually have the following form: http://lh3.googleusercontent.com/93u...DQg
If all that wasn’t enough for what’s coming out of the box, there are also no charges for resizing the images and caching them when using that Google’s magic URL. Yes, you read it correctly, this is a free of charge service and you pay only for the actual storage of the original image.
You don’t have to worry about anything when it comes to serving images for your next big idea today. All you need to do is to upload your images once, extract the magic URL and then use it directly on the client-side by updating the arguments depending on the environment. Prototyping applications similar to Snapchat or even more complicated ones could be implemented over a weekend.
Besides the already mentioned tutorial from the documentation, you can play with a live example on gae-init-upload (which is based on the open source project gae-init).
The photo in the example is taken by Aleksandra Kiebdoj.
Google Cloud community articles and blogs
713 
20
713 claps
713 
20
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
The world chico.. and everything in it...
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@CabeFSAtwell/take-advantage-of-distributed-cloud-storage-with-cubbit-357b4498ade4?source=search_post---------47,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cabe Atwell
Feb 28, 2019·2 min read
Cubbit has launched their Kickstarter campaign to bring its distributed cloud storage device to the market, which will enable you to store data free of charge. Cubbit offers nearly the same user experience as Dropbox, but runs on a distributed data center made up of Cells (nodes). Users simply connect the Cell to a router, which then becomes attached to the p2p network and 1Tb of storage becomes immediately available.
If more storage is needed, you can connect several USB-based hard drives, which become part of the distributed cloud. For every 2Gb of physical storage, 1Gb is accredited to your account. Up to 4Tb can be allocated via each Cell, and each Cell can be stacked together for as much storage as you need, and all of it free with no monthly subscription. What’s more, each Cell provides four independent accounts that can be shared (or not) with others, each with their own files and folders.
On the hardware side, the Cubbit Cell packs a dual-core Arm Cortex-A53 processor (up to 1.2GHz), 512Mb or 1Gb of DDR4 RAM, a USB 3 port, and an Ethernet port. Security is handled with AES-256 encryption and uploaded files are split into dozens of chunks with each piece distributed across the network of Cells, so your data remains safe and secure.
Cubbit claims their architecture is 10 times ‘greener’ over traditional cloud services that host data on servers, which operate 24/7 and need to be cooled to maintain stable function. The startup also states that since their Cell Distributed Cloud Storage is handled by everyone that use the p2p service, the world saves 100-tons of CO2 a year for every terabyte. For every 4Tb used, Cubbit saves the same amount of energy consumed by a standard refrigerator. The Cubbit Cell is now available with pledges starting at $340 and up.
517 
517 claps
517 
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@openbom/openbom-cad-integrations-connected-cloud-is-expanding-to-microsoft-onedrive-75302f239c8c?source=search_post---------305,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Dec 20, 2017·2 min read
Cloud storage is popular these days. I’m pretty sure you’re using at least one of many available cloud file storages at work to store documents and related information. In the previous OpenBOM update (here is a generic version) we introduced support for cloud storage in OpenBOM CAD plug-ins such as Solidworks, Autodesk Inventor, and Solid Edge. Read more in our press release, here.
In the past few weeks, we’ve received a number of requests from our users for OpenBOM to also support OneDrive, the Microsoft Cloud and file storage service.Today, I want to show you how the OpenBOM CAD plug-in also integrates with Microsoft OneDrive. The following video shows both Solidworks and Siemens Solid Edge uploading files and connecting links to OpenBOM from OneDrive.
Note — OpenBOM is uploading native 3D files as well as PDF (2D and 3D) files automatically generated from CAD system.
Conclusion. We believe in information linking. This is one of the most fundamental elements of the OpenBOM vision. In the modern manufacturing world, there are many systems and elements of information that must be connected together. OpenBOM is connecting information dots by allowing you to create a BOM and connecting together all needed information together.
Tell me about your cloud experience and BOM management requirements combined with CAD design.
PS. Microsoft OneDrive integration you’ve seen in these videos will be available after the next OpenBOM product release update.
Best, Oleg @ openbom dot com
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Want to learn more about PLM? Check out my Beyond PLM blog and PLM Book website.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
"
https://medium.com/bb-tutorials-and-thoughts/how-to-host-a-next-js-static-website-with-gcp-cloud-storage-2bc481b9cef8?source=search_post---------88,"There are currently no responses for this story.
Be the first to respond.
Next.js is the React framework for static websites, the JAMstack, Production, Desktop, Lightweight Apps, Pre-rendered apps Mobile Web, etc. There are so many advantages using Next.js for your react application such as Pre-Rendering made easier, Exporting a static website with a single…
"
https://medium.com/@isaackimutai/ubbey-the-future-of-cloud-storage-dc764f1bf2bc?source=search_post---------87,"Sign in
There are currently no responses for this story.
Be the first to respond.
kimutai
Jul 14, 2018·6 min read
Since time in memorial, there has been need to store data in day to day life. Factors such as security of data, cost, availability and compatibility have driven evolution of data storage and currently we have cloud storage. Enterprises and businesses today are embracing cloud storage in data management and there has been need to have efficient and best cloud storage systems and platforms. Choosing the best cloud data storage provider that knows your exact business requirements can make life simpler for you and contrarily failure to choose the right data storage provider is a total failure because it will place you at a disadvantage with respect to your competitors
Having a cloud storage comes with many benefits if and only if one could avoid the common problems. It becomes tricky when trying to select the right cloud storage provider before giving up your data for cloud storage and it is therefore important to consider the following:
Universal Labs has developed a decentralized blockchain network platform that is called Ubbey. Ubbey network refers to the decentralized network that is powered by Universal Labs and permits its users to do exchange of digital and physical assets in a peer-to-peer way. These company allows an asset to be transferred from one Internet Protocol to another once the asset has been registered on their protocol. This is completely different from the traditional cloud storage because no information about the user is collected. The company also ensures that the users’ files are encrypted and distributed across its decentralized network and only permits the user alone to have access to their data and files.
The Ubbey Network is keen to develop the worlds’ top rated decentralized cloud storage network where users are given total control of their files while enjoying the cheapest pricing tied together with personal ownership, maximum privacy and enterprise grade security. Currently individuals are creating huge amounts of data (ranging from new photos and videos, new audio recording from lectures and conferences and new documents from work or school) each day just on their mobile phones and computers. The challenging issue is that all this information is enormous and these devices simply cannot accommodate the increasing volumes of information each day. These individuals are left with an option of cloud storage which has been accepted despite the hitches in security are intimidating the transparency of these services especially when we consider the cost users have to pay in order to sustain their accounts.
The following is the functionalities of the UBBEY account system. They are the basis of a starting point for the development of the UBBEY account system:
Ubbey box is a hardware device developed by Universal Labs and may be used in place of a Personal storage for individuals or group of people and avails participants in the ecosystem with huge data storage space on its platform. It permits the users to store data and backup the same data on one place and also allows them to access the file from any place and at any time.
Ubbey box is definitely the answer to offline storage and cloud storage. It is actually the first storage that provides solutions to both online and offline storage challenges. It provides users with a huge space of storage ranging from 1TB to 2TB options available in its initial version.
The Ubbey token is known as You tokens. These tokens have several uses which include:
The company intends to release a total of 10,000,000,000 YOU coins into the public. The following is the distribution of the tokens:
Presale took place in June and Public sale is still ongoing.
By coming up with a product that will solve people’s vast storage needs and offer something more than just storage, Ubbey network has set itself up for huge success. Having a product out already inform of Ubbey box (can be pre odered here http://ulabs.technology/pre-order) gives it more credibility as the public can experiment on it live. The network and platform also has a team of experts and developers leading the team and experinenced team of advisors too.
website twitter telegram whitepaper Intro Video
Blog by: Isaac Kimutai Bitcointalk: https://bitcointalk.org/index.php?action=profile;u=2211010
Telegram: @Isaac_kimutai
Facebook: https://web.facebook.com/isaac.kimtai.16
Steemit: https://steemit.com/@kimutai/
Medium: https://medium.com/@isaackimutai
Disclaimer: This blog is Part of the ARTICLE GROUP
blockchain cryptocurrency ico data storage
Authors get paid when people like you upvote their post.If you enjoyed what you read here, create your account today and start earning FREE STEEM!
Originally published at steemit.com on July 13, 2018.
Cryptocurrency Investor/promoter (Digital marketing is my thing, creation of meaningful and informative content is my strength)
127 
1
127 
127 
1
Cryptocurrency Investor/promoter (Digital marketing is my thing, creation of meaningful and informative content is my strength)
"
https://medium.com/@alibaba-cloud/security-and-monitoring-practices-alibaba-cloud-storage-solutions-part-3-47d3bcc1a2a9?source=search_post---------184,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 11, 2021·6 min read
By Shantanu Kaushik
Cloud storage plays an important role in every aspect of cloud computing. Storage is the basis of any successful application deployment, database solution, information sharing, or any other cloud storage-based scenario. In the previous article of this series, we discussed the need for proper storage optimization and the cloud monitoring solution by Alibaba Cloud.
Optimizations enroll the need for Cloud Monitor to gather important metrics and make adjustments to your cloud service for a better and more productive event cycle. In this article, we will continue discussing error diagnosis, using network logging tools, E2E tracking/diagnosis, troubleshooting, and OSS sandboxing.
Storage errors could boil down to a large-scale outage. To prevent this, you need to be as proactive as possible with error diagnosis, including server return with error information to the client when the client-side applications are faulty. Alibaba Cloud Monitor records these errors and displays detailed reports to outline and showcase the complete error information when required and requested.
You can always retrieve the information for specific errors by filtering for the required data type. You can also include individual requests from server log, network log, and client log. The best way to diagnose an error is to combine the error information, including the Object Storage Service (OSS) error code, HTTS status code, and OSS error detail.
You can read more about OSS error responses here. This will give you a detailed explanation of error codes and possible causes.
The Object Storage Service (OSS) provides server and network logging. You can log user requests to track end-to-end requests and the details associated with these requests. Various issues can be backtracked using the network logging function. You can retrieve the user application data, storage, and network logs. These logs can provide detailed information to assist with diagnosing issues related to traffic conditions between the server and the client.
An end-to-end process starts with a client request that is processed over a network and sent to the Alibaba Cloud OSS server for processing and execution. This end-to-end tracking process ensures the diagnosis of potential problems and troubleshooting.
Let’s outline some common performance issues and their solutions:
1. Low End-to-End Latency (Average)
There could be an issue where end-to-end latency is low, but the client request latency is too high. In this scenario, the client-side will experience request delay. This could be due to:
2. High Average End-to-End Latency
High end-to-end latency may be caused by the slow performance of client applications. This could be the result of slow network speeds or a limited number of available connections.
Check to see if there are a large number of connections in the using TIME_WAIT. If there are, you can adjust the core parameters to overcome this issue. Alternatively, when the number of available connections is insufficient, you need to address the parameters that might be affecting the client CPU, memory, and network resources. You should increase the number of threads or connections to overcome this issue.
You can also try and optimize the client-side application and adjust the application configuration to implement an asynchronous access method. You can analyze the application performance and optimize it if necessary.
System monitoring is a vital tool to find a cause and fix it. You can analyze and pinpoint the resource on the client-side that is causing the bottlenecks. The second step would be to optimize the resource usage or increase client resources, such as CPU, memory, and network bandwidth.
High end-to-end latency due to network factors is generally short-lived. You can use tools like Wireshark to investigate these network problems and fix them.
If you notice a sudden spike in storage capacity and you cannot find a valid reason, such as an increase in upload requests, it could be due to:
When your Alibaba OSS bucket comes under attack, the Object Storage Service automatically sends the bucket to a sandbox. Let me explain: Here, Sandbox is an isolation mechanism for buckets that are under attack or are compromised. It is a part of the safety operations developed by Alibaba Cloud to ensure that your operations and data will remain safe, even if a bucket has been compromised.
If your bucket is under attack, OSS automatically adds the attacked bucket to the sandbox. The bucket in the sandbox can still respond to requests, but the service quality of your application will not be normal.
To prevent the Object Storage Service from adding your bucket to the sandbox, you can use Anti-DDoS Pro.
Another solution is to configure a reverse proxy using Alibaba Cloud Elastic Compute Service (ECS) and configure the Anti-DDoS Pro instance. The IP address resolved from the default domain of a bucket changes dynamically for added security measures. To use a fixed IP address to access the bucket, an ECS instance can be used to set up a reverse proxy and attach the elastic IP of an ECS instance to an Anti-DDoS Pro instance to ward off DDoS attacks and HTTP floods.
Cloud storage is highly elastic and offers enhanced performance when compared to traditional storage practices. When undergoing a technological paradigm shift, the inclusion of more agile practices is required. Storage plays a crucial role. Maintaining that storage by monitoring it closely and troubleshooting problems is highly recommended.
When it comes to containerization and microservices with DevOps, a lot of things depend on storage. Alibaba Cloud has devised architectures to support and uplift the overall service quality to ensure uninterrupted service and high-scalability usage scenarios.
We will discuss the complete architecture and usage scenarios of the Apsara File Storage NAS solution by Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@projectclover/clover-finance-to-integrate-with-0chain-connecting-dapp-builders-to-censorship-resistant-data-e804d9e30948?source=search_post---------220,"Sign in
What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
Clover Finance
Sep 6, 2021·2 min read
Clover Finance is excited to announce its integration with 0Chain, a high-performance decentralized cloud storage network. Clover will leverage 0Chain to distribute dApp files across the distributed web to bring the first fully censorship-resistant DeFi experience.
0Chain is a distributed storage architecture that distributes fragmented files across multiple layers of nodes, making it nearly impossible for a malicious actor to tamper with data while also ensuring that its users have complete ownership, control and transparency of their data.
With the integration, third-party dApp builders deploying on Clover Finance will no longer need to worry about hosting expenses and be able to distribute their files across 0Chain protocol. dApp files can be accessed via various frontends including the official Clover Wallet on iOS, Android, and Chrome with an upcoming “dApp Store” tab. Clover dApp store will accommodate a wide range of Clover-native and Sakura-native dApps and seamlessly interact with users’ wallets in one place. Clover wallet users will be able to download files directly from the “dApp Store” tab and run them locally on their device.
The overall vision for Clover Finance is to become a multi-layer operating system, whose smart-contract layer comes within the core, storage layer is distributed across the web, and public key infrastructure remains non-custodial across various frontends.
“0Chain is a decentralized storage layer that can be leveraged by other protocols, platforms and solution providers. We look forward to increasing our exposure within the Polkadot ecosystem by having Clover’s dApp developers leverage 0Chain as a decentralized storage layer and realize the benefits thereof, including cost savings”
— Mo Siam, COO 0Chain
“As a builder and a web3 enthusiast, I’m very excited to work with 0Chain on this integration. It’s very promising to see that 0Chain is capable of achieving the power of truly distributed storage. There will be many more integration angles with 0Chain moving forward.”
— Burak Keceli, CTO Clover Finance
0Chain is a high-performance decentralized storage network designed to eliminate business threats such as censorship, privacy liability and data breach. 0Chain helps entities achieve GDPR compliance, localization, tokenization and monetizes private data sharing.
The decentralized platform offers high performance, high quality of service, collaboration, localization, and streaming to address a broad spectrum of applications.
API| Docs | Telegram | Reddit | Twitter | Forum | GitHub
Clover is a powerful smart contract platform that enables Ethereum developers and projects to migrate their contracts onto Polkadot. By minimizing the changes required to run existing Solidity smart contracts on the new network, Ethereum projects can simply replicate their dApp and deploy it to Clover using MetaMask, Truffle, Remix, and other familiar tools.
[Website] [Telegram] [Discord] [Twitter] [Github] [WhitePaper]
A Foundational Layer for Cross‑chain Compatibiliy -https://clover.finance
141 
2
141 claps
141 
2
A Foundational Layer for Cross‑chain Compatibiliy -https://clover.finance
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/how-your-data-is-stored-or-the-laws-of-the-imaginary-greeks-54c569c17a49?source=search_post---------325,"There are currently no responses for this story.
Be the first to respond.
If you don’t work in computers, you probably haven’t spent much time thinking about how data gets stored on computers or in the cloud. I’m not talking about the physical ways that hard disks or memory chips work, but about something that’s both a lot more complex and a lot more understandable than you might think: if you have a piece of data that many people want to read and edit at once, like a shared text file, a bank’s records, or the world in a multiplayer game, how does everyone agree on what’s in the document, and make sure that nobody overwrites someone else’s work? This is the problem of “distributed consensus,” and in order to discuss it, we’ll have to discuss bad burritos, sheep-tyrants, and the imaginary islands of ancient Greece.
“The imaginary islands of what?,” you ask?
It turns out that the original scientific paper about one of the most important methods used to solve this problem was written in the form of a lengthy discussion of how the part-time parliament of the imaginary ancient Greek island of Paxos managed to pass laws, despite nobody ever reliably showing up for work at the legislature. It was a wonderful metaphor for how a bunch of people can agree on what to write to a file, even though they might be unreachable or distant for a time — and the paper is both one of the funniest serious research papers ever published, and one of the best explanations of a complicated algorithm I’ve ever seen. And since this metaphor worked so well to explain one part of this problem, and because it’s a lot more fun than talking about file systems, I’m going to use it to explain all of it to you.
The reason this metaphor works so well is that with both files and laws, we have some data which many people wish to change, many people wish to read, and none of these people want to spend forever doing so. Simultaneous reading of a law-book is limited by the number of people who can see the book at a time, just like reading a file can be limited by the speed at which data can come off a disk; simultaneous writing requires that everyone somehow agree not to overwrite one another, but without having to have lengthy debates before proceeding. Short of having every single person in the world take turns, with one person holding the book at a time, how can we solve this problem?
So instead of a conversation about file systems and database transactions, let’s transport ourselves to the islands of the Fictional Aegean Sea, where everyone is obsessed with issuing decrees and making laws: they just need to make sure that everyone can read up on the law when they need to. We’ll go through some twisty passages, and by the time we’re done, you’ll actually understand distributed consensus the way professional computer scientists do. Only with more sheep.
(Incidentally, there will be a bunch of islands, and their names are entirely made-up, except for the one which sparked this whole story, about which more later.)
We begin our story with the simplest of all of these islands, Pseudemoxos, home to a single hermit writing laws and decrees for her own use. At first, she used a very simple and obvious method: a notebook and a pencil. She could add laws, change laws, and reorganize laws at will, by simply erasing and writing new things down, wherever she pleased. (And this is, essentially, how the disks in your own computer worked until the late 1990’s)
This method appeared to work reasonably well, except for the tendency of overly erased paper to wear thin. The hermit became horribly aware of the flaw in her design, however, when one day a crate of dubious burritos washed up on the shore. Halfway through writing a change in the law — having erased text and begun to write something — she experienced what we politely refer to in computer science as a “production emergency.” When our hermit returned to her law-book, somewhat the worse for wear, she found that not only had she forgotten what she was about to write, but there was now an empty space in the middle of the law-book filled with neither the old nor the new law, but scribbles written in such haste that even she couldn’t read them!
(The metaphor, of course, is for what happens when a computer or a program suddenly fails partway through a write — the data on disk can get hopelessly corrupted. The metaphor extends as well to more serious failures of the hermit, where events like a head crash could cause damage not only to the words being written, but leave a rather unpleasant and permanent mark across the entire page.)
Both of these problems are solved with what is known as journaling. The rule is to never erase anything — just like when people keep logbooks or lab notebooks. Now, our hermit keeps a stack of paper. When she wishes to make a change to the laws, she adds a note to her journal:
June 4th, 10:32 — Append to the laws of food safety: The consumption of burritos which have been at sea for an undetermined length of time is forbidden.
Every edit, be it an insertion, change, or deletion, is logged at the end of the journal, in pen. The hermit is protected from an emergency, because nothing is ever erased; at worst, there is an incomplete or damaged journal entry (missing, perhaps, its final period), the sign of a failed attempt at writing, which one may simply ignore. The journal has the additional advantage of serving as a record of all changes; to see how the law looked a year ago, for example, our hermit can simply read the journal and stop at that date.
The one obvious flaw of the journal is that reading it is more difficult than reading a notebook: to know the current law on food safety, one has to start from the very beginning of the journal and read all the way through to the end, keeping track of all changes to the food safety law as one goes, a process which becomes steadily more onerous as time goes by and the journal gets longer.
To simplify this, our hermit keeps a stack of empty law-books handy. Whenever the journal gets too long, she sets aside some time to make a digest of the law: she reads the journal from beginning to end, keeping notes on scrap paper if needed, forming a description of the current state of all of the law, and then writes out that current state into the law-book. On the cover of the law-book, she writes “This was the state of the law as of [some date].” Further reading of the law then only requires that she read the digest, and the journal of changes made after the digest’s date. Journals older than this can be kept, if a record of the change history prior to the digest is occasionally needed, or simply thrown away.
If our hermit is too occupied with the business of drafting new legislation to regularly make a digest, she can simply employ a scribe (that is, a second computer program running in parallel to the first) to assist her. The scribe will simply read the journal up to some agreed-upon time, writing a digest and informing our hermit when he has finished, so that she can start using the new digest. The scribe need speak to nobody in order to do his job, so the hermit can continue in her labors, her silence unbroken.
This method has still more benefits. Our hermit having become widely known for her wisdom (for example, her lessons on avoiding sea-borne Mexican cooking), many people may wish to read her law. So long as these readers are content to know the digest of the law up to a fairly recent time, they can simply make their own copies of a digest, and read freely without needing to disturb the sage at her work. Only a reader who needs an absolutely up-to-the-minute view of the laws need bother her directly.
(The use of journaling for individual computers began in the late 1990’s, and became common in the mid-2000’s. Your computer today probably does this.)
Near Pseudemoxos is the somewhat larger island of Fotas. This island is sparsely inhabited, but there are enough people that the Fotans have all agreed that they need a mutual system of laws to govern their lives. However, the Fotans are known for their fierce independence: they are unwilling to allow any other Fotan, or group of Fotans, to have power over them, and so have decided that any and every Fotan shall have the power, independently, to enact a law, simply by writing down the law they wish to enact on a piece of paper, together with the date and time and their name, and sending it by messenger to the Library of Fotas, where a scribe will append it to the law-journals.
(If it’s not clear, this represents the case where many people are trying to simultaneously write a single file without making any attempt to reach consensus about what goes into the file before they do so. It’s perhaps unsurprisingly difficult to do so without all hell breaking loose, but with a bit of cleverness — and some restrictions about how people learn about changes — it’s actually possible!)
To see some of the ways in which this can go wrong, let us begin by considering Agnes, who lives on one side of the island and is very concerned with the purity of ritual sacrifices. She enacts a law:
June 1st, Noon — Law 32.1.2 on the sale of sheep is amended to read: The sale or purchase of any sheep whose wool is not perfectly white shall be forbidden on penalty of death. Signed, Agnes.
On the other side of the island, Basil, who has recently had a number of black sheep born which he is having trouble selling, enacts his own law:
June 1st, Noon — Law 32.1.2 on the sale of sheep is amended to read: No person may refuse to buy or sell a sheep on the account of the color of its wool. Signed, Basil.
Later that day, the scribe finds himself with two new laws, with the same time-stamp, even. What is he to do? When Basil next arrives in town to sell a black sheep to Galatea, and she refuses to buy a non-white sheep, and they go to read the law-book, which law should they obey? And what should happen when the scribe tries to write the next law-digest — what will Law 32.1.2 say? (In computer terms, two people have tried to write different data to the same part of the same file at once; the result could be absolutely anything. Agnes’s law may win, Basil’s law may win, we may end up with every other letter coming from each law, or anything else)
To illustrate a more subtle kind of problem, imagine that Agnes wants to perform a “read-modify-write” of the law — that is, she looks at the current law, decides to make a change, and then writes the change. But just as she is doing this, Basil makes a conflicting change to the law. For example, at 10:00, Agnes reads the law to be
Section 32.1.3. Any person selling a goat must pay a tax of one obol to the library general fund.
She ponders this, and at 10:02 issues the following write:
June 1st, 10:02 — Law 32.1.3 shall be amended to change “one obol” to “two obols.” Signed, Agnes.
Unfortunately, at 10:01, Basil (who has little patience with goat-taxes) issued:
June 1st, 10:01 — Law 32.1.3 shall be replaced with “The Ides of February is national olive day.” So there. Signed, Basil.
Our scribe, attempting to reconcile the laws in order, is going to be very confused: when he reaches Agnes’ entry, law 32.1.3 is talking about National Olive Day, and doesn’t mention “one obol” anywhere. What is he to do? (And if you think this case is obvious, imagine if Basil’s law had instead replaced “the library general fund” with “Basil!” The scribe has no way of knowing if Agnes knew about this change or not before her own edit.)
Situations like this happen whenever multiple writers can compete to edit the same book of law. There are numerous solutions, each with different pros and cons. These solutions fall into two basic categories. One is “eventual consistency” (or “weak consistency”), where a few rules about how writes work will allow everyone to write at the same time without speaking to one another. The cost of this is that nobody can know the exact state of the law at this instant; they can only know how it was a short time ago. The other is “strong consistency,” where all kinds of write are possible and everyone can know the current state of the law, but at the cost of each write requiring a process of consensus-building.
Both of these turn out to be useful. Sometimes, you have information where a bit of staleness is alright; for example, when the inhabitants of Fotas are making their annual yearbook, and exchanging pictures of one another’s profiles, there’s no pressing need to have the most up-to-the-moment pictures, and so the simpler eventually-consistent protocol will do nicely. On the other hand, for actual laws, strong consistency will prove the more useful, despite its cost. (This is true with computers as well. For example, images you upload to Google are stored using an eventually-consistent system, while the access control lists which define who can view what are stored using a strongly-consistent one)
Let us return to the island of Fotas.
We could easily solve the first of our two problems — contradictory laws at the same timestamp — simply by agreeing upon a tie-breaking rule so that no two timestamps can ever match. For example, we could break ties in timestamp by the name of the author, in alphabetical order, so that in case of a race Agnes’ changes will always happen before Basil’s. (Or if names match, we could assign each Fotan a unique number ahead of time) Once the timestamps no longer match, there is no confusion: later laws always supercede earlier ones, and so when Galatea comes to town, Basil’s law is the one on the books.
If mutual overwriting of this sort is a problem, it can be avoided by prior agreement about who can write what. For example, for one week Agnes might be allowed to edit only even-numbered laws, and Basil odd-numbered ones, switching the next week. (In computers, the space of files is often divided up so that two writers never attempt to write the same file at all, much less at the same time. For example, each newly-uploaded photo might be automatically assigned a unique name, so that only its uploader ever need write to that file)
To solve the second problem, we simply decree it out of existence: we change the rules of the journal to forbid any “edit” statements, allowing only replacement, addition, or deletion. This means that no statement in the journal can ever depend on the current state of the law for its interpretation. Agnes would have had to instead write her law in the form
June 1st, 10:02 — Law 32.1.3 shall be replaced with “Any person selling a goat must pay a tax of two obols to the Senate.” Agnes.
and, since her write had a later timestamp, her version would win over Basil’s without confusion.
Such a system of writing laws has the benefit of speed and simplicity: anyone can still write a law at any time, without consulting with anyone else. However, reading the laws becomes surprisingly tricky. Say you want to know the current goat-selling tax, and so you go to the library and ask to read Law 32.1.3. Arriving at 10:05, you read the digest and the journal, and find that it reads: “Any person selling a goat must pay a tax of one obol to the Senate.”
You see, Agnes changed the law at 10:02 by her time — but her messenger hasn’t arrived yet! All you can know is that, as of the last time the journals were written, this was what the law said.
To make things a little better, messengers can check in at the library: whenever a messenger arrives from a given Fotan, after they deliver their messages, they write a note on the board indicating that all of the given Fotan’s messages have been delivered at such-and-such a time. A visitor can then look at the board. If the last message from Agnes was at 9 that morning, and the last message from Basil was at 7 the previous evening, and so on, then you know, at least, that the laws you are reading were accurate as of 7 the previous evening — the earliest of the times shown on the board, since all edits prior to that must have arrived. (This time is known as the “low-water mark”)
Of course, if a particular Fotan isn’t very litigious, their updates might be rare, and so everyone will be wondering if they have made a recent change which simply hasn’t arrived at the library yet, or if they simply haven’t had anything to report in weeks. To avoid this problem, each Fotan must send a messenger to the library at regular intervals, whether they have any updates to make or not, so that the board remains fresh.
Now, the Fotans are a creative people, and so they quickly realized that this steady stream of messengers could simplify their lives further. Rather than having to make the tedious trek to the library themselves, they can simply maintain their own copy of the law-digest and law-journal. Their messenger simply returns, each time, with all of the changes to the law-journal which have been made since the last time they sent a messenger to the library, as well as a copy of the dates which were on the board then. They simply update their own copy of the law-journal, digesting as needed, and can now refer to their own copy with as much ease as they could to the central reference copy.
As the Fotan population progressed, however, the traffic at the library became a problem, what with all of the messengers running back and forth and the scribes copying journals and digests, and the task of simply making a law to change the goat-tax became obnoxiously slow. Fortunately, the Fotans realized that they had solved their own problem: having their own copies of the law, they no longer needed a single central library!
Instead, several branch libraries were opened. Messengers could simply deliver and receive updates at the nearest branch libraries, and other messengers would travel between each pair of branch libraries, transferring copies of all of the updates to the journals. Ultimately, a “tree map” was built: a diagram connecting each of the branches to one another, and each Fotan to a branch library. Updates were delivered only along the routes shown in the tree, but since each Fotan or library was connected to each other Fotan or library, eventually, any change made by any Fotan would reach every other Fotan. And since everyone would ultimately have the same journal in hand, their law-books would eventually be consistent!
This has many nice advantages: for example, when having to navigate a difficult or expensive road, such as the treacherous toll-road which is the only access from the central island to its eastern mountains, only one messenger need traverse that path; he will simply deliver updates from the West to the East, and vice-versa, with the branch libraries at opposite ends of the road propagating the information to the rest of their ends of the island.
(You may notice, at this point, that there is no longer any meaningful difference between an individual Fotan and a branch library — and that’s exactly right! Everyone has a copy, and so long as they continue to send messengers along the appropriate routes, everyone can change the law simply by writing in their own journal, and trusting in their messengers to propagate the information elsewhere)
This method is thus very efficient, but has three notable weaknesses. First, it is impossible for anyone to know the current state of the law with certainty. Unlike writing to one’s own journal, like a Pseudemoxian hermit, one no longer has the guarantee that, after finishing a change, anyone reading in the future will see that change; only that anyone reading sufficiently far in the future will see that change. (This is known as a lack of “read-after-write” consistency)
Second, it is impossible to perform a read-modify-write. This means that, if there are ever two Fotans attempting to change the same law, the consequences are unpredictable: neither Fotan, at the moment that they issue the change, can know with certainty if another change hasn’t already begun, and whether that other change will end up happening before or after theirs, meaning which change will end up in their law-journals after all changes have arrived at both of their homes. To circumvent this, a Fotan has to both find some way to guarantee that he is the only Fotan attempting to change a given law, and that he has read an up-to-the-minute version of the law prior to changing it.
Third, this method is vulnerable to network failures: imagine that the road leading up to a single Fotan’s remote house is cut off by a rockslide. No updates can reach the outside world from him, and so nobody can assume that they have all the updates from any time more recent than the last message to get from this Fotan to the safe side of the road. But neither can they assume that he transmitted nothing since then; for all they know, he is blissfully unaware of the rockslide and continuing to write to his own journal, awaiting a messenger who will never come. The entire system therefore comes to a halt, with nobody’s low-water mark advancing, and therefore nobody able to form a new digest, all because a single road failed!
In computing practice, such problems are real and serious. Whenever a single node (a computer, or perhaps a datacenter) becomes disconnected, production engineers need to immediately evaluate the situation and determine if it is likely to be fixed and reconnected quickly. If it does not, every other server will be unable to form digests, and so their journals will continue to grow, causing problems for readers, while the isolated writer becomes more and more out-of-sync. If the problem cannot be fixed quickly, the isolated nodes are often switched off completely, and other methods are used to route people who wish to connect to them to other computers elsewhere — often through a much slower external network, but hopefully one which was not compromised. However, the other nodes will continue to wait. If it becomes clear that a fix will not be short in coming at all, there is only one alternative: remove the missing nodes from the network entirely, telling the other nodes to pretend that they no longer exist and that no updates from them will ever be forthcoming. The other nodes progress, but any writes which the isolated node performed after the isolation — unless they can be copied off and transferred to the main network by other means — will be permanently lost.
So what we have seen above is a reasonable solution for fiercely independent islanders like the Fotans, who want to be able to write quickly and aren’t entirely concerned with being able to read the latest version of something. But what happens in situations where that simply isn’t acceptable? Laws are actually a good example — if you commit a crime at 10:00, it matters a lot whether the law against it was passed at 9:00 or 11:00. Knowing the law up-to-the-moment is important.
The problem became far worse when Fotas went into the law-book business. You see, Fotas is surrounded by many much smaller islands, each of which has its own industry and thus requires its own law-book. Being small islands, they do not have the resources to maintain the intensive Fotian system of scribes and messengers themselves; and even if they did, they would never be able to train those scribes and messengers to work as quickly or efficiently as those of the Fotans, who are (one must admit) kind of obsessed with this. These small islands therefore continued for years to use the simplest and most brute-force solution: each island had its own hermit with a law-journal, in the Pseudemoxian Style, and everyone who wished to read or change a law simply lined up and dealt with the hermit one-by-one. It is slow and inefficient, and may the Immortal Gods protect them if their hermit happens to get sick, or be swept away by a tidal wave. But lacking the resources of an island like Fotas, they simply continued about their way.
The Fotans therefore smelled an opportunity of offering their law-books as a service to their neighbors; those client islands could simply read and write their information from a part of the Fotian law-book, each island receiving its own dedicated chapter which nobody else could write.
The neighbors, at first, were thrilled: because everyone no longer had to line up to speak to a single hermit, the process became much faster, even taking into account the travel time to Fotas. (In fact, the Fotans worked to improve this travel time by setting up embassies on these various islands which were part of the Fotan network) It was also much more reliable, as everyone was reminded when a tidal wave destroyed several islands and severely damaged Fotas itself. Because each Fotan had replicas of the full law-book, it was easy to recover. (The Fotans later improved on the system by having each chapter maintained by only some Fotans, spread across the island; this gave them the same general reliability without the expense of copying each client island’s laws to absolutely every point on Fotas) And since Fotas was close to so many islands, the islands could even begin to use the Fotian system as a way to reliably send messages to one another.
But the two problems we saw above became much more evident. For example, on the island of Parafoitas (one of the client islands), Andros’ wine company had been using Fotian storage to keep track of its orders. One day, Andros took an order for 100 amphorae of wine for a local politician’s wedding, and entered it in the order-book. The next day, one of his employees checked the order-book — but that employee’s messenger to Fotas happened to go to a different port than Andros’, and this port had not yet received the updates from the previous day’s write due to a mud-clogged road. The employee therefore didn’t know about the order, and the wine didn’t make it until the day after the wedding! (Andros considered keeping a board in his office listing at what time each order-entry had been made, which everyone would check prior to filing or fulfilling orders, but stopped when he realized that he had just made his very own hermit-board, in which case what the hell was he paying these Fotans for, anyway?)
On the island of Siranos, things went no better. There, they had tried to resolve the problem of read-modify-writes by having Baucis in charge of even-numbered laws on Monday and Galen the odd-numbered ones, switching on Tuesdays, and so on, so that only one person might try to affect a law at a time. But as it happened, Galen was quite impatient to change a certain even-numbered law, and so he did so at precisely midnight on Tuesday, as soon as he could. Unfortunately, Baucis had made a change to the same law at 11:58 the previous night, and her change was not yet visible to Galen when he made his own change — so Galen issued a change, thinking he was the only person doing so, and unintentionally overwrote Baucis’ work.
Some of the neighboring islands, on the other hand, were quite satisfied; Epifoitas, for example, had been using Fotian storage to keep an archive of their poetry. Once a poem was committed to the archive, it would never be changed, only read, and new poems perhaps added in response; as such, there was never a concern that a poem might be overwritten. For them, the Fotian system was both reliable and inexpensive. But on the whole, there were enough islands who wanted to regularly read and write their texts that the inadequacies of the Fotian method became clear.
So now we come to the island of Paxos. This is the original imaginary Aegean island of Leslie Lamport’s invention which led to this whole metaphor, and in fact the method he described is universally known as “the Paxos algorithm” because of it. (All of the other island names in this article have been my own invention)
The good news about this algorithm is that it’s no more complicated than what we just discussed, and his paper discusses it in the same style; if you’re comfortable reading this, you can now simply pick up Lamport’s paper “The Part-Time Parliament” and read it without difficulty. The bad news is that I can’t think of any way to explain it which is shorter than Lamport’s explanation, and that would make this already-long story insanely long, so I’ll leave you to his tender mercies for the details. But I can give you a summary of the idea:
The Paxons were concerned with keeping track of their own laws, and as such were very concerned with having read-after-write consistency, so that everyone might know the current law of the land. They also wished to have read-modify-write consistency, since otherwise they might accidentally pass conflicting laws. Together, this kind of consistency is often referred to as strong consistency, whereas the weaker properties of the Fotian system are called eventual (or weak) consistency.
The details of the Paxon problem (which you’ll learn more about if you read the paper) were slightly different: their laws were passed only by their Parliament, which met in a single house and so they didn’t have to worry about members suddenly becoming unreachable due to mudslides or tidal waves. But instead, they had a part-time parliament: legislators who were prone to coming and going as they pleased, becoming unreachable not because of a natural disaster but because of a particularly good amphora of wine. And due to poor acoustics in the hall, oratory was impossible, and legislators had to communicate with each other via messengers, just like the Fotans. So despite the superficial differences, the part-time parliament of Paxos posed all of the same logistical complications as the spread-out parliament of Fotas.
The core idea of the Paxos solution is simple: in order to make a change to the laws, you get a majority of Paxons to make the same change. At that point, if someone is trying to make a contradictory change, when they try to build up their own majority, it’s guaranteed to include at least one person who knows about the other change, and who can stop them and say “Wait! We are already voting on something different!” (Because if you have two sets of Paxons, each of which is bigger than half, there must be at least one person in common between them!) Likewise, when you wish to read the laws, you ask a majority of Paxons for the latest version of the law; again, if any change has been made, at least one of them must have heard of it. The details amount to a method of keeping track of which ballots are currently in progress, based on each Paxon having their own law-journal and note-pad where they track their own votes and the messages they have received.
Lamport’s method provides several important guarantees: it has read-after-write consistency, in that once the consensus condition has occurred for a proposed law, it is guaranteed that every future attempt to read that law will see that consensus; it makes read-modify-writes possible, in that once a change to a given law begins, either that change will end with no intervening writes having been allowed, or (if it was discovered partway through that an intervening write had already started) that change will fail unambiguously and everyone will know to try again; and it further satisfies the “progress condition,” that “if a majority of the legislators were in the Chamber and no one entered or left the Chamber for a suﬃciently long period of time, then any decree proposed by a legislator in the Chamber would be passed, and every decree that had been passed would appear in the ledger of every legislator in the Chamber.”
However, it does this at a cost. Passing a law — that is, writing to the system — requires building a consensus among a majority of members. If some of the members are distant from the originator, then this is potentially a very slow process; you can no longer add a law by simply writing it in your own journal. Reading a law becomes a slow process as well, as that process now requires asking a quorum of Paxons about their view of the law.
To moderate this, in practice Paxon systems provide two methods of reading: “Read-latest,” which performs the quorum read as above, and “read-recent,” which consists of simply checking your own log-book. Recent reads lack the consistency guarantees of the Paxos system, but they are quick, and in practice many systems require these strong guarantees only some of the time. (e.g., Agnes and Basil may wish to do a read-latest when they are resolving their dispute over the sale of sheep, but on an ordinary day when one of them is heading to market, they will content themselves with a read-recent before leaving the house)
Nonetheless, this means that the Paxos method always has a nontrivial speed cost, and as the number of people involved grows, this cost increases rapidly. (Even in the absence of long transit times, simple random variation in the time-to-answer of the individual Paxons takes its toll, as each operation requires waiting for over half of them to answer, so you end up waiting for the slower individuals)
One interesting property of these systems is that they can be combined. For example, in both the Fotas and Paxos systems, each legislator had a copy of the law-journals and their own note-pad. The design of those systems relies only on the fact that the legislator can write in their own books with the guarantees of strong consistency. In the case of those being ordinary notebooks, written and read only by one legislator, this is trivially achieved.
But there’s no reason that this could only be achieved with a notebook, and this lets us solve more problems. Imagine a network of islands, each individually small, but separated from each other by large seas, as in the Pacific. (Or in computer terms, imagine a network of datacenters, each within a building, but spread over the entire world) Providing a strongly-consistent store using Paxos over such distances is horribly impractical, because each read or write requires a quorum, which requires multiple inter-island trips. However, each island can maintain its own strongly-consistent store using any of the means above, and then a separate, inter-island organization can maintain its own laws using any means it wishes, simply replacing individual notebooks with single-island stores. A client limited to a single island can then deal only with their own island’s system, while clients doing inter-island business can use a different system but get all of the robustness advantages of having more than a single point of contact on their island.
This possibility led the Siranoi to reconsider their own system. Remember that this island had tried to achieve strong consistency by dividing up their laws by day, so that Baucis could write even-numbered laws on Monday and Galen on Tuesday, and vice-versa. Even though this simple system ran in to problems, it revealed an important truth: someone interested in the laws on sheep-selling on Monday was likely to still be interested in it on Tuesday, and changes of interest were relatively rare; and likewise, accidents which caused people to simply vanish — leaving nobody to deal with the laws on sheep — were also relatively rare. And even though many strongly-consistent methods are slow, it’s OK if you have to do something slow on rare occasion, if your day-to-day is quick.
This led the Siranoi to ask themselves if, for any subject, they could simply elect a Tyrant who would be responsible for all laws related to that subject. So long as everyone could easily find out the Tyrant for any particular subject, and the Tyrant himself did not become overloaded with requests, this would achieve an even simpler form of strong consistency: anyone wishing to change the laws on that subject, or know the latest laws on the subject would simply communicate with the Tyrant, Pseudemoxian-Hermit style; whereas anyone wishing to simply get a good idea of the latest situation would read their own copy of the general law-books, copied to them Fotan-style.
The basic principle is simple. Say Baucis wishes to know the sheep-law. Baucis inquires of the central registry of Tyrants, “Who is the current Sheep-Tyrant?” If the registry says that Philemon is, then Baucis immediately knows where to go. If the registry says that nobody is, then she simply proposes a law to this registry, “Baucis shall be the Tyrant of Sheep.” If this law passes, then Baucis is now the sheep-tyrant, and can proceed entirely on her own; if the law fails, then another law must have passed in the interim, so she simply repeats her query.
As you may have guessed, the central registry of Tyrants is nothing more than another strongly-consistent store. For very small groups, a single hermit may be workable, but both for reasons of scale and reliability, it’s typically better to use the Paxos method to build the central Tyrant-registry. While inter-island Paxos can be extremely slow, you only need to access it on rare occasion, when you want to find out (or become) the Tyrant for some particular subject; ordinary communication on that subject is then one-to-one.
This method has some wonderful advantages. If only one person is interested in a given subject (a common case, especially if the subjects are fairly narrow) then that person can become the Tyrant of that subject themselves, and need not deal with any neighbors at all; they can simply proceed like a hermit, reading and writing from their own book, secure in the knowledge that nobody else is permitted to change the law on their subject. If many people share a particular interest, then all of those people will have to queue up to speak to one Tyrant, but the Tyrant can immediately give them an answer, without having to wait for information to be carried across the island.
There are a few problems, however. The first is dead Tyrants. Baucis, having reigned as the Tyrant of Sheep for many years, one day had a heart attack. Nobody knew about this, though, and so everyone concerned with ovine matters ended up queueing at her door, waiting forever for her to show up, and all sheep-law came to a halt for several days, until finally the door was broken down, and discovering her death, the Siranoi passed a law revoking her Tyranny. After this happened once, the Siranoi formalized the solution in a simple fashion, namely terms of office: rather than passing “Baucis shall be the Tyrant of Sheep,” Baucis would propose “Baucis shall be the Tyrant of Sheep until Thursday at Noon.” If she is still alive, and interested in sheep, then sometime that Thursday morning she can propose another ballot measure to extend her reign.
The second problem is overloaded Tyrants. It’s one thing to be the Tyrant of Sheep on a small Aegean island; quite another to be the Tyrant of Sheep of New Zealand, where sheep outnumber people 7:1. The queues outside the Tyrant’s door would become quite intolerable!
Fortunately, nothing in this system requires the Tyrant to be an individual — simply that the Tyrant must provide a strongly-consistent representation of their subject, as well as transmit updates to all other law-journals using some robust method. So on New Zealand, rather than a single individual being elected Tyrant of Sheep, a group of interested ranchers joined to form the Kiwi Sheep Tyranny Combine. Being a relatively small group of relatively reliable people, they can maintain a shared law-book using Paxos (or any other system) with reasonable ease and speed. If they find themselves routinely overloaded, they can simply add new members to their organization, so that more people can service client requests.
And rather usefully, they can continually improve their methods. When the KSTC is small, they might use a large-print journal which everyone can read at once, and in-house simply take turns; or they might divide up subjects amongst themselves, and if someone is temporarily unavailable deal with the matter on an ad hoc basis, someone else filling in; or they might use Paxos; or ultimately, as the KSTC grows, they might even subdivide sheep-law in their own way and perform internal master elections to determine, say, the Sub-Tyrant of Shearing in the Otago Region. What’s nice about this is that their clients never need understand, or even know, the details of how they store information in-house; so long as the KSTC provides the guarantee that anyone coming to the door will be treated to a strongly-consistent representation of the sheep-law, the methods can change repeatedly.
The classic version of this master election protocol is called Chubby.
If you have made it this far, you have just learned some of the most challenging topics in distributed computing. Nearly every problem in datacenter- or planet-scale computing boils down to these issues: how do you get a bunch of computers, often distant from one another, connected via unreliable links, and prone to going down at unpredictable intervals, to nonetheless agree on what information they store?
In practice, there are four methods which are commonly used:
The choice of systems, and how to combine them, is therefore a matter of practicality. For example, if one wishes to maintain a system where many users can simultaneously see changes to a piece of shared data in nearly real-time — perhaps a shared document, or a computer game — then it is helpful for that single piece of shared data to be administered by as small a group as possible. Master election works well in this case, with the individual master being optimized to handle a single piece of data quickly. Latency comes from the message transit time from the client to the master, and from queueing delays at the master; the latter can be resolved by making the master itself bigger (even turning it into a small cluster itself), while the former is just a problem.
On the other hand, if one wishes to serve billions of images — data which tends towards the bulky — which are uploaded by users, then there is little advantage to be had from strong consistency, except as far as the uploading user is concerned. You can then handle the upload process itself by having a single server communicate with the user until the data is fully uploaded; since we know that this user is the only one looking at the pictures until the upload is complete, that single server is the “master” of that data by default with no additional trickery. Once the upload is complete, eventual consistency is more than enough. (This brings up even more interesting questions, like the possibility of “partial replication:” only some sites having a copy of each picture, but any site being able to access another site’s pictures if need be. My own work, a few years ago, was in that field)
The best thing about these systems is that, from a client’s perspective, they are defined not by the methods but by the guarantees they provide. If you tell your clients that there is a strongly-consistent system here, and they can perform writes, read-current, and read-latest by coming to this Greek, so to speak, then you can continually change the method which you use (from single data files up through master election which selects clusters which themselves use Paxos or what-not) as the needs of your clients grow and change, without requiring any sort of consensus or agreement among them.
And with this, I now leave you. You have wandered through a series of ridiculous stories about imaginary islands, Sheep-Tyrants, mudslides, and the like; but what you just learned isn’t a baby version of computer science at all, but the actual real thing, what professionals spend their time on every day. So I hope that, even if you’re not a computer scientist yourself and never plan to be, you now have a better understanding of what’s involved in modern computing — or at least, the part which involves sheep.
This story previously appeared on Google+.
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMIfamily. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
To learn more, read our about page, like/message us on Facebook, or simply, tweet/DM @HackerNoon.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
993 
9
993 claps
993 
9
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Either political analysis of authoritarian regimes, or interesting facts about science, depending on my mood.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://betterprogramming.pub/build-your-own-in-home-cloud-storage-1aa74b5c6397?source=search_post---------27,"Sign in
There are currently no responses for this story.
Be the first to respond.
Randal Kamradt Sr
Apr 22, 2020·8 min read
Got an old computer collecting dust in the corner? Turn it into a networked storage solution! I have just such a computer, so my weekend project will be to give it a second life as sharable storage for my network, and for my in-home Kubernetes cluster (yes, I’m a true tech geek).
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/twigcp-166-a01086c17eb5?source=search_post---------71,"There are currently no responses for this story.
Be the first to respond.
Here are the main stories for this past week :
“Take control of your Kubernetes clusters with CSP Config Management” (Google blog). Create and manage a consistent configuration for your Kubernetes resources in a multi-cluster setup.
“Go global with Cloud Bigtable” (Google blog). Increase the availability and durability of your data by copying it across multiple regions or multiple zones within the same region.
“How does your cloud storage grow? With a scalable plan and a price drop” (Google blog). Cloud Storage has had technical excellence for a little while (one API with flexible storage classes, secure and durable, among other things), and now this pricing model should only help more customers use it at scale.
“Qwiklabs — Data Analysis quest” (google.qwiklabs.com). Use the code 3g-bigquery-785 to get the first lab free! Get started on learning how to query petabytes and even start to build machine learning models.
“Simplify enterprise threat detection and protection with new Google Cloud security services” (Google blog). Introducing Web Risk API; Cloud HSM going beta; and new Cloud Armor features..
From the “Tensorflow 2.0 Alpha is here” department :
From the “collaborating with Open Source projects to make them and GCP better” department:
From still my favorite “Customers and partners talk best about GCP” department :
From the “addressing specific use-cases with battle-tested solutions” department :
From the “making ML work for you” department :
From the “Certification from someone who’s taken it” department :
From the “ask the experts” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
That is all for this week!-Alexis
Google Cloud community articles and blogs
20 
20 claps
20 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.upthere.com/the-tale-of-the-loading-spinner-that-would-not-spin-9d92ae45ae93?source=search_post---------373,
https://medium.com/bb-tutorials-and-thoughts/building-angular-static-website-with-gcp-cloud-storage-be3410f881a8?source=search_post---------69,"There are currently no responses for this story.
Be the first to respond.
There are a number of ways you can build a website with Angular such as Java with Angular, NodeJS with Angular, NGINX serving Angular, etc. For the single-page applications, all you need to do is to load the initial index.html. Once you load the index.html the Angular framework kicks in and do the…
"
https://medium.com/pitchground/publist-introduction-412312b04f24?source=search_post---------208,"There are currently no responses for this story.
Be the first to respond.
With an increase in the usage of several files, pdfs, documents and various other folders saving in our Drives and inside dozens of different cloud and desktop apps every day, it has also started seemingly to get more difficult to stay organized and structured.
We know you agree with us!
"
https://blog.bluzelle.com/the-blockchain-approach-to-data-storage-problems-cbfdcd7c7bd8?source=search_post---------383,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
The human race has truly entered the Information Age. Once the epitome of innovation, the floppy disk’s performance is now eclipsed by thumbnail-sized SD cards that boast over 500,000x the capacity of the already-ancient technology. Terabyte-denominated drives are becoming the norm, and prefixes like peta-, exa- and zetta- are being used to quantify the mind-boggling amounts of data flowing in cyberspace.
Far from being a buzzword, the term Big Data refers to the processing of exponentially growing data sets. As advances in the Internet of Things and Artificial Intelligence power forward, the strain on storage is only set to worsen. New data centers are cropping up at a rapid rate to alleviate this strain, but will it be enough?
IDC predicts that, by 2025, there will be 163 zettabytes of data in the world. An IBM report, in the same vein, found that 80% of data today is unstructured — that is, text-heavy information that cannot be properly interpreted using traditional analytical methods. Companies collect hordes of this data, but, unless they can derive insights from it, it serves little purpose in growing their business. Instead, they begin to stockpile the unstructured data, incurring costs as they must constantly upgrade their storage capacity to make room for the useless information.
Cloud databases, in their current iteration, leave much to be desired by way of scalability — expanding capacity under such a model proves to be costly. For dApps (or ‘decentralised applications’) bolted on top of blockchains, there is a need for a more purpose-built method of storage to handle the titanic amounts of data these applications consume. Blockchains by themselves, whilst technically databases, are not built for high-volume use-cases.
There is a pressing need for a new solution that reflects the philosophical underpinnings of a decentralised web. Notably, one that embraces the power of a distributed infrastructure to not only scale efficiently, but to harness the security afforded by blockchain technology to avoid data-breaches and eradicate the all-too-prevalent single point of failure architecture.
Decentralisation in data storage has already proven to be highly effective in reducing costs for the end user — instead of relying on a few companies with dedicated data centers (offering pricing that reflects a relatively uncompetitive market), blockchain protocols have enabled users to monetise unused storage space (or indeed, rent out dedicated rigs), with few barriers to entry. The creation of a free market for storage not only vastly lowers costs as hosts compete against each other, but also gives purpose to the troves of otherwise unused space scattered across the globe.
Another edge the blockchain approach has over traditional cloud storage is that of dependability: disasters in geographical locations that encompass central databases can range from minor nuisances caused by power outages, to severe ramifications halting business operations as an earthquake destroys one such centre. This problem is nonexistent in distributed networks, as redundant copies of data are dispersed across a number of nodes. In the case where one node goes offline (whether temporarily or permanently), an identical replica of the data can simply be retrieved from another — as such, a user has access to their data around the clock, with no downtime.
Immutable and secure storage is impressive by itself. For real-time applications in IoT and related fields, however, speed is of vital importance given the constantly flowing data from device to device. The concept of swarming is proposed as a method for ensuring minimal latency to guarantee high-speed transfers. Much like torrenting, devices within a swarm — a large group of nodes in a peer-to-peer network — will retrieve data from the nearest and fastest nodes (querying files by the hash of their content). Leveraging a platform such as the Ethereum blockchain, however, adds an incentivisation layer to the traditional approach seen in torrenting — nodes are rewarded for replicating data.
It would be overly optimistic to say that the technology is already perfected. Blockchain tech is still in its infancy, and there are still elements in need of optimisation (two that spring to mind are scaling the blockchains themselves and reducing transaction fees). Once the kinks have been ironed out, there is then what is perhaps the largest barrier to overcome — adoption. That said, when blockchain platforms have proven themselves to be effective, the offering of better security, reliability and lower costs will no doubt see businesses flock to distributed databases for their storage needs.
(Disclaimer: this article first appears in App Developer Magazine.)
Get-Started Guide|Website| Whitepaper(English)
| Newsletter | Telegram | Twitter | Reddit | Github | Developer Slack |
Bluzelle is a decentralized storage network for the creator economy.
140 
Get our latest updates Take a look.
140 claps
140 
Written by
Bluzelle is a decentralized data network for dapps to manage data in a secure, tamper-proof, and highly scalable manner.
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
Written by
Bluzelle is a decentralized data network for dapps to manage data in a secure, tamper-proof, and highly scalable manner.
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/how-can-blockchain-technology-improve-cloud-storage-360226b3d3df?source=search_post---------129,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jul 2, 2020·4 min read
By Jitendra Bhojwani
In recent years, cloud storage has emerged as the preferred option for more and more businesses owing to its flexibility, capacity, and easy data access. Moreover, reputed cloud storage providers such as Alibaba Cloud offer strong data encryption and other reliable provisions to ensure maximum security. Many companies have shifted their data centers onto the cloud due to high redundancy, load balancing, data integrity, and backup options.
One of the latest trends that are poised to gain momentum is to use Blockchain technology for cloud storage. There are many advantages of Blockchain-based cloud storage and in this blog, we will focus on some of them.
In Blockchain-based cloud storage, data is divided into multiple encrypted segments that are interlinked through a hashing function. These secured segments are distributed across the network and each segment resides on a decentralized location. There are strong security provisions like transaction ledgers, encryption through pubic/private key, and hashed blocks. It assures reliable and robust security against hackers. Even advanced hackers are unable to decrypt the data due to the sophisticated 256-bit encryption.
Let’s look into an unlikely case of some hackers decrypting the data. Even in such scenario, each tedious decryption attempt leads to just a small segment of data being decrypted and not the entire file. The extreme security provisions not only discourage malicious hackers but also make hacking a useless pursuit from a commercial point of view due to extreme efforts required for trivial achievements (and very high probabilities of failures).
Another important thing to consider is that the owners’ data is not stored on the node. It helps owners to retain their privacy. There are strong provisions for load balancing as well.
In the distributed cloud storage the users are connected in a secured P2P network that provides them a decentralized way to store their mission-critical data without compromising on the security. There are other powerful benefits of using Blockchain technology to store your data. It is up to 10 ten times faster and can lower the total expenses to half. Distributed cloud storage strategically combines transparency and security with the hash function, encryption through public/private key, and secured transaction ledgers.
The cloud-based decentralized storage also saves the clients from many security threats. Moreover, the access to encryption keys and un-encrypted files is restricted to the end-users only with the help of client-side encryption. All these factors offer the clients complete control over their diverse data assets.
One of the major concerns during cloud storage is ensuring that the original data has not been tampered with. The inherent verifiable architecture of Blockchain offers a trusted way to track the storage and backup history and confirm that the original data has not been tampered with. Instead of storing data, it stores only the encrypted hashes of related data blocks thus providing the footsteps of the data to verify its originality.
By sequentially linking the different blocks through cryptographic hashes the Blockchain automatically organizes the data in a verifiable order. As every single block is linked to the previous ones, it produces a decentralized, encrypted, and verifiable transaction ledger in a distributed form.
Blockchain provides complete transparency which simply means that any shady activity in the network can no longer remain in shadow. Moreover, its sequential storage ensures that each transaction can be verified at any time. By creating a strong, organized, and interconnected mesh of blocks the Blockchain-based cloud storage also facilitates immutable transaction records to verify ownership and identity.
Blockchain’s distributed and hash encrypted ecosystem helps in building a trustworthy ecosystem and proactively protects against handling and fraud while reassuring that no compliances are breached by any member.
The use of smart contracts provides a strong technical provision to automatically enforce condition-based rewards and penalties. In short, it can be compared to self-sustainable machinery that can work automatically, independently, and accurately. Instead of residing on a central machine, the Blockchain is equally distributed across many systems that offer its immunity against major attacks.
Blockchain-based cloud storage perfectly combines security and scalability with its interlinked blocks, hashing functions, and decentralized architecture. It makes the technology an ideal choice for adding an extra layer of security to the cloud storage. By wisely incorporating Blockchain technology in cloud storage the businesses can reduce their IT expenditure, enhance the security, and streamline the flow of information.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
10 
10 
10 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/we-distribute/the-do-everything-system-an-in-depth-review-of-hubzilla-3-0-692204177d4e?source=search_post---------343,"There are currently no responses for this story.
Be the first to respond.
As a platform, Hubzilla holds a tremendous amount of versatility and depth in what it can do. Mike Macgirvin, the platform’s creator, has famously quipped that the system does not have a commercial analogue to draw comparisons to — it’s not just a social network, it’s not just a forum, it’s not just a cloud storage solution, and it’s not just a content management system.
According to the definition on the project website:
Hubzilla is a powerful platform for creating interconnected websites featuring a decentralized identity, communications, and permissions framework built using common webserver technology.
So what does that mean, and who is this for? Let’s dive in to the latest release, and walk through this system one step at a time. There’s a lot of different features and aspects to this platform, which is partially why it’s so difficult to apply one standard label to it.
By default, Hubzilla runs on the LAMP stack — that’s Linux, Apache, PHP, and MySQL. If you have any experience with setting up Wordpress, Drupal, or a phpBB installation, you’ll feel right at home here. The project provides no-frills instructions on how to get a standard Debian-based installation up and running.
If you’re feeling adventurous, it is possible to go off of the beaten path and substitute components — for example, you can set up an installation that sits on top of FreeBSD + Nginx + Postgres, and you may see significant performance gains in doing so. The caveat is that it’s up to the sysadmin to determine how best to configure those components.
Most of this setup and configuration were extremely straightforward. The only tricky part I experienced here was in setting up a mail tool to send out service emails for my hub. Ultimately, I settled on postfix after asking for advice.
The first thing to understand about Hubzilla is that every user has a channel. This is a central concept to how the whole thing works.
What is a channel, exactly? Basically, it’s a space on the web that contains its own stream, one of which is populated with various objects and provides a permissions system and mechanism for subscribers. With this in mind, channels can be used for:
Conceptually speaking, any channel can be one of those four things. Any channel can cross-interact with any other kind of channel. It’s even possible to set up a channel to act as a feed bot, pulling in statuses and interactions from multiple sources upstream.
Forum channels are particularly noteworthy because they are analogous to Facebook Groups. Hubzilla uses the bang syntax, a tried and true convention used by GNU Social and its predecessor StatusNet.
That’s all you have to do, and in turn everyone that’s subscribed to the forum channel will get your post. From there, they’ll be able to send their interactions back to the relay for participants to retain each other’s responses.
This model is well-suited to ad-hoc group communication, and as a result the community around Hubzilla uses them effectively. Some channels focus on playing chess, others talk about TV series, and some focus exclusively on the project itself.
Hubzilla’s network stream is comparable to what you’d expect with Facebook or Diaspora. It shows all of the content coming in from your channel’s connections, and includes status updates, photos, comments, and reshares. Because the platform is not a microblogging network, statuses are threaded together with comments.
As a bonus, certain widgets can extend the functionality of how your stream is used. For example, you can filter statuses by Privacy Groups (comparable to Diaspora’s Aspects) or you could filter by hashtags or dates, or by contact affinity. One of the most useful widgets I use also connects me directly to forum channels, so that I can easily interact with them without leaving my stream.
Incidentally, it’s possible to write entirely custom widgets, but we’ll get into that later.
From the ground up, the platform is built around user privacy. The core design philosophy revolves around giving users a permission system so that they can define who should receive something, and who should see it.
This is no small undertaking — every feature and every function ties into two things: permissions, and decentralized access. Hubzilla is capable of defining who is eligible to receive a piece of data, and who is allowed to access a resource that lives in a particular space online.
The decentralized access component of Hubzilla is called OpenWebAuth, and it’s something that no other federated web platform currently offers.
Most traditional systems rely on privately sending everything between mutually connected servers — status updates, images, files, videos, and likes are all pushed back and forth through a relay system. This can cause significant scaling issues, and as a side effect, most federated systems are constrained to send only a few types of activity objects to each other.
Hubzilla, by contrast, only sends status updates, interactions (editing a post, updating your profile), likes, and messages. Everything else lives natively on the host server, and if someone wants to access something from you, they’ll have to visit you.
The beauty of this is that Hubzilla is designed to recognize who is visiting, and grant permissions as necessary. Hubs are contextually aware of who is accessing a resource, be it a page, a comment form, a wiki, or anything else the system can provide.
This experience is seamless, and can make two connected spaces on the web act as if they were one. All of the system’s features are built around this concept, meaning that you can play chess, edit a wiki, comment on someone’s photo, access a file, or message a group through someone else’s website.
Hubzilla’s core is built on a concept called Nomadic Identity, and it’s something that greatly differentiates Hubzilla from other federated platforms.
The idea is relatively simple: your channel does not have to be tied to a specific domain or a particular hub. Instead, it is entirely possible to migrate your posts, files, and contacts to another server, and you can connect multiple channels together to act as a relay system for every post you make.
Cloning a channel is a relatively straightforward process: export your channel and profile data, and provide it for upload when registering an account on a different server. You can decide at any time which of your cloned channels will serve as a primary, and which will serve as a secondary relay.
It’s worth noting that when you post from one channel, your connected clones will relay that post on whatever hubs they’re on, and vice versa. This actually makes the platform relatively censorship-resistant, as a channel could theoretically have many clones across the federated web.
The initial registration process can seem intimidating at first to newcomers. Upon registering, users are prompted to create their channel, set permissions, then edit their profile details and start uploading pictures. Profiles feature an extensive set of fields, most of which are optional and can be updated at a later time.
One interesting side feature of profiles is that it’s able to create “things” and append them to your profile. What exactly is a thing? It’s essentially a small object containing a few metadata fields that lives in your profile section.
You can append things with verbs such as “has”, “wants”, “likes”, or “dislikes”, and you can use that to build small lists on your profile organized by those verbs.
Overall, it feels a little gimmicky, but at the same time it serves as a nice stand-in for the lists you might expect on a Facebook profile. It also allows your other Hubzilla contacts to like those objects as well, without relying on a service-wide like graph.
One of the first thing you’re going to want to do when using Hubzilla is find some channels to connect with. By default, Hubzilla connects to a directory system of known channels who have opted in from all across the network. It’s possible to use the directory to find active channels based on interests as well as locations.
Some channels in the directory are inactive or abandoned, but many of them are active within Hubzilla’s small, tightly-knit community. Additionally, the directory can show you all channels present on your own hub, which can be handy for people who want to use the platform to build a community of their own.
Out of the box, Hubzilla uses bbcode for formatting text. For anyone that has extensive experience in using PHP message boards (vBulletin, phpBB, and SMF come to mind), the syntax and conventions will be all-too-familiar.
Posts in Hubzilla give the impression of being somewhere between a status update, a forum post, and a blog entry. In truth, you can easily use it for either purpose, and all posts can include HTML content, media, and attachments. Upon toggling certain features, posts can also include voting tools for proposals, and can even accept sticker responses.
If writing all of your posts in BB markup isn’t in line with your tastes (hello, 2004!), don’t worry! It is also possible to switch to using Markdown by default, which ultimately requires you to type fewer characters.
This platform leverages two methods for tagging: traditional hashtags, as well as category tags. The main difference is that hashtags are used to find all related posts that a hub knows about, whereas category tags are more useful for providing a topical filter on your channel’s landing page.
If you end up using post tags, they’ll be displayed on your channel as a filter for similar tagged content. This is a fairly standard expectation of blogging platforms, but it’s a million times more organized than what Facebook’s timeline can offer. As a bonus, you can use these to filter subscription feeds — for example, if you’re part of Planet Gnome and only want to syndicate your Gnome-related posts over there, you can do that.
In some circumstances, you’ll receive comments from people that you aren’t connected to. Sometimes it’s a person who just wanted to offer their thoughts on your post. Other times, it’s a spam bot. Hubzilla makes it easy to moderate comments coming from people you don’t know, giving you the opportunity to decide what to keep and what to discard.
The platform ships with an events system, and overall it’s comparable to MeetUp or Facebook Events.
Creating an event will place a marker for it on your channel’s calendar, making it easy to keep tabs of what’s going on at a monthly, weekly, and daily view. Clicking any event will bring up a lightbox with a direct link to an event’s post.
One added benefit of calendar events is that they get inserted into your channel’s stream upon creation. People who can see your event can decide on their attendance, in turn adding your event to their own calendars.
At first glance, the photo album functionality is quite similar to what you’d expect from Facebook, DeviantArt, or even Flickr. Photos can be sorted into individual albums, given captions, and allows users to tag each other in pictures.
If a photo contains EXIF data, Hubzilla will allow a user to show where the photo was taken using OpenStreetMap tiles. Individual albums can also be embedded into different widgets on your site, making it easy to build out an artwork showcase or a wedding gallery.
One nice bonus here is that any photos that you upload to your channel will instantly be put into cloud storage, making it very easy to retrieve entire albums from a file manager.
One of the things that really sets Hubzilla apart from other communication platforms is its inclusion of permissions-based cloud file storage. When enabled, the system provides a DAV share that can be accessed either by way of browser, or through your local file manager.
As far as behavior is concerned, the use case is not unlike using Dropbox or Nextcloud. One interesting feature is that all of your photo albums are stored here as well, so it’s easy to always retrieve your pictures when needed.
It takes a little bit of getting used to, but overall this core feature ranks as one of the more powerful and intuitive parts of the the platform. This feature can effectively replace iCloud, Google Drive, or DropBox with a decentralized solution.
Another neat aspect of Hubzilla’s cloud sharing capabilities is the ability to sync calendars and addressbooks across various different clients. This effectively allows Hubzilla to act as a drop-in replacement for iCloud’s Calendar system
In my testing, I was able to connect and sync calendar events between Android, GNU/Linux, and Hubzilla’s native web interface. There’s a great community guide that demonstrates how to treat CalDAV as a native resource in Android.
Unfortunately, I had difficulties with getting it to work for the native Calendar app on macOS, but I have no doubt that this may work in the near future — the main issue resided with how Apple’s Calendar app leverages DAV integrations. This is an area of Hubzilla that is a little bit under-documented, as the core audience at this moment is predominantly GNU/Linux users.
One other neat thing is that you can import iCal feeds from other websites. This makes it possible to track events from your favorite venues or groups, and each calendar is also provided for any devices that the calendar is being synced with.
The only real downside is that the Events calendar and your personal calendar are two different systems that don’t touch each other, and some of the functionality does not overlap between these two places. At some point in the future, it is hoped that these two things will be bridged together.
Another great feature that integrates directly into channels is Hubzilla’s wiki system. Users can create an indefinite amount of wikis for a given channel, and create markdown-formatted pages for each wiki.
Creating wiki pages and formatting content is fairly straightforward, and provides the basic set of features for what you’d expect from a wiki system. Pages can link together, images can be embedded from your photo albums, and changes can have a revision message included for posterity.
It’s also possible to compare changes between revisions and revert to earlier versions of a page, which could be useful in situations where multiple people are editing the same wiki.
The platform ships with a chatroom that people can visit in-browser. Because Hubzilla automatically knows who the visitor is, identity is automatically assigned and permissions are handed out.
The chatroom is relatively simple, and feels like a mashup of Facebook Messenger and IRC, and every interaction happens in real-time. The use-case is not completely obvious in 2018, given the plethora of dedicated messaging apps out there, but it’s still a welcome addition to the platform.
Like Wordpress or Drupal, Hubzilla also offers an extensive plugin system that allows site admins and users to greatly increase the platform’s functionality. Some of these plugins are purely cosmetic, such as the ability to set a default landing page for logged in users, while other plugins dramatically extend what the system is capable of doing.
Although the plugin largely exists as a proof-of-concept, it is entirely possible to play a game with other Hubzilla contacts. It works as advertised, and movements are recorded by federating comments back and forth with the FEN notation format. As a side effect, it’s possible to rewind through the entire game history and watch the breakdown of moves.
Some of the more interesting plugins actually pertain to Hubzilla’s ability to federate. Out of the box, Hubzilla only federates to other instances of itself using the Zot protocol. However, the plugin architecture allows users to federate to Diaspora (and by extension, Friendica), Mastodon, GNU Social, and anything else that manages to speak either OStatus or ActivityPub.
Unfortunately, not all participants in the conversation will be able to see all of the comments. People on Diaspora can’t see Mastodon comments, and vice versa. This could theoretically be solved if every actor supported one common protocol between them, but we’re still a far way off from that.
For networks that don’t support direct federation, Hubzilla can act as a client app and send over a copy of a post to networks such as Libertree, Pump.io, Twitter, Wordpress, Dreamwidth, Livejournal, InsaneJournal, and hypothetically any other network that someone writes a plugin for.
As one of the more active theme developers on the Hubzilla platform, I feel comfortable with developing themes for it. There are several layers to this system that are noteworthy, and as someone who has developed many themes for the platform over the years, I feel that I can provide some special insight here.
By default, the platform ships with a Bootstrap-derived base theme called redbasic. It’s a relatively no-frills theme that provides many views and templates for the core Hubzilla experience.
Because of the platform’s high dependency on this theme, it is generally recommended to write a “derived theme”, which overrides defaults based on what is present in your theme folder (CSS, JS, templates, views, etc). When I started getting into Hubzilla theme development, I used this guide to build my first derived theme.
Hubzilla ships with its own templating framework, called Comanche. The project describes it as such:
Comanche is a markup language similar to bbcode with which to create elaborate and complex web pages by assembling them from a series of components — some of which are pre-built and others which can be defined on the fly. Comanche uses a Page Decription Language to create these pages.
Here’s a quick demonstration of how I’m using it on the Network page in my theme. It lives in $themename/pdl/mod_network.pdl, and overrides the defaults provided by the base theme that it’s extending. Keep in mind, this only affects the layout of the Network page. Other views will need to be modified by their own respective mod_{view}.pdl file
In practice, it isn’t all that different from Drupal’s regions and blocks system, or the widget framework used by Wordpress. It’s incredibly easy to use, and frontend developers can drop their preferred widgets into specific regions.
Any existing widget can be inserted into a region via a Comanche template, and it’s possible to even write your own and use it with the template system.
One of the lesser-explored areas of the platform involves the possibility of writing custom widgets. Doing this can greatly extend the functionality of the interface, and fundamentally change how the whole thing works.
As someone who doesn’t write a whole lot of PHP, figuring out the conventions for loading and rendering specific object field data took a lot of figuring out. Basically, you’ll need two files: one .tpl file for providing a markup template, and one .php file for populating that template with values from database fields.
Once you’ve got that set up, you’re in business. Just load the widget into a region on your theme. In my case, it was [widget=usermenu][/widget]. Some trial and error was involved in properly building the channel-specific URLs for each link, and some additional logic still needs to be written to hide certain menu items from visitors. As a first attempt, though, I was pretty satisfied.
Any CMS wouldn’t be complete without the capability to create pages, and Hubzilla leverages all of the conventions I’ve just highlighted for that purpose.
Every single page can have use its own layouts, blocks, widgets, and menus to build any kind of page you want. Additionally, all of this can hook into data provided by Hubzilla’s extensive internal API, meaning that you can populate UI elements with data that lives in your channels.
Working with the Page / Block / Widget paradigm is similar to how Drupal or Wordpress approach this problem, but the interfaces for building these different elements can sometimes feel counter-intuitive.
It’s not always clear when you should develop a block with an HTML content type, or when you should build a block with something else. Because they can all be intermingled together within a page, it’s not entirely clear whether that matters.
Last but not least, I’d like to talk about the Hubzilla community. It is a rather small group, with a few hundred installations and a couple thousand active users. Much of this community has relied on organic growth, with virtually no advertising, promotion, or hype to speak of.
One neat thing is that Hubzilla dogfoods itself for all of its community operations. The project announces its new releases through one channel, community developers talk about their projects through another, and a Support Forum exists to help people who are hosting their own hubs.
As far as developer empowerment is concerned, the Hubzilla community is second to none. It is very common to reach out to the community with a request for help, and receive detailed advice the same day that you ask for it. This applies to inquiries ranging from “Why is this badge not updating?” to “How do I hook into specific database fields for a custom plugin that I’m writing?”
For all of the endless platitudes about “The Open Source Community” supporting itself, Hubzilla actually embodies the majority of its positive virtues. A community of people are using a platform and hacking on it together collaboratively, and very little drama exists within the community itself.
My overall ruling on this system is obviously going to be biased; I have been running Hubzilla on my personal server for the past few years, and have followed its development for a very long time.
I’m absolutely blown away at everything this system provides and is capable of. The fact that so many of its bespoke technologies are passed over in favor of other solutions feels downright criminal.So much of what this system provides surpasses what other federated platforms have even begun to think about — aside from Friendica, virtually none of the other popular federated systems even support organized photo albums, and most can only speak one federation protocol.
Hubzilla still has some rough edges, specifically when it comes to mapping resources that are intended to be accessed through different endpoints. One of my biggest frustrations is the fact that it actually ships with two calendar systems — one for events, the other for CalDAV. It has been suggested that eventually events will be synced between the two of them, and I’m willing to overlook such shortcomings because of what the platform and community already provide.
At this time, Hubzilla is best suited towards people who want to host their own websites that can also act as a cloud storage provider as well as a decentralized social communication platform. It is meant for the homesteaders of the Internet generation, people who want to withdraw from third-party services and data providers.
Sean Tilley is an active user of the Hubzilla platform, and uses it on his personal website. He also has developed a handful of themes to change Hubzilla’s core look and feel.
Reporting on decentralization and the free web.
473 
8
Some rights reserved

473 claps
473 
8
Reporting on decentralization and the free web.
Written by
Editor of WeDistribute. Obsessed with Free Software and Decentralization. Also makes things, sometimes with Elixir.
Reporting on decentralization and the free web.
"
https://medium.com/@hubtechinfo1/why-artificial-intelligence-and-cloud-storage-is-a-deadly-combination-8834a26c7dc7?source=search_post---------117,"Sign in
There are currently no responses for this story.
Be the first to respond.
hubtechinfo
Jan 29, 2020·4 min read
Present era is definitely an era of technological advancements and upliftments. We all are witnessing a tremendous growth in the IT sector where emphasis has been laid down on making machines smarter and intelligent just like humans. You have often heard about an interesting and popular term “Artificial Intelligence”. Most of the big IT companies in both India and USA are working on this technology. So, what this technology is all about and why it is gaining so much attention among the IT enthusiastic?
We have decided to cover this topic in this so that our valuable readers will come to know about the benefits of this technology. Furthermore, we will also elaborate that why the combination of Artificial Intelligence and Cloud Storage is a deadly combination that is producing Thousands of new jobs every year in the IT sector.
Artificial intelligence is basically a leading advancement in the technological upliftments of this decade where machines are made to learn things by themselves just like human beings by collecting and analyzing the data based on different experiences.
In simple words, AI technology is designed to make machines more intelligent and smarter just like humans. To implement AI, another very important term known as Machine Learning which is a core part of AI needs to be understood.
In Machine learning, data from various categories is collected and then analyzed to get some useful information. Billions of Gigabytes of data is collected on the daily basis and then analyzed to get some relevant data for the machine. For example, if a hacker attacks on a machine and the machine is unable to defend that attack, then a change in the working algorithm of the machine is made by analyzing the data from other places will help to defend that attack automatically from the next time.
Now, it is really very important to understand that the change in algorithm is made by the experience of other machine that has successfully defended that attack. This data is collectively taken from the data inputs from cloud storages. Though it is really very hard to realize both Machine Learning and AI because of the requirement of a continuous change in the working algorithms. But that doesn’t enough to stop the people who are on these projects.
Complex algorithms are designed from the data inputs and applied on the machines to realize AI. Machines will simply learn things from the experiences and observations they made on regular basis. Many different areas like Speech recognition, Computer Vision and Symbolic learning, Robotics are parts of AI which are pretty much complex and hard to implement.
Cloud Storage is another very important aspect of AI and ML which is used to store Millions of Gigabytes of data online. The collected data from online Cloud storage can be fetched anytime when it is required.
As we already mentioned above, there is a need of huge input data to make machines intelligent and smarter just like humans. That’s why Cloud Storage becomes an integral part of this technological advancement.
Most of the data gathered from online resources is stored directly in the cloud data storage space or servers from where it can be retrieved easily without any use of physical hard drives. This directly makes management of data and implementation of AI much easier by eliminating the use of physical storage devices.
There are a number of countless advantages of using Cloud Data Storage space and we strongly believe that it would make the implementation of AI more effective.
After learning both the concepts separately, now we will explain you why this combination of AI and cloud storage is effective?
Here the most important thing to understand is that there is a need of continuous data input from different resources for AI. All these data inputs are manipulated and analyzed to get the relevant data useful for Machine Learning and Artificial Intelligence. That’s why, both of these are integrated to make sure that the data processing of the inputs is done precisely and effectively. This combination is proving itself to be very beneficial for the businesses that requires AI solutions for marketing and leads purpose.
Leading tech giant companies like Microsoft, IBM and Google has created their own personalized AI clouds that are helping themselves to store Millions of Gigabytes of data at one place for its processing and analyzation. Moreover, Annual reports have confirmed that combining both AI and Cloud Storage technology has given them a boost for expanding their businesses.
AI based machines are performing pretty much effectively and efficiently in terms of every aspect. For an instance, AI based educational tablets can take the help of Cloud storage to retrieve the previous educations research work to help the students in more effective manner. Furthermore, using Cloud for implementing AI in the Health sector is also very influential as Doctors can now view Millions of similar cases if the data is present in the Intelligent Cloud.
There are boundless capabilities of both AI and Cloud storage that is responsible for making it a deadly combination. From a basic level of intelligence of Machine to make it as smart as humans was not an easy task. But after the introduction of Cloud storage in this context, it has been made possible very precisely.
Conclusion: There is no doubt that there is a huge potential in the combination of AI and Cloud Storage to deliver magnificent results to the users in for the betterment of technology. Moreover, Thousands of brains are working behind the curtains to make this combination more powerful, effective and deadly. Devices like Amazon Alexa or Virtual Assistants like Microsoft Cortana and iPhone Siri are very simplest scenarios of the implementation of AI that are having their importance in our daily life.
Tech News | Virtual Reality | Cryptocurrency News | Digital Marketing | SEO | https://hubtechinfo.com
1 
1 
1 
Tech News | Virtual Reality | Cryptocurrency News | Digital Marketing | SEO | https://hubtechinfo.com
"
https://medium.com/productivity-in-the-cloud/5-cloud-enabled-apps-for-designers-76b36741a535?source=search_post---------251,"There are currently no responses for this story.
Be the first to respond.
Crafting great design is no peace of cake! Luckily there is a new breed of tools that integrate with your cloud storage and address some of the biggest challenges faced by designers today.
InVision let’s you manage, collaborate and test your design projects. Used by companies like AirBnB, Evernote, Box, Shopify and others, this tool is extremely powerful. Connect your Dropbox (Box support coming soon) and sync all your project assets. Your first project is free.
Also worth trying out: Zurb Apps (the interface was too slow for us, and has no cloud storage support), RedPen (great design, cloud storage support, but only works for annotations).
Marvel is probably the easiest way to create interactive prototypes from your web or mobile designs. You start by connecting your Dropbox or Google Drive and simply select designs straight from there. After that, its just a matter of minutes before you create clickable prototypes. Share them, test them, iterate — create great design.
Also worth trying out: Solidify (let’s you create clickable prototypes, but has no cloud storage support)
POP let’s you link quick paper sketches (or finished designs) and create interactive prototypes including awesome effects. This thing gives life to boring sketches and designs. On top of that, it integrates with Dropbox and let’s you bring in your assets straight from there.
Also worth trying: Launch (let’s you create mobile prototypes in your browser, no cloud storage support).
Procreate is a powerful tool for illustrations with its dual-texture brush system and ultra high definition (4096 x 4096px). And you can bring all your brushes, textures, swatches and save your projects with Dropbox. A bonus is their video functionality that records you creating your art. All saved on your Dropbox of course.
Also worth trying: Sketchbook by Autodesk (also comes with Dropbox support)
Splashbox is pretty simple, but saves a lot of time. Just connect your Dropbox and it will automatically deliver hi-resolution photos from Unsplash straight to your Dropbox. Plus when you connect you get the initial 500 or so photos right away. Awesome resource!
This is a pretty powerful idea — Octobox let’s you organize (tag, create collections and stacks) from stuff in your Dropbox. This is ideal for collecting visual inspiration, storing snippets and of course keeping all your notes.
Let us know your favorite apps in the comments.
Originally published at blog.cloudfender.com.
A collection of tips & tricks about productivity in the…
10 
10 claps
10 
Written by
Keep all your files in one place. Sync Dropbox, Google Drive, Box & more to the cloud service you love at www.cloudfender.com
A collection of tips & tricks about productivity in the cloud from all over Mediu
Written by
Keep all your files in one place. Sync Dropbox, Google Drive, Box & more to the cloud service you love at www.cloudfender.com
A collection of tips & tricks about productivity in the cloud from all over Mediu
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/the-high-price-of-traditional-cloud-storage-2909d737e1c9?source=search_post---------83,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Nov 26, 2018·5 min read
Each year, the amount of data annually created roughly doubles. Cloud computing has emerged as a highly efficient and profitable enterprise where the largest companies on the planet are investing billions of dollars and Moore’s Law continues to hold.
In such an environment, wouldn’t you expect cloud storage prices to drop?
Unfortunately, the answer is no.
There are only three major cloud storage providers in the world outside of China: Amazon, Google, and Microsoft. While cloud storage prices initially dropped when Google entered the market, prices have essentially stagnated for the past five years, although the cost of hard drives have decreased by about 50 percent over that same timeframe (dollar-per-gigabyte) [source].
Cisco estimates that by 2021, there will be four internet-connected devices for every person in the world. That’s a lot of data, and it’s not even counting technology that isn’t connected to the internet like 4k cameras and other devices that create some of our biggest datasets. In spite of this proliferation of data, the cost to store data is remaining flat. That means those who are responsible for storing data (and paying to have it stored) are seeing their costs rise at the same rate as we create data. So if your storage bill is roughly 10% of your total costs, in 12–18 months, it will be closer to 18%, assuming all other numbers remain the same. This ratio can quickly become burdensome to small, open source projects or free services that we all use on a daily basis.
Cost of Innovation
One such case is transfer.sh, a command line file transfer tool created by Remco Verhoef. In an average month, transfer.sh is used more than 1,000,000 times to easily transfer files worldwide. The project started out as all projects do — Remco wanted to simplify his own life by making it easier to send file transfers from one place to another. His open source project quickly generated interest and users quickly grew. The service was free, but that meant that Remco was the one footing the storage bill. A few months ago, transfer.sh announced it could no longer afford its Amazon S3 bill and would shut down at the end of November.
I’d never heard about transfer.sh at the time, but when it was announced the service would shut down for financial reasons, its passionate community quickly rallied to find a solution. I did some research and knew that Storj could help transfer.sh stay alive! I recently contacted Mr. Verhoef to discuss my findings, and we’ve been working with the transfer.sh team to store their data on our internal infrastructure while we fine tune our new network for production launch.
“When I mentioned my plans to shut down transfer.sh, there where many people trying to help out, asking if I couldn’t transfer the domain to them, etc. But when I started transfer.sh, I didn’t want to create a site bloated with advertisements and timers all over. I just wanted to build something quick and cool to just fulfill one need, transfer files. For over 4 years, I paid about $150k to $200k in total, just to keep a free service, which I believed in, running. Now, with having Storj as a partner, we don’t have to put ads on the platform and can continue transfer.sh under my original vision. It also opens up new opportunities, like continued development, adding Storj as an extra storage layer into the open source transfer.sh codebase,” Remco says.
Once our new decentralized cloud storage platform launches in production, we will work with transfer.sh to migrate all its data from our servers to the Storj network. We are excited to be working with such an awesome tool and are happy we could help save it in time.
Flighting Big Cloud
Transfer.sh’s challenge highlights a larger problem many companies are facing. Use of data is growing at an exponential rate, hard drive prices are decreasing, but centralized cloud storage providers are not reducing their prices to match. What can be done?
We think that one clear answer lies in the fact that the vast majority of disk drives in the world are severely underutilized.
Decentralized storage has the ability to empower the technology community in new ways. By creating a more affordable storage platform for companies, developers and individual users, we expect we will see many use-cases that were once deemed too expensive become a reality. Unlike centralized cloud storage providers, decentralized storage platforms don’t have to invest $600M in a new data center, pay for expensive fire suppression systems, or repave the data center parking lot. Those cost savings are passed along directly to users.
And even with decentralized cloud storage being in the early days of its innovation cycle, it is still drastically more affordable than centralized solutions. As it continues to evolve, it is likely to drop in price, just like centralized cloud storage did in its early days. This will be hugely impactful for consumers and developers. If it were not for this new economic model made possible through decentralized storage, we might never see another massive drop in cloud storage costs. As our technological capabilities evolve, a paradigm shift is often necessary for the next wave of innovation to reach the shore. Our team at Storj is excited to see what cool use-cases the decentralization tide washes in.
By Shawn Wilkinson, Cofounder and Chief Strategy Officer at Storj Labs
Originally published at storj.io on November 26, 2018.
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
See all (1,648)
100 
100 claps
100 
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/build-aws-s3-compatible-cloud-storage-on-gcp-with-minio-and-kubernetes-2adc0a367f98?source=search_post---------25,"There are currently no responses for this story.
Be the first to respond.
Applications today generate more data than ever, and this upward trend is expected to keep up in foreseeable future. How do you handle this ever growing storage requirement of your application? A storage solution that can run where your application runs and can scale with it in an automated manner, is the way to go. Add multi-tenant capabilities and it becomes near perfect!
Minio provides a reliable, lightweight object storage service. Running it on an orchestration platform like Kubernetes, adds automated storage mapping and multi-tenant capabilities. Such setup has a clear separation of concerns — one of most important parameters for scalability. Also, it can be very easy to find and isolate errors in such a setup.
Minio running on orchestration platforms like Kubernetes is a perfect solution for growing storage needs.
In this post, we’ll see how to build AWS S3 compatible object storage server on Google Cloud Platform with Minio and Kubernetes. We’ll also see how you can scale this setup for a multi-tenant environment.
Minio is a lightweight, AWS S3 compatible object storage server. It is best suited for storing unstructured data such as photos, videos, log files, backups, VM and container images. Size of an object can range from a few KBs to a maximum of 5TB. Salient Minio features include
For readers not aware of Kubernetes terminology, I’ll quickly go through all the terms used in this post.
Pod: A pod is the smallest unit of computing in Kubernetes. It is a group of containers running in shared context.
ReplicaSets: A ReplicaSet ensures a specific number of pod replicas are always up and running. While ReplicaSets are independent entities, they are mainly used by Deployments as a mechanism to orchestrate pod creation, deletion and updates.
Deployment: A deployment can be thought of as an abstraction containing Pods and ReplicaSet.
Service: A service defines a logical set of Pods and a policy by which to access them. The set of Pods targeted by a Service is determined by a Label Selector (defined in Service’s yaml file).
Persistent Volumes: A Persistent Volume (PV) is a piece of networked storage in the cluster with storage specific details abstracted away.
Persistent Volume Claims: A Persistent Volume Claim (PVC) is a request for storage by an application/pod.
To get started you’ll need a Kubernetes cluster running on Google Compute Engine (GCE). Follow these detailed steps on setting up a Kubernetes cluster on GCE.
With persistent volumes (PV) and persistent volume claims (PVC) — Kubernetes makes it very easy to abstract away physical storage details from your application. You can just create PVs with the physical storage in your cluster and then let your application ask for storage it needs via PVCs. As a storage request is made via PVC, Kubernetes maps it to actual storage (PVs) automatically.
Let’s explore this further in Google Compute Engine context. GCE has disks that serve as physical storage for your compute nodes. On Kubernetes, you can create PVs that use these disks as the backbone physical storage.
Later, as you deploy Minio on the Kubernetes cluster, you can create PVCs to request for storage that you need for that particular Minio instance. Kubernetes automatically binds matching PV to PVC. This is known as static binding in Kubernetes world, and yes, there is dynamic binding too, but we’ll skip that for now. Read more about binding here.
Now that you’ve a clear picture of how things work, lets start with creating a GCE disk.
This creates a disk named disk1 with a size of 10GiB. Now create a PV based on the GCE Disk we just created.
Download and save the file as minio-gce-pv.yaml. You can then create a persistent volume using the command:
A deployment encapsulates replica sets and pods — so, if a pod goes down, replica set makes sure another pod comes up automatically. This way you won’t need to bother about pod failures and will have a stable Minio service available.
But before creating the deployment, we need to create a persistent volume claim (PVC) to request storage for the Minio instance. As explained above, Kubernetes looks out for PVs matching the PVC request in the cluster and binds it to the PVC automatically.
This automation can come in very handy if you need a large scale multi-tenant environment with varying storage requirements. You can spin up a Minio deployment (with a PVC requesting appropriate storage therein), per tenant. Kubernetes automatically binds the PVCs to PVs. This way you have a multi-tenant, stable, S3 compatible object storage server at your command!
Here is how you can create a PVC and a single pod deployment running Minio Docker image.
Download and save the file as minio-standalone-deployment.yaml. Notice that we create PVC first and then the deployment uses it as its volume. You can then deploy Minio using the command:
Now that you have a Minio deployment running, you may either want to access it internally (within the cluster) or expose it as a Service onto an external (outside of your cluster, maybe public internet) IP address, depending on your use case.
You can achieve this using Services. There are 3 major service types — default type is ClusterIP, which exposes a service to connection from inside the cluster. NodePort and LoadBalancer are two types that expose services to external traffic. Read more about services here.
Below yaml file configures a LoadBalancer service for your Minio deployment.
Download and save this file as minio-service.yaml and run the command —
The IP address where the service is served generally takes a couple of minutes to be created after the above command is run. You can check the IP using —
Once you have the IP address available, you can access Minio via the address
http://<Service_IP_Address>:9000/
Access Key and Secret Key remain the same as the environment variables set in minio-standalone-deployment.yaml.
Note that LoadBalancer will work only if the underlying cloud provider supports external load balancing.
Kubernetes comes bundled with a neat dashboard. You can easily track your Minio pod’s memory, CPU usage and many other metrics via the dashboard.
To access the dashboard, execute the command —
Access the URL mentioned against kubernetes-dashboard. Here is how my dashboard looks
Need help? We hangout on Slack. Join us!
Google Cloud community articles and blogs
70 
3
70 claps
70 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Minio.io
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/dxchainglobal/dxchain-releases-the-first-blockchain-cloud-storage-product-b014f4ab2305?source=search_post---------68,"There are currently no responses for this story.
Be the first to respond.
We are excited to share with our community the launch of DxChain Testnet and SDK, named Alps Mountains, after 3 months of development and testing since the release of Testnet Beta — Andes Mountains at the end of September 2018.
The launch of Testnet and SDK has again showcased the DxChain research and development team’s capability of delivering the project on time, and implementing the distributed storage powered by blockchain as promised. Moreover, the DxChain Testnet SDK will provide blockchain engineers, researchers and DxChain’s partners with solutions to data storage and file management in a distributed and secure system.
In the previous release, Testnet Beta (Andes Mountains), DxChain introduced major functions including transaction, file storage, a block browser, and started the scalability testing. Please find more details on our official blog.
medium.com
After the release of Testnet Beta, DxChain team continued focusing on the development of the data storage features. In a nutshell, features and performance of the DxChain Testnet are significantly improved in this release. Key breakthroughs of this release include:
In addition to adding features and improving the performance of the testnet, we also updated new features of the DxChain Testnet Explorer, and released the DxChainPy SDK and a brand new product — DxBox.
In this release, we improved the function of the DxChain Testnet Explorer, and added some new features. For example, detailed information about all types of contracts and transactions are searchable, including the file contract id, initial settings, current status, etc. You can find out the various possible status of the storage contracts:
(1) Ongoing, means the lifespan of a storage contract, from initialization to expiration; (2) Verified, means the contract has been verified by blockchain and the storage provider has got the reward from storing the data; (3) Vacant, means the contract is established but there is no file uploaded by the client, therefore, there is no need for the providers to submit the storage proof; (4) Failed, means the provider is not able to submit the storage proof before the contract expires.
DxChainPy is a Python implementation toolkit for DxChain API, and it marks the debut of DxChain SDK. Currently, the following chain data can be obtained from DxChainPy: network status, consensus information, wallet balance, miner status, provider settings and client details; while operations below are also supported: uploading, downloading and deleting files. From now on, by using DxChain API, developers are able to query on chain data and use features including transactions and file storage to build their own applications based on DxChain Testnet. Please visit our official website and GitHub for more details.
Along with the release of DxChainPy SDK, we also launched a new product — DxBox. DxBox is a decentralized blockchain storage product powered by DxChain testnet, and it can provide secure & private cloud storage for users all over the world.
According to the design of DxChain, economic incentive mechanisms are used to attract more nodes to share their idle storage capacity and to become the storage miners. In other words, the storage consumers will pay DX tokens to storage providers as rewards. At present, since all nodes are hosted by DxChain project team, we provide free qualifications for our community members and supporters to try out the blockchain storage through using DxBox. That means you can have your own secure and decentralized blockchain storage at no cost, and experience the blockchain storage capability provided by DxChain! Try it out here: https://explorer.dxchain.com/register/
In 2018, we released the Testnest Alpha and Beta versions in June and September respectively. The DxChain Testnet and SDK — Alps Mountains has been ready to launch at the end of December 2018, however, taking into account the new year holidays, we decided to officially release this version on January 7, 2019.
According to DxChain’s technical road map, we will enhance the data storage capacity before starting the development of the data computing features. DxChain has designed a “chains-on-chain” architecture to address the issues of data storage, computing and privacy protection. To be more specific, there will be a master chain, used for operating blockchain transactions while coordinating with two side chains — the computing chain and data chain. The computing chain is used for parallel computing applications to process large volumes of data, which eventually powers machine learning and business intelligence, while the data chain is used for storing data, protecting users’ privacy, and supporting the computing chain.
After the release of DxChain testnet and the data storage features, we will continue our development and follow our schedule to advance the development progress. The next phase of DxChain development will focus on supporting smart contract and solidarity-based economy, and building the programming interface between storage and blockchain system. The next release is projected at the end of 2019 Q1.
More efforts are being devoted to product development! We look forward to sharing with you more good news soon. Stay tuned!
About DxChain
DxChain is the world’s first decentralized big data and machine learning network powered by a computing-centric blockchain. DxChain is a public chain, it designs a revolutionary “Chains-on-chain” architecture to make blockchain function as a computing unit — data storage and computing, so that the technical characteristics of blockchain can be truly extended to a broader field, promoting the next generation of technology from the bottom. For more information, please visit www.dxchain.com.
Telegram: https://t.me/dxchainTwitter: https://twitter.com/DxChainNetwork
www.dxchain.com Big Data meets Blockchain
145 
145 claps
145 
Written by
http://www.dxchain.com — decentralized big data and machine learning network powered by a computing-centric blockchain
www.dxchain.com Big Data meets Blockchain
Written by
http://www.dxchain.com — decentralized big data and machine learning network powered by a computing-centric blockchain
www.dxchain.com Big Data meets Blockchain
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/the-developer-show-tl-dr-046-8d677c8c335?source=search_post---------245,"There are currently no responses for this story.
Be the first to respond.
Highlights: Localization on Google Play, automatic line breaking tool for Japanese on the web, new storage classes for Google Cloud Storage
The Developer Show is where you can stay up to date on all the latest Google Developer news, straight from the experts.
Have a question? Use #AskDevShow to let us know!
Drupe and Noom have expanded globally by localizing their apps on Google Play. drupe increased their daily active users by 300%, and actions per average daily user increased 25% in the Indian market. For more details on each and a link to the localization checklist, check out the post.
Here’s a cool new open source tool that does automatic line breaking for Japanese on the web. This is particularly important because line breaking for Japanese otherwise occurs randomly, usually in the middle of a word. The tools uses Cloud Natural Language API to analyze the input sentence, and it concatenates proper words in order to produce meaningful chunks utilizing “part-of-speech” tagging and syntactic information. The GitHub link is on the post.
There’s just been a major refresh of Google Cloud Storage which includes new storage classes, data lifecycle management tools, improved availability, and lower prices. All to make it easy for you to store your data with the right type of storage. More details with charts and examples are on the post.
Google Stackdriver was designed to make ops easier by reducing the burden associated with keeping applications fast, error-free, and available in the cloud… and it is now generally available for hybrid cloud monitoring, logging, and diagnostics. For more on the announcement and how to use Stackdriver including code, check out these links:
Chrome 55 Beta is now available with input handling improvements, async and await functions, CSS automatic hyphenation, and more. All the details and code are on the post.
Engineering and technology articles for developers, written…
6 
6 claps
6 
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate for Google. Improving life through science and art.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@nutanix/dont-let-2017-amazon-aws-s3-outage-like-errors-affect-you-again-c25217f73370?source=search_post---------289,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
Mar 1, 2017·3 min read
On February 28th, 2017, several companies reported Amazon AWS S3 Cloud Storage Outage. Within minutes, hundreds and thousands of twitter posts started making rounds across the globe sharing their experiences how their apps went down due to this outage.
Image Source: https://twitter.com/TechCrunch
The issue that kicked off around 9:44 Pacific Time (17:44 UTC) on 28th Feb, 2017, was reported primarily due to error in simple storage buckets hosted in the us-east-1 region, tweeted AWS.
Image Source: https://twitter.com/awscloud?lang=en
This AWS S3 outage led to major websites and services, such as Medium, Docker Registry Hub, Asana, Quora, Runkeeper, Trello, Yahoo Mail, etc. falling offline, losing images, or left running haphazardly.
According to sources, this AWS S3 outage disrupted half the internet. Because, Amazon S3 is used by approximately 148,213 websites, and 121,761 unique domains, according to SimilarTech.
Even though, AWS fixed it after juggling for several hours, its impact was huge.
Image Source: https://status.aws.amazon.com/
According to Nick Kephart, senior director of product marketing for San Francisco-based network intelligence company ThousandEyes, who monitored this situation closely said that during the outage information could get into Amazon’s overall network, but attempting to establish a network connection with the S3 servers was not possible. Due to this, all traffic was dead in its tracks. Hence, all the sites and apps that hosted data, images, or other information on S3 were affected.
As a company, how could you avoid this situation, or fool-proof your system against such outages in the future? Here’s how:
People who did not go for Region level DR and opted for AZ level DR in their session were affected. When you opt for Region level DR, you will have to copy data from one S3 Region to another S3 Region and sync that S3 data between Regions. This helps with availability of a backup, hence helping you cope with such outages in future.
Cross region replication helps to make it easier for you to make copies of your AMIs/snapshots to another in a second AWS region, so that you can always keep a system running in another region. Or, so to say, run the stand-by environment in another region.
Use CloudFormation template to create a cluster in another region, so that you don’t have to wait for a longer duration to spring back to normal. It will not take more than 2 hours for you to bring the cluster and whole environment back on track with CloudFormation.
All technologies might fail at some point. A Plan B in place is the best way forward.
Even though large swaths of the internet went down due to this Amazon AWS S3 Cloud Storage Outage, several sites and apps were not affected by it. Reason: They had their data spread across multiple regions.
If you know of any other way to mitigate such outages, do share it with us in the comment section below or tweet us @BotmetricHQ.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
See all (807)
1 
1 clap
1 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-storage-re-defined-alibaba-cloud-oss-part-2-43bb4c932bea?source=search_post---------180,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Dec 28, 2020·6 min read
By Shantanu Kaushik
Cloud storage is one of the most essential modules of the complete cloud computing resource. A dynamic and scalable storage solution can make all the difference when deploying applications with high read/write functionality or an extended user-base.
The Alibaba Cloud Object Storage Service (OSS) was designed to embrace the challenges faced by developers and administrators and turn them into benefits. The well-defined benefits and features make OSS a one-stop storage product for the cloud.
The Alibaba Cloud OSS delivers and protects the user data to provide a highly-available resource for static websites, mobile applications, or a DevOps solution. In this article, we will discuss the benefits and features of the Object Storage Service, along with its usage scenarios. We will showcase different models that are applied for different use-cases and how well they are formulated to tackle challenges.
Security is a pivot point for any product that handles massive amounts of data and information. It is imperative that the security practices associated with the Object Storage Service meet the industry standards.
The Alibaba Cloud Object Storage Service (OSS) offers multi-level security protection. Enterprises can leverage features, such as server-side and client-side encryption, hotlink protection, permission control, IP blacklist/whitelist, and WORM (write once read many) policies. With the Object Storage Service, you can easily isolate resources for multiple tenants and enable zone-disaster recovery.
The Alibaba Cloud Object Storage Service (OSS) is based on a multi-redundancy architecture with high-tolerance to faults. It is the backbone of data storage needs for every product within the Alibaba Cloud lineup. OSS has been known to eliminate single point of failures (SPOFs) to ensure a highly-reliable and available data storage solution.
Storage solutions need to be intelligent. A storage solution is the prime mover for any product and takes care of delivery and data storage needs. The Object Storage Service (OSS) facilitates multiple redundant copies of data for backup purposes and to maintain high-availability scenarios. It also provides image processing, video snapshot, SQL in-place query. OSS is known for its seamless integration with the Hadoop ecosystem and other Alibaba Cloud services, such as MaxCompute, E-MapReduce, Function Compute, Database Systems, and Data Lake Analytics.
The Alibaba Cloud Object Storage Service (OSS) helps users extract more value from their data. With simplified data processing and availability of products to cater to different types of processing needs, the Alibaba Cloud OSS is one of the most feature-rich products available. Apsara Video for Media Processing (MTS), MaxCompute, Image Processing Service (IMG), and BatchCompute are known for providing multi-scale data processing power to the user.
The diagram below shows how this works:
The Alibaba Cloud OSS provides one of the most advanced rich media processing services. With that, the array of formats and product connections within the Alibaba Cloud product cycle makes it a highly reliable and scalable solution.
The Object Storage Service (OSS) lets you manage the content of your website isolating different content types. The static objects, such as scripts, videos, and images, are actively separated from the dynamic content. The Content Delivery Network (CDN) creates cache copies of the static data and stores the data at the edge nodes in multiple regions for higher availability. This can also be achieved using the Border Gateway Protocol (BGP).
The diagram below shows how this works:
The Object Storage Service (OSS) allows for large amounts of data to be downloaded with concurrent requests by optimizing and providing high bandwidth operations. This is achieved using Transfer Acceleration.
Quick Info: The Alibaba Cloud Object Storage Service improves the data transfer (upload and download) speeds using Transfer Acceleration. It is an end-to-end transfer solution that incorporates protocol stack tuning, optimal route selection, smart scheduling of operations, and optimized transfer algorithms with server-side configurations of OSS.
The distributed data-centers are used for this acceleration mechanism. Every time a transfer request is received, the transfer mechanism decides on an optimal path and configuration for enhanced speeds and user experience.
By separating the static website content with the dynamic content, the Alibaba Cloud OSS ensures an enhanced content delivery for websites and mobile applications. The data source could be the same, and in most cases, a single instance Elastic Compute Service (ECS) resource will handle both requests at the same time for data delivery. Both of these requests will be handled, and the content will be delivered based on the object call request. This ensures the elastic scaling of resources for a more cost-efficient solution.
We have already covered the tools and resources that are available for the Object Storage Service (OSS) to leverage. This further enables the OSS to store and provide mass data storage options for images, audio, and video files.
The Alibaba Cloud OSS also supports storage and delivery for other file types. The user can access (read and write) the OSS using any terminal, SDKs, APIs, web and mobile applications, without middleware. The OSS console packs a lot of access functionality, and the user applications can directly read and write data to the OSS. This ensures a secure, flexible, and cost-effective solution for handling mass storage.
The Object Storage Service (OSS) supports multiple storage types. As a user, you can store all types of data. A well-defined storage algorithm stores data depending on the type of data/object that is being stored. This enables a high-performing storage solution to work with efficiency while delivering data.
Let’s think of a scenario where the storage function is an archive type. This data may not be accessed for a while. In this case, the data will be stored with functionality that stores it with longer and more widespread access cycles.
Similarly, the data objects that require frequent access are stored for quicker delivery and access cycles. This ensures an in-sync data storage and retrieval solution based on the demand and data type. The diagram below shows how this works:
The Object Storage Service (OSS) works with redundancies and synchronizes data to any region in real-time if a disaster is detected. Multiple redundancies are in place for quickly and efficiently dealing with disasters and to ensure the maximum availability of data with the OSS.
The Elastic Compute Service (ECS) and the Content Delivery Network (CDN), work in-sync with keeping the OSS instances updated and ready to be replaced if a disaster occurs. As one server resource goes offline, the standby resource is automatically deployed to continued availability. The OSS secures important data to maximize service stability and business operation continuity.
The Object Storage Service (OSS) is a revolutionary product that combines storage with smart solutions to alter the way industry defines storage. The OSS has redefined the way storage works by selecting storage types depending on the variable access times for specified data. The combination of MTS, ECS, Function Compute, Max Compute, and CDN among other products by Alibaba Cloud has made the Object Storage Service a boon to the storage industry.
The redundancies in place account for any disaster recovery solutions. The Database Link and Transfer Acceleration alongside side MTS for media processing makes this the best-in-class service.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/@storjproject/storj-master-plan-45bfb63c6b38?source=search_post---------351,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Oct 6, 2016·6 min read
The plan that will take Storj and Storj Labs Inc. from zero users to a next generation storage platform for the web consists of four stages:
Let´s take a closer look at how Storj is planning to accomplish each one of these goals.
When you download the latest cat picture sharing app, most likely you won’t know or care about where the application data is actually being stored. Developers make the decision on data storage for the many applications their users rely on. Traditionally, developers have predominantly used centralized cloud storage options on platforms such as Amazon AWS, Google Cloud, and Microsoft Azure.
We are building Storj to be a better data storage option for developers’ applications than traditional centralized cloud storage solutions. Some of the main benefits are:
These are just some points that make Storj a better option than the traditional cloud.
The first step is to make Storj easy to integrate and use for developer applications. This makes developers’ lives easier, and allows them to pass along the cost savings, security, privacy, and performance features to their users.
If you store your pictures on something like Dropbox, can you then download them again from your iCloud photos app? Well the answer is no, because Dropbox controls that information. We want to put the control back in the user’s hands. Imagine a world where you can store your data in one of your applications, and retrieve that same data from any of your other applications at your discretion. That’s the power of a unified storage layer with the users in control. Even with information sharing between applications, the process is very secure because the data is encrypted with keys only the user controls. Plus, only the user can approve information sharing between applications.
Now a good question would be: “Why would applications give up their current control of user data?” While this practice would keep users inside their application and nowhere else, the answer is pretty simple: user base. Consider an App 1 with features A and B, App 2 with features C and D, and App 3 with features A and E. They each have a user base of 5,000 users. App 1 and App 2 implement Storj as their backend and use its standards for storage, user accounts, and authentication. Now all their combined 10,000 users have access to features A, B, C and D in both applications with minimal effort. App 3 just can’t compete with the combined user base and features of Apps 1 and 2.
If this sounds familiar, that’s because it already exists in some closed personal cloud implementations like iCloud and Google Drive. Unfortunately, Apple and Google control access to this data, when it should be the users themselves instead.
Renters pay farmers in SJCX to store their data in the network. Farmers will use SJCX to cover their costs, and it will also be used in various ways to support/organize the network and prevent spam. We envision that as the network progresses, applications will support direct usage of SJCX, making it easier for farmers to spend their SJCX. This creates a self-sustaining ecosystem where SJCX is recycled back into the network instead of being sold.
Since the only source of farmer’s income is through renters, we also envision farmers working closer with renters to increase the amount of data and data usage in the network. For example, if a renter uploads a series of very popular files, some farmers might even pay that renter to ensure they get an early copy of the data, as they will earn much more money from the many users that will request to download that data soon after. Another example could be farmers cooperating with renters to promote data downloads to third parties, as getting more views for the renters’ content results in more income for the farmers. This would be a special use case and not the rule, but because Storj is an incentivized open market, farmers and renters are free to work together in unique ways. This collaboration can open up some novel methods of content distribution.
Unfortunately, humans are usually the weakness of any system. People make mistakes, so we must replace them with public, auditable, and transparent systems that function logically. With the advent of Bitcoin and blockchain technology we finally have a great example of how to build these systems. While public, auditable, and transparent systems are great, they rarely scale easily. For example, Bitcoin can only handle five to seven transactions per second, while the centralized VISA system can handle an average of 2,000 transactions per second. However, there are some ideas on how this could be solved, but there is still lots of work left to do.
We want to automate as much of the Storj network as we can and rely on trustless technologies as much as possible. This will take time, and while it may be impossible to make any system fully trustless, we will try to get it as close as we can. The litmus test is that the entire Storj team should be able to take a trip to the Moon (where internet connectivity is lacking), and the network should still continue to function and thrive. This is actually already implemented now, as it is still possible to retrieve data from the underlying P2P network even with the Storj Labs Inc. bridge services offline, as long as the application has the extra logic to handle that case.
Other than just automating to ensure better privacy, security, and robustness, we need to accommodate future use cases. Right now we assume that you are a human signing up for the service. Going forward, we might need to be more open minded. For example, in the future your smart fridge is not going to be able to swipe a credit card to pay its data needs. It should be able to create an account on Storj, and use that storage space without human interaction. As long as the smart fridge pays like the rest of us (though cryptocurrency), everything should be fine. This automation will enable Storj to be used for Distributed Autonomous Organizations and IoT.
We should always remember that Storj was named after the futuristic StorJ concept by Bitcoin Core developer Gregory Maxwell. He described a vision of a cloud storage system that was run by many trustless autonomous AIs. This sparked the vision and name that is Storj today. We want that future now, but we understand that we will get there one building block at a time.
So let´s recap how Storj will be building the next generation storage platform for the web:
Shawn Wilkinson
CEO/CTO of Storj Labs Inc.
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
200 
4
200 
200 
4
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
"
https://blog.upthere.com/a-smarter-way-to-store-everything-you-want-f39bdfcae933?source=search_post---------395,
https://medium.com/the-longaccess-company/designing-a-cloud-storage-service-pen-or-pencil-272d91e74645?source=search_post---------63,"There are currently no responses for this story.
Be the first to respond.
I’ve always been fascinated by limitations in product design.
Not design limitations, these are set by someone else, even if this is the laws of physics or humans. But limitations by design: the ones you decide to put in place, even if you don’t have to.
Take pens and pencils. Pens have a design limitation we are all familiar with: What’s written in ink is nearly impossible to erase. Pencils don’t have this limitation.
On the other hand, the decision to sign contracts using a pen and not a pencil, is a limitation by design in the contract signing procedure: the involved parties could obviously use pencils, but they don’t like the erase feature.
From an entrepreneur’s point of view, design limitations are usually considered an opportunity: Overcome the limitation, and your product will offer a new feature that no one else could offer until now.
Ink can’t be erased? You invent the erasable ink, or the correction fluid.
Limitations by design are a totally different beast. They are design choices that seem to take some functionality or freedom away from the user, but they are actually there to provide more freedom or functionality in other ways.
A good example in the tech industry is Path, a social network, where you can’t have more than 150 “friends”. They don’t do it to limit their users, to give them less freedom. To the contrary, their intention is to increase “the comfort to share personal moments”. The (artificial) maximum number of 150 friends, is not there to limit but to give more freedom.
In the real world, a design limitation often turns out to have side effects so interesting and valuable, that is later treated as a limitation by design.
I don’t know the facts for sure, but I’m pretty sure Twitter’s 140 character limit was actually a design limitation: The initial version of the service relied heavily on (SMS) text messages and text messages can’t be longer than 140 (8-bit) characters. It turned out that not being able to write long texts lead to a new “format”, what we call “microblogging”, that appealed to many users and developers: In retrospect, one could argue that even if Twitter didn’t have the design limitation introduced by text messages, they should have invented it, as a limitation by design.
Which leads us to one of my favourite mind games: Pick a service or a product and try to think what would happen if you introduced an extra limitation —by design.
What if there was a photo sharing service that allowed you to upload only one photo per day —or only very small pictures or only very large pictures? What if there was a commenting system that made comments public after a very long period, like a day or more after they were posted? An email service where all emails are deleted 3 days after they are sent —or one that will send all emails delayed by one or more years? A word processor that only gives you a couple formatting options and nothing more? An e-shop with only one product per day, or one with only ten items per day?
BTW, most if not all of the above already exist. Some of them are successful, others not —but that’s beside the point.
It’s an interesting exercise: once I set an artificial limitation, I start to explore how this would affect the product. Would it become more appealing to some users? Is there a group of users that actually need it? Would this limitation allow developers to do things they couldn’t do before? Would it allow the product to be offered at a significantly lower price?
Truth be told, I usually just get a useless or inferior product. But some times the result is unexpected —and even when it’s not, it’s a great starting point for brainstorming.
Take for example cloud storage.
When we use a service like Dropbox or Box, we expect to be able to add, update and delete files.
What if we were not allowed to delete or update files? It doesn’t sound very appealing, does it? Why on earth would someone use such a service?
Wait!
On second thought, this service could be useful to anyone looking for a way to store documents that shouldn’t be altered. People do it everyday: They print agreements and contracts, and sign them in an effort to make it difficult or impossible to alter them. Also archivists are constantly on the look for services that will preserve data: If they could carve them in stone, they probably would —and they are grateful to older civilisations that did.
And… this is exactly what we are building: a long term personal digital archiving service. People will be using longaccess.com to store documents, photos, videos that they want preserved for decades —in most ways, the opposite of deleting or replacing.
So, yes, this “limitation” will be one of the most important features of our product: you will not be able to delete or alter your files.
To put it in a different way, in a world where all cloud storage services resemble pencils, we decided to create one that writes in ink.
[*] Photo by Theilr
Panayotis Vryonis is the Founder and CEO of longaccess, a secure, long-term, personal digital archiving service.
Our mission is to safeguard and protect personal digital…
22 
22 claps
22 
Our mission is to safeguard and protect personal digital archives for the future.
Written by
Dad, geek, entrepreneur. My home is www.vrypan.net, my blog is http://blog.vrypan.net/
Our mission is to safeguard and protect personal digital archives for the future.
"
https://medium.com/@modern-cdo/spicing-up-data-lifecycle-management-with-hybrid-cloud-storage-recipes-170f49f4808f?source=search_post---------132,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sandeep Uttamchandani
Aug 4, 2015·4 min read
This article is a part of the blog series on Changing Data Platform Landscape
As enterprises grapple with exponential data growth, they are looking for new models to efficiently store, analyze, and archive data. Software-defined storage (SDS), as well as the hybrid cloud model, is shifting wasteful “just-in-case” provisioning into seamless “just-in-time” provisioning and storage management for optimal total cost of ownership (TCO). The hybrid cloud model allows enterprises to leverage both their existing data centers’ deployments as well as the cloud in a seamless fashion, leveraging well-established workflows and management tools. The focus of this blog post is to discuss emerging intersection points between traditional data lifecycle management and hybrid cloud storage.
Let’s level-set on the lowest common denominator for this discussion — storage hardware trends, and their implications on data management. A decade ago, storage tiers were mainly rotating disks with different speeds, and sequential media such as tapes. The industry’s research focus was mainly on closing the latency gap between main memory with nanoseconds service times and disks operating at milliseconds. Today, storage tiers have spread across the entire latency spectrum: Non Volatile Memory (NVM) with expected service times of hundreds of nanoseconds, to NAND Flash operating at an order of microseconds, to innovations in high density disk technologies such as Shingled Disks with higher access times. An interesting observation is the emergence of distinct $/IOP and $/GB tiers — flash-based storage excels with regards to $/IOP, while disk-based technologies are optimal with regards to $/GB. So, depending on the IOPS/GB of the data, and the application’s latency constraints, the data needs to be placed on the appropriate tier to best utilize the storage resources.
Data lifecycle management deals with data placement and management from the initial creation of data to its eventual deletion. The key point is that the quality of service (QoS) requirements for data are time-varying i.e., at the time of initial creation, the latency, throughput, durability and recovery point objective (RPO)/recovery time objective (RTO) availability, may be different from data that has aged and is being retained mainly for batch processing and archiving/compliance. By combining data lifecycle management with the differentiated properties and pricing of storage tiers across on-premises and cloud resources, enterprises can exploit storage hardware innovations to derive the lowest total cost of ownership (TCO) specific for their deployment and usage models.
To consume cloud resources, enterprises are increasingly adopting the hybrid cloud model that allows consuming cloud resources as a natural extension of their on-premises data-center deployment. Specifically in the context of storage resources, the cloud offers a variety of protocols (block, object, etc.), and tiers with varying $/GB/month pricing. Also, it offers a growing number of scale-out services for structured and semi-structured solutions, blurring the lines between traditional storage and data management solutions. It is important to realize that cloud storage is more than just storage resources — it has virtually unlimited compute resources that can be collocated with storage in the cloud. Also, given the economies of scale and competitive cloud wars, enterprise customers benefit from ever-decreasing prices of cloud resources, also referred to as “the race to zero.”
Customers commonly think of hybrid cloud storage as a tier of storage that is used mainly for infrequently accessed data copies or archive data. While those have been the founding use cases, there are a growing number of intersection points between data lifecycle management and hybrid cloud storage means. The following list of recipes is not meant to be comprehensive, but more to get you thinking of the possibilities!
In summary, disruptions in storage technologies are redefining not just the emerging storage architectures, but also influencing traditional workflows/IT processes within the data-centers. The model of a hybrid cloud comes naturally to storage use cases, motivated by exponential data growth and limited IT budgets. While hybrid cloud storage emerged mainly for data archive and availability use cases, it is increasingly playing an important role in traditional Data Lifecycle Management. Hybrid Cloud provides not just another storage tier for on-premise applications, but a combination of on-demand compute capabilities, with differentiated tiers for cost, performance, durability, and a globally accessible (object) storage namespace.
Originally published on August 4, 2015.
Democratize Data+AI — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author. AIForEveryone.org
1 
1 
1 
Democratize Data+AI — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author. AIForEveryone.org
"
https://blog.1inch.io/defi-in-crypto-what-is-it-and-why-does-it-matter-now-4dadfc575832?source=search_post---------248,"What are your thoughts?
Also publish to my profile
There are currently no responses for this story.
Be the first to respond.
If you think about it, there are similarities between decentralized finance (DeFi) and the arrival of cloud storage in 2006. Back then, the big idea was the creation of a simple, scalable service that could easily plug into existing software.
To some extent, the DeFi segment mirrors this idea, making hefty waves of innovations the likes of which have never been seen before.
DeFi was one of the major drivers behind the crypto revival in 2020. Still, it remains one of the segment’s underdeveloped areas.
As DeFi products pick up steam and billionaire entrepreneur and investor Mark Cuban names DeFi as an industry with a chance to explode, the monthly volume of decentralized exchanges soared from just $39.5 mln in January 2019 up to $173 bln in May 2021.
DeFi: what is it?
But what is DeFi, and how does it work? Why has it grown so much lately, making a major impact on the crypto segment in general?
It is crucial to note that the implementation of new solutions in financial projects is not new. They facilitate most transfers of value. However, third-party interference is still needed to navigate legislation in different countries, operate within the economy’s main sectors and follow various models making transfers possible.
DeFi uses top-notch solutions to remove centralized models and provide financial services anywhere and to anyone. DeFi services and apps are mostly based on public blockchains. They replicate existing offerings built on the rails of common technology standards, offering innovative alternatives to existing financial services.
At the same time, DeFi apps provide consumers with more control over their funds through personal wallets and trading websites that work with customers instead of institutions.
While DeFi is based on the blockchain, its applications in the financial world are quite broad. DeFi projects can cover a range of areas, including:
Advantages of DeFi
Here are some key advantages that decentralized finance can offer.
Why is it so popular?
Coin issuers and traders are flocking to DeFi projects because they are preferred platforms for crypto strategies, such as yield farming, where some traders are collecting high returns by pooling and lending cryptocurrencies in exchange for interest and fees, while also often being awarded additional coins as incentives.
Paul Veradittakit, a partner at Menlo Park, California-based Pantera Capital Management LP, said: “We can really see decentralized exchanges make a huge dent in the market and potentially overtake centralized exchanges.”
Unlike most traditional crypto exchanges, a DeFi project does not charge issuers to list new tokens and generates revenue through transaction fees. Similarly, it does not check on customers and their identities, which most traditional crypto exchanges started to do because of regulatory pressure to detect and combat money laundering and other illicit activities.
DeFi functions as an open financial network that is multifunctional, transparent and decentralized.
DeFi’s performance in 2021 and its openness put the entire blockchain field on the radar screens of traditional investors. According to DeFi Pulse, the total USD value locked in DeFi surpassed $80 bln. This corresponds to more than 500% growth between January and August 2021.
So, such projects and individuals who are willing to make money have boosted the whole sphere.
Like during the initial coin offering boom that helped fuel the bitcoin-led crypto segment four years ago, this led to the creation of hundreds of new apps and accompanying tokens in recent months.
Is DeFi secure?
No, it is risky. Many believe that DeFi is the future of finance and, therefore, investing in this segment could lead to massive gains. But it is hard for newbies to pick promising apps from a wide range of those constantly popping up on the market.
Since DeFi became all the rage last year, there have been several noticeable fiascos. For instance, the meme coin YAM slumped significantly and burned, sending its market capitalization from $60 mln to zero in less than an hour. Other DeFi apps, including Hotdog and Pizza, faced the same issues, and many customers lost substantial funds as a result.
On the tech side, smart contracts are robust, but they can not be changed once the rules are integrated into the protocol, which could also lead to risks.
When will DeFi go mainstream?
While more and more people are involved in DeFi applications, it is hard to say when they will become part of most people’s lives. Much of that depends on who finds them useful and why. Many believe that various DeFi projects have the potential to become the next Robinhood, drawing in hordes of new users by making financial applications more inclusive and open to those who do not traditionally have access to such platforms.
But tech behind DeFi apps is new, experimental and not without problems, especially with regard to security or scalability.
Developers hope to eventually rectify these problems. Ethereum 2.0 could tackle scalability concerns through a concept known as sharding, a way of splitting the underlying database into smaller pieces.
The future of the industry
Despite all the advantages of DeFi, a killer app has not yet arrived. In part, it could be due to resistance from the traditional financial system, as financial authorities and companies are not ready for integrating new tech. They see DeFi as a threat, not an opportunity.
But this could change soon, and the future of decentralized finance looks quite prosperous.
Unfortunately, the DeFi boom has also attracted a bevvy of scammers and swindlers. CipherTrace, a cryptocurrency intelligence company that delivers cryptocurrency AML compliance solutions, says that by July 2021 total losses from illicit activities within the DeFi industry reached more than $361 million, already making up three-quarters of the total hack volume this year — a 2.7x increase from 2020. DeFi-related fraud continues to rise, as well. At the time of the CipherTrace’s August 2021 report, DeFi-related hacks accounted for 54% of major crypto fraud volume, whereas last year DeFi-related fraud only made up 3% of the year’s total.
But it is fair to say that the DeFi industry faces the same problems as the crypto space as a whole. It is still in its early stages, with a huge potential for further evolution. And DeFi is probably the only sub-sector within crypto that has provable traction and a clear value proposition.
That is all for now. Stay tuned for more helpful content coming soon!
A distributed network for decentralized protocols on multiple blockchains.
249 
249 claps
249 
Written by
A distributed network for decentralized protocols enabling the most lucrative, fastest and protected operations in DeFi.
The 1inch Network unites decentralized protocols whose synergy enables the most lucrative, fastest and protected operations in the DeFi space.
Written by
A distributed network for decentralized protocols enabling the most lucrative, fastest and protected operations in DeFi.
The 1inch Network unites decentralized protocols whose synergy enables the most lucrative, fastest and protected operations in the DeFi space.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/why-and-when-to-avoid-s3-as-a-data-platform-for-data-lakes-c802947664e4?source=search_post---------330,"Sign in
There are currently no responses for this story.
Be the first to respond.
Farhan Siddiqui
Mar 25, 2020·8 min read
Data lakes are all the rage these days in large enterprises. A data lake is a single store for both raw copies of source system data and transformed data for use in tasks like reporting, visualization, advanced analytics, and machine learning.
Object stores (like S3) are becoming the platform of choice for data lakes because of two main reasons:
"
https://medium.com/google-developers/building-a-simple-web-upload-interface-with-google-cloud-run-and-cloud-storage-eba0a97edc7b?source=search_post---------10,"There are currently no responses for this story.
Be the first to respond.
Posted by Matt Cowger, Global Head of Startup Architects
In my time since joining Google, more than one startup has asked about the possibility of receiving files into Google Cloud Storage (GCS) so that they can be the start of a Pub/Sub based pipeline for all sorts of use cases — image recognition, data ingestion, etc.
To me, this seems like a natural fit for the product. However — there’s no available ‘shim’ layer for doing these — there’s no way to treat GCS like a FTP server, or a direct (simple) web upload target. There are work arounds with gcsfuse or gsutil, but those require end users to install and utilize command line products that are Google specific, and even then have the challenge of not being usable without direct Google credentials in the project.
However, even a naive implementation of such a system would be suboptimal (I suppose thats what its called naive). By placing a single FTP or web upload shim (likely hosted out of a single GCP zone), we negate much of the power and performance of Google’s global network and GCS’s distributed nature, as well as potentially limit performance to the network interface upon which our shim runs. Optimally, we’d want something with the following characteristics:
Using these ideas as my guide, I developed a prototype to do exactly that, using only 2 Google products (Cloud Run and Google Cloud Storage).
Ultimately the solution to the problem was to use the fact the GCS supports signed URLs — URLs that have all relevant (time limited) authentication information built into them. We can use those as a way to avoid the need to deliver standard GCP credentials. By using signed URLs we also avoid issues #1, #2 and #4 — all of the bulk upload traffic from the user to GCS is carried directly to the nearest Google Cloud edge point over HTTP, and not through a single system somewhere.
The two biggest challenges are to:
Generating signed URLs within GCP is a fairly simple process, and supported by a wide number of languages and their associated GCP SDKs. Because Python is superior to all other languages in every way, I chose to use it as my base, but it could be done in nearly any language you prefer.
After a few setup procedures detailed in the GitHub repo for this project, I centered on a key function to generate the URLs:
It’s very simple, and very short — consider this just a backend API. After parsing some incoming parameters (note: there are gaping security holes here — this function uses a client generated value with absolutely no sanity checking), we ask the API to sign a URL for this path in the bucket that’s good for 60 minutes.
It’s worth remembering that the Python itself is just an API — it does not really handle any of client client side code. However, for ease of testing and deployment, I also have the static part of the site (HTML, CSS and JavaScript) served from the same container, but that could be replaced with GCS website serving as a later optimization.
Running this small Python script is its own interesting thought process: Do we run this on a Compute Engine instance, as a AppEngine instance, as a Cloud Function, in Cloud Run or in Kubernetes (GKE)? All of these would work, but for my case, I wanted as little management as possible, making Cloud Functions and Cloud Run the top contenders. Both support scale-to-zero and per-millisecond billing, meeting requirement #5. For me — I’m most comfortable testing, deploying and managing containers, so I went with Cloud Run.
The last important component is the client side work, where the user enters the file they wish to upload with a standard form, then the request for the signed URL is made:
And then lastly the form itself is submitted on button click by the user. This is special because the target of that form is the GCS signed URL directly rather than the Python service, meaning we are only limited by client bandwidth for the upload and maximize the performance benefit of Google’s network.
Once the code has been pushed to Cloud Run (gcloud run deploy), its ready to go (note, I’m skipping the process of building a container — that process is left up to the reader, but the Dockerfile is in the repo)!
You can find the full repo on my GitHub: https://github.com/mcowger/gcs-file-uploader
Engineering and technology articles for developers, written…
342 
2
342 claps
342 
2
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
News about and from Google developers.
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@zeepin/galacloud-distributive-encrypted-data-storage-network-by-zeepin-16caac88be22?source=search_post---------224,"Sign in
There are currently no responses for this story.
Be the first to respond.
Zeepin
Aug 1, 2018·5 min read
The importance of data storage has become increasingly prominent with the development of the Internet and digitalization wave. Meanwhile, in recent years, data security has been an issue invariably challenging lots of major user data and cloud storage suppliers, which has given rise to loss of data owned by individuals and corporations. For example, the massive user data leakage of Facebook in March this year had a serious negative impact.
At present, there are several pain points with the centralized cloud storage service: First, security threats. Since the vast majority of cloud storage suppliers are centralized, their vulnerabilities to safety threats and attacks have eventually led to data theft and loss. In addition, client encryption standards and centralized management private keys still cannot guarantee data confidentiality and security.
Second is the data monopoly and vicious competition of large companies. Some companies use data analysis and sell the information to third parties for the purpose of profiting over user privacy. Data transactions lack transparency and users cannot truly gain complete data ownership and control.
Third, users are faced with risks of service provider shutdown and operational suspension. This often happened in the past few years, causing hundreds of millions of users without option but to transfer data or faced data loss.
Faced with the growing needs of cloud data storage and the widespread concerns over safety and privacy, the centralized data storage seems powerless to remove these obstacles. Here comes distributed storage, providing more reliable and effective solution.
In a distributed storage network, users from all over the world rent out their hard disk space to form a decentralized network, in which each disk represents a node of the network. Files that clients upload will be encrypted, split into multiple shards, and stored in a network composed of a sheer number of distributed nodes. Compared with the traditional centralized data storage service, distributive cloud storage has features in high autonomy, anonymity, tamper-proof, low-cost, and fast speed. Users will have strict data security and privacy protection.
GalaCloud is the distributed encrypted cloud storage system that ZEEPIN team has dedicated to build, which has noticeable advantages over the traditional solutions. Not only will it effectively safeguard data safety, but also has innovative meanings in its economic model as well as its connection with dApps.
The GalaCloud project is supported by ZEEPIN Foudation. It is a key component in ZEEPIN’s ecosystem. Providing distributed storage space and service for the dApps, it is an indispensable part of ZEEPIN public chain infrastructure. Also, GALA will function as a shared proof of staking for GalaCloud, which will help Gala coin to reach its greatest extension of application.
GalaCloud DESNet refers to the distributed storage system provided by GalaCloud, which is composed of a basketful of GalaHub nodes and GalaBox nodes. In the entire distributed storage network, GalaHub will perform to dispatch the sharding of files, searching for address in a way to coordinate all GalaBox(es). Storing the encrypted shards of files in client sides, GalaBox focuses on providing service for data shard storage, receiving requests for uploading and downloading and storing received encrypted files in its disk in a distributed way. GalaBox can be obtained through the crowdfunding issued by ZeeFund platform. Users can insert hard disk to join in distributed storage network for mining.
Here are the main advantages of GalaCloud:
As it is distributed, it becomes a place free of centralized nodes where any third-party centralized system or organization will be unnecessary. It’s your call to decide when and where to upload or download files like texts, pictures, sound tracks or videos.
GalaCloud is decentralized and allows all files, being sharded and encrypted, to be stored in a distributed network. What’s more, dynamic copies will be simultaneously generated. You are the only owner of your data unless you share your private key with others.
GalaCloud, on a basis of smart contract of Zeepin public chain, has adopted a set of standard management protocol to manage. As nodes will automatically operate in the network according to the standard protocol, trust based on centralized system is replaced by trust built on protocols. This is how an entirely autonomous network comes into being.
All information, sharded and encrypted, will be stored in distributed network and travel to each distributed node of GalaBox. Shards generate Hash that will be automatically maintained by nodes. In case of sharding redundancy failing to make it to security threshold, new nodes will be generated and distributed to new data nodes. Users will keep their private keys to themselves. Data loss will never happen unless users delete data by themselves.
GalaCloud is an innovation of ZEEPIN, well integrating data storage service and the sharing mode, which is mainly reflected in from mining revenue function. Users need to pay Gala for storage space according to the management protocol of GalaCloud. Gala earned from this will be distributed to each single node that provides storage service and space, i.e. GalaBox and GalaHub. That is to say, through renting out the hard disk space resources, global users get GALA coins, the cryptocurrecy reward; Accordingly, users can use GALA to upgrade the data storage service. This economic incentive model will greatly facilitate the circulating value and the diverse application scenarios of GALA.
Another innovative feature of GalaCloud lies in its syncing with dApps in the ecosystem. In an era of a massive flow of blockchain-based applications, users are more demanding when it comes to data privacy and security. GalaCloud will be applying to ZEEPIN public chain as a blockchain-based infrastructure that can provide distributed storage and enable synchronization of data storage of mutiple dApps. For instance, source code storage of ZeeRights will be seamlessly connected to GalaCloud, in a move to provide distributed storage service that is secured and encrypted for the creatives.
The GalaCloud whitepaper will be released tomorrow. In the coming weeks, ZEEPIN community team will also launch GalaHub application, GalaBox G1 crowdfunding and other exciting campaigns, in efforts to create a reliable and win-win distributed data cloud storage ecosystem.
ZEEPIN Team
Join the GalaBox crowdfunding and GalaHub selection: https://galacloud.io
How to apply for GalaHub node: https://medium.com/@zeepin/how-to-become-a-galahub-node-in-galacloud-network-2fee427e49b6
For more detailed information about GalaCloud please refer to GalaCloud White paper: https://www.zeepin.io/about
Zeepin:
Website: https://www.zeepin.io/
Telegram: https://t.me/zeepin
News Channel: https://t.me/ZeepinNews
Twitter: https://twitter.com/ZeepinChain
Facebook: https://www.facebook.com/ZeepinChain/
Youtube: https://www.youtube.com/c/Zeepin
SubReddit: https://www.reddit.com/r/ZEEPIN/
Instagram: https://www.instagram.com/zeepinchain/
Linkedin: https://www.linkedin.com/company/zeepin-foundation/
Discord: https://discord.gg/YcPhNXC
Galaxy:
Website: https://cryptogalaxy.one/
Telegram: https://t.me/CryptoGalaxyOne
Twitter: https://twitter.com/TheHubGalaxy
Facebook: https://www.facebook.com/CryptoGalaxyOne/
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
496 
496 
496 
A decentralized network for creators & creative assets. |website: https://www.zeepin.io |
"
https://medium.com/@getliner/love-how-medium-focuses-on-the-network-while-at-the-same-time-being-one-of-the-greatest-8efc577fef34?source=search_post---------287,"Sign in
There are currently no responses for this story.
Be the first to respond.
LINER
May 23, 2016·1 min read
Ev Williams
Love how Medium focuses on the “network”, while at the same time being one of the greatest publishing tools out there. In particular, Medium’s follow feature, text-shots, and vibrant highlighting culture are killer :0
Favorite quote has to be, “We compete on user experience”. Why did cloud-storage finally take off with DropBox? There were cloud-storage services before, but DropBox simply transcended their competitors with the “folder” based cloud storage.
26 Highlights / 14 Comments via Linerhttp://liner.link/ay4RV?medium-is-not-a-publishing-tool
Highlight while you read (http://getliner.com). Highlight to save sentences on the web. Great with @Pocket and @Evernote
2 
2 
2 
Highlight while you read (http://getliner.com). Highlight to save sentences on the web. Great with @Pocket and @Evernote
"
https://medium.com/@polkadotters/interview-with-youan-of-crust-network-f5a3e8fb6fb2?source=search_post---------281,"Sign in
There are currently no responses for this story.
Be the first to respond.
Polkadotters | Kusama & Polkadot validators
May 6, 2021·8 min read
Crust Network is providing decentralized cloud storage for the whole Polkadot ecosystem. Utilizing blockchain technology, your data can be now stored securely and in a decentralized fashion. For today's interview, we talked with Youan who is a brand manager at Crust Network.
What are the characteristics of decentralized storage? Is decentralized storage really necessary for Web 3.0?
Distributed, privacy-focused, efficient, non-monopoly.
We are in a digital age. If the off-chain data of a blockchain application is stored in a centralized service provider, then it is not a completely decentralized application.
There are two different applications of Crust Network — Crust Cloud and dApp file storage. Can you tell us more about the Crust Cloud, is it comparable to Google Drive or Dropbox? What feature list do you provide?
There are much more than two applications that can be built on top of Crust Network. And we have summarized four typical application categories. WebSite/dApp hosting, general off-chain/NFT file storage, accelerated P2P content delivery network, and cloud storage.
We spent most of our development resources on the protocol design and implementation; we develop applications mostly for capabilities demonstration and market education purposes. Crust Cloud and Github Crust/IPFS Pin Action are two examples. In addition, we have released the Crust Grants Program to encourage and incentivize developers to build more useful applications on top of Crust. Here is a sample list of those applications. You can go to our granted list to find more projects, most of those applications are now being built by the development community.
In general, if you build Cloud Storage applications such as Crust Cloud on top of Crust, resource-wise you will benefit from the underlying network
Why do we need dApp file storage? How easy is it to integrate Crust into my existing dApp?
Hosting DApps on Crust and IPFS will give several benefits:
It’s quite easy to integrate Crust into the existing DApp/Website deployment flow. Take Uniswap as an example (https://github.com/Uniswap/uniswap-interface/pull/1342/files). It’s actually just a few lines of code.
Do you support only EVM (Solidity) based dApps or also apps written in the Substrate native smart contract language — Ink!?
As of right now — no, we don’t support customized smart contracts, EVM, or Ink!. The most important interface of Crust is to provide storage order functions so that users can use the functions to manage their files. In the long run, we have a plan to enable our users to be able to create contracts to manage their own data as digital assets.
Do you have any integrations with other Polkadot ecosystem projects? Many smart contract platforms (Moonbeam, Edgeware, Plasm, or Phala) and other projects (like Subsocial) would certainly utilize your decentralized storage service.
Sure, we have reached storage cooperation intentions with many projects inside and outside the Polkadot ecosystem, including Moonbeam, Litentry, Reef, Plasm, Darwinia, Apron, etc. After the Polkadot parachains launch, they will use Crust for off-chain storage. You can find all the cooperative announcement articles on our Medium page.
You decided to use two algorithms to run the network — GPoS (Guaranteed Proof of Stake) consensus model and also MPoW (Meaningful Proof of Work). Can you explain your motivation for using them?
Compared with traditional PoS, GPoS has added the role of guarantor. We have continued Polkadot’s DPoS economic model. Token holders can obtain income by helping the node guarantee. We hope that token holders, nodes, and users can be closely combined to make tokens a virtuous circle.
As for MPoW, compared to the traditional PoW, all the work performed by the Crust node is meaningful. We use TEE hardware to prove the workload of the node. Therefore, the node does not need to perform meaningless calculations.
How do the workload reports work?
We use a technology named TEE. Similar to the safe area on the iPhone, TEE is a set of software and hardware functions that together provide a safe and isolated execution environment for applications. All Crust nodes need to have a CPU that supports the TEE function. TEE will supervise and check the workload of the nodes and report the workload to the chain.
I noticed that quite a few projects in the Polkadot space are leveraging TEE technology (most notably you and Phala Network). What’s so appealing about the TEE and why it makes such a good fit for a blockchain? Why didn’t we see TEE-based applications before Polkadot came into space?
TEE is a set of software and hardware functions that together provide a secure, isolated execution environment for applications. You can think of TEE as a black box. The encrypted data and applications enter the black box to process the data, and only the encrypted result is retained. TEE can help ensure that data is not copied, stolen, or misused so that individuals can use their data without giving up control.
TEE technology has been used by many mainstream manufacturers and is a relatively mature technology. It fits very well with the concept of blockchain decentralization and democracy. As far as I know, Phala Network and Oasis Network are also using this technology.
How would you compare yourself to other popular decentralized storage solutions like Filecoin or Siacoin? Do you have any advantage over them?
Crust and Filecoin are also node incentive layers based on the IPFS network. This is different from most other storage applications.
The two biggest advantages of Crust are the GPoS and MPoW. The design of GPoS just mentioned does not bound the nodes deeply, which is more conducive to the distribution of our nodes around the world and promotes a virtuous circle of tokens. MPoW uses TEE technology so that nodes do not need to do redundant calculations. Most importantly, this will greatly shorten the file packaging time, which is very important for an application.
In addition, Crust is in the Polkadot ecosystem. We chose it because of its huge heterogeneous sharding nature. The interaction of information between projects will allow Crust to derive more imaginative functions.
How can someone become a part of Crust Network and provide its storage to it? What incentive do you have for people that want to add their storage to the network?
Basically, as long as you have a computer with a TEE function and a storage hard disk, you can be a Crust node. This requirement is very simple.
You can find the relevant mining guide on the WIKI of our official website.
As for how to attract, the current generous mining rewards are the best attraction, and we don’t even need to take the initiative to attract miners. But the current problem is that our nodes are not distributed enough, most of them are concentrated in China.
How much data are you actually able to store at the moment? Can you scale up to exabytes? 🙂
You can find relevant information on Storage Explorer on our official website. At present, the total storage capacity of the entire network is 650PB, which is when we have not vigorously promoted mining profit. Of course, we hope that the storage capacity will be larger and larger, but at the same time, we will develop the application side in a balanced way to promote the real use of Crust storage in the market.
Why did you choose to build on the Polkadot/Substrate? Did you like the development experience?
Our team considered Ethereum at the beginning of the project, but its problems included high transaction fees, low TPS, and so on. These are very unfriendly to an application chain that requires frequent transactions. We have also tried the Tendermint of Cosmos, but its degree of customization of the chain cannot meet our needs.
The Substrate framework can meet our needs very well, and its on-chain governance and fork-free upgrades can also help the application to develop in a long-term and stable manner.
Are you heading to obtain a Kusama parachain slot in the upcoming auctions? Have you already finished integration with Rococo and made your technology ready?
Sure, we are the first wave parachain on Rococo(ParaId: 7777), and we always update our code as soon as Rococo or Cumulus are updated.
Are you going to create a crowdloan so your supporters can help you with winning the slot? How are you going to incentivize users to support Crust Network?
Of course, we will. At present, we have announced the reward rules for the Kusama slot auction. You can find this article on our medium called Kusama Parachain Auction Plan.
During the crowdloan campaign period, we will give supporters a reward of 1 KSM:0.07 CRU per week, for a total of 10,000 CRU. If the auction is successful, a total of 20,000–30000 CRUs will be given to all supporters. All rewards will be issued as soon as the crowdloan campaign period ends.
I suppose you will also go for a parachain slot on Polkadot when it’s possible. 🙂 Will you stick with the same CRU token for both Kusama and Polkadot or are you planning to introduce a dual-token model like a lot of networks do?
Whether it is Kusama or Polkadot, we will use the native token CRU for rewards. In Crust’s economic model, we reserve 40% of the tokens developed by the community for the maintenance of slots and auctions, which is about 2 million CRUs. At the same time, our tokens have been circulated in the market and can be traded. In the Kusama slot auction rewards, we stipulate that the rewards are issued immediately. I believe our plan is very sincere and competitive.
Wish all the luck in the upcoming auctions and I hope we will see Crust Network running on Kusama soon!
If you like this article, consider supporting us by changing your present Polkadot & Kusama validator to POLKADOTTERS!
Stay tuned for further information about the Polkadot ecosystem projects here on Medium, our Twitter channel Polkadotters as well as in our Facebook Group Polkadot unofficial! If you are feeling really generous, you can send some DOTs to our donate address 🙂
16etYNuwvwbZYxy4FifqMq4KwZCMVSmTJ4XtE3WE3Cn1fQRQ
Czech bloggers & community builders. We are validators of Polkadot, Kusama, Darwinia, Crab, Bifrost, HydraDX, StaFi, Centrifuge under the name: POLKADOTTERS
21 
21 
21 
Czech bloggers & community builders. We are validators of Polkadot, Kusama, Darwinia, Crab, Bifrost, HydraDX, StaFi, Centrifuge under the name: POLKADOTTERS
"
https://medium.com/firebase-developers/using-the-new-list-api-in-cloud-storage-for-firebase-12d667fdc130?source=search_post---------15,"There are currently no responses for this story.
Be the first to respond.
Cloud Storage for Firebase recently launched support for listing files in buckets and folders for iOS, web, and Android clients. This was a hotly requested feature, because without it, developers wishing to upload many files to Cloud Storage generally had to make a database record of every object, so they could be reliably located and worked with later. While I think associating each file with a database record is still a good idea (more on that later), there are times when it’s very convenient to be able to get a list of files without having to first query a database.
Imagine you allow your users to upload audio recordings to Cloud Storage using a naming structure like this:
{uid} and {audioId} are variable placeholders; the uid is a user’s ID as assigned by Firebase Authentication, and the audioId is some random unique identifier. avatar.jpg is a special image used for the user’s profile picture.
One thing to keep in mind first is that Cloud Storage doesn’t actually have “folders” like you’re accustomed to in a computer filesystem. It just has names with paths that look like UNIX folders. So, “users” and “audio” here aren’t actually a folders. What actually exists here are just objects with paths that look like this:
You can see that “users”, “xyz”, and “audio” are not individual objects, and they can’t be created or deleted using the Cloud Storage for Firebase SDKs. They’re just path components that are used to organize other objects, making it easier to think about the contents of your storage bucket, for the purpose of organization and navigation. However, sometimes we still refer to them as “folders” (especially in the Firebase console) to make it clear that they’re being used as organizers.
OK, with that settled, let’s say you want to delete the avatar image for a user. That’s easy, as long as you know how to build the path to that image. I’m using Kotlin on Android for my client code samples in this post. Code for other platforms will be similar.
You would have probably already written similar code to create and update the avatar image in the first place. Pretty standard stuff.
Now, instead let’s say you want to delete all of a user’s audio files in organized under “audio” as shown above. You can try it like this, similar to the previous example:
But that fails with an error saying “Permission denied”. Why? Well, as I mentioned before, folders don’t really exist like fully pathed objects in Cloud Storage. But, what you can do here instead is use the new list API to list all the objects under that path. Or, to be more accurate, list the objects with that specific prefix. The word “prefix” is important here, as what we are actually going to do here is ask Cloud Storage to produce a list of all objects that begin with a common prefix string. Once we’ve listed those objects, then we can iterate and delete them individually, like this:
Note that listAll() yields a list of StorageReference objects that point to the individual objects that exist at the given path prefix. The list operation is shallow, and doesn’t include files organized in deeper path components. The objects are organizationally siblings to each other.
If there are going to be a lot of objects in the list, you might have a problem. You could run out of memory storing all those references. Or you might not want to wait for the entire list to be generated at once. Instead, if you want to page through the results, you can call list() instead of listAll(). Check the documentation for an example of that.
We’re not quite done yet! There’s a bit more to know. Anyone who has an app that directly reads and writes objects in Cloud Storage should be using Firebase security rules to protect who can perform what operations on objects stored in different paths. In the past, you probably just indicated “read” permission for download access. Before the list API was available, for the file example objects given above, the rules to allow anyone to download only a user’s avatar and audio files (and no other objects organized under the UID), might have looked like this:
Now that the list API is available, you can control list permission separately from download permission. With Firebase security rules, read permission breaks down into get and list permission. The get method allows someone to download a single object (and access its metadata), and list allows someone to list objects (but not download them). Granting read permission is the same as granting both get and list, so any wildcarded object granted read access can also be listed with its siblings. However, this might not be what you want.
Here’s a specific case where you’ll need to take advantage of this split. Let’s say you want a user to be able to control download access to some, but not all, of their individual audio files. However, you don’t want anyone to be able to list the entire contents of their files, as that would reveal too much information. To make this work, let’s say that the user needs to indicate which objects should be publicly accessible by adding a piece of metadata to the object. Security rules can use this metadata to check if anyone should have access:
First, note the very first line rules_version = ""2"";. This enables the use of the granular list permission used later. Without this line, rules will default to version 1, and you’ll get an error when publishing rules that use the list permission.
The above match on the path pattern /users/{userId}/audio/{audioId} is indicating that matched audio objects:
Also note that the rule ends with the wildcard {audioId}. This is required for the list API to work. The following rule will not enable the list API, because it won’t match the full paths of objects organized under audio:
One last thing to note is that read permission only implies list permission with Cloud Storage security rules version 2. So, with version 1, read permission only provides get access. But with version 2, read provides both get and list. This also means that when you upgrade your rules to version 2, your existing read permissions also start providing list permission. So, you should review your rules to make sure this is really what you want to allow.
To learn more about Firebase security rules for Cloud Storage, take a look at the documentation. They’re an important part of making sure the data you put in Cloud Storage is properly controlled.
Tutorials, deep-dives, and random musings from Firebase…
223 
7
223 claps
223 
7
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Tutorials, deep-dives, and random musings from Firebase developers all around the world. Views expressed are those of the authors and don’t necessarily reflect those of Firebase or its parent companies.
"
https://medium.com/@duyluan/gi%E1%BA%A3i-quy%E1%BA%BFt-b%C3%A0i-to%C3%A1n-data-pipeline-v%E1%BB%9Bi-%C4%91%C3%ADch-%C4%91%E1%BA%BFn-l%C3%A0-bigquery-8f8a7359ff76?source=search_post---------340,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nguyễn Ngọc Duy Luân
Sep 17, 2018·9 min read
Để mình kể cho các bạn nghe cách mà mình làm data warehouse ở The Coffee House nhé, tất nhiên dùng BigQuery rồi
Vào tháng 1/2018, mình mới tham gia The Coffee House (TCH) với vai trò là data engineer. Khi đó công ty chưa có data warehouse (DWH) đúng nghĩa, chỉ là một chỗ dump data từ các bảng bên production sang cho một số hệ thống khác chui vào lấy ra làm báo cáo mà thôi. Bọn mình cũng dùng Holistics Software làm giải pháp dashboard và dump data. Hết, những gì thuộc về hạ tầng data của công ty chỉ có thế.
Về hạ tầng database của production, khi đó các hệ thống như ERP, mobile app (cái xịn vl của TCH ấy) sử dụng Postgres. Các database này đa phần chạy riêng, và việc sync một số data nhất định (ví dụ: user profile) được thực hiện bằng cách gọi API từ hệ thống này sang hệ thống khác.
Vấn đề lúc này là báo cáo chạy khá chậm, một số báo cáo cần chạy phức tạp thì không đủ khả năng đáp ứng, và nhiều data bị phân tán, không đuợc làm sạch hay tổ chức theo kiểu phục vụ cho báo cáo (và data science hay data mining nói chung).
Giải pháp đầu tiên là chạy data pipeline một cách có hệ thống, dễ quản lý, scalable trước cái đã. Giải phép ETL của Holistics vẫn đáp ứng được nhưng chi phí khá cao. Thế nên mình giải quyết vụ này trước.
Giải pháp mình chọn là Pentaho, vì trước đó mình đã quen với giải pháp này, chỉ đơn giản là schedule một job tới giờ chạy load incremental vậy thôi, xong.
Tiếp theo là data warehouse, cần phục vụ cho nhu cầu query lấy số nhanh của tất cả các phòng ban trong công ty. Thật may mắn là mọi người càng lúc càng data-driven nên team data bọn mình càng có nhiều việc để làm :D Đó là tín hiệu tốt đấy chứ.
Để chọn data warehouse, ngay lập tức một số thứ pop-up trong đầu:
Redshift được AWS xây dựng dựa trên Postgres nhưng lưu trữ data theo dạng columnar. Redshift thì nổi tiếng quá rồi.
Và có một tips nhỏ dành cho các bạn mới chuyển từ công nghệ DWH truyền thống sang những dạng “Big Data Warehouse”: rất nhiều thứ cần được load vào theo dạng batchđể đạt tốc độ nhanh nhất có thể, trừ nhu cầu stream dữ liệu vào. Ở đây tạm thời chúng ta chỉ quan tâm đến batch thôi nhé, stream nói riêng sau.
Một trong những cách được AWS recommend để load data vào Redshift đó là đưa dữ liệu của bạn thành các file trên S3, sau đó Redshift sẽ hút các file này vào chứa trong bảng của nó. Bạn có thể tiếp tục transform dữ liệu bằng cách viết custom script, dùng các câu query để transform rồi INSERT vào bảng hay cách nào đó tùy ý.
Vấn đề của Redshift là giá đắt quá. Một instance Redshift bèo bèo thôi đã tốn cỡ $150 / tháng rồi. Và để query dataset lớn thì lại càng phải tăng cấu hình server lên, thậm chí chạy cluster nhiều con server với nhau mới kham nổi. Với ngân sách có giới hạn (và bản thân mình cũng không muốn chi quá nhiều tiền), mình cảm thấy Redshift không phải là cái phù hợp.
Trước đó cũng nghĩ tới giải pháp dùng AWS Athena query file trên S3, mỗi file bao gồm các record đã sinh ra hoặc thay đổi kể từ lần load trước, nhưng như vậy không update được dữ liệu và phải viết CTE hoặc view trước khi có thể query bình thường => vừa tốn công, tốn thêm phí scan data mà chậm đi nữa => bỏ qua.
Snowflakes về cơ bản giống như Redshift. Mình có cảm tưởng Snowflakes chính là Redshift nhưng được customize một chút ở tầng trên để dễ dùng hơn. Nói chung là giá cũng không rẻ, và phải chọn instance để chạy nên mình cũng không cảm thấy scale dễ dàng ở quy mô của TCH hiện tại.
Khi dùng BigQuery, mình thích vì nó chẳng bắt mình suy nghĩ về server. Hình thức tính phí là tính theo lượng data được scan bởi query engine Dremel (là cái do Google làm ra cho BigQuery). Ổn, mình thích hình thức này. Chỉ $5 / 1TB scan mà thôi, và mình ước tính chi phí để query cũng sẽ không quá nhiều.
BigQuery, giống Redshift, cũng hỗ trợ “hút” data từ Google Cloud Storage lên (cụ thể là query trên file). Nên mình sẽ dùng Cloud Storage làm nơi staging chứa tạm data trước khi chuyển vào data warehouse thực sự. File trong GCS được set lifecycle = 7 ngày, tức 7 ngày sau khi nó được upload lên GCS thì nó sẽ tự xóa đi để đảm bảo việc transform và load luôn nhanh.
BigQuery cũng hỗ trợ các câu SQL INSERT, UPDATE và quan trọng nhất là MERGE. MERGE cho phép bạn chạy 1 câu và có thể quyết định INSERT record đó nếu key chưa tồn tại hoặc UPDATE data mới.
Đây là yếu tố tiên quyết để giữa cho data của DWH bắt kịp với môi trường production. Với các doanh nghiệp mà data dạng log ghi xuống là xong thì có nhiều lựa chọn khác để chạy DWH, ví dụ như ClickHouse của Yandex chẳng hạn, nhưng với bản chất của đơn hàng online có status đổi thường xuyên thì mình phải update kịp thì mới ngon.
Sau khi thử nghiệm MERGE, mình cảm thấy rất hài lòng, và lượng data scan phục vụ cho công tác ELT dữ liệu, làm sạch, chuyển đổi cũng không quá lớn nên mình đã chọn BigQuery.
Tính tới thời điểm hiện tại, BigQuery vẫn đang làm mình hài lòng vì performance cực tốt, query chạy cực nhanh trên các bảng nhiều chục GB (bọn mình chưa có data lên tới TB đâu, nhưng Google nói là TB hay PB cũng không là vấn đề).
Tóm lại, chọn BigQuery vì:
Để làm việc này, mình có một cái pipeline như sau:
Apache NiFi là một tool rất tuyệt để load data mà không cần code gì thêm, bạn có thể down nó về tại link: https://nifi.apache.org/. File do NiFi chạy xong và upload lên Google Cloud Storage là file định dạng AVRO, một định dạng binary và columnar, để đạt tốc độ query nhanh và ít chiếm dung lượng (tức tiết kiệm tiền BigQuery và tiền lưu trữ Google Cloud Storage nữa). Ngoài ra còn một định dạng ngon nữa là ORC, NiFi cũng hỗ trợ luôn, nhưng khi dùng BigQuery để scan các file này thì tạch nên chưa thử lại.
Còn việc load từ MongoDB sang thành file phải dùng custom script python là do mình cần đảm bảo đủ field, đúng data type các thể loại. Cái script này đơn giản, chỉ đọc DB, gom file thành batch 10.000 record / file JSON rồi upload lên Google Cloud Storage thôi chứ không có gì nghiêm trọng.
Cái quan trọng là MERGE script. Nó làm nhiệm vụ như sau:
Cuối cùng là để schedule đống này thì cần dùng cái gì? Mình dùng Jenkins vì nó giao diện Blue Ocean đẹp, quản lý được các step chạy (mỗi step là một bảng), biết khi nào fail, fail chỗ nào, lỗi gì. Bạn có thể dùng Airflow, Azkaban hay bất kì tool scheduler nào khác đều được cả.
Hiện tại 1 pipeload lần gần như mọi data quan trọng chạy chỉ tốn khoảng 6 phút (mỗi step tầm vài giây tới vài chục giây là hết mức), rất nhanh và mình cực kì hài lòng với điều này. Mình viết thêm script check trùng và đối chiếu với production db để đảm bảo data chạy đủ, nhanh và không bị thiếu record (check tới mức status luôn, lệch phát vỡ mồm mất).
Điều tuyệt vời nhất là mọi sức mạnh xử lý đều chạy trên hạ tầng của BigQuery, mình không phải load bất kì data nào về server của riêng mình nên tốc độ rất nhanh, không bao giờ phải lo tới chuyện hết CPU, hết RAM, hết SSD hay server chết cả. Và cũng nhờ vậy mà data được process, transform rất nhanh, số liệu có cực nhanh. Server mình chỉ để schedule chạy scrip thôi, và chỉ cần một server cùi bắp là đủ. 1GB RAM 1 nhân CPU.
Nếu muốn, mình hoàn toàn có thể optimize cho flow này load 5 phút / lần, nhưng hiện tại chưa cần mức độ nên thôi cứ thong thả.
Vài dòng chia sẻ với các bạn.
206 
7
206 
206 
7
"
https://betterprogramming.pub/loading-images-from-cloud-storage-on-a-kotlin-android-app-d1574a059e04?source=search_post---------56,"Sign in
There are currently no responses for this story.
Be the first to respond.
Rosário Pereira Fernandes
Jun 15, 2020·3 min read
If you’re developing an Android app and you’re storing images in cloud storage for Firebase to later display in your app, you’re probably using an image loading library such as Glide, Picasso, Fresco, or firebase-ui-storage. Yes, these libraries can do the job, but what if I told you there’s a more pleasant and efficient way?
In this article, I’ll introduce you to the new kid in town, Coil, and I’ll show you how to use it to load your cloud storage images with less boilerplate code.I’ll assume you’re already familiar with cloud storage for Firebase and already know how to use it in an Android app. If not, you may want to take a look at the Documentation.
Coil is an open-source Image Loading Library for Android. Unlike the other pre-existing libraries (Glide, Picasso, etc), Coil was built entirely in Kotlin and is backed by Coroutines. It’s also:
Because Coil uses Java 8, one of the prerequisites is that you enable Java 8 in your Android project. You can do so in your module’s build.gradle file:
Then you add it as a dependency on that same file:
The simplest way to use Coil is by calling the ImageView.load() extension functions that the library provides:
You can also specify a placeholder or any other additional configuration on a trailing lambda:
Apart from the extension functions, Coil also provides other ways to load images, such as Requests and Image Loaders, which I won’t go through for brevity. But you can learn more about them in their Documentation.
Let’s say you have an image stored on Cloud Storage for Firebase, which can be accessed using the path “users/me/profile.png”. To display this image on an ImageView in your Android App, you may need to get its download URL first and then pass it to Coil, as the docs suggest:
It looks simple, right? But what if I told you that even this can be simplified? That’s where firecoil comes in.
firecoil is an open-source library that allows you to load images with Coil by simply providing the StorageReference of said image. To use firecoil, you’ll need to add the jitpack maven to your project’s build.gradle file:
Next add the firecoil dependency to your module’s build.gradle file:
Now with firecoil you no longer need to get the download URL. You can simply pass your StorageReference to the ImageView.load() extension function provided by the library:
Since firecoil is built on top of Coil, you can also use a trailing lambda to specify additional configuration:
That’s it! With this simple and concise snippet, your app should now have beautiful images displayed.
Google Developer Expert for Firebase 🔥 | GitHub Star … Views and Opinions are my own.
115 
115 
115 
Advice for programmers. Here’s why you should subscribe: https://bit.ly/bp-subscribe
"
https://medium.com/@kevinsimper/google-cloud-storage-cors-not-working-after-enabling-14693412e404?source=search_post---------21,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kevin Simper
Aug 13, 2018·1 min read
If you experience that the Google Cloud Cors is not working even after you did “gsutil cors set cors.json gs://your-bucket/”, it may be because you are using the wrong root domain, only one of the domains will have the headers enabled on the request, so
storage.googleapis.com/your-bucket ⬅ will not have headers
your-bucket.storage.googleapis.com ⬅ will have cors headers
Something very easy to overlook and Google by default links to the storage.googleapis.com/your-bucket from the interface, so if you copied that and thought it work.
They do say it here, but that is the only place, which I think is not enough
https://cloud.google.com/storage/docs/cross-origin:
I really like building stuff with React.js and Docker and also Meetups ❤
370 
11
370 claps
370 
11
I really like building stuff with React.js and Docker and also Meetups ❤
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@guillaume-blaquiere/i-agree-they-serve-almost-the-same-purpose-2111003443f3?source=search_post---------320,"Sign in
There are currently no responses for this story.
Be the first to respond.
guillaume blaquiere
·Nov 6, 2020
Neekey
I agree, they serve almost the same purpose. Cloud Run is more backend oriented (API) and App Engine can do both (you can serve static files for the front end, and your app as backend).
So you can fix this with the new compatibility with Global HTTPS load balancer (set a backend as Cloud Storage for serving static files, the others endpoints to CLoud Run).
The IAP compatibility is not yet available on Cloud Run and is more web oriented than API oriented.
So, if you go deeper, the bundled services are deprecated and above all useful for the 1st generation of App Engine. Forget them for the 2nd generation, and thus it's not an advantage for App Engine.
Cloud Run allows bidirectional streaming (for modern website), that App Engine doesn't support.
A last word about cold start. Min Instance on Cloud Run is now in beta and far less expensive than App Engine. I wrote an article on this and I submitted it to Google for a review before publication. Stay tune!
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/iot-5g-extreme-ideas-lab/intel-internet-of-things-iot-at-ueab-2e4e4e538475?source=search_post---------316,"There are currently no responses for this story.
Be the first to respond.
Continuous growth in developing open source software and availability of Cloud Storage (Online data storage) is on the rise, no doubt, and Intel at UEAB is at the forefront of this revolution. 6th May 2016 saw another successful Intel IoT event .The event saw 83 attendees converge at the Information Systems and Computing (ISC) Main lab to explore and improve their skill using Intel Galileo and Intel XDK IoT Edition with an aim of creating societal solutions. The attendees came from different departments in the university a clear indication of their interest on IoT. However, majority of the attendees came from ISC and Technology departments as expected. The major focus of the event was to introduce the attendees into the IoT world using Intel Galileo and Intel XDK IoT Edition.
The event kicked off at 9.35 am after a good number of attendees had arrived at the venue, I (Victor Sidho ISP UAEB) gave a welcoming keynote address thanking everyone for arriving and encouraged them to participate actively during the event. I then took them through the event schedule and made them aware that the event was fully sponsored by Intel and introduced the hashtags for the day #IoTUEAB and #IntelAtUEAB.
Mr. Onsongo Nyamwaya [HoD, ISC] took the floor and introduced IoT from the networking perspective defining it as an environment in which objects, animals or people are provided with unique identifiers and the ability to transfer data over a network without requiring human-to-human or human-to-computer interaction. He emphasized that IoT leads to a shift in how networks are designed and we should embrace it.
Thereafter, Marvin Ngesa (ISP DeKUT) gave a deeper overview of IoT by use of examples of what they do at their institution. He displayed different video clips on what they do and what Intel Worldwide team is doing to better the world using IoT. He also gave an introduction of the 5G network stating that the mobile industry makes an evolutionary leap every 10 years; giving the advantages of the network. He challenged the attendees to be part of the making of 5G network.
Ngetich Towett(ISP Moi University)who was also in attendance was given a chance to speak. He then encouraged non-IT students to embrace and love technology and that it is never too late for them to do amazing things using Intel IoT. He inspired most of them by giving an example of himself that he is not pursuing a course related to IT. He urged the attendees to attend an Intel event that he is organizing at the Moi University-main campus on May 27th, 2016.
We wrapped up the morning session and headed off for lunch.
Ngesa Marvin conducted the afternoon session and it was a practical implementation of what the attendees had learnt during the morning session. The attendees were grouped into six groups and each was given the Intel Galileo Kit for development. Marvin then took the newbies into systematic installation of Intel Galileo drivers and the Arduino IDE then showed them the code structure. .He advised the attendees to later apply the knowledge in building something that could solve problems in our own country. At the end they developed prototypes and most importantly learnt and interacted with the board and the kit.
The event ended at 4.35pm with closing remarks from. I appreciate Ngesa Marvin and Ngetich Towett for taking their time to attend and participate the event. I also thank the lecturers who were there at the event: Mr. Onsongo Nyamwaya, Mr. Ochola Micah and Mrs. Rose Nyamwamu. Special gratitude to Intel for granting us such an amazing opportunity. I cannot forget to thank his organizing committee: Oluchi Ibeneme, Naomi Kirui and Edward Kipkalya for the success of the event.
Reported by
Sidho Victor — ISP UEAB.
Originally published at intelstudentpartnerseastafrica.wordpress.com on June 6, 2016.
Welcome to The IoT Xtreme Ideas Lab.
Written by
Electronic Engineer. Software Innovator, Intel. Deep Learning Abantu. IoT Champion for GDGs. Co-lead, GDG Nairobi #AI #IoT #5G Freak. Opinions = Mine
Welcome to The IoT Xtreme Ideas Lab. In this publication, I share do-it-yourself Electronics, Embedded systems & Internet of Things projects. I believe Education should be free & accessible to all. I am currently plotting world domination through Open Source, Software & Hardware.
Written by
Electronic Engineer. Software Innovator, Intel. Deep Learning Abantu. IoT Champion for GDGs. Co-lead, GDG Nairobi #AI #IoT #5G Freak. Opinions = Mine
Welcome to The IoT Xtreme Ideas Lab. In this publication, I share do-it-yourself Electronics, Embedded systems & Internet of Things projects. I believe Education should be free & accessible to all. I am currently plotting world domination through Open Source, Software & Hardware.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@azehrams/1-tb-of-cloud-storage-at-64-99-usd-one-time-payment-b9d1ddb0b109?source=search_post---------144,"Sign in
There are currently no responses for this story.
Be the first to respond.
ANGU RANSOM(RANSBIZ)
Oct 27, 2016·3 min read
1 TB of Cloud Storage at 64.99 USD One Time Payment
Cloud computing is on the rise with various cloud storage offers by top tech companies around the world. This week, I will share with you a special offer I got through a partner of the Avangate affiliate Network.In my last post, we saw how to get 2 GB of free storage with Google. ZOOLZ storage is offering a lifetime back up with onetime payment unlike the usual monthly payments by other cloud service providers.
Before getting into the details of this service, I will like the reader to know that this post contain a promotional link to the product page on avangate . It is a promotional offer, so when you purchase, I will get some commissions which will keep me blogging . Ok, so let me make the offer known.   In this 64.99 Dollars ( 59.99 EUR ) offer you will benefit from 1 TB storage which can be used by two people. You will pay only once and all your data will be store on Amazon web services servers for a life time. In addition to this, you will get. You data are secured, with unlimited external/NAS back up support.
Ad
This world first cold storage technology will offer the user an innovative experiencefor a life time.
With this cloud services, you cannot store duplicated files, thanks to the duplication technology of Zoolz. This ensures, your storage is maximized.
Secondly, you will be able to back up unlimited USB and fire wire and Network Drives.
Also,you can easily select your photos thanks to the preview thumbnail on all your images.
Restoring your data is also free of charge.
With Zoolz cloud services, you can access all your devices thanks to the Zoolz mobile app for iOS and Android.
Zoolz cloud services are being used by companies over the word in all domains such as education, mining , engineering, health care, Technology, construction, among others. Names like Duffels, Heskins,H2H, BEZTAK,West Medical, Havard Library ,Dell, Acer and more. It’s a service worth using.
You can opt in for either home or business cloud storage. You can as well get storage from 50 GB to to 10 Pentabyte of storage.
You can contact me ( email: ransbiz@live.com) to send you a special offer link with your desired cloud service and storage size. I will offer you other soft wares at the best discounts .You can as well request coupon codes for other software too.
It is noteworthy, that Zoolz is the only cloud solution that keeps your data even when you disconnect your drives.This inaugural cloud storage solution provides a long term storage unit, for your entire data, on every single one of your external, internal and network drives.
You can get zoolz cloud storage using the Avangate payment gate way No monthly renewals. Hope you will enjoy the offer
Originally published at www.ransbiz.com.
Social media ,Blogger,teacher,anti- scamming,motivation,entrepreneurship and education of the mind,Self made Ideas,Mobile marketing,bulk SMS,STEP BY STEP ,love
Social media ,Blogger,teacher,anti- scamming,motivation,entrepreneurship and education of the mind,Self made Ideas,Mobile marketing,bulk SMS,STEP BY STEP ,love
"
https://medium.com/hackernoon/hosting-a-free-static-website-on-google-cloud-storage-d0d83704173b?source=search_post---------34,"There are currently no responses for this story.
Be the first to respond.
This guide walks you through setting up a free bucket to serve a static website through a custom domain name using Google Cloud Platform services.
Sign in to Google Cloud Platform, navigate to Cloud DNS service and create a new public DNS zone:
By default it will have a NS (Nameserver) and a SOA (Start of Authority) records:
Go to you domain registrar, in my case I purchased a domain name from GoDaddy (super cheap). Add the nameserver names that were listed in your NS record:
PS: It can take some time for the changes on GoDaddy to propagate through to Google Cloud DNS.
Next, verify you own the domain name using the Open Search Console. Many methods are available (HTML Meta data, Google Analytics, etc). The easiest one is DNS verification through a TXT record:
Add the TXT record to your DNS zone created earlier:
DNS changes might take some time to propagate:
Once you have verified domain, you can create a bucket with Cloud Storage under the verified domain name. The storage class should be “Multi-Regional” (geo redundant bucket, in case of outage) :
Copy the website static files to the bucket using the following command:
gsutil rsync -R . gs://www.serverlessmovies.com/
After the upload completes, your static files should be available on the bucket as follows:
Next, make the files publicly accessible by adding allUsers entity with Object Viewer role to the bucket permissions:
Once shared publicly, a link icon appears for each object in the public access column. You can click on this icon to get the URL for the object:
Verify that content is served from the bucket by requesting the index.html link in you browser:
Next, set the main page to be index.html from “Edit website configuration” section:
Now, we need to map our domain name with the bucket we created earlier. Create a CNAME record that points to c.storage.googleapis.com:
Point your browser to your domain name, your website should be served:
While our solution works like a charm, we can access our content through HTTP only (Google Cloud Storage only supports HTTP when using it through a CNAME record). In the next post, we will serve our content through a custom domain over SSL using a Content Delivery Network (CDN).
Drop your comments, feedback, or suggestions below — or connect with me directly on Twitter @mlabouardy.
#BlackLivesMatter
192 
3
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
192 claps
192 
3
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
CTO & Co-Founder @Crew.work — Maker & Indie Entrepreneur @Komiser.io, Writer/Author and Speaker — Blog: https://labouardy.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/android-dev-moz/fbs-8ff8b6fa3d2c?source=search_post---------90,"There are currently no responses for this story.
Be the first to respond.
Ao desenvolver uma aplicação capaz de armazenar ficheiros para disponibilizá-los à outros utilizadores, você provavelmente encontrou o Cloud Storage for Firebase (também chamado Firebase Storage) como uma das tecnologias que permitem fazer isso de forma simples, fácil e rápida. E se você escolheu o Firebase Storage dentre as outras opções, devo parabenizá-lo pela escolha. Mas além de armazenarmos os dados de forma simples, precisamos garantir que eles estejam seguros. E para fazer isso no Firebase Storage, utilizamos as regras de segurança — o assunto principal deste artigo.
Quem já viu/utilizou as Regras da Realtime Database, ao aceder à aba de Regras do Storage na Consola do Firebase notará que a estrutura das regras de segurança são diferentes. Isso porque as regras da Realtime Database ficam organizadas em uma árvore JSON, enquanto que as do Cloud Storage parecem um JSON sem aspas:
Esta organização é utilizada em alguns dos serviços da Google Cloud Platform como o Cloud Firestore (provavelmente falarei dele num dos próximos artigos).
Tal como na Realtime Database, as regras de segurança do Cloud Storage permitem controlar operações de leitura e/ou escrita em ficheiros. Podemos também controlar como estes ficheiros são estruturados e quais meta-dados eles possuem.
Para utilizarmos as regras de segurança do Cloud Storage, precisamos de conhecer 2 palavras reservadas: allow e match.
Tal como indica o nome, allow indica se uma operação (read ou write) é permitida ou não, de acordo com uma condição especificada (opcional). Por exemplo:
Lembra quando nós colocávamos ambas regras (.write e .read) da Realtime Database em true para testarmos, deixando a base de dados totalmente insegura? Pois é, o equivalente no Cloud Storage é:
Nota: Isto irá deixar o Storage público, o que significa que qualquer pessoa (mesmo que não seja utilizador da sua aplicação) pode armazenar e obter ficheiros do seu Cloud Storage. Utilize estas regras apenas para testes.
Se quiséssemos fazer o inverso do exemplo acima, na Realtime Database colocaríamos o .write e .read em false. Assim, ninguém (além dos administradores do projeto) teria acesso à base de dados. No Cloud Storage isto pode ser obtido com as seguintes regras:
E quando utilizavámos o auth!=null na Realtime Database? Permitíamos que a base de dados fosse acedida apenas por utilizadores registados no Firebase Auth. No Cloud Storage também é possível:
Como você já deve ter reparado, em todas as regras que escrevemos até agora, utilizamos a palavra match. Ela serve para especificarmos o caminho onde queremos aplicar cada regra. Este caminho pode ser especificado ao detalhe (caminho exato e estático) ou através de variáveis (caminho dinâmico).
Este é o match mais simples, utilizado quando já sabemos o caminho exato do ficheiro à que queremos aplicar as regras. Por exemplo, para aplicar regras aos ficheiros fotoPerfil.png e miniatura.png localizados no diretório /imagens :
Outra forma seria dividir o caminho, criando ramificações na árvore:
Por vezes, podemos não conhecer o nome de um ficheiro ou de um diretório. Nesses casos, utilizamos variáveis tal como fazíamos na Realtime Database (utilizando o cifrão $), mas aqui colocamos o nome da variável entre chavetas. Por exemplo:
Para além de controlar permissões para armazenamento e leitura de ficheiros, podemos também validar os ficheiros que estão a ser armazenados no Cloud Storage. Vou deixar apenas um exemplo para demonstrar a validação, para não estender demais este artigo:
Com a regra acima, garantimos que o ficheiro que está a ser armazenado:
E esta foi uma introdução às Regras do Cloud Storage for Firebase. No próximo artigo irei mostrar algumas formas de utilizar os meta-dados dos ficheiros. (Spoiler Alert: vamos começar a desenvolver uma app semelhante ao Google Drive)
Caso tenha alguma dúvida ou sugestão, pode me contactar pelo email rosariofernandes51@gmail.com ou pelo Telegram. Será um prazer conversar com você. 🙂
Artigos em português, escritos pela comunidade GDG Maputo.
37 
37 claps
37 
Artigos em português, escritos pela comunidade GDG Maputo. Faça parte da nossa comunidade: https://www.meetup.com/GDG-Maputo/
Written by
Google Developer Expert for Firebase 🔥 | GitHub Star … Views and Opinions are my own.
Artigos em português, escritos pela comunidade GDG Maputo. Faça parte da nossa comunidade: https://www.meetup.com/GDG-Maputo/
"
https://onezero.medium.com/flickrs-big-change-proves-you-can-t-trust-online-services-e34af51ed4a?source=search_post---------327,"Sign in
There are currently no responses for this story.
Be the first to respond.
Lance Ulanoff
Nov 21, 2018·4 min read
I need a digital moving van.
I have 60 days to relocate more than 2,100 images from their home on Flickr to another safe digital haven. The once popular photo-sharing destination…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/luxor/siaprime-a-friendly-sia-fork-61969ebebe83?source=search_post---------264,"There are currently no responses for this story.
Be the first to respond.
SiaPrime, decentralized cloud storage for business and corporations: “Built for a world where companies and organizations around the globe abandon centralized providers and take control of their valuable private data.”
Given the network difficulty and the lack of up-to-date GPU mining software for the Blake2B PoW algorithm, you’ll need a Blake2B ASIC like Bitmain’s A3 or Innosilicon’s S11.
To start mining, pick one of the Stratum nodes below (ensure it is port 3335), and type in the pool setting as follows:
URL: stratum+tcp://scp-us.luxor.tech:3335(set to your nearest location)
scp-eu.luxor.tech:3335
scp-asia.luxor.tech:3335
Worker: [LuxorUsername or YourSCPAddress].workername (workername can be anything, but avoid using symbols or special characters as it may be invalid). More on user accounts here.
Password: 123
For the other nodes (Pool 2 and Pool 3), feel free to use any of the other region nodes, such as scp-eu.luxor.tech:3335.
Once you’ve started hashing, grab yourself a beer! It’ll take about 5 minutes for your worker to appear on our stats page. To find your user, simply go to mining.luxor.tech and type in your wallet address at the top of the searchbar. You should see something like this:
To make your life easier, we developed the smartest Mining Bot: Lifeguard. He is available 24/7 and always in a good mood to answer all of your questions and track your miners status and performance.
Luxor is building infrastructure to support the next generation of digital assets. Learn more about us 👉 here.
We are available on Twitter and Discord ping us there. We would love to hear from you!
Luxor has spent the past 2 years building North America’s…
57 
57 claps
57 
Luxor has spent the past 2 years building North America’s largest Bitcoin & Altcoin mining pools. Through mining thousands of blocks, we’ve gained a deep understanding in extracting and valuing hashrate.
Written by
Luxor is a Bitcoin mining pool and full-stack crypto mining company. The financialization of hashrate starts with us. https://www.luxor.tech/
Luxor has spent the past 2 years building North America’s largest Bitcoin & Altcoin mining pools. Through mining thousands of blocks, we’ve gained a deep understanding in extracting and valuing hashrate.
"
https://medium.com/@insync/insync-1-3-with-file-matching-is-finally-here-94a6ae957bc1?source=search_post---------394,"Sign in
There are currently no responses for this story.
Be the first to respond.
Insync
Oct 14, 2015·5 min read
We are glad to announce our biggest release yet, Insync 1.3.
Insync 1.3 comes with one of our most requested features, file matching. This allows you to automatically migrate from Google Drive with the help of our new and simplified on-boarding process.
A notable addition to Insync 1.3 is our external browser login. This allows Insync to use your default browser when adding an account. This resolves SSO and proxy related issues some users have encountered and simplifies login by using your existing Google session so you won’t have to log in again.
Our on-boarding process has been simplified and made more efficient for our users. There are two options upon setup:
Upon onboarding you can just select your old Google Drive folder and that’s it! Insync will automatically detect your Google Drive Folder and will sync the files that were already downloaded. It also downloads new files that you might have added after you stopped using the Google Drive application. If you wish to change the name and location of your Google Drive folder, you can also do so using Insync.
2. You can opt to create a new folder for your files or use an existing folder to sync.
Upon setup, Insync lets you determine the location and the name of your Insync folder.
You also have an option to either sync all of your files or use our selective sync feature to choose which files you would want to sync locally. You can also choose to convert your Google Drive files to Microsoft Office or OpenDocument formats.
The selective sync feature was also improved by adding two things:
To make syncing more space-efficient, Insync now has the option to not sync new files or folders added your Google Drive account. This helps if you only want to sync specific files. By unchecking “Sync new children of partial folders”, new files will not be downloaded to your Insync folder. New files under fully synced folders (as compared to partially synced folders) will still be downloaded.
Last but not the least, our most awaited feature is finally available in 1.3, file matching.
This feature matches files + folders in your file system with existing ones in your Google Drive. This feature will help speed up adding an account since it identifies the Google Drive app in your computer. This is because re-syncing is not needed, thus avoiding duplication. The availability of this feature allows you to use your old Google Drive folder with Insync. The files previously available in your old Google Drive folder will be identified and will be reused by Insync. Easy right?
File matching, new on-boarding, improved selective sync and other new features are now available on Insync 1.3.
Here are the complete release notes:
Let us know your thoughts on our Support Center :-)
Do more with cloud storage. Do more with Google Drive.
10 
6
10 
10 
6
Do more with cloud storage. Do more with Google Drive.
"
https://medium.com/@duhroach/optimal-size-of-a-cloud-storage-fetch-8c270b511016?source=search_post---------72,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Nov 2, 2017·3 min read
One of the backbones of video streaming is the sweet spot between user connection speed, cloud hosting performance, and the window size you use to buffer and request data.
And for MegaStreams (a video streaming service company), this sweet spot is critical to their operations.
Their current architecture is serving video data directly from Google Cloud Storage buckets to users on mobile and desktop devices. Over the past month, they’ve been trying to figure out the right way to duplicate their content, and optimize their window sizes, so they can ensure consistent buffering to their clients across US and Europe.
So, in order to figure that out, let’s look at what the read-performance is from GCS at various window sizes.
To figure this out, I wrote a small script which:
What we can see here is that for fetches <= 1MB we can see very consistent read costs through HTTP fetches. Once we get above that, we can see that we’re quickly bottlenecked by the HTTP protocol, rather than anything GCS related.
We can switch our test just slightly to get a better view of things. Let’s take an 8MB file, and fetch it through different sized chunks (note : smaller is better in this graph):
This graph helps us see our read performance from another perspective. As the chunk size decreases, the overhead, per-transaction increases, thus making things slower.
Combined, these two graphs illustrate something important : much like write throughput, GCS is extremely strong in terms of single stream throughput. Which means that for both uploads, and downloads, GCS is at it’s best performance when you’re favoring larger requests, around 1MB in size, otherwise, the per-transaction overhead of the requests themselves will start to slow things down.
For MegaStreams, this was a massively helpful piece of data. They now had the ability to determine what level of regional redundancy they needed for serving their video content in order to optimize the 1MB+ GCS target to improve buffer speeds. Data saves the day!
There’s some great tools to help you profile your networking perf.
Don’t forget to check you’re using the right region for your GCP instance.
And make sure you’re using the right CPU counts for your networking performance.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
70 
1
70 
70 
1
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
"
https://medium.com/@alex.okrushko/hi-jose-68e8f8da3fd6?source=search_post---------300,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alex Okrushko
Aug 2, 2018·1 min read
Jose
Hi Jose,
The example that I gave in the article was for Google Cloud Storage, which says:“ Clients should use truncated exponential backoff for all requests to Cloud Storage that return HTTP 5xx and 429 response codes, including uploads and downloads of data or metadata.”
Another example that you might have seen is gmail. If the app determines that there is no network connection it starts re-querying with exponential backoff.
In Firebase Console it’s also used in many places, e.g. the piece that I’ve implemented was querying notifications for the app.
Software Engineer at Google working on Firebase Console.
1 
1 
1 
Software Engineer at Google working on Firebase Console.
"
https://pub.towardsai.net/cloud-storage-vs-database-how-the-cloud-organizes-its-services-175d3a1cbdf3?source=search_post---------102,"I am currently studying for an AWS certification, which seems more enjoyable than I could have ever imagined. The Cloud, which is much more complex than you can ever think, comprises thousands of separate services, and as such could be organized in a multitude of ways. The most common configuration used by Cloud providers is by grouping…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@duhroach/optimizing-google-cloud-storage-small-file-upload-performance-ad26530201dc?source=search_post---------48,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Oct 12, 2017·4 min read
DNA SOAP is a bio-tech company focused on understanding more about the human genome.
Their latest release was a software package that allowed a distributed protein folding computing model; where internal machines and external machines could be used together to calculate protein folding operations.
While this was a great, cost effective way to tackle a very compute-heavy problem, their new software design wasn’t performing as well as they wanted. Any time they would create a new data set to be worked on, there was a 5–6 minute delay before any of the clients would start doing computation.
Their architecture was pretty straightforward. Once a new genome dataset came in, it would be mirrored across regions, and then a Data Coordinator would chop up the genome into workable blocks, and upload that data to Google Cloud Storage. Clients listening on the service would be notified of the available work via Pub/ Sub, and would grab the blocks directly from GCS to start working.
The problem, as defined by DNA SOAP, was that it took too long to upload all the files to GCS. Here was the interesting part : The data coordinator would modify the number and size of blocks based on the expected number of clients in that region. So with lots of users, there could be a lot of smaller files being sent around. The assumption then, is that there must be some correlation between file sizes and GCS upload performance.
This seems like a straightforward enough problem to duplicate. We can generate a bunch of small files, upload them to a GCS bucket, and download them individually from the GCS bucket, and see what the time difference is.
To test this, I created a small python script which generated files of various sizes, and then uploaded each file to a GCS regional bucket 100 times (random name each time to remove collisions). Below, you see the performance (bytes/sec) charted against the size of the objects.
As the graph shows, upload speed improves as the object size improves, which is mostly due to the reduction of transactional overhead per event.
The reason for this is that GCS is really strong in terms of single stream throughput, but there’s a fixed latency cost per request that’s related to ensuring that the files are replicated and uploaded properly. As such, this transactional overhead is higher for smaller files, and as that number increases, so does the amount of overhead. As the files get larger, the transactional overhead is smaller, resulting in higher throughput.
This concept of high-overhead-per-operation is not a new one. If you’ve ever done SIMD programming on the CPU, the same idea exists : batch your operations together so that the overhead of each operation is mitigated across the set.
For DNA SOAP, fixing their upload performance on GCP followed the same idea: Parallel uploads.
Gsutil provides the `-m` option which performs a parallel (multi-threaded/multi-processing) copy which can significantly increase the performance of an upload. Here’s the same test, but batch uploading 100 200k files using `gsutil -m cp <src folder> <dst folder>` rather than uploading each one individually.
Although it’s more pronounced at the right side of the graph, the log-scale version shows that -m gives significant improvements in performance at the left side (smaller file sizes) as well.
GCS is a powerhouse when it comes to upload performance, but that power comes once you hit a certain level of efficiency. The transactional overhead of tons of small files means lots of additional round-trips (to ensure consistency) and thus, less performance.
The takeaway? It’s more performant to batch upload your files with `-m` but if you have to do things one-at-a-time, make sure you’re using the direct API rather than going through GSUTIL for each one.
Which is faster, TCP or HTTP load balancers?
Did you know there was a connection between core count and egress?
Want to know how to profile your kubernetes boot times?
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
113 
113 claps
113 
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/hybrid-cloud-storage-cross-cloud-replication-5b5a3dee8ff1?source=search_post---------122,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 8, 2018·11 min read
11.11 The Biggest Deals of the Year. 40% OFF on selected cloud servers with a free 100 GB data transfer! Click here to learn more.
Alibaba Cloud Hybrid Cloud Storage Array allows you to build cloud backup solutions between different server, both on premises and on the cloud. However, there are cases where you may require real-time data protection, such as if you work in a company where everyone uses a shared directory for centralized document storage. If you need these documents to trigger backup within a very short period of time (with short RTO), or need them to be quickly distributed to other users, then the Cloud Storage Gateway’s replication mode will meet your needs.
Cloud Storage Gateway is a pure software gateway form of online cloud hosts and offline virtual machines. It provides NFS and CIFS file protocol support for client hosts, and uploads files written to the gateway to Alibaba Cloud Alibaba Cloud Object Storage Service (OSS). Cloud Storage Gateway supports two modes: cache and replication. The difference between these two modes is that the replication mode guarantees both the local data and data on the cloud are full data, while the cache mode only keeps metadata of the hot and cold data locally. When you access cold data, you need to pull the corresponding data from the cloud.
In this article, we will use a backup and a recovery operation to demonstrate how Cloud Storage Gateway quickly backs up a user’s local data to Alibaba Cloud OSS, and conveniently reverse-synchronizes the data from OSS to the user’s local environment. The deployment and configuration flexibility of the storage gateway can recover the backup data in Alibaba Cloud OSS to Alibaba Cloud, a third-party cloud, or the customer’s offline data center during data recovery, as long as the target cloud storage gateway is connected to the Alibaba Cloud OSS network.
Cloud Storage Gateway-based cross-cloud replication architecture diagram
First, we need to deploy a cloud host in the customer’s cloud environment to support Cloud Storage Gateway. In order to facilitate the rapid deployment of Cloud Storage Gateway, a virtual machine image file is provided to directly generate a cloud host. First enter the Cloud Storage Gateway page of the Alibaba Cloud console, select the East China 2-Shanghai region, and click Create Gateway Cluster in the upper right corner to create a gateway cluster. A gateway cluster is a concept similar to a label or a group, which makes it easy for users to aggregate multiple gateways into one page for management and maintenance.
Create a gateway cluster
After creating a gateway cluster, go to the cluster page and create a gateway. Note that select Local data center as the Location, and File gateway as the Type. Then go to the next step to download the KVM image. This image is compatible with cloud vendor T’s cloud host image, and can be used to create cloud vendor T’s cloud host.
Create a gateway and download an image
In the last step, download the certificate. After the download is complete, the steps to create a gateway on the Alibaba Cloud console are completed.
Download the certificate to complete creation of the gateway
Go back to cloud vendor T’s console and select the Shanghai region. Choose Image > Custom Image, and then click Import Image.
Enter the image page
The first time you import a custom image, you need to enable the cloud vendor’s object storage function. You can open it by following the corresponding steps. Then upload the Cloud Storage Gateway image file to the OSS. Copy the link to the image file.
Link to the file in OSS
Go back to the Import Image page, copy the image file link, and then paste it in to the Image File URL field, and then enter other required fields. Click Start import.
Importing an image
It takes 5–10 minutes to import the image file. As shown in the following picture, we can see this image in the images list.
Image imported successfully
Click Create Cloud Host on the right side of the images list to create a cloud host. The next step is to customize the specifications of the cloud host. Cloud Storage Gateway supports different configurations. You can choose the appropriate configuration according to your business needs. If you have any questions, you can also seek advice from Alibaba Cloud’s Cloud Storage Gateway team. Here, let’s select the lowest-level configuration to deploy Cloud Storage Gateway.
Cloud Storage Gateway host configuration
After successfully purchasing the cloud host, wait a few minutes for the system to complete initialization. Note that Cloud Storage Gateway’s web management and NFS and CIFS protocols require some additional ports. Therefore, we need to enter the security group of the cloud host and open the following ports.
Opening Cloud Storage Gateway ports
By default, the cloud host should have already added a suitable security group policy. Just add the ports that we want to open into the policy.
Then, we can enter https:// IP address>:443 in the browser to open Cloud Storage Gateway’s web management interface. You need to provide the AK of your Alibaba Cloud account and set the gateway’s administrator username and password. At the same time, you need to upload the certificate that you downloaded when you created the gateway. This certificate has the same role as the certificate of the Hybrid Cloud Backup client.
Cloud Storage Gateway registration interface
After completing the registration, we can see that the gateway status has changed from Initialized to Activated on the Cloud Storage Gateway page of the Alibaba Cloud console. This means the cloud storage gateway is already online, and we can use the Alibaba Cloud console to monitor the resource status of the cloud storage gateway in cloud vendor T’s network.
Cloud Storage Gateway activation
In addition, after completing the registration, we will be able to use Cloud Storage Gateway’s management interface.
Cloud Storage Gateway’s management interface
Next, you can complete Cloud Storage Gateway configuration in three simple steps: cloud resource management, cache settings, and NFS/CIFS mounting.
Cloud resource management connects Alibaba Cloud OSS resources to gateways. Before setting cloud resources, you need to create OSS buckets in East China 2 — Shanghai region. Then Cloud Storage Gateway will automatically pull all available OSS buckets within this region for you to choose from. Note that we need to choose Internet for this region, because the network of cloud vendor T and Alibaba Cloud’s Cloud Storage Gateway are connected through the Internet.
Create cloud resources
Cache settings are the process of formatting the data disk we used for creating the cloud host. Selecting this disk completes the cache settings.
Cache settings
The last step, NFS/CIFS mounting. Because we created a Linux host when demonstrating Hybrid Cloud Backup, here we only set up NFS (CIFS settings are similar). We need to enter the NFS name, select the cloud resource that we created earlier, as well as the cache path and user mapping. You can edit the Read-Only Client List and Read-Write Client List fields based on your IT environment conditions. In this demonstration, because the webserver host and the cloud storage gateway that we created earlier are in the same intranet environment, we enter the intranet IP address of the webserver host in the Read-Write Client List field. Select Synchronization mode. In Advanced Settings, select Yes for Ignore Delete to ensure that the local delete operation will not delete OSS files on the cloud. Click OK.
NFS settings
NFS Settings > Advanced Settings
So far, the configuration of Cloud Storage Gateway is complete. Next, we can log on to the webserver cloud host that we created earlier to mount the NFS directory provided by Cloud Storage Gateway. You can use the showmount command to check whether the mount on Cloud Storage Gateway is in place.
View Cloud Storage Gateway’s NFS directory
Note that the name of the NFS directory displayed by showmount is not the directory name used when mounting. We should mount the directory using the NFS name we entered when creating NFS. The name of the NFS we created was crosscloudnfs.
Mounting NFS directory to the client
After using the df -h command, we can see that in the replication (synchronization) mode, the directory has a space of 40 GB. The reason is that in the replication mode of Cloud Storage Gateway, the local cache space and the OSS space on the cloud are equal. Therefore, the size of the local cache space determines how much space will be used by OSS on the cloud. In the cache mode, the OSS space on the cloud is an extension of the local cache space, so after mounting NFS to the client, the size we see is 256 TB. You need to take this into consideration when selecting the mode and doing business planning.
Directory space in the replication mode
After completing this step, the user server, Cloud Storage Gateway and Alibaba Cloud OSS are connected. Next, we will perform two replication operations to simulate Cloud Storage Gateway’s cloud backup scenario when multiple users share the same storage: one user transfers the db_file directory that was backed up earlier to the cross_cloud_gw directory, and the other user transfers the server_log_new directory.
Transferring two directories to Cloud Storage Gateway
The transfer is in process. You can view the file status of OSS on Alibaba Cloud console. These two directories and the files are uploaded to OSS almost instantly. The reason why the file size of some files is displayed as 0 KB in OSS is that the writing of these files into Cloud Storage Gateway’s cache disk has not been completed. After Cloud Storage Gateway completes the writing, these files will be uploaded to Alibaba Cloud OSS synchronously. In addition to providing the OSS upload API, Cloud Storage Gateway also implements file slicing, verification, error retransmission and other mechanisms to ensure efficient and reliable file upload.
OSS file status during the upload
You can view resource usage in the monitoring page of Cloud Storage Gateway’s management interface. We can also monitor the use of CPU, memory, disk, network and other resources on Alibaba Cloud console. Alibaba Cloud’s Cloud Storage Gateway implements a set of traffic control algorithms associated with front-end and back-end traffic, to ensure the front-end read and write performance when the gateway cache space is sufficient; and to control the front-end write speed when the gateway cache space is highly occupied, so as to ensure that the local cache space of the gateway will not be fully occupied.
Cloud Storage Gateway resource monitoring
During the transfer process, you can also view the space usage of the NFS directory cross_cloud_gw on the webserver host, namely the mount client.
NFS directory space occupation
After all cached data has been uploaded to OSS synchronously, the cache status of the gateway becomes Sync complete.
Sync complete
Next, we will simulate a disaster scenario: when files in a directory of the webserver cloud host are deleted by mistake, or cloud host T’s cloud host is unavailable for a long time, how can we use Cloud Storage Gateway to recover OSS files from the cloud to a cloud host directory or to a new cloud host? Here we will demonstrate the latter case: reversely synchronizing OSS files to the cloud storage gateway deployed internally by Alibaba Cloud for access by ECS on Alibaba Cloud.
Creating a cloud storage gateway on Alibaba Cloud is similar to creating an offline cloud storage gateway, and is omitted here. We directly move to the NFS settings of the cloud storage gateway. Unlike the previous step, here we need to select Yes for Reverse sync to reversely synchronize OSS files from the cloud to the local cache of the gateway for access by front-end users, so as to achieve the goal of disaster recovery.
Creating Cloud Storage Gateway’s NFS directory
After creating a new NFS directory, you can mount it. In reverse sync mode, the cloud storage gateway’s NFS directory automatically synchronizes OSS files to the local cache space. As a result, the files are easily recovered and can be accessed on any host that has mounted the directory. Of course, if there are many large files, the time required for synchronization depends on the bandwidth. When synchronizing files from OSS to Cloud Storage Gateway’s cache, there is also a verification algorithm to ensure file consistency.
Reverse synchronization of Cloud Storage Gateway
If you want to reversely synchronize files in OSS back to cloud vendor T’s cloud storage gateway, which is the seventh step in the architecture diagram at the beginning of this article, you can simply enable Reverse sync in NFS settings at the cloud storage gateway deployed by cloud vendor T. Then data will be automatically synchronized to the gateway’s cache for access by the front-end host.
By using the file backup and reverse sync file recovery functions in the synchronized replication mode of Cloud Storage Gateway, you’ve already experienced the ease-of-use of Cloud Storage Gateway. Cloud Storage Gateway, as a pure software, boasts flexible deployment and simple configuration, and can be seamlessly connected to your existing business system. The “backup immediately after writing” replication mode is perfect for backing up centralized shared directories. Cloud Storage Gateway supports recovering backup files from Alibaba Cloud OSS to any cloud vendor’s host, including Alibaba Cloud’s ECS host or even your own offline data center, which perfectly solves problems with file backup in cross-cloud (multi-cloud) scenarios. You can recover data to any locations that have deployed Cloud Storage Gateway. Distributing recovered data using gateways is extremely efficient.
To learn more about Cloud Storage Gateway, visit www.alibabacloud.com/product/sgw
Reference:https://www.alibabacloud.com/blog/hybrid-cloud-storage%3A-cross-cloud-replication_594132?spm=a2c41.12244989.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
1 
1 clap
1 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@tiwari_nitish/presto-modern-interactive-sql-query-engine-for-enterprise-ce56d7aea931?source=search_post---------50,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
Jun 15, 2018·7 min read
Data analysis and querying techniques offer insight into what users want, how they behave and much more. As growing volumes of data get into enterprise clouds, analysis technology is evolving to help enterprise make sense of this data.
The data analysis field itself is not new; there have been various tools and technologies in use for quite some time now. However as the data size reaches all-time highs, efficiency, responsiveness and ease of use of these tools are more critical than ever.
As enterprises move to object storage based private clouds that integrate with applications directly, it makes sense for the analysis technology to plugin to the object storage as well. Not only does this make the process efficient, it is easy to maintain as well.
In this post, we’ll talk about Presto — a fast, interactive, open source distributed SQL query engine for running analytic queries against data sources of all sizes ranging from GBs to PBs. Presto was developed at Facebook, in 2012 and open-sourced under Apache License V2 in 2013. Starburst, a company formed by the leading contributors to the project, offers enterprise support for Presto. Their engineers recently wrote a blog post about querying object store data lakes with Presto.
Presto can connect to various data sources with its Connectors. We’ll take a special look at the Hive connector, which lets Presto talk to Minio server. With Presto querying data from Minio server in your Private cloud — you have a secure and efficient data storage & processing pipeline.
Use cases
With Presto running queries on Minio server, multiple query users consume the same data from Minio, while applications producing data write to Minio. This leads to effective separation of compute and storage layers and hence flexible scaling options.
Such deployments can be then used for variety of use cases. For example
Why Minio
Minio is a highly scaleable and performant object storage server. It can be deployed on a wide variety of platforms and can plug in directly to any S3 speaking application. With features like SSE-C, Federation, Erasure coding, and others, Minio offers enterprise class object storage solution.
This makes Minio an ideal choice for private cloud storage requirements.
Why Presto
Presto allows querying data where it lives, including Hive, Cassandra, relational databases or even proprietary data stores. Here are some advantages:
Comparison with Hive
Deploy Presto with Minio server
To connect Presto to Minio server, we’ll use the Presto Hive connector. Why Hive connector? Presto uses Hive metadata server for metadata and Hadoop s3a filesystem to fetch actual data from an S3 object store; both of these happen via the Hive connector.
Presto only uses the metadata and data via the Hive connector. It does not use HiveQL or any part of Hive’s execution environment.
To deploy Minio with Presto, we’ll need to setup Hive first. Only then can the Presto Hive connector use the hive metadata server.
Remember to use the actual values of Minio server endpoint and access/secret keys above.
3. Setup Hive —Install stable release of Hive by downloading relevant tarball from here.
We’re using derby here as the metadata database for testing/demonstration purposes.
By default it listens on port 10000.
By default, metadata server listens on port 9083. We’ll use this as the metadata server for Presto.
If you’re doing this for testing purposes and dont have real data on Hive to test, use the Hive2 client beeline to create a table, populate some data and then display contents using the select statement.
4. Setup Presto — Presto installation steps are explained on the documentation page. Follow these steps and create the relevant config files. We’ll now move to the step where we configure Presto Hive connector to talk to the Hive metastore we just started.
Note the metastore URI pointing to the Hive metastore server created in the previous step.
This mean Presto is now up and running. You can also access Presto UI at http://localhost:8080/ui/ via your browser.
Presto UI offers great details about each query being executed as well. Click on a query to see details like resource utilization, timelines, stages and tasks done by workers etc. Here is how the details page looks like
You can now use the Presto CLI to issue queries for data stored on the Minio server. You can also use one of the Presto Clients.
To confirm if the table created in Hive is available via Presto client, start the cli tool and issue a select statement for the same table. You should see the same contents.
In this post, we learned about why and how Presto is taking the centre stage as the tool of choice when querying large datasets from platforms like Minio. We then learned the steps to setup and deploy Presto on private infrastructure.
While you’re at it, help us understand your use case and how we can help you better! Fill out our “Best of Minio” deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
63 
5
63 
63 
5
Minio.io
"
https://medium.com/@openbom/solidworks-openbom-plug-in-early-availability-of-cad-and-pdf-file-sharing-b937560232c2?source=search_post---------306,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Nov 8, 2017·2 min read
For the past few releases we’ve been working on how to connect OpenBOM to cloud storage. You may have noticed our first experiment demonstrating how to upload files to OpenBOM reference properties (Dropbox and Google Drive are the first cloud storages we currently support, more will come). Check out this blog I recently posted for more info, Connecting cloud storage to Bill of Materials in OpenBOM.
Today, I want to share great news about our ongoing experiment with cloud storage. The new version of SOLIDWORKS native OpenBOM plug-in (currently in early availability) is capable of uploading CAD files and automatically generating PDF files to cloud storage connected to OpenBOM. What this means is you’re getting access to these files in a contextual form of reference attributes (links) from a generated OpenBOM Bill of Materials.
Check out the following video for more details and to see it in action:
Conclusion. OpenBOM is cloud enabling desktop CAD systems by allowing the creation of cloud-based Bill of Materials; share it online and collaborate with contractors and suppliers. The OpenBOM flexible data model makes it really easy to create BOMs with flexible structure and the ability to enhance them with any attributes. The new CAD and PDF file upload is a continuing next step in the development of OpenBOM. In many situations, engineers need to share not only a BOM, but also related CAD data. OpenBOM now opens up new opportunities to share files together within a BOM providing a even more context.
Let me know if you like these new features. The new Solidworks plug-in is currently in early availability and can provided to you for evaluation. Please reach out to me at oleg @ openbom.com
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Want to learn more about PLM? Check out my Beyond PLM blog and PLM Book website.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
"
https://medium.com/linode-cube/as-cloud-storage-adoption-grows-scalability-demanded-bebcb5eb35da?source=search_post---------105,"There are currently no responses for this story.
Be the first to respond.
By Jack M. Germain
How are you set for cloud space this year? No doubt, you will need more before you know it.
An independent survey of IT executives found that storage scalability tops their wish list of strategies for managing data storage in 2017. That ranking is twice as much as the next highest demand in the survey.
So, if you were guessing your storage needs will be more along the lines of new storage hardware or more features for other cloud tasks, think again. Storage scalability ranks far higher.
In a first-of-its-kind survey, global market research firm Vanson-Bourne polled IT execs in the US, Germany and the UK. The results corroborate recent research on cloud deployments as a whole. Those studies show that scalability is the primary reason for moving to the cloud in the first place.
The new data, commissioned by Zadara Storage, underscore the importance of storage scalability as a key strategy for meeting business growth demands this year. Many of the survey participants indicated that storage scalability remains an unmet demand.
The survey results reflect the rankings offered by 400 IT leaders. The IT professionals totaled 100 in Germany, 100 in the UK, and 200 in the US. Vanson Bourne’s researchers conducted the survey in late November 2016.
The survey’s bottom line: the IT industry’s need for symmetrically scalable cloud storage is unquestionably the most critically pressing. The storage must instantly scale up or down to meet business requirements.
Clearly, that is a key challenge to growth in 2017. Other research confirms that assessment. Plus, the analyses of IT demands for storage scalability will not disappear in the coming year.
Here is a quick summary of the 2017 wish list results:
Organizational size had little to no impact on IT ranking of 2017 wish list items, according to the survey. The tallied responses came from IT leaders in organizations with between 1,000 and 3,000 employees, and from enterprises with over 3,000 employees.
The results followed the same ranking as the aggregate, and their percentage wish list items varied by only a percentage point or two. The survey asked respondents to share their top strategy for managing their organization’s data storage in 2017. Scalability was cited by nearly twice as many as those who selected the next highest response, outranking other mission-critical IT demands.
Geographic location did not seem to skew the survey results either. One-third of IT decision makers responding in Germany, the UK and the US, ranked “having cloud storage that scales up or down according to my organization’s needs” as their top wish in 2017.
IT leaders around the world were remarkably united in their sentiment on their desire for scalability. Their responses displayed only minor differences.
For example, 15 percent of UK IT leaders ranked strong SLAs as their main wish. That wish tied for second place with deploying software-as-a-service features, so as to make their architectures more manageable.
The second most popular wish for IT decision makers in both the UK and US this year was getting stronger service level agreements from a cloud storage vendor. However, German respondents ranked getting important storage features in the cloud that match on-premise features as second.
In this case, twice as many — 20 percent — of IT leaders in the US wished for stronger SLAs. Just 11 percent of their counterparts in Germany put stronger SLAs on the wish list.
Another slight difference involved ranking services to complete mundane storage tasks markedly higher. Unlike their peers in the UK or US, German IT leaders ranked that need at 14 percent of total German responses. That compared to 9 percent in the UK and 6 percent in the US.
This demand for freedom from the mundane tasks was tied with deploying storage-as-a-service so that IT management would be easier. Some 15 percent of the German respondents also were interested in obtaining important on-premise features in the cloud. They ranked this wish as the second most important item for 2017.
Lower wish list items ranked these three less popular needs:
IT decision makers are ever more frequently relying on the public cloud. However, concerns around the integrity of data and level of service from cloud vendors have yet to be addressed, according to the report writers.
Based on the survey results, regardless of whether cloud users rely on the public or private cloud, IT decision makers need a flexible, scalable and reliable data-storage solution first and foremost. That storage solution must work to handle the organization’s short-term needs. It also must be able to rapidly adapt over time.
Other industry research conducted in the interim corroborate those assessments. They confirm that scalability is the main reason for moving to the cloud in the first place, as reflected by 51 percent of the respondents. Other migration reasons reflect business agility (46 percent) and cost (43 percent).
Scalability requires an elastic, agile storage architecture. That structure must provide IT leaders with the flexibility to define the storage by their business needs rather than the other way around, according to Zadara Storage.
That is far more readily achieved with a software-defined storage platform, noted Zadora. A traditional legacy solution no longer is a smart way to go with storage. You seek a better solution than always purchasing more than you need, and then having to manage it. Upgrading it and then replacing it every three to five years is an inefficient and cost-wasting strategy.
Scalability also requires both technical and business model changes, Zadara recommends. The architecture has to support a variety of enterprise-grade capabilities. Think in terms of any storage type, such as file, block or object.
Also consider any storage location, such as public or private cloud, as well as private, hybrid or multi-cloud. You need to consider any storage protocol. That includes NFS and CIFS, which are hard to find in the cloud, as well as FibreChannel, iSCSI, S3 and Swift.
Another issue to consider when dealing with scalability is the capacity to change on-the-fly swap out media. For example, you can change SSDs for HDDs, high-RPM SAS, SATA or 6TB 7200 RPM repository drives, or swap out the type of storage or location at will, without days or even hours of lead time.
More importantly, the storage provider cannot lock in customers with bloated, up-front payments and long-term commitments. Instead, they have to enable scaling down and scaling up. They have to charge on a pay-as-you-consume basis, after the storage is consumed and not before.
Scalability, flexibility, reliability and hourly billing are the linchpins of Linode’s forthcoming Block Storage plan, being readied for beta release this summer.
Take the reality Sam Coyl, CEO of Netrepid, faced. His company recently decided to scrap a Dell EqualLogic system in favor of another storage scalability solution. A large part of that move resulted from traditional storage being too hard to scale up/down and manage.
“With traditional storage, we were confined to the storage we had purchased. To expand our capacity, we needed to invest a lot of capital, and there was significant delay while waiting for additional storage…to arrive and be installed. This made it hard to be as responsive to our clients, both internal and external, as the market demands,” he said.
The agility and client responsiveness that resulted was a dramatic improvement. His company can now spin-up storage nodes in minutes rather than waiting a week for new storage hardware to arrive.
“We turned a technical challenge into a tremendous business advantage,” said Coyl, acknowledging the intrinsic advantage of using cloud storage.
Please feel free to share below any comments, questions or insights about your experience with cloud data storage solutions, including scalability, flexibility, and reliability and hourly billing. And if you found this blog useful, consider sharing it through social media.
About the blogger: Jack M. Germain is a veteran IT journalist whose outstanding IT work can be found regularly in ECT New Network’s LinuxInsider, and other outlets like TechNewsDirectory. Jack’s reporting has spanned four decades and his breadth of It experience is unmatched. And while his views and reports are solely his and don’t necessarily reflect those of Linode, we are grateful for his contributions. He can be followed on Google+.
We’re covering everything from tech news and industry…
4 
4 claps
4 
Written by
Cloud Hosting for You. Sign up today and take control of your own server! Contact us via ticket or email for all support inquiries: https://www.linode.com/contact
We’re covering everything from tech news and industry happenings to event recaps and general tips.
Written by
Cloud Hosting for You. Sign up today and take control of your own server! Contact us via ticket or email for all support inquiries: https://www.linode.com/contact
We’re covering everything from tech news and industry happenings to event recaps and general tips.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.upthere.com/respecting-your-content-at-its-fullest-form-1c9e8d965f8f?source=search_post---------391,
https://medium.com/apple-etc/why-apple-doesn-t-offer-unlimited-cloud-storage-d5431708ebf0?source=search_post---------112,"There are currently no responses for this story.
Be the first to respond.
I didn’t see any of Google I/O, but during the keynote they announced a new product called Google Photos. There are many things I could talk about related to this, but I wanted to focus on cloud storage.
Google is offering unlimited storage as part of this new product offering.
That’s right. Without having to pay a penny, you can keep all your photos in Google’s cloud servers.
Too good to be true?
Perhaps. The small print is that the offering is only unlimited if you accept Google keep ‘high quality’ rather than original, uncompressed versions. If you do want to upload your originals, you get 15GB free and then need to pay either $1.99 per month for 100GB or $9.99 for 1TB.
Either way, there’s no doubt that Google Photos looks like a great product with good storage options.
One theme that has emerged from this new Google offering though, is how much more generous it is than Apple who offer a mere 5GB for free and charge $3.99 for 200GB and $19.99 for 1TB.
Though I’ve written before about Apple’s crappy storage limits, and there’s no doubting that 5GB free storage is much too stingy, I don’t think it’s a case of simply comparing like for like when looking at what Google (and indeed Amazon, Dropbox, and others) offer in comparison.
Because Apple’s storage is pretty much baked into the operating system, the number of Apple users who will make the most of Apple’s iCloud offerings is going to be high. Half a billion people would very easily start to quickly make the most of any amazing cloud storage option from Apple.
Google and Amazon can make their grand announcements, but how many users will actually start utilising their unlimited offerings? I don’t see it being nearly as high.
If Apple was to suddenly offer unlimited storage, there would be a huge rush on their servers unlike anything Google or Amazon would ever likely experience.
It is because of this that I suspect Apple is more cautious and conservative in their storage options.
That said, 5GB is not enough. It strikes me that something like 25GB of free storage for all iCloud users would be much more reasonable.
It’ll be interesting to see if Apple announce any changes in both storage options and pricing at WWDC next week.
A collection of posts by Sam Radford about Apple and…
A collection of posts by Sam Radford about Apple and related companies
Written by
Husband, father, writer, Apple geek, sports fan, pragmatic idealist. I write in order to understand.
A collection of posts by Sam Radford about Apple and related companies
"
https://medium.com/@modern-cdo/spicing-up-data-lifecycle-management-with-hybrid-cloud-storage-recipes-23bb511287c4?source=search_post---------185,"Sign in
There are currently no responses for this story.
Be the first to respond.
Sandeep Uttamchandani
Aug 4, 2015·4 min read
As enterprises grapple with exponential data growth, they are looking for new models to efficiently store, analyze, and archive data. Software-defined storage (SDS), as well as the hybrid cloud model, is shifting wasteful “just-in-case” provisioning into seamless “just-in-time” provisioning and storage management for optimal total cost of ownership (TCO). The hybrid cloud model allows enterprises to leverage both their existing data centers’ deployments as well as the cloud in a seamless fashion, leveraging well-established workflows and management tools. The focus of this blog post is to discuss emerging intersection points between traditional data lifecycle management and hybrid cloud storage.
Let’s level-set on the lowest common denominator for this discussion — storage hardware trends, and their implications on data management. A decade ago, storage tiers were mainly rotating disks with different speeds, and sequential media such as tapes. The industry’s research focus was mainly on closing the latency gap between main memory with nanoseconds service times and disks operating at milliseconds. Today, storage tiers have spread across the entire latency spectrum: Non Volatile Memory (NVM) with expected service times of hundreds of nanoseconds, to NAND Flash operating at an order of microseconds, to innovations in high density disk technologies such as Shingled Disks with higher access times. An interesting observation is the emergence of distinct $/IOP and $/GB tiers — flash-based storage excels with regards to $/IOP, while disk-based technologies are optimal with regards to $/GB. So, depending on the IOPS/GB of the data, and the application’s latency constraints, the data needs to be placed on the appropriate tier to best utilize the storage resources.
Data lifecycle management deals with data placement and management from the initial creation of data to its eventual deletion. The key point is that the quality of service (QoS) requirements for data are time-varying i.e., at the time of initial creation, the latency, throughput, durability and recovery point objective (RPO)/recovery time objective (RTO) availability, may be different from data that has aged and is being retained mainly for batch processing and archiving/compliance. By combining data lifecycle management with the differentiated properties and pricing of storage tiers across on-premises and cloud resources, enterprises can exploit storage hardware innovations to derive the lowest total cost of ownership (TCO) specific for their deployment and usage models.
To consume cloud resources, enterprises are increasingly adopting the hybrid cloud model that allows consuming cloud resources as a natural extension of their on-premises data-center deployment. Specifically in the context of storage resources, the cloud offers a variety of protocols (block, object, etc.), and tiers with varying $/GB/month pricing. Also, it offers a growing number of scale-out services for structured and semi-structured solutions, blurring the lines between traditional storage and data management solutions. It is important to realize that cloud storage is more than just storage resources — it has virtually unlimited compute resources that can be collocated with storage in the cloud. Also, given the economies of scale and competitive cloud wars, enterprise customers benefit from ever-decreasing prices of cloud resources, also referred to as “the race to zero.”
Customers commonly think of hybrid cloud storage as a tier of storage that is used mainly for infrequently accessed data copies or archive data. While those have been the founding use cases, there are a growing number of intersection points between data lifecycle management and hybrid cloud storage means. The following list of recipes is not meant to be comprehensive, but more to get you thinking of the possibilities!
In summary, disruptions in storage technologies are redefining not just the emerging storage architectures, but also influencing traditional workflows/IT processes within the data-centers. The model of a hybrid cloud comes naturally to storage use cases, motivated by exponential data growth and limited IT budgets. While hybrid cloud storage emerged mainly for data archive and availability use cases, it is increasingly playing an important role in traditional Data Lifecycle Management. Hybrid Cloud provides not just another storage tier for on-premise applications, but a combination of on-demand compute capabilities, with differentiated tiers for cost, performance, durability, and a globally accessible (object) storage namespace.
Originally published at 34.208.194.110 on August 4, 2015.
Democratize Data+AI — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author. AIForEveryone.org
Democratize Data+AI — real-world battle scars to help w/ your journey. Product builder(Engg VP) & Data/ML leader (CDO). O’Reilly Author. AIForEveryone.org
"
https://medium.com/@megbarclay/i-avoid-online-banking-because-of-the-incredible-hastle-it-would-be-to-have-my-identity-stolen-daff64c28bf5?source=search_post---------277,"Sign in
There are currently no responses for this story.
Be the first to respond.
Meg
Nov 20, 2016·1 min read
Randomly Me
I avoid online banking because of the incredible hastle it would be to have my identity stolen. I figure all the money spent on those postal stamps is a cheap insurance policy. Also, maybe I am old enough to find bookkeeping easier if I have a backup paper trail. I realize I will have to cave some day, but I’m in avoidance right now.
Re cloud storage, I use it for neutral stuff like work, but not for financial or personal stuff. Plus, it’s not as reliable as people assume, so you need hard storage anyway for important items. After a photographer friend had a bunch of his work disappear from the iCloud, I am more careful than ever with my own storage.
Writing, because talk is cheap
4 
4 
4 
Writing, because talk is cheap
"
https://medium.com/coinversation-protocol/coinversation-partners-with-stratos-for-decentralized-cloud-storage-1eab5b6983e0?source=search_post---------107,"There are currently no responses for this story.
Be the first to respond.
We are delighted to announce that Coinversation Protocol is to partner with Stratos. Coinversation will deploy and run web pages on Stratos and will store user data using Stratos decentralized storage.
Through this cooperation, Stratos will provide Coinversation Protocol with secure and scalable decentralized storage services to store important user and transaction data, ensuring the ownership of these encrypted data belong to the customer themselves. Coinversation Dapp will also be deployed on Stratos Decentralized Data Mesh to assure the running codes are stored in a secure yet decentralized environment.
Why Stratos
Decentralized cloud storage is very important for Defi, especially synthetic assets.The transaction of synthetic assets needs to record a large amount of user data and transaction data. These massive amounts of data, especially user data, need to be stored efficiently and securely. However, traditional decentralized cloud storage often faces problems with unstable security as well as high cost. With Conversation Protocol, cloud storage becomes more efficient, secure, and decentralized..
Stratos’s decentralized data mesh enables scalable, reliable, self-balanced next-generation data storage, making retrieving data from the blockchain simple, efficient and flexible regardless of the data size.
“We are excited that Coinversation will deploy and store data on Stratos Decentralized storage in the future. The performance and features of Stratos’ decentralized storage will be fundamental for the evolution of synthetic asset protocol.” Lin Chen, Co-Founder of Coinversation.
“We hope that through the combined effort of Stratos and Coinversation Protocol, sensitive user and transaction data of DeFi, especially synthetic assets, can be stored securely and permanently through Stratos decentralized storage.” Bin Zhu, Founder of Stratos.
Coinversation Protocol is the first synthetic asset issuance protocol and decentralised contract trading exchange based on the Polkadot contract chain. It uses the token CTO issued by Coinversation Protocol and Polkadot(DOT) as collateral, and synthesizes any cryptocurrencies or stocks, bonds, gold and any other off-chain assets through smart contracts and oracles. The assets minted by all the users correspond to the liabilities of the entire system, and the proportion of each user’s liabilities has been determined at the time of forging, so that their respective profits can be calculated. Because such a collateral pool model does not require a counterparty, it perfectly solves the problems of liquidity and transaction depth in decentralised exchange(DEX).
Website｜Telegram | Twitter | Github
Stratos is the next generation of decentralized Data Mesh that provides a scalable, reliable, self-balanced storage, database, and computation network. Stratos is born for scaling blockchain process capacity while retaining the decentralized benefits of a distributed protocol including trustlessness, traceability, verifiability, privacy, and other competitive strengths.
Stratos is best positioned to support data storage and adoption for developers and users in this ever-expanding digital economy. Stratos strives to make decentralized data adoption easier for the blockchain industry and Web 3.0.
Stay tuned for more info and follow us at:
Twitter | Telegram Group |Telegram Announcement Channel| Medium | Discord | LinkedIn
Make Asset accessible
80 
80 claps
80 
Coinversation Protocol is a synthetic asset issuance protocol and decentralised contract trading exchange based on the Polkadot contract chain.
Written by
Coinversation: Decentralized Synthetic Asset Issuance Platform on Polkadot
Coinversation Protocol is a synthetic asset issuance protocol and decentralised contract trading exchange based on the Polkadot contract chain.
"
https://medium.com/@HOSTINGdotcom/3-questions-to-ask-when-selecting-a-cloud-storage-provider-9f305f0eef9a?source=search_post---------134,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Dec 30, 2015·3 min read
The cloud is a blessing to small businesses looking to cut costs. On-site servers cost thousands of dollars upfront and even more over time, with maintenance and electricity expenses. Connecting to another server through the Cloud allows you to trade in those expenses for a more affordable access fee.
For small business data storage needs, it is clear that the Cloud is the way to go. The only question is, which of the many cloud storage service providers is best for your small business needs?
Not all Cloud providers are equal. While HOSTING, Dropbox, Box, Amazon Cloud Drive, GoogleDrive, Microsoft OneDrive, and iCloud, to name a few, enable basically the same capabilities, there are distinct differences that likely make one a better fit than the others.
So, how do you select the best possible service for your small business? Here are three questions to ask when identifying the cloud storage service that is right for your business.
Before committing to a cloud storage provider, it is necessary to prioritize the tasks that cloud storage could make more efficient and less costly for your business.
Many small businesses initially embrace cloud storage due to its file backup capabilities. Prioritizing file backup makes reliability an important feature for small businesses to consider.
Perusing comprehensive client reviews and ratings of various cloud storage providers will help small businesses home in on the traits and features that make a specific service more reliable than the other.
Other characteristics to consider include specialized capabilities, such as allowing users to display files for uninstalled, and customer satisfaction ratings.
For the 61 percent of small businesses functioning in an industry that requires compliance such as HIPAA for healthcare and PCI for companies that accept credit cards, security is a top priority.
Violating compliance standards can lead to costly consequences. For example, if a hospital’s security is breached and protected health information (PHI) is compromised, it could face up to $1.5 million per violation in HIPAA fines, in addition to potential legal fees from a class action lawsuit.
Last year, New York-Presbyterian Hospital/Columbia University Medical Center faced a record-breaking $4.8 million HIPAA fine, and a couple months ago, Cancer Care Group, one of the country’s biggest radiation oncology practices, paid a $750,000 HIPAA fine after widespread non-compliance was discovered.
Avoiding these regulatory nightmares requires a cloud solution that can keep your data secure.
While many small businesses may be able to use a free plan for cloud storage at first, if growth is a goal, it is important to understand the prices associated with expanding your data needs.
Small businesses can evaluate a cloud storage provider based on the amount of free storage space it offers. On the other hand, if business slows, it does not make sense to pay for unused storage space. In this scenario, the ease with which the cloud storage provider allows a user to adjust its data price plan is important to consider when selecting a service.
Careful consideration of these three factors — prioritization of tasks, need for compliance to a standard, and cost of data — will facilitate the selection of a cloud storage service that is the right for your business needs.
About the author
Sarah is an analyst at Clutch. As a member of the marketing team, she conducts research that aims to help businesses and consumers select and use IT services and software. Clutch is a Washington, DC-based research, reviews, and ratings platform for B2B. It identifies leading software and professional services firms that deliver results for their clients.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
1 
1 
1 
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://thinkdiff.net/how-to-upload-files-from-firebase-cloud-functions-to-firebase-cloud-storage-9d8b1a0f65e5?source=search_post---------52,"Recently I have been working on a module in our app, where I need to write a JavaScript function that will run in our Firebase Cloud…
"
https://blog.bitsrc.io/using-airtable-as-a-handy-cloud-storage-for-your-apps-ae6cce0efe11?source=search_post---------0,"Airtable is an online service that you can use as a very convenient backend storage option.
Why is it convenient? Because:
The Airtable company provides the basic service for free, with some limitations you need to be aware of.
Tip: Use Bit to reuse components between apps. It helps your team organize and share components, so you can build new apps faster. Give it a try.
bit.dev
The API has a limit of 5 requests per second, which is not high, but still reasonable to work with for most scenarios.
Using the Airtable Node.js library which I’ll introduce later on, you don’t have to worry about handling those rate limits yourself, since it comes with built-in logic to handle them, plus an automatic repeat of any rejected request.
The free plan has a limit of 1200 records per each base, which means across all the base tables. Paid plans allow more entries.
One of the things you’ll likely appreciate the most as a developer is a good API documentation.
AirTable has a very useful documentation and one of the those I like to point out when it comes to having a great example of docs for developers.
The examples are many and the parameters you can pass are all well documented.
They provide a set of URL and Node.js examples, and all of those are based on the actual data you have in your tables.
Also, any API call parameter set already comes with your API keys and tables and view names pre-filled. It’s great because you don’t have to look at a random example and switch context, and you can work with your own data.
When working with Airtable you’ll find some terms familiar if you used databases before, and some unique to Airtable.
A base is a database, a set of tables related to your application.
A table is one set of data, organized in columns
A view is a particular representation of a table. You can have the same data presented in different views.
You always reference one view, one table, and one base to fetch your data.
Tip for SQL users: a view is not what you are used to; every table has at least one view.
Once you create an Airtable account you can see the list of sample bases. You can create a new base by selecting a workspace, and clicking “Add a base”:
You’ll be presented with a list of choices.
You can start with a template, and Airtable provides a lot of different templates to choose from:
You can also import an existing spreadsheet, or start from scratch, which is what we’ll do now.
Choose a color and an icon:
And clicking the base icon will open the table view. Here you have the visualization of your “My base” base, showing the “Table 1” table, with the “Grid view” view:
Edit the table fields
From here, clicking the down arrow at the right of each column gives you many choices. Use “Rename field” and “Customize field type” to create the table structure you want.
Every field has a type. You can choose from many predefined fields, which should satisfy most of your needs:
Enough with the description of Airtable as a product. Let’s see how to work with its API.
For the sake of following the examples, let’s keep the default column names and types.
Airtable provides an official Node.js library, available on npm as airtable. Install it in your app using
Once the library is installed, you can use it in a Node.js app
where API_KEY is an environment variable you configure before running the program.
As you work with your API, you are likely to use other variables that set the name of a base, the name of a table, and the name of a view:
Use those to initialize a base, and to fetch a table reference:
The VIEW_NAME variable will be used when selecting elements from a table, using the table.select() method.
Let’s now see how to perform some very common data operations.
You can add a new record by calling the create() method on the table object. You pass an object with the field names (you don’t need to pass all the fields, and the ones missing will be kept empty).
In this case, if there is an error we just log it, and then we output the id of the row just entered.
You can get a specific record by using the find() method on the table object, passing a record id:
From the record object returned by find() we can get the record content:
You can get the record id using record.getId() and you can get the time the record was created using record.createdTime.
Update one or more fields of a record, using the update method of the table object:
In this case, fields not mentioned are not touched: they are left with whatever value they were before.
To clean the unlisted fields, use the replace() method:
To delete a record from the table, call the destroy() method of the table object:
So far we’ve seen how to operate with a single record object. How can you iterate upon all the records in a table? Using the select method of the table object:
notice we used the firstPage() method. This gives us access to the first page of records, which by default shows the first 100 items.
If you do have more than 100 items in your table, to get access to the other records you have to use the pagination functionality. Instead of calling firstPage() you call eachPage(), which accepts 2 functions as arguments.
The first, processPage(), is a function that is called upon every page of records. In there, we simply add the page records (available through the first parameter, filled automatically by the Airtable API).
We add those partial set to the allRecords array, and we call the fetchNextPage()function to continue fetching the other records.
The processAllRecords() function is called when all the records have been successfully fetched. There is one single parameter here, which contains an error object if there is an error, otherwise, we are safe to use the allRecords array values.
Airtable is a useful service to use for building prototypes and to build small applications that need a fair amount of data.
It’s great to be able to see the data and edit it through a well-thought user interface. It’s especially nice if you have non-technical people that need to interact with that data.
The free service allows up to 1200 records per base, with paid plans you can get up to 5000 if you have the Plus plan, and 50.000 records with the Pro plan, and more using the enterprise plan.
Feel free to comment below, suggest idea or ask anything! thanks 😃
blog.bitsrc.io
blog.bitsrc.io
blog.bitsrc.io
The blog for advanced web development. Brought to you by the Bit community.
1.5K 
4
1.5K claps
1.5K 
4
Written by
I write tutorials for developers at https://flaviocopes.com
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
Written by
I write tutorials for developers at https://flaviocopes.com
The blog for advanced web and frontend development articles, tutorials, and news. Love JavaScript? Follow to get the best stories.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-storage-exploder-221c5b4d219c?source=search_post---------31,"There are currently no responses for this story.
Be the first to respond.
A colleague asked whether Cloud Functions could provide unzip functionality to Cloud Storage. Another colleague pointed me to a solution — similar in principle — for Firebase Functions that provides image transformation; he also suggested “exploder” as a more evocative title ;-)
My interest was piqued and I developed the following as a proof-of-concept (it works) but I urge you to not use this in production.
It’s an excellent and quite common request from customers that Google Cloud Storage (GCS) should include common file processing tools. The goal would be that these transformations happen service-side to save the complexity of downloading an object from GCS, processing it and returning the result(s) to GCS. For those that are not familiar with GCS, it’s a service in Google Cloud Platform (GCP) that provides effectively limitless “object” aka “BLOB” aka “unstructured” file storage.
You may get started for free with Google Cloud Platform but I suspect, if you’re reading this, you’re likely already a beloved GCP customer and may be seeking a template solution for this problem. Read on, dear customer.
You’ll need a GCP project, at least 2 GCS buckets, and Cloud Functions enabled:
Customarily, I’m all command-line but I find the Console experience for Cloud Functions to be mostly excellent and I find myself preferring it. Let’s do this both ways, starting with the Console:
Click “Create Function” and
For convenience, I’m going to use the inline editor.
Replace index.js with:
NB Replace [[YOUR-ROOT]] in lines 13+14 with the value of ${ROOT}. Or whatever names you used to create the buckets.
and replace package.json with:
Click “Create” to deploy the Cloud Function to GCP.
All being well:
Our Cloud Function should be triggered on *all* object changes to our GCS “receive” bucket. You may read more about so-called Object Change Notification (OCN) here. In practice, our Function should be more prudent in filtering the OCNs for those that are intended for our exploder.
I recommend you file a reasonably small zip file (${TESTZIP}) to test.
and, to confirm you can run the following command although the cp command should provide sufficient confirmation of success or failure.
Browsing GCS buckets and objects is facilitated with the Console’s Browser:
which should show something similar to:
If you drill into the “receive” bucket, you should see your uploaded ${TESTZIP} file:
And, if you wait a few seconds and navigate to the “explode” bucket, you should see:
GCS stores objects in a flat namespace (one per bucket). As a convenience, GCS presents objects that contain / in their object name as if these formed a conventional directory hierarchy. In the code, you will see that I prefixed unzipped files with a Linux epoch value (in this case 1510952315489) and a /. The Browser presents the bucket’s contents as if there were a directory called 1510952315489 (your value will differ but it will be unique and corresponds to the epoch value) but the implementation is that all these image files image-1.jpg, image-2.jpg are actually named: 1510952315489/image-1.jpg, 1510952315489/image-2.jpg…
That worked…. my test.zip in the “receive” bucket is now exploded in the “explode” bucket.
And, you should see our console.log output in the Cloud Logging logs:
and you can enumerate the bucket using the gsutil CLI:
Cloud Functions is fully supported by the Cloud SDK (“gcloud”). Assuming you have index.js and package.json in the current directory. You do *not* need to npm install the packages. Just the two files:
Hopefully this will trigger some ideas around other uses of Cloud Functions. Once again, this is a proof-of-concept only and needs work before it would be usable for anything more.
You can delete Cloud Functions individually:
You may delete buckets after recursively delete all their objects — please be VERY careful using this command:
Alternatively you can simply delete the project which will delete everything within it too:
Thanks!
Google Cloud community articles and blogs
111 
7
No rights reserved
 by the author.
111 claps
111 
7
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/firebasethailand/%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B1%E0%B8%81-firebase-storage-%E0%B8%95%E0%B8%B1%E0%B9%89%E0%B8%87%E0%B9%81%E0%B8%95%E0%B9%88-zero-%E0%B8%88%E0%B8%99%E0%B9%80%E0%B8%9B%E0%B9%87%E0%B8%99-hero-e65db2d1873f?source=search_post---------14,"There are currently no responses for this story.
Be the first to respond.
Cloud Storage for Firebase บริการที่ให้คุณสามารถ Upload และ Download ไฟล์ได้อย่างมีประสิทธิภาพและปลอดภัยบน Google Cloud Storage พร้อมรองรับการขยายขนาดอัตโนมัติระดับ petabyte ที่ให้คุณสามารถอัพโหลด รูปภาพ, ไฟล์เสียง, วิดีโอ และไฟล์อื่นๆได้ ซึ่งรองรับทั้ง Android, iOS และ Web
ในการพัฒนา Cloud Storage for Firebase ขอแยกออกเป็น 5 parts ดังนี้
เมื่อพร้อมแล้ว…ก็เปิด Android Studio ขึ้นมา โดยจะสร้างโปรเจคใหม่ หรือจะใช้โปรเจคเดิมก็ได้
ถ้าสร้างโปรเจคใหม่ ให้ไปดูการ Set up Firebase ที่บทความนี้ก่อน
medium.com
เมื่อ Set up Firebase เรียบร้อยแล้ว ก็ให้เพิ่ม Storage SDK ใน build.gradle ของ app-level แล้วกด Sync ก็เป็นอันจบส่วนที่ 1 ละ
การเข้าถึงไฟล์ใน Cloud Storage for Firebase ทั้ง read และ write โดยปกติ เราจะต้องทำการ Authentication ผ่าน Firebase Authentication ซะก่อน แต่เพื่อให้เราสามารถเข้าใจบทความนี้ได้โดยไม่ต้องอ่าน Firebase Authentication เราจะมาทำให้มันเข้าถึงได้แบบ public กัน โดยให้เข้าไปที่ Firebase Console เข้าไปที่โปรเจค จากนั้นเลือกเมนู Storage แล้วเลือก tab ที่ชื่อว่า RULES จะพบหน้าตาของประมาณนี้
จากนั้นก็ปรับ code ให้เข้าถึงไฟล์ได้แบบ public กันตามนี้
ไฟล์ทั้งหมดที่เราอัพโหลดไว้จะถูกเก็บไว้ใน Bucket ซึ่งโครงสร้างก็เหมือนกับการจัดเก็บไฟล์ใน hard disk โดยการอ้างอิงนั้นเราสามารถอ้างอิงได้ทั้ง โฟลเดอร์และไฟล์ เพื่อใช้ในการ upload, download, delete และการจัดการ metadata
เริ่มด้วยการประกาศตัวแปร StorageReference รับค่า Instance และอ้างถึง bucket
เราสามารถอ้างอิงถึง โฟลเดอร์ และ ไฟล์ ในลำดับต่างๆลงมาได้ด้วย child()
นอกจาก child() ก็ยังมี getParent() ที่ใช้อ้างอิงลำดับที่อยู่เหนือตัวที่อ้างอิง และ getRoot() ที่ใช้อ้างอิงถึง bucket
และหากเราต้องการดูค่าต่างๆกับตัวแปรที่เราอ้างอิงอยู่ก็สามารถ getPath(), getName(), และ getBucket() เพื่อดึงค่า String มาแสดง
ในการ upload ไฟล์ เราจะต้องอ้างอิงไปยัง path และระบุชื่อของไฟล์ซะก่อน ตัวอย่าง จะ upload ไฟล์ชื่อ firebase.png ไปที่โฟลเดอร์ photos
สำหรับการ upload จะมีด้วยกัน 3 ประเภท
วิธีการนี้จะต้องทำการแปลงไฟล์ให้เป็น byte และนำไปเก็บไว้ใน memory ก่อน จากนั้นจึงค่อยส่ง byte ที่แปลงมาให้กับ Cloud Storage for Firebase
จากตัวอย่าง เราจะเอารูปภาพจาก ImageView มาแปลงเป็น byte() แล้วใช้คำสั่ง putBytes() ในการ upload นอกจากนี้เรายังสามารถเช็คผลลัพธ์ได้จากการ เพิ่ม addOnSuccessListener และ addOnFailureListener เพื่อที่จะได้ callback และ handle ต่อไป
วิธีนี้คือการเอาไฟล์มาแปลงให้อยู่ในรูปแบบของ stream แล้ว upload ไปยัง Cloud Storage for Firebase
จากตัวอย่างผมดึง path ที่ได้จากการเลือกรูปใน gallery มาแปลงให้เป็น stream ด้วยคำสั่ง FileInputStream แล้วใช้คำสั่ง putStream() ในการ upload
ดึงรูปจาก gallery อย่าลืมประกาศ permission กันด้วยหละ ผมเองขอ WRITE_EXTERNAL_STORAGE ไปเลย เพราะจะได้ทั้ง READ และ WRITE เพื่อเอาไว้ใช้ตอน download ต่อไป <uses-permission android:name=”android.permission.WRITE_EXTERNAL_STORAGE”/>
วิธีนี้เป็นการเลือกไฟล์ที่อยู่ในเครื่องมา upload ไปยัง Cloud Storage for Firebase
จากตัวอย่างผมดึง path ที่ได้จากการเลือกรูปใน gallery มาแปลงให้อยู่ในรูปแบบของ File และเอามาทำเป็น Uri แล้วก็ใช้คำสั่ง putFile() ในการอัพโหลด ซึ่งหากสำเร็จก็จะนำ URL จาก Cloud Storage for Firebase มาแสดง ดังรูป
และเราสามารถเข้าไปดูข้อมูลไฟล์ทั้งหมดใน Firebase Console ได้ด้วย โดยไปที่เมนู Storage จากนั้นเลือก tab แรกชื่อ Files ก็จะเห็นโฟลเดอร์ photos และไฟล์ firebase.png ภายในโฟลเดอร์
เรายังสามารถเพิ่ม Metadata ไปกับไฟล์ได้ด้วย ซึ่งเราจะสามารถอัพเดท หรือดึงข้อมูลนี้มาได้ภายหลัง
เมื่อคลิกที่ไฟล์ firebase.png ใน Firebase Console ก็จะพบ metadata ด้วย ซึ่งปกติจะไม่มีโผล่มา
ในการอัพโหลดไม่ว่าจะเป็นการ putBytes(), putStream() หรือ putFile() มันจะ return ค่าที่อยู่ในรูปของ UploadTask กลับมา เพื่อให้เอาเอาไว้ใช้กับ Listener ในการดูสถานะความเปลี่ยนแปลงต่างๆ ซึ่งตัว UploadTask ยังสามารถเอามาจัดการการ upload ได้อีกดังนี้
และนอกจาก addOnSuccessListener และ addOnFailureListener ที่เป็น Listener ให้เราเอาไว้ดูผลงานแล้วก็ยังมี addOnProgressListener และ addOnPausedListener เพื่อไว้คอยเฝ้าดูสถานะการ upload อีก โดยเฉพาะ addOnProgressListener ผมชอบมากเพราะเอามาใช้กับ ProgressDialog เพื่อดู % การ upload ได้ละ และเพื่อความเข้าใจที่ชัดเจนมากขึ้นเราจะ implement แบบครบเลย
จากตัวอย่างที่ทำให้ไปหากมีการกด pause ที่ ProgressDialog จะมีปุ่ม resume โผล่ขึ้นมาให้กด upload ต่อไปได้ด้วยนะเออ
ในการ download ไฟล์ เราจะต้องอ้างอิงไปยัง path และระบุชื่อของไฟล์ซะก่อน ตัวอย่าง จะ download ไฟล์ชื่อ kaizen.png จากโฟลเดอร์ photos
สำหรับการ download ไฟล์จะมีด้วยกัน 3 ประเภท
วิธีนี้จะ download ไฟล์มาจาก Cloud Storage for Firebase แล้วแปลงเป็น byte เก็บลง memory เครื่อง วิธีนี้จะต้องระวังว่าถ้าโหลดไฟล์ใหญ่มาแล้วแปลงไปเก็บในหน่วยความจำที่ไม่เพียงพอในขณะนั้น จะทำให้แอพ crash ได้ ซึ่งเราสามารถจำกัดขนาดไฟล์ที่เราจะโหลดมาได้เพื่อหลีกเลี่ยงการ Crash (หรือไปใช้ method อื่นเหอะ)
วิธีนี้จะทำให้เราสามารถเข้าถึงไฟล์เมื่อเรา offline ได้ และสามารถส่งไฟล์ต่อไปให้แอพอื่นได้อีก และวิธีนี้คือสาเหตุที่ขอ permission WRITE_EXTERNAL_STORAGE จากข้อ 3.2 นั่งเอง
วิธีการ download แบบนี้จะสามารถเพิ่ม Listener ในการดู progress ได้ด้วย เราก็เลยทำ ProgressDialog ซะเลย
วิธีนี้เป็นการเอาไฟล์ที่เราอ้างอิงไปดึง URL ออกมาแล้วจึงไป download หากสำเร็จจะ return ข้อมูล URL แบบเต็มของ Cloud Storage for Firebase กลับมา ในรูปแบบของ Uri
ในการลบไฟล์ เราจะต้องอ้างอิงไปยัง path และระบุชื่อของไฟล์เหมือนเคย ตัวอย่าง จะลบไฟล์ชื่อ kaizen.png จากโฟลเดอร์ photos
โดยเราสามารถเพิ่ม Listener ไปเพื่อ handle ผลลัพธ์ได้
จากการเพิ่ม metadata ไปในข้อ 3.3 แล้ว เราก็ยังสามารถที่จะ ดึง หรือ อัพเดท ข้อมูลใน metadata ได้อีก
ปกติเวลาเรา upload ไฟล์ภาพไป Cloud Storage for Firebase ก็จะมีการเก็บค่า metadata บางอย่างอยู่แล้ว เช่น name, size และ contentType หรือเราก็สามารถ custom ค่าไปเก็บได้เช่นกัน ซึ่งเราสามารถจะดึงข้อมูลทั้งหมดนี้ออกมาได้
เราสามารถอัพเดท metadata หลังจากอัพโหลดไฟล์ไปแล้วได้ โดยระบบจะอัพเดทเฉพาะ field ที่เราเลือกเท่านั้น ส่วน field ที่ไม่ได้เลือกก็จะไม่มีการอัพเดทใด
Cloud Storage for Firebase เป็นบริการที่มีทั้งฟรี และมีค่าใช้จ่ายในกรณีเกิด limit ซึ่งเราสามารถดูการใช้งานของเราที่ Firebase Console เลือก Storage แล้วเลือก tab ชื่อ USAGE
ส่วนเรื่อง Price plan ของ Cloud Storage for Firebase ก็มีดังนี้
แบบฟรีจะเก็บไฟล์ที่ 5GB, bandwith 30GB และสามารถ upload ได้ 50,000 ครั้ง และ download ได้ 50,000 ครั้ง แต่เอาเข้าจริงกล้องถ่ายรูปบนโทรศัพท์สมัยนี้ก็ละเอียดยิ่งนัก หากเราไม่ทำการ resize หรือ compress ก่อน 5GB ที่เก็บนี้ น่าจะไปถึงได้ไม่ยาก เรามาจำลองการใช้งานดูหน่อยดีกว่า
แบบฟรี จำลองเก็บรูปได้ 2,500 รูป แบบ high-resolution และ bandwidth สามารถ upload และ download ได้ 15,000 รูปแบบ high-resolution
ส่วนสิ่งที่ยังไม่ได้ลงรายละเอียดให้คือ Security & Rules ให้ไปอ่านกันเองก่อนนะครับ เดี๋ยวผมจะทำบทความเรื่องนี้แยกออกมาทั้ง Realtime Database และ Storage
เป็นอย่างไรบ้างครับ เรื่อง Cloud Storage for Firebase ผมว่ามันเจ๋งมากเลย implement ไม่ยากด้วย code สั้นมาก ผมไปลองโหลดตัว quickstart ของ Firebase ใน GitHub มา คิดว่าของผมเข้าใจง่ายกว่านะ 555 Source code ทั้งหมดของบทความนี้ก็ไปโหลดที่ GitHub มาดูได้ครับ
github.com
สำหรับวันนี้ขอลาไปก่อน แล้วพบกันใหม่กับบทความถัดไป…ราตรีสวัสดิ์ พี่น้องชาวไทย
Let you learn and share your Firebase experiences with each…
275 
2
275 claps
275 
2
Let you learn and share your Firebase experiences with each other.
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Let you learn and share your Firebase experiences with each other.
"
https://medium.com/swlh/triggering-cloud-storage-with-cloud-functions-in-java-177d7c3c1208?source=search_post---------58,"There are currently no responses for this story.
Be the first to respond.
Google Cloud Functions is a serverless execution environment for building and connecting cloud services. With Cloud Functions, you write simple, single-purpose functions that are attached to events emitted from your cloud infrastructure and services. Your function is triggered when an event being watched is fired. Your code executes in a fully managed environment. There is no need to provision any…
"
https://medium.com/google-cloud/upload-download-files-from-a-browser-with-gcs-signed-urls-and-signed-policy-documents-f66fff8e425?source=search_post---------367,"There are currently no responses for this story.
Be the first to respond.
Small javascript application showing how to upload/download files with GCS Signed URLs and Signed Policy Documents. This article will not cover in detail what those two mechanisms are but rather demonstrate a basic application that exercises both on the browser. This is a simple client-server app that uploads files using these two mechanisms from a user’s browser. SignedURLs w/ javascript has been done many times before (see references); this article describes SignedURLs and Policy document differences and implementations.
Briefly, SignedURLs and Policy Document based operations are similar: a URL with a signature that an unauthenticated user can perform certain GCS operations. There are a couple of differences to note:
The code snippet provided here is Flask application that provides signedURLS and Policy documents to a javascript browser. Snippet also shows how to configure CORS access for SignedURLs
Note: this is just a basic sample with some awkward javascript, nothing more. The sample here allows you to generate URLs to upload+download any file in the bucket by specifying its name.
You can find the git version of this article here:
github.com
Add to /etc/hosts.
Set CORS policy to allow request from a test domain:
(ofcourse use the value, not literal)
(you may want to enable Developer Mode in Chrome; that will show you the CORS requests too)
You can also genereate upload/download urls with gsutil or any google-cloud-storage library
Policy Document allows for uploads only but also provides several configurations and conditions canonical SignedURLs do not: you can define a prefix/path with which a user can upload multiple files with one URL. In the example below, one URL can upload N files to the bucket and each file must have the prefix /myfolder/ and be of Content-Type: text/plain
Policy documents can also get embedded into a form as described in the links above and in the snippet below
As an HTML form, you can post the same generated signature, file and policy document. For example:
Google Cloud community articles and blogs
45 
1
45 claps
45 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@nathansebhastian/firebase-as-simple-database-to-react-app-7c6b6c03014d?source=search_post---------258,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nathan Sebhastian
Mar 6, 2019·4 min read
Firebase is an all-in-one backend as a service provider (BaaS) that provides database, authentication, cloud storage among their many services. In this tutorial you’re going to learn how to use Firebase Real Time Database service in React application.
You’re going to build a simple team list application, where users can add, delete and edit team member information.
Create your database application
First, you need to create your application in Firebase console.
Then head over to the Database menu and scroll a bit down into Choose Real Time Database section.
Set the security rules to start in test mode.
This makes your database insecure, but it’s okay for the purpose of this tutorial.
Then grab your Firebase credential from the Project Overview tab
You can use the source code from this codesandbox and fork it:
Copy and paste your Firebase credentials into the config.js file:
Start your React application with create-react-app
Then install firebase and Bootstrap (so you can skip writing your own css.)
Then you can remove everything from src/ since you don't need most of the boilerplates
Let’s write Firebase configuration in a separate config.js file:
You’ll import this config into your App.js later.
This file will serve as React entry point:
It’s time to write your App.js file. Let's initialize our Firebase App in the constructor:
Then you can write the logic for getting and saving data:
Put these writeUserData and getUserData in componentDidMount and componentDidUpdate respectively
All that’s left is to write the render and handle submit form logic:
Now your React application is ready to read and set data into your Firebase database. Here is the final demo again:
You might wonder is it safe to put Firebase Api key in the config where experienced programmers can easily get it. Actually, it’s okay to put it there because Firebase has security rules that ensures only authenticated users can access your database. I just haven’t setup the security rules in this tutorial.
I will write a more complete tutorial that covers authentication, database rules and using Firebase cloud storage for user uploaded images in my next post, so stay tuned!
Also, I’m writing a book on learning React properly without the stress. You might wanna check it out here.
Originally published at sebhastian.com.
A senior software developer with experience in full-stack JavaScript. Nathan has published over 100 JavaScript tutorials at https://sebhastian.com
See all (14)
18 
18 claps
18 
A senior software developer with experience in full-stack JavaScript. Nathan has published over 100 JavaScript tutorials at https://sebhastian.com
About
Write
Help
Legal
Get the Medium app
"
https://rom.feria.name/how-to-make-your-google-drive-or-microsoft-onedrive-private-fc81732d827?source=search_post---------309,"For most users, cloud storage is provided by either Google or Microsoft, or both. Google Drive and Microsoft OneDrive are familiar with educators and students due to their their institutions’ subscriptions. In addition, you get free storage from either companies, when you sign up for their free Gmail or Office 365 accounts. Whilst both companies secure your data, your data are not guaranteed to be private — yes, the data is encrypted at rest, but the encryption used are managed by them, not you. What does this mean? Well, they can always decrypt it…
"
https://medium.com/backticks-tildes/react-native-image-upload-to-firebase-cloud-storage-ios-24b8dc0a9c8b?source=search_post---------3,"There are currently no responses for this story.
Be the first to respond.
In this tutorial, we will go through how to upload images to Firebase Cloud Storage in React Native applications.
React Native
React Native is a Javascript framework by Facebook used to build cross mobile native applications(iOS, Android, and Windows). We will build an example app for iOS. You can read more about React Native here.
"
https://medium.com/google-cloud/ruby-gcp-uploading-pictures-to-cloud-storage-4c136dd2405f?source=search_post---------106,"There are currently no responses for this story.
Be the first to respond.
Uploading content to a storage provider is one of the most basic tasks I do with a cloud provider. At least it should be. This is a quick tutorial on uploading files to Google Cloud Storage with Ruby. Code for this tutorial is here.
The code to upload a file is simple.
First, require the library and create a new gcloud object using your project id and the service account key. You can get the project id from cloud console. It may not be the same as your project name.
Then use the gcloud object to create a storage object.
After that, create a cloud storage bucket. Cloud storage bucket names must be unique within all of Google Cloud. Bucket names are also potentially globally visible so they should not contain proprietary information. For more information see bucket naming requirements and bucket naming best practices.
In the example, the bucket is called my_goat_pictures.
Once you create the bucket, you can upload a file using the create_file method. In the example, I uploaded a picture called goat.jpg and it was saved as uploaded_goat.jpg in the cloud storage bucket.
You can verify that the file was uploaded correctly by using the storage browser in the web console. https://console.cloud.google.com/storage/browser
When you create a bucket, you can specify an access control list (ACL), the location of the bucket, and what class of storage to use. You can also turn on object versioning for the entire bucket. If you are using the bucket to serve a static website (perhaps one generated with Jekyll) you can set up the index/main page for the site and the 404 page when creating the bucket.
Likewise, there are many options that can be specified when creating a file. Cache-control information, content metadata (encoding, language, disposition), custom metadata, and encryption keys can be specified. You can also include a checksum and the file will only be created if the checksum matches the value calculated by Cloud Storage.
For more information about the Cloud Storage API check out the documentation.
09/29/16
Originally published at www.thagomizer.com on September 30, 2016.
Google Cloud community articles and blogs
3 
3 claps
3 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Rubyist, Data Nerd, Lazy Dev, Stegosaurus. Cloud Developer Advocate @ Google. Disclaimer: My opinions are my own. *RAWR*
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/pinterest-engineering/improving-efficiency-and-reducing-runtime-using-s3-read-optimization-b31da4b60fa0?source=search_post---------344,"There are currently no responses for this story.
Be the first to respond.
Bhalchandra Pandit | Software Engineer
We describe a novel approach we took to improving S3 read throughput and how we used it to improve the efficiency of our production jobs. The results have been very encouraging. A standalone benchmark showed a 12x improvement in S3 read throughput (from 21 MB/s to 269 MB/s). Increased throughput allowed our production jobs to finish sooner. As a result, we saw 22% reduction in vcore-hours, 23% reduction in memory-hours, and similar reduction in run time of a typical production job. Although we are happy with the results, we are exploring additional enhancements in the future. They are briefly described at the end of this blog.
We process petabytes of data stored on Amazon S3 every day. If we inspect the relevant metrics of our MapReduce/Cascading/Scalding jobs, one thing stands out: slower than expected mapper speed. In most cases, the observed mapper speed is around 5–7 MB/sec. That speed is orders of magnitude slower compared to the observed throughput of commands such as aws s3 cp, where speeds of around 200+ MB/sec are common (observed on a c5.4xlarge instance in EC2). If we can increase the speed at which our jobs read data, our jobs will finish sooner and save us considerable time and money in the process. Given that processing is costly, these savings can add up quickly to a substantial amount.
If we inspect implementation of the S3AInputStream, it is easy to notice the following potential areas of improvement:
Architecture
Our approach to addressing the above-mentioned drawbacks includes the following:
We further enhanced the implementation to make it a mostly lock-free producer-consumer interaction. This enhancement improves read throughput from 20 MB/sec to 269 MB/sec as measured by a standalone benchmark (see details below in Figure 2).
Sequential reads
Any data consumer that processes data sequentially (for example, a mapper) greatly benefits from this approach. While a mapper is processing currently retrieved data, data next in sequence is being prefetched asynchronously. Most of the time, data has already been pre-fetched by the time the mapper is ready for the next block. That results in a mapper spending more time doing useful work and less time waiting for data, thereby effectively increasing CPU utilization.
More efficient Parquet reads
Parquet files require non-sequential access as dictated by their on-disk format. Our initial implementation did not use a local cache. Each time there was a seek outside of the current block, we had to discard any prefetched data. That resulted in worse performance compared to the stock reader when it came to reading from Parquet files.
We observed significant improvement in the read throughput for Parquet files once we introduced the local caching of prefetched data. Currently, our implementation increases Parquet file reading throughput by 5x compared to the stock reader.
Improved read throughput leads to a number of efficiency improvements in production jobs.
Reduced job runtime
The overall runtime of a job is reduced because mappers spend less time waiting for data and finish sooner.
Potentially reduced number of mappers
If mappers take sufficiently less time to finish, we are able to reduce the number of mappers by increasing the split size. Such reduction in the number of mappers leads to reduced CPU wastage associated with fixed overhead of each mapper. More importantly, it can be done without increasing the run time of a job.
Improved CPU utilization
The overall CPU utilization increases because the mappers are doing the same work in less time.
For now, our implementation (S3E) is in a separate git repository to allow faster iterations over enhancements. We will eventually contribute it back to the community by merging it back into S3A.
In each case, we read a 3.5 GB S3 file sequentially and wrote it locally to a temp file. The latter part is used to simulate IO overlap that takes place during a mapper operation. The benchmark was run on a c5.9xlarge instance in EC2. We measured the total time taken to read the file and compute the effective throughput of each method.
We tested many large production jobs with the S3E implementation. Those jobs typically use tens of thousands of vcores per run. In Figure 3, we present a summary of comparison between metrics obtained with and without S3E enabled.
Measuring resource savings
We use the following method to compute resource savings resulting from this optimization.
Observed results
Given the variation in the workload characteristics across production jobs, we saw vcore reduction anywhere between 6% and 45% across 30 of our most expensive jobs. The average saving was a 16% reduction in vcore days.
One thing that is attractive about our approach is that it can be enabled for a job without requiring any change to a job’s code.
At present, we have added the enhanced implementation to a separate git repository. In the future, we would likely update the existing S3A implementation and contribute back to the community.
We are in the process of rolling out this optimization across a number of our clusters. We will publish the results in a future blog.
Given that the core implementation of S3E input stream does not depend on any Hadoop code, we can use it in any other system where large amounts of S3 data is accessed. Currently we are using this optimization to target MapReduce, Cascading, and Scalding jobs. However, we have also seen very encouraging results with Spark and Spark SQL in our preliminary evaluation.
The current implementation can use further tuning to improve its efficiency. It is also worth exploring if we can use past execution data to automatically tune the block size and the prefetch cache size used for each job.
To learn more about engineering at Pinterest, check out the rest of our Engineering Blog, and visit our Pinterest Labs site. To view and apply to open opportunities, visit our Careers page.
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
165 
2
165 claps
165 
2
Written by
https://medium.com/pinterest-engineering | Inventive engineers building the first visual discovery engine https://careers.pinterest.com/
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
Written by
https://medium.com/pinterest-engineering | Inventive engineers building the first visual discovery engine https://careers.pinterest.com/
Inventive engineers building the first visual discovery engine, 200 billion ideas and counting.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/volterra-io/kubernetes-storage-performance-comparison-v2-2020-updated-1c0b69f0dcf4?source=search_post---------336,"There are currently no responses for this story.
Be the first to respond.
In 2019 I published a blog: Kubernetes Storage Performance Comparison. My goal was to evaluate the most common storage solutions available for Kubernetes and perform basic performance testing. I had results for GlusterFS, CEPH, Portworx and OpenEBS (with cStor backend). This blog has been popular and I received a lot of positive feedback. I’ve decided to come back with few updates on progress in the storage community and their performance numbers, which I promised in my last blog. I extended my testing scope to include 2 more storage solutions:
Let’s start with the storage backend updates and their installation description, then we will go over the AKS testing cluster environment and present the updated performance results at the end.
As of January 2019, the CNCF storage landscape and solutions have changed. It has grown from 30 to 45 solutions under the storage banner, there were also governance expansions of public cloud integrations such as AWS EBS, Google persistent disk or Azure disk storage. Some of the new solutions focused more towards distributed filesystem or object storage as Alluxio. My original goal, and continues to be the same, is to evaluate block storage options. Let’s revisit my original list.
GlusterFS Heketi was second worst in performance results and its improvements are zero and it is mostly a dead project (Heketi as REST orchestrator not GlusterFS itself). If you look at their official GitHub, you can see that they are placing it into a near-maintenance mode and there is not any update in terms of cloud-native storage features.
PortWorx remains still in the top commercial storage solutions for Kubernetes according to the GIGAOM 2020 report. However there hasn’t been a significant technology or architecture change claimed in release notes between versions 2.0 and 2.5 from a performance point of view.
The best open source storage, CEPH orchestrated via Rook, produced 2 new releases and introduced a new CEPH version called Octopus. Octopus brings several optimizations in caching mechanisms and uses more modern kernel interfaces (See more at the official page).
The only major architecture change happened in OpenEBS, where it introduced a new backend called MayaStor. This backend looks very promising.
I also received a lot of feedback from the community on why I did not test Longhorn from Rancher. Therefore I decided to add to my scope.
I evaluated Longhorn and OpenEBS MayaStor and compared their results with previous results from PortWorx, CEPH, GlusterFS and native Azure PVC. The following subsection introduces storage solutions added into the existing test suite. It also describes installation procedure and advantages/disadvantages of each solution.
Longhorn is cloud-native distributed block storage for Kubernetes, developed by Rancher. It was designed primarily for microservices use cases. It creates a dedicated storage controller for each block device volume and synchronously replicates the volume across multiple replicas stored on multiple nodes. Longhorn creates a Longhorn Engine on the node where volume is attached to, and it creates a replica on a node where volume is replicated. Similar to others, the entire control plane runs and the data plane is orchestrated by Kubernetes. It is fully open source. It’s interesting that OpenEBS Jiva backend is actually based on Longhorn or at least initially it was its fork. The main difference is that Longhorn uses TCMU Linux driver and OpenEBS Jiva uses gotgt.
How to get it on AKS?
Installation to AKS is trivial
2. Mount /dev/sdc1 with ext4 filesystem into /var/lib/longhorn, which is the default path for volume storage. It is better to mount the disk there before Longhorn installation.
3. The last step is to create a default storage class with 3 replicas definition.
Advantages
Disadvantages
OpenEBS represents the concept of Container Attached Storage (CAS), where there is a single microservice-based storage controller and multiple microservice-based storage replicas. If you read my previous blog post from 2019, you know that I was playing 2 backends — Jiva and cStor. I ended up using cStor and its performance results were really bad. However 1.5 years is a long time and the OpenEBS team introduced a new backend called MayStor.
It’s a cloud-native declarative data plane written in Rust, which consists of 2 components:
How to get it on AKS?
Installation on AKS is straight forward, and I followed their quick start guide.
echo 512 | sudo tee /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
However I decided to enforce them via k8s daemonset below instead of ssh into every my instance.
2. I had to label my storage node VMs.
3. Then I applied all manifests specified in MayaStor repository.
4. When everything is running, you can start creating storage pools for volume provisioning. In my case I created 3 storage pools with a single disk per node.
5. It is important to check the status of each storage pool before you can proceed with StorageClass definitions. State must be online.
6. The last step in the process is StorageClass definition, where I configured 3 replicas to have same testing environment as for my previous storage solutions.
After I had finished these steps I was able to dynamically provision a new volumes via K8s PVC.
Advantages
Disadvantages
IMPORTANT NOTE: The results from individual storage performance tests cannot be evaluated independently, but the measurements must be compared against each other. There are various ways to perform comparative tests and this is one of the simplest approaches.
For verification I used exactly the same lab with Azure AKS 3 node cluster and 1TB premium SSD managed disk attached to each instance. Details you can find in the previous blog.
To run our tests I decided to use the same load tester called Dbench. It is K8s deployment manifest of pod, where it runs FIO, the Flexible IO Tester with 8 test cases. Tests are specified in the entry point of Docker image:
At the start, I ran Azure PVC tests to get a baseline for comparison with last year. The results were almost the same, therefore we can assume conditions remained unchanged and we would achieve the same numbers with the same storage versions. Updated full test outputs from all tests from 2019 plus new MayStor and Longhorn tests are available at https://gist.github.com/pupapaik/76c5b7f124dbb69080840f01bf71f924
Random read test showed that GlusterFS, Ceph and Portworx perform several times better with read than host path on Azure local disk. OpenEBS and Longhorn perform almost twice better than local disk. The reason is read caching. The write was the fastest for OpenEBS, however Longhorn and GlusterFS got also almost the same value as a local disk.
Random IOPS showed the best result for Portworx and OpenEBS. OpenEBS this time got even better IOPS on write than native Azure PVC, which is almost technically impossible. Most probably it is related to Azure storage load at different times of test case runs.
Latency read winner remained the same as last time. LongHorn and OpenEBS had almost double of PortWorx. This is still not bad since native Azure pvc was slower than most of the other tested storages. However latency during write was better on OpenEBS and Longhorn. GlusterFS was still better than other storages.
Sequential read/write tests showed similar results as random tests, however Ceph was 2 times better on read than GlusterFS. The write results were almost all on the same level and OpenEBS and Longhorn achieved the same.
The last test case verified mixed read/write IOPS, where OpenEBS delivered almost twice higher than PortWorx or Longhorn on read as well as write.
This blog shows how significantly an open source project can change in a single year! As a demonstration let’s take a look at comparison of IOPS between OpenEBS cStor and OpenEBS MayaStor on exactly the same environment.
Please take the results just as one of the criteria during your storage selection and do not make final judgement just on my blog data. To extend my last summary from 2019 on what we can conclude from the tests:
Of course this is just one way to look at container storage selection. The interesting parts are also scaling and stability. I will keep an eye on other evolving projects in the CNCF storage landscape and bring new interesting updates from performance testing and scaling.
Volterra Edge Services
351 
9
351 claps
351 
9
Volterra Edge Services
Written by
Living on the Edge in Volterra. Co-founder & former CTO of tcp cloud (acquired by Mirantis in 2016).
Volterra Edge Services
"
https://medium.com/@macsources/news-amazon-cloud-storage-for-5-8bc9d2bfbb82?source=search_post---------199,"Sign in
There are currently no responses for this story.
Be the first to respond.
MacSources
Nov 27, 2015·1 min read
Amazon is offering unlimited Cloud storage for one year as part of their Black Friday deals. Normally, they charge $60 for one year of service, but their Black Friday special reduces the cost to $5. That means, you can store an unlimited amount of photos, documents, and other files for only $5. Amazon offers free apps to access your stored files for both your mobile devices and desktop computer. You can also autosave your photos directly from your phone into the Amazon Cloud. And, Amazon won’t ever reduce the resolution of your images. Amazon Black Friday Cloud Storage Deal
Originally published at www.macsources.com on November 27, 2015.
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
"
https://medium.com/@taskade/modern-tsundoku-%E7%A9%8D%E3%82%93%E8%AA%AD-how-to-overcome-digital-hoarding-for-knowledge-workers-1bcc1ade7531?source=search_post---------319,"Sign in
There are currently no responses for this story.
Be the first to respond.
Taskade
Dec 2, 2020·9 min read
The Japanese term Tsundoku (積ん読) — piling up unread reading materials — is undergoing a renaissance of sorts. Ever since cloud storage became readily available and dirt cheap, we’ve been spiraling down the rabbit hole of digital hoarding.
Keeping pristine digital hygiene isn’t easy. But it gets even more complicated when companies switch to remote work. Without a solid data management plan, distributed organizations can quickly lose their way in a deluge of information.
A 2016 survey by Veritas found that 82% of IT decision-makers are notorious data hoarders.(1) To make matters worse, 85% of enterprise data is an unorganized mess that costs employees plenty of time — 2.5 hours each day — to plow through.(2)
Of course, digital hoarding isn’t just a problem of the top brass. Forgotten browser bookmarks, thousands of unread emails, duplicate files… Sounds familiar? While creative pursuits require a dash of disorganization, you can’t hope to keep that game up forever.
In today’s article, we set out to find a solution:
💡 Before you start… This article is part of our long-running series on knowledge management. Be sure to check other similar articles when you’re done reading:
So, without further ado…
How many times have you bought a book/newspaper, subscribed to a newsletter, or archived an email because you thought you’d need it “one day”?
Don’t worry, we all do that once in a while.
The elegant term Tsundoku (積ん読) can either describe unread reading materials — usually books — or the habit of collecting them. According to Big Think, the term blends “[…] tsunde-oku (letting things pile up) and dukosho (reading books).”(4)
Tsundoku are either impulsive or “just in case” book purchases. Sometimes, we fall for them because the cover art looks cool. Other times, it’s the allure of owning stuff, the tactility of hardbacks, the smell of paper… you get the idea.
While the traditional, 19th-century meaning of Tsundoku refers to printed materials, its premise is still very much alive today as “digital hoarding.”
Here are some modern, digital Tsundokus we collect:
While collecting digital stuff doesn’t seem like a problem — except for the wallet — piling up gigabytes of data can eventually become a burden.
As one Reddit user admits:
“I have a hard time getting rid of screenshots or pictures because I may need them later. I have over 15K pix on my phone alone, nearly 500 apps, and I follow over 2K people on Insta & Twitter. I always feel like it’s associated with “FOMO”, it feels like I will be missing out if I don’t follow everybody, everything, if I don’t keep (up with) everything ever.”(5)
Things get a bit more complicated when digital hoarding takes over our professional lives and starts affecting work performance.
A 2020 survey by Centre for Research and Evidence on Security Threats (CREST) found that there are four common scenarios of digital hoarding.(6)
Here are some of the most popular workplace Tsundokus:
Regardless of the motives, digital hoarding can seriously affect the performance of remote organizations. Which takes us to the next point.
According to the CREST report we mentioned earlier, digital hoarding may pose serious security and compliance risks. It can also impair productivity by overwhelming employees with an excess of uncurated information.(6)
And it only gets worse:
The good news is, the modern case of Tsundoku isn’t that difficult to manage.
The first ingredient necessary to overcome Tsundoku is a unified, collaborative, team-wide system for storing, organizing, and retrieving data.
Your team needs to understand the importance of data management and know how to deal with information on a personal and organizational level.
Here are three knowledge-management schemes that’ll help you take control of your team’s data (follow links for more details):
Each of these methods will let your distributed team separate the wheat from the chaff and keep only the important stuff.
You can cram over a few weekends and try to clean up your team wiki or knowledge base. But if you don’t get your team on board, the digital clutter will pile up again.
And let’s not forget that you have little control over thousands of unread emails, hundreds of bookmarks, and hoards of duplicate files stashed by individual users.
Here’s what you should go instead:
Freedom from digital clutter starts with awareness. Do you know how much Tsundoku your team collects? More importantly, are you aware of your own hoarding habits?
Follow these tips to get it under control:
In most organizations without a clear data policy, indexing digital assets and keeping them organized can be a challenge.
The good news is, Taskade is here to help. 🐑
Jump over here to sign up for a free account today. 👈
Don’t panic! You don’t have to part with your precious bookmarks, files, and emails (yet). But you also can’t stick to all that digital clutter forever.
Think of your team’s knowledge like a garden. As long as you weed the flower beds and cut the grass, the balance between assets and clutter is maintained. The moment you give up the maintenance, it’s glamour fades and the weeds quickly take over.
Here are a few tips that’ll help you weed your digital garden:
Make sure to schedule regular cleanups in your team’s calendar so everybody knows when it’s time to roll up their sleeves.
The “collecting” aspect of Tsundoku isn’t a problem in itself. It’s the lack of actionability that prevents your team from capitalizing on all that intellectual value.
You want to make sure that all bits of information your team stores — emails, documents, wikis, guides — are actionable and put to good use over their life cycle.
And that’s it! 👋
Here’s a free template that includes all the tips discussed in this article. Don’t forget to copy it to your Taskade Workspace and share with the rest of the team!
For some, digital hoarding — the 21-century take on Tsundoku — has become a hook that’s not much different from compulsive social media use or smartphone addiction. It’s stimulating, addictive, and makes us feel genuinely productive.
Except, you already know that isn’t true, right?
It doesn’t matter if you’re a custodian of documents, photos, videos, browser bookmarks, or unread emails. If the thought of parting with digital clutter makes you feel strangely anxious, it’s time for some spring cleaning.
💡 Before you go… If you’re hungry for more tips on how to stay organized in the digital world, check this awesome interview with Cal Newport, the author of Digital Minimalism: Choosing a Focused Life in a Noisy World.
Resources 🔗
(1) https://www.veritas.com/news-releases/2016-09-27-veritas-research-shows-digital-hoarding-behaviour-is-pervasive-with-employees-willing-to-give-up-almost-anything-but-their-data(2) https://www.hitachivantara.com/en-us/pdf/infographic/are-you-data-hoarder-infographic.pdf(3) https://coolnessgraphed.com/image/169367546204(4) https://bigthink.com/personal-growth/do-i-own-too-many-books?rebelltitem=3#rebelltitem3(5) https://www.reddit.com/r/hoarding/comments/gycwcg/digital_hoarder/(6) https://crestresearch.ac.uk/comment/the-risks-of-digital-hoarding/(7) https://www.veritas.com/content/dam/Veritas/docs/reports/V0479_Data-Genomics-Index-Report.pdf
Taskade is the unified workspace for distributed teams. Chat, organize, and get things done. — Get Taskade at https://www.taskade.com — Web, Mobile, Desktop.
Taskade is the unified workspace for distributed teams. Chat, organize, and get things done. — Get Taskade at https://www.taskade.com — Web, Mobile, Desktop.
"
https://medium.com/linedevth/%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87%E0%B8%A3%E0%B8%B0%E0%B8%9A%E0%B8%9A-resize-%E0%B8%A3%E0%B8%B9%E0%B8%9B%E0%B9%83%E0%B8%AB%E0%B9%89-line-chatbot-%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2-firebase-extensions-335137b42254?source=search_post---------345,"There are currently no responses for this story.
Be the first to respond.
การแสดงผลรูป original ขนาดใหญ่ที่ผู้ใช้อัพโหลดเข้ามานั้น มักส่งผลทำให้ระบบของเราตอบสนองได้ช้าลง ใช้งาน bandwidth เพิ่มขึ้น และเป็นเหตุที่ทำให้ cost ของการให้บริการสูงขึ้นด้วย โดยเฉพาะการแสดงผลที่ไม่จำเป็นต้องใช้รูปต้นฉบับ เช่นการแสดง thumbnails ของสินค้าเป็นต้น
เป็นกระบวนการที่นักพัฒนาเลือกทำเพื่อลดต้นทุนที่จะเกิดขึ้น เพราะมันคุ้มกว่า หากเทียบกับค่าใช้จ่ายของ storage ที่เสียไป และเพื่อให้เห็นภาพเรามาดูตัวอย่างกัน โดยผมจะสมมติว่ารูปที่ผู้ใช้อัพโหลดเข้ามามีขนาด 500KB และมีคนเปิดดูรูป 1,000,000 ครั้ง(แบบไม่ติด cache)
จากการเปรียบเทียบจะเห็นว่า รูปที่ resize แล้วจะสามารถลดการใช้ bandwidth ได้ถึง 10 เท่า ซึ่งถ้ายิ่งคนเปิดดูเยอะก็จะยิ่งประหยัด ในขณะที่พื้นที่ จะเพิ่มขึ้นเพียง 50KB เท่านั้น
อีกเรื่องที่เราควรพิจารณาก่อนการพัฒนาระบบ Image resizing ขึ้นมาก็คือ เราควรให้ระบบนี้อยู่ใน front-end หรือ back-end ดี? ซึ่งโจทย์ข้อนี้มันมี trade-off ของมันอยู่ แต่ส่วนตัวผมเลือกทำที่ back-end ด้วยเหตุผล
เกริ่นมาซะยาวเลย เข้าเรื่องเลยละกัน บทความนี้ผมจะพาทุกคนไปพัฒนาระบบย่อรูปให้กับ LINE Chatbot ด้วย Firebase Extenstions ผู้ช่วยในการย่อรูป ที่สายย่อต้องชอบ เพราะคุณไม่จำเป็นต้อง setup ตัว storage เอง และไม่ต้องเขียนโปรแกรมในส่วนของการย่อรูปเอง นั่นก็หมายความว่าคุณมีโอกาสจะเป็น Harry Potter ได้ เพราะคุณจะสามารถเสกของออกมาได้ในเวลาอันสั้นนั่นเอง 555+
สำหรับใครที่ยังไม่เคยพัฒนา LINE Chatbot ด้วย Cloud Functions for Firebase ให้ทำตามขั้นตอนของบทความด้านล่างนี้(ข้อ 1–3 ก็พอ) แต่หากใครมีประสบการณ์ตรงนี้แล้ว ข้ามไปขั้นตอนที่ 2 ได้เลย
medium.com
หมายเหตุ: เนื่องจากการพัฒนา LINE Chatbot ด้วย Cloud Functions คุณจำเป็นต้อง request APIs นอก Google domain ทำให้คุณต้องเปลี่ยนแพลนการใช้งานจาก Spark ไปเป็น Blaze แต่ข้อดีคือคุณจะได้โควต้าฟรีเพิ่มขึ้นใน Blaze แบบทวีคูณ
ในโฟลเดอร์ functions ของโปรเจคที่เราได้เตรียมไว้(ขั้นตอนที่ 1) ให้เปิดไฟล์ package.json ขึ้นมา แล้วเพิ่ม dependency ชื่อ uuid-v4 เข้าไป เนื่องจากเราจะใช้มันสร้าง public url ของไฟล์ที่อัพโหลดขึ้น Cloud Storage ในลำดับต่อไป
ถัดไปเปิดไฟล์ index.js ขึ้นมา แล้ว import ตัว firebase-admin เข้ามาพร้อม initialize ตามด้วย dependencies สำหรับจัดการไฟล์
จากนั้นสร้างฟังก์ชันใหม่ชื่อ uploadPhoto โดยภายในฟังก์ชันให้เขียนเงื่อนไขเพื่อรับ webhook event จาก LINE ซึ่งเราจะสนใจเฉพาะกรณีที่ผู้ใช้อัพโหลดรูปเข้ามา
ต่อมาก็สร้างฟังก์ชัน upload() โดยฟังก์ชันนี้จะทำหน้าที่ในการดาวน์โหลด binary ออกมาจาก LINE และอัพโหลดไปยัง Cloud Storage for Firebase บริการฝากไฟล์ที่ให้พื้นที่ฟรี 5 GB
หมายเหตุ: ปกติการอัพโหลดไฟล์ขึ้น Cloud Storage ด้วย Firebase Admin ไฟล์จะได้สถานะเป็น private ซึ่งการเพิ่ม firebaseStorageDownloadTokens ที่มีค่าเป็น uuid นี้ เป็นเทคนิคในการทำให้เราได้ public url ออกมา
สุดท้ายก็สร้างฟังก์ชัน reply() เพื่อส่ง URL ที่ได้จากการอัพโหลดกลับไปยังห้องแชท
Firebase Extensions เป็นบริการที่ช่วยลดเวลาในการพัฒนาฟังก์ชันต่างๆ ที่เรามักจะต้องพัฒนาขึ้นมาใช้เองใน Firebase โดยนักพัฒนาเพียงแค่ติดตั้ง extension ที่ต้องการ ก็จะสามารถใช้งานได้ทันที(ประมาณต้มมาม่ากิน) ซึ่งการย่อรูปก็เป็นหนึ่งในบริการดังกล่าว
เริ่มจากเข้าไปที่ หน้ารวม Extensions แล้วเลือก Resize Images
เมื่อกดปุ่ม Install แล้ว ระบบจะพาไปหน้าที่ให้เลือกโปรเจค ก็ให้เราเลือกโปรเจคเดียวกันกับที่สร้าง LINE Chatbot ไว้ จากนั้นจะเข้าสู่หน้ารายละเอียดการติดตั้ง
โดยขั้นตอนการติดตั้งส่วนที่ 1 และ 2 จะเป็นรายละเอียดว่าจะเปิดใช้ API อะไรบ้าง และขอสิทธิ์การเข้าถึงของ extension ที่จะติดตั้งเป็นอย่างไร อ่านเสร็จก็กด Next ไป
ส่วนที่ 3 คือส่วนที่ให้เราตั้งค่าต่างๆ โดยจุดที่น่าสนใจมีดังนี้
การติดตั้งจะใช้เวลาประมาณ 3–5 นาที ซึ่งระหว่างนี้ผมจะให้ทุกคนกลับไปแก้โค้ดที่ฟังก์ชัน upload() ในไฟล์ index.js กันอีกหน่อย เพราะผมต้องการเปลี่ยนการ return จาก url รูปต้นฉบับ ไปเป็น object ที่ประกอบไปด้วย url ของทั้งรูปต้นฉบับและรูปที่ถูกย่อ
หมายเหตุ: รูปแบบของ thumb url จะขึ้นอยู่กับ size และ path ที่เราตั้งค่าในส่วนที่ 3
ส่วนสุดท้ายก็เปลี่ยนการ reply ในฟังก์ชัน uploadPhoto จาก text ธรรมดา ให้กลายเป็น Flex Message ซะหน่อย ก็จะได้โค้ดทั้งหมดเต็มๆแบบนี้
เป็นอย่างไรกันบ้างครับ เร็วใช่มะ สำหรับวิธีพัฒนาระบบย่อรูปที่ผมมาแชร์ เอาจริงๆถ้าได้วางแผนมา แล้วลงมือโค้ดเลย คงไม่เกิน 3 ชม.เสร็จ ซึ่งสมัยก่อนหากผมต้องพัฒนาระบบนี้ขึ้น production คงต้องใช้เวลาตั้งแต่คิด เตรียม server และเขียนโค้ดอีกเป็นวันๆ และนอกจากการพัฒนาระบบย่อรูปให้ Chatbot แล้ว นักพัฒนายังสามารถนำมันไปประยุกต์กับแพลทฟอร์มอื่นๆอย่าง iOS, Android, Web และ Game ได้อีก อย่าลืมกลับไปลองเล่นกันนะครับ
และเพื่อที่คุณจะไม่พลาดบทความใหม่ๆจากทั้ง LINE Developers Thailand และ Firebase Thailand ก็อย่าลืมกด Follow ตัว Publication กันด้วยน้า วันนี้ลาไปก่อน แล้วพบกันใหม่บทความหน้า สวัสดีคร้าบ
Deliver world-class developer experiences and learning through LINE API
88 
1
88 claps
88 
1
Closing the distance. Our mission is to bring people, information and services closer together
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Closing the distance. Our mission is to bring people, information and services closer together
"
https://medium.com/@insync/insync-extends-google-drives-cloud-storage-for-businesses-2a084f264f69?source=search_post---------152,"Sign in
There are currently no responses for this story.
Be the first to respond.
Insync
Jan 22, 2015·2 min read
We recently moved our blog to Medium. The following post was originally published on insynchq.tumblr.com at July 10, 2013.
Insync, an alternative Google Drive client with multiple account support for Windows, Mac and Linux has just announced the release of Insync Business.
Insync Business transforms a company’s Google Drive storage into a document management system. It comes with a dashboard with centralized billing, member management and storage analytics.
Screenshots
The dashboard’s gives administrators a bird’s eye view of company and employee storage usage.
Insync Business dashboard with storage analytics
Members tab
Assigning licenses
MSI support
Insync Business supports MSI push install through GPO/Active Directory to efficiently get team members started with Insync on their Windows desktops. Detailed instructions on how to install Insync MSI via GPO/AD are available on the MSI tab in the business dashboard.
Pricing
Insync Business is only $10 per user / year (minimum of 5 users). Try Insync Business free for 15 days. No credit card required.
What’s next?
Insync Business is all about getting more out of your Google Drive storage and to that end, we think that adding “management-like” features are important. Examples include remote wipe, centrally excluding certain file types, etc.
Expect more of these types of features in the coming weeks and months.
Do more with cloud storage. Do more with Google Drive.
See all (9)
Do more with cloud storage. Do more with Google Drive.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@jakestein/etl-data-from-60-sources-into-snowflake-with-stitch-4ffd27876f4f?source=search_post---------389,"Sign in
There are currently no responses for this story.
Be the first to respond.
Jake Stein
Aug 31, 2017·3 min read
We are happy to announce that Stitch now supports native ETL into Snowflake. This integration enables users to load data from more than 60 data sources directly into a Snowflake data warehouse in minutes. Our customers have been giving us great feedback since we launched Snowflake integration into beta, and we’re excited to announce that it is generally available.
We built Stitch because modern cloud data warehouses are fundamentally different from the legacy, on-premises data warehouses of the past. These new technologies require a new model of ETL, and no data warehouse takes this model further than Snowflake. Stitch and Snowflake are a killer combination for your company’s modern analytics infrastructure because they align on separation of compute and storage, elastic scalability, support for rich data types, a cloud-built architecture, and a business model based on usage.
Snowflake has true separation of compute and storage, which enables far more flexibility than is possible with a traditional data warehouse. Unlike other data warehouses, when you need more storage, you can increase that without being forced to simultaneously increase compute since Snowflake keeps them separate. The ability to independently control compute and storage means that you you only pay for what you need.
Stitch enables Snowflake users to analyze all of their data by integrating with more than 60 data sources, including databases, SaaS tools, ad networks, and more. We also sponsor and integrate with the Singer open source ETL project, which makes it easy to get any additional or custom data sources into Snowflake. The combination of Stitch and Singer make sure that you leave no data behind, and Snowflake ensures that your data warehouse can handle anything you need to analyze.
Snowflake’s elastic scalability makes it fast and cost-effective to do in-database transformations. Traditional ETL — or extract, transform, and load — meant that data needed to be transformed prior to loading into the data warehouse. This made a lot of sense when your data warehouse was a physical piece of hardware sitting in your data center because provisioning additional resources might take weeks or months. With a data warehouse built for the cloud, you can scale it up or down, on demand. This allows users to load their raw data directly into the data warehouse immediately following extraction, with transformation happening afterwards, usually via SQL. This extract, load, transform workflow is known as ELT, and Stitch was built from the ground up to enable it.
Modern data sources such as web APIs generate data as JSON or XML, with complex nested data structures. Some data warehouses don’t support these types, and so we need to convert to a different data type or de-nest the data prior to loading. In Snowflake, these rich data types are natively supported and can be directly loaded and queried via SQL. This means customers get access to high-fidelity data from any data source, using the language they already know.
Both Stitch and Snowflake are cloud-native applications. You can be up and running in minutes, and there’s no maintenance to do, dials to turn, or hardware to manage.
We’ve also seen a growing trend of users who graduate to Snowflake after hitting the limits of other databases. Our destination switching feature makes it easy for existing Stitch users to take their existing integrations and start loading them into Snowflake right away.
Both Stitch and Snowflake are priced based on usage, which makes them economical for businesses of any size. Stitch offers an unlimited two-week free trial where you can load all of your historical data into Snowflake for free, and a free-forever tier for small data volumes where you can load up to five million records per month.
Start your free trial of Stitch today and load data into Snowflake in minutes.
CEO & Co-founder of Stitch
See all (113)
32 
32 claps
32 
CEO & Co-founder of Stitch
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@macsources/lima-ultra-personal-cloud-storage-review-8acbe8d2f334?source=search_post---------187,"Sign in
There are currently no responses for this story.
Be the first to respond.
MacSources
Feb 17, 2017·8 min read
Share on FacebookShare on TwitterShare on Pinterest
I am a fan of academic discussion and research. I have tended to review, re-review and over-research a piece of tech, to find the best at the best price etc. It is an entirely different feeling to be the one doing the testing, instead of being the one to nitpick the reviews. One of my most controversial reviews of 2016 was regarding the Lima device. I had the pleasure of talking with a few commenters regarding the device, who had dissenting opinions to my review. There were some weaknesses of the device, some that I personally took to the build team. The transfer speeds were too slow, data was stored in a proprietary format, the data did some weird things (upside down pictures and delayed access, some discussed loss of data, but I did not have that problem) the power cable was too big, just to name a few. My personal experience showed the device worked better than it did for some of the other testers/reviewers. Thus, I was excited to see the enhanced features of the Lima Ultra.
The Lima Ultra product arrived in a blue and white 6 3/4″ square box, displaying the white rectangular Lima Ultra device. In bold letters, you will find the slogan “The Personal cloud you will love.” Turning the packaging over, Lima promises to allow you to store your files at home on your own hard drive and to be able to access them from anywhere the internet can reach. You can access this data on any device and for a single upfront fee instead of monthly payments. The Lima Ultra currently supports IOS, Windows, android, and Linux (yes Tux made the cut this time). The product promises an enriched mobile media player, automatic backup of your device, and all with easy setup. Basically, connect the lima to your router, plug in an external hard drive (not included) then install the apps and sync. How hard can it be?
Removing the product from the box, you will notice a blue/white inner box with the surrounding slip cover. Within the smaller box, you will find the Lima Ultra device, which measures 3 5/8″ long by 1 7/8″ wide by 7/8″ thick and weighs 4.7 ounces. When compared to the last generation of lima device, the current design is flatter and more rectangular. Underneath the main device, you will find 2 rectangular cardboard boxes. One contains the 45″ all white flat ethernet cable. The other has a Universal power cable adaptor, with prongs for USA (Type A), UK (Type G), and what appears to be European Union (Type C). The power cord is much too short at 41 inches. When I say too short, I mean absolutely unreasonable. To make this worse, I absolutely am not a fan of the large boxy end. We all have experienced the power cord Tetris dilemma. Making a plug that blocks other ports or hinders the ability to use other gear is unacceptable in 2017. At a minimum, the company needs to allow the prongs to rotate, to be more accommodating to a variety of power options.
For a surge protector or power strip that is stacked vertically, the current power plug forces you to utilize the socket on the end of the chain. Guess what, other devices have to use that same socket. Additionally, the power cable is round and the ethernet cable is flat. They should have chosen one type of cable and stuck that instead of using a flat one and a round one. The small features truly make the difference in my eyes.
The instruction manual is written in English, French, German, Spanish, and Italian. Each language has a single page dedicated to the setup of the device, your own personal cloud. Place a movie into the movie folder and you can stream on the go. Place a song into the music folder and create your own Spotify. You do not have to rely on local storage to stream your library to any device you want. You can move the files on and offline with a single swipe. The device promises to be able to act as a remote to play your movies and videos on your TV using Chromecast (I do not have one to test). To combat one of the biggest complaints of the original Lima, you can link 2 Lima Ultra devices, in 2 different places and they will mirror each other. Lose 1 device and you still have a full backup. The storage is still proprietary, it is still dependent on lima tech.
If you are like me, your router ethernet ports are all utilized. I had to sacrifice the old lima port to make room for the new Lima Ultra. As an aside, I do have a TP-Link 5 port Gigabit switch that is on order and will be here any day. This should help me with my ethernet port real-estate issue. I plugged up a Toshiba 1 TB USB 3.0 Hard Drive (CANVIO CONNECT II), to the Lima Ultra. Once everything was connected, I navigated to install.meetlima.com, on both my PC desktop and my MacBook Pro. It took about 4 minutes to download the app. This was despite 87.67 Mbps download speed and 11.96 Mbps upload on my system, tested via speakeasy.net speed test. It seems that their servers were the slow point of the download, as I was able to update iTunes and to complete other activities without much lag. For a 62.2 MB download, this should have been done in about a quarter of the time. Once downloaded (4 minutes of my life waiting) it will ask you to select language (Deutsch, English, Español, Français, Italiano) and then click Install. Your screen will navigate to a “Preparing to Install: Setup is preparing to install Lima on your computer.” This took roughly 15 seconds to complete. You will need to restart your computer.
After my computer restarted, there were no instructions. I turned to the meetlima.com link, to the install.meetlima.com link and the support.meetlima.com link to find additional information. As I already had the original Lima, there was no other pop-up nor any type of instruction. I went to my taskbar in the bottom right of my desktop and saw the Lima icon. Since I already had the Lima installed I had to add the Lima Ultra. This may provide a different experience than if you never had the Lima. Within the app, select “Pair a Lima” and then follow the on-screen instructions.
While searching the Compatibility information through the website, I discovered that you can combine your Lima Ultra with the original Lima. This is no different than combining two Lima units. This will solve the issue of redundancy as this will allow you to backup your information on 2 different hard drives. This also promises faster uploads, faster backup, and faster transfer speeds. To transfer your music folder from iTunes to lima, go to file explorer and select your music folder and drag the music you are interested in into the Lima (you cannot currently transfer playlists). I started with a few movies stored in my iTunes movie folder.
Toy story 3 transferred in 20 seconds. Thor transferred in 17.8 seconds, Chronicles of Narnia transferred in 17.86 seconds. The same issue is realized on this lima that is on the other lima device, you cannot watch DRM protected movies. It is a waste to put them onto your device. The speed of this device is really astounding. In roughly 3 minutes all of my pictures from my phone were on the Lima Ultra 1 Tb drive. I transferred the entire kid rock album from iTunes to Lima Ultra in in 9 seconds. This played on the internal player of the device quickly. I then transferred my entire 42 GB iTunes media library over to the Lima Ultra device in 12:39 (total of 7131 items). The calculation I got was ~50 Mbps transfer. (0.05GB/second).
You can utilize this device like you would a hard drive. Drag and drop files from one point to another. Unlike a stand-alone hard drive, they are on the cloud and you can access them anywhere. You cannot then take the hard drive and plug it into a computer and utilize it like a portable drive. The speeds of this system are markedly increased, with a Quad-Core CPU clocked at 1.5GHz, Gigabit Ethernet port. In fact, it is reported to be ~40x faster than Lima 1.0. When compared to the original lima, the current lima Ultra will transfer at 240Mbps vs the original lima (16Mbps). Thus, 100GB should transfer in about 60 minutes, compared to 13 hours 53 minutes for the original. A .pdf, .xslx, .doc, 2mb picture should take about 1 second on the Lima Ultra vs 3 -8seconds on the original Lima. With my testing, I still got speeds less than the ideal conditions listed. Either way, it is definitely faster than the original.
There are many people who are upset about the Kickstarter campaign. I have had a lengthy discussion with people through my original lima coverage. I was not a backer, a Kickstarter campaign follower, nor am I a paid advertiser. I have the device, I am testing the device and this is my own assessment of how it worked for me. The original lima worked okay but was very buggy. The current Lima Ultra is much closer to what they promised than the original lima. I have read countless reviews on the devices, both original and Lima Ultra. I have transferred multiple files and tested my own home network and I have never obtained the promised speeds.
Additionally, everyone talks about streaming videos. Without talking about DRM stripping (I am not advocating doing that) the files in iTunes are protected and cannot be played in this manner. Personal videos will transfer to the photo roll and will play just fine. The data transfer speed is markedly superior on this device. Once I get my Ethernet switch I am going to combine my Lima Ultra and my Lima original.
The online help is actually quite good if you give it a chance. The Lima Ultra data storage, data accessibility appears to be much more stable over the past 2.5 weeks of testing. After doubling up on the devices, this should increase redundancy and eliminate the complaint of a single hard drive failure (THIS DOES HAPPEN). The team tried to gather information over the past few years to provide a better experience for the end user. As stated, there were issues with the Kickstarter and people are going to dislike the product for that reason alone. For the average tech user, this device will serve a purpose and seems to do it pretty well. More advanced NAS have more advanced features. I warn you, never trust a single hard drive with critical information. Redundancy is key. A fire can wipe out an entire home. So having multiple backups in a home, though safer, can still be lost. That is the beauty of the cloud. This can be a very powerful alternative to Dropbox and shutter fly, etc. As stated in my previous review, I do have a Synology Diskstation ds416J. Having worked with the NAS for over a year, I can say that the Lima Ultra is incredibly easy to use. A one-time fee of $129 dollars will save you the monthly dropbox fee.
Additionally, the data is yours and not visible to other sources. If needed, you can create an automatic offline storage of the lima through the Lima Preferences window advanced settings. Personally, it seems that the team at Lima has worked to shore up the weaknesses of Lima 1.0. I am pleased with the device and it works for me. Overall I rate the device at 4/5 stars.
Learn more about the product at meetlima.com/ultra. Follow on Facebook and Twitter.
Originally published at macsources.com on February 16, 2017.
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
"
https://betterprogramming.pub/java-process-messages-from-rabbitmq-and-upload-data-to-minio-cloud-b70ecd2e82be?source=search_post---------247,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kirshi Yin
Sep 10, 2020·7 min read
In this tutorial, we are going to learn how to process data from message broker RabbitMQ, and upload files to MinIO cloud storage. To make it more exciting, we are going to explore Epublib, an epub processor library to read…
"
https://medium.com/@tiwari_nitish/object-storage-in-practice-creating-a-reliable-data-store-9b424a22e8e?source=search_post---------372,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
May 10, 2016·5 min read
In my previous post we learnt the why and what of object storage. Specifically, we learnt why at all a new storage paradigm is required and what it does to alleviate modern unstructured data problems. We saw how object storage lets you access objects at application layer with simple API calls over HTTP(s) and does away with overhead of handling files in traditional way. Let us take a real life use case, and see how object storage can help make sure your application’s unstructured data is stored in an easy to get, resource light and reliable manner.
WordPress is one of most used content management platforms. As per wikipedia, over 60 million people have used WordPress to host their websites or blogs, and around 24% of the top 10 million websites on the Internet are based on WordPress. Naturally, WordPress websites are homes to millions of images and videos — unstructured data in its crudest form.
Currently, if you have your own WordPress installation, and you’re using WordPress out of the box, all the images, videos etc. are stored in a folder on your server’s file system. This is probably okay if you have a few images and expect few visitors. But as the site grows and you add new posts (with each generally including multiple videos and/or images) server file system fills up, sometimes even slowing the overall system.
If we could separate the file storage and the web server, like we generally separate web server and database server, it would free you up from worries of server failure, data backup and other overheads related to managing data.
So, the use case here is to override the default WordPress file upload process. So that instead of usual save to file system, putObject() is called to store the uploaded files in an object store. Later when files need to be retrieved, getObject() can get the files. This way you abstract away the storage details and focus on keeping your web server running.
Above use case takes an overly simplified view of how files can be simply put and get from the object server. In reality, things are bit more complex. Applications need file meta data to make some sense of a file. Metadata is generally a small chunk of structured data, i.e. predefined set of fields like author, uploaded timestamp, file type, file id and so on. Structured nature of the meta data means it is a good candidate to be stored in the database.
So, files sit in the object store, metadata goes to the database. And here is how it generally works — when the application needs to upload a file, it creates the metadata and stores it to the database, along with putting the file to the object store. Later when the file is needed, the application queries the database for the metadata and the based on the available info, gets the file.
Coming back to WordPress, it is not an exception, it uses the similar metadata based file handling even when files are stored in server’s file system. Each file uploaded to WordPress is treated as a post and has an ID assigned to it. In addition, there are several fields like author, date edited, title, etc. that are updated when a file is uploaded to WordPress. To be specific, the tables wp_posts and wp_postmeta are updated.
Going ahead with our plan to use object storage instead of file system storage in our WordPress installation, it makes sense to keep the metadata aspect of the files unchanged. We will just override the part where files are physically stored and retrieved from the local disk.
Armed with all the analysis, let us try to understand how to create a WordPress plugin to override the file upload process. The plugin should ensure that files are uploaded to the object storage server, while the metadata creation and storage remain unchanged. For the uninitiated, WordPress offers great deal of flexibility via plugins, you can easily extend or modify a feature with your plugin code. Here is a detailed tutorial how to create one.
To start with, you’ll need to call the add_action() WordPress method. This method helps you trigger a PHP function (from your plugin) when a specific event happens. WordPress provides several events for plugin developers that can be used as hooks to trigger specific functions. I have used the admin_init hook for now. As you get a hold of different functionalities required for the plugin and various hooks related to them, you can add other hooks.
add_action( ‘admin_init’, ‘wp_minio’ );
As you’d have guessed, wp_minio() is the function that will be triggered. Let’s see what it should look like. First of all we’ll use the minio-js library to call the Minio fPutObject() API. To do that, we can call a .js file with the file path and the filename (of file being uploaded) as parameters from wp_minio().
$execution_cmd = node fput-object.js ‘.$file_location.’ ‘.$file_name.’ ;
The fput-object.js file should handle the parameters passed and call the fPutObject() API with required arguments. Read more about the fPutObject() API here. This will upload the file to your object storage server. What follows is the routine process of creating file metadata, saving it to WordPress database and creating file thumbnails. But all this is nothing new, WordPress already does it. You can refer to the source code to see how it’s done.
Next part is make sure the files from the object server are accessible during the normal functioning of WordPress. To do this, we can frame file URL by concatenating the object server endpoint name, bucket name and the filename (given, the object permissions are set to at least authenticated user). You can get the bucket name using the API listBuckets() and the filename from the WordPress database. Just concatenate these with the endpoint to create the URL and let WordPress seamlessly access the files.
One of the key concerns of application developers is where do they dump all the files uploaded to their application. Till now this was generally a folder on the server’s file system. With open source object storage at their disposal, uploading and retrieving files is as easy as simple API calls. In this post we saw how can we implement object storage with metadata stored in tandem, to create a robust file storage mechanism that is easy to scale and manage in the long run.
We would love to know if you already use object storage in your applications or plan to do so and how, do let us know in the comments section!
While you’re at it, help us understand your use case and how we can help you better! Fill out our best of Minio deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
See all (472)
50 
3
50 claps
50 
3
Minio.io
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/serverless-etl-on-google-cloud-a-case-study-raw-data-into-json-lines-d20711cd3917?source=search_post---------216,"There are currently no responses for this story.
Be the first to respond.
I’m working on a task that consists of populating BigQuery tables with Tomcat and Nginx access log data. Every day, web servers upload new log files to GCS, containing raw data from the previous 24 hours. Data need to be converted into a format that is understood by BigQuery Jobs in order to be loaded into the tables.
I decided for the JSON Lines, or newline delimited JSON, to be the target format instead of CSV due to the nature of the data I’m handling. Since one can’t predict the data that will be transformed during this ETL process (e.g. URLs can include several UTF-8 chars, including commas), I decided not to rely on commas as delimiters to avoid problems.
So the goal is set: given standard access logs generated by web servers, they need to be transformed into newline delimited JSON files in order to fit BigQuery Jobs’ input arguments. Such jobs run later in the data warehousing process.
Tomcat logs, for example, are generated in the Common Log Format:
Which means:
And BigQuery Jobs need them as input in the JSON Lines format:
What I’ll describe in this article is how I designed and coded a solution to achieve this goal using Cloud Functions and Node.js Client for Google Cloud Storage. Although processing access logs is my main goal, the overall strategy may fit a wider range of ETL processes — with specific changes, mainly in the Transform phase. I’ll explain the solution and comment on the most important parts of the code I wrote to solve the problem. Sources are available on GitHub and links provided along with the text.
Please take a look at the below picture, it illustrates the designed workflow. Two Storage Buckets are used, and two Cloud Functions as well.
Since loading a JSON Lines formatted file into BigQuery (step 5) is a well-documented process, the present article is focused on the raw data to newline delimited JSON conversion (step 3).
The source bucket — intended for long-term storage — has the Standard Storage Class, but its files’ storage class is automatically changed to Archive when they turn 2 days old in order to decrease costs. The target bucket — short-term storage — also has the Standard Storage Class, but files are deleted when they turn 2 days old because they are no longer needed after being loaded into BigQuery.
Log files are composed of several lines — each of them representing a resource delivered by the HTTP server. The client for Google Cloud Storage allows us to download all content from a given file into memory for further processing, as follows:
A promise is returned by file.download() when there are no args in the call. The promise resolves with an array of buffers, and the first element contains all file contents. We need to convert it to a string before moving forward. Since the logs are encoded in UTF-8, calling toString() is enough in this case.
File download hint: download() can also be used to store the content into a local file. Please refer to the docs for details.
Stream reading hint: there’s a second option to read data from Cloud Storage: createReadStream(). In case you have data to be processed on a streaming pipeline, you might give it a try.
My first action to tackle the problem was to develop a javascript function that converts a raw log line into a JSON object. Basically, it uses a regular expression to break apart the values and assigns them to the JSON object’s attributes. Source code is available on GitHub.
Please notice the function returns a JSON string instead of a JSON object. This is the intended behavior so the code can be adapted to run an equivalent batch ETL process using Cloud Dataflow’s Storage Text to BigQuery template.
Now, let’s take a look at another important piece of the ETL code:
Splitting the previously downloaded raw content into an array of strings is also straightforward with the help of regular expressions.
Each element means a log entry that needs to be transformed into a newline delimited JSON. I’m using the ndjson library to help with this. ndjson.serialize() returns a transform stream that accepts JSON objects and emits newline delimited JSON. The transform job is done!
PasstThrough stream hint: you may use a transform function that does not return a stream. But, since having one is very useful in the next step, please consider using the built-in class stream.PassThrough to box the transformed data before moving forward.
— Wait, wait! What’s that LogParser class you’ve used?
— Good question! It’s simply a higher-level abstraction for the function that converts a raw log line into a JSON object, mentioned at the beginning of this section. The source code is available here.
The 3rd and last step is very simple: consists of storing the transformed content into a new file.
Once we have the JSON lines available in a stream, all we need to do is read such data and then write to a GCS file.
Cloud Storage client comes with the file.createWriteStream() method. As its name says, it creates a stream to write contents into a GCS file. Being a stream it can be nested to ndjsonStream, as shown in the above code snippet. Since writing the file is asynchronous, enclosing the pipe() call in a promise scope may help the Cloud Function to return properly when the process finishes, so I recommend this.
Please notice createWriteStream() accepts multiple options. I used resumable: false because I want the target files to be uploaded completely at once. Given their sizes are usually less than 10MB, this option makes them upload with a better performance.
The steps described in this article were designed to accomplish a serverless ETL workflow powered by GCP Cloud Functions and Javascript. At the end of the process, the transformed data is pretty much ready to be uploaded to BigQuery using a second Cloud Function as described in https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json.
Some words about performance: a 256MB-memory Cloud Function instance takes on average 8 seconds to process a 1MB file (~9,000 log entries). In this case, most of the processing time is spent by network operations as I have noticed.
I hope the article helps developers and data engineers with similar needs to design their own processes, by adjusting or improving what I’ve done so far.
The reference code is available on GitHub: https://github.com/ricardolsmendes/access-logs-dw-gcp-js (src/raw-to-ndjson-gcs-file-converter.js). A Cloud Function that uses it resides at https://github.com/ricardolsmendes/access-logs-ndjson-gcs-gcf.
Happy 2020, and that’s all ;)
Google Cloud community articles and blogs
108 
3
108 claps
108 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
head of data @ ciandt.com • hobbyist tech writer • dad • birder
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/cryptocurrency-hub/what-is-storj-the-decentralized-cloud-storage-b5e05519c95f?source=search_post---------108,"There are currently no responses for this story.
Be the first to respond.
In our age, trust in centralized cloud storage decreases. We are afraid that our data may leak or the servers may go down. Moreover, millions of people need access to the cloud, which requires high bandwidth that storage can’t always ensure. Storj solves this problem: it’s a decentralized cloud storage where files are split and spread across multiple nodes. No one but a user can access it (even the Storj devs). Let’s see how it works.
All cloud storage users have the same requirements: data should stay private, hackless, and always available. But in centralized cloud storage, it remains vulnerable. Google Drive or iCloud have access to your data which is stored on the servers that can go down or get hacked.
Storj (pronounced as “storage”) stores your data across a network of geographically diverse nodes. Your files are encrypted and then spread across 80 nodes. For retrieval, only 29 pieces of data need to be available, so it’s nearly impossible for your file to go offline or get accessed by hackers.
Storj was one of the first projects to implement decentralized data storage. The platform was conceived in 2014 and launched in 2018. Storj raised $30 million in an ICO held in 2017.
1. VeChain Price Prediction 2021
2. Why I Don’t Waste Time Investing In Bitcoins
3. How to launch a token on Stellar Blockchain Network?
4. How and Where to Buy DWS (DWS) — An Easy Step by Step Guide
We described the flow from the perspective of a Storj customer. However, anyone having some free disk space can become a node, too.
Storj is open-source and lets partners interact with their API.
STORJ is a utility token used for payments on the platform. In the Storj network, you can rent the disk space up to 150 GB/month for free. For more, pay $4/TB and $7/TB as a bandwidth cost. If you host files as a node, you will be rewarded with STORJ. The average payouts per month are $1.5/TB for disk space, $20/TB for egress bandwidth. See more about Storj pricing here: for customers and for nodes.
In 2017, STORJ migrated to the Ethereum blockchain. Now, it functions in the ERC-20 format.
Buy STORJ in two cases: if you want to leverage the decentralized cloud or if you to invest in Storj.
The tokens are traded on a wide variety of crypto exchanges, including Binance, Coinbase, and Huobi. These platforms allow you to buy this cryptocurrency for another crypto in a matter of seconds. However, you need to sign up first — it takes time, and you have to entrust the platform with your data.
This is not the main risk, though. Storing your tokens on centralized exchanges may be insecure, as crypto stored there doesn’t really belong to you. In CEXs, you don’t keep your private keys, which goes against the proverb “Not your keys — not your crypto”. Here’s the risk: exchanges sometimes limit the fund’s withdrawals, and if the platform gets hacked, your money may be stolen.
On ChangeNOW, you can buy STORJ without registration, and you don’t have to store your cryptocurrency anywhere but in your wallet to do so. You send us some BTC, ETH, or one of the other 200+ assets, and we send you STORJ in a matter of 5 minutes. ChangeNOW partners with many liquidity providers and at the moment of the swap, picks the one that offers the best exchange rate. We don’t have hidden fees — all payments are already included in the estimated rate.
Any wallet that stores ERC-20 tokens works. For this article, we picked three that would fit best for different situations and needs.
This hardware wallet with top-notch security is almost impossible to hack. Your private keys are stored offline on a flash drive device, so you will need it every time you want to send your tokens. If you are going to store your STORJ rather than trade or actively use it, Ledger is the best choice.
This is a top users’ choice for making the most of Ethereum. MetaMask is a software wallet available for iOS, Android, and for browsers such as Chrome, Firefox, Brave, and Edge. If you already have or plan to get other ERC-20 tokens, MetaMask is the best.
This is a multi-asset wallet that supports hundreds of different assets. Available for mobile and desktop, Trust Wallet is a good choice if you want to manage all your crypto portfolio in a single place.
While concerns regarding the privacy of our data are rising, Storj has created decentralized cloud storage — a platform where data is stored across the network and is unavailable to anyone but its owner. STORJ token is used in the network to pay for the file hosting and to reward the nodes. You can start using Storj right now for free. And, whenever you want to buy STORJ to invest or to leverage the decentralized cloud, feel free to do so on ChangeNOW.
Crypto & NFTs | Psychology | Mysticism
53 
53 claps
53 
Discover how the Game of Life will be Played in the Metaverse
Written by
ChangeNOW is an instant cryptocurrency exchange service for limitless crypto conversions. We support over 200 coins and are account-free! https://changenow.io/
Discover how the Game of Life will be Played in the Metaverse
"
https://medium.com/linedevth/%E0%B9%81%E0%B8%8A%E0%B8%A3%E0%B9%8C%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%AA%E0%B8%9A%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%93%E0%B9%8C%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%9E%E0%B8%B1%E0%B8%92%E0%B8%99%E0%B8%B2-liff-x-firebase-%E0%B8%82%E0%B8%B6%E0%B9%89%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B8%94%E0%B8%B1%E0%B8%81%E0%B8%8A%E0%B8%B1%E0%B9%88%E0%B8%99%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%84%E0%B8%B8%E0%B8%93%E0%B8%95%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%A3%E0%B8%B9%E0%B9%89-f04df046c2c3?source=search_post---------339,"There are currently no responses for this story.
Be the first to respond.
ช่วงปลายปีที่ผ่านมา ผมมีโอกาสได้เข้าไปมีส่วนร่วมในทีมที่จะผลักดันให้มี Internship Program หรือโครงการนักศึกษาฝึกงานให้เกิดขึ้นใน LINE ประเทศไทยเป็นครั้งแรกในชื่อโครงการว่า LINE ROOKIE โดยหน้าที่ความรับผิดชอบของผม ก็คือการพัฒนา LIFF(LINE Front-end Framework) ในการรับสมัครน้องๆนักศึกษา
โจทย์หลักของ LIFF ตัวนี้คือต้องรองรับการเข้าถึงทั้งจากแอป LINE และ browser ภายนอก ซึ่งผมได้ใช้เวลาหมกตัวไปกับโปรเจคนี้นาน 15 วัน จนได้ขึ้นโปรดักชั่น โดยตลอดเส้นทางในสายการผลิต ผมได้เรียนรู้เทคนิคและเคล็ดลับต่างๆในการพัฒนามากมาย ที่เชื่อว่านักพัฒนาหลายท่านน่าจะยังไม่รู้
บทความนี้จะมาบอกเล่า เรื่องที่คุณต้องรู้ ก่อนเอา LIFF ที่มี Firebase อยู่เบื้องหลังขึ้นโปรดักชั่น โดยจะขอแบ่งกลุ่มเรื่องที่คุณต้องรู้ออกเป็นหัวข้อใหญ่ๆตามนี้
สิ่งหนึ่งที่คุณต้องรู้ในการพัฒนา LIFF เพื่อรองรับ external browser คือ จะมีบางฟังก์ชันที่ไม่สามารถทำงานได้ใน external browser ได้แก่
นอกจากนี้บางฟังก์ชันก็ไม่จำเป็นต้องเรียกใช้ใน LIFF ตัวอย่างเช่นคำสั่งที่ใช้ login และ logout เนื่องจาก LIFF จะทำการ login ให้อัตโนมัติทุกครั้งหลัง init อยู่แล้ว ดังนั้นปุ่ม login และปุ่ม logout ก็ควรจะแสดงเฉพาะใน external browser
ดังนั้นเพื่อประสบการณ์ใช้งานที่ดีของผู้ใช้จึงควรคำนึงเรื่องนี้ด้วย โดยในโปรเจคนี้ผมจึงใช้คำสั่ง liff.isInClient() ในการเช็คว่าผู้ใช้เปิดจาก LIFF หรือ external browser และอาจใช้งานร่วมกับ liff.isLoggedIn() เพื่อตรวจสอบว่าผู้ใช้ได้ login เรียบร้อยแล้วหรือไม่
หาก flow การทำงานของ LIFF ที่คุณกำลังพัฒนาอยู่ มีส่วนที่ต้องใช้ chatbot เข้าไปเกี่ยวข้อง เช่น การส่งข้อความยืนยันหลังจากสมัครผ่านทาง chatbot เราก็สามารถใช้ฟีเจอร์ชื่อ Bot Link เพื่อแนะนำให้ผู้ใช้ของเรา add ตัว chatbot เราเป็นเพื่อนตั้งแต่ก่อนเริ่มใช้งาน LIFF ได้
โดยให้เข้าไปที่ Developer Console เลือก LINE Login Channel ที่เราสร้าง LIFF ไว้ จากนั้นไปที่ tab ชื่อ Basic settings แล้วเลือก chatbot ที่ต้องการให้ LIFF แนะนำผ่านเมนู Linked OA ตามภาพ
จากนั้นให้ไปที่ tab ชื่อ LIFF แล้วเลือก LIFF เป้าหมายของเรา เมื่อเลือกแล้วจะเจอเมนูชื่อ Bot link feature ตามภาพ
ให้เราเลือก Normal หรือ Aggessive เพื่อให้ LIFF แนะนำ chatbot ของเรา โดยแต่ละประเภทจะแสดงวิธีแนะนำ chatbot ของเราต่างกันตามภาพ
นักพัฒนา LIFF น่าจะเคยเจอปัญหาที่แก้โค้ดแล้ว หน้าเว็บไม่เปลี่ยน สาเหตุมันมาจาก cache นี่หละ โดยผมจะขอแบ่ง cache ในการพัฒนา LIFF ออกเป็น 2 แบบ
Cache จาก URL ของ CSS และ JScache ประเภทนี้คือ cache ระดับ URL ของไฟล์ที่เรา import เข้ามา ไม่ว่าจะเป็น CSS หรือ JS ซึ่งวิธีแก้ง่ายๆ คือให้เราเติม query string ที่เป็น dynamic ตามท้าย URL นั้นๆไป
Cache จาก URL ของ HTMLในการเปิดหน้าเว็บบน LIFF ทุกครั้งมันจะมีการ cache ผลลัพธ์ของ URL นั้นๆไว้ระยะเวลาหนึ่ง ซึ่งหากต้องการให้เห็นผลลัพธ์ใหม่ในทันที เราจะต้องเข้าไปอัพเดท query string ตามท้าย URL ที่เราผูกไว้ใน Developer Console
ตั้งแต่ข้อนี้ไปจะพูดถึงฝั่งหลังบ้านนะครับ โดยถ้าย้อนกลับไปดูข้อ 1.2 จะเห็นว่าผมแนะนำให้เก็บ Access Token ที่มาจาก LIFF เอาไว้ เพราะว่ามันช่วยให้บริการที่เราพัฒนาปลอดภัยมากขึ้น
เมื่อไรก็ตามที่มี request ไปที่ API เราสามารถตรวจสอบได้ว่า Acess Token ตัวนี้ยัง valid หรือไม่ อีกทั้ง response ที่ได้จากการ verify ก็เอามาเปรียบเทียบกับ Channel ID ได้ด้วย ว่า Access Token ตัวนี้เกิดจากบริการของเราจริงๆ
นอกจากตัว Access Token ที่ได้จาก LIFF จะสามารถ verify ได้แล้ว เราก็ยังสามารถใช้มันมาเพื่อดึงข้อมูลผู้ใช้ออกมาได้ด้วย ดังนั้นเพื่อความสดใหม่และปลอดภัย ผมก็แนะนำให้ดึงข้อมูลผู้ใช้จากระบบหลังบ้านผ่าน Access Token ตัวนี้ครับ
ถึงแม้ Access Token ที่ได้จาก LIFF จะถูก revoke ทุกครั้งที่ปิด LIFF แต่ในกรณีที่เรารองรับ external browser ด้วยมันจะไม่ revoke ให้อัตโนมัตินะ ดังนั้นผมแนะนำว่าเมื่อผู้ใช้ทำ transaction เสร็จ ที่ระบบหลังบ้านจะต้องมีการยิง API เพื่อ revoke ตัว Access Token ออกทันที เพื่อป้องกันผู้ไม่หวังดีเอา Access Token ไปยิง API ซ้ำๆ
สำหรับนักพัฒนาที่ติดปัญหาว่า จะ debug ตัวเว็บของเราอย่างไรเมื่อเราทดสอบหรือใช้งานบน LIFF ซึ่งจากการที่ผมได้ไปหมกตัวในกลุ่ม LINE Developers Group Thailand ทำให้ผมทราบว่า มันมีตัวช่วยชื่อ vConsole อยู่ ซึ่งการติดตั้งก็ง่ายมากๆ เพียงแค่ import ตัว JS SDK เข้าไปแค่นั้นเอง
github.com
เนื่องจากโปรเจคนี้เป็นโปรเจคที่เข้ามาแบบไม่ทันตั้งตัว ผมจึงเลือกใช้บริการ Host ที่พร้อมใช้งาน ติดตั้งง่าย มีเสถียรภาพ ปลอดภัย และ deploy สะดวก นั่นก็คือเหตุผลที่เลือก Firebase Hosting มาอยู่เบื้องหลัง LIFF ตัวนี้
เราสามารถกำหนดอายุของ cache ในไฟล์ต่างๆผ่านค่า Cache-Control ใน headers ของไฟล์ firebase.json ได้
โดยสิ่งที่คุณจะต้องรู้คือเมื่อกำหนดค่า Cache-Control ไปแล้ว browser ของ client คนนั้นๆ จะไม่ไปอ่านไฟล์จาก server อีก จนกว่า cache ของไฟล์นั้นๆจะ expire ดังนั้นการกำหนดค่า Cache-Control ควรทำต่อเมื่อพร้อมจะ deploy ขึ้นโปรดักชั่นแล้วเท่านั้น และอายุที่ตั้งขึ้นอยู่กับว่า เรามีโอกาสแก้ไขไฟล์นั้นบ่อยแค่ไหน
เรื่องที่หลายคนอาจมองข้ามในการพัฒนา API คือการจำกัดวงผู้ที่มีสิทธิ์เรียกใช้งาน(ยกเว้นเปิด public ให้ใครเรียกก็ได้) โดยเราควรระบุ origin ว่ามี domain ไหนสามารถ request มาแล้ว API จะ response กลับไปบ้าง เช่น
นอกจากนี้ควรจำกัด method ที่ให้ request เข้ามาด้วย เช่น
ข้อจำกัดหนึ่งของการ submit ฟอร์มไปยัง API ที่สร้างจาก Cloud Functions for Firebase ก็คือการส่งข้อมูลฟอร์มที่กรอกไปพร้อมกับ binary ของไฟล์ นั้นไม่สามารถทำได้ โดยทางเลือกใช้ได้มี 2 ทางด้วยกัน
ใช้ SDK ที่เป็น javascript ของ Cloud Storage for Firebaseในการอัพโหลดไฟล์จากหน้าเว็บ แล้วจึงส่ง path ของไฟล์ไปกับ form-data เพื่อเก็บลงในฐานข้อมูล แต่วิธีนี้จะต้องลบไฟล์ออกในกรณีที่ submit ฟอร์มไม่สำเร็จด้วย เพื่อไม่ให้เกิดไฟล์ขยะใน storage
แปลงไฟล์ที่ผู้ใช้อัพโหลดเข้ามาเป็น Base64วิธีนี้จะทำที่ฝั่ง client ก่อนการ submit ฟอร์ม โดยเอาข้อมูล Base64 ที่ได้จากการแปลงไปใส่ใน input ประเภท hidden ไว้ จากนั้นเมื่อ submit ก็ให้ส่งข้อมูลทั้งหมดผ่าน form-data ไป วิธีนี้ handle ง่ายกว่า แต่อาจเจออาการหน่วงๆบ้างเมื่ออัพโหลดไฟล์ขนาดใหญ่กับมือถือรุ่นเก่าๆ
หลายคนต้องไม่รู้เรื่องนี้แน่ๆ ว่าจริงๆแล้วเราสามารถส่งข้อมูลไปยัง API ที่สร้างจาก Cloud Functions for Firebase ได้สูงสุดครั้งละไม่เกิน 10MB ดังนั้นหากฟอร์มของคุณเปิดให้อัพโหลดไฟล์เข้ามาด้วยวิธีการแปลง Base64 คุณจำเป็นต้องคำนวนว่าไฟล์ขนาดใหญ่ที่สุด + ข้อมูลในฟอร์ม จะต้องมีขนาดไม่เกิน 10MB อย่างกรณี LINE ROOKIE ผมกำหนดไว้ที่ 7MB เป็นระยะที่ผมทดสอบแล้วว่าปลอดภัย
เรื่องนี้ฟังดูเหมือนเรื่องเล็ก แต่จริงๆแล้วมันมีรายละเอียดที่ค่อนข้างสำคัญ โดยในโปรเจคนี้ผมจะให้นักศึกษาสามารถอัพโหลดไฟล์ resume เข้ามา และด้านล่างนี้คือสิ่งที่คุณต้องคำนึง
ชื่อไฟล์ที่จะ save ลง local temp ต้องเป็น dynamicการตั้งชื่อไฟล์เป็น static เช่น temp.pdf นั้นอาจก่อให้เกิดปัญหาเมื่อมี transaction เข้ามาหนาแน่น เพราะไฟล์อาจถูกเขียนทับกันได้
ชื่อไฟล์ควรเป็นค่า Unique ที่เดาได้ยากคุณไม่ควรเปิดโอกาสให้คนอื่นเดา path ไฟล์ได้ง่าย ดังนั้นชื่อไฟล์ที่ตั้งควรเป็นค่าที่ไม่ซ้ำและเดาได้ยาก และหากคุณกำลังคิดว่าจะตั้งชื่อไฟล์ด้วย userId หรือ key ใดๆที่สามารถเชื่อมโยงมาถึงตัวผู้ใช้ได้ ผมก็แนะนำให้ hash ตัว key เหล่านั้นก่อน เพื่อความปลอดภัย เช่น
ควรระบุนามสกุลไฟล์เท่าที่เราอนุญาตเท่านั้นเริ่มจากที่คุณต้องตรวจสอบนามสกุลหรือ content-type ของไฟล์ที่วิ่งเข้ามาก่อน ว่าตรงกับที่คุณอนุญาตหรือไม่ และก่อนอัพโหลดไฟล์ก็ต้องมั่นใจว่านามสกุลที่ตั้งเป็นนามสกุลที่อนุญาตแล้วเท่านั้น เพื่อป้องกันการ execute ไฟล์ใน server ของเรา
ตอนแรกคิดว่าปัญหาการอัพโหลดไฟล์เข้า API ที่สร้างจาก Cloud Functions for Firebase จะหมดแล้ว ปรากฎว่าเมื่อทดสอบอัพโหลดไฟล์ที่มีขนาดใหญ่กว่า 5MB ก็ยังไม่เข้าอีก โดย error log ที่ได้มาประมาณนี้
ผมนี่รีบเปิด stackoverflow เลยว่ามันแก้ยังไง และผมก็ได้วิธีแก้มาละ โดยเราต้องระบุ property ที่ชื่อว่า resumable โดยมีค่าเป็น false ไปกับคำสั่งในการอัพโหลดไฟล์ แล้วคุณจะโหลดไฟล์ขนาดมากกว่า 5MB ได้
หลังจากเราอัพโหลดไฟล์ขึ้น Cloud Storage for Firebase แล้ว ลำดับถัดไปผมก็จะเอา path ของไฟล์นั้นไปบันทึกลงในฐานข้อมูลด้วย
ก่อนอื่นคุณจะต้องทราบก่อนว่า ณ วันนี้หากเราอัพโหลดไฟล์ขึ้น Cloud Storage ผ่าน Firebase-Admin SDK โดยปกติไฟล์ที่อัพโหลดไปจะมีสถานะเป็น private นั่นคือได้ URL มาคนข้างนอกก็เปิดไม่ได้ ดังนั้นวิธีที่จะทำให้ URL เป็น public แบบง่ายที่สุดก็คือการเพิ่ม meta data ชื่อ firebaseStorageDownloadTokens และกำหนดค่า uuid ลงไปใน property สำหรับการอัพโหลด ตัวอย่าง
คราวนี้ก็ได้เวลาดึง URL จากการอัพโหลดละ โดยให้คุณสร้าง URL ตามรูปแบบนี้ แล้วคุณจะสามารถเข้าถึงไฟล์ที่คุณอัพโหลดไปได้ทันที
การกรองข้อมูลที่ผู้ใช้ส่งเข้ามาจากฝั่งหน้าบ้านนั้นดี เพราะช่วยลดภาระของระบบหลังบ้านเราได้ แต่อย่างไรนักพัฒนาก็ห้ามไว้ใจการ validate ข้อมูลเพียงฝั่งหน้าบ้านเท่านั้น เพราะจริงๆผู้ไม่ประสงค์ดีสามารถศึกษาและปลอมแปลงวิธีการ request ไปยังระบบหลังบ้านของเราได้อยู่ดี
ดังนั้นการ validation ที่สมบูรณ์ควรมีครบทั้งทั้งฝั่งหน้าบ้านและหลังบ้าน
ในภาษาโปรแกรมมิ่งทั้งหลายก็มีตัวช่วยในการตรวจสอบข้อมูลที่เข้ามาอยู่มากมาย แต่ผมพิสูจน์แล้ว ว่าฟังก์ชันหลายๆตัวนั้นเป็นมาตรฐานทั่วไปโดยเฉพาะเหมาะกับฝั่ง western ซึ่งอาจไม่เหมาะกับข้อมูลที่เฉพาะเจาะจงหรือ เหมาะกับคนไทย
ดังนั้นการใช้ Regex ถือเป็นตัวช่วยที่ดี ที่ช่วยให้เราเขียนโปรแกรมได้สั้น และข้อมูลตรงตามความต้องการมากที่สุด
เรื่องง่ายๆที่หลายๆคนพลาด การตรวจสอบค่าที่เข้ามาด้วย regx ไม่ว่าจะเป็น alpha หรือ alphanumeric ทำไมจะต้องระวังค่า undefined ด้วย มาดูตัวอย่างด้านล่างนี้
ผมทดสอบโดยใส่ input ที่หลากหลายเข้าไป ก็พบว่า regex ทำงานถูกต้อง จนกระทั่ง ผมทดสอบโดยการไม่ส่ง input ใดๆเข้าไปเลย ปรากฎว่าโค้ดด้านบนมัน return เป็น true จ้า(งานเข้าเลย)
ซึ่งผมก็คิดอยู่ตั้งนานกว่าจะถึงบางอ้อว่า เมื่อเราไม่ส่งค่าใดๆมากับ input มันจะได้ค่าเป็น undefined และ regex มันมองว่า undefined คือ alphanumeric ที่มีตัวอักษรระหว่าง 4–20 ตัวพอดี ดังนั้นเคสนี้เราจะต้องเช็คค่า undefined ไว้ด้วย
ข้อความ response ที่กลับมาจาก server ในกรณีที่ fail หรือ error บางครั้งเราก็ไม่จำเป็นต้องบอกรายละเอียดผู้ใช้งานทั้งหมดว่าสาเหตุเกิดจากอะไร เช่น “Access Token ของคุณไม่ถูกต้องนะ” เพราะบางครั้งมันเป็นการชี้ช่องให้ผู้ไม่ประสงค์ดีทราบว่าระบบหลังบ้านของเราตรวจสอบอะไรอยู่บ้าง
ดังนั้นในกรณีที่มี error จากการ validate ที่เราไม่ต้องการให้ผู้ไม่หวังดีทราบ เราอาจใช้ข้อความกลางๆเช่น “Oops! Something went wrong” เป็นต้น
ในกรณีที่คุณสร้าง LIFF ที่รองรับ external browser คุณอาจต้องคำนึงว่าผู้ใช้งานของคุณอาจเปิดเว็บดังกล่าวในคอมพิวเตอร์สาธารณะได้ เช่นร้านคอมพิวเตอร์ และโดยทั่วไปค่า auto-complete ของ HTLM Form จะเปิดไว้
ดังนั้นเพื่อป้องกันไม่ให้ผู้ใช้คนอื่นๆ เห็นข้อมูลที่ผู้ใช้คนก่อนหน้าได้กรอกไว้ ก็แนะนำให้ปิดค่า auto-complete ของ form ไว้นะครับ
หมายเหตุ: เครื่องมือที่ผมใช้วัดความเร็วในการเปิดหน้าเว็บ LINE ROOKIE คือ Google PageSpeed Insights
เรื่องนี้ผมเลือกไว้เป็นข้อแรกของการเพิ่มความเร็วของเว็บเลย เนื่องจากปัญหาการโหลดเว็บช้า น่าจะมาจากขนาดไฟล์รูปเป็นสำคัญ ซึ่งจากการได้ทำโปรเจคนี้ ผมได้ลองใช้เครื่องมือหลายตัว และพบว่า tinypng.com เป็นเว็บที่เอาไว้ compress ภาพทั้ง JPG และ PNG ที่ช่วยลดขนาดภาพไปได้เยอะ แถมคุณภาพของภาพก็ยังดีแบบแยกด้วยตาไม่ออก
tinypng.com
ในการโหลดหน้าเว็บขึ้นมา ปกติมันจะไปโหลดรูปทั้งหมดในหน้ามาด้วย ซึ่งบางครั้งผู้ใช้ก็ไม่ได้ไถไปจนเจอรูปทุกรูป ดังนั้นเทคนิคของ Lazy load of screen images จะมาช่วยให้ browser โหลดภาพเฉพาะที่ผู้ใช้เห็นเท่านั้น วิธีทำก็ลองดูตัวอย่างด้านล่างนี้ได้เลย ง่ายมั๊กๆ
หากเว็บที่คุณพัฒนามีการใช้ custom font วิธีการที่จะโหลดหน้าเว็บให้เร็ว เราจะใช้เทคนิคที่แสดง font ของ system ก่อน และเมื่อเว็บโหลดตัว custom font เรียบร้อยแล้ว ก็ให้เปลี่ยนมาใช้ custom font แทน
หลังจากที่เราได้เขียน CSS และ JavaScript จนเสร็จแล้ว ก็ให้เราเอา souce code ทั้งหมดไปทำการ minify โดยผมเลือกใช้เครื่องมือออนไลน์อย่าง cssminifier.com และ javascript-minifier.com แต่แนะนำว่าให้สร้างเป็นไฟล์ใหม่นะครับ เพราะกรณีที่เราจะกลับไปแก้ไข จะได้เข้าใจและแก้ไขโค้ดได้ง่าย
หวังว่าบทความนี้จะเป็นประโยชน์กับผู้ที่พัฒนา LIFF ด้วย Firebase Hosting และ Cloud Functions for Firebase ที่มีแพลนจะเอางานขึ้นโปรดักชั่น และขอฝาก LINE ROOKIE ไว้ด้วยนะครับ มีลูกบอกลูก มีเพื่อนบอกเพื่อน ให้มาสมัครกันได้ตั้งแต่วันนี้ถึงสิ้นเดือนกุมภาพันธ์ 2020 แล้วพบกันใหม่บทความหน้า ราตรีสวัสดิ์พี่น้องนักพัฒนาชาวไทย
Deliver world-class developer experiences and learning through LINE API
148 
1
Thanks to Tan Warit. 
148 claps
148 
1
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Closing the distance. Our mission is to bring people, information and services closer together
Written by
Technology Evangelist at LINE Thailand / Google Developer Expert - 🔥Firebase
Closing the distance. Our mission is to bring people, information and services closer together
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/how-to-backup-a-bigquery-table-or-dataset-to-google-cloud-storage-and-restore-from-it-6ef7eb322c6d?source=search_post---------19,"There are currently no responses for this story.
Be the first to respond.
BigQuery is fully managed and takes care of managing redundant backups in storage. It also supports “time-travel”, the ability to query a snapshot as of a day ago. So, if an ETL job goes bad and you want to revert back to yesterday’s data, you can simply do:
However, time travel is restricted to 7 days. There are situations (playback, regulatory compliance, etc.) when you might need to restore a table as it existed 30 days ago or 1 year ago.
For your convenience, I’ve put together a pair of Python programs for backing up and restoring BigQuery tables and datasets. Get them from this GitHub repository: https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/tree/master/blogs/bigquery_backup
Here’s how you use the scripts:
To backup a table to GCS
The script saves a schema.json, a tabledef.json, and extracted data in AVRO format to GCS.
You can also backup all the tables in a data set:
Restore tables one-by-one by specifying a destination data set
The scripts use the BigQuery command-line utility bq to put together a backup and restore solution.
bq_backup.py invokes the following commands:
It saves the JSON files to Google Cloud Storage alongside the AVRO files.
bq_restore.py uses the table definition to find characteristics such as whether the table is time-partitioned, range-partitioned, or clustered and then invokes the command:
In the case of views, the scripts store and restore the view definition. The view definition is part of the table definition JSON, and to restore it, the script simply needs to invoke:
Enjoy!
After I published this article, Antoine Cas responded on Twitter:
He’s absolutely right. This article assumes that you want a backup in the form of files on Cloud Storage. This might be because your compliance auditor wants the data to be exported out to some specific location, or it might be because you want the backups to be processable by other tools.
If you do not need the backup to be in the form of files, a much simpler way to backup your BigQuery table is use bq cp to create/restore the backup:
Yes! The data is stored in Avro format, and the Avro format employs compression. The two JSON files (table definition and schema) are not compressed, but those are relatively tiny.
As an example, I backed up a BigQuery table with 400 million rows that took 11.1 GB in BigQuery. When storing it as Avro on GCS, the same table was saved as 47 files each of which was 174.2 MB, so 8.2 GB. The two JSON files occupied a total of 1.3 MB, essentially just roundoff. This makes sense because the BigQuery storage is optimized for interactive querying whereas the Avro format is not.
BigQuery can export most primitive types and nested and repeated fields into Avro. For full details on the Avro representation, please see the documentation. The backup tool specifies use_avro_logical_types, so DATE and TIME are stored as date and time-micros respectively.
That said, you should verify that the backup/restore works on your table.
Google Cloud community articles and blogs
142 
3
142 claps
142 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Data Analytics & AI @ Google Cloud
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/ontologynetwork/ontology-partners-with-zaico-to-empower-its-inventory-management-system-increasing-traceability-29a604bf6123?source=search_post---------278,"There are currently no responses for this story.
Be the first to respond.
Ontology, the public blockchain specializing in decentralized identity and data, has signed a Memorandum of Understanding (MOU) with leading cloud inventory management software company, ZAICO. Ontology aims to provide solutions to ZAICO’s inventory management platform to help increase traceability, transparency, and trust.
ZAICO makes inventory management of large amounts of stock faster and easier. The free cloud application for inventory management is accessible through web and smartphone applications for iPhone, iPad, and Android. ZAICO’s cloud storage system allows for inventory to be counted in real-time, allowing its employees to work anywhere at any time.
By adopting Ontology’s blockchain technology, specifically its attestation service, ZAICO will develop greater control over how its information is shared and utilized. Together, the companies are building an anti-falsification solution to promote accurate and traceable inventory management. As a result of the partnership, ZAICO’s Japanese clients will soon enjoy secure, trustworthy blockchain-based inventory management at a low cost.
Commenting on the partnership, Li Jun, Founder of Ontology, said, “Inventory management has historically been plagued with lengthy, unreliable processes and lists housed on excel sheets which are prone to error and leave businesses vulnerable to malpractice. Now, via the use of blockchain in its inventory management systems in partnership with Ontology, ZAICO will increase traceability, transparency, and trust, in its services. Our recent partnership with AP.LLC will help facilitate integration with ZAICO and other third parties, as we further promote decentralized identity solutions within inventory management.”
Tamura Toshihide, Founder and CEO of ZAICO Inc., said “ZAICO develops and sells innovative cloud inventory management software with the vision to improve societal efficiency by collecting, arranging, and providing information on goods around the world, with the power of technology. With this partnership, we are thrilled to be able to provide improved and enhanced features powered by blockchain-based solutions which benefit our clients.”
About Ontology
Ontology is a high performance, public blockchain specializing in decentralized identity and data. Ontology’s unique infrastructure supports robust cross-chain collaboration and Layer 2 scalability, offering businesses the flexibility to design a blockchain that suits their needs. With a suite of decentralized identity and data sharing protocols to enhance speed, security, and trust, Ontology’s features include ONT ID, a mobile digital ID application and DID used throughout the ecosystem, and DDXF, a decentralized data exchange and collaboration framework. For more, visit ont.io.
About ZAICO
ZAICO is a cloud inventory management software that has been supported by a total of 120,000 users. It is a free to start service that can be used on PCs, tablets, iPads, iPhones, and Android applications, and supports the smooth development of businesses by optimizing inventory management as well as reducing the number of personnel and costs involved in inventory management. A series of research support services, including ZAICO, were accredited by the Ministry of Education, Culture, Sports, Science and Technology (MEXT) as a “Research Support Service Partnership Accreditation System” on April 1, 2020. ZAICO continues to expand and enhance its services and functions to contribute to the improvement of Japan’s research environment, the promotion of science and technology, and the fostering of innovation.
You can find more details on our website for all of our decentralized solutions across identity and data, or keep up with us on Twitter. Our Telegram is for discussion, whereas the Telegram Announcement is designed for news and updates if you missed Twitter!
A high performance, open-source blockchain specializing in digital identity and data.
26 
26 claps
26 
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Written by
Active project domain: https://ont.io/
Ontology is a high performance, open source blockchain specializing in digital identity and data. ONTO: http://onto.app/downloadpage/TW  Telegram: http://t.me/OntologyNetwork
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/twigcp-grpc-web-is-here-data-safe-and-compliant-in-cloud-storage-private-dns-zones-and-fd0610b437be?source=search_post---------91,"There are currently no responses for this story.
Be the first to respond.
Here are the main announcements for this past week :
From the “TIL” department :
From the “best title for a technical piece” department :
From the “Google Cloud community is awesome” department :
From the “incomplete and likely biased, but always fun comparisons” department :
From the “Beta, GA, or what?” department :
From the “all things multimedia” department :
This week’s picture is taken from the Private DNS Zones announcement :
That is all for this week!-Alexis
Google Cloud community articles and blogs
12 
12 claps
12 
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google Cloud Developer Relations
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/the-technical-practice-of-distributed-locks-in-a-storage-system-4128936efe97?source=search_post---------255,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Mar 10, 2021·9 min read
Mutually exclusive access to shared resources has always been a problem that many business systems need to solve. In distributed systems, a general-purpose distributed lock solution is often used. This article discusses the implementation principle, technical selection, and specific practices of distributed locking on Alibaba Cloud Storage.
In a standalone environment, when shared resources cannot provide the mutual exclusion capability to prevent data damage caused by multiple threads or processes simultaneously reading and writing the shared resource, the system needs a mutual exclusion capability provided by a third party. In most cases, it is the kernel or a class library that provides mutual exclusion capability. The following figure shows that the process obtains an exclusive lock from the kernel or class library. Then, the process can exclusively access shared resources. After standalone environments evolve into distributed environments, we need a distributed service that provides the same function through which different machines obtain locks. The machines that obtain the locks can access the shared resources exclusively. These kinds of services are collectively called distributed lock services, and those locks are called distributed locks.
We can abstract the concept of distributed locks. A distributed lock must be a resource that can provide concurrent control and output an exclusive state:
Lock = Resource + Concurrency Control + Ownership Display
Take a common standalone lock as an example:
Spinlock = BOOL + CAS (Optimistic Lock)
Mutex = BOOL + CAS + notification (Pessimistic Lock)
Spinlock and Mutex are both Bool resources. The atomic CAS instruction is: When the current value is 0, set the value to 1. A machine will hold the lock if it succeeds, and the machine will not hold the lock if it fails. For example, AtomicInteger does not provide the Ownership Display. Although resource (Integer) and CAS are also provided, since prompt ownership is not explicitly prompted, AtomicInteger is not regarded as a lock. Of course, the Ownership Display can be considered as the packaging of a service provision form.
In a standalone environment, the kernel has a “God’s eye” and can know if a process is alive. When a process fails, the kernel can release the lock resources held by the process. However, this becomes a challenge in a distributed environment. We need to provide locks with a new feature to cope with various machine failures or crashes: availability.
As shown in the following figure, any service that provides the three features can provide the capabilities of distributed locks. The resource can be a file or a key-value (KV) pair, which is operated through atoms, such as a file or KV creation, indicates the ownership through the successful creation result, and ensures the availability of the lock through TTL or session.
Distributed locks are divided into the following types based on the security of lock resources:
A distributed system based on asynchronous replication faces the risk of data loss (lock loss) and is not secure enough. Such a system often provides fine-grained lock services through the TTL mechanism. It applies to time-sensitive services, need to set a short validity period, have short-term tasks, and are subject to relatively limited impact on the business if the loss of locks occurs.
A distributed system based on the Paxos protocol ensures multiple copies of data through the consensus protocol and has high data security. It often uses the lease (session) mechanism to provide coarse-granularity lock services. This system has certain usage requirements and applies to services that are sensitive to security, need to hold locks for a long time, and cannot afford the loss of locks.
Alibaba Cloud Storage has accumulated a lot of experience in improving the correctness of distributed locks, the availability of locks, and the efficiency of lock switching.
Exclusion is a basic requirement of distributed locks. Multiple user occupancy is not allowed. Then, how do storage and distributed locks avoid this situation?
Each lock of the server is bound to a unique session, and the client ensures the validity of the session by regularly sending heartbeats, ensuring the ownership of the lock. When a heartbeat cannot be maintained, the session and the associated lock node are released, and the lock node can be preempted again. The key point is how to ensure the synchronization between the client and the server so the client can perceive an expired session. The following figure shows the validity period of the session is maintained at the client and server. The client starts timing (S0) when the client sends the heartbeat, while the server starts timing (S1) when the server receives the request. This ensures that the client expires before the server. After the user creates the lock, the core worker thread can determine whether there is enough validity period before the core worker thread performs the core operation. In addition, we determine the validity period based on the system time instead of wall time. The system clock is more accurate and will not move forward or backward. The clock may have a millisecond-level deviation in the second-level system time. In the worst-case scenario, if NTP jumps, only the clock speed can be modified.
Have we achieved perfect distributed lock exclusion? No, there is a situation where the access mutual exclusion of business based on the distributed lock service may be compromised. As shown in Figure 9, the client tries to preempt a lock at S0 and successfully obtains a lock at the back end at S1. Therefore, a validity period window of the distributed lock is generated. Within the validity period, the client performs an access operation on the storage at S2. This operation is completed quickly, and then at S3, the client determines that the lock is still in the validity period and continues to access the storage. However, this operation takes a long time and exceeds the expiration time of the distributed lock, and the distributed lock may have been obtained by another client. Therefore, two clients may operate on the same batch of data at the same time. There is a small chance this situation could occur.
For this scenario, the specific solution is to ensure a sufficient validity period window of locks when the client operates on data. Of course, if the service provides a rollback mechanism, the solution is more comprehensive. This solution is also used by storage products that use distributed locks.
A better solution is if the storage system introduces I/O Fence capability. Here, we must mention the discussion between Martin Kleppmann and antirez, the author of Redis. Redis introduced redlock to prevent the loss of locks caused by asynchronous replication. This scheme introduced the quorum mechanism. The system needs to obtain the lock of the quorum. This ensures the availability and correctness to the greatest extent, but it still has two problems:
The problem of unreliable wall time can be solved through non-wall time MonoticTime (Redis still relies on wall time), but only one system in a heterogeneous system cannot ensure the correctness of data. As shown in Figure 10, client 1 obtains the lock and garbage collection (GC) occurs during data operations. The ownership of the lock is lost when GC is complete, resulting in data inconsistency.
Therefore, two cooperative systems are needed to complete mutually exclusive access. The IO Fence capability is introduced into the storage system, as shown in Figure 11. The global lock service provides global auto-incrementing tokens. The system returns token 33 and brings the token to the storage system after client 1 obtains the lock. Then, GC occurs. The system returns token 34 and brings the token to the storage system after client 2 obtains the lock. The storage system rejects the requests with the smaller token. When client 1 re-writes data after a long time of full GC recovery because the token recorded by the storage layer has been updated, the request with token 33 will be rejected. This protects data.
This is in line with the design idea of the Apsara Distributed File System, which is an Alibaba Cloud distributed storage platform. The Apsara Distributed File System supports write protection similar to I/O Fence. It introduces Inline File types to cooperate with Seal File operations, providing write protection similar to I/O Fence. First, the SealFile operation is used to close the files on the cs to prevent the previous owner from continuing to write data. Second, InlineFile prevents the previous owner from opening the new file. These two functions also provide token support in the storage system.
Storage distributed locks ensure the robustness of locks through continuous heartbeats, so users do not have to pay high attention to availability, but abnormal user processes may continuously occupy the locks. A session blacklist mechanism is provided to make sure locks are released securely and scheduled eventually.
The user can query the session information and add the session into the blacklist to release the lock held by a process in the event of suspended activity. After that, the heartbeat cannot be maintained, which will cause session expiration and the safe release of the lock node. Here, we do not force delete the lock but choose to disable the heartbeat for the following reasons:
When the lock held by a process needs to be rescheduled, the lock owner can delete the lock node. However, if the lock owner encounters an exception, such as process restart or machine failure, then in order to have the lock put into a new process, you must wait until the original session expires before you can successfully preempt the lock in a new process. A distributed lock has a session lifecycle of dozens of seconds by default. It may take a long time before the lock node can be re-preempted when the process with the lock unexpectedly exits with the lock not being released.
We must compress the session lifecycle and enable faster heartbeats to improve the switching precision. This imposes more access pressure on the backend server. We optimized the structure to compress the session further.
We also provide CAS lock releases based on specific business scenarios. For example, if the Daemon process finds that the lock-holding process fails, it releases the lock by CAS so other processes can immediately preempt the lock. For example, a unique ID of a process is stored in the lock node. The locks that are no longer used will be forcibly released and put into preemption again. This method can completely avoid the waiting time required to preempt the locks after the process is upgraded or restarted unexpectedly.
Alibaba Cloud Storage provides a comprehensive distributed lock solution that has been tested in the medium to long term use of many Alibaba Cloud products. It is stable, highly reliable, and provides SDKs of multiple programming languages and RESTful integration solutions.
Distributed locks provide mutually exclusive access to shared resources in a distributed environment. Distributed locks are used to improve the efficiency of services or implement the absolute mutual exclusion of accesses. When accessing the distributed lock service, you need to consider issues, such as access cost, service reliability, switching accuracy, and correctness of distributed locks, to use distributed locks correctly and reasonably.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
16 
16 claps
16 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/s3-failure-highlights-the-need-for-decentralized-services-like-storj-ab30a5769cf8?source=search_post---------218,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Storj Labs
Mar 7, 2017·3 min read
S3 Failure Highlights the Need for Decentralized Services like Storj
Amazon S3, the world’s largest cloud storage platform, went offline last week, taking down a large percentage of internet services with it. Trello, Quora, Heroku, and Slack are just a handful of the many services that were affected. It was also reported that even IoT connected thermostats and lightbulbs stopped working. We now know the cause of the outage to be a mistyped command during routine server maintenance. My heart goes out to the engineer that committed that error (as an engineer I’ve definitely made mistakes like this), but this unfortunate outcome simply should not have been possible.
As the world becomes more and more connected, and we become more reliant on platforms like S3, we will face increasingly negative impacts in our lives when they fail. What might be a minor inconvenience today, like not being able to change the temperature on your Nest, might be your connected self driving car slamming into a highway construction barrier because it lost access to a data feed tomorrow. While these types of terrible failures may be far in the future, that is clearly the direction we are headed, and the impact that these kinds of service failures have today is very real. No one would build a piece of critical infrastructure like a bridge, only held together with a single bolt whose failure could bring the whole structure tumbling down. And we shouldn’t build internet infrastructure with single points of failure either.
We built Storj because we wanted to create a more robust, distributed system for object storage. After all, the cloud is just someone else’s computer, so why does it have to be Amazon´s? Though there are still some problems to solve to improve security, privacy, durability, and ease of use, Storj is now fully functional: We have resolved all the fundamental problems, while continually optimizing the code to find the best solutions.
Because Storj doesn’t host infrastructure, but instead uses a distributed and decentralized network of devices, we offer a very special value proposition that no other traditional storage network can offer. Should our object storage service fail completely, users would still be able to retrieve their data. This is possible today as long as the application keeps copies of network locations and authorization keys. We are working on building this into our various tools and libraries by default, so every application will have this functionality without requiring any additional developer/user effort. For extra safety, the encrypted shards of data are distributed redundantly on different types of hardware and networks which are located in many different countries, keeping crazy singular failure events from disrupting functionality.
The future is bright with innovation, but let’s make sure we build those innovations on platforms that really can’t fail, rather than those that just promise not to. Let´s provide services that don’t cease to function due to a single error, but keep functioning perfectly even when the entire system has failed. This is the promise of decentralized, distributed, and crowd-sourced platforms and one we are turning into reality here at Storj.
Shawn WilkinsonCEO/CTO of Storj Labs Inc.
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
See all (1,648)
102 
3
102 claps
102 
3
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/swlh/loading-data-into-bigquery-from-cloud-storage-complete-guide-e212f5c2db6?source=search_post---------29,"There are currently no responses for this story.
Be the first to respond.
In this article, we will build a streaming real-time analytics pipeline using Google Client Libraries. We will create a Cloud Function to load data from Google Storage into BigQuery. This is a complete guide on how to work with tables, different file…
"
https://medium.com/@3commastutorials/storing-information-on-the-blockchain-ecd6da74a403?source=search_post---------317,"Sign in
There are currently no responses for this story.
Be the first to respond.
3Сommas Blog
Aug 20, 2020·5 min read
Currently, the most popular non-local information storage solution is cloud storage like Google Disk, Dropbox, Mega, and databases like MySQL and MongoDB. However, companies may control the content of these repositories, and your information may be censored.
In this article, we will review the ways of storing information on the blockchain, as well as the pros and cons.
User interaction with the database
In practice, user interaction with the repository comes down to three steps:
1. A user uploads data to a company’s server using a desktop or web application;
2. The company imports information about new data to the information processing center;
3. To gain access to their data, the user sends a request to the data center, which provides information access.
Undoubtedly, this model has several advantages:
• CRUD — an acronym for four basic functions used when working with databases: create, read, update, and delete. This is a standard model of user interaction with the database.
• Often the speed of information processing depends only on the user’s Internet speed.
Otherwise, such centralized repositories are not the most reliable file storage. Information about the files you upload is transferred to third parties, and as such centralized servers often get targeted by hackers.
Data repositories on a blockchain
Using a blockchain for record information is not the best idea since a block, a structural unit of the blockchain, has a limited size. For example, the size of the bitcoin block is 1 megabyte; thus it is not possible to send a file larger than 1 megabyte to the blockchain. We also have to take into account the cost of sending this file.
Let’s have a look at the block # 637352 of the Bitcoin network.
A fee for adding transactions to the block equaled 0.47462040 BTC or $4372. Let’s assume that this block is “full,” thus equal to 1 megabyte. It turns out that to send a 1Mb file, we need to pay more than $4000. We also have to remember that the file will be visible to every network participant.
However, the Bitcoin blockchain is excellent for sending short messages. The average sentence in English consists of 15–20 words, where one word, on average, consists of 6 characters. In total, we get about 140 characters in one sentence or 140 bytes of information.
As a result, we get $0.5 per message + commission for transferring funds.
Peer-to-peer file systems
The most popular peer-to-peer file system is IPFS or the Interplanetary File System. This blockchain technology is built on the BitTorrent protocol, which involves breaking files into fragments and storing multiple copies of those files on the system participants’ computers.
This method has several advantages:
• The file will be downloaded by the users who are interested in it;
• Popular files are downloaded/distributed very quickly;
• The data is address-dependent, so it is impossible to fake the internal contents of the file;
• It is a peer-to-peer solution.
Reviewing shortcomings, we can note that files can be uploaded to the network only if the user is online, and as such, a system serves only static data. Besides, one can access the file only if they know its name.
In this scheme, the blockchain is used as an intermediary that connects participants and is responsible for verifying the authenticity and integrity of the files.
Decentralized cloud storage
These are ordinary cloud storage options similar to Dropbox. Except that the data is not placed on the company servers, but on the devices of users who rent them out.
Using such solutions, network participants do not have to be constantly online to send information. It is enough to upload the file to the cloud storage once. Such storages are stable, fast, and have huge capacities.
However, they are only suitable for serving static data and do not support search by content. Moreover, they are not free, as participants rent equipment from each other.
Storj and Sia
These companies operate on the trading platforms principle. They promise cheap, prompt, and safe storage; however, this does not mean that their services are cheaper than the ones of such giants as Google, Amazon, or DropBox. It’s just that they get profit not only from rental rates but also from commissions for transactions generated by downloading and extracting data.
The operation scheme of Storj and Sia is, in fact, intermediation between those who lease hard drives and those who rent them. Blockchain is used as a register of transactions, financial settlements, and authentication of files in databases. At the same time, the user data itself is stored outside the blockchain and can be deleted or become inaccessible any time if the lessors decide to delete the files or simply disconnect their device from the network.
Filecoin
Filecoin is a platform based on the same ideas as Storj and Sia. Their difference is only in two details:
Using these innovations, as well as a unique consensus algorithm that stimulates an increase in network disk space, Filecoin intends to outrun Google and Amazon in terms of storage capacity in the next few years.
Maidsafe
The main idea of ​​Maidsafe is to create a fully encrypted P2P network that will be a database for the anonymous exchange of information through encrypted layers. It’s an analog of Tor for cloud storage. This will be possible through the three elements of Maidsafe:
Conclusion
Using a blockchain for information storage has some disadvantages. For example, the speed of downloading a file from Sia storage will be significantly lower than from Dropbox. However, this is compensated by the security of user data.
There currently is an ongoing development to speed up the file transfer and increase the reliability of decentralized file storage. The Filecoin project is working in this direction and has already invested $275 million in improving the infrastructure in 2017.
Trading blog. Tools, bots, strategies.
Trading blog. Tools, bots, strategies.
"
https://medium.com/@alibaba-cloud/security-and-monitoring-practices-alibaba-cloud-storage-solutions-part-2-8a9a7c5f3266?source=search_post---------169,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 7, 2021·6 min read
By Shantanu Kaushik
In the previous article of this series on security and monitoring practices with Alibaba Cloud Storage solutions, we talked about security and went through how important and effective the Access Control provided by Resource and Access Management (RAM) service is. We also talked about data encryption and how server-side and client-side encryption works.
In this article, we are going to cover how important service monitoring is and how the collected metrics can account for increased data durability and enhanced user experience.
The Object Storage Service (OSS) provides metrics using the monitoring service. You can measure the running status and performance logs of the system using the monitoring service. It also offers a service to track requests, collect business trend statistics, analyze the usage data, and alert/diagnose system problems in real-time.
These metrics can be distributed among basic, performance, and billed usage metrics. These can also be classified as user-level metrics and bucket-level metrics.
The real-time monitoring service with the Alibaba Cloud OSS is capable of providing minute-level metric data collection and aggregation. This enables a deep analysis of the collected metric to evaluate and change any business strategy in place. Real-time monitoring can record traffic fluctuations and peak hours to better provision ECS, storage, and other resources accordingly.
Minimizing costs related to your storage needs is one of the essential practices to keep while maintaining and leveraging cloud infrastructure. Billable usage metrics can help you determine how.
The Alert service for Alibaba Cloud OSS can be used to set alerts based on the metrics collected by the monitoring service and with metrics using the custom monitor. This service can be used to set custom profiles to include multiple cloud resources, application groups, or a single instance. Cloud Monitor can be configured to send alerts using DingTalk or emails.
You can set rules to set up alerts in host monitoring with a detection frequency of once per minute. You can also set up a threshold-based alert rule to monitor performance usage metrics to monitor resources in Alibaba Cloud services.
Cloud Monitor is a service that is extremely useful to monitor Alibaba Cloud resources used by internet applications in real-time. It is a one-stop, user-friendly monitoring solution that has set industry-standards in cloud monitoring. Alibaba Cloud Monitor can help monitor IT infrastructures and perform dial-testing for public network quality.
Cloud Monitor can check on events, custom metrics, and logs to present a detailed report for the user to define future strategies, improve system availability, and reduce the Operations & Maintenance (O&M) costs of your IT infrastructure.
Let’s take a look at an illustration depicting the architecture of Alibaba Cloud-Cloud Monitor.
Cloud Monitor has been seamlessly integrated with the Alibaba Cloud computing universe. As soon as you create your Alibaba Cloud account, the cloud monitor is automatically activated. It is available within your dashboard, allowing you to easily keep an eye on how your Alibaba Cloud products and services are running.
You can manage the entirety of the monitoring service from your panel. You can set alert rules and process different forms of monitoring data visualizations to better understand your service metrics. The Cloud Monitor OpenAPI can be called to access the Alibaba Cloud OSS metrics data from anywhere. You can integrate this API and use the SDK resources to build on it.
The collection of metrics of your storage service will provide you with data that will outline the exact performance and reliability levels of your service. Cloud Monitor will help you gather metrics for any Alibaba Cloud services deployed using an Alibaba Cloud account.
Let’s take a look at an indicative chart that shows a drop in user availability. Whenever the availability drops, it indicates a failure of some requests or is indicative of an unoptimized system.
If your system is sending unavailability alerts, you need to optimize your system and track the monitoring data to comprehend the situation with urgency.
You need to keep an eye out for failed requests by analyzing how many traffic requests were received. This number should be equal to the valid request count.
Cloud Monitor enables performance monitoring using metrics that indicate latency. It shows the maximum latency and average latency time needed for API operations. Another factor is the end-to-end latency (E2E) indicator. To calculate the overall time taken by a request, the cloud monitor indicates the time needed to read, execute, and send responses. If there is any network delay due, the Cloud Monitor will help you with a diagnosis.
Fluctuation in performance depends on a lot of factors. In between server-side and client-side, there are a number of factors that might slow down performance. To diagnose performance problems, the first step is to set up a baseline of performance for a specific business scenario that you are trying to diagnose.
Afterward, you can start to identify the factors that are associated with performance. It could be TCP overload or not-so-capable base network architecture on client-side or various other factors. However, the most important step is setting up a viable baseline to start this diagnosis. After you have set a baseline, you can begin to monitor and standardize responses that you receive from the cloud monitor as indicators of performance bottlenecks and begin to fix those.
In this article, we covered how monitoring is important for any solution to work properly. The Object Storage Service (OSS) is deeply connected with the Cloud Monitor and provides critical metrics to make changes to your system for better performance acceleration and enhanced reliability.
In the next article, we will continue explaining how error diagnosis, troubleshooting, and logging functions work. We will also cover Alibaba Cloud OSS sandboxing scenarios and how it helps with overall data reliability for enterprises.
1. Security and Monitoring Practices — Alibaba Cloud Storage Solutions — Part 3
We will discuss Error Diagnosis, logging, and OSS sandboxing scenarios.
2. Apsara File Storage NAS — What and How?
We will discuss the complete architecture and usage scenarios with the Apsara File Storage NAS solution from Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/the-capital/identifying-the-winners-in-crypto-storage-54fd64175475?source=search_post---------243,"There are currently no responses for this story.
Be the first to respond.
About your author: CryptoQuestion is an independent platform providing free resources for cryptocurrency investors. From an on-demand Q&A service to online courses, from books to our weekly Moonshot Monday podcast. Visit us at www.cryptoquestion.tech
"
https://medium.com/@memority/the-first-part-of-memoritys-functionality-analysis-b2709c7219e5?source=search_post---------326,"Sign in
There are currently no responses for this story.
Be the first to respond.
Memority
Jul 14, 2018·2 min read
Nowadays storage system market shows a good tendency of growth. According to Statista, the global enterprise storage systems market doubled from 5.6 billion U.S. dollars (in second quarter of 2009) to 13.6 billion U.S. dollars (in the fourth quarter of 2017). The most well known giants of data storage market are Google Drive, Dropbox and Amazon S3. Dropbox claims 300 million users on their site as of May 2014. Google Drive has 240 million users as of September 2014 (based on Thenextweb).
We present the first part of Memority’s functionality analysis, explaining three advantages that a data owner gets when using the platform. Memority platform is compared with the most popular centralized data storages — Google Drive, Dropbox and Amazon S3. Comparison will follow the next required characteristics: storage decentralization, creation of several file copies in independent storage and high payments to hosters for providing space for users files storage.
Dropbox, Amazon S3, Google Drive — storages are centralized. If you use centralized servers, based on which traditional cloud storages work, then your files and files of other users are stored in one specific place, which increases the probability of their loss, removal, modification or viewing by third parties. The Memority storage system is a collection of independent repositories from around the world, including not only specialized hosting providers, but also computers of ordinary users.
Dropbox, Google Drive — do not create copies of files. Amazon S3 — can create copies of files, but does not guarantee this. Storing only one copy of the file increases the risk of losing the stored data. A decentralized storage system consisting of potentially unreliable host computers has a high risk of data loss or removal if the file is stored in a single instance. The Memority system stores several copies of data (10 by default) on unrelated storages.
Dropbox, Amazon S3, Google Drive — have only a few independent storage units. If there are problems on any of them, files or access to them disappear for many (sometimes most) users. Not attractive for hosters. A small number of active hosters leads to an increase in the level of centralization. This increases the risk of access to copies by third parties and reduces the level of security of storage. The Memority’s mechanism of hosters motivation is aimed at connecting as many computers as possible to maximize the number of independent repositories.
To sum up all this information, storage decentralization is a unique feature of Memority platform. Only Memority has a large number of independent hosters in the network due to the mechanism of hosters motivation.
A blockchain-based platform for a completely decentralized, super-secure storage of valuable data.
17.5K 
17.5K 
17.5K 
A blockchain-based platform for a completely decentralized, super-secure storage of valuable data.
"
https://blog.sia.tech/meet-sia-the-most-viable-non-financial-application-of-blockchain-technology-afe6e7412a25?source=search_post---------210,"With our latest v1.4.1 release, the Sia decentralized cloud storage network is ready for the next stage of adoption.
The world is enamored by blockchain technology’s potential. When properly implemented, blockchains enable direct, trustless transactions between parties. They will ultimately allow us to make our marketplaces more efficient and competitive — saving us money, empowering individuals, and fostering innovation.
Bitcoin is the quintessential financial application of blockchain technology. Our current financial system is filled with fee-taking intermediaries like banks, credit cards, transfer agents, and lenders. We relied on these third parties because it was impossible for individuals and businesses to transact without trust; we therefore outsourced our trust to third parties.
Bitcoin allows us to transact directly, without needing to trust our counterparties. Instead we trust that the Bitcoin blockchain stores an accurate record of all transactions.
Today our intermediaries serve as gatekeepers, extracting value from our financial markets and creating significant barriers to entry. Over time, Bitcoin will eliminate most of these third parties and enable trustless transactions and exciting new global competition.
The cloud storage industry is very similar to the financial industry in that trust is a necessity; we therefore rely on trusted third parties to store our data. Amazon, Google, and Microsoft control the majority of the cloud storage market, and their prices are suspiciously similar at about $20 per TB per month. Today it is possible to purchase a refurbished 1 TB hard drive for about $20 — so buyers are essentially paying Amazon each month the entire cost of the hard drives used to store their data!
This is ludicrous, but even more ludicrous is the cost to download or stream data from these platforms. Amazon, Google, and Microsoft charge $60+ per TB of bandwidth, meaning that users pay a steep price to get their data off the cloud. (Of course, if you want to keep your data on Amazon, it is virtually free to access it from other Amazon services. This is how centralized cloud providers lock you into their ecosystems.)
This restricts how much data buyers can afford to upload, causing potentially valuable data to be thrown away or never stored at all. Social media, wearable devices, self-driving vehicles, IoT devices, and countless other innovations will produce exponentially greater amounts of data over the next decade. Amazingly, by 2025 we are expected to create 463,000 PB of data per day.
This also restricts the kind of applications and business models we can build. It is difficult to release a new video streaming product, for example, because bandwidth is so expensive. Streaming a single 4k movie can cost $1–3 dollars on Amazon S3, which is simply unaffordable for new startups and severely restricts business model innovation.
What would happen if we could eliminate the need for trust and allow anyone in the world to offer storage and bandwidth at any price they want?
We believe that Sia will usher a cloud storage renaissance, reducing storage prices by 10x and bandwidth prices by 100x, leading to significant increases in the amount of data stored by humanity and enabling previously impossible bandwidth-hungry applications. Put simply, Sia is the next step in cloud storage, and it will obsolete the existing Amazon/Google/Microsoft monopoly.
Sia is a global cloud storage marketplace that uses the Sia blockchain to enable trustless, peer-to-peer transactions between users. Anyone can download the Sia software to rent storage space and anyone can offer storage capacity as a host. Today, storage prices on Sia are about $1 per TB per month and bandwidth prices are less than $1 per TB.
How is this possible? By removing intermediaries, Sia creates a “perfect” marketplace, forcing hosts to compete for business. This drives prices to their absolute lowest and makes it impossible for traditional providers like Amazon to ever compete. Hosts on Sia do not have to worry about building and maintaining enormous datacenters, employing thousands of employees, and marketing their services. They only need to worry about providing reliable storage capacity to renters on Sia.
When uploading data to Sia, the software breaks files up into redundant pieces, encrypts each piece, and then uploads the pieces to dozens of hosts across the world. Renters and hosts enter into custom smart contracts called file contracts that are stored on the Sia blockchain. These serve as cryptographic SLAs, dictating prices for storage and bandwidth, contract length, uptime requirements, and more. Because renters and hosts both pay into file contracts in advance, and files are stored redundantly, bad behavior is disincentivized and uptime is on-par with Amazon.
There are many “decentralized” cloud storage projects in the blockchain space, but Sia is years ahead. No existing project matches Sia’s progress or feature set, and they will likely never catch up. Below are some ways Sia stands out:
With the new release of v1.4.1, Sia is now production-ready for cloud storage and is beginning to compete with traditional providers. Version 1.4.1 introduced seed-based file recovery, allowing users to backup and recover their files using just their 29-word seed. Sia today allows renters to upload 20+ TB per node and reaches bandwidth speeds of 300 Mbps up/down. Next, Sia will gain blazing-fast video streaming, integrations with popular applications, and file sharing features to allow Sia to compete head-to-head with Amazon S3.
Sia is the most viable non-financial application of blockchain technology and will be the first to scale to tens of thousands of users and beyond. The blockchain industry deserves to see useful, scalable, and realistic applications. Sia’s team will continue to set a strong example.
Give Sia a try today! Download v1.4.1 from our website and join our community on Discord. Learn how to get started with our user guide.
Decentralized storage — Sia, Skynet, and cryptocurrency.
931 
2
Thanks to Matthew Sevey, Steve, and Manasi Vora. 
931 claps
931 
2
Written by
Nebulous COO, building Sia and Obelisk
Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by
Nebulous COO, building Sia and Obelisk
Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://betterprogramming.pub/firebase-storage-gotchas-63a7cfef7677?source=search_post---------217,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Dal Busco
Feb 4, 2020·3 min read
When was the last time you reverted several working days?
I recently took some time to make the assets, moreover the content, private for every user of our web editor for presentations, DeckDeckGo.
About
Write
Help
Legal
Get the Medium app
"
https://betterprogramming.pub/how-to-upload-files-to-firebase-cloud-storage-with-react-and-node-js-e87d80aeded1?source=search_post---------16,"Sign in
There are currently no responses for this story.
Be the first to respond.
Claire Chabas
May 18, 2020·8 min read
I’ve come to work on several projects that required the implementation of a file uploader and always found it was not that easy.
Handing a file format opens different questions: “What exactly is a file? What are we…
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/taking-payments-to-the-next-level-with-raiden-e36fbc99f64f?source=search_post---------219,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Dec 12, 2018·5 min read
At Storj, we are building a next-generation decentralized cloud storage platform. We make decentralization possible by enabling anyone to rent out their extra hard drive space, similar to how you would rent out an extra room on Airbnb, or your car on Turo. One of the challenges we face is implementing a payments system that is accurate, timely, and can scale to millions of users.
In October, our CTO Philip Hutchins published an article in Forbes about creating scalability on the Ethereum blockchain, which detailed some of the difficulties and challenges we’ve experienced sending token payments to our hundreds of thousands of storage node operators. Every month, Storj pays the nodes for the bandwidth and storage capacity provided. At the time of peak usage of our V2 Storj network (we are currently working on a ground up rewrite called V3), we were sending payments to over 150,000 nodes in more than 180 countries and territories. The current version of the Ethereum network can processes about 15 payments per second, meaning if the network only handled our STORJ payments, it would have taken the entire Ethereum network over four hours to complete all those transactions. At the peak of our payment processing, our transactions accounted for about 8% of all Ethereum transactions.
The transaction fees associated with these payments also had a major impact on our business. The Ethereum network fees reached a peak of $5 a transaction, which, when multiplied by the number of payments (150k), amounts to a total of $750,000 in fees. We prefer not incurring the equivalent of one and half 2018 Lamborghini Aventadors in fees just because we have a particularly busy month in Ethereum.
Thankfully, with the creativity and hard work of many at Storj, we were able to get by. Overcoming this obstacle also required the Storj community to exhibit understanding and patience, especially during peak saturation times when several payments were delayed until transaction fees became reasonably low enough for us to initiate payouts. Although the endeavor was ultimately successful, this is a process that no one is eager to repeat going forward.
Although this solution is in place, we’re not out of the dark yet. We foresee both rapid growth of the Storj network and inevitable periods of peak level Ethereum fees on the horizon. These conditions can once again interfere with our ability to process payments in a timely and efficient manner (and frictionless payouts are a core feature of our network). With that in mind, the Storj Strategy Team (whose mission is: solving tomorrow’s problems today), undertook development of our next-generation payment system.
Exploring Micropayments
After extensive research, we found that a micropayment-based approach holds the most potential for success. Micropayments are a way to pay for many small transactions between parties. A nice example of this is buying a drink at a bar. You allow the bartender to swipe your credit card, and you get your beer. Then, when your friends arrive, and you buy 3 beers for them, and then 5 more for the next wave of friends, the drinks are tallied, but the transaction is deferred. So by agreeing to this open tab, which you will later close with one transaction, multiple purchases which occurred over a period of several hours are conveniently reduced to a single transaction. In terms of the Storj network, this means we will keep track of numerous payments off-chain, and then sum them into an on-chain transaction that is of course trustless and resistant to fraud.
During our research, we learned that here are many talented teams working on solutions around blockchain-based micropayments, including L4, Lightning Network, and Raiden. When we began evaluating the different platforms as a possible solution for our particular concerns, we were profoundly impressed by all of their work
The Raiden platform stood out because it was furthest along in development, and allows us to easily pay in our native STORJ tokens (the token used for all node storage and bandwidth payments). We were honored to meet the Raiden team at the Web3 Summit and Devcon 4 this year. We are quite grateful for their development support and educational assistance, as well as their feedback and help so far.
Scaling Tests
As we stated before, we’ve already successfully sent as many as 150,000 payments a month on the current Storj network (V2). So we decided it made sense to begin testing Raiden with 1,000 payments, then ramp up to 10,000, and finally 100,000 payments, which approximates the scale that we previously established. The scripts we used for these tests can be found here and we invite anyone who needs to process massive volumes of payments to check them out. We hope our efforts will be in some way helpful to you.
For our initial round of tests, we set up two Raiden nodes, created and funded a micropayment channel between them, and then used these nodes to send a defined number of payments to completion. We found that, after this process was finished, we could send a payment between the nodes at an average rate of about one every 1.5 seconds. Here are the raw results:
1,000 payments in 25 minutes 10,000 payments in 4 hours and 11 minutes 100,000 payments in 44 hours and 30 minutes
Although we encountered some issues early on in our tests, we made fairly simple adjustments, and were able to scale our throughput up to production-level on the current network.
Next Milestones
This demonstration proves we can send 100,000+ payments over a single channel with minimal friction. Moving forward, we will open channels and send payments to 100,000 active nodes. This test will be significantly more challenging, and we look forward to the lessons we’ll learn. Future posts will examine these additional tests, and the high-level design for implementing Raiden in the Storj payment system.
Our goal is to provide quick, reliable, and cheap payments for everyone in the Storj ecosystem. While micropayments as a technology is still in its early stages, it looks like a promising solution. We are extremely grateful for the Raiden team’s help and support as we accelerate toward a decentralized future.
By Shawn Wilkinson, Chief Strategy Officer at Storj Labs
Originally published at storj.io on December 11, 2018.
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
667 
2
667 claps
667 
2
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@factoryhr/setting-up-pimcore-to-use-google-cloud-storage-service-a-short-guide-6b1b5660657d?source=search_post---------127,"Sign in
There are currently no responses for this story.
Be the first to respond.
Factory.hr
Apr 28, 2021·3 min read
This guide will show you how to set up Pimcore to use the Google Cloud Storage service. Google Cloud Storage SDK and stream wrapper will be used. The guide is based on Pimcore documentation for Amazon AWS Storage Setup.
Assume you want to upload files to your Pimcore application, but for some reason, be it that other applications or services need to access those same files via Google Cloud Storage or you just want to have a backup of your files saved on the cloud - you want the process of uploading files to Google Cloud Storage to be automated.
You only want to upload a file to Pimcore and let it handle uploading the same file to Google Cloud Storage by itself. To achieve this, a bit of coding is needed which will be shown in this guide.
In this guide, it is assumed that you are familiar with Google Cloud Storage, you have a bucket on the cloud and you downloaded a JSON file containing mainly your private key, but other useful data as well.
The solution consists of the following steps:
Run the following command in your Pimcore document root:
Create a new file (or re-use existing) with the following code in /app/startup.php:
Create a new file (or re-use existing) with the following code in /app/config/services.yml:
Create a new file named /GoogleCloudStorageListener.php with the following code in your bundle source: /src/CommonBundle/EventListener/GoogleCloudStorageListener.php
Create a new file named /GoogleCloudStorageListener.php with the following code in your bundle source: /src/CommonBundle/EventListener/GoogleCloudStorageListener.php
Create a new file named /constants.php in /app/constants.php and put the following code in it:
Migrating existing content from Pimcore application directories to Google Cloud Storage is done by using gsutil tool.
Run the following command from your Pimcore root directory to copy the contents of the Pimcore application’s assets directory to your bucket on Google Cloud Storage.
Here’s the official documentation for gsutil installation and gsutil rsync usage. You’ll most certainly find it useful!
Visit your Pimcore administration and try to upload a file, the file should be uploaded and visible in Pimcore administration as well as Google Cloud Storage.
Deleting a file in Pimcore administration results in deletion of the file on the cloud. Vice versa does not apply.
We design and develop native mobile & web applications for startups and successful companies.
1 
1 
1 
We design and develop native mobile & web applications for startups and successful companies.
"
https://medium.com/the-artificial-impostor/tensorflow-training-cv-models-on-tpu-without-using-cloud-storage-60b20f0a7cd6?source=search_post---------111,"There are currently no responses for this story.
Be the first to respond.
(This blog is also published on my personal blog)
Recently I was asked this question (paraphrasing):
I have a small image dataset that I want to train on Google Colab and its free TPU. Is there a way to do that without having to upload the dataset as TFRecord files to Cloud Storage?
First of all, if your dataset is small, I’d say training on GPU wouldn’t be much slower than on TPU. But they were adamant that they wanted to see how fast training on TPU can be. That’s fine, and the answer is yes. There is a way to do that.
The reason why you need to use TFRecord and Cloud Storage is that unlike GPU, TPU is connected to your VM via an internal network, and TensorFlow has not implemented a way for the TPU to read from your local disk. It only supports reading TFRecord files from Cloud Storage.
There’s one exception — if your dataset is small enough to fit into memory, TensorFlow can send your entire dataset over the network to the TPU Host, and you can avoid TFRecord and Cloud Storage.
Here’s how to do it. I’ll convert this Colab notebook that trains an image classification model using TFRecord files into two notebooks. The first one downloads a subset of the TFRecords files from Cloud Storage and converts them into Numpy arrays. The second one loads the Numpy arrays and train them on TPU.
(You don’t have to read from TFRecords or use the tf.data API in the first notebook. You can read raw image files using PIL or OpenCV and convert them into Numpy arrays as well. Anything that converts the dataset into Numpy arrays will do.)
We use the first 4 TFRecord files as the training dataset and the last 2 as validation dataset. This translate to 920 images in the training, and 450 images in the validation.
The parse_tfrecord function is taken directly from the original notebook. The raw jpeg files were serialized into the TFRecord files. We need to decode them using tf.image.decode_jepg to get the actual image array.
Now we use the .numpy() method to convert the tensors generated from the tf.data.TFRecordDataset into Numpy arrays, stack the arrays and dump the results to a mounted Google Drive folder.
Do the same things for the validation dataset and we’re good to go!
Link to the First Notebook.
Now we remove the TFRecord-related parts in the first notebook, load the Numpy arrays from Google Drive, and use tf.data.Dataset.from_tensor_slices API to create a Dataset instance.
Do the same to the validation set, and the model should be able to train!
Link to the Second Notebook.
I created these demonstrative notebooks with very low resource requirements. I’m not sure how the in-memory Dataset instance is stored on the TPU host, so I don’t know how many images you can practically use without getting an OOM from the TPU (they’ll need to fit in the memory of your VM first, of course). I’ll leave it to the readers to try and find out.
Towards human-centered AI. https://veritable.pw
2 
2 claps
2 
Written by
Data Geek. Maker. Researcher. Twitter: @ceshine_en
Towards human-centered AI. https://veritable.pw
Written by
Data Geek. Maker. Researcher. Twitter: @ceshine_en
Towards human-centered AI. https://veritable.pw
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/bitclave/bitclave-announces-integration-with-storj-labs-to-harness-the-power-of-distributed-cloud-storage-ad283f64d036?source=search_post---------12,"There are currently no responses for this story.
Be the first to respond.
BitClave, a decentralized search engine, is integrating with Storj Labs, a decentralized cloud storage platform with secure encryption and storage capabilities, to host multimedia content for the BitClave Active Search Ecosystem (BASE). This partnership will enable BASE to improve performance, maintain security and keep costs in check.
BitClave uses Ethereum blockchain which has storage capabilities. However, storing information on the Ethereum blockchain is expensive. Users have to pay 2000 GAS for a single instruction to store 32 bytes of data which is about $0.02 when using average 4gwei for GAS price. Storing large video files at this rate was impractical. So BitClave needed a better solution.
When businesses use BitClave to reach out to potential customers who are using the search engine, they upload content for customers to view through the platform. This data will be stored on the Storj platform until the customers view the offer. Once the end-users view the offer, the supporting content is sent to them for review as well. The Bitclave platform is completely decentralized, where businesses pay users directly for viewing offers and Storj for storing the multimedia content.
According to BitClave’s founder Alex Bessonov, the former chief security officer of LG Electronics: “In BASE, in addition to a large number of records that costs a lot of GAS, we also need to deal with cases where the single record is huge. In BASE, when a business creates an OFFER, often it would like to include some promotional multimedia material like JPEG images, audio or even video clips. Ethereum is not capable of storing such large multimedia files. So, we needed to find a distributed and secure storage solution. We, at BitClave, successfully integrated and evaluated Storj as the provider of such off-chain storage. We observed that we can safely store video files with long durations of tens of seconds as part of our OFFER object.”
BitClave chose to leverage Storj’s distributed cloud storage platform because of its security, performance and affordability. The Storj platform decentralizes cloud storage by using a peer-to-peer cloud storage network. Storj partners with individuals that share their unused disk space in exchange for STORJ, the company’s ERC20 token. The company then rents this capacity to companies who need cloud storage. When companies store data on the network, Storj encrypts the data and divides files into shards, which are stored across the network. File owners are able to download the files and unencrypt them at any time because they have a private key that no one else possesses. Storj’s security and privacy initiatives has allowed them to build the platform with a mindset of privacy by design. BitClave’s customers can be assured that any data uploaded will remain private and secure.
“There is a growing demand for decentralized platforms that put the power and value of personal data back in the hands of users, and BitClave and Storj are natural allies that share this common goal and philosophy,” said John Quinn, chief revenue officer and cofounder at Storj Labs. “We’re pleased to be working with BitClave as they integrate decentralized storage into their strategy to give customers control over their online search data.”
BASE system integration with Storj platform gives BitClave the ability to leverage the decentralized cloud storage system for its large multimedia files. This gives BitClave the power to provide the same great quality of service to its customers while keeping the costs down. Learn more about BitClave’s decentralized search by downloading the white paper here.
Decentralized Search Ecosystem based on Blockchain…
984 
1
984 claps
984 
1
Decentralized Search Ecosystem based on Blockchain Technology
Written by
The future of search is here! World's First Decentralized Search Ecosytem. 🔎 Blockchain based. 🔗 Check Desearch.com 🚀
Decentralized Search Ecosystem based on Blockchain Technology
"
https://medium.com/the-longaccess-company/our-offer-was-so-good-it-backfired-9751aa440600?source=search_post---------222,"There are currently no responses for this story.
Be the first to respond.
Last month, we officially launched BigStash. In short, BigStash lets you store your infrequently accessed files in a cheaper (but slower to retrieve) service, ideal for archiving purposes. It’s the kind of service usually offered to the enterprise, but we believe there is a significant number of consumers who need something like this too.
We decided to go ahead with a big offer, something that would make us stand out in such a competitive market.
“Get 5TB free for one year.”
As expected, it worked and we got a lot of traction just because of it.
What we did not expect, were the negative comments! “It’s a scam”, “2 weeks later, company and domain name doesn’t exist”, and “Sponsored by the NSA” were some of the comments we got on our Facebook page. Because, to a good number of people this sounded too good to be true: They thought there must be something we are hiding.
So let me tell you the inside story, and you can decide if offering 5TB to every user, for free, for one year, was a good business decision.
We are using Amazon’s AWS Glacier to store user data. That’s $0.01/GB/month (plus a number of other complicated charges, per request, retrieval bandwidth, etc, but let’s leave them aside, minimizing these is what we’ve been working on for the last year). In simple terms, this means: If you upload nothing, your 5TB will cost us nothing. If you upload 5TB it will cost us $600/year.
So, the more each individual user is using their year-long trial offer, the more they cost us. On the other hand, we have a good reason to believe that heavy users (the ones that cost us more), are more inclined to stay with us after the trial, and that, given the nature of our service (long term storage), they will stay with us long enough to amortise the initial acquisition cost.
And yes, the $500/year we charge for a 5TB plan is less than what it would cost us to store 5TB for a year, but if this is a popular usage pattern, we should probably offer even larger plans.
Which brings us to the next point.
Consumer cloud services pricing is largely influenced by usage patterns. Low-level PaaS services like AWS expose every single cost involved. They are great for developers and the enterprise but not for the consumer market.
Take for example AWS Glacier (BigStash makes heavy use of it). To calculate actual costs, one will have to take into account storage costs, the cost per request (including a hard-to-decode note that: “You can retrieve up to 5% of your average monthly storage (pro-rated daily) for free each month. If you choose to retrieve more than this amount of data in a month, you are charged a retrieval fee starting at $0.01 per gigabyte. […] In addition, there is a pro-rated charge of $0.03 per gigabyte for items deleted prior to 90 days.”) and data transfer costs.
But most consumers don’t like doing complicated calculations to find out what they will actually pay. They want a simple pricing model, associated to something they understand and they can control (like the space they need). It’s up to us, the service providers, to estimate and aggregate all hidden costs, and offer a simple and compelling package.
So, we need these data points: We need to have a big sample of actual usage patterns that will help us decide if our pricing makes sense. I would take it even further and say that if we could pay to get usage pattern data, we would do it —our business depends on it.
And, in a way, that’s what we do: We are willing to invest a good part of our funding to offer the service for free to a large number of users, just to make sure we have good data to prove our business model.
Two weeks ago, I sent an update to our investors: I was able to dig into data, cluster usage, identify usage patterns, do cost and profit estimates based on tens of thousands data points. Just for this, it was worth every GB of storage we offered for free.
BigStash is still under heavy development, and every now and then we will bring the site down for a few minutes, or make a change that will break something until we fix it again and so on. That’s what being in Beta means, and we don’t hide it.
In other words: The service is not yet up to the quality standards we would like it to be, and it wouldn’t feel right to charge users before it gets there.
The way we see it, our current offer is just returning the favour our current users do us —when they test on various local setups, environments and use cases, and when they help us find bugs and UI/UX shortcomings.
So? Do you still think we are crazy to offer so much space for free?
To me it sounds like a good business decision, but I’d like to hear your thoughts too. Leave a note here or tweet @vrypan.
Our mission is to safeguard and protect personal digital…
20 
Thanks to Sofia Gkiousou. 
20 claps
20 
Written by
Dad, geek, entrepreneur. My home is www.vrypan.net, my blog is http://blog.vrypan.net/
Our mission is to safeguard and protect personal digital archives for the future.
Written by
Dad, geek, entrepreneur. My home is www.vrypan.net, my blog is http://blog.vrypan.net/
Our mission is to safeguard and protect personal digital archives for the future.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/using-google-container-builder-service-from-gitlab-ce-93d96dea06bb?source=search_post---------360,"There are currently no responses for this story.
Be the first to respond.
I am a big fan of Gitlab but when it comes to container registry, Google Cloud Container Builder is lot more flexible, fast and also economical with very less overhead. Whether there’re lots of build or lots of pulls(adopting container orchestration platform, Kubernetes) or managing docker images with privilege to pull is an ease with Google Cloud Platform Container Builder. Vulnerability scanning is there out of the box.
Create a new service account from IAM & admin section with suitable name and download the key. Then assign the roles for container builder to the service account. For now, we give Cloud Container Builder Editor,Storage Admin, Project Viewer roles. Login to gitlab server and activate the service account: gcloud auth activate-service-account [ACCOUNT] --key-file=[KEY_FILE]
Create $IMAGE_NAME environment variable which will be the name we will be using while pulling docker image. For eg, mycoolimage is the IMAGE_NAME of gcr.io/myproject/mycoolimage.
For easiness, we install gcloud sdk on the gitlab server and create a shell runner using the token for the project.
Now, on each push on any branch, it triggers the container build with the tag name as branch.
When a new docker image is created with the same tag as previous, the last one is untagged and the newer image is assigned with the tag. So, when you have a lots of builds, unusable images are piled up on cloud storage. Here is how to remove them:
A Kubernetes cronjob that deletes untagged google container images at the end of day:
Google Cloud community articles and blogs
94 
3
94 claps
94 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
DevOps | SRE | #GDE
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/illumination/storj-and-sia-two-dapps-you-need-to-know-about-4aa26257ab?source=search_post---------275,"There are currently no responses for this story.
Be the first to respond.
Decentralized Cloud Storage will Change the Future
Web3.0 has unequivocally become the talk of the town as blockchain and decentralization becomes more mainstream. Web3.0 is so named because it moves away from centrally controlled cloud apps and software and puts power into the hands of users — along with a much higher prioritization in privacy and flexibility. At the core of the…
"
https://medium.com/google-cloud/private-access-to-gcp-apis-through-vpn-tunnels-ab0217918e38?source=search_post---------370,"There are currently no responses for this story.
Be the first to respond.
—
10/3/19: Please instead see the official solution here:
cloud.google.com
—
This tutorial demonstrates how to use APIs for Google Cloud Platform (GCP) services from an external network, such as your on-premises private network or another cloud provider’s network. This approach allows your on-premises servers that are connected to your private network to access GCP services without using public IP addresses.
GCP customers often have workloads spanning cloud proividers and their on-premises datacenters connected via VPN. In many of those situations, customers need to access various Google Cloud APIs where arbitrary outbound traffic from any system is restricted or acutely controlled.
This not a problem while the workload is running on GCP: accessing GCP APIs from within a VPC is directed towards Google-internal interfaces where those services resides. When accessing these same APIs from other restricted cloud providers or even your own datacenter the situation is a bit different: customers need to either enumerate and selectively allow wide Google API IP ranges or apply the same treament to the traffic as if the workload is within GCP: send the traffic securely through the VPN Tunnel.
This article is a baseline walkthrough of how to setup an strongswan ipsec tunnel to Google and then access GCP APIs and your VMs residing on Google.
VPC Service Controls are also enabled to demonstrate how to lock down specific API access to just authorized projects. That is, once the private API is enabled, you can optionally lock down access to APIs like Google Cloud Storage such that it is only accessible via the tunnel. This is an optional security measure you can employ to further restrict access.
The following diagram summarizes the overall architecture that you create in this tutorial.
Connect the two networks through a VPN tunnel and set the routing to resolve and emit GCP APIs through that tunnel.
Notes:
This tutorial uses the following billable components of Google Cloud Platform:
You can find the full git repo of this article here:
github.com
In the tutorial, we will setup the following remote an local networks.
Local:
GCP:
The steps outlined below sets up the following (in order):
This section details setting up the project on GCP.
It is advised to run all these commands in the same shell to avoid resetting environment variables.
First export some variables (feel free to specify your own projectIDs, ofcourse):
Create project that will host the VPN gateway and VPC API access (substitute the projectID with your own; this article uses GCP_PROJECT_NAME in the examples below)
Export the envionment variables for the project and network region to setup
Create the custom network within this project that will host the VPC and private API access:
This VM is used for testing connectivity from the remote VM to GCP. This VM will not have a public IP allocated and is accessible via VPC>
Verify no public address is allocated:
This example sets up another GCP project to ‘simulate’ a remote network. This project does NOT use Cloud VPN and instead terminates the VPN traffic directly on a VM. In reality, you can setup any cloud provider VPN or appliance while this article shows raw, low-level ipsec configuration
As before, the following command uses a project called ONPREM_PROJECT. In your case, you should setup this project as (for example): ONPREM_PROJECT-[randomcharacters]
Export some environment variables and substitue the variables with your project names:
Configure a new custom network with a specific CIDR range. You can configure any range but for consistency with the ipsec configuration below, the CIDR ranges below are used.
Configure firewall rules on the ‘simulated’ network to allow ipsec traffic inbound and internal traffic to the privateIP ranges for GCP.
This VM will host the ipsec tunnel and provide DNS resolution for the onprem network.
Since our onprem network is yet another GCP project, setup a route via gcloud
Normally, you would use BGP or directly set routetables but on GCP, these routes need to get setup on control plane.
Setup another VM instance on the onprem network that will send traffic and DNS resolution through the VPN Gateway (instance-1)
In the end, you should have two VMs setup on your onprem project:
NOTE public_IP of ONPREM VPN Gateway) is: >>>> 35.192.118.145 <<<<
Create VPN using Cloud Console.
Use the following specifications for the VPN. The remote peer IP is the IP address of the VPN host VM we setup previously 35.192.118.145
Use a new shared secret (eg: $SHARED_SECRET = python -c ""import uuid; print str(uuid.uuid4())"")
NOTE public_ip of GCP VPN GATEWAY that gets allocated. In this article, its: 35.184.203.133
NOTE THE VPN GATEWAY PUBLIC IP >>>> 35.184.203.133 <<<<*
export the ip as an environment variable for later use
On the GCP network, allow traffic inbound from your onprem network range 192.168.0.0/20
Configure the “ONPREM” VPN gateway
Remember to replace leftid= with the IP address of the VPN Gateway VM on $ONPREM_VPN_IP
Remember to replace right= with the IP address of the VPN Gateway VM on $GCP_VPN_IP
then
Configure strongswan secrets:
In the current example, the values would be:
Configure ipsec.conf:
Remember to replace leftid= with the IP address of the VPN Gateway VM on $ONPREM_VPN_IP
Remember to replace right= with the IP address of the VPN Gateway VM on $GCP_VPN_IP
Verify tunnels are created
2f. [Remote] Add next-hop route thorugh VPN Gateway
Verify the tunnels are up:
Open up a new window in instance-1, and run ip xfrm monitor.
You should see ipsec traffic through the gateway vm instance-1
Private access for Google APIs from remote systems requries a special CNAME map:
www.googlapis.com: CNAME=restricted.googleapis.com --> 199.36.153.4/30
On remote gateway VM (instance-1), open up two new shells.
In one window, run ip xfrm monitor, In another window, access the GCE VM and a private API:
Also verify the traffic through to 199.36.153.7 is via the tunnel
The session with ip xfrm monitor shows activity which indicates data sent via tunnel.
The following sequece will configure and test DNS resolution and direct connectivity to:
On remote gateway VM (instance-1):
Note ONPREM_PROJECT, the settings in your file will be different.
NOTE: GCP VMs overrides certain host entries likes /etc/resolv.conf so you may need to reset this if you leave the 'onprem' VMs running.
This should resolve to the IPs provided locally with CNAME and resolve to the 199.36.153. range
Note the IP address you connected to: 199.36.153.5
At this point, we have verified GCS API calls through the tunnel
We have verified the remote gateway sends traffic to GCP via the tunnel. We will now configure another host ‘on prem’ which will also send its traffic through to GCP via this gateway:
Normally, routes are added directly if using BGP or static route:
However, since this tutorial simulates ‘onprem’ on GCP, add routes via gcloud instead:
Verify API traffic is through the tunnel by running ip xfrm monitor on the VPN Gateway host (eg, instance-1 on project=ONPREM_PROJECT_NAME)
At this point, the VPN connectivity to GCP APIs and VM is established via the tunnel. Optionally enable API access to GCP services such that it must go through a trusted project (in our case, via the tunnel).
See Service Perimeters as well as Cloud IAM Roles for Administering VPC Service Controls.
First enable a security perimeter such that GCS access is only available via project=gcp-project:
Note: once you enable this, all acess to GCS for the given bucket is blocked except via this specific project.
on a host ‘on prem’ force traffic for www.googleapis.com to not go through the tunnel. Do this buy making the DNS resolve to the external address. On instance-2, comment the name resolution entry:
Then attempt to access GCS:
(note the address is outside of the tunnel route: 74.125.70.95)
Now add in name resolution such that the traffic transits the gateway and tunnel:
(note the address resolved is 199.36.153.4)
When you have completed this tutorial, delete your project to avoid incurring further costs.
Google Cloud community articles and blogs
58 
1
58 claps
58 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/technology-hits/100-gb-free-cloud-storage-for-you-1b0ddb1fb6fd?source=search_post---------46,"There are currently no responses for this story.
Be the first to respond.
We have been familiar with Google drive's cloud-based storage solution. I believe all Gmail users store their important data and pictures in Google drives because it has a synchronization feature that saves our…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.upthere.com/the-price-of-free-15ee655c075b?source=search_post---------379,
https://medium.com/codingthesmartway-com-blog/firebase-cloud-storage-with-angular-394566fd529?source=search_post---------7,"There are currently no responses for this story.
Be the first to respond.
This post has been published first on CodingTheSmartWay.com.
Subscribe On YouTube
Firebase Cloud Storage is a cloud service that let’s you store and retrieve user generated content (like images, audio, or video files). The service is highly scaleable and part of the Firebase platform, so that you can make use of the Firebase Cloud Service on various platforms.
If you want to use Firebase service in your Angular application the AngularFire2 library is your choice, as this is the official library. In the most recent version of AngularFire2 the support of Firebase Cloud Services have been added.
Now, managing files in your Angular application by using Firebase Cloud Service got a lot easier. Let’s try out a practical example.
In the first step we’re going to initiate a new Angular 5 project by using Angular CLI in the following way:
$ ng new ngcloudstorage
Change into the newly created project folder next:
$ cd ngcloudstorage
Check if everything is working by starting up the development web server:
$ ng serve
And finally try to access the default Angular application in the browser.
Adding Firebase / AngularFire2 to your project is easy. Install both packages by using NPM:
$ npm i firebase angularfire2
Now the project setup is ready and in the next section we’re able to start with the implementation.
To make use of Firebase and AngularFire2 within the Angular project we need to make AngularFireModule and AngularFireStorageModule available. To do so, let’s add the following import statements in file app.module.ts first:
Next you need to add both modules to the imports array as well:
The AngularFireModule is added by calling the factory method initializeApp. Calling this method requires us to pass over the Firebase configuration as an object. The object contains the following properties: apiKey, authDomain, storageBucket and projectId. The values of these configuration properties needs to set according to your Firebase account. To look it up login to the Firebase backend (Firebase console), create a new Firebase project and then click on the link Add Firebase to your web app.
A popup window opens in which you can see the relevant key-value pairs.
Now that the modules are made available in our Angular application we’re ready to inject AngularFireStorage into component AppComponent in file app.component.ts:
As we’re going to use Bootstrap 4 CCS classes in the template implementation we need to make sure to add Bootstrap to our project. This can be done in two steps. First of all install Bootstrap via NPM:
$ npm i bootstrap
Next, include the Bootstrap CSS file in styles.css by adding the following line of code:
Now the Bootstrap CSS file is included and we can make use of Bootstrap’s CSS classes in the template implementation.
Insert the following template code in file app.component.html:
If you now access the application in the browser you should be able to see the following output:
The user can use the button to the select a file from the computer for download. As we’ve connected the upload($event) handler method to the change event of the file input control we need to implement this method next to handle the upload process and store the file in Firebase Cloud Storage.
Now we need to implement method upload(event) in class AppComponent.
On class level two members are created, one of type AngularFireStorageReference and one of type AngularFireUploadTask. Inside the upload method we’re first creating a unique identifier which is then used to create a new Firebase storage reference by calling the ref method and passing in the id.
With that storage reference available we’re then able to upload the file which has been selected by the user by using the following line of code:
Now we’re nearly ready to try it out, there is just one step left to complete. Therefore open the Firebase console for your project once again and select from the left-side menu the link Storage. In the Storage view select tab Rules:
Change the default rules to the following to enable upload files without needing to authenticate first:
Now we’re ready to upload the first file. The result should then be visible in the Files tab of the Firebase Storage site:
Now that the uploading of documents to Firebase Cloud Storage is working, we’re ready to further extend the application. The next feature which should be added is a progress bar showing the progress of the upload process. Again we’re using some of Bootstrap’s CSS classes to include a progress bar in the template code of AppComponent:
The progress is set by binding style.width to the value of uploadProgress. This observable has not been defined yet, so we need to further extend the implementation in app.component.ts. First let’s import Observable from the RxJS library by adding the following line of code:
Add the following class member to class AppComponent:
The variable uploadProgress is declared as an Observable of type number. Now we can use this variable within method upload in the following way:
The call of this.task.percentageChanges() is returning an Observable of type number which gives us access to the current progress, so that the progress bar is filling up as the upload process runs.
To finalize the sample application let’s extend the output (in file app.component.html) to also display the link to the file in Firebase Cloud Storage once the upload has been finished successfully. Add the following code to the template:
In app.component.ts we now need to declare the downloadURL class member with type Observable<string>.
Finally the implementation of the upload method has to be extended to set downloadURL to the Observable which is returned from calling method this.task.downloadURL:
The running upload process can be further controlled. If you want to give the user full control of the process (pause, cancel and resume) you first need to include the following set of buttons in app.component.html:
The click events of those buttons are bound to the respective methods of the AngularFireUploadTask object:
Controlling the state of the buttons (enable / disable) helps to let the user know which action is available. In the template code from above the disabled property of the button has been bound to the following expressions:
These checks require access to a variable with name uploadState. Let’s declare this variable in class AppComponent:
And make sure that this variable is initialized in the upload method by calling the snapshotChanges method of the AngularFireUploadTask object:
Here we’re making use of the map operator to extract the state information from the task snapshot. This operator is part of the RxJS library. So you need to make sure that the following import statement is added:
Now the final result should look like the following:
Just try out the upload different sizes of PNG / JPEG files and control the upload process by using the buttons Pause, Cancel and Resume.
The summarize, let’s take a look at the complete code of our sample application.
app.component.ts:
app.component.html:
This post has been published first on CodingTheSmartWay.com.
Top Online Course Recommendations
Master Angular (Angular 2+, incl. Angular 5) and build awesome, reactive web apps with the successor of Angular.jsGo To Course …
Learn how to build a Web Application using Angular, Firebase and TypescriptGo To Course …
Disclaimer: This post contains affiliate links, which means that if you click on one of the product links, I’ll receive a small commission. This helps support this blog!
CodingTheSmartWay.com
542 
10
542 claps
542 
10
Written by
Full-stack Web Developer, CodingTheSmartWay.com
CodingTheSmartWay.com is a blog about latest web and mobile technologies.
Written by
Full-stack Web Developer, CodingTheSmartWay.com
CodingTheSmartWay.com is a blog about latest web and mobile technologies.
"
https://medium.com/@cryp2gem/0chain-the-big-picture-part-1-3-2ee11c9a522a?source=search_post---------228,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cryp2Gem
Feb 5, 2021·4 min read
Entrepreneur, Investor, Blockchain & Crypto enthusiast, Tech fan. Moving forward! www.0cventures.com
Last week 0Chain ($ZCN) price exceeded $1 for the first time since the presale in 2018, following an increase of public recognition by some of the crypto market commentators, namely CoinBureau article and Bitboy’s YouTube video. This makes 12 months price increase for $ZCN above 5.000% (five thousand). Pretty spectacular in my book. Despite the incredible growth, I believe the fundamentals of 0Chain indicate that $ZCN is still a very much undervalued asset, and if you intend to stick with me through the end, you’ll find out why.
This is PART 1 of a 3-part fundamental overview article of 0Chain, an enterprise-grade decentralized cloud storage solution, one of the most ambitious blockchain projects in crypto that will play a foundational part in the new WEB3, and “fair data” economy.
PART 1 — the massive real-world problems 0Chain solves in regards to data security, privacy, and censorship, and the way it solves them.
PART 2 — how and why 0Chain technology is far superior to anything currently available on the market.
PART 3 (the fun part) — we’re going to take a look at the very interesting $ZCN token economics, and think about the potential in terms of adoption and future ZCN token price.
Now, I don’t like to write a lot, and many smarter people than me have already put things into perspective before, so expect a lot of Twitter content and links to other articles ;)
Enjoy!
1.DATA BREACHES (SECURITY AND PRIVACY OF DATA)
Are you starting to get the picture?
The problem with centralized data security isn’t getting better anytime soon, in fact, it’s getting worse every year. Why? No matter how cheesy it may sound, data really is the new oil. Data is $$$ and with this, a honeypot for hackers.
2.CENSORSHIP AND FREE SPEECH
In recent times we were able to witness the big tech power of de-platforming any single person or de-hosting any enterprise that would not comply with their status quo preserving demands or even worse, pose a challenge to their dominance. To avoid being too political, I think we can all agree…it’s a problem! This article explains how 0Chain can solve this problem.
3.GDPR & CCPA COMPLIANCE
The EU imposed GDPR (General Data Protection Regulation) regulation in 2018 and was quickly followed by countries all over the world. These regulations have been a major pain for enterprises. It causes additional costs, not negligible by any means, and if an enterprise fails to comply…well, let’s just say you don’t want to be their compliance officer. The largest fine was issued to Google Inc — 50 million EUR.
Thanks to 0Chain, this can all be easily avoided. Read about 0Chain’s enterprise easy plug-n-play solution here:
Oracle Cloud Infrastructure blog — article 1
Oracle Cloud Infrastructure blog — article 2
0Chain is a high-performance, enterprise-grade decentralized cloud and it solves ALL of these massive problems, and more!
Because data on the 0Chain network is being split into multiple servers with different keys, the potential attacker needs to obtain ALL the keys to get access. On top of that, every chunk of data can be encrypted. This makes 0Chain practically impossible to breach or censor, and you can be sure that your data remains private. Also, the blockchain provides full transparency of all data activities, giving the owner a full-circle view and control over his data.
In Part 2/3, we’re going to take a closer look at the innovative technology that the 0Chain team has been building heads down for the last few years. We’re also going to do some comparisons to other projects in this space.
Stay tuned!
Researchers & fundamental analysts (::) Building crypto thirsty Cryp2Gem community. Searching and researching for blockchain future disruptive innovators.
588 
588 
588 
Researchers & fundamental analysts (::) Building crypto thirsty Cryp2Gem community. Searching and researching for blockchain future disruptive innovators.
"
https://medium.com/hackernoon/25-things-you-should-know-about-amazon-elastic-file-system-2023255303ea?source=search_post---------346,"There are currently no responses for this story.
Be the first to respond.
This content is part of / inspired by one of our online courses/training. We are offering up to 80% OFF on these materials, during the Black Friday 2019.
You can receive your discount here.
As part of my work in creating the content of Practical AWS training, I was taking some notes about EFS, that I’ll be including in this training but I am sharing some of them here.
In this blog post I am sharing some points about Amazon Elastic File System (EFS), some of these facts are opinionated and most of them are based on my own experience.
1 — When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use.
2 — Think of EFS as a managed Network File System (NFS), that is easily integrated with other AWS services like EC2 or S3.
3 — By the way, S3 could neither be an alternative to a Network File System nor a replacement for EFS... S3 is not a file system.
4 — Sometimes when you think about using a service like EFS, you may also think about “Cloud Lock” and its negative sides on your business but to reduce the Time To Market, increase productivity and costs, EFS could be a good choice.
5 — GlusterFS is an open source alternative to EFS but when thinking about the amount of work to manage it, the complexity to keep it stable and all of its maintenance efforts, some organizations would prefer Cloud Lock over spending more money or time.
6 — This is really a business and a strategic choice and every organization have its own choices.
7 — What I liked the more about EFS is the extreme simplicity to use it.
8 — Creating an EFS file system can be done easily using the console or the CLI.
9 — EFS uses NFSv4.x protocol that fixes issues of the v3 and comes with performance and security improvements while introducing a stateful protocol.
10 — The replication of files between multiple availability zones in a region is insured automatically by EFS.
11 — Making an EFS backup may decrease your production FS performance; the throughput used by backup counts towards your total file system throughput.
12 — You can decide on your security (e.g which EC2 instance could have access to an EFS file system) since EFS support users and groups read, write and execute permissions, works with Security Groups and could b integrated with AWS IAM.
13 — EFS support encryption.
Don’t forget to download our mini ebook 8 Great Tips to Learn AWS.
14 — EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale.
15 — Throughput and IOPS (Input/output operations per second) will be scaled according to how much your storage will grow/shrink.
16 — It is possible to use your EFS from your in-premise data center and attach your storage directly to EFS using Direct Connect
17 — EFS is expensive compared to EBS (probably 10x more EBS pricing)
18 — EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark and measure to insure your if EFS is a good solution for your use case.
19 — EFS distributed architecture results in a latency overhead for each file read/write operation.
20 — Most Network File Systems are not being used in the right way. This is a must-read: Amazon EFS Performance Tips.
21 — If you have the possibility to use a CDN, don’t use EFS, keep just files that can not be stored in a CDN.
22 — It is evident to say, but don’t use EFS as a caching system, sometimes you could be doing this unintentionally.
23 — EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use SQLite with EFS… even if it was possible to do it before.
24 — EFS is only compatible with Linux, if you’re using another OS, find another solution.
25 — Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving this could take some time and needs some efforts, so brace yourself. A good way to measure and ameliorate performance is integrating CloudWatch service and choosing the right metrics to focus on while benchmarking your application in a staging environment.
We are building online courses for everyone and every level: Newbies, intermediate and skilled people.
Our goal is giving people the opportunity to learn DevOps technologies with quality courses and practical learning paths.
If you are interested in Practical AWS training, you can make an order. You can also download our mini ebook 8 Great Tips to Learn AWS.
#BlackLivesMatter
189 
5
189 claps
189 
5
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Founder of www.faun.dev community. Tech author, cloud-native architect, tech-entrepreneur, and startup advisor
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@macsources/morro-data-cloud-storage-gateway-review-bringing-enterprise-tech-into-your-home-f1a7f15a696e?source=search_post---------193,"Sign in
There are currently no responses for this story.
Be the first to respond.
MacSources
Sep 14, 2017·8 min read
Share on FacebookShare on TwitterShare on Pinterest
Data files are getting bigger and we need options to quickly and securely store and move our files. Living in the data age can sometimes be overwhelming. It is now common to have smartphones with up to 256 GB of baseline internal storage. If your device supports memory cards, you can utilize this feature to store images, videos, music, etc. Unfortunately, Apple does not allow portable storage in their iPhones/iPads. Instead, Apple hopes that you will utilize their online cloud storage, which has a tiered storage fee scale. The first 5 GB of data are free and you can pay $0.99 per month for 50GB, $2.99 for 200 GB, or $9.99 for 2 TB (USA). This may seem like a reasonable option, but when you consider paying $120 for a single year and you own nothing, this is much less reasonable. I have tested devices such as the Lima and Lima Ultra, which are devices that you buy once and link to a hard drive. With a single purchase and without recurring fees, you can link a large hard drive to an internet linked device. These devices have garnered quite a bit of controversy with Kickerstarter issues, connectivity issues, data storage issues. I did not have similar issues with the Apollo Cloud device, which has been tested as well. With the ability for up to 10 people to utilize the device, Time Machine capabilities and a 4 TB size, it has been my go-to personal cloud storing option. I also have a Synology Diskstation with 12 TB of storage options, but this may be a little too complicated for some. I was thus really curious about the Morro Data Cloud Storage Gateway.
Packaging: The Morro Data Cloud Storage Gateway arrived in a plain 6 inch by 8 inch by 3 1/4 inch cardboard box with a white slip cover. The cover has a cartoony cloud drawing representing “a Folder in the Cloud.” The left side of the box provides information about the device and a QR code link to additional information. If you desire to store files in the cloud without a need to backup, sync and share files between remote offices, speed up cloud storage access, the Morro Gateway may be a great option for your home/business. The right side of the packaing details the contents of the packaging: Morro Data CacheDrive G40 with Intel Dual Core Processor, 1TB HDD Cache, Power adaptor with multi-country prongs. Furthermore the panel details the need for a network cable, internet connection and a Morro Cloud service plan subscription. Evaluating the rear of the packaging, you will again see the cartoon cloud drawing linking devices in New York, London and Tokyo. The Morro Cloud Storage Gateway comes with a 3 year warranty and promises no per user fees and unlimited storage growth.
Unboxing: Once you remove the slip cover and open up the plain cardboard box, you will notice a few pieces of paper: a 3×5 card with a link to support, a 2 1/2 inch diameter Morro Data Cloud sticker, and a folded quick start instruction manual. Beneath the papers, the package is separated into two zones by white styrofoam packing. To the right of the box, you will find the charging equipment: 1 15/16 inch wide by 3 1/8 inch tall by 15/16 inches thick charging adaptor with exchangeable tips: type A (USA, Canada, Mexico, Japan), Type I (Australia, Argentina, China, New Zealand), Type D (India), Type F (Europe, Russia) and Type G (Great Britain/UK, Ireland, Cyprus, Malta, Malaysia, Singapore and Hong Kong). The power adaptor has a 97 inch cable and an included velcro cable management strap. I wish that there was a small storage bag for the accessories, as I fear they will get lost without it.
The device measures 4 1/2 inches by 4 3/8 inches by 1 7/8 inches thick and weighs 1 lb 7.4 ounces. Along the front of the device, there are two USB interfaces, one colored yellow and one colored blue. Additionally there is an audio output panel. Along the left of the device, they included a large port for an SD XC card and a K lock port (computer lock). I am a fan of K locks for devices, especially for colleges, dorms, cubicles, etc. The back of the device has an AC adaptor 19V input port, VGA and HDMI ports, Ethernet port, Audio/optical out and two more USB 3.0 USB A ports (blue).
Setup: Reviewing the instruction manual, setup was pretty straightforward. I wish that it was as easy as the 3 step process promised by the instruction manual, but there were a few hiccups that required attention. After you remove the product from the packing, you will notice that there is no included ethernet cable. If you do not already have one lying around, you may have to purchase one. I would recommend Newegg.com or Amazon for this cable as Walmart and BestBuy are not typically the best choices. Plug the power adapter into the wall socket or surge protector and then into the device and then connect the ethernet cable. For step two, you will need to navigate to https://account.morrodata.com and follow the prompts. You will need to create a business account by adding email, your full name and then choose between Morro Data CloudDrive, CloudNAS, CloudNAS Business, or CloudNAS Enterprise. I plugged the Cloud device into my TPLink 8 port switch and then attempted to find the Gateway with my MacBook Pro. Unfortunately, I was unable to find/detect the gateway. Along the bottom of the device, the company provided the UID. You are supposed to be able to search the system for the UID, if the Gateway name is not immediately detected. I was unable to complete this step either using my MacBook Pro.
To complete step 2, I logged into the system using my hardwired desktop PC. You will need to create a Gateway Name for the CacheDrive G40 device and then you will need to create a permanent Team Name and Team portal (cannot be changed). You will then need to add your credit card information, review the information and then select “create.” Once you create the TeamPortalName, you can use the link https://{team_portal}.morrodata.com to log into the system. Once logged in, you will see notifications along the top right and along the left you will see Dashboard, File System, Devices, Team, Apps, Logs. The online application helps you to navigate the system, with suggestions and walk-throughs. You can add members of the team by selecting the team tab and then clicking the ” + ” along the bottom right. There are three types of users: Business Administrator, Global Administrator, or Standard User. You can manage access, Read/write, Read or no Share. Give them a network user name, select an email and allow them to link to your network. Once online, each step of the setup is directed by active/realtime navigation. An active arrow will point at an option and provide you with information about the tab.
The included instruction manual mostly details the setup process, but not the use and the management of the system. Luckily the above instructions were detailed within an included walkthrough/tutorial system. The company did a great job making this system intuitive for the average user. If you have questions, you can navigate to the top of the screen and select the help icons. There are a plethora of written descriptions as well as video instructions that will help you to navigate this system, without an IT college degree. There is a mild learning curve to understanding groups, pools, shares, Gateway, etc. The real-time navigation/tutorial really makes this more understandable and helps you to navigate the system. The dashboard tab is really designed to show the mapping of your cloud storage group/pool/shares and user permissions.
If you select file system, you will notice that the initial page is blank. You will need to select the little orange “+” along the bottom right, which will open the provider page. Again, the real time/active navigation/tutorial is incredibly helpful, intuitive and may be one of the best tutorials I have used to date. You can select between Dropbox or onedrive (dual cloud logo) and then add a name for the group. You will then need to log into the system (I chose Dropbox) and give permission to see the files. I had to do a lot of reading to understand this process. As a single home user, this system may be more than you need, as I can link to dropbox directly. However, it is nice to be able to have shared folders among devices, using the Morro device. Additionally, it is really nice to be able to have cached copies of larger dropbox files to share on the local network. This keeps the bulk of the file in the cloud and allows you quicker access to the files. As of 9/12/17, there was a new firmware update (2.1.1-Build3175). To update the firmware, select devices, choose the little blue down arrow to the top left and then this will open a gateway tab to the right. Select maintenance, update the firmware and then wait the 3–4 minutes for the server to reboot.
I was able to talk to Tony Chang tonight, a very helpful team member of Morro. The mistake that I made above was not creating a share. To do that, I needed to go to File system and select the “+” icon in the bottom right and then select share. Add the name of the share and then you can access the Morro from your home network and from the File Explorer. Copy the link and then you can drag and drop files into the file explorer as you would any other folder. This will create a sync folder in dropbox and you can access it from the web, you can access the file through file explorer and with finder on my Macbook. After I created the share folder, I was able to complete the tasks that I had issues with previously. This system worked amazingly well, once I understood the step that I had forgotten.
Summary: This system was really easy to setup using my desktop but not wirelessly with my laptop, at least not at first. Creating the share folder was the step that I missed. Being the only person on my network (except my wife), this system was much easier for her to understand than my Diskstation. She can now access the bigger videos on her desktop, stored on dropbox and cached locally. We use Dropbox for our phone picture/video storage, and some of the videos are rather large. She likes that she can see the files, even if the internet is down, thanks to the Cache files. This system would work incredibly well for photographers, for small business or for friends that are at a distance to be able to upload larger files. Web uploads to dropbox can take a while. All of the data stored on the 1TB HD is encrypted with AES-256 and all transit data is encrypted by SSL. You get the benefit of compression, file splitting for upload to the cloud into 96MB sizes for faster upload (additional data loss protections). Use the Time Sync feature to sync data when it is convenient to you.
If you have multiple offices, multiple business contacts, this system may be ideal for you. For a base of $499 and a once yearly $29 dollar fee, I can control my data, on my time, in my own way. I get up to 10TB of storage using this system. For business users, you can get similar features for a $10 per month fee and 2 devices can be linked. It is rather exciting to have Enterprise type data management in our own homes/businesses. Now you can work on a file at home or at work and sync it to your device and your partners can access this anywhere in the world. If you have multiple of these devices, they will sync across the devices and they can access the cache file incredibly quickly.
Learn More about the Morro Data System. Follow them on Facebook and Twitter.
Originally published at macsources.com on September 14, 2017.
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
MacSources is a digital media blog for resources and reviews. We cover all Technology that tickles our fancy. But mostly Apple. 
"
https://medium.com/google-cloud/automating-and-scheduling-a-linux-filesystem-sync-with-google-cloud-storage-576a07ca2fdb?source=search_post---------82,"There are currently no responses for this story.
Be the first to respond.
TL;DR: I learned how to migrate my legacy filesystem data over to Google Cloud Storage, using the Storage Transfer Service. Find below a script I wrote that will help you to do the same.
Motivation: I have a bunch of content scattered on various servers and hosting services, as I’ve been putting stuff online for a few decades — well before cloud was generally available. The setup is typically a Linux filesystem, and the host already has an HTTP server, or I can set one up. Some of these servers’ filesystems are still having files added/removed, and I want a way to get the result of this process, i.e. the filesystem contents, moved over to Google Cloud Storage. This grants a migration path forward for the applications, as I can eventually migrate the services modifying the filesystem, use CDN, etc.
It turns out that Google Cloud offers a Storage Transfer service, and that in addition to supporting import from AWS S3 buckets, there’s also a way to load data from HTTP servers. You can read about the transfer service here: https://cloud.google.com/storage/transfer/ and of relevance to this document, a protocol for describing objects to be imported over HTTP using the TsvHttpData format here: https://cloud.google.com/storage/transfer/create-url-list
Briefly, TsvHttpData requires a 3-column TSV file containing:
You can then use gcloud to create a scheduled job that retrieves the TSV file and imports any missing/updated objects.
I wrote a script that will recurse through a subdirectory of files, find the files that have been modified since the last version of a TsvHttpData file was created, and update that TsvHttpData file with new information (newly created files, omitted files that no longer exist, updated MD5 checksums for modified files). It does the right thing to reduce I/O by not recalculating MD5 checksums for files that haven’t changed. Here’s the source in a GitHub gist:
The next thing to do with this script is put it on a cron, as well as a post-processing step that removes the last-modified timestamp that I added as an extra column to the TsvHttpData format (to save on I/O). The two commands look like:
Please let me know with a ❤ or a comment if you found this useful!
Google Cloud community articles and blogs
18 
Some rights reserved

18 claps
18 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@stevenc81/free-self-hosted-ghost-blog-part-iii-gcp-cloud-storage-and-cloudflare-cdn-6a3806cf5c7b?source=search_post---------59,"Sign in
There are currently no responses for this story.
Be the first to respond.
Steven Cheng
Sep 14, 2017·4 min read
This is a continuation from Free Self-Hosted Ghost Blog Part-II (GCP Load Balancer, Let’s Encrypt and Cloudflare CDN)
Alright! Let’s get this over with. I promise this is the last, and perhaps the most technically challenging one. So, brace yourself :D
The goal of the post is to show you how to use the free GCP Cloud Storage (analogous to AWS S3) to work together with Cloudflare free tier to achieve global CDN. Again, free is the keyword!
First, you need to create a bucket. There are some limitations to the GCP Free tier to watch out for.
So it’s important to be careful when creating one. Pay attention to the red boxes below to select the right settings. For the bucket name you may want to use the domain you would like to serve the static contents from for identification reason. We will later use the Cloudflare to cache the contents. In my case I used cdn.steven.news. You can pick your subdomain you feel the best.
Once completed, you can start uploading the static contents to the bucket. And remember to set them to public so we can reference them later
To confirm everything works, visit the link by clicking into Public link, and you should be directed to a new page like https://storage.googleapis.com/cdn.steven.news/self-hosted-ghost-blog-on-gcp-free-tier/1.png
Please note cdn.steven.news is the bucket name as well as the domain will be serving the contents from. I will show you how to allow TLS/HTTPS connection to it from a Load Balancer in the next section.
Remember how to set up a load balancer in the previous post? Today we are going to extend it to cover CDN loads by adding a new backend service (to Google Cloud Storage), and frontend service (to take traffics to CDN) and a forwarding rule matching these two.
Creating a new backend service that connects to the Google Storage is quite straightforward. You will find the option as shown in the screenshot
Creating a new frontend service is similar to the previous post too. Remember to use https and we could reuse the previously generated tls certificate
Okay, now we need to think about how to get traffic that goes to cdn.steven.news/<path to resource> to the right path in Google Cloud Storage. This is the path rule you will need to set in the load balancer
This rule will take whatever goes after cdn.steven.news and append it to Cloud Storage Bucket path, in this case it's cdn.steven.news (since the bucket name and domain name are the same)
Then we are here! You should have a working CDN load balanced with HTTPS on. How cool is that, but it doesn’t stop here. In the next section I will show you how to further cache them with Cloudflare to achieve global CDN (for free)
This part is going to be easy. Simply add a new DNS record that points cdn.steven.news to the new IP address generated by the frontend service generated by the load balancer and you are all set! But remember to toggle the orange cloud on to take advantage of the awesome cache provide by Cloudflare!
I hope this series is useful in learning some basic techniques with GCP, Cloudflare and Let’s Encrypt. I know it was fun for me to learn all these with the added bonus of setting up my own blog. If you have any questions at all, please feel free to ping me up on Twitter!
Next post I’d like to talk a bit about Ethereum and how to create a Dapps. Stay tuned!
Cheers!
Originally published at steven.news on September 14, 2017.
https://twitter.com/stevenc81
75 
1
75 claps
75 
1
https://twitter.com/stevenc81
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/meet-lima/our-selection-of-gifts-for-privacy-lovers-3ac74c2ddd1e?source=search_post---------377,"There are currently no responses for this story.
Be the first to respond.
Privacy is everything. Here’s a selection of devices that will help you protect yours.
Wish to stay safe from hackers? CUJO is a smart firewall that keeps your smart devices safe from cyber threats so you can stay secure and private online. Their statement is simple: “Your home is full of smart devices. They are not protected by Antivirus, leaving your home open to cyber criminals. CUJO secures everything from tablets and PCs, to TVs and baby monitors.” You buy the device, and then commit to a monthly subscription. For big fans of IoT and smart devices.
From $99 (6 months) to $179 (18 months), then $8.99 per month.
Anonabox router is a router that focuses on providing you “Internet security, privacy and freedom”. Yeah, we love the idea! It uses the Tor Network and additional VPN services so you can browse the Web, send emails, and share files privately. You simply plug it into an existing router or any Ethernet connection, and you become anonymous online.
From $60 to $120.
The all-in-one security solution with HD camera, siren, and air monitor. You control what’s happening in your home. Set Canary on a shelf, connect it to the Internet, and that’s it! Canary will detect unusual activities at home: you can see it, set off the alarm from your phone, and call the emergency services. It’s smart, and it will learn the differences between ordinary and unusual activities. A must-have to keep a peaceful mind when outside.
$175.
Tired of losing your keys? Not having enough copies for friends? That won’t be a problem anymore. With Lockitron, you are the Keymaster. Lockitron is a device that goes behind your door and replaces your door key. On the outside, it looks exactly the same. On the inside, it looks like this:
It auto-locks behind you and automatically unlocks as you approach. You control it from your smartphone. You can create guest keys, and you know who enters or leaves your home at anytime of the day.
$99
Don’t want the government to snoop into your privacy? Scared of storing your personal photos on a server somewhere? It might be time to build your Personal Cloud and give your privacy all the care! Lima will safely store your personal files at home, in your own hard drive and make them available to you everywhere. Automatic download of your photos and videos from your smartphone, total privacy, no monthly fees … Lima gives you total control, and it feels good.
Get a Lima today for $99 (Lima Original) or $129 (Lima Ultra).
The first Cloud that respects your privacy.
68 
68 claps
68 
Written by
Access your files from all your devices. Keep those files safe at home. Just the way it should be.
The first Cloud that respects your privacy.
Written by
Access your files from all your devices. Keep those files safe at home. Just the way it should be.
The first Cloud that respects your privacy.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@Technology_Adv/box-vs-dropbox-which-is-best-for-your-business-f20309b204e0?source=search_post---------285,"Sign in
There are currently no responses for this story.
Be the first to respond.
TechnologyAdvice
Mar 30, 2017·8 min read
Public cloud storage gives your business the freedom to share, edit, and collaborate on work assets in real-time, from any location. And if you lose a corporate device or your office is submerged by the flood of the century, all your data is safe and secure.
It’s no wonder businesses across the globe are making cloud storage part of their virtual IT environment.
The public/private cloud storage market will surpass $56 billion by 2019, according to MarketsandMarkets, and two of the biggest players on the field are Box and Dropbox. The 2016 Intel report on cloud adoption and security “Building Trust in a Cloudy Sky” stated that 93 percent of those surveyed used cloud software, with 59 percent stating their belief that public cloud can reduce cost of ownership. If you’re considering a solution for your business, you’ve likely wondered about the difference between Box v. Dropbox. On the surface, they seem to offer a similar solution for business customers: secure cloud storage with collaborative and administrative tools.
While that’s true, there are some clear distinctions in packaging, functionality, user experience that set them apart.
Before we get into specific features, let’s take a look at Box vs Dropbox from a 10,000-foot view.
Box, Inc. released its IPO (initial public offering) early this year and enjoyed a period of significant growth, now valued by Hoovers at $216 million. Their user base includes 37 million people and 47,000 organizations.
Dropbox hasn’t gone public yet, so it’s hard to put a number on their revenues. According to recent reports, they now have over 500 million users, which dwarfs Box’s user base. But then 200 million of those users are paying customers who purchase Dropbox Pro or Dropbox for Business.
Both vendors are about 10 years old, and both offer multiple plans for business and personal accounts. But it’s important to note how Box and Dropbox uniquely position themselves in the market. It wouldn’t make much sense if they offered the same product, targeted toward the same customer, for different prices — which is why they don’t.
While Box does offer two personal storage plans, their product is first and foremost an enterprise solution. Consequently, its features and user experience are tailored to the needs of the CIOs and IT departments:
“We’re a 100 percent enterprise-focused company. All the technology we’re building goes toward asking how do we make it easier, or more scalable, or simpler, and just a better way for businesses to share and manage and access this data.” — Aaron Levie, Box CEO
Dropbox, on the other hand, began as a consumer-facing product and gradually made its way into the workplace as a form of “consumerized IT.” In other words: people liked Dropbox; people used Dropbox at work; workplaces saw the benefits of Dropbox; workplaces adopted Dropbox as a business solution.
“We’re solving really important problems for a big chunk of the world, not just Silicon Valley. Our users are trapeze artists, high school football coaches . . . physicists who collaborate across the world.” — Drew Houston, Dropbox CEO
These different approaches to file management pose unique challenges to each vendor.
For Box, that might mean improving the user experience on an individual level, making their platform easier to use and easier to administrate. For Dropbox, it might mean providing deeper administrative control.
Let’s take a closer look.
The first thing you’ll need to consider is the core product: how much does it cost, and what are you getting for the money?
Box offers three pricing tiers for businesses. Their “Starter” package, aimed at 3–10 person teams, provides 100 GB of storage. The “Business” tier — for medium-sized teams with more advanced needs — provides unlimited storage; and the “Enterprise” tier adds some advanced features like help desk, workflow automation, and metadata management.
Dropbox offers three plans for businesses with 5 users and up: Dropbox for Business at the Standard, Advanced and Enterprise. The Standard plan includes most business capabilities and 2TB of storage. Users who want more granular administration, IT, and domain controls should upgrade to a higher plan. This is a departure from the former one-plan business approach, but gives companies of different sizes and needs options without paying enterprise-grade prices.
Box provides a full suite of advanced security features, from file encryption to custom data retention rules and enterprise mobility management (EMM). Administrators can decide which users are granted access to files and folders and create user groups for easier assignment. There are seven levels of access control, which address access, preview, editing, and sharing.
Files themselves are protected by TLS and multi-layered encryption, file versioning, and expiration controls, as well as custom content security policies. Users can also apply passwords to confidential files as needed.
Features:
Dropbox includes standard security for a public cloud server — more than enough for the average business to protect their sensitive data. With Dropbox for Business, you’ll get 256-bit AES encryption (same as Box), group management and sharing restrictions, and the ability to remotely wipe data in the case of a compromised account or hostile termination.
Administrators can use the admin dashboard to track user and team access stats, including logins, devices, and sharing, as well as add and remove team members. Dropbox for Business does not offer data retention policies, which could be a drawback if you’re in a highly-regulated industry, but probably won’t affect most users.
Features:
Collaboration is one of the biggest selling points of cloud storage for a business, so it’s important to consider how a given solution will help your team work together. What tools do vendors offer, beyond basic file sharing?
Both vendors perform pretty well in this category. First, they both offer flexible sharing options including link sharing, invite-only sharing, and external sharing. Second, they both enable teams to collaborate in real time using team or group folders. Users can comment on files, work directly from Outlook, and access their storage account through native mobile apps for most devices. Not only that, but both vendors have built-in file recovery and versioning to keep users from losing their work.
Dropbox unique feature: Request files from internal or external personnel through the “File Request” tool; shared files automatically show up in your folder
Box unique feature: Create new documents inside of the application using Box Notes (the editing tools are fairly basic, so most users will still need a third party application)
Both platforms integrate with Office 365, DocuSign, and other content apps.
Obviously, neither of these products are designed to be a full-fledged project management solution.
But they can support existing project strategies by helping your teams manage work in an orderly, repeatable fashion — especially files and documents directly related to the project, such as RFPs, budgets, gantt charts, media assets, and so forth.
With Dropbox, you can use team folders and groups to collaborate on shared assets in real time. Each user can customize their notification preferences to stay up-to-date without being inundated. Dropbox also has a built-in photo organizer (Carousel), which makes it easy for teams to share multimedia files associated with a project.
With Box, you can create workflow automation rules to manage files based on certain conditions (e.g. if a file in Folder X is edited by a certain user or team, send the file to another specified user for review and approval.)
This level of task management admittedly primitive, but it can still be helpful for teams that manage a cyclical review process, such as editorial or design teams.
Almost every business has a system or set of systems that support their core business operations. In sales, this might be a CRM database. In the medical field, it might be an EHR system. Whatever the case, it’s crucial that your file management tools integrate seamlessly with your enterprise software. That means you need to be able to upload, download, and sync files from directly within the application.
How do Box and Dropbox line up with this standard?
For starters, they both integrate with Office 365 and Outlook. This is important since Outlook, in many ways, is still the gold standard for work email and calendaring (and since almost 80 percent of Fortune 500 companies have used or are using SharePoint).
Beyond that, each platform uses APIs to integrate with a variety of third-party applications.
Dropbox claims a list of 300,000 connected apps; Box, a mere 1,000.
Dropbox’s universality, in large part, can be attributed to its role as a consumer and business app. Box, on the other hand, mainly focuses on business-critical apps such as CRMs and marketing platforms. Here are some out-of-the-box highlights (not a comprehensive list):
The verdict?
Box and Dropbox both offer solid solutions for public cloud storage — including versatile file sharing, mobile access, and reliable security. The “better” product will depend on your unique priorities and budget. If you’re still having trouble deciding, give us a call. We’d love to help.
And remember, Box and Dropbox aren’t the only public cloud storage solutions. Other options for private cloud storage run within your company’s firewall, but these bring along with them the problems of build, manage, and upkeep throughout the cloud’s life cycle. Public cloud storage provides the ease of SaaS with most of the security. To see a list of options sorted by industry, features, and operating system, check out our product selection tool.
This article was updated March 30, 2017 to include Dropbox Business updates and more recent information regarding cloud usage.
Originally published at technologyadvice.com on March 30, 2017.
We know a few things about technology, but our real speciality is helping people. Unbiased research. Crowd sourced reviews. Expert advice. technologyadvice.com
1 
1 
1 
We know a few things about technology, but our real speciality is helping people. Unbiased research. Crowd sourced reviews. Expert advice. technologyadvice.com
"
https://blog.brand.ai/how-to-share-a-design-system-in-sketch-part-2-github-84a7bc3973cd?source=search_post---------209,"InVisionApp, Inc.
InVisionApp, Inc.
Freehand is the visual collaboration tool disguised as an online whiteboard.
InVisionApp, Inc.
By using InVision brand assets you agree to follow our guidelines packaged with the media kit. For further information about use of the InVision brand assets, please contact support@invisionapp.com.

"
https://medium.com/google-cloud/gcs-hmac-signedurl-3166b995f237?source=search_post---------269,"There are currently no responses for this story.
Be the first to respond.
Sample for HMAC SignedURL for Google Cloud Storage. Normally, you use RSA private key to sign a URL for GCS. However, another way to sign a url is to enable interop mode for GCS and use HMAC keys and the GCS XML API. HMAC is faster in the signing operation but is currently associated with either a user’s or service account (EDIT: 8/12/19: GCP Service accounts now support HMAC!:  https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create) …. Anyway, i thought i’d write this up since i didn’t find and example of this anywhere though it was mentioned in the documentation below. You can find the source code on HMAC signing below as well.
From the docs and the cloud console:
“Note: Signing with the RSA algorithm is supported for the Cloud Storage XML API, but not the JSON API. You can also sign URLs using HMAC when using the XML API for interoperable access.”
NOTE: “The Interoperability API lets you use HMAC authentication and lets Cloud Storage interoperate with tools written for other cloud storage systems. Turn on this API only if you require interoperable access for the current user. This API is enabled per project member, not per project. Each member can set a default project and maintain their own access keys.”
“Use an access key to authenticate yourself when making requests to Cloud Storage. The key is linked to your Google user account.”
Again, your HMAC key is associated with your useraccount and at the time of writing 8/3/18, service accounts are not supported…well,by 8/12/19, it is now!:  https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create
so, in the example below, just replace the steps to get the key and ID with the service account flow. I’ll leave the following as userbased though!
Anyway, to use this sample, first enable iterop access and generate a key:
Substitute the key,secret into the sample (make sure you have access to the bucket/object).
(i deleted the key i have in the image above!)
You can find other gcs and gcp auth samples here:
github.com
Google Cloud community articles and blogs
4 
4 claps
4 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@VictorManriqueY/97-of-our-google-sprint-prototype-was-bullshit-eceba3f3b88b?source=search_post---------359,"Sign in
There are currently no responses for this story.
Be the first to respond.
Victor Manrique
Sep 29, 2016·9 min read
A few weeks ago I read GV’s “Sprint” book. We were in the middle of some product strategy research and it felt like this could bring us something interesting. I mean, “Solve big problems and test new ideas in just 5 days”, who doesn’t want that?!
Right. So by now I guess you all know what a Google Design Sprint is about, but just in case, this is it:
And then, bam! It just happens that by Friday, during the last minutes of the sprint, and when you would think you failed miserably, it all works. OK, so this is how we came to that point…
Monday is about getting your long-term goal and main questions defined, by collecting information on those and mapping it all.
We started by having a short chat to set the objective and key challenges, and made the initial people-tasks-result map. We set our target customers on the left, their main goal on the right, and filled the steps in between.
After what should have been a quick lunch together (first mistake: we had a massive lunch at a nearby cafe and nobody could further think later), we got some “experts” in the room to create “How Might We” notes and found some interesting takeaways to current problems around our long-term goal and questions.
As expected, our initial map was completely wrong, so a few tweaks here and there, and this is how it looked like in the end of the day. We chose a key persona and a critical part of the process.
“Monday: Define your long-term goal and the main questions to answer. Ask the experts for input and map it all. Choose one question, one target user and one step in the whole problem-solution map.”
That became our main focus for the rest of the week. On to Tuesday!
On Monday, when you leave the office around 6pm, you feel great! Okay, you are as tired as if a bunch of stormtroopers beat you for a few hours, but it’s not that bad, trust me, at least you are still in for dinner…
The good part about Tuesday is that the day itself is pretty much about dreaming, thinking and drawing on your own, so that’s quite inspiring.
We started the day with a round of lightning demos about products and features that we found useful for our final solution (from now on, “the prototype”). Afterwards, we hung those on the wall, and each team member decided on which part of the map they wanted to work on. We spent the rest of the afternoon and early evening prototyping and sketching.
In the end of the day, instead of showing our stuff to the others, we kept everyone’s prototype sketches in a locker, because at that point you are all tired and grumpy.
“Tuesday: Focus on one of the map’s steps. Get ideas from other great products and features. Get all those lego blocks and sketch a 3-steps prototype on how the user would ideally go through that step successfully. Keep it for yourself.”
Want to know if our prototype sketches were any good at all? Wednesday is your day.
If you screw up on Wednesday, better spend the rest of the sprint watching funny babies’ videos. Nothing will save you, so you’d better nail the day.
So we were all really excited to show our stuff to the others and get quick feedback, decide, and move forward to the storyboard part of the day. But deciding what you’ll work on for Thursday and show to your testers on Friday, is not trivial. So Google folks came up with a whole voting system that may seem complex at first, but works.
What you do first is hang all prototypes on the wall. Then just let people see them for a while, and follow the instructions. After these steps you can read in the book, the deciders (remember it was important to bring your CEO/Founder?) will do what they are here for: decide.
So that sounds perfect in theory, but in real life, we had two deciders, myself and the company founder. After 15 minutes of hard thinking, analysing and discussing, we both came to the conclusion that we’d rather go for the riskiest and most innovative sketch, we are not there to do what everyone else is doing, we are here to go nuts and find a golden egg, we said.
“Wednesday morning: Go through all the 5 voting steps. Trust me, it works. Decide what you’ll do, and from now on, do NOT change your mind no matter how difficult it gets. Do it that way or die with it.”
Time for Wednesday afternoon: the mighty storyboard.
Yes, you heard it right. Once your deciders have taken the final decision on which ideas to prototype and test, die with it. We almost did, die I mean.
I’m going to explain this part of the story in a way that you can feel the pressure too. Just scroll all the way down if you are not into horror movies:
“Wednesday-afternoon: Believe in your ideas and don’t give up like we almost did, really. Make it happen in a 15-steps storyboard.”
We were now ready for Thursday. We’ll have 24 hours to fake a full prototype that feels realistic and does not break that much. It can’t be worse that Wednesday afternoon, right?. We are very wrong, again.
Wow, you made it all the way down here. OK, I know you are tired and want to get to the juicy part: people bashing us.
In short, on Thursday we built a fake but fully working html prototype with AngularJS in 16 hours. UX, UI, text, and everything, for real. How? I’m still wondering. I guess we had a sick developer with us and the rest of the horsemen got superpowers for a day. Other than that, I really have no clue.
Oh and we had pizza at the office. It was cool.
“Thursday: Fake a prototype that looks real. Don’t go the html way, it almost killed us and we finished it at 23:55 pm that night. Use keynote instead. Your testers won’t realise and certainly won’t care.”
Guess what happened on Friday?
We had scheduled 5 interviews for Friday. At 9 am, 10:30 am, 12:00 pm, 2:00 pm and 3:30pm. Real target users came to our office, without having a clue about our stuff rather than they’d get 100 euros as a small thank-you gift for giving us some feedback on a random prototype.
I’m not going to detail you the whole testing and feedback process, because I’d have to go over 4 more walls full of those sticky notes. Rather than that, here is the summary of what happened:
At the same time that I was doing the interviews, we had live video for the rest of the sprint team, so they could take notes and further analyse the tester’s reactions. But there was too much pink, and too less green. We had pretty much nothing and the sprint week was starting to look like a quite spectacular failure (Yeah, OK, “learning experience”).
“Friday: Schedule 5 tester visits along the sprint week. Make them open questions and listen. Their reactions are gold, their feedback useless. Get your team to take notes as you go, and look for patterns.”
But this story is not going to end this sad. We all know happy endings are a must, at least in good movies (pun intended).
I’m not sure exactly how we came across the 3%. I think I was finished with the fourth or fifth interview and someone said: “Hey man, sharing and discussing files via the chat like Slack is something they all like, but the thing is I saw three of them get really excited when they knew they could just drag&drop those files into a folder and save them.”
We didn’t expect that to be a such a “popular” thing. I mean, it was something pretty simple (and even stupid) to be honest. We had other ideas that were far more promising.
But we thought: “oh well, what if we add the so-called unlimited storage to that?” Then maybe, and just maybe, we could get WeTransfer, web WhatsApp/Slack, Google Drive/Dropbox and the email all in one.
After a few weeks, we created our very own “Watercooler”. Now it’s called Sharechat (I know, I know, you liked the other name more) and you can give it try on Sharechat.io
The kitten (and I) will love you if you click on the green heart to recommend this post. Otherwise, well, we’ll love you anyways. ;)
@victormanriquey
Ps. Thanks Gijs, Jessica, Erwin, Irene and Niels for reading and improving this.
Product guy at @ sharechat.io
See all (345)
60 
60 claps
60 
Product guy at @ sharechat.io
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@duhroach/google-cloud-storage-sequential-file-names-f1ba1205c6a8?source=search_post---------41,"Sign in
There are currently no responses for this story.
Be the first to respond.
Top highlight
Colt McAnlis
Oct 26, 2017·4 min read
BUMBLECAM is a nursery camera company, and their application uploaded snapshots of the nursery cam on a regular basis to the owners of the camera. Their setup was pretty simple: Every half second, the camera would snap a photo, and store it locally. In battery saver / bandwidth saver mode, a bunch of pictures would be batched up on the device before uploading to the Google Cloud Bucket for the owner.
The problem they were seeing was that their upload times for the images was painfully slow. During the course of 20 minutes, this slowdown would result in a serious backlog of images that needed uploading, eventually resulting in the camera running out of memory, and stop working.
So let’s run down the checklist here.
First, we ran perfdiag on a sample bucket they created, and verified it had a high throughput from the source. Way higher than what the developer was seeing. This meant that the bucket itself was performing properly in terms of connection and upload speed from the client; the problem had to be in the type of data being uploaded, or how it was being organized.
Since the cameras were embedded hardware, we knew they weren’t using GSUTIL to do the uploads, but rather the native Python APIs to upload files. As such, we knew that the uploads were going through the fastest possible API.
Next we checked the sizes of the files, they were about 100k each. So, they weren’t big enough to use the composite file upload, and the developer was properly using parallel upload API, so we should be seeing max throughput there.
Basically, all of the most common cases for performance, things were already set up in the ideal manner.
At this point, I needed to do research myself, so I turned to Michael Yu’s NEXT 2017 talk, where he provided details on how GCS works behind the scenes. Here’s a generalization of how things work:
When uploading a number of files to GCS, the frontend will auto-balance the connections to a number of shards to handle the transfer. This auto balancing is, by default, done through the name/path of the file; Which is very helpful if the files are in different directories, since each one can be properly distributed to different shards.
Which means that how you name your files could have an impact on your upload speed.
When the files are co-located in the directory structure, and the names are sequential, the requests are constantly shifting to a new index range, making redistributing the load harder and less effective.
And that was exactly our issue. The developer was using the timestamp in the file path. (e.g. YYYY/MM/DD/CUSTOMER/timestamp)
One solution to this problem is to manually break up your linearly-named files into folders, and then upload the folders in parallel. For example, the foo and bar folders can be uploaded in parallel w/o stomping all over each other. However, Bumblecam wasn’t too thrilled at this option, since this would cause some new “house keeping” dependencies to show up in various parts of the pipeline, not to mention that it might cause other scaling issues down the line if they didn’t continue to create new folders.
What we finally settled on was , prepending a hash of the filename to the filename itself. There’s lots of hash functions out there, but we settled on one which generates a uniform distribution of values over a fixed range (e.g. 00000000 — FFFFFFF). This allows GCS to partition the fixed range into shards for better load balancing.
For BUMBLECAM , this was an easy fix, for a massive increase in performance.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
See all (78)
141 
3
141 claps
141 
3
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@cryp2gem/0chain-the-big-picture-part-2-3-d6eb227222ab?source=search_post---------233,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cryp2Gem
Mar 24, 2021·9 min read
Entrepreneur, Investor, Blockchain & Crypto enthusiast, Tech fan. Moving forward! www.0cventures.com
PREFACE
This is PART 2 of a 3-part fundamental overview article of 0Chain, an enterprise-grade decentralized cloud storage solution, one of the most ambitious blockchain projects in crypto that will play a foundational part in the new WEB3, and “fair data” economy.
PART 1 — the massive real-world problems 0Chain solves in regards to data security, privacy, and censorship, and the way it solves them.
PART 2 — how and why 0Chain technology is far superior to anything currently available on the market.
PART 3 (the fun part) — we’re going to take a look at the very interesting $ZCN token economics, discuss why $ZCN is an asset with non-speculative value 0and think about the potential in terms of adoption and future ZCN token price.
Enjoy!
Even though I recognize that there are many players in the decentralized storage market (Filecoin, Sia, Storj, Arweave, Crust, or others), I believe 0Chain has the upper hand. 0Chain is purposefully built for the Enterprise market, which is the biggest and most performance-demanding data market, while still catering to other less demanding markets such as retail and archival.
0Chain is an entire ecosystem built from scratch. It borrows and expands on the model-view-controller (MVC) design concept. 0Chain has two co-existing networks: the storage network (blobbers) and the consensus network (miners, sharders). The consensus network is responsible for block propagation, state storage, and serves as an access controller to the storage network. The consensus network can be thought of as a Storage-View-Control (SVC) layer.
This separation of duties allows for two networks to scale independently of each other yet interdependent economically through the ZCN token whose value is driven by the amount of data stored on the storage network by clients and users, the storage capacity provided by Blobbers (storage providers), as well as the stakes put upfront by the miners (and sharders) to participate in the consensus network.
Furthermore, this separation allows 0Chain to provide a DSN (decentralized storage network) with features that are customary of centralized networks yet still closer to the edge and consumer than centralized services which typically gravitate into remote locations to save on cost. This in itself makes the 0Chain storage layer a solution candidate to IoT data, which needs expansive and cheaper storage at the edge, as edge storage is at a significant premium today to core cloud (AWS, etc).
If you want to go into details you can read up more in the whitepapers here.
0Chain also aims to utilize spare (or dedicated) capacity in data centers across the world. There are over 50k MSPs (managed service providers) in the US alone, over 150k globally with some % of idle hardware. The pitch is simple: “We enable you the possibility of a new revenue stream by utilizing your idle resources”. Not a hard sell. This approach also allows for flexibility, and consequently a much wider range of use cases than the competition. 0Chain can serve any market and is suitable for the storage of static data as well as dynamic data which requires high ingress and egress traffic. It is able to accomplish this with high efficiency and much lower costs than its centralized counterparts.
DECENTRALIZED STORAGE PROVIDERSProjects based on IPFS (interplanetary file system) like Filecoin, Crust, and others are very limited in regard to their use case, as they are only suitable for archival storage, which is static data.
Sia only challenges the nodes storing the files at the end of a contract, not on an ongoing basis during the time they are stored. This means one can provide a storage service but not respond to data requests in between, limiting its use case. Due to poor quality nodes that cause significant node churn, Sia is facing serious performance issues. Utilizing the spare capacity of private computers around the world has proven not to be ideal for the enterprise market.
Storj breaks down the data into a lot of pieces which makes it solid for consumer service providers and not really meant for enterprise-grade storage. It also lacks transparency due to its high reliability on centralized satellites. This somewhat stomps the nature of decentralized ethos.
There is a well-researched paper made by Sculptex going in-depth into the tech comparison of different storage projects, including those mentioned above. Read the article here.
CENTRALIZED INCUMBENTSCentralized cloud storage solutions (AWS, Azure, Google Cloud, etc.) are currently the most convenient and scalable options for data storage. But the problems are clear, lack of security, privacy, people not owning their own data, as well as high costs, especially for large amounts of storage retrieval.
There is no exclusion of centralized clouds offering 0Chain solutions to their customers. In fact, 0Chain is an advanced AWS technology partner. 0Chain Github repo includes a docker.aws section which enables an easy deployment to AWS infrastructure if needed.
0Chain also partnered with Oracle and can solve a pretty darn real pain point for enterprises, like GDPR/CCPA compliance. Liability costs are very real and growing, just ask Google.
Its common practice for large enterprises to be careful about big changes. They experiment on private “sandbox-es” then gradually transfer to more cost-effective public networks. This process helps them get comfortable with the immediate benefit of a high-performance blockchain like 0Chain’s without taking on significant security risk. 0Chain is already working with a Fortune 100 enterprise that will migrate to the public network once they spin up their mainnet. I anticipate this event will force others to follow, as the public network benefits are enhancing enterprise competitiveness.
You can read a good article on 0Chain from an Enterprise perspective here.
In December 2020, 0Chain mothership repo was released to the public which indicates the scope and ambitiousness of this project. Over the last four years, the team was busy working and producing gigantic results compared to projects with significantly more funding and developer power. Public and private Github repos include more than 7000 commits that will eventually become open-source.
Did I mention 0Chain is the only decentralized storage capable of streaming? Imagine content creators being able to own and control their own content, collecting 100% of royalties, zero middlemen, enabling them to restrict access to selected groups, while having a cheaper hosting cost to be online. Hello gamers, YouTubers, podcasters, and other influencers, artists. 0Chain is here to cater to your needs!
More details about streaming in this article!
A thing worth emphasizing is 0Chain’s technology is 100% agnostic to everything! Any blockchain or any traditional enterprise can easily integrate 0Chain products to leverage security, privacy, and compliance to their customers. On top of that, anyone can develop flexible data storage solutions according to their own needs on 0ChainNet. New data business models can be developed that will rival centralized competition without any possibility of de-platforming.
If you’re building a project you can partner with 0Chain to own your data and provide a single source of truth for your data! Email saswata@0chain.net
0Chain’s low-cost hardware requirements enable wide distribution, scalability of the network, and progressive decentralization.
Most people can grasp the concept of Uber drivers. Well, 0Chain is “Uberizing” cloud storage. Imagine instead of owning a car and offering rides to customers, you own servers and hard drives in an infrastructure sufficient environment (ideally in a data center). Coupled with staking $ZCN this allows you to earn passive income by providing data storage through the 0Chain network.
With a higher earnings potential, lower investment, and almost 0-time spent, compared to uber drivers not a bad deal at all! Feel free to join the DataUber.net mailing list and receive the latest updates on how to join the data revolution.
It’s really quite simple. If we want to achieve the promise of a fair data economy where consumers and enterprises own their data without the need for centralized intermediaries, the data needs to be byzantine fault-tolerant! The data storage layer sits right at the foundation of that. Everything 0Chain is building is for future needs! They are ahead of the curve. They are skating to where the puck is going to be. 0Chain is a game-changer in the cloud storage space, and you can screenshot that!
Stay tuned for part 3 of the series where we’re going to take a look at the very interesting $ZCN token economics, discuss why $ZCN is an asset with non-speculative value and think about the potential in terms of adoption, and future ZCN token price potential.
In the meantime, if you want to learn more about 0Chain, I STRONGLY recommend you to watch Ask Feebs (director of operations for 0Chain) YouTube episodes where he gives a high-end breakdown of 0Chain blockchain, storage, and ZCN supply in a very simple, easy to digest manner.
AskFeebs — 0Chain storage & blockchain video
AskFeebs — 0Chain ZCN supply explained
ACKNOWLEDGMENTS: I would like to thank 0Chain ambassadors, Siam, Sculptex, Chad, and Tiago for all the help on this article!
Researchers & fundamental analysts (::) Building crypto thirsty Cryp2Gem community. Searching and researching for blockchain future disruptive innovators.
221 
221 
221 
Researchers & fundamental analysts (::) Building crypto thirsty Cryp2Gem community. Searching and researching for blockchain future disruptive innovators.
"
https://medium.com/@olamilekan001/image-upload-with-google-cloud-storage-and-node-js-a1cf9baa1876?source=search_post---------5,"Sign in
There are currently no responses for this story.
Be the first to respond.
Olalekan Odukoya
Oct 30, 2019·11 min read
In this relatively short tutorial, I’ll be showing you how to upload an image to Google Cloud Storage and also how you can resize the image.
As developers, one of the things we shouldn’t do is to always store an Image to our database as this could open up our application to all sort security vulnerabilities, it also eats up unnecessary space in our database since they are stored as a blob file, and finally, it could lead to an increase in the cost of backing up data. Solutions to these problems could either be to Store our images in the file system, and create links/pointers to these images in the database but this could be slow when we need to fetch those images. Another way this problem can be solved is to serve our files with a CDN (Content Delivery Network). A Content Delivery network is often used to accelerate the effectiveness and efficiency of serving contents from a server to the clients by caching them and making them available to the clients as quick as possible. However, serving just images with a CDN can be very expensive, and the thing is, since CDN tends to thrive by caching contents, you might have uploaded a new image or file but it wouldn’t be available to the users until you clear the cache. The best option is to use an online file storage web service on the cloud.
The Cloud is just a remote server sitting somewhere, where you can store your content and access them quickly, easily, securely, and from anywhere in the world. One of the advantages of using the Cloud is because of how elastic, accessible, and secure it can be. A lot of companies offer these cloud-based services including Tech Giants like Amazon and Google.
In this tutorial, we will be using Google’s cloud-based service known as Google Cloud Platform. Google Cloud platforms offer a range of cloud-based services but the one we are going to be focusing on is going to be Google Cloud Storage. Google cloud storage is a service on GCP that allows you to store static files like images and files. The advantage of using this is that instead of storing say an image in your database, you instead, upload the image to GCS and get a link/url to that file in which you can then store into your database hence, saving disk space in your database and reducing time spent in trying to backup your data.
Before we can start uploading images or any files, we need to first create an account, to do so, copy and paste the URL below into your browser and click Enter. This takes you the Google Cloud platform’s website.
After the page loads, you should see something similar to what I have below.
Click on the “Go To Console” Button. This takes you the Cloud Console. Which looks something similar to what we have below.
You’ll notice that I painted a part of the picture, click on that part on your screen. This redirects us to a page where we are being asked to create a project. Give it a name and click on the create button. This action creates a new project for us in which we can associate our GCS project with.
Yours might be way different from mine if it’s your first using GCP.
In the search bar, type Cloud Storage.
After doing so, you should be redirected to a page similar to what we have below.
You’ll notice that the “Create Bucket” button isn’t active, and also there is a button that is asking us to “Signup for free”. Click on the “Sign Up Button” and fill all the necessary information. You might also be asked to enter your Credit/Debit Card Details before you can be able to complete the “Sign Up exercise”.
After you are done, the “Create Bucket” button should be active, click on it, you should see a modal similar to what we have below.
If not, just give your bucket a name. Then, keep hitting the continue button until you get to the last select box. Finally, click on the create button.
After, completing the clicking exercise, you should be redirected to a page similar to what we have below.
Click on the create button to finally create a Bucket. A Bucket is basically a place on the cloud where all of your data are being stored. According to the Google Cloud documentation, a Bucket is said to be a “Container that holds your data.” To read more about Buckets visit the link below.
That being done, we can now move to our development environment and start writing some code!
To set up your dev environment for this little project, open your terminal or console and type in the commands below.
if you are using Visual Studio Code as your Code Editor you can type the command below after the initial ones to open up the Editor.
Once you are in your directory and have confirmed, type in the command below to initialize your project.
The command below is often used whenever a new project is to be started in Node.js. This allows us to keep track of all install dependencies when have used in developing the project. The result of running the command is shown below. A json file with some meta-data about our project.
The next thing we need to do is to install all necessary dependencies our app needs for it to function. In your terminal, enter the command below to install a couple of dependencies we need to start our app’s server.
We need express for creating a server that is going to power our application, body-parser for parsing requests whenever it hits our server, and multer for parsing files which must have been part of the request object.
After successfully completing the action, a node_modules directory is being included in your project directory. This folder contains all the dependencies our application needs for it to be able to run.
After you are done, in your root directory create a file and name it index.js. This is where we are going to put the code which is going to create a server for our application. At this rate, your application’s project structure should look similar to the picture below.
In your index.js file, paste the code below into it.
After you’re done with that, run the command below to start your server.
This would create a server that listens for requests on port 9001. To kill the process, click on ctrl + c at the same time.
Starting our app with node directly would not be efficient as when we make changes to the app when we have to manually kill the server, and then restart before it can be able to notice any changes in our application. A solution to this problem is to install nodemon which we can use to automatically start our project anytime we make a change to any file in our project. To install it run the command below in your terminal.
This installs it as a dev-dependency in the project. Now, go to your package.json file and change it’s contents to what is in the snippet below.
You’ll notice that in the script key, a new set of key-value pair has been added to it. Now, go to your terminal and run the command below to start your application server with nodemon.
You should see something similar to what we have below.
Now that we have a server running, we can now start making requests in order to push an image or file into our bucket. But to do so, we need a dependency and also some certain credentials from our bucket to do so.
In your terminal, type in the command to install the google cloud storage dependency which is going to help us with the image upload.
Create a folder in the root of your project and name it assets. This is where we are going to put the files and images we want to push into our bucket. Once you are done with that, put a random picture in it and go back to the GCP console. We need to create a service account key which gcs is going to use to authorize us to put a file in our bucket.
On the GCP console, search for API, click on the api & services option. You should be redirected to a new page. Scroll down and click on the storage link
After clicking on the link, you will be redirected to a new page. At the left section of the page, click on the credentials button, afterward, click on the add credentials button as shown below.
Then, click on the service account option. Fill in the forms and then click on the create button as shown below.
You should be redirected to the create role section. Scroll down or search for storage. Select it and also select API KEYS ADMIN.
Once that’s done, you are once again being redirected to another page. Just click on the create key button as shown below.
Click on create
After it is being created, an automatic download of the account key is being automatically downloaded. You need to keep this file very safe.
In the previous section, we created a service account key and also downloaded it on our machine. So, we need to make use of this file before we can be able to do anything. In your root directory create a folder and name it config. Copy and then paste the JSON file we downloaded into it. Also, in the config directory, create an index.js file and paste the code below into it.
What we have done here is basically a configuration of the @googale-cloud/storage dependency we installed earlier on in the project.
After doing this we can now start with the core functionality of our application, which is image/file upload.
In the root directory create a folder and name it helpers. Then, create a helpers.js file in it. We are going to use this file to create some helper functions that would help us with the image upload. However, replace the index.js file in your root directory with the code snippet below.
You’ll notice that our app has changed. We actually haven’t done anything much, all we have done is just to use the other dependencies as a middleware in our application.
Proceed to the helpers.js file and paste the code snippet into it.
What we have done here is to first import the @google-cloud/storage we initially configured and then we proceeded in linking it to our bucket. Afterward, we created a function that returns a Promise. The function expects an object which contains the blob object. We then extracted the file name and the file (buffer) from the object. We clean it up, then, create a read stream. Once it is done writing the stream, it returns a URL to the image.
Once that is done, go back to your index.js file and replace the existing code with the code snippet below.
Open up POSTMAN, click on form-data, then, type file (it is important that the key is named file since we already configured multer to grab our file in the files key) under the key column, and select a random file on your system then, finally, make a POST request to…
if that’s successful, you should see a link to the image.
However, at this rate, the image would not be publicly available. You need to go back to the gcs console and edit permission of the file.
Go to permissions => Add Members => storage object viewer.
Once that is complete, the file can now be publicly available.
In this relatively short tutorial (lol) we can see that gcs have made file upload easier. With few lines of code, you already have your file uploaded. It also has a very secure system for authorized users who can have access to the files in your bucket.
The application’s source code can be found…
Also, if you’d like to ask me further questions, feel free to hit me up on Twitter.
To learn more about gcs, check out the docs here
You can also try this awesome library for the file upload in case you do not want to go through the stress of writing the code all over again.
Arigato!
Data Science, Tech, and Finance.
902 
15
902 
902 
15
Data Science, Tech, and Finance.
"
https://medium.com/efficient-u/google-photos-is-a-much-better-option-than-apple-photos-on-your-iphone-61decdafa5c6?source=search_post---------386,"There are currently no responses for this story.
Be the first to respond.
People still don’t know how the “Cloud” works — it’s the butt of many jokes, and it’s all because of Apple. The iCloud Photo Library, iCloud Photo Sharing, Photostream, confusion about setting up each device, the panic of wondering if simply tapping a couple buttons will erase your precious photos for good — I’ve watched users give in and pay for the extra iCloud service only…
"
https://medium.com/@openbom/the-nuts-and-bolts-of-openbom-cad-plug-ins-for-cloud-storage-41800899a90a?source=search_post---------121,"Sign in
There are currently no responses for this story.
Be the first to respond.
OpenBOM (openbom.com)
Jan 13, 2018·3 min read
As part the sequence of recent announcement about OpenBOM, you may have heard about the OpenBOM plug-ins which now support cloud configurations. If so, you might be asking yourself, ’what?! a bill of materials and CAD files in the cloud? Where OpenBOM is going?’
Today I want to provide you with further explanation and the bigger picture of what you can achieve with OpenBOM and the recently updated CAD plug-ins. The OpenBOM CAD plug-ins now have “cloud settings” for files. Why so? Well, we’ve been asked many times to store files in OpenBOM. However, we hesitated going down that road. There are simply too many cloud file storage providers out there — the world doesn’t need another one.
So instead, we decided to integrate OpenBOM with external cloud storage(s). We are working with most mainstream providers such as Google, Dropbox, OneDrive, enterprise storage such as Box.com, and specialized engineering CAD storages such as Autodesk Forge and Onshape. We think we need to connect to GrabCAD as well (so, if you’re using GrabCAD, please reach out to me — I want to understand how many of you are out).
OpenBOM is able to “cloud-enable” any company using existing CAD desktop systems with it’s ability to extend collaboration and communication to contractors and suppliers. Moreover, we not only provide BOM sharing and real-time collaboration with distributed teams, we also help make available 3D CAD files and auto-generate neutral geometry files.
Below is a diagram that gives you step-by-step instructions on how to configure an OpenBOM plug-in.
1- OpenBOM plug-in install. You can download it from OpenBOM integration page. When in CAD systems find “Configuration” or Setting in a specific plug-in.
2- Select [x] to enable cloud settings in the plug-in and autenticate cloud storage. We recommend to configure one cloud storage account per team, which makes things easier.
3- Cloud settings are OFF means plug-in works as before. No content will be taken to the cloud.
4- BOM is created using metadata only. Thumbnail image preview is included.
5- Cloud setting is ON. In such configuration OpenBOM plug-in will upload files to the cloud storage.
6- Native CAD files will be uploaded.
7- Neutral geometry files will be automatically generated and uploaded.
Check our documentation, here, for more information. If something isn’t clear or missing, please let me know. In addition, here are a few video with some examples:
Conclusion. OpenBOM cloud file storage integration is a powerful mechanism that allows you to upload customer files in native or CAD neutral geometry to the cloud. It will open up your company improved communicating and efficient collaboration.
Heads up! That’s not all. We are working towards future improvements of our CAD plug-ins to support revisions, standard catalogs, and many other things. What do you think?
Best, Oleg
PS. Let’s get to know each other better. If you live in the Greater Boston area, I invite you for a coffee together (coffee is on me). If not nearby, let’s have a virtual coffee session — I will figure out how to send you a real coffee.
Want to learn more about PLM? Check out my Beyond PLM blog and PLM Book website.
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
See all (479)
3 
3 claps
3 
Online tool to manage you Bill of Materials and Part Catalogs. Real-time collaboration for teams and supplier, sync data with CAD, PLM, ERP. More - openbom.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/wolox/transform-cloud-storage-performance-into-an-f1-car-d00f7103bcca?source=search_post---------95,"There are currently no responses for this story.
Be the first to respond.
Wolox is always searching for new ways to improve product quality. One of the most critical factors that influences any app’s UX is the loading time of images, videos and other types of media files.
This post assumes that you have adopted a Storage as a Service (SaaS) solution to save and deliver your content. If you apply the steps listed below, your SaaS performance will significantly improve and users will be able to load the app’s files with the speed of a Ferrari.
See also: The Good, the Bad and the Ugly Things About the New RecyclerView
One of the most used SaaS platforms is S3 provided by Amazon. You can store the app’s media files in a bucket to provide a reasonable loading time without spending much money. As soon as your app starts being used by people from different parts of your country or even from other parts of the world, the loading time will slow down.
Amazon and almost any SaaS provider allow you to set up the storage system in different regions of the world. Choosing the closest one to your users is an important step to guaranteeing the best possible response time. The data latency will increase proportionally to the distance between the storage system and the user. This might not be a problem if you are developing a project with local users, but it might be an obstacle when trying to reach new users from other regions.
A high performance solution that will decrease latency is setting up a CDN (Content Delivery Network): a distributed set of servers with high availability around the world. The CDN can store multiple copies of the files in your system and will have the responsibility of serving them.
When users request a file from the cloud storage, instead of using the S3 bucket (if you are using Amazon), the request will be routed to the CDN hostname. DNS will assign the petition to the most suitable server according to distance, availability and other metrics. Finally, the selected server will handle the request and thus the latency time will be reduced significantly.
WebP is an image format developed by Google.
“The WebP format essentially aims at creating smaller, better looking images that can help make the web faster”
We strongly recommend using WebP. Compared with JPEG, a WebP image can be up to 34% smaller without showing considerable loss of quality. Furthermore, WebP supports transparencies such as PNG files, but with one-third of the file size.
After reading this, you may be interested in using WebP, but still wondering why isn’t WebP so popular? The answer to this is that WebP is still not a standard file format. Some browsers don’t support WebP neither the image libraries for mobile applications. Learn more about this on Caniuse.
As WebP is quite new, trying to integrate it with a project’s image libraries may be tough. For example, we struggled trying to convert images to WebP with imagemagick due to an existing bug in this image library.
To conclude, using WebP may significantly reduce the size of all your images. However, it still has some downsides. Consider the advantages and disadvantages of WebP before using it!
See also: Extracting Configuration in Ruby Application
As we stated before, our solution includes setting up CDN. The results were good, but we wanted files to load even faster, so we gave WebP a try and the results were impressive.
To measure the improvement, we came up with a test script that takes a group of image links, makes a request for each one and outputs the load time.
The following screenshot shows the output. The difference between the load time for each group of pictures was huge. The first group of links comprised images in their original format taken from a bucket and the second group had the same images, but converted to WebP and hosted in a CDN.
Check out the test script in the following gist.
We hope this post helps you to improve your SaaS performance!
Posted by Sebastián Balay (sebastian.balay@wolox.com.ar)
www.wolox.com.ar
We are a tech company redefining the way things work.
6 
6 claps
6 
We specialize in end-to-end development of high impact products, providing technological solutions to start-ups and companies that are seeking to innovate and need support in developing their ideas. In January 2021, we became part of Accenture.
Written by
Driving Innovation www.wolox.co
We specialize in end-to-end development of high impact products, providing technological solutions to start-ups and companies that are seeking to innovate and need support in developing their ideas. In January 2021, we became part of Accenture.
"
https://medium.com/pragmatic-programmers/deploying-to-cloud-storage-with-hugo-8d21664a94ec?source=search_post---------154,"Sign in
There are currently no responses for this story.
Be the first to respond.
The Pragmatic Programmers
Jan 28, 2021·5 min read
👈 Deploying to Netlify | TOC | Deploying to a Traditional Web Server 👉
Hugo has built-in support for deploying your site to cloud storage provided by Amazon S3, Google Compute Storage, and Microsoft Azure. To use these services, you need to configure Hugo with the information it needs about where to place the files.
Let’s configure Hugo to deploy to Amazon S3.
"
https://medium.com/@nutanix/understanding-compliance-in-aws-cloud-68f9292973d1?source=search_post---------296,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nutanix
May 9, 2016·5 min read
Understanding Compliance In AWS Cloud
Cloud Compliance is one of the major concerns among all users. Enterprises using cloud storage or backup services have to make sure that they are following the best practices in Cloud to ensure that their businesses run smoothly. They have to make sure that they have a good knowledge about their data storage and backup and their infrastructure is compliant when in the cloud.
Issues pertaining to Cloud Compliance arise once enterprises start using the cloud storage or backup services. When their critical data is migrated from their internal storage to cloud, it becomes very important for them to observe closely how that data will be stored so that they remain compliant with laws and business regulations. When it comes to Cloud Compliance, enterprises have to think about what data they should move to the cloud and what they should keep in-house. These questions have to be answered by their cloud services providers and should be written into SLAs to maintain compliance.
Every now and then, AWS Compliance answers thousands of questions which users ask about how to accomplish and preserve compliance in the cloud. Among several other things, enterprises are keen to take advantages of the cost savings and safety at scale being offered by AWS while upholding strong security measures and regulatory compliance at the same time. Set of laws across industries and geographies can vary and also feel complex. Let’s see and analyze answers to some of the commonly asked questions asked about compliance in the AWS cloud. This will also help in clearing up possible mistaken beliefs regarding how operating in the cloud might affect compliance.
Nearly every guideline requires enterprises to sufficiently look after their substantial and informational assets. To do this, there is an implied or understood ability to organize and establish:
What type of information is stored on a system?
Where is the information stored?
Who has the right of entry to your system?
What they can access?
Is the access authorized?
All of these queries involve some level of rights of the possessions in question, and that is where Cloud Compliance issues turn out to be evident. In a public cloud environment, some of these questions can be answered with certainty however; some of them might pose a compliance problem. Let’s see some of them here:
Certifications and Attestations
Compliance certifications and attestations are the evidences that show that something is true. They are considered by a third-party, self-regulating auditor and result in a certification, audit report, or verification of compliance.
Laws and Regulations
Customers using cloud services remain accountable for complying with appropriate compliance laws and regulations. In some of the cases, AWS offers important features such as security features, enablers, and legal agreements such as the AWS Data Processing Agreement and Business Associate Agreement. These features help a lot in supporting customer compliance. It is also true that many requirements under applicable laws and regulations may not be liable to certification or attestation.
Alignments and Frameworks
Compliance alignments and frameworks involve available security or compliance requirements which have been published already for specific purposes. AWS offers important security features and enablers that include compliance playbooks, mapping documents, and whitepapers for these types of programs.
It is not necessary that all the requirements under specific alignments and frameworks need certification or attestation; nevertheless, some alignments and frameworks are covered by other compliance programs. For example, NIST guidelines can be mapped to appropriate FedRAMP security baselines.
To make sure that your business is fulfilling the necessities of Cloud Compliance, you need to know the areas you need to be aware of. The first thing that every enterprise needs to learn is the importance of thorough knowledge and understanding about the type of cloud services that they use. Once enterprises are fully aware about the offerings of their cloud services provider, they can look at the data that they are going to move to the cloud. For security and compliance reasons, it is better to keep some of the highly confidential data on an internal network and not move to the cloud. Or, if at all any data needs to be moved to a cloud infrastructure, it should be kept in a private cloud that is hosted on the premises. There, access to both the physical and logical infrastructure can be provided. To learn more about AWS Compliance, please read this AWS Blog post.
The second thing to understand for the enterprises is to decide which data they are going to put on the cloud and for that they need to look at the contracts with their cloud services provider. So, let’s consider that if it is going to be an internal cloud, is your enterprise going to have internal SLAs and internal compliance checklists? And if it’s external, you have to undoubtedly identify with your cloud provider what type of data should be kept in the cloud, how they’re going to look after them, how they’re going to back them up, and how you may have the right to audit the security and compliance framework that your cloud provider builds around your data.
Much of compliance is about ensuring proper controls that organizations need to have to get access to assets. Applying IAM Policies to achieve security compliance has also become a necessity. At the same time, it is important for them to know to what extent they can access their data and how it can be maintained. The best way to ensure all this is through audit. Botmetric gives you an extensive set of over 75 thorough audits to run on your cloud infra.
These audits are centralised around- Cost, Security, DR/Backup, and Performance. It regularly checks organization’s infrastructure and keeps it updated. Enterprises need to always remember that it’s their data and they are fully responsible for it; they have to remain in control at any given stage. They need to make sure that they classify their data. They need to understand that some of the data might not be suitable for the cloud. It requires to be kept internally. Enterprises should have the correct understanding and cooperation with the cloud provider settings. Finally, they should try to keep an incident response plan ready. Having an incident response plan will help them in facing any type of e-discovery and authorizations to get access to data stored on the cloud.
To understand how Botmetric is helping enterprises achieve compliance in the AWS Cloud, take up a 14-day free trial today. Check DR/Backup audits that you should run to achieve compliance in AWS Cloud. Run all the free intuitive audits and adhere the best practices to achieve maximum ROI from your cloud.
Is your cloud infrastructure complaint? Do you audit it regularly? Tweet to us. We would love to know.
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
1 
1 
1 
We make infrastructure invisible, elevating IT to focus on the applications and services that power their business.
"
https://medium.com/geekculture/choose-the-best-cloud-storage-service-for-your-needs-da38fcebb9f6?source=search_post---------115,"There are currently no responses for this story.
Be the first to respond.
Choosing the best Cloud service for your business or personal use can be a daunting task given the number of options we have at the moment. Cloud services are not a new thing, but its’ popularity has grown considerably in the last decade and it will only grow higher given…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@blockbank/aibb-partners-with-iagon-to-create-cloud-storage-for-users-87a7e5b98921?source=search_post---------128,"Sign in
There are currently no responses for this story.
Be the first to respond.
BlockBank
Aug 13, 2018·2 min read
AiBB, the world’s smartest Ai-based crypto trading platform, announces a partnership with Iagon in order to bring users more functionality on their all-in-one crypto trading and advisement platform.
AiBB’s platform is packed with trading features including a single login across exchanges, a smarter Ai advisor, price accuracy, trade alerts and following pro traders. Smart contract making, wallet functionality, and banking services in the EU make a platform like no other — and now secure decentralized storage helps keep user trades safe, too.
Iagon offers proprietary encrypted, distributed storage utilizing blockchain and sharding protocols and storage will be accessible through the AiBB application. They will be able to hold IAG tokens on AiBB, increasing the functionality for traders and offering them access to Iagon’s ecosystem.
Using Iagon’s next-generation storage for AiBB users helps the company find the best solutions emerging in order to support more features for users at the highest level. Being an Iagon early adopter means better security for AiBB users as well as additional chances to partner and collaborate on the future of Ai on the cloud. Blockchain brings exciting opportunities to both.
Both teams have a commitment to Ai-based systems where storage and processing power are at the heart of changing the computing world.
You can learn more about the AiBB token sale here.
You can check out Iagon’s revolutionary cloud here.
To participate in the conversation, check out AiBB’s channels:
Telegram | Facebook | Instagram | Reddit | Medium | LinkedIn | White Paper |
BlockBank’s vision is to create an all-in-one crypto application that simplifies the user experience without compromising security, privacy, or decentralization
See all (15)

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
BlockBank’s vision is to create an all-in-one crypto application that simplifies the user experience without compromising security, privacy, or decentralization
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@storjproject/there-is-no-cloud-it-s-just-someone-else-s-computer-6ecc37cdcfe5?source=search_post---------368,"Sign in
There are currently no responses for this story.
Be the first to respond.
Storj Labs
Mar 16, 2016·2 min read
Once you come to terms with the fact that the “cloud” is just someone else’s computer, this begs the question as to what type of computer (or collection of computers, ie a network) do you want your data stored on.
Data storage in the cloud is split between private clouds and public clouds or a hybrid of the two. Private clouds offer better security and privacy, however, as deployment and configuration is left to the developer, private clouds have traditionally been defined by large capital investment, high complexity and low usability. With public clouds, data is transferred to and stored at the third-party servers of the service provider, which offers low capital investment, low complexity and high usability, with the rub that the data is less secure and private.
Now, what if you had a public cloud option, which had all of the ease of use benefits of traditional public clouds, plus the default state of the stored data was not only encrypted end-to-end, but also distributed across a widely decentralized network of nodes? Your data would be like a handful of encrypted sand being blown into an expanse of encrypted beach, not stuck in a centralized “fortress” with a big “hack me” sign on it (https://www.linkedin.com/pulse/even-hackers-get-hacked-john-quinn?trk=mp-author-card). The resting state of your stored data on the public cloud would be secure: https://www.technologyreview.com/s/545621/why-were-so-vulnerable/.
Now imagine that your data was not only more secure, but that it could be retrieved more quickly using P2P technologies and be more reliable than current public cloud offerings (7 9’s vs 4 9's). Now imagine this vastly superior public cloud offering costing 50% less than traditional public cloud offerings. No, I am not talking about the tooth fairy here, I am talking about Storj.
With Storj, all stored data is highly secure at rest: encrypted end-to-end, sharded into millions of mini files and then widely distributed across thousands of nodes.
It would take a hacker with the powers of Gandalf, Merlin and Dumbledore combined to identify, collect, decode, decrypt, reconstitute and integrity check the data stored on Storj’s network and that sounds like pure fantasy to me.
John Quinn, Co-founder Storj Labs
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
82 
82 
82 
Storj (pronounced storage) is an open source decentralized cloud storage platform. Learn more at https://storj.io.
"
https://medium.com/@alibaba-cloud/alibaba-cloud-hybrid-backup-and-disaster-recovery-solution-9a664864982a?source=search_post---------270,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Sep 27, 2018·7 min read
By Alibaba Cloud Storage Team
In the era of digital economy, data is growing exponentially. In just a few years, the amount of data has jumped from terabytes to PBs and even ZBs. The total amount of global data is predicted to reach 19.4 ZB by 2018. The global data growth rate will be more than 25% per year in the next few years, and the global data is expected to hit 50 ZB by 2020.
Undeniably, data is the core of digital operations, and data security determines the survival of enterprises. If a data center fails, enterprises may face irreversible damages. In August 2018, an international cloud service provider reported data leakage because its sales staff did not follow the specifications on a bucket. In July 2018, a Chinese cloud platform was exposed to a serious failure, which directly led to the loss of data of a startup company. Because of this incident, the startup had to rebuild the company from scratch.
In January 2017, the O&M personnel of a code hosting platform mistook the production environment as a test environment in multi-terminal switching and deleted the database of the production environment. In November 2014, a financial payment company experienced a system failure, causing nearly 400 million duplicates to be reported.
These are only a few examples that illustrate the importance of data security for enterprises. According to statistics by IDC, 55% of companies that experienced disasters in the past decade collapsed. For the remaining 45%, 29% went bankrupt within two years due to data loss. Only 16% survived.
According to a Gartner report, two-fifths of the companies that experienced system downtime due to major disasters never resumed operations, and one-third of the remaining companies went bankrupt within two years.
In this context, enterprises are in urgent need to strengthen data protection.
A hybrid cloud disaster recovery solution is the first choice for enterprises’ digital transformation. In the past, a traditional backup and disaster recovery solution is an architecture system similar to the production center, built in a disaster recovery center. Although this solution meets the data backup and replication needs of the production center, it poses many challenges to enterprises due to its long implementation period, inconvenient implementation, expensive equipment, and complicated O&M.
Compared with the traditional backup and disaster recovery solution, the hybrid cloud backup and disaster recovery solution features high efficiency, high availability, high cost performance, and free O&M. This modern solution helps customers securely and efficiently back up files, databases, virtual machines, and even the whole system locally or on the cloud. In addition, the application server system backed up on the cloud can work as a virtual server, which delivers the required RPO and RTO to ensure stable services and disaster recovery on the cloud.
The hybrid cloud backup and disaster recovery solution has the following advantages:
Hybrid backup recovery (HBR) is an easy-to-use and cost-effective online backup service. This service helps customers back up data from desktops, servers, and virtual machines to a backup repository on Alibaba Cloud, which ensures secure and efficient cloud storage, backup, and management for customer data.
Different from the HBR service, hybrid disaster recovery (HDR) provides cloud + local dual backup and cloud disaster recovery for enterprise applications. The HDR service protects servers, files, and applications based on the hardware and software of the backup and disaster recovery server to ensure service continuity. In addition, the HDR service can be used for disaster recovery drills and data analysis.
In response to data protection in the big data cluster architecture, Alibaba Cloud released the first big-data backup and disaster recovery solution on the public cloud.
Customers need to back up the production environment locally to ensure quick recovery of local data if the local IDC data is mistakenly deleted or a disk failure occurs.
The local backup data is remotely disaster-tolerant. In case of disasters in the local IDC, services can be quickly restored in the disaster recovery center so that services are continuous or the interruption time is short.
The service system consists of database servers, file servers, application servers, and other servers.
The HBR service has the following advantages:
A Hadoop big data cluster in the local data center contains hundreds of terabytes of data. Self-building a remote disaster-tolerant cluster of the same size will cause a lot of idle resources and high costs. If the required RPO is close to 0, this cannot be satisfied by the traditional DistCp solution.
The HBR service has the following advantages:
In summary, Alibaba Cloud Hybrid Backup and Disaster Recovery solution has the following advantages:
Currently, Alibaba Cloud is the first to provide customers with the most complete hybrid cloud backup and disaster recovery solution, including the local data center, private cloud, cross-public cloud, and other hybrid cloud scenarios. Alibaba Cloud hybrid backup and disaster recovery solution is widely used in finance, manufacturing, education, hotels, retail, government, enterprises, and other industries. This solution provides not only data protection for enterprises’ local data centers and multi-cloud environments, but also new channels for traditional enterprises to access the cloud and enjoy the convenience and benefits brought by cloud computing.
Reference:https://www.alibabacloud.com/blog/alibaba-cloud-hybrid-backup-and-disaster-recovery-solution_594007?spm=a2c41.12053283.0.0
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
52 
52 claps
52 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/google-cloud-storage-downsizer-af0048591e40?source=search_post---------61,"There are currently no responses for this story.
Be the first to respond.
Nobody stop me… this is too much fun ;-)
App Engine Standard provides an Images service that many customers value and often use to create thumbnails (in Google Cloud Storage). Firebase has a Cloud Functions sample that uses ImageMagick for thumbnails too.
Here’s my version. It uses file streaming so should (haven’t tested) scale better and it creates multiple downsized (thumbnail) images. I decided to use Cloud Source Repositories to host my code which works well though would benefit from change triggering of new Cloud Functions deployments… files FR.
I think you may benefit from Google Cloud Platform’s Free (as in beer) Tier because Google Cloud Functions, Google Cloud Storage and Google Cloud Source Repositories appear all to be included.
Open a bash terminal and get coding:
CSR is a service that provides hosted, private Git repos. I’m generally lazy and use a regular file system for hosting my code but, keeping changes is great with Git. Once the CSR repo is created, clone it locally for your code:
Create index.js:
And package.json:
Open Visual Studio Code (or your preferred editor). Replace the value of ‘root’ (#10) with the value of your bash env variable ${ROOT}. Save it.
Visual Studio Code includes a Git client and so you may stage your changes and define a commit using it. Alternatively, if you prefer to use the command-line:
Any time you change either of these files, repeat the add, commit, push command to ensure your files are reflected correctly in CSR:
Let’s deploy this code as a Cloud Function
Sticking with the command-line, you should be able to:
You’ll note that I’m mixing up “downsizer” and “thumbnail”, apologies. I started with “thumbnail” but now prefer “downsizer”. It’s more… dramatic. Fortunately, Cloud Functions permits aliasing. We give Cloud Functions the name “downsizer” but we tell it that the function exported in the Node.js code is called “thumbnail”. The value of the source flag is specific to ${PROJECT}, the fact that the repo is called “default” and that we’re using “master” for simplicity.
A few minutes later…
Or, if you’d prefer:
Because we created buckets for this project and have yet to use them, they’re both empty:
Find your favorite image and use gsutil to copy it to the ‘trigger’ bucket. This will trigger the Cloud Function to generate 4 thumbnails of our image in the ‘thumbnail’ bucket. In my example, I’m also moving (renaming) the file to “/2013/road-trip/henry.jpg” to show that the source path is preserved.
Here’s me with my best buddy:
This image is 4000x3000 pixels and is 1.7 MiB
All being well, you should expect 4 thumbnails to appear in the ‘thumbnails’ bucket. Here’s my result:
Enumerate these using “gsutil ls -l” I learn that they are 18364, 26154, and 15758 bytes per the list order which sounds about right. So, let’s confirm:
The 64x64 is actually 64 by 48 because my original image was not square (64/4000*3000=48)
And, 256x256 is actually 256 by 192 for the same reason:
I’ve become slightly obsessive recently about grepping Stackdriver Logging logs so, I’ll control myself and just give you an example:
NB the “freshness” flag which I learned of earlier today and, in this case, just retrieves the last 10 minutes’ logs.
I revised the sample to leverage header metadata to specify the desired thumbnail sizes. The code above reflects this. Now, if the object includes a ‘goog-meta-sizes’ header with an array of sizes, these will used instead of the default set of [“256x256”,”128x128"",”64x64""]:
Results in:
NB I find it difficult to track down the specifications for Cloud Functions event data. It is documented, here and then, for Cloud Storage Objects here.
This isn’t vastly different to my earlier Exploder post but, absent alternatives from Google or 3rd-parties, it’s straightforward to build, deploy and run, Cloud Functions for event-driven processing.
Feedback always welcome!
You can delete the Cloud Functions:
You may delete buckets after recursively delete all their objects. Please be VERY careful using this command:
Alternatively you can simply delete the project which will delete everything within it too:
Thanks!
Google Cloud community articles and blogs
85 
No rights reserved
 by the author.
85 claps
85 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/google-cloud/accessing-google-cloud-storage-using-aws-sdk-and-oauth2-1c7764025810?source=search_post---------80,"There are currently no responses for this story.
Be the first to respond.
With Google Cloud Storage you have the option to access the service via two distinct APIs: XML or JSON APIs. Why two? Well, the XML endpoint supports the boto framework and offers compatibility with AWS S3 while the JSON Endpoint is compliant with the rest of Google Cloud API infrastructure and delivers some unique advantages (see Performance And Cost Differences Between Apis). Almost all GCS client libraries uses the JSON API while gsutil can employ both. For most users, its recommended to use the JSON API that comes with our client library anyway.
Recall I said compatibility with S3..does that mean we can use AWS’s S3 client library to interact with GCS? Well, yes for simple usecases but as with most things, there are some caveats. In this article, we will cover the overrides a colleague of mine and I employed to AWS’s stock java and golang clients to allow it to interact with GCS using oAuth2 access_tokens. We did not modify the client core classes in anyway and simply used the authentication overrides already surfaced by the client.
NOTE: the technique is alpha quality and is _not_ supported by google support; I only tested the basic operations and documented some of issues I know about.
This article is co-authored by my collegue who contributed the golang version in a day! Thanks @yfuruyama.
Google already documented simple techniques to use AWS’s client libraries against GCS as part of the simple migration story here:
The snippets in the second link details basic usage of S3 client libraries while using HMAC credentials common to both Cloud Providers (well, hmac was added to GCP for the S3 compatibility..). Whats the “problem” with HMAC? Well, its just static, long-term username and password based credentials (long term until you manually revoke to set). Sure it will work…you just have to provision the credentials on GCP either as a user or service account.As mentioned, in either case, you need to manage the key and secret manually, e.g. inlining into code:
Whats wrong with HMAC keys?…well, its username/password, long-lived secrets! Oauth2 tokens are short-lived and can be derived in any number of forms for GCP based on the environment.
If you invoke the above snippet an turn up wire logging, you’ll see the headers sent while fetching file.txt from bucket:
Two things to note:
Up until now we’ve described how to use HMAC and AWS client libraries…but google uses oauth2 bearer tokens for authentication (well, for the most part, there are also JWTAccessTokens and OIDC tokens).
Lets see if we can account for the differences in the API request above with overrides for Google using AWS’s S3Clienthere.
Theres nothing to do here, the client does that for you already with
settings. Note we didn’t have to specify the bucket name in the URL- that gets added in directly as part of the request by the library. We’ve also used “auto” in the region since we don’t have to specify anything like the region while dealing with GCS.
We need someway to intercept and override the Authorization header sent and instead of HMAC based key, acquire and substitute an oauth2 bearer token using whatever GCP provided credentials are in context. Fortunately, AWS's client library (in java and golang atleast), allows for such an override. In our case, we implement a custom AWS4Signer which just injects another override AWSSessionCredentials. The Credential override reads in a GoogleCredentials object (whichever you provide), coaxes out its access_token (and performs automatic refresh), then pretends that token is AWS's AWSAccessKeyId (i could've embedded it as AWSSecretKey or AWSSessionToken ...any of those are available when i really need to do the substitution in CustomGCPSigner).
At this point we have the GCP access_token pretending to be the AWSAccessKeyId. Our override of the Signer gets called and automatically has access to this credential and the request object it will send out. What we're doing now is reading in the AWSAccessKeyId and adding it into the Authorization: Bearer <AWSAccessKeyId> header value so when the request is sent, we send over our GCP token.
Ok, this is a hack since we’re pretending AWSAccessKeyId is the something else but we are not altering AWS's code in anyway really...just overriding the standard library to use our implementations. How do you use this in a client?
First bootstrap GCP credentials and custom signer, then tell AWS to use that signer in the client configuration
Needless to say, you can inject any Credential type you choose:
This is tricky…we need to transform each header we see into the google formatted one. In CustomGCPSigner.java, this transform was done while overriding the Signer for authentication. First we created a map and then checked each inbound header from the AWS client library and ran a substitution. We also added in any custom metadata the client would be sending in as part of the object (eg x-amz-meta- -> x-goog-meta-)
Note, it seems atleast in java, I was not able to remove the header keys provided (the AWS SignableRequest did not allow that)...so i just ""added in"" the google headers.
We also need to add on a special header while interacting with GCS for certain operations. As the name suggests, x-goog-project-id denotes the projectID in context for the request. Not all GCS operations use this header but it is need for the ListBuckets type operations
Now we’ve got the overrides in place, lets upload a file and set some standard custom metadata and headers
You’ll see in the wire trace the Authorization header, the bucket name in the host, the duplicated x-* set of headers as well as the 'custom` metadata values
and to confirm:
There are several advanced aspects we did not cover but which may or maynot work:
This mode is available in both AWS and GCP which allows the authenticated client to incur the costs of the GCS operation. I did not verify this but I suspect this should ‘just work’ since the credential is already bootstrapped.
I’m not familiar with S3’s capabilities in this area and didn’t test this. GCS on the other hand allows for resumable uploads…you know, the usecase as described in here: “If somewhere during the operation, you lose your connection to the internet or your tough-guy brother slammed your laptop shut when he saw what you were uploading, the next time you try to upload to that file, it will resume automatically from where you left off.”.
If AWS’s library does not support this mode, you will have to use gsutil or GCS client libraries directly (which handles this automatically in JSON). (if you want to use signed+url and resumable upload and really want to see the details, see the link in the references section).
The technique outlined here is not SignedURL but regular authentication. You can sign normally using a service account credential file without using these overrides or for that matter, AWS client libraries
The complete source code described in this article can be found in in the following it Repos
github.com
github.com
Note, we’ve only covered java and golang. We made an attempt at overriding the python boto library set but I didn’t understand boto overrides for the RequestSigner to make much progress. This is as far as I got in python. If you find time or interested in contribuing, all commits are welcome!
This article is primarily a way to show how to use oauth2 tokens with AWS’s client library. It is not officially supported and should be used with some caution…Its intended for existing AWS S3 users with simple usecases who may not need to retool their application just to write to GCS (i.,e you dont’ have to learn a new GCS library just yet). For those users, the authentication override described here is superior to plain HMAC while still giving you time to migrate over to GCS’s JSON API and library set. Long term, I’d recommend moving to using the officially supported set.
Google Cloud community articles and blogs
16 
1
16 claps
16 
1
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by

A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@alibaba-cloud/what-is-cloud-storage-and-how-does-it-work-6c3246edff1a?source=search_post---------130,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Nov 20, 2019·4 min read
By Hitesh Jethva, Alibaba Cloud Community Blog Author.
Data is a crucial component of any business. We all deal with gigabytes or terabytes of data in our day to day work. And when you store a lot of files in your PC it’s most likely that your PC can turn slow, resulting in poor performance. Although there are plenty of external storage solutions to store and backup your data, these storage devices are not failsafe. You can lose your important data due to any mishap such as breakage, theft, damage, and many others.
There is nothing worse than losing your data or the fear of losing it. This is where cloud storage comes with its fascinating features and great capabilities that you can get remotely sitting at the comfort of your home or office sitting at your PC or while outside traveling with your Smartphone or tablet.
Cloud storage is a convenient and reliable option to store and retrieve data. There are many different cloud storage services available in the market. Before going for a cloud service, it’s important to understand what exactly cloud storage is and how it works. So, let’s get started.
The cloud storage is a storage space available to store data on remote servers which can be accessed from the cloud (or the internet). The data is managed, maintained and backed up remotely, for which the users generally pay a monthly or per consumption rate.
Cloud storage uses data centers with massive computer servers that physically store the data and make it available online to users via web. Users can remotely upload their content, store them and retrieve the data as and when required. With the introduction of cloud, now you don’t need to purchase servers, external hard drives and memory sticks to carry your data from one place to another.
Cloud storage works simple and easy. In cloud storage, information is stored in data centers located anywhere in the world and maintained by the third party. As the data is on hosted servers, so it is easily accessible through a web interface.
The cloud storage uses a chain of servers that includes both master control server and other storage servers. The servers are all linked with one another and it can be utilized depending upon your use and requirements, and billed accordingly. Cloud storage saves your lot of money otherwise you would have to spend on more strong servers as your business needs increases. But in cloud storage, you only pay for the space occupied by your data in the cloud.
Cloud storage generally supports multiple file types of all sizes, so you can easily upload all your important content such as documents, videos, photos, music, movies and more. Once you sign up for cloud storage service and upload your files, you can sync it with your Smartphone, tablet or other mobile devices for easy access while on the go.
The most fascinating feature of cloud storage solutions is you can upload your files or download them from anywhere across the World even without carrying your laptop. You just need internet access and any mobile device on which you need to download the application of your service provider. Then, simply login to your account and upload or download the files.Cloud also allows you to share your content with other users or your team members working for your company. You can give them access to remotely edit or read the files shared with them. Your team members can collaborate and work together on a project and edit the documents shared with them. You can invite people to give access to your files and folders and everyone with the access can see the shared documents in their account.
The data stored in cloud storage is secure and protected by a combination of strong password so any unauthorized person cannot access the files shared or uploaded by you. Also, cloud storage has the feature of restore and recovery of data for accidentally deleted files, folders or documents. It gives you the option to restore your data by linking your account with any device like tablet, Smartphone or PC.
Cloud storage is an example of technological advancement that provides us an innovative method to maintain and manage our content. For a nominal monthly fee, anyone can store their data securely on an online location, without worrying about space. It’s easy to upload and download your files with the cloud. Additionally, they are secure, safe and can be accessed by you and anyone whom you authorize. The files can be accessed anytime, from anywhere, using a PC or mobile device.
This article is originally published on cloud storage services site.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
20 
20 
20 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
"
https://medium.com/google-cloud/replicate-data-from-bigquery-to-cloud-sql-2b23a08c52b1?source=search_post---------356,"There are currently no responses for this story.
Be the first to respond.
With the popularity and the power of the Clouds, some think that unicorns exist: One database that can rule them all!However, technology, and physics, can’t achieve this dream. So, each cloud provider proposes many data storage options and each fits very specific use cases.
Analytics databases, like BigQuery, are very efficient to process petabytes of data in seconds. Aggregations, complex queries, joins with billions of rows create new insight and new values for the companies.
The bad side is the latency. To process large volume of data, a cluster of CPU/Memory couples (named “slots” in BigQuery) need to be provisioned before starting the query. And even if it’s very fast, it takes about 1 second. And the process is the same, even if the volume of data is low, or if the query is simple.
It’s not a concern for analytics, but not for real-time, like website or API serving.
A low latency database like Cloud SQL is required.
Cloud SQL is a managed database service that can run MySQL, PostgreSQL and SQL server. These database engines can answer simple queries in a few milliseconds and are perfect for websites and API servers.
However, the Cloud SQL databases can’t scale out (add additional servers in parallel to process the data) but only scale up (increase the quantity of CPUs and the memory on one server). Read replicas (additional servers in parallel) can also run a query on only one server (replica).
Because of that, the performances are low to process efficiently a large amount of data (which is limited to 10 terabytes)
Businesses need powerful insight with the capacity to compute and deep dive into petabytes of data, and to be able to browse the result at low latency.
The ideal would be to combine both worlds, but unicorns do not exist!
The solution is to perform the computation in BigQuery and to export the result to Cloud SQL for low latency.
Data duplication can be scary or viewed as an anti-pattern. It’s not so obvious.
BigQuery allows to export data in CSV files and to store the file in Cloud Storage. Cloud SQL allows to import CSV file from Cloud Storage.
In principle, the process seems obvious. However, it’s not so simple!
To sequence this process, an orchestration is required. Cloud Workflow is perfect for that!
Let’s write this pipeline!
BigQuery can export the data stored in a table. It’s free but you can’t select the data that you want or format/transform them. It’s a raw copy and paste of BigQuery storage to Cloud Storage in CSV format.
It’s not ideal, because we have to correctly format the data to be inserted in Cloud SQL. Hopefully, another option exists: the EXPORT statement.
This statement takes export’s options and a query in parameter. The result of the query is stored in one or several files in Cloud Storage. A simple query performs the export! And a workflow connector exists for running queries!
Let’s start our Workflow with that step.
Because you have to insert a job and read data, the service account that runs the workflow must have access to the data (be bigquery.dataViewer on the table at least) and be able to create a job in the project (be bigquery.jobUser). To write the export files in Cloud Storage, the role storage.objectAdmin is required.
Now, we have to import the data in Cloud SQL. An API call can be performed to the Cloud SQL Admin API. However, this time, no connector exists, and we have to call the API directly.
For the call, you can note the OAuth2 auth argument. It’s required to allow Workflow adding authentication header to the API call. The Workflow service account needs to be Cloud SQL admin to perform an import.
Because :
We have to:
To achieve that, we can get the status in the result of the import call. The variable is named operation. If the status is not DONE, we have to wait and check it again, else we can exit.
The API has the good idea to provide a selflink to get the operation status. The check query is easiest thanks to it.
We have the 2 sides of the process: the export and the import. We must bind them, and Cloud Storage is the place where the relation is made. But… the connexion is not so easy!
Indeed, if you have a close look at the import definition, you can import only one file at a time. And BigQuery export can create a set of files.
Therefore, we have to iterate over the files created by BigQuery during the export and invoke, for each file, the import step in Cloud SQL process (or sub-workflow)
In addition, if you have a large number of files, the Cloud Storage list API answer can contain several pages. We must iterate over the file AND over the pages also.
As you can see in my GitHub repository, the subworkflow list_file_to_import is not recursive and doesn’t call itself in case of the next page to browse. The whole result listResult is sent back to the invoker.
In fact, Cloud Workflow has a limit in the depth of subworkflow calls and you can get a RecursionError if you reach it.So, the trick is to exit the subworkflow, with the required information to let the invoker choose to call again the same subworkflow with different parameters, the nextPageToken in that case.
You can find the full Workflow code in the GitHub repository. You need to customize the assignment step to set your values, or to update this first step to dynamically get the values from Workflow Argument.You might also need to add some error management checks if you have issues to handle automatically
With that workflow built and deployed, you have to execute it when required. It can be on a schedule, with Cloud Scheduler, or on an event.
For now, you have to write a Cloud Functions (or Cloud Run/App Engine) to catch the event and run a workflow execution. However, soon, you will be able to do it with Eventarc out of the box.
Data analytics and low latency databases are complementary and, to leverage the power of both, a safe and orchestrated workflow is the right way to reach the next level.
Google Cloud community articles and blogs
29 
2
29 claps
29 
2
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
GDE Google Cloud Platform, scrum master, speaker, writer and polyglot developer, Google Cloud platform 3x certified, serverless addict and Go fan.
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/the-shadow/tales-of-a-scorched-coffee-pot-chapter-25-f6b688d774b2?source=search_post---------262,"There are currently no responses for this story.
Be the first to respond.
Though Microsoft’s cloud storage OneDrive platform uses a few different icons for its “Something Went Wrong!” message, there’s one that cracks Jack Lincoln up, every time, which would be the one which resembles a pile of doodie, wearing a birthday hat. Allegedly this is supposed to be a piece of birthday cake, but it doesn’t really resemble such, even a half-crumbled one, and anyway, that wouldn’t make any…
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/cryptoinferno/inferno-picks-storj-d8c97f69b4ce?source=search_post---------298,"There are currently no responses for this story.
Be the first to respond.
Storj, ‘Decentralized cloud storage that is automatically encrypted, easily implemented, highly performant, exceptionally economical, and ridiculously resilient.’ That’s what they say about themselves, anyway. But what does Inferno say?
Storj is one of those Old Timer crypto projects. Not as old as some, but it’s been around since 2014 and given that it’s still here, that makes it one of the longest-lasting initiatives around. It was first demoed in March 2014, at a Bitcoin hackathon. The team later raised around $500,000 in BTC.
After Ethereum launched the project ultimately became a token on the Ethereum blockchain. It has received VC funding and a successful second token sale, picking up $30 million in June 2017. Already in beta, Storj is launching properly in early 2019, which means that five years of work are about to come to fruition.
The idea of Storj is to decentralise file storage, giving the benefits of a conventional platform like Dropbox without the centralisation that puts files at risk from deletion, tampering or theft. It’s all encrypted, and the sharding mechanism means that files can be stored in multiple locations without overloading the blockchain — not every node has to store every file, but there’s enough redundancy to make sure you can always recover your documents. It works similar to Torrents, except that in the case of Storj only the file owner knows where their files are stored. A set of rules ensures that there’s enough redundancy, but not too much, which would be inefficient. Regular (hourly) audits ensure that all the pieces of the files are always there so nothing is lost.
Storage comes in at $0.015 per GB per month. You’ll find prices for commercial providers vary wildly, but that actually seems to come in at cheaper than most on a small scale.
Now, five years is a long time in crypto, and a number of other similar projects have arisen — distributed services (storage, computing, bandwidth…) are a big thing for blockchain. So does Storj have what it takes? We’ll find out in January, but with a large community of storage providers, it does look set to gain some genuine traction. You can find out more here.
Red hot news, scorching wit and searing opinion pieces from Crypto Inferno.
Join us onTelegram: https://t.me/crypto_infernoReddit: https://www.reddit.com/r/CryptoInferno/Twitter: https://twitter.com/CryptoInferno_Facebook: https://www.facebook.com/CryptoInferno/Medium: https://medium.com/cryptoinfernoSteemit: https://steemit.com/@crypto.inferno
We are your guide in a hostile crypto world.   Hot as hell.
50 
50 claps
50 
We are your guide in a hostile crypto world.   Hot as hell.
Written by
We are your guide in a hostile crypto world: analysis, insides, blockchain. Hot as hell.
We are your guide in a hostile crypto world.   Hot as hell.
"
https://medium.com/@galigio/benefits-vs-risks-of-cloud-storage-2ddfa56c35c7?source=search_post---------167,"Sign in
There are currently no responses for this story.
Be the first to respond.
Galigio
May 23, 2016·5 min read
Today utilizing an external drive is the most often used strategy for having an efficient backup storage. On the other hand, the people who contemplate utilizing cloud computing for this purpose often wonder if the technology is worth the attempt.
Users of the system ensure that there’s no reasons why anyone should keep from utilizing this system as it guarantees numerous additional benefits as when compared to the traditional methods. The truth that you’ve to fork out a “minuscule” amount of cash on a -monthly- basis for the use of cloud data storage is one motive behind prospective users to be careful.
The following benefits of the technology are cause enough to make sure that this expense property is truly worth it.
Extensive storage space: The most elementary advantage of utilizing the cloud is that you can store any quantity of information, which is difficult when using drives. The system is very simple to utilize as the account is created within a few minutes, as opposed to the effort and time spent on going searching for an additional drive.
No Physical existence: Once you have saved your data on the cloud, it becomes the obligation of the supplier to worry about its preservation. Rather than purchasing and saving these numerous added drives, one only has to stay attached to the web in order to access the stored information.
Convenience of automatic back-up: The consumers of cloud computing do not have to trouble making sure that they have related the external drive to their computers and that they take back-ups occasionally. The options on the cloud system may be altered as per the user’s setting as to whether the back-up must be taken several times in one day or once each day. The only obvious prerequisite for the system to be copied is the internet must be related and anything else is looked after.
Easy restoration: In usual conditions, locating and repairing a hard disk drive from backed up information is a long and troublesome process which calls for the providers of the pc technician. The cloud consumers are spared from such an annoyance as this restoration procedure is made quick and simple. If at all the users however have questions about managing this on their very own, they could always seek help from the companies plus they’ll be more than happy to oblige.
It is important to understand the dangers involved when transferring your business into the cloud before contemplating cloud computing technology. You should perform a risk assessment procedure before any control is handed over to a service provider.
First of all you should deeply understand what is the real privacy protection offered by the Cloud Provider. Nowadays the best (more secure and/or more respectful of your privacy) Hosting — Cloud Providers are redeploying their storage servers from USA to Switzerland. This happens because in general Swiss has a better privacy protection and it is outside the European Union and United States Laws. This means that in Swiss, disclosure order/warrant have less possibilities to have effects than in US or EU if they have not a very good legal motivation.
More in general, it is possible to affirm that any location of the Cloud Provider could be good if you understand the local privacy legislation before you use its services. Moreover the Cloud Providers have to guarantee an encrypted storage without detaining any key that can, directly or indirectly, decrypt the information you save or use through its servers. Consequently all the data transmission have to use https protocols with a recommended minimum of 1024 bits encryption. On my side I privilege nothing that offers less than a 2048 bits https protection.
Usually you can have a good representation about the security of the services offered and the legal legislation that will affect your Cloud Service contract if you read carefully the EULA.
For this reason, before committing, you should inquire which privacy and security laws will apply to the information and where your information will be saved. In case the information will be saved outside of your Country, you will also need to be informed as to the laws and regulation demands in that specific geographic place.
Moreover you have to understand if the Cloud Provider that you are choosing to support your business, is really able to guarantee an adequate service level. Don’t be worried to send inquires if the information provided in their Service Level Agreement don’t solve tour doubts or you think that the provided services could not match with your needs in future because they seems not enough flexible.
Anyway, try to prefer a Cloud Service Provider that has an effective Customer Support Service that can promptly (24/7/365) solve your problems. Cloud Service reseller are (sometimes) cheaper but they could not be the best choice for your business technical needs.
Last but not least, don’t underestimate the security holes represented by the human factor. You have to offer the right level of training to your employees and colleagues because just a single mistake by them, can be transformed, by an hacker or a competitor, in a potential debacle for all your Cloud System. Invest in learning courses about Cloud risks for all your employees and colleagues, let them understand what are the potential risks of using Cloud Services. Don’t be afraid to introduce “difficult” technical concept (as e.g. man-in-the-middle attacks, different encryption systems, social engineering tactics, the importance of digital signs, etc..) to them.
For this reason, in order to reach a better security level, configure all the electronic devices (common: tablets, smartphones, laptops & uncommon and often more dangerous: all the hardware with an embedded OS especially if they are able to communicate data through Internet or your phone provider) in order that they use your VPN by default. If you will be able to secure all the transmission through your reliable VPN you could have solved at least the 40–50% of the potential mistakes that are normally made by your employees or colleagues.
Linux, Social Media Marketing, IoT, InfoSec and AI. Trying to deep understand what will be the next step in Tech and what it’s really going on today.
See all (703)
Linux, Social Media Marketing, IoT, InfoSec and AI. Trying to deep understand what will be the next step in Tech and what it’s really going on today.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/technicity/crypto-diaries-recent-innovations-in-the-blockchain-space-5ddd3abf817d?source=search_post---------250,"Sign in
There are currently no responses for this story.
Be the first to respond.
Faisal Khan
Apr 8, 2020·4 min read
The onset of the COVID-19 pandemic has shaken all the existing norms in the financial markets and digital assets were no exception. Premier digital…
About
Write
Help
Legal
Get the Medium app
"
https://blog.upthere.com/upthere-has-a-new-home-6d3b22d2a2b5?source=search_post---------361,
https://medium.com/@darshakrana/its-a-brave-step-but-i-don-t-see-any-downside-to-it-33fa5b522e5f?source=search_post---------303,"Sign in
There are currently no responses for this story.
Be the first to respond.
Darshak Rana
·Nov 13, 2020
Vivek Naskar
It’s a brave step but I don’t see any downside to it.
Apple had been charging for cloud space since its inception. Amazon also has been following the same with its cloud storage. Google seems to following the same. I don’t think this strategy would cause any negative effects.
Though I am not aware of any nitty gritty but I learned a lot from your story. Well done.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
10 claps
10 
1
✦20 X Medium Top Writer ✦ Founder of Spiritual Secrets ✦ Writer. Thinker. Spiritual. Pacifist. Engineer. Reader. Chef. ✦ darshak.substack.com
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-cloud/automating-cloud-storage-data-classification-overview-35a63f39bb02?source=search_post---------60,"There are currently no responses for this story.
Be the first to respond.
Authors: Priyanka Vergadia, Jenny Brown
“Get Cooking in Cloud” is a blog and video series to help enterprises and developers build business solutions on Google Cloud. In this series we plan on identifying specific topics that developers are looking to architect on Google cloud. Once identified we create a mini series on that topic.
In this miniseries, we will go over the automation of data classification in Google Cloud Storage, for security and organizational purposes.
In this article we will cover the overall use case and define the problem.
Sometimes security goes beyond protection of external threats — maybe you need to keep part of your data separate from the rest of your dataset for confidentiality reasons. Organizational structure may necessitate a similar type of classification.
Either of these use-cases could be a manually-intensive processes, so we’ll take a look at how to make these situations easier by automating the classification of data you upload to Google Cloud Storage.
In the next few blogs, we’ll use Cloud Functions, Cloud Storage, and the Cloud Data Loss Prevention (DLP) API to help automate the classification of data uploaded to Google Cloud Storage.
To illustrate this scenario, we’ll work on helping our friends over at Dinner Winner (because we love food :)). Dinner Winner is an application that collects recipe submissions from users all over the world, and posts regular winning recipes for everyone to enjoy.
The recipe submissions need to be assessed for clarity and scrubbed for any identifiable information, before they are sent on to judging. And after the anonymous judging takes place, winners are contacted, and their recipes are posted to the application!
To get a solid understanding of where we might run into issues, let’s see how their system works today:
During the competition times, users upload their recipes in the text format, these recipes get stored in a blob store. From there, they’re manually checked for private information like emails and phone numbers of the participant. This private information is then removed, before sending to the judges to vote. And, as we know the winning recipe then makes it on the platform for everyone to see.
So, the issue here is multi-layered. Submitted recipes can’t mix with reviewed recipes due to the potential private information from contestants, and corruption of the contest, and the current process is manual, which means problems across the board, especially when it comes to volume. Quarantining and classifying that data can be complicated and time consuming, especially given hundreds or thousands of files a day.
In an ideal world, we would be able to upload to a quarantine location, and have files automatically classified and moved to the appropriate location based on the classification result.
So we’ve established that Dinner Winner needs an automated process for sorting submissions and categorizing reviewed submissions. Let’s break down the ingredients and review the recipe we’ll be following.
This request flow keeps the confidential data separate from the non-confidential and is fairly automated such that as soon as the file hits the Google Cloud Storage bucket, all the other processes kick off automatically to protect the data.
If you want to know more about constructing this setup, check out the next blogs in this series, where we’ll walk through step by step!
If you’re looking to classify data in an automated fashion, you’ve got a small taste of the challenges involved and the ingredients needed. Stay tuned for more articles in the Get Cooking in Cloud series and checkout the references below for more details.
Google Cloud community articles and blogs
139 
139 claps
139 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/@tiwari_nitish/deploy-minio-cloud-storage-to-mesosphere-dc-os-b0968fde8fd?source=search_post---------104,"Sign in
There are currently no responses for this story.
Be the first to respond.
Nitish Tiwari
Feb 8, 2017·5 min read
Container orchestration is gaining traction as the default way to deploy applications. Developers are architecting their modern applications from the ground-up to run in containers, which enables faster deployment and more resilience. Even legacy applications are adopting containers in every way they can to access these advantages.
Of the many characteristics that make an application container ready, the way it handles unstructured data is one of the most important. Back in the day, the default way to handle unstructured data was to dump all of it onto the server’s file system, but using the host filesystem doesn’t make any sense for containerized apps. This is because in an orchestrated environment, a container can be scheduled — or rescheduled — on any of the hosts in a cluster, but data written to a previous host can not be rescheduled with that container.
The best solution is to use a cloud storage system with an easy to use, widely accepted API to handle storage. AWS S3 is an option, but what if you’d rather be in control of your application data? What if you want to run unstructured data storage in the cloud with your infrastructure, or run a cost effective solution on premises and still use S3 as a protocol for data transfer? There is a gap between no cloud storage and completely managed object storage, that Minio cloud storage server strives to fill.
Minio is a cloud native storage server that provides an open source alternative to AWS S3.
Cloud native applications are designed to take advantage of the fluid nature of resources in a cluster. A cloud native application doesn’t need resource management that will eventually compete with a cluster’s orchestration layer; it should rely on the orchestration layer to run applications wherever resources are allocated.
As a true cloud native application, Minio focuses on storage and does that very well. It leaves out the resource management responsibility to orchestration platforms like Mesosphere DC/OS (datacenter operating system). This allows Minio to scale very well as compared to applications with their own resource management mechanisms.
DC/OS allows containerized applications to scale in a sustainable manner by running several isolated instances of the application. Take for example, a HTTP server, which can be easily containerized due to its stateless nature. With Docker containers and DC/OS you can scale your HTTP serving capacity by adding as many instances as required to handle extra load.
In a cloud native environment, scalability is not a function of the application but the orchestration.
Minio is designed to scale in a similar manner. Each of your DC/OS cluster tenants can have their own isolated Minio server instance backed by the storage required for that tenant. This way, you can accommodate new tenants and storage requirements, by adding a new Minio instance for a new tenant.
Not only scale, this design helps keep the failure domain limited.
The complexity of the first Minio instance is no different than the millionth Minio instance.
Remember, an application doesn’t automatically become cloud native when running in a container or on an orchestration platform. Design makes an application cloud native!
Deploying an application on DC/OS is simple; you can use a Universe package, or create a customized config file. We at Minio recently released an official universe package to enable single click Minio deployment on a DC/OS cluster.
In the rest of this post, I explain the process of deploying a Minio stand alone server on DC/OS with our new universe package and discuss how to scale this setup for a multi-tenant environment.
To get started, you’ll need a cluster with DC/OS 1.8 or later running. You’ll also need Marathon-LB installed. Note the IP address of the public agent(s) where Marathon-LB is running; you will need it later to locate the load balancer. Alternately, you could configure a hostname to point to the public agent(s) where Marathon-LB is running.
You can use either the DC/OS UI or the command line interface to install the Minio package.
Visit the DC/OS admin page, and click on “Universe” on the left menu bar. Then click on the “Packages” tab and search for Minio. Once you see the package, click the “Install” button on the right hand side.
Next, you’ll need to enter configuration values like the storage and service type you’d like to use with your Minio instance. Finally enter the public Marathon-LB IP address under “networking >> public-agent”, and click “Review and Install”.
This completes the install process. You’ll now need to get the access key and secret key from the Minio container logs. Click on “Services” and select Minio service in DC/OS admin page. Then go to the “logs” tab and copy the accesskey and secretkey.
You can connect with the Minio instance via either the web browser or Minio mc.
To install Minio package via CLI, type
Rest of the process remains largely same as the above GUI based install process.
The DC/OS CLI also provides options to install customized packages via the dcos install command. Refer to the CLI reference doc for more details.
Minio supports different modes, other than the default mode which we deployed above. These can come in handy based on your requirements. You can easily create deployments based on these Minio modes via a custom config script.
While you’re at it, help us understand your use case and how we can help you better! Fill out our best of Minio deployment form (takes less than a minute), and get a chance to be featured on the Minio website and showcase your Minio private cloud design to Minio community.
Minio.io
4 
4 
4 
Minio.io
"
https://medium.com/@Egnyte/businesses-are-looking-forward-to-a-hybrid-future-71507c059a48?source=search_post---------318,"Sign in
There are currently no responses for this story.
Be the first to respond.
Egnyte
Feb 17, 2018·1 min read
Even with the evolution of cloud storage, hybrid solutions offer performance, cost, and security benefits that are too valuable for businesses to ignore — and we have the data to prove it. Egnyte’s team of data scientists analyzed 14 petabytes of data to see why hybrid (a mix of cloud and on-premises infrastructure) has become such a popular deployment model for businesses:
As you can see in the visual, the growth of rich content has pushed more than 82% of businesses to utilize hybrid infrastructure. As content continues to evolve and grow, that number will only get bigger. Want to learn more about how the hybrid model can help your business? Click here.
Connect better. Protect better. Do more together.
See all (717)
Connect better. Protect better. Do more together.
About
Write
Help
Legal
Get the Medium app
"
https://architecht.io/spotifys-ai-powered-ipo-and-apple-s-google-powered-cloud-storage-76a7af53acf3?source=search_post---------96,"This is a reprint (more or less) of the ARCHITECHT newsletter from March 1, 2018. Sign up here to get new issues delivered to your inbox.
Spotify filed for its IPO on Wednesday and, not surprisingly, the company’s F-1 filing places a heavy emphasis on Spotify’s use of artificial intelligence and machine learning to power personalization. In fact, here’s the company’s leading bullet point in the section describing growth strategies:
We will continue to (i) invest heavily in research and development, (ii) make strategic acquisitions in order to enhance our product capabilities, and (iii) make our offerings more attractive to existing and prospective Users. We will continue to invest in our artificial intelligence and machine learning capabilities to deepen the personalized experience that we offer to all of our Users.
I don’t know for sure, but I suspect Spotify’s early embrace of deep learning to power machine listening has paid large dividends over the past few years. Basing recommendations and playlists on genre and related artists is tried and true, but the ultimate factor behind why we enjoy music is how it sounds. And only now are we actually able to (1) catalog pretty much every song ever and (2) use machines to analyze how they’re constructed.
On the acquisition front, Spotify also notes how some of its acquisitions over the years — including the Echo Nest one that I covered years ago while at Gigaom — have bolstered its AI capabilities.
Overall, I’d argue that Spotify is another great example of how AI can actually be applied successfully today, without getting hung up on how it’s going to completely transform your company or the entire world. You have data that’s amenable to these types of models, you analyze it, and you use the results to improve your customers’ experience. At the current moment, especially with the mass-market tools coming online, I think that’s a prudent and very achievable approach.
It’s been pretty widely known that Apple has been using AWS for iCloud storage, and this week news broke that it’s also now using the Google Cloud. That’s probably a bad idea, for some of the reasons that I pointed out in the last newsletter about Dropbox’s IPO and move from AWS to its own infrastructure. Even if cloud storage is cheaper, easier and not really a competitive advantage, it’s hard to see how lining your blood rivals’ pockets is a good idea in the long run.
This is especially true as consumer tech moves into the era of smart devices, which are part of a virtuous cycle wherein devices, data and cloud services all feed off each other. It would be a different story if Google and AWS were just selling cloud computing infrastructure, but they’re not. And by continuing to support those business lines, Apple is indirectly helping improve Alexa and Google Home, and also whatever third-party products emerge from the AI APIs AWS and Google are selling to developers.
I’ve really come around on Apple’s focus on making great hardware and putting user privacy first, but looking at it coldly it’s difficult to see how Apple will continue to compete in the smart-home era if it doesn’t start to own its entire supply chain.
I’m keeping this brief, but the bottom line is that aside from mandating where data is stored and how breaches must be handled, the EU’s GDPR also places regulations on how data can be used. Companies and applications that like to use data in inventive ways probably want to get their privacy policies and user consent methods in order. Here are a couple articles talking about this issue:
Derrick Harris
architecht.io
The rich get richer. You have to think this says a lot about Google’s plans for ramping up its business around Google Home.
geekwire.com
The Allen Institute for AI has been working on this type of problem for years, while everybody else was pursuing deep learning for object and voice recognition, and now reinforcement learning. It’ll be a huge deal if they pull it off, but even so speed and scale of the the systems will affect how it’s used.
nytimes.com
In case you forgot, Intel acquired computer-vision chipmaker Movidious a while back and has been slowly beefing up that product line. Now you can buy a motherboard to complement the USB-pluggable stick Intel announced last year.
venturebeat.com
Deep-learning-powered tools in the hands of EMTs is a cool and potentially important development. But as I wrote last year, getting people to trust these tools (and getting liability right if something goes wrong) is still a hurdle to overcome.
wired.com
I don’t know where the law of diminishing returns kicks in for corporate AI labs, but Alibaba is going international with its latest one.
cnbc.com
This is a novel approach to both PR and outsourcing the creation of models, but I don’t suspect it will overtake Kaggle anytime soon.
geekwire.com
Right. If the results are mostly accurate and humans are still involved to oversee and steer the process, the exact whys of AI models shouldn’t necessarily be a deal-breaker. This is probably a different argument in areas where there’s a real possibility of latent bias, though.
a16z.com
I largely agree with this re: jobs, but it will be incumbent upon employers to put resources into training and to give employees a clear roadmap of where things are headed and how they might change.
techcrunch.com
If you want a high-level overview of how deep learning might tie into existing data architectures and efforts, this is pretty good.
oreilly.com
This is pretty cool research, and really should be applied to self-help books, too ;-) Learning only from success is a pretty limited approach to gaining real knowledge.
ieee.org
Kate Crawford is very insightful about issues regarding bias in data and models. Why are more people suddenly so interested in this after years of discussion? “I think it has a lot to do with scale. … [I]f you have a system that’s producing a discriminatory result say, for example, in a search, or in ads, that is affecting a billion to two billion users a day, easily. So, at that scale, that can be extremely costly and extremely dangerous for the people who are subject to those decisions.”
microsoft.com
The case in the feature image seems pretty easy for even a novice to detect ;-)
ieee.org
People often ask how AI and quantum computing will work with/against each other. Maybe it will be by AI actually helping researchers understand how quantum systems work.
simonsfoundation.org
Axon, previously known as Taser, is making a big shift toward tasing people and into police body cameras and selling SaaS. This is obviously a big win for Microsoft, but you have to imagine that storing lots of evidence could put a target on that service.
zdnet.com
Patch those servers and close the UDP port, memcached users, or know you’re helping fuel a surge in DDoS attacks. DigitalOcean instances are being abused at a particularly high rate, FWIW.
cloudflare.com
Splunk is wisely building a lot of security tooling into its product.
zdnet.com
By accelerators, they mean GPUs. What if everyone was premature in writing off OpenStack for dead, because edge computing is the real private cloud, it’s going to require some sort of management platform, and OpenStack is positioning itself for that job?
nextplatform.com
If you’re running HashiCorp Vault on GCP, this should be an exciting development. I’m not sure how regularly Google helps build and support these open source integrations, but that level of involvement seems like a good thing.
googleblog.com
Just your friendly reminder to remember Alibaba.
alibabacloud.com
The Supreme Court is hearing the Microsoft email case as we speak. Win or lose, Congress has to act in order to provide some much-needed clarity here.
bloomberg.com
This is an interesting prediction, premised on the notion the cloud computing had postponed hardware refreshes, but that will change now that cloud plans are figured out. Also, AI and IoT will drive sales.
theregister.co.uk
This is probably a fair skewering of Oracle’s cloud strategy going back several years, but buried in the criticism is the fact that cloud revenue is growing faster than traditional revenue is shrinking. Also, comparing Oracle’s data center capex to Amazon, Google and Microsoft isn’t exactly apples to apples.
theregister.co.uk
If you look past the Google marketing, this is actually a pretty useful summary of a McKinsey report on cloud security.
blog.google
Well there you have it. Also, this is based on a handful of talks from the O’Reilly Software Engineering conference happening this week.
thenewstack.io
For the purveyor of such an embattled application, Facebook invests in and contributes an incredible amount of infrastructure innovation. At this point, I wonder if it’s the flagship app, or Instagram and WhatsApp, driving the efforts.
facebook.com
Beyond the FPGA-based AI accelerators it previewed last year, I would expect to see more custom cloud processors and also edge hardware from Microsoft at some point. The latter could even help it sell more software.
geekwire.com
If we ignore any potential bias because IBM helped author this study, there’s still something here. By the time many enterprises are ready to embrace something new, incumbents have also caught on or just acquired the startups leading the charge. Also, how do we classify AWS and Google?
technologyreview.com
An underappreciated failure mode due to the long-running and cascading natures of the failures. This seems like a good problem for machine learning to catch things early.
acolyer.org
architecht.io
Open source is the past, present and future of big data. The bigger question to me is where the next generation of “big data” projects will live, and how they’ll be commercialized and adopted. As I’ve said before, Apache owned the last generation but it seems like that has run its course.
datanami.com
See above. Twitter and other big-tech companies release stuff into Apache, but mostly stuff with ties to the past. Perhaps the CNCF is the home for the next generation, as the focal moves away from Hadoop.
twitter.com
This is actually a really good interview for hearing about how a large non-digital company is thinking about data and also cloud/edge computing.
theregister.co.uk
Remember (briefly) when the next big thing was going to be analytics software that “explained” correlations in natural language? That’s nothing compared with data visualization via virtual reality.
techcrunch.com
Perhaps Nvidia should be investing more heavily to make this a thing, considering the onslaught it’s about to face in machine learning processors.
nextplatform.com
Lots of new stuff, but the biggest be support for launching Spark clusters on Kubernetes, which is a boon for both projects if it works well.
databricks.com
But really, this is just a good framework for thinking about data science, generally. It’s remarkable that years after “What is data science?” seemed like an everyday question, it still needs explanation. I think the rise of AI and the backlash against “big data” didn’t help.
fast.ai
Enterprise IT interviews and analysis: AI, cloud-native, startups, and more
6 
6 claps
6 
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
Written by
Founder/editor/writer of ARCHITECHT. Day job is at Pivotal. You might know me from Gigaom - way back in the day, now.
Once a site about next-gen enterprise IT and the people building it; now a place where Derrick Harris occasionally blogs about tech-related things.
"
https://medium.com/conversations-with-tyler/tyler-cowen-ted-gioia-music-history-jazz-9a042d13b268?source=search_post---------55,"There are currently no responses for this story.
Be the first to respond.
To Ted Gioia, music is a form of cloud storage for preserving human culture. And the real cultural conflict, he insists, is not between “high brow” and “low brow” music, but between the innovative and the formulaic. Imitation and repetition deaden musical culture — and he should know, since he listens to 3 hours of new music per day and over 1,000 newly released recordings in a year. His latest book covers the evolution of music from its origins in hunter-gatherer societies, to ancient Greece, to jazz, to its role in modern-day political protests such as those in Hong Kong.
He joined Tyler to discuss the history and industry of music, including the reasons AI will never create the perfect songs, the strange relationship between outbreaks of disease and innovation, how the shift from record companies to Silicon Valley transformed incentive structures within the industry– and why that’s cause for concern, the vocal polyphony of Pygmy music, Bob Dylan’s Nobel prize, why input is underrated, his advice to aspiring music writers, the unsung female innovators of music history, how the Blues anticipated the sexual revolution, what Rene Girard’s mimetic theory can tell us about noisy restaurants, the reason he calls Sinatra the “Derrida of pop singing,” how to cultivate an excellent music taste, and why he loves Side B of Abbey Road.
Listen to the full conversation
Read the full transcript
TYLER COWEN: Hello. I’m very honored today to be here with Ted Gioia. Ted is probably my favorite writer on music and also my favorite person to read and follow on Twitter, and he has a new book out, which has made a big splash, called Music: A Subversive History. Ted, welcome.
TED GIOIA: Thank you for having me.
COWEN: Let’s start with some questions about music. Do you think our collective memory from music is decaying more rapidly because communications technologies move so much faster and preserve things so much better?
GIOIA: What people don’t understand is that, for most of history, music was a kind of cloud storage for societies. I like to tell people that music is a technology for societies that don’t have semiconductors or spaceships. If you go to any traditional community, and you try to find the historian, generally it’s a singer. Music would preserve culture; it would preserve folklore.
Well, nowadays, we rely on cloud storage to be the preserver of these same things. And I think there’s a strange shift. Both we rely on the cloud to preserve our music, but also, we no longer rely on music to preserve our culture. This is potentially a dangerous thing because it could create a situation where our musical lives grow more and more distant from our actual social lives with the people around us in our larger community.
COWEN: Does music today still carry new ideas? If you think about radio in the 1960s, there’s the idea of drug culture, of psychedelia, antiwar protests. Those are often carried by music and by the radio. But today, the internet carries the ideas more or less for free. Do we even need music for that?
GIOIA: There’s a very prevalent view now that music is just diversion or idle entertainment. You know, Steven Pinker is the great exponent of this. He calls music auditory cheesecake, and he’ll tell you that music is just for brain stimulation. For example, I would listen to a song the same way I might drink a martini or use recreational drugs.
I really think this misses the point. I do think music is embedded in ideas and culture and takes place in the world at a much more intricate level than Pinker understands. For example, hardly a week goes by when I don’t read about a musician somewhere in the world getting into trouble with politicians. Putin will try to stop a group from performing. In Saudi Arabia, somebody will be thrown into prison for a song.
Recently the Hong Kong protests have used music very actively. I just read the other day about a protest song in Hong Kong, where the composer has to remain anonymous because it’s so dangerous to have composed this song. This is a good reminder of how powerful music is. It’s not just diversion.
Another example: recently there was a rap song in Thailand that criticized the government, and this really shook up the politicians. The funny thing is, they tried to respond to it. The government released its own rap song, which of course was widely mocked and ridiculed. But it tells you that the people in position of authorities know that music conveys ideas. It conveys power. It’s not just idle entertainment. One of the things I try to do in my history books, for example, is to show that larger power of music.
COWEN: Is the advent of streaming for music a net gain or a net loss for the quality of music? Not counting convenience of the listener, but just the quality of the music. Don’t they now put all the good stuff in the first 30 seconds of the song?
GIOIA: Well, that’s right. You have to have people hold on for a few seconds in order for you to get your royalty from the stream, so people have actually changed how they compose music to match the technology. But I will say, that’s always been the case to some extent. I do believe every kind of new technology changes how people sing and perform.
If you go back into the 1920s, the first really advanced microphones were developed, and this allowed people to sing differently. All of a sudden, Bing Crosby comes along. He can use these new microphones, and he can sing at a very whispery, conversational voice, which is very expressive. So if you compare, for example, recordings made in 1925 with 1935, a completely different way of singing took place.
We’re going through the same thing right now. People are singing differently to prosper on streaming sites. I don’t think that bothers me as much as the economic impact of streaming and also just the impact on sound quality with the compressed sound. In fact, I would say that music is the only form of entertainment in which the technology has gotten worse during my lifetime.
I go to movies now, and it’s this big screen and surround sound. Video games put the Pong that I used to play to shame. TV is so good, it’s being called a golden age of television. But in music, most of us listen to songs on these lousy handheld devices. Most people in my generation had better sound systems as teenagers than they do now. That worries me more than the whole idea of how songs are written. I’m really concerned about the technology lessening the whole listening experience.
COWEN: Why has music been special in this regard? You can still buy an audiophile sound system, but it seems fewer people are interested.
GIOIA: I think the key thing is that the technology stopped advancing, so people aren’t excited about it anymore. I think it’s very useful to go back and look at record companies in the 1930s, 1940s.
RCA was the premier record company back then, but they were also the Apple computer of their day. RCA invented new microphones. They invented the 45 RPM single. They invented a lot of sound technologies, and they did it to enhance the listening experience. Classic example: in the 1960s, RCA really commercialized color television. The reason they did it is because they owned NBC, and they understood, if you improve the technology for the audience, it will get people to consume the TV shows.
What happened in the 1980s is, record labels stopped investing in new technology. The way Columbia would invent the long-playing album or Sony would invent the Walkman — that stopped, and they handed technology off to Silicon Valley, which had very different reasons. They didn’t want to increase the listening experience. They wanted to sell devices or advertising.
I do believe that there’s a fundamental change. People can, it’s true, buy high-degree audio equipment, but they don’t because the companies in the music industry have stopped innovating.
COWEN: Now, you also have a background in management consulting and venture capital. So tell us, does Spotify have a viable business model, yes or no?
GIOIA: I’m well known as a critic of streaming, and I also believe that the economics of streaming are fundamentally flawed, but I don’t believe it’s going to go away. I do believe there’s going to be a painful retrenching and downsizing. We already see Netflix, which has $15 billion in debt, announce the other day they’re going to borrow $2 billion more. They’ve got a huge audience, but they can’t even cover their costs. They’ve been negative cash flow every quarter for five straight years.
Spotify still isn’t profitable. I believe Spotify will become profitable, but they’re going to do it by putting the squeeze on people. Musicians will suffer even more, probably, in the future than they have in the past. What’s good for Spotify is not good for the whole music ecosystem.
Let me make one more point here. I think it’s very important. If you go back a few years ago, there was a value chain in music — started with the musician, worked for the record label. The records went to the record distributor. They went to the retailer, who sold the record to the consumer. At that point, everybody in that chain had a vested interest in a healthy music ecosystem in which people enjoyed songs. The more people enjoyed songs, the better business was for everybody.
That chain has been broken now. Apple would give away songs for free to sell devices. They don’t care about the viability of the music sub-economy. For them, it could be a loss leader. Google doesn’t care about music. They would give music away for free to sell ads. In fact, they do that on YouTube.
The fundamental change here is, you now have a distribution system for music in which some of the players do not have a vested interest in the broader musical experience and ecosystem. This is tremendously dangerous, and that’s the real reason why I fear the growth of streaming, is because the people involved in streaming don’t like music.
In fact — and this is amazing — the CEO of Spotify said, “We’re not in the music business. We sell subscriptions. We don’t sell music; we sell subscriptions.” That’s very dangerous, and that tells you that you have parties here that are going in completely different directions, and it’s not going to be good for the health of our music culture.
COWEN: What’s the chance we simply regret the entire advent of the internet?
GIOIA: Well, I don’t think that’s true. I am not one of these Luddites. I hear people in the music industry saying, “Oh, Ted, everything would be great if the internet would just go away.” The record labels wanted to make that happen, so they sued everybody. They sued Napster. In fact, I say that the record labels had their strategy, which I call the three Ls, which was lobbying, legislation, and litigation.
I don’t think those are strategies. The strategy they should have is the consumer experience. I wish that the record labels had invested in new recording technologies — super vinyl, better technology for the listener — instead of just suing everybody and trying to pass new laws.
COWEN: As you well know, if you compare American popular music in 1963 to American popular music in 1968, over a period of five years, it sounds really quite completely different. And you can tell, upon hearing music from either year, which year it’s from. That doesn’t seem to be the case today. Is there still real innovation in American popular music?
GIOIA: Well, what happened in the 1960s is an anomaly. I don’t think it will ever happen again. And what happened is — you must give credit to the Beatles because everyone was imitating the Beatles, and they changed their sounds every six months, every year.
People always tell me, “Ted, do you like the highbrow music and the lowbrow music?” I say, “No, no. The real conflict is not between high and low. The real challenge in music is the formula, where formula emerges, and then everybody imitates the formula.” And that’s what deadens your musical culture, the repetition of the formula. And the Beatles, for a period of five, six years, made sure there was no formula. This is amazing. It never happened before. It probably will never happen again.
The real challenge in music is the formula, where formula emerges, and then everybody imitates the formula. And that’s what deadens your musical culture, the repetition of the formula.
If you tried to imitate the Beatles in 1964, ’65, you soon were out of date. For example, the Monkees tried to do exactly that. The Monkees imitated a certain Beatles sound. But by the time the first episode was on TV, the Beatles were already off to something else.
I do think you had this amazing period for five, six years where there was no set formula in the music business. It was an amazing time. I think we should enjoy it for what it was, but I don’t think we should expect it to come back.
COWEN: What’s the most subtle Beatles song?
GIOIA: The most subtle Beatles songs?
COWEN: Yes.
GIOIA: Well, that’s a hard one. I’m one of these strange chronologists who believed the Beatles just got better and better. And I think you could make a case each album is better than the last.
When you get to side two of Abbey Road, for me, and when they stitch together all these song fragments — these weren’t even completed songs. These were just things they had been sitting on, and they found some way to bring it all together into this album in which they had a complete suite to literally end their career on the album. To me, that’s probably the epitome of both subtlety and artistic expression.
COWEN: But if we look at broader trends — so in the 1960s, we have what we now call classic rock and the Beatles, also in the ’70s. The early 1980s, rap comes along. It’s still with us — maybe that’s surprising. In the early ’90s, you might say there’s electronica. But what has been since then that’s new? What’s the next big thing? Or has it stopped?
GIOIA: Well, there’s a certain irony here. The music business always prided itself on disrupting the culture with some new sound. But the big thing in the last 20 years is, the music business itself has been disrupted. They’re on the receiving end, and tech companies in Silicon Valley have done the disrupting. And what they’re disrupting is not the sound of music. It’s actually the whole socioeconomic setting of music.
Now, you ask yourself, what did the music industry do to respond to this? What was their big innovation? And it’s almost laughable. If you go back to the early years of this century, when the internet was taking over music distribution and the music culture, the biggest innovation in music was — and this is sad to say — it was the TV reality-show singing contest. [laughs] This was the big innovation that the music industry used to respond to this complete disruption and everything else.
“Well, we’ll do American Idol; we’ll do America’s Got Talent.” It would be funny if it wasn’t so pathetic.
I want to make one more observation, though. There are periods in music where it seems like there’s a lull and that we’ve reached some happy end point. My historical research tells me that those happy end points never last. Take for example, the rise of rock and roll. Five years earlier, the music industry thought they had it all solved. “Oh, people want lifestyle music. We’ll do romantic ballads by Sinatra.”
They came out with mood music albums. They invented these amazing albums called bachelor pad albums back then, and novelty songs. And the music industry in 1952, 1953 thought they had it all figured out. They thought they had reached the perfect song for every moment in your day. And then five years later, rock and roll changed everything with disruption.
How did this happen? What they don’t realize, people want disruption. The same thing happened in the 1970s. The early 1970s was the gentle singer-songwriter. You could take those Carole King albums or James Taylor albums. You could play them for Grandma. Grandma would love them. You couldn’t have something that was more beautiful and had wider appeal, but people wanted disruption again. So at the end of the ’70s, you had punk rock, disco, new wave.
The important thing to understand is, not only is there disruption in music. People crave the disruption. Look at right now. Right now, we’re told that we’ve reached this happily ever after. Tech companies were going to use artificial intelligence to find the perfect song for every moment in our day. It will be curated. There’ll be these wonderful feedback loops. Robots will create the perfect musical life.
My belief, based on my understanding of music history, is that won’t happen. There will be disruption, and it will come from an unexpected place, and this happily ever after we’ve supposedly reached won’t last.
COWEN: What’s the popular song you’re most embarrassed to admit to really liking?
GIOIA: What are the popular . . . I mentioned the Monkees before. I like the Monkees.
COWEN: So do I. “Last Train to Clarksville” is excellent. Gilbert O’Sullivan, I’m a big fan of, and that’s kind of terrible.
GIOIA: I love well-crafted pop music.
COWEN: But what really embarrasses you? What admission can I squeeze out of you?
GIOIA: When I was a teenager, I listened to Emerson, Lake, and Palmer.
COWEN: Now that’s embarrassing.
GIOIA: Right before I discovered jazz, I was listening to Keith Emerson. This was the quandary I was in.
COWEN: It was jazz, in a way.
GIOIA: It prepared me for jazz. It really did. When I was a teenager, I was playing piano, and this was the problem I faced. I liked rock because of its emotional immediacy, but it didn’t have the sophistication I wanted. Then I loved classical music like Bach for the sophistication, but it didn’t have the emotional immediacy. And I said, “I need something that brings together both.”
Then I walked into a jazz club. Literally, I walked into the Lighthouse in Hermosa Beach, California. I was a high school student. I sat down, the music started, and within 10 seconds, I said to myself, “This is what I’ve been waiting for.” Really, it was this epiphanal moment. But before that, it was Keith Emerson.
COWEN: Who is the best shaman in the history of rock music? It’s a key theme in your book, right? Music: A Subversive History. The importance of the shaman, the rebel, the outsider.
GIOIA: I would say Bob Dylan. I think that there’s this mystical aspect of music in which it becomes very hypnotic, but it’s also embedded in the culture. It’s life changing. It reaches on a personal level but also has these broader social levels. I don’t think anybody’s brought that together better in my lifetime than Bob Dylan.
I was very happy when he won the Nobel Prize. I know purists were angry. Songs are not literature. But hey, if you go back to the roots, Homer was a singer. If you go back to the roots, Sappho was a singer. The origin of literature is song. So for someone like Dylan, he would capture that for me.
COWEN: Was Jim Morrison a wonderful shaman but a terrible musician and poet?
GIOIA: I think he was. How many times can you rhyme fire and higher?
COWEN: “I Am the Lizard King,” right? That’s shaman-like.
GIOIA: Okay, I will not dismiss the importance of someone’s charisma and their personal magnetism. And someone like Jim Morrison possessed that definitely. And you have to give the Doors credit for that. But if you were evaluating Morrison just on his songwriting skills, I don’t think they hold up.
It’s funny, one of my favorite pianists, Friedrich Gulda, did these theme and variations on “Light My Fire.” I thought he was a brilliant musician, but that was just not the piece you could turn into theme and variations. It was even worse than Diabelli’s Melody that Beethoven worked with.
COWEN: Is Gulda better as a Beethoven player or a jazz musician?
GIOIA: Well, he’s famous more as a Beethoven player, and he’s great, but he’s very underrated as a jazz musician.
COWEN: But also is a Beethoven player. His Sonatas №1, №25 are maybe the two best recordings of those sonatas.
GIOIA: And Gulda’s an amazing guy. I don’t know why he’s forgotten. He’s also a great composer. His compositions should be resurrected. Occasionally I see someone play them. But a fascinating guy, and he’s one of these people that was punished by being able to do too many things well because society wants to put you in a pigeonhole.
I felt this in my own life. Society wants to put you in a certain pigeonhole that you just do this, and if you don’t want to live in that narrow space, you’re punished. And Gulda was like that.
COWEN: If we think about the accelerated advancement of women in music — today it might be St. Vincent, Laura Marling, but there are many, many more. Do they fit into the shaman model the same way that the men do?
GIOIA: Well, my book shows that, again and again, throughout history, female musicians were innovators. Then once their work was assimilated by the broader society, these origins were hidden from view. One of the reasons why women were innovators is because their music tended to tap more into the emotional power of music.
If you go back to Plato, it’s very clear that there are two kinds of music. There’s a music that he favors, which instills order in society. It sings the praises of great men, and it brings people together for the social good. But he understood that there was a second kind of music that was really involved in personal emotion and self-expression. He associated that with women, and rightly so, because that really comes out of Sappho. And it comes out of what I call the three Ls of female singing: the lullaby, the love song, and the lament, always associated with women.
So the idea that women have a special place in musical culture going back for thousands of years is something I take very seriously. And I think it’s surprising, in fact, nowadays, how much connection there is between the music women sing now and the origins of the music that came out of women thousands of years ago.
COWEN: Why are restaurants so much noisier today? And they’re still getting noisier.
GIOIA: In fact, I’ve got to say I prefer the quiet restaurant, but I understand everybody else wants the noisy restaurant. And I do think we’re going back to René Girard territory, where everything’s imitation, where you choose the restaurant not on what’s the best food, but what other people are doing that I can imitate. There are two restaurants in town. You go in the one with the most people. I think that imitative behavior patterns explain much more in society than we care to admit.
COWEN: But there’s much more noise pollution more generally. Restaurants are noisier. It seems that music, in general, is louder. And in terms of dynamic compression, the range is much narrower. So why is there this general tendency toward more noise? Why are markets undersupplying peace and quiet?
GIOIA: Because they want to stand out. It’s interesting, in my book I talk about the very first musicians, who were hunter-gatherers. What they did was fascinating because back then there were no loud sounds. You could live your whole life in prehistoric times and maybe never hear a loud sound unless you went near a waterfall or maybe during a thunderstorm. But for the most part everything was quiet.
So that’s why there’s a plausible theory that the early hunter-gatherers invented choral singing to hunt. They were scavengers, and they didn’t try to kill the lion themselves. They let the lion kill the prey. Then they would sing together to scare away the lion, and they would get the food. That tells you that back then, loud sounds were so rare that they were an amazing expression of power.
You could live your whole life in prehistoric times and maybe never hear a loud sound unless you went near a waterfall or maybe during a thunderstorm. But for the most part everything was quiet.
So that’s why there’s a plausible theory that the early hunter-gatherers invented choral singing to hunt.
The thing to remember is, even today, loud sounds are an expression of power, notoriety. So you have competition in terms of sound, and the restaurants believe — and maybe rightly — that they’re going to stand out with the noisier environment. Now, once again, I will avoid those restaurants. I’ll go to the quiet one, but I really think the same way there was an arms race in the 1960s, there’s a noise race in society right now.
COWEN: When and why did jazz stop being cool?
GIOIA: Well, you know, cool stopped being cool. I wrote this book called The Birth and Death of the Cool a few years ago. It was a shock to me because I was going to write a history of cool as this timeless thing. People always want to be cool. Then as I was doing the research, I realized that, in fact, the cool ethos was something that had a beginning and an end. In around the year 2000, cool became uncool.
COWEN: Why did that happen?
GIOIA: Why? I’ve got a larger theory — which is still very primitive — about these 50-year cycles between hot and cool in society. I think we’re in the middle of a hot phase right now. It’s a culture of anger we live in. I don’t think anyone can deny that.
But I do believe that you had this birth of the cool with Miles Davis recording that around the year 1950, and for the next 50 years we had a cool society in which cool jazz was at its peak. There were certain cool techniques that you used in your day-to-day life, which were irony, humor. You would deflect aggression with irony or with a joke.
Then around 2000 — and people will say 9/11 made this happen, but I think the roots were already before that — everything changed. People now pride themselves on their frankness. “I’m going to say it just the way it is. I’m going to say it right to your face.” And so it goes, from cool to hot.
Back to your earlier question, when did jazz stop being cool? Well, as cool itself phased out, jazz lost a lot of its audience, and jazz itself had to embrace more of a hot dynamic to thrive in the new setting.
COWEN: Can jazz today still sound hot in a musical world where basically all the rhythms have been tried out? People have heard everything. If you play Varèse, “Ionisation,” for someone today, they might like it even. It doesn’t sound that weird, but they’re not deeply impressed. It doesn’t sound that radical. They’ve heard rap. They’ve heard electronica. Are the rhythms of jazz just boring in 2019?
GIOIA: Well, I think there’s a larger question there that relates to that. As you may know, I’ve listened to lots of new music. Over the course of a given year, I’ll listen to more than a thousand newly released recordings.
COWEN: And every year you put out a great list of what is best from what you listen to. I’ll just tell everyone.
GIOIA: That’s right, my 100 Best.
COWEN: It’s on your home page.
GIOIA: I spend two to three hours per day listening to new music. One of the things I’ve observed is that the most creative music happening these days usually is at the intersections of different genres because the genres themselves have been overcome with the formula. As I said a little while ago, the formula is the curse. If you try to get on country radio, there’s a certain set sound you imitate, and it leads to this sameness. But there’s amazing music being made in the intersection of the genres.
For example, in jazz right now, there’s amazing music happening in Los Angeles and London, of musicians who are taking jazz music and contemporary music and dance beats and hip hop and R&B, even some other classical and folk ingredients. They’re bringing it all together, and it does sound exciting, and it sounds new, and it doesn’t sound clichéd, but it’s only because they’re leaving the formula behind.
Unfortunately, and this is the real mismatch in our society, the music industry rewards you for following the formula. That’s how you get on radio. That’s how you get a record contract. But in fact, the creative world rewards you for going into these intersections between the genres.
COWEN: Why are the blues disappearing from popular music?
GIOIA: That’s interesting. I think we’re reversing this whole amazing process that happened during the 20th century. I like to think of the African influence as reversing what I call the Pythagorean Paradigm, which emerged 2,500 years ago, where Pythagoras developed the first scales in Western music. But even more important, he developed this mathematical notion of music that every note came in a scale, every scale was tuned, and you always played in tune.
We accept this so instinctively that we don’t even think twice. When you go to the orchestra, the first thing they do is tune up. Well, in the 20th century with this infusion from blues and jazz, you could play notes that were no longer in tune. You could bend the note. You could distort the sound, and this was an amazing reversal of 2,500 years of mathematical Pythagorean music.
COWEN: But early classical music did that plenty, right? Was the Pythagorean tradition ever so dominant?
GIOIA: If you go back — and of course it’s hard to speculate on what music sounded like 2,000 years ago — but if you read the books, they are obsessed with tuning and scales. The music guides from ancient times are obsessed with tuning systems, and they tried to squeeze out sounds that didn’t fit into the scale.
So you had a reversal in the 20th century with the advent of black music, but now that’s dying out in many ways. Just auto-tune — the idea is every note’s going to be perfectly in tune. Or a lot of this digital music — everything is perfectly in tune, and the bent blues notes are disappearing.
COWEN: But isn’t it that distortion and note bending have been picked up by, say, indie music, and they’re being done in more commercially appealing ways than the blues, and maybe the blues is a bit exhausted?
GIOIA: I think for a while it was being assimilated in popular music, but I believe that has stopped in the last five, ten years. I would love to see some PhD student study hit records of different decades and figure out how frequently notes deviate from pitch, and how that’s changed over time. I think you would find an amazing reduction in that.
So we’re going back to this mathematical vision of music as everything is perfectly in tune and perfectly organized and perfectly aligned rhythmically. There’s less syncopation in music. Everything is becoming regularized. Now probably, the advent of digital technologies leads to it, but I think we need a new African infusion in our music that shakes things up and gets us away from mathematics and back toward sound.
COWEN: And rap music is not that new infusion for you? Yes or no?
GIOIA: Well, rap music is also becoming formulaic. Rap — it’s interesting — 30 years ago, NWA did an album that was so dangerous, the FBI tried to shut down the record label. Nowadays, that same album has been enshrined as a national historical treasure by the Library of Congress. The Smithsonian last year announced they’re putting together a panel of 50 academics and scholars to come up with the official Smithsonian guide to hip hop. So rap and hip hop is also becoming streamlined and mainstreamed.
Also, you see — and this has been going on for a number of years — rappers, to have really big sales, team up with a pop star. So you have that melody, which is often very auto-tuned, and once again, very much this Pythagorean mathematical approach.
There’s people trying to tame hip hop, so it’s hard to understand where the disruption’s going to come from. I tend to think there will be a big disruption in music over the next few years, but it won’t be a repetition of jazz or hip hop or something from the past. There’ll be something new coming at us, which will surprise us.
COWEN: Are prosperity, peacetime, and racial integration bad for the blues?
GIOIA: Well, that’s a big question. I think that you look at the blues, and it’s definitely a music of anguish and tragedy and personal suffering. But from a completely different point of view, the blues did other things as well. It broadened our musical techniques, and also, it was a music of lifestyle values.
I tend to think that the blues anticipated the sexual revolution of the 1960s. All the things that were sung about in blues music in the 1920s were about these lifestyle excesses that didn’t reach mainstream society for 40, 50 years.
COWEN: But we had them in the ’20s too, right? The ’30s are sexually more conservative than the ’20s had been, it seems.
GIOIA: Well there was, but in fact, it’s very interesting to go back and see this. For the first 10, 20 years of the 20th century, if you wanted to sing about sex in a song, you would put a picture of black people on the cover of the sheet music, and the idea was that it was acceptable.
You know “Won’t You Come Home, Bill Bailey?” It’s a song about an adulterous affair. You could sing that song, you could publish that song, but you had to associate it with black lifestyles, and there were a bunch of other songs as well. Whenever you sang about something that was sexy or dirty, you put black people on the cover of the sheet music.
This is how it entered the music industry. You’re right, there were certain behavior patterns there that were everywhere, but where it really went mainstream and came out and was considered acceptable, it’s acceptable in many people’s eyes to have free love or altered mind states. That was the 1960s, and that came out of blues music first, when it first was sung about on the airwaves.
COWEN: How and why was John Hammond such an amazing spotter of musical talent?
GIOIA: Well, those people have disappeared. Those people have disappeared.
COWEN: Why is that?
GIOIA: I don’t know. I really don’t know, but I do think there was this notion of having a talent scout who knew a lot about music, had been around for many years, really understood the ups and downs, often was a musician, too. And someone like Hammond then could discover Billie Holiday or Aretha Franklin or Bob Dylan. Bruce Springsteen he discovered.
COWEN: Count Basie, many others.
GIOIA: Count Basie — you could go on for a long time — out of his cumulative experience. In the 1960s, though, the record industry decided that the best way to scout new talent was to rely on young people. This is sad but true. They believed that the path to success was to match their music to the musical taste of a 16-year-old. Because of that, they needed young people that understood that vibe.
For a while, that worked really well. But what you sacrificed over the long run was having people like John Hammond, who had seen music change up and down, in and out, over a period of decades, and brought that larger vision.
I think it’s a larger thing of how do record labels spot talent then? How do they spot it now? What they’re doing now doesn’t seem to be working. They’re having enormous trouble building careers of younger musicians. Even if the first album does well, there’s this sophomore curse. And I do think that it would be good if they took more time understanding the process of discovery because discovering talent for a record label is as important as R&D for a tech company.
COWEN: Let’s try a round of overrated versus underrated.
GIOIA: Let’s go.
COWEN: Heavy metal music — overrated or underrated?
GIOIA: I think it’s underrated. If you say that you like metal music, that’s supposedly shameful, or maybe you’ve got some dark, satanic impulses. But metal music has persisted at very high levels of virtuosity, and they take musicianship seriously. They take the entertainment aspect of it seriously. Sometimes it’s almost performance art. I’m not personally a huge fan of metal music, but I think in terms of the whole music ecosystem, it’s tremendously underrated.
COWEN: Leo Tolstoy.
GIOIA: I think Dostoevsky was better. I know that’s controversial. Okay, let me clarify this. I think Tolstoy is overrated as a novelist, but he’s underrated as a social figure because he influenced people that joined these communes. There’s still Tolstoyan communes out there in Britain. Many people adopted vegetarianism because of Tolstoy. People changed how they viewed marriage.
The impact he had on the broader society has been hidden from view. The Soviet Union tried to change Tolstoy’s image because he was so dangerous. They reframed him as just a novelist.
COWEN: A nationalist to some extent.
GIOIA: Very much so, but I would think that if you view Tolstoy just as a novelist, you don’t understand the full scope of his influence.
COWEN: Horror fiction — overrated or underrated?
GIOIA: I believe all genre fiction deserves more credit from serious literary figures. There’s a tremendous amount of creativity. Stephen King’s one of the best writers out there. Those novels are beautifully crafted, and in terms even of things like character development, plot structure. No, horror fiction’s underrated.
COWEN: Elton John.
GIOIA: Elton John in the 1970s, underrated. Elton John after the 1970s, overrated. His early material is amazing, is amazing. Just the number of good songs, including ones that were never hits, and his singing — his singing’s extraordinary. His singing is still good, don’t get me wrong. But the way Elton sang in the 1970s, very underrated.
COWEN: R.E.M. — overrated or underrated?
GIOIA: Overrated.
COWEN: Why?
GIOIA: Once again, it feels too much like a formula for me.
COWEN: But say, the first two or three albums — wasn’t it something new?
GIOIA: I’m going pass on that.
COWEN: Murmur, “Letter Never Sent.”
GIOIA: Like I said, to me, it sounds formulaic. But once again, I’d have to go back and listen again to give you a more authoritative view on that.
COWEN: Stan Kenton.
GIOIA: Terrible person in his private life.
COWEN: What did he do?
GIOIA: I think he sexually abused his daughter.
COWEN: Oh, I didn’t know that.
GIOIA: I don’t have all the details at hand, and those kind of aspects I don’t like to probe into, but there’s this whole issue of how do we deal with Michael Jackson? How do you deal with Roman Polanski? My general belief is that, most of the time, you’ve got to separate artistry from personal life — most of the time. But I do believe that there’s a certain crossing point. Like Hitler and painting, for example. You don’t put a painting exhibit of Hitler on, regardless of what the quality is.
COWEN: But it was also bad. In that sense it’s overdetermined, right?
GIOIA: But the whole question is, should Michael Jackson’s music be put off the air? My general belief is Kenton was a great innovator in jazz, but I believe that does not . . . And I want to be very clear about this because I feel this very strongly. Your artistic skill, your intellectual ability, your talents do not give you permission to violate rules of human decency in your private life. That said, you do evaluate the artistry, for the most part, on different terms. But I don’t like it — all these artists that think that they can exploit people just because they’re famous.
COWEN: K-pop — is any of it interesting?
GIOIA: I have yet to hear the K-pop album that excites me. But I do believe, in principle, the idea that we’re getting musical innovations from outside the English-speaking world that are going global is a good thing. Personally, K-pop’s not my cup of tea, but I love this idea that someone can upload something on the internet today in Indonesia, and I can hear it at home tomorrow.
COWEN: Who is the great underrated 20th-century classical music composer?
GIOIA: My favorite is Shostakovich, who, to me, is at the peak for a variety of reasons, both in terms of just the sheer artistry of the works and also his ability in his personal life to take oppression and tragedy and not only overcome it, but use it as an impetus to get even to a higher level on his creative platform.
COWEN: Where should the neophyte start with Shostakovich?
GIOIA: I love the symphonies and the string quartets, but those preludes and fugues — oh, that’s maybe my favorite classical piano work of the 20th century.
COWEN: Which recording?
GIOIA: The Jarrett one is probably the place to start, surprisingly enough. I would say Keith Jarret, yeah.
COWEN: John or Paul — who was better?
GIOIA: I am more of a Paul fan because he had the better musical craft. But he needed John because John had the edginess. This is my view: if it was just Paul without John, it would have become sentimental, and you wouldn’t want to listen to it. If it was just John without Paul, it would have become incomprehensible. You needed both. But if I had to pick one, I’d go for Paul because with my background as a musician, Paul mastered the craft more.
COWEN: What is it that you sing in the shower?
GIOIA: Unfortunately, it’s the same thing everybody else sings, which is probably some lousy commercial jingle or some pop tune I heard the previous day. I’d love to tell you that I work through opera arias or I do the Wagnerian repertoire, but no. It’s probably the Juicy Fruit jingle or the Beverly Hills theme song.
COWEN: Is Prince actually an interesting musician?
GIOIA: Yes.
COWEN: So much quantity, but what is there in Prince when you go back to it, it sounds better?
GIOIA: I like the stuff right before he died. He was going to go back to just piano and voice. It was jazzier. It was more downscaled, and a little of that was recorded, but I would have loved to have seen where Prince would have gone if he had lived another 10 years.
COWEN: Do you think music today is helping the sexual revolution or hurting it? Speaking of Prince.
GIOIA: It’s very interesting. If you go back to the earliest songs in human history, they were linked to fertility rituals. There was an idea that the king would have sex with a goddess, which, usually, the high priestess had to step in because it was hard to find a real goddess, and there were songs associated with it. They were very explicit. Some of them I couldn’t even say to you, Tyler, because I would get into trouble because of the explicit quality of the works.
The point I would make is, songs these days are very similar. Someone studied recent hit songs, and 92 percent of them refer to sexuality. The typical hit song has 10½ reproductive phrases. That’s the word the researcher used — not just the dirty parts, the reproductive phrases in the songs. I do think there is — and this I bring up in my new book — the long-standing connection between music and sexuality.
Even as we see a new Victorianism and sexual primness entering our larger mainstream culture, there’s a tremendous force that forces popular music to address sexual issues.
COWEN: But is music in some way antisex or a substitute for sex? Or maybe some kinds of romantic music, like Bruce Springsteen — they’re best for people who are not in love? And if you’re actually in love, you don’t need Bruce Springsteen. And now we’re doused in music and the internet, and we have less sex.
GIOIA: Well, I believe, actually, Darwin was right. He thought music was linked to sexual selection, and we use music to attract a mate.
COWEN: When you created it, right?
GIOIA: No —
COWEN: But now we’re in a world where you don’t have to create it.
GIOIA: It’s very interesting. There’s market research and focus groups about how people use music in their day-to-day life. Take, for example, this: you’re going to bring a date back to your apartment for a romantic dinner. So what do you worry about?
Well, the first thing I have to worry about is, my place is a mess. I’ve got to clean it up. That’s number one. The second thing you worry about is, what food am I going to fix? But number three on people’s list — when you interview them — is the music because they understand the music is going to seal the deal. If there’s going to be something really romantic, that music is essential.
People will agonize for hours over which music to play. I think that we miss this. People view music as distance from people’s everyday life. But in fact, people put music to work every day, and one of the premier ways they do it is in romance.
COWEN: Let’s say you were not married, and you’re 27 years old, and you’re having a date over. What music do you put on in 2019 under those conditions?
GIOIA: It’s got to always be Sinatra.
COWEN: Because that is sexier? It’s generally appealing? It’s not going to offend anyone? Why?
GIOIA: I must say up front, I am no expert on seduction, so you’re now getting me out of my main level of expertise. But I would think that if you were a seducer, you would want something that was romantic on the surface but very sexualized right below that, and no one was better at these multilayered interpretations of lyrics than Frank Sinatra.
I always call them the Derrida of pop singing because there was always the surface level and various levels that you could deconstruct. And if you are planning for that romantic date, hey, go for Frank.
COWEN: What was music like in ancient Greece? What do you think?
GIOIA: Well, I do believe that there were different types of music. There was this officially sanctioned music that was preserved, but there was a more exciting music from outsiders that people liked but also feared.
Here’s an interesting thing. If you’re a music student nowadays, one of the first things you learn are the musical modes. These are essentially the building-block scales of music. And they have names. There’s the Phrygian mode, there’s the Lydian mode. But what no one tells you is the Phrygians and the Lydians were the enslaved populations in Greece, and they have these exciting sounds. Those were the dangerous modes. So the exciting sounds were associated with these enslaved people and outsiders.
Unfortunately, the musical culture that’s preserved is the kind that supported the ruling class. For example, the most popular lyric poet in ancient Greece was Pindar, who sang the praises of heroic men and powerful people. This is the establishment music, but there was another kind of music that was darker, deeper, and more emotionally intense.
COWEN: What are three parts of the world where their musics are currently seriously underrated?
GIOIA: Well, I tend to think that the places to look are the countries that have large population bases, diversity, but don’t get heard often on the radio or don’t get picked up much by record labels. Let’s take, for example, Brazil and Indonesia. It’s very interesting.
There’s a jazz piano player named Joey Alexander. He’s a kid. He was a child prodigy. He got a record contract around age 14. He’s from Indonesia, but he had to move out to New York. He’s now winning awards and showing up on the Grammys. When his jazz album came out, somebody told me this was the first time in history an album by an Indonesian music hit made the billboard chart.
That’s amazing. Indonesia has a huge population and a great musical culture, a very diverse one, too.
I try to listen to music from all over the world, so I’m always trying to figure out what is happening in Asia. Just two days ago, I did a search on Bandcamp for any recording in the last month from Jakarta. I’m trying to find these things. So I would look at those large countries with diverse populations that have been left out of the music industry’s radar screen.
COWEN: Why is the music of the pygmies so good?
GIOIA: Well, listen to how they sing! There’s a view in the public mind that if you listen to traditional African music, it’s just intense rhythms. But that’s not true at all. And the pygmy music — a lot of this is this vocal polyphony.
They sing stuff, and I swear, you could not write it down. If you’re a trained musician, you have a good ear, you can listen to this over and over again, and you can listen to it the 20th time and you’re still saying, “What’s going on here? How are they doing that?” That’s one of these great musical experiences where people can go through their whole life and not hear it, but they really should track it down.
COWEN: What’s the solution to contemporary classical music being too academic? Or would you not accept that premise?
GIOIA: Contemporary classical music now is great. There’s a lot of exciting things happening, and classical composers are mixing up classical music with rock and jazz and folk styles. There’s all sorts of interesting things happening.
But the problem is, they created a public image over the course of 50 years that classical music was just noise and difficult to listen to. So they’ve got to overcome their own reputation. But if you sit down and listen to contemporary classical music seriously over the course of several months, people would be amazed at how much they enjoy it.
COWEN: After Philip Glass, what is it from contemporary classical music that will last?
GIOIA: Well, this is interesting because, like I say, you could listen to Jennifer Higdon or David Lang. There are a bunch of great composers out there. Caroline Shaw — wow, just amazing stuff. I don’t know if it’s found an audience yet. It deserves to find an audience. It deserves to last. But the question is going to be more a socioeconomic one. Will these great composers that are around us now get the platform they need to reach people?
COWEN: What percent of the people who go to the opera actually enjoy it?
GIOIA: It’s interesting. I came to opera late, and when I did, I actually had to force myself to listen to opera. At a certain point, I realized this was a gap in my musical education. There was a period, day after day, I would just listen to the opera, and I would study it to fill that gap. But I’ve met many people that are passionate about it. My older brother, Dana.
COWEN: He writes operas.
GIOIA: This is interesting. I was named after my uncle Ted, who died before I was born. He was a merchant marine, never went to college, died in a plane crash. And my mother says that Uncle Ted was the smart one in the family and that supposedly he had all Dante’s Divine Comedy memorized in Italian. You could just open up the book at any line and say it and he could quote from it.
He corresponded with a great Mozart scholar, Alfred Einstein, on Mozart because my uncle was an expert on Mozart. He was just a sailor. Never went to college. When they came out with an album by Haydn’s brother of music, they had to get my uncle to write the liner notes.
COWEN: You mean Michael Haydn.
GIOIA: That’s right. At that time, the only biographies were written in German, so they had to go to my uncle, the sailor, to help them with the liner notes. Anyway, the story I’m told is that when he was 11 years old or 12 years old, he snuck out of home one night where he hid clothes in his bed and then snuck out the window, took the streetcar into downtown LA to see the opera. Twelve years old! So that’s passion.
Yeah, there are people out there that are absolutely passionate about opera. They will live and die for it, God bless them. But you don’t sell season tickets just on the basis of those people. There are a lot of folks that are just going there to get their cultural street cred.
There are people out there that are absolutely passionate about opera. They will live and die for it, God bless them. But you don’t sell season tickets just on the basis of those people. There are a lot of folks that are just going there to get their cultural street cred.
COWEN: You mentioned your brother, Dana, who’s been a well-known poet, CEO. He ran National Endowment for the Arts. Where do you and he most disagree about music?
GIOIA: Well, like I say, he —
COWEN: He loves Samuel Barber more than you do, or what would it be?
GIOIA: Like I say, Dana loves opera, he loves opera, he’s passionate about opera. And opera is . . . like I say, I’ve gotten an education in opera, but it was very pragmatic, doing that. I’m more of a jazz person. But Dana knows a remarkable amount of jazz. You get Dana involved in a music conversation, even though that’s not his specialty, he can keep up with me point for point on everything.
COWEN: We’ve been talking about music, but you’ve had other lives, in fact. Was it a waste of time to get a Stanford MBA?
GIOIA: Well, I did it for the worst possible reason. I came from a poor family, and when I went to college, I felt I owed it to my parents to get some marketable skills. Even though I knew that jazz was my thing, and I was practicing the piano three hours a day, and I was very interested in arts and culture, I got this MBA to have those marketable skills because I felt that I owed that to Mom and Dad.
But as it turned out, it has proven very useful. You’d be surprised how much of my music research, I rely on what I learned at the Boston Consulting Group and McKinsey, just applying those analytical skills.
COWEN: What would be an example of that?
GIOIA: Well, let me give you an example. When I was at the Boston Consulting Group, we were analyzing new product introductions. So, the idea—a new product is coming out, a new computer or something, and you have to predict how it’s going to sell over the next five, ten years.
We had these mathematical formulas we used to predict the spread of an innovation, and I found that these mathematical formulas had all been developed to track diseases. These were the same formulas they used at the World Health Organization. If there’s a new flu somewhere, they use the same formulas to track the spread of a disease that we used to track the spread of an innovation.
Now, fast forward. Fifteen years later, I’m working on researching the history of jazz, and I’m studying New Orleans, and it was such an unhealthy city. In fact, New Orleans was the unhealthiest city in the United States at the time when jazz was invented. And I asked myself, could there be a connection between the spread of disease and the spread of a musical innovation? Because based upon what I learned at the Boston Consulting Group, there might.
Then I started looking at other situations, and I found a surprising number of situations in history where unhealthy settings and situations had created artistic revolutions. Most people date the start of the Renaissance to the year 1350 in the city of Florence. They don’t realize, 1348 was the Great Plague in Florence. What people don’t realize, the troubadour revolution spread from the South of France into the rest of Europe. It followed the exact same dissemination patterns the Black Death did.
So, classic example: I am taking analytical tools that I learned at the Boston Consulting Group and McKinsey, and I’m applying it to music problems. I’ve done this in many instances. A lot of my work on blues is taking aspects of things I learned back then.
COWEN: Was it a waste of time to get a philosophy, politics, and economics degree from Oxford?
GIOIA: In full disclosure to you, I have to say that I was able to get a degree from Oxford in philosophy, politics, and economics without ever studying the economics. I used a loophole. I didn’t do any economics. Most of my approach was philosophy. An extremely abstract kind of philosophy interests me.
COWEN: You mean like analytic philosophy?
GIOIA: Well, yeah, which I studied at Oxford. But existentialism. I dug into everything. I still read a lot of philosophy, and I believe it has made me a much better music writer. My first music book, The Imperfect Art, was essentially applying philosophical concepts and using them to build a new aesthetic philosophy out of jazz practices. And I think in my latest book, I’ve come back to some of it. I really believe that understanding the history of philosophy and contemporary thought helps me at every stage in my music writing.
COWEN: How has your own Sicilian-Mexican family background shaped your approach to music history and criticism and the ideas in your book Music: A Subversive History?
GIOIA: I came from a very complex ethnic past, and it’s often a question, how much of that shows up in your own musical preferences? I grew up in the thick of an Italian family, where I was always around my Italian relatives, some of them that didn’t speak English, and to some extent, my Mexican relatives as well.
And I look at the Italian culture’s love of melody and the Italian composers’ great emphasis on melody. And I know in my own musical preferences, melody is so important to me.
This is why I wrote a book on West Coast jazz and cool jazz — people like Chet Baker or Paul Desmond. And people say, “Well, they’re not hot enough, Ted. Those aren’t the real jazz musicians.” But I have a real soft spot in my heart for any jazz musician that really masters the art of crafting a melody. And sometimes I wonder if that’s my Italian heritage coming to the forefront.
COWEN: Does music criticism have an economic future? Newspapers are laying off their music critics, right?
GIOIA: Well, this is a big thing. My sons are both interested in arts and culture. My nephews are, and it seems everybody in my family wants to be a writer. And I always say, “Why doesn’t somebody in a family want to trade mortgage derivatives or something? Why does everybody want to be a writer?” And the truth is, it’s hard at this phase in history to make a living as a writer.
The one thing I tell people is, if you’re going to do this, you must find some area of expertise and that you know that better than anybody else. So I still think that in music, if you want to be a music writer, you can do it. But you must have the goal of being one of the few people in the world that really knows your section of music as well as anybody.
Then it’s not just writing. People will call on you to do liner notes, give talks. They’ll want to consult your expertise. You’ll get called to be a judge on panels. The key thing as a writer is to develop the expertise, and then to use writing as one aspect of what you do.
COWEN: Can you ever trust someone who does not have actually good taste in music?
GIOIA: Well, you might be able to trust them in other things, but there’s no substitute for taste and discrimination in music. I think Wynton Marsalis was right. He said — and this is controversial — but he said, if you take somebody who spent their whole life just eating McDonald’s hamburgers, you could take them to the best Michelin-starred restaurant in the world — they wouldn’t enjoy it because they would not have cultivated their taste to understand that.
Music is no different, so unless you’ve cultivated your taste, your ability to appreciate — especially new sounds — will be severely limited.
COWEN: Let’s say a smart 18-year-old comes to you, and that person has been listening to good popular music but says, “Now I’ve learned there’s more out there. I want to cultivate better taste in music.” What do you tell them to do?
GIOIA: Well, I tell them the same thing I tell anybody in any field whatsoever, which is work to get outside your comfort zone. I don’t care what you do. I don’t care whether you’re a scientist or an engineer or a teacher or a music critic or music fan. The most broadening experiences you will have in your life is when you go outside your own comfort zone into new territory.
So I would tell them to do what I do. I have consistently, over a period of years, tried to expose myself to new kinds of music that I hadn’t heard before. And if you do that, you will learn. But also, you’ll enjoy it. This is the path to enjoyment, to open your ears up to these new experiences.
COWEN: For our final segment, I have a few questions about what I call the Ted Gioia production function. Is it better to work and read to music? Or should those be separate activities?
GIOIA: It depends. I believe you can make a very strong philosophical case for what I call the New Age philosophy of music. And that philosophy is that music should be integrated into every aspect of your life or can be integrated into every aspect of your life. I believe that.
Now what I have to say is, in practice, the New Age music that did this was lousy and unlistenable. But I still believe very much, in principle, it’s okay to have music integrated in your life. I know it’s very fashionable to say background music is awful, or music should always be in the foreground. But after having done all the research I’ve done in music history, I now see the exact opposite.
And I’ll just give a couple examples. It’s amazing how many surgeons use music while they operate — 60 percent of surgeons will have a song on while they’re cutting you open. We now learn that at the highest level of peak athletic performance, a lot of care is taken to what songs you listen to while you do your athletic work. And I could give you 50 other examples, but the point is there’s nothing wrong with music being integrated into life experiences, and in fact, we should cultivate that.
COWEN: How is it you manage to listen to so much music?
GIOIA: I think the most important skill anyone can develop is time management skills, how you use your day. But there is one principle I want to stress because this is very important to me, and when people ask me for advice — and once again, this cuts across all fields — but this is the advice I give. In your life, you will be evaluated on your output. Your boss will evaluate you on your output. If you’re a writer like me, the audience will evaluate you on your output.
But your input is just as important. If you don’t have good input, you cannot maintain good output. The problem is no one manages your input. The boss never cares about your input. The boss doesn’t care about what books you read. Your boss doesn’t ask you what newspapers you read. The boss doesn’t ask you what movies you saw or what TV shows or what ideas you consumed.
But I know for a fact, I could not do what I do if I was not zealous in managing high-quality inputs into my mind every day of my life. That’s why I spend maybe two hours a day writing. I’m a writer. I spend two hours a day writing, but I spend three to four hours a day reading and two to three hours a day listening to music.
People think that that’s creating a problem in my schedule, but in fact, I say, “No, no, this is the reason why I’m able to do this. Because I have constant good-quality input.” That is the only reason why I can maintain the output.
COWEN: And what’s your most significant tip for how we can learn to listen to music better?
GIOIA: The most important thing right now is to understand that the best music in our society is under the radar screen for many complex reasons. Record labels are looking for the formula. Radio stations are following the formula. Even these amazing curated playlists are just a feedback loop. They’ll tell you what to listen to next week based on what you listened to last week. And because they’re a feedback loop, they won’t show you anything new or interesting.
So what you need to do, if you really want to broaden your horizons as a listener, is to get exposed to new things. Pick somebody. It doesn’t have to be me. I recommend a new album pretty much every day on my Twitter feed. And every year, I talk about the hundred best albums of the year, and I focus specifically on albums most people would never hear otherwise. So you can turn to me, but it doesn’t have to be me. Find somebody who you trust as a guide, and let them open your ears to these new experiences.
If you do that, you will be rewarded infinitely because music is an amazing part of our life. And if you don’t enjoy the riches of it, you’re selling yourself short.
COWEN: Ted Gioia, thank you very much. And for all of our listeners and readers, I very much recommend Ted’s new book. Again, that’s Music: A Subversive History. Thank you, Ted.
GIOIA: Thank you for having me.
Listening Produces Knowledge
29 
2
29 claps
29 
2
Written by
The Mercatus Center at George Mason University is the world’s premier university source for market-oriented ideas.
A podcast in which esteemed economist Tyler Cowen engages with today's most underrated thinkers in wide-ranging explorations of their work, the world, and everything in between. For new episodes, visit conversationswithtyler.com/episodes.
Written by
The Mercatus Center at George Mason University is the world’s premier university source for market-oriented ideas.
A podcast in which esteemed economist Tyler Cowen engages with today's most underrated thinkers in wide-ranging explorations of their work, the world, and everything in between. For new episodes, visit conversationswithtyler.com/episodes.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/blockchannel/episode-16-partly-cloud-with-a-chance-of-ethereum-164fa444bcc2?source=search_post---------259,"There are currently no responses for this story.
Be the first to respond.
On this episode of BlockChannel, McKie, Dee and Dr. Petty sit down with Shawn Wilkinson of Storj. Storj is a decentralized cloud storage provider that was created back in 2014. Since its inception, Storj has been hard at work on creating “cloud farms” of user submitted disk storage, where individuals who “rent’ out their additional disk space can receive monetary incentives for supporting the network. Originally built on XCP (Counterparty), Storj now plans to make a migration away from the now abandoned project (XCP is platform created originally on the bitcoin blockchain), and move their token and service over to Ethereum, to leverage its smart contracting capabilities, and reduced fees.
Support the Show! ETH Donation Address: 0xa368e33E927D825F5FD05463E6A781414672251c
Show Link(s):
Storj: storj.ioEthereum: ethereum.orgCounterParty: counterparty.io/Swarm: swarm-gateways.net/bzz:/theswarm.eth/
Intro/Outro Music “Wait Luther by Faruhdey: Faruhdey — Wait-luther-outro
Show Sponsor(s):Gnosis PM: gnosis.pm
BlockChannel is a new media & educational hub focused on…
5 
5 claps
5 
BlockChannel is a new media & educational hub focused on the socio-cultural/economic issues related to blockchain technologies like BTC/ETH/& HNS. Visit BlockChannel.com for more resources; and SoundCloud.com/BlockChannelShow for our official podcast.
Written by
BlockChannel is a new media & educational hub focused on the socio-cultural/economic issues related to blockchain technologies like BTC/ETH/& ZEC.
BlockChannel is a new media & educational hub focused on the socio-cultural/economic issues related to blockchain technologies like BTC/ETH/& HNS. Visit BlockChannel.com for more resources; and SoundCloud.com/BlockChannelShow for our official podcast.
"
https://medium.com/@cameroncoward/embrace-redundancy-and-never-again-put-all-of-your-escape-keys-in-one-keyboard-57be7388e6bb?source=search_post---------257,"Sign in
There are currently no responses for this story.
Be the first to respond.
Cameron Coward
May 17, 2018·2 min read
If there is one thing we computer people can agree on, it’s that redundancy is good. Your RAID array and cloud storage are there specifically to backup redundant copies of your data. You have a half-dozen redundant computers scattered around your home just in case five of them suddenly explode. But, have you every considered the frightening fact that you only have one ESC key on your keyboard?
To address that troubling situation and provide the proper redundancy, Glen Akins of Photons, Electrons, and Dirt has a created a small USB keyboard that has just one key: the always useful ESC key. You may remember Akins’ “awesomely impractical” giant three-key keyboard that we covered back in March. But, while that was built mostly as a goof, this new ESC-only keyboard is very useful …if you really like pushing the escape key and don’t want to accidentally hit the tilde key or something.
Those of you who are concerned about the tactile feel of your escape keys can rest easy: Akins used the typist favorite Cherry MX Blue switch. There is also a 3-key version if you don’t get the joke and want something that is more useful. Both versions can be programmed for whatever functions you like. If you want to build your own, Akins has very graciously provided the PCB and 3D CAD files, as well as assembly instructions.
Author, writer, maker, and a former mechanical designer. www.cameroncoward.com @cameron_coward
See all (22)
11 
11 claps
11 
Author, writer, maker, and a former mechanical designer. www.cameroncoward.com @cameron_coward
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hackernoon/what-does-cloud-storage-mean-to-you-78d4841b7baf?source=search_post---------94,"There are currently no responses for this story.
Be the first to respond.
Over the past decade, cloud technologies have evolved from simple Infrastructure as a Service offerings to diversified services and middleware portfolios. One of the major areas of advancement is towards the cloud storage offerings.
When discussing cloud storage, one of the common questions comes up is that what does cloud storage means to someone.
If we look at modern cloud storage, it ranges from general purpose block storage comes for servers to higher level storage services such as Relational databases, NoSQL. In addition, the cloud offers different storage configurations for underlying infrastructure. As you can see, cloud storage is a broader topic which needs further understanding when selecting storages for different use cases.
This article focuses on providing a higher level overview of different cloud storage options and application context so that it will help you to take effective decisions on storage selection in the cloud. I will be also using AWS cloud storage services as examples for cloud storage options so that it will be helpful to find further details about practical use cases.
This is the most common characteristics which we understand when it comes to storage looking from a physical storage standpoint. If we look at different physical storage options, magnetic hard disk storage, solid state drives and tape drives are the common configurations comes into mind. In the context of cloud, these storage configurations offered to consumers either as dedicated offerings or as shared disk storage where the inputs and outputs (IOPS) and storage capacity is shared with others.
When starting a server in a cloud, it’s more likely you are either using pre-defined physical storage configurations or customized configurations for the disk storage, where the operating system runs. It’s also possible to attach addition storages or expand the storage to these servers when needed.
If we take AWS for example, it provides solid state storage (SSD), magnetic hard disk storage (two subtypes called throughput optimized and cold start storage) and NVMe storage.
When using cloud services, one of its strengths comes with the virtualization by providing convenient building blocks for application development. This is also the same for the storage. Although physical storage configurations are needed, most of the applications require higher level capabilities like availability, fault tolerance, durability, security & etc. Since most of the cloud providers use commodity hardware it is of utmost importance that these abstractions are provided to simplify the use cases.
Few common storage building blocks most of the cloud service providers offers are block storage (reliability with redundancy and reusable), object storage (reliable storage with redundancy and higher-level APIs), network file systems (reliable shared storage).
Some of these storage building blocks even provide the ability to select the physical storage configurations like physical storage type (e.g; SSD or Magnetic Hard Disk).
In AWS, EBS is the block storage offering which could be attached to EC2 instances (Virtual Machines). Since it has redundancy built in, a physical disk failure won’t affect the availability of storage. AWS S3 is one of the popular options for object storage where it exposes a REST API to communicate with the service where multiple data center level redundancy is there. AWS EFS is their network file system offering for shared storage that could be shared across several EC2 instances at runtime. Amazon Glacier is a tape-based storage option for data archival. AWS KMS is special purpose storage offerings with virtual and physical hardware security module support for storing encryption keys. You can find some applications of these storage options in AWS EBS vs Instance Storage Patterns for Application Use Cases article.
These storage services are mostly considered as a platform as service offerings by various cloud providers which includes, managed or cloud-native relational database services, NoSQL database services, caching storage, queue storage services and etc.
Although I have classified Amazon S3 storage building blocks considering it as an object storage option, it is difficult to draw the line between some of the storage building blocks like S3 and application storage services due to the wide range of functionalities they provide.
These storage services are directly usable as application-level building blocks to design solutions. If we look at the underlying infrastructure, most of these use virtual storage options underneath to provide the reliability, security & etc.
If you think of setting up your own relational database system instead of using these application storage services, it should be possible. However, it might become challenging when it comes to managing those where it requires to do security patching, implement support for disaster recovery, implement fault tolerance and high availability & etc where it requires expert knowledge and continuous commitment which adds up for the total cost of ownership.
In AWS, Amazon RDS provides ranges of managed relational database offerings including MySQL, MariaDB, Oracle, Microsoft SQL Server, PostgreSQL and cloud-native Amazon Aurora database. AWS DynamoDB is the NoSQL offering provided by Amazon which is a simple key-value based database which is massively scalable. Amazon ElastiCache offers fully managed Redis and Memcached for cache storage.
#BlackLivesMatter
27 
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
27 claps
27 
Written by
Solutions Architect and a Content Specialist. For more details find me in Linkedin https://www.linkedin.com/in/ashanfer/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Solutions Architect and a Content Specialist. For more details find me in Linkedin https://www.linkedin.com/in/ashanfer/
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://m.chmarny.com/federated-not-balkanized-the-future-of-data-and-its-current-cloud-challenges-7d6d463117d8?source=search_post---------310,"few longer thoughts, because every once in a while 140 characters is just not enough

I learn best by doing. And recently, most of the projects I’ve been building are either REST or gRPC-base services deployed as container images into Cloud Run on GCP. That means that I increasingly find myself recreating a lot of the same infra and app deployment flows.


Why not Medium My main reason for migrating off Medium was the paywall Medium introduced while back. I actually understand why they did it. The unlimited access price: $5/month ($50/year) is too high, but still, I get it.
For me though, the objective was to allow readers to easily discover and read my posts.


Increasing large amount of technical news I read come from the posts shared on Hacker News or on Twitter. While both of these services have search options, neither of these seem to be advanced enough to narrow my searches at the desired level or to setup automatic delivery.


All complexity needs to be abstracted, right? This reductionist statements misses nuance around the inherent cost/benefit tradeoffs, especially when you consider these over time.
Don’t get me wrong, there often are good reasons for additional layers to make things simpler (grow adoption, lowering toil, removing friction, etc.


I recently joined the Office of CTO in Azure at Microsoft and wanted to ramp up on one of the open source projects the team has built there called Dapr. Dapr describes itself as:
A portable, event-driven runtime that makes it easy for developers to build resilient, microservice stateless and stateful applications that run on the cloud and edge and embraces the diversity of languages and developer frameworks.


We are entering a period where custom, highly-optimized, vertical solutions are becoming viable option again. This is a good news for ISVs with proven domain expertise and skilled development resources.
Why do I think so? We now have:
Plethora of feature-rich developer frameworks, message queues, scalable data stores, and even lower-level components in the OSS community with great documentation and a large number of use-case validation Growing number of custom solution companies (more than just ISVs) with existing deep vertical/domain expertise who are also increasingly now investing in hiring and training strong development teams Virtually every Cloud provider offering either a raw Kubernetes service or managed container execution platform which (regardless how you feel about these technologies) creates ubiquitous surface area that can be addressed with a single solution Yes, there still are many ways in which these custom development efforts can fail.


When dealing with file permissions in a non-root image or building apps that include static content (like css or templates), I sometime get an error resulting from the final image content mismatch with my expectations.
Most of the time the errors are pretty obvious, simple fix and rebuild will do.


While the idea of a serverless platform and long running workloads does seem somewhat “unnatural” at first, smart people are already working on that (looking at you @Knative community). In the meantime, a simple approach is sometimes all you may need.


A co-worker recently told me about flic.io buttons. These button caught my attention because they can include triggers for single, double, or hold click and can be easily wired up to all kinds of actions.
I instantly thought of of a few really interesting applications.


Next week, April 9–11, Google will be hosting this year’s Cloud Next Conference in San Francisco. The conference is already sold out, but there will be a livestream from keynotes and video available shortly after the sessions.
This year, we have a lot of content to share, and I have the privilege of presenting in four sessions — and hope to do at least six live demos.

"
https://medium.com/@alibaba-cloud/security-and-monitoring-practices-alibaba-cloud-storage-solutions-part-1-3caf5782b9d1?source=search_post---------178,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Jan 7, 2021·5 min read
By Shantanu Kaushik
Cloud storage has evolved in-sync with cloud computing. Over the years, different practices and trends have set a steady pace of development for the industry and the technology that drives it. Security and access control define the two most important parameters for any technology to work correctly.
Maintaining functionality and user experience is critical. Any product that can maintain a steady delivery system can account for high data durability. This persistence of service is what sets the Alibaba Cloud Storage solutions apart. In this article, we will showcase the implementation scenarios that make your choice of cloud storage a secure one.
Integration with industry-leading tools like the Alibaba Cloud Resource Access Management (RAM) service and the latest encryption model is the key focus of this article.
Alibaba Cloud storage products, including Object Storage Service (OSS), Block Storage, and Apsara File Storage NAS offer extensive security capabilities. Security features like server-side encryption, client-side encryption, log audit, fine-grained access control, and hot-link protection are offered as standards. Retention policies based on WORM and lifecycle policy management are also an added benefit that comes by default with Alibaba Cloud Storage solutions.
The Object Storage Service (OSS), Block Storage, Apsara File Storage NAS, and Storage Capacity Unit (SCU) offer an access control list (ACL). You can use this feature to control access based on permissions. You can assign read/write access based on user type. You can select authorization based on public and private access lists. You have the option to define these policies based on specific needs.
Resource Access Management (RAM) is used for identification and access control. You can set policies based on user responsibilities and manage users by configuring RAM policies. RAM allows you to create and define policies that can control resources assigned to a particular user of a group.
Object Storage Service provides hotlink protection to avoid unauthorized access to your buckets. You can easily set the HTTP/HTTPS referrer to configure the referrer whitelist to allow requests from specific domain names. Access is also provided if the HTTP/HTTPS referrer is included with the request to access the OSS resource. Hotlink protection prevents any hotlinking of data in public read and public read/write buckets.
Server-side encryption for uploaded data processes the data to encrypt it while the data is uploaded and decrypts it back when it’s downloaded. Server-side encryption is used to protect static data, which is recommended for high data security scenarios that may include online document collaboration or examples of deep learning.
You can implement the following methods for server-side encryption:
In this method, the Customer Master Keys (CMKs) are rotated regularly to ensure better security practices and to enable regular encrypting and decrypting operations on objects. Object Storage Service uses AES-256 for encryption of objects with different data keys. The data keys are generated and managed by OSS.
SSE-KMS enables encryption and decryption of large amounts of data using a specified CMK ID or a default key stored in KMS. This is relatively a more cost-worthy method as there is no requirement of transmitting user data to KMS through networks.
KMS is secure, can be easily managed, and uses AES-256 encryption. Alibaba Cloud KMS has high integrity, security, and availability features and offers a seamless experience. It allows you to custom-build encryption/decryption solutions to align with your business needs.
Before the data is uploaded to OSS, client-side encryption is performed to encrypt objects. With client-side encryption, symmetric encryption is achieved by generating a random data key. This random data key is generated by the client and uploaded as a part of object metadata, which is stored on the Object Storage Service (OSS). Whenever an object is downloaded, the random data key is decrypted using the CMK, and the generated data key is used to decrypt the object.
There are two ways to use Client-Side encryption:
The illustration below is an example of CMK managed by KMS:
The basic steps included are:
Upload
Download
In this scenario, the CMKs have to be generated by the client. When an upload operation is executed, you need to perform the client-side encryption of the object. You need to upload an asymmetric or symmetric CMK.
The illustration below depicts this scenario:
Encrypt an Object
Decrypt an Object
Data Security is an essential component of any service that is responsible for handling critical and sensitive data. Alibaba Cloud hosts a variety of services that enable and execute security parameters at the highest level. It is imperative to maintain data while at rest or while transmitting. It is the level of integration that makes any solution a reliable one. Alibaba Cloud’s line of products works in sync to maintain and extend the high-standards of data durability.
1. Security and Monitoring Practices with Alibaba Cloud Storage Solutions — Part 2
We will focus on Object Storage Service Sandboxing, overall monitoring, and metrics collection with Alibaba Cloud Storage Solutions.
2. Apsara File Storage NAS — What and How?
We will discuss the complete architecture and usage scenarios with the Apsara File Storage NAS solution by Alibaba Cloud.
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://lugassy.net/how-moving-from-pub-sub-to-avro-saved-us-38-976-year-ec6c33ea7d08?source=search_post---------341,"One of our Google App Engine projects started doing around 5,000 requests a second (~432M a day). We needed a way to do fast ingestion and analysis on these requests using BigQuery. Below is what we tried and ended up using.
Apparently every single hit to App Engine gets logged in what’s called the request_log, which is an enhanced, very detailed and JSON-based Apache/nginx-like access log.
With a few clicks, you can export the entire request_log into day-specific BigQuery tables which you can then query. Set up old table dates to automatically expire and you’re good to go!
Instead of logging entire requests, you can write small events (messages) to a Pub/Sub topic, and have it ingested (using DataFlow) to BigQuery. This is the classic, cookbook approach which is also the most flexible.
Unlike streaming, ingesting data in batch into BigQuery is free. Sure, this is NOT streaming, but if you’re fine with requests taking 5–10 minutes to become available for querying, then you would love this.
Thanks to my pal Alan Lavintman for the idea and cost saving!
Thanks to Matthieu Monsch for avsc and all streams insights!
Have a better way to do log rotation for Google App Engine? Let me know!
Thoughts about Startups, Development & Ops by Michael…
237 
6
237 claps
237 
6
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
Written by
Co-Founder, HashUnited.com. Built 7 other companies, sold 3, worked with dozens more. Writing about startups, dev & ops.
Thoughts about Startups, Development & Ops by Michael Lugassy
"
https://medium.com/hackernoon/ipfscloud-a-decentralised-cloud-storage-platform-12ed938a9307?source=search_post---------36,"There are currently no responses for this story.
Be the first to respond.
Receive curated Web 3.0 content like this with a summary every day via WhatsApp, Telegram, Discord, or Email.
simpleaswater.com
As the world is moving towards decentralization revolution, a lot of apps(or dapps) are developed every day. A true dapp needs a truely decentralized infrastructure which includes a decentralized content distribution network. A lot of projects are working to create sustainable decentralized content distribution network as we have seen in the below post.
hackernoon.com
One of them is IPFS. I have been working with it for quite a while and recently I decided to create a simple dapp using it.
Let me introduce you to the IpfsCloud. In short, it’s google drive on IPFS.
Here is the link to the github repo: https://github.com/vasa-develop/ipfscloud
Honestly saying I am not able to decide what things should I add to this project. Should I add social authentication feature(login via google, facebook, github etc) to it? Add your suggestions in the comments…
Thanks for reading;)
About the Author
Vaibhav Saini is a Co-Founder of TowardsBlockchain, an MIT Cambridge Innovation Center incubated startup.
He works as Senior blockchain developer and has worked on several blockchain platforms including Ethereum, Quorum, EOS, Nano, Hashgraph, IOTA etc.
He is currently a sophomore at IIT Delhi.
Hold down the clap button if you liked the content! It helps me gain exposure .
Want to learn more? Checkout my previous articles.
medium.com
medium.com
hackernoon.com
hackernoon.com
Clap 50 times and follow me on Twitter: @vasa_develop
#BlackLivesMatter
103 
103 claps
103 
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Entrepreneur | Co-founder, TowardsBlockChain, an MIT CIC incubated startup | SimpleAsWater, YC 19 | Speaker | https://vaibhavsaini.com
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
"
https://medium.com/@loginradius/cloud-storage-vs-traditional-storage-12fc9562477f?source=search_post---------188,"Sign in
There are currently no responses for this story.
Be the first to respond.
LoginRadius
Oct 27, 2020·2 min read
With Information Technology becoming more and more Cloud based nowadays (due to industry demanding reliability and scalability in their infrastructure), the Cloud storage system has become a very feasible solution. Various organizations are migrating their data to cloud storage, due to a few simple reasons. They want data to be easily accessible, cost effective and reliable.
**How is Cloud storage better than any traditional data storage**
**Use Case**
LoginRadius identity storage provides the above solution, LoginRadius is managing its infrastructure on the cloud and has never experienced data breaches or down-times. Infrastructure that makes sure you retain certain vital attributes in the storage is critical, this necessitates your user’s identities being stored in an extremely reliable system such as is implemented with LoginRadius identity storage. Utilizing some extremely robust cloud storage providers( Microsoft Azure) LoginRadius offers top of the line availability and reliability of user data.
Originally published at https://www.loginradius.com.
LoginRadius customer Identity management platform serves over 3,000 businesses with a monthly reach of over 1.2 billion users worldwide. https://loginradius.com
LoginRadius customer Identity management platform serves over 3,000 businesses with a monthly reach of over 1.2 billion users worldwide. https://loginradius.com
"
https://medium.com/hacking-and-slacking/manage-files-in-google-cloud-storage-with-python-73cbd9b1b010?source=search_post---------24,"Sign in
There are currently no responses for this story.
Be the first to respond.
Todd Birchard
Jun 19, 2019·6 min read
I recently worked on a project which combined two of my life’s greatest passions: coding, and memes. The project was, of course, a chatbot: a fun imaginary friend who sits in your chatroom of choice and loyally waits on your…
"
https://medium.com/@daviddalbusco/hi-frank-fd4b58c0ea53?source=search_post---------315,"Sign in
There are currently no responses for this story.
Be the first to respond.
David Dal Busco
Feb 5, 2020·1 min read
Frank van Puffelen
Hi Frank,
Thank you for your answer, Cloud Storage is an amazing tool!
Actually, according my recent tests, uploading a file to Cloud Storage through the Firebase SDK client does auto-generate a download URL even if no extra call to get a a download URL is performed respectively it generates a download URL automatically in any case.
I don’t know if it’s a configuration thing, the fact that I used the client side and not the server side or else, but that’s the actual behavior I noticed (and even one of the reason why I reverted a big part of my implementation).
About revoking an existing download URL from the console which generate a new download URL, it’s also something I observed in my recent tests (see Gif below).
Freelancer by day | Creator of DeckDeckGo by night | Organizer of the Ionic and IndieHackers Zürich Meetup
Freelancer by day | Creator of DeckDeckGo by night | Organizer of the Ionic and IndieHackers Zürich Meetup
"
https://medium.com/flutter-community/firebase-cloud-storage-in-flutter-flutter-an-firebase-tutorial-c5de7835c6cd?source=search_post---------23,"There are currently no responses for this story.
Be the first to respond.
Hello there Flutter Dev 🙋‍♂️
In this tutorial we will be going over Cloudstorge on Firebase and how to integrate that with your mobile application. This tutorial is part 5 of a free Firebase and Flutter course that has weekly videos. To get the videos as they come out, make sure to subscribe to my Youtube Channel.
Today we’ll do a simple task that is probably a very common task in mobile app development. We will provide the user with a UI to select and upload a photo, save that photo in a post and display that to them in a collection. The photo will be stored in Firebase CloudStorage and we’ll use a URL to the photo to display to users in the app. We’ll start off by updating the UI to allow us to upload a photo.
I’m starting off with this project for my UI which is the final code from part 3 of the series. If you don’t have a project to follow along with you can use this code base for the tutorial. Download the code and open it in VS Code.
Before we start with the code lets setup our cloud storage. Open up the firebase console and click on the storage icon in the left toolbar. Click on create bucket, choose the location and continue. You will now have what is called a “bucket” where you can store files. You can think of this as a hard drive that you access through a url web request. Each of the files here will have an access token / url that you can access only through that url with the attached token. You can set visibility by controling the access level for that download url token. This is the url we’ll use to display the image in the app.
Let go over a quick implementation overview. We’ll create a service that wraps the provided firebase storage package. This service will take in a File object and a title and upload that to the storage. When the operation is complete we will return the url which is what we'll put inside of our post as the imageUrl. The file that we're passing in will be selected using the UI presented by the image picker library. Let's get to it.
We start off by adding the firebase_storage and the image_picker package to the pubspec.
Firebase storage is to interact with the Firebase Cloud Storage, the image picker is to show the user a UI that will allow them to select an image from their device.
Under the services folder create a new file called cloud_storage_service.dart. We’ll give it a function called uploadImage that Takes in a required file as well as a title. You can pass in the UID, or anything you'd like to identify your images by.
To access the Firestore Storage instance we use the FirebaseStorage.instance static property. The storage library works similar to the firebase documents. You can get a reference to a file that doesn't exist yet and then add the data in there that you want. We'll get a reference to our future file using the title and the date epoch to keep it unique. Once we have our reference we will call putFile and pass it in the selected File. This will give us a StorageUploadTask. This object has an onComplete Future that returns a StorageTaskSnapshot (similar to firebase snapshot). We can await that future and once we have the snapshot we can use the StorageReference returned and get the downloadUrl. We'll return the url when the task is complete or null.
Open up the locator.dart file and register the CloudStorageService with the get_it instance.
We’ll start off by wrapping the ImagePicker library into our own class. This way our business logic is not dependent on any third party packages. It's something I like to do, if you go the additional step and add it behind an interface then you can mock it out during testing as well.
Create a new folder called utils. Inside create a new file called image_selector.dart
I know it seems silly to have a class that wraps one line, but you can do much more with it than this. You can keep the file in memory until you’re certain it’s uploaded, you can have different sources passed in from different functions, etc. The main reason for this is to remove the dependency of ImagePicker from any of the code in the app that has to make use of the functionality.
Open up the locator.dart file and register the ImageSelector with the get_it instance.
Finally open up the CreatePostViewModel where we'll locate the selector and then make use of it in a function called selectAndUploadImage. We'll also import the CloudStorageService for later use. We'll use the selectImage function to set the image to upload and display that to the user in the UI.
In the same viewmodel update the addPost function to upload the image if we're not editting the post. We'll then use that url as the imageUrl in the post. For error handling I would show a snack bar if the imageUrl comes back null that indicates to the user that the image upload has failed.
Next, open the Post model and add the new imageFileName String that we'll use to later delete the post.
Now we can go onto the UI for the functionality. First thing to do is update the CreatePostView and add a gesture detector onto the grey rectangle we're displaying. When tapped we'll call the selectImage function. We'll also add a conditional to make sure when an image is selected we show it in that grey block. Update the container in the create_post_view that has the text in it to the following.
If you run the app now, tap on the FAB, enter a title and tap on the image block you’ll see the image picker pop up. Select an image and it should be showing in the grey block in place of the text :) Add the post by pressing the FAB and it’ll send it up to the cloud and return you a url.
If you open up the cloud storage now you’ll see a file with the title you enetered and a number after it. That’s the image you uploaded. Next up is displaying the image.
To display the images from the cloud storage we will use the cached_network_image package. Add it to your pubspec.
Open up the post_item and we’ll update the UI. First thing is to make sure when we have an image we don’t want to give the list a fixed size. We’ll check if there’s an image. If there’s an image we set the heigh to null (meaning wrap content), otherwise we set it to 60.
That’s it. You can now, post an image to the cloud storage. Then see it load as an item in the list of posts :)
Last thing to do is to delete the image again when a post is removed. This can be done by simple getting the ref and calling delete on it. Open up the CloudStorageService and we'll add a delete function.
Open up the HomeViewModel, locate the CloudStorageService and then after deleting the post from the firestore db call the delete function on the CloudStorageService as well.
And That’s it. Basic Cloud storage functionality wrapped into service for easy use. Make sure to follow me on Youtube for the rest of the series. Until next week :)
Dane Mackier
www.twitter.com
Articles and Stories from the Flutter Community
186 
3
186 claps
186 
3
Articles and Stories from the Flutter Community
Written by
A full stack software developer focused on building mobile products, its tools and architecture. Always reducing boiler plate code and experimenting.
Articles and Stories from the Flutter Community
"
https://medium.com/dxchainglobal/attention-dxchain-engineers-show-you-the-real-secrets-hidden-in-dxbox-b08cfdbd44de?source=search_post---------284,"There are currently no responses for this story.
Be the first to respond.
DxBox, DxChain’s first blockchain cloud storage product, is the application to provide secure & private cloud storage for users all over the world.
More in-depth research please check:
medium.com
In recent two months, DxChain held a series of DxBox Testing Events with more than 5,000 participants joined. What impressed us was that we received a huge amount of meaningful comments and reviews.
Your comments and suggestions are of great value to us!
Among these reviews, we summarized the top 3 reasons why users like DxBox:
1) High speed/transaction and convenience of uploading/downloading
2) Simple and clean interface
3) Easy to understand and user-friendly
Thanks for every community member’s support and encouragement which inspire us to do better in the research and development of DxChain products.
Welcome to participate in the free trial for DxBox Testing!
Have your own secure and decentralized blockchain storage now: https://explorer.dxchain.com/register/
In response to the support and trust of DxBox enthusiasts, DxChain engineers Jacky Wang and Manxiang Zhang were invited to answer 10 questions which community users concern the most.
The major difference between DxBox and Dropbox is the architecture, where DxBox implements blockchain technology and the others use the traditional centralized client-server model.
Unlike the decentralized system, there are server administrators for the centralized system who have direct access to user data, leads a potential risk on the data breach. Moreover, files uploaded to a centralized system will be replicated and stored in different data centers. However, once the data center got hacked, all files will be exposed.
The decentralized storage network of DxChain provides a solution to solve the problems above.
1) By using DxBox, storage provider only has encrypted file pieces, making it almost impossible to recover the original file, ensuring the data security and privacy.
2) In addition, by using blockchain technology, the system is more flexible. The traditional client-server model requires large data centers built with special hardware and specialties to maintain the status. However, by adapting blockchain technology, each computer can be a storage provider which can be easily added and maintained.
3) Last but not least is the problem we aim to solve. Traditional cloud storage service like Dropbox is a centralized service provider that makes profits by meeting clients’ needs. In DxChain economic model, the node could act as two roles: client and host. The client can earn DX tokens by sharing data, and the host can get profit by providing idle storage capacities. Thereby, the ultimate goal of DxChain is to build a more secure, reliable, and profitable decentralized ecosystem.
We limited the upload file size to be less than 10 MB at a time for testing purposes. However, the user is able to upload files as many times as they want. The entire file storage size is around 5 TB so far and can be easily expanded by adding more hosts. Therefore, theoretically, the storage size is unlimited.
The sharing feature is not supported by DxBox yet, the file you download is what you’ve uploaded before. After implementing this feature, we will provide a third-party anti-virus software interface for users to decide whether to start the anti-virus plugin.
In a word, yes. DxChain’s design can technically ensure data security. Therefore, DxChain can provide unlimited file storage service with abundant economic incentives.
The file will be encrypted and divided into shards using erasure coding algorithm and stored to different storage providers based on certain parameters. According to the erasure coding algorithm, only a certain amount of data pieces are needed to recover the original file. The use of storage proof submitted by the host periodically and erasure coding algorithm immensely ensures the data privacy and security.
For instance, a file is sharded into 30 data pieces and stored to 30 storage providers based on the user’s parameter. The original file can be recovered as long as any 10 pieces are available.
Suppose the probability that the storage provider can securely store a data piece for one day in the DxBox model is 90%, then the probability that a file cannot be recovered is:
The probability that the file can be recovered after 5 years is:
As a conclusion, DxChain can guarantee the security of files to a large extent.
The price totally depends on the supply and demand in the storage market and we do not have any control over it. Like buying apples from grocery stores, the sellers have strong bargain power when they have few competitors in the market. However, the sellers will decrease the price to draw customers if there are lots of competitors keeping a large stock of apples.
The theory applies to the storage market as well, there will be a great number of storage providers available when the decentralized storage network of DxChain breaks the data monopoly of technical giants. On this occasion, the storage providers will compete with each other by lowering down the price to draw more users’ attention.
Currently, users can only use DX Token as the currency for file storage, and we are not going to implement this feature in the near future. However, based on the users’ request, we may add a feature to convert different types of currency to DX Token more conveniently.
This feature is going to be added in the next release of DxBox. Currently, DxChain Testnet has already supported the batch uploading and you can check the file uploading status when multiple files are uploaded.
We are planning to build a file management system based on the storage service in the near future. Users can make a data directory just like any popular cloud storage systems, including customizing and categorize your files with tags.
For the demonstration purpose, DxChain engineers mostly focused on the implementation and optimization of the storage feature instead of the login/register system. However, we will definitely implement this feature and making the system more secure in the future.
As mentioned before, currently we do not support file sharing feature, you can only download the file you uploaded. But data sharing is the core feature we want to design and implement in the next phase. What we want is not simply data sharing among friends for free, but the owner can get profit by sharing their data, idle storage and computing capacities to build a low-cost, secure, decentralized blockchain cloud storage network. We are working on it to meet DxChain community members’ needs.
About DxChain
DxChain is the world’s first decentralized big data and machine learning network powered by a computing-centric blockchain. DxChain is a public chain, it designs a revolutionary “Chains-on-chain” architecture to make blockchain function as a computing unit — data storage and computing, so that the technical characteristics of blockchain can be truly extended to a broader field, promoting the next generation of technology from the bottom. For more information, please visit www.dxchain.com.
Telegram: https://t.me/dxchainTwitter: https://twitter.com/DxChainNetwork
www.dxchain.com Big Data meets Blockchain
29 
29 claps
29 
Written by
http://www.dxchain.com — decentralized big data and machine learning network powered by a computing-centric blockchain
www.dxchain.com Big Data meets Blockchain
Written by
http://www.dxchain.com — decentralized big data and machine learning network powered by a computing-centric blockchain
www.dxchain.com Big Data meets Blockchain
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/coinversation-protocol/coinversation-partners-with-crust-network-for-decentralized-cloud-storage-4629e36389a0?source=search_post---------97,"There are currently no responses for this story.
Be the first to respond.
We are delighted to announce that Coinversation Protocol, a synthetic asset issuance protocol based on Polkadot AMM and DEX, partners with Crust Network. Coinversation will deploy and run web pages on Crust and will store user data based on Crust decentralized storage.
About Crust Network
Crust implements the incentive layer protocol for decentralized storage. It is adaptable to multiple storage layer protocols such as IPFS, and provides support for the application layer. Crust’s architecture also has the capability of supporting a decentralized computing layer and building a decentralized cloud ecosystem.
Crust Network successively joined Substrate Builders Program and Web3.0 Bootcamp, and also obtained a Web3 Foundation Grant.
Website｜Telegram |Twitter | Github | Discord| Medium
About Coinversation
Coinversation Protocol is the first synthetic asset issuance protocol and decentralised contract trading exchange based on the Polkadot contract chain. It uses the token CTO issued by Coinversation Protocol and Polkadot(DOT) as collateral, and synthesizes any cryptocurrencies or stocks, bonds, gold and any other off-chain assets through smart contracts and oracles. The assets minted by all the users correspond to the liabilities of the entire system, and the proportion of each user’s liabilities has been determined at the time of forging, so that their respective profits can be calculated. Because such a collateral pool model does not require a counterparty, it perfectly solves the problems of liquidity and transaction depth in decentralised exchange(DEX).
Website｜Telegram | Twitter | Github
Make Asset accessible
163 
163 claps
163 
Coinversation Protocol is a synthetic asset issuance protocol and decentralised contract trading exchange based on the Polkadot contract chain.
Written by
Coinversation: Decentralized Synthetic Asset Issuance Platform on Polkadot
Coinversation Protocol is a synthetic asset issuance protocol and decentralised contract trading exchange based on the Polkadot contract chain.
"
https://medium.com/boostnote/how-to-use-the-cloud-storage-ac771171fa44?source=search_post---------110,"There are currently no responses for this story.
Be the first to respond.
You can sync your Boost Note’s data among devices if you use the cloud storage feature.
Login is needed for the cloud storage, so let me explain how to login for Boost Note!
2. Open the preferences → General → Sign in
3. You can choose GitHub or email login
4. Completed! Close the tab and back to app.
5. You can create a cloud storage!
We’ve revealed a new collaboration wiki service called BoostHub.
It will be a collaboration version of Boost Note but much more powerful. The feature, called “Custom Blocks”, will let you make custom react components, integrated with various 3rd party app apis like GitHub and Trello, and embed them into markdown contents like MDX.
To know more, please check this slide.
Develop as one, grow as one
2 
2 claps
2 
Written by
Develop as one, grow as one. Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Written by
Develop as one, grow as one. Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Boost Note is a powerful, lightspeed collaborative workspace for developer teams.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@saturngod/koofr-df8aa6cc0e22?source=search_post---------286,"Sign in
There are currently no responses for this story.
Be the first to respond.
saturngod
·Jul 26, 2021
Cloud Storage ထဲမှာ အကြိုက်ဆုံးပြောပါ ဆိုရင်တော့ Dropbox ပဲ။​ မြန်ပြီး စိတ်ချရတယ်။ Conflict တွေ ဖြစ်တာ သိပ်မရှိဘူး။ ရှိခဲ့လည်း Error ပြပေးတယ်။ Google Drive က mac မှာ ပြဿနာ အရမ်းများတယ်။ အရမ်းလည်း နှေးတယ်။ Office 365 သုံးနေတော့ One Drive က ရပေမယ့် မသုံးဖြစ်ဘူး။ Mac မှာ node.js ရဲ့ modules တွေ နဲ့ ပြဿနာတွေ တက်ပြီး error များတယ်။
Stacksocial မှာ Koofr ကို တွေ့ပြီးတော့ မဝယ်ခင်မှာ စမ်းသုံးကြည့်သေးတယ်။ အဆင်ပြေတယ်။ Google Drive, One Drive ထက် ပိုအဆင်ပြေတယ်။ Sync က တော့ နှေးတယ်။ ဒါပေမယ့် သုံးလို့ ရတယ်။ လက်ခံလို့ ရတယ်ပေ့ါ။
Koofr ကို Stacksocial မှာ lifetime အတွက် 100GB ကို $30 နဲ့ လက်ရှိ ရောင်းနေပါတယ်။ Droppbox 3 လစာ ပဲ ဖြစ်တော့ တန်တယ် လို့ ထင်ပြီး လက်ရှိ သုံးနေတယ်။ 250 GB ကို $60 ပေးရပါတယ်။ အခု ထက်ထိတော့ အဆင်ပြေနေသေးတယ်။ Mobile App က မကောင်းပေမယ့် သုံးလို့ ဖြစ်တယ်။
ကြိုက်သည့် အချက်ကတော့ လိုချင်သည့် folder ပဲ sync ထားလို့ရတာပဲ။ Folder တစ်ခုကို backup အနေနဲ့ ထားပြီး အလုပ် folder တစ်ခုပဲ sync လုပ်ပြီး သုံးနေတယ်။ အခုချိန်ထိတော့ 100 GB က လုံလောက်တယ်။
3 claps
3 
Husband,Father. Love to do Mobile App Developer and running a startup in Yangon, Myanmar. Founder of COMQUAS. Creator of Rabbit Converter.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/hd-pro/understanding-the-cloud-for-photographers-c1396eff632f?source=search_post---------227,"There are currently no responses for this story.
Be the first to respond.
Unless you are an old school photographer who works exclusively with film, you have heard about the cloud. If you have used services like Dropbox, Google Drive or OneDrive, then you are already using the cloud. The cloud is basically a network that provides services to end users. Storage is one of those services, and photographers need them to upload photos for archive or sharing purposes. In this article I will describe the cloud in detail and how it can be used by photographers as part of their digital workflow. I will be discussing the Adobe Creative Cloud (Adobe CC) for most examples. It is easy to get overwhelmed with so much information about a topic, I will keep things basic to better understand the fundamentals. Then build upon that knowledge to get more experience.
The cloud is a term used in information technology to describe an online system of storage, applications and even virtual networks are offered as services by a provider. This moves traditional information assets like servers from on premise data center in the office to a remote location over a public data network like the Internet. Storage and software applications are the benefits for photographers when using the cloud.
When photographers finish with a shoot, they must provide their clients or customer a copy of the photos. It was simple enough during the days of film. The photographer develops the film into prints and gladly either personally hand them over to the customer or snail mail the photos. With digital, photographers can now more easily distribute the photos by burning the images onto a storage media like DVD or CD. However, that requires the same process as prints, meaning the photographer must either meet with the customer or mail them. That can take some time out of anyone’s schedule. The best way to distribute images is to upload the photos to a folder that resides on a remote server and have the customer download the images individually or as one compressed file. This is what we refer to as “the cloud”.
Things have gotten better with software that it can now be offered over the cloud as well. One such software company is Adobe and their Creative Cloud suite. It targets creatives, whether they are filmmakers, editors, producers, artists or photographers. Adobe offers their software in terms of packages that cater to a creatives requirements. This makes pricing much more practical since if you are a photographer, you will probably just need Adobe’s suite of applications that are for photography and not for illustrations and drawings. You simply sign up on Adobe’s website. Once you are there you can sign up by creating your account and then logging in. During sign up you can choose the package you need, and in this case the photography subscription option (you can choose other packages as you see fit for your requirements if not photography alone).
Once you are signed in you will get an Adobe ID. Save this to your password manager if you have one, otherwise remember your username/password or sign in from Facebook or Google’s g-mail (options).
You don’t always need to sign in to the portal on Adobe’s website to use the Creative Cloud. You only sign in to make changes to your settings, update billing information, change address or add new applications to your subscription plan. The best way to use it is by installing the software on your desktop or laptop computer or the app version for smartphones and tablets. The instructions are available from the website and is really straightforward once you have created your account and subscribed to a package.
When using the software application on your desktop (example is from a macOS), you should see the following icons installed from the /Applications folder (also can be seen in Launchpad):
From a smartphone or tablet, you should get the app from the Play Store for Android users or the Apple Store for iPhone iOS users. You should then see this:
The subscription will give users access to storage on the cloud where their Lightroom application stores images. The last time I checked, Adobe offers 20 GB of free storage for the photography plan. Some photographers will be willing to pay more than others. There are other ways to get access to free storage on the cloud which I will be discussing later.
I am going to get straight to it on how you can use the cloud for your workflow. There is no formal step by step process since I am sure everybody has a different workflow. Some photographers prefer to process from their desktop prior to the cloud while others post straight to it. There are two ways you can leverage the power of the cloud I will discuss here.
#1 Lightroom
Adobe has two versions of Lightroom (LR) now. The version most photographers may be familiar with is now called Lightroom Classic. This requires installation of the software on your laptop or desktop just like in the old days. You can use this pretty much the same way you have been using Lightroom. The new Lightroom CC (Creative Cloud) also requires installation but is cloud aware. This means that you can open RAW, JPEG and other file formats in Lightroom CC and synch it to your Adobe CC storage on the Internet.
NOTE: Do make sure that you have an active Internet connection when synching photos to the cloud.
You will notice that there is a cloud icon on the upper right corner. This shows you if the images are synching up to the cloud storage. The convenience of this is that you can take photos you edited with Photoshop directly to the cloud. You can then access it from anywhere you go where you have access to the Internet. You can open it up with your Lightroom CC app on a smartphone and make some minor edits on the image. From here you can share it on social media. The new Lightroom CC provides photographers with mobility to have access to their images and work with them from anywhere they go. Only requirements is access to the Internet to open the images from the cloud.
The main feature of Lightroom CC is its support for mobile devices. You can simply take a photo on your smartphone and then add it to your photo library in Lightroom CC. From there you can make some minor adjustments to edit the image before sharing. The storage Adobe provides with the photography plan is 20 GB, which may not be suitable for most professional and commercial photographers unless they pay for additional storage. The costs could add up quickly, especially with more shoots that require more storage. This is where the second way to use the cloud helps.
#2 Cloud Drives
You can sign up for free or subscribe to other cloud storage services. You can call them “cloud drives” or “online folder”, they are basically a place where you can upload your files for sharing or backup. Examples are Apple’s iCloud, Google Drive, Microsoft’s OneDrive and Dropbox to name a few. When you use these services you don’t need an Adobe ID, they are separate. In this case you might be retouching photos from the Adobe suite using LR and Photoshop. After you are done then you can upload the photos to the cloud storage provider of your choice.
On Microsoft’s OneDrive, users get 5 GB of free storage. A photographer who shoots plenty of TIFF or archives RAW images can quickly max this out. That is why cloud storage providers also offer more storage by signing up to their premium plans. It can make a lot of sense especially if you require more storage space for your images. Dropbox offers 2 GB but users can upgrade to higher tier plans to get as much as 3 TB to however much space they need.
Once again, having the cloud means you can roam anywhere and access your images. This also provides an easier way to share images with customers. For example you can upload photos to a Dropbox folder and then share and password protect it. You basically e-mail a link from the Dropbox website to your recipient i.e. the customer. The customer then opens that link which is either password protected or not and then get access to the shared images.
These two cloud storage solutions should work fine for most photographers. Obviously not if the requirement is commercial printing of the images. This is ideal for online work that includes portfolios, e-commerce and headshots.
So what I hope I was able to show is what the benefits are to using a cloud solution for photography.
On another note, you don’t actually want to work directly with large bitmaps or uncompressed files that take up more than 50 MB of memory over a public data network. For one thing, if your link gets hijacked or compromised by hackers, you could lose data. Such connections are more practical for GbE (Gigabit Ethernet) connections on a LAN than over a WAN (the cloud). While you can store and make minor edits over the cloud, it still doesn’t seem practical to do full retouching or editing unless you have a really fast Internet connection.
Note: This article is not a paid endorsement of any of the products discussed. These are based on the author’s own opinions and experience. Please do your own due diligence before using a product.
Multimedia, Imaging, Audio and Broadcast Technology
262 
1
262 claps
262 
1
Written by
Editor HD-PRO, DevOps Trusterras (Cybersecurity, Blockchain, Software Development, Engineering, Photography, Technology)
Multimedia, Imaging, Audio and Broadcast Technology
Written by
Editor HD-PRO, DevOps Trusterras (Cybersecurity, Blockchain, Software Development, Engineering, Photography, Technology)
Multimedia, Imaging, Audio and Broadcast Technology
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@giuliano/build-a-spark-bot-to-share-box-files-with-your-colleagues-in-seconds-720021d7a92d?source=search_post---------267,"Sign in
There are currently no responses for this story.
Be the first to respond.
Giuliano Iacobelli
Mar 21, 2017·4 min read
When you become addicted to a cloud storage solution like Box the amount of files stored there can be a lot. For this reason you really don’t want to leave your team collaboration tool every time you need to share a file with your team mates.
This Spark bot allows you to search by name any file in your Box account without leaving the chat.
Note: this blueprint will work on Box accounts with paid plans since it relies on file sharing capabilities.
Go to https://developer.ciscospark.com/apps.html and create a Bot.
Fill the basic info to create your bot (if you need a 512x512 icon you can use this one http://i.imgur.com/HQTF5FK.png)
Once the bot is created you’ll see an Access token, copy it and save it temporary somewhere.
This app is available as a Blueprint, a pre-built template to help you get started with proven integration solutions. To get started click here or on the button below:
You’ll be prompted to pick a name for your project and then a wizard will start. After that Stamplay will prompt you to:
Once you have connected the two services click on Next.
Then you’ll be ask to provide the name of your bot. Here you need to type the name you gave your bot when you created it, the bot name precedes the @sparkbot.io part of its email address. (e.g the name for mybot@sparkbot.io is mybot)
Now you’re ready to leverage your bot in 1-to-1 conversations or invite him in Spark rooms for the benefit of your team. To search for a file you only need to tell him:
“getfile [FILENAME]”
In the example below you can see how it is used in a private conversation. In a group the only difference is going to be that you need to mention the bot before sending him the actual command.
This Spark bot is ran by a single Flow that capture any incoming message, checks if it contains the GETFILE command with a conditional step and, if so, it strips away the file name using the Code Block, search for it on Box and eventually returns the search result back on Spark.
Box Find File action currently searches across all files (by name or content) starting from the user’s root folder on Box.
If you want to refine your searches only inside a specific Box folder you can do that by adding the Id of the folder in the Ancestor Folder IDs configuration field for the Find File action.
You’re good to go now. Just start uploading pictures in the upload folder and you’ll start seeing your content organized.
At Stamplay we make it easy for people to automate processes and create high value integrations by tying together APIs. If you need help to connect your apps or have an API that you want to make easy to connect with tweet us at @stamplay and/or drop us a mail at support@stamplay.com
Enjoy!
Co-Founder @Stamplay, Lego™ for APIs. Enabling people to unleash the power of APIs. A #500STRONG Software Engineer in love with web products. Hip Hop addicted.
See all (2,485)
32 
32 claps
32 
Co-Founder @Stamplay, Lego™ for APIs. Enabling people to unleash the power of APIs. A #500STRONG Software Engineer in love with web products. Hip Hop addicted.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/google-developers/firebase-google-cloud-whats-different-with-cloud-storage-a33fad7c2b80?source=search_post---------1,"There are currently no responses for this story.
Be the first to respond.
In my previous posts about the relationship between Firebase and Google Cloud Platform (GCP), I talked about what’s different between the two for both Cloud Functions and Cloud Firestore. In this post, the topic will be Google Cloud Storage (Firebase, GCP), a massively scalable and durable object storage system. In this context, an “object” is typically a file — a sequence of bytes with a name and some metadata.
It’s tempting to think of Cloud Storage like a filesystem. In many ways it’s similar, except for one important fact: there are actually no directories or folders! Here’s how it really works. The top level of organization is called a bucket, which is a container for objects. Each bucket is effectively a big namespace full of objects, each with unique names within that space. Object names can look and feel like they have a directory structure to them (for example, /users/lisa/photo.jpg), but there are no directories or folders behind the scenes. These path-like names are helpful for organization and browsing in both the Cloud and Firebase consoles. Sometimes, we just use the word “folder” to make it easy to describe these paths.
Here’s what the Cloud console looks like when you’re browsing the contents of a storage bucket:
And here’s what the Firebase console looks like for the same bucket:
You’ll notice that the Cloud console puts a lot more data and functionality on the screen than the Firebase console. The Cloud console exposes the full power of Cloud Storage, while the Firebase console exposes only those features that are likely to be important to Firebase developers. That’s because the use cases for Cloud Storage tend to be a little different, depending on your perspective.
Cloud developers are often also enterprise developers. And, as you might guess, enterprise developers have broad requirements for object storage. They’re using Cloud Storage for backups and archives, regional storage, hosting static content, controlling access to objects, versioning, loading and saving data in BigQuery, and even querying data directly. Cloud Storage is very flexible! A majority of the access to an object comes from backend systems, often other Cloud products and APIs, and the SDKs. But for Firebase developers, the use cases are typically more narrow.
The most common use case for using Cloud Storage in a mobile or web app is handling user-generated content. A developer might want to enable users to store images and videos captured on their devices, then let the users view the files on other devices or share them with friends. Firebase provides SDKs (Android, iOS, web, Unity, C++) for uploading and downloading objects in storage, directly from the app, bypassing the need for any other backend components. To help with privacy and prevent abuse, Firebase also provides security rules, paired with Firebase Authentication, to make sure authenticated users can only read and write the data to which they’ve been given permission. Collectively, these SDKs and security rules are referred to as “Cloud Storage for Firebase”.
When you create a Firebase project, or add Firebase to an existing GCP project, a new storage bucket is automatically created for that project. This helps reduce the amount of configuration required to get started with Cloud Storage in a mobile app. Since all Cloud Storage bucket names across the entire system must be unique, this new bucket is named “[YOUR-PROJECT-ID].appspot.com”, where YOUR-PROJECT-ID is the globally unique ID of your project. You normally don’t even need to know this bucket name, as it’s baked into your Firebase app configuration file. When Firebase gets initialized, it’ll automatically know which bucket to use by default. In fact, this bucket is usually referred to as the “default storage bucket” for Firebase.
All that said, Firebase developers are not limited to using only Firebase SDKs, and Cloud developers can opt into using Firebase to read and write existing buckets. If you started out with Firebase, you can use any of the Cloud-provided tools, SDKs, and configurations at any time. And if you have existing data in Cloud Storage to use in a mobile app, you can certainly add Firebase to your project to make that happen.
Everyone who wants to access a Cloud Storage bucket from the command line can use gsutil, provided their Google account is authorized to do so. gsutil provides commands to upload and download files locally, and to configure a storage bucket. It’s often more convenient to use this tool rather than either of the web consoles, especially for bulk operations.
There’s one interesting difference between Firebase and Cloud. As you’ve just read, they both provide access control to objects in storage buckets, but those rules are mutually exclusive from each other. You use Cloud IAM to control access to an object only from backend systems and SDKs, but you use Firebase security rules to control access only from mobile applications using the Firebase client SDKs. These access control mechanisms don’t overlap or interfere with each other in any way. Note that the Firebase Admin SDK is actually a server-side SDK, which can also be used to access Cloud Storage. In fact, the API it provides is actually just a wrapper around the Cloud SDKs, which are controlled by IAM.
How are you using Cloud Storage? I’ve used it in a couple experimental projects. There’s a connected video doorbell that stores images of guests who ring the doorbell and a “universal translator” that translates speech recorded on a mobile device. Check those out, and let me know what you’ve built as well!
Engineering and technology articles for developers, written…
932 
7
932 claps
932 
7
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
firebase-consultant.com, Firebase GDE, engineer, developer advocate, Xoogler
Engineering and technology articles for developers, written and curated by Googlers. The views expressed are those of the authors and don't necessarily reflect those of Google.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@HOSTINGdotcom/what-is-cloud-storage-nas-vs-san-8a964406efa3?source=search_post---------125,"Sign in
There are currently no responses for this story.
Be the first to respond.
HOSTING
Mar 7, 2015·1 min read
What is Cloud Storage — NAS?
NAS is generally used for file-level computer data storage but NAS provides both storage and a file system. In fact, NAS is useful for more than just general centralized storage provided to client computers in environments with large amounts of data. By providing storage services, NAS can enable simpler and lower cost systems such as load-balancing and fault-tolerant email and web server systems.
What is Cloud Storage — SAN?
SAN is a dedicated network that provides access to consolidated, block level data storage. Common uses of a SAN include databases, server clustering, messaging applications (e.g. Microsoft Exchange), and high I/O, transaction-intensive apps.
NAS vs. SAN
NAS provides both storage and a file system, whereas SAN only provides block-based storage and leaves file system concerns on the “client” side. Also, SAN commonly utilizes Fibre Channel interconnects while NAS typically makes Ethernet and TCP/IP connections.
HOSTING uses these leading storage technologies and a fully redundant architecture. Whether it’s dedicated servers, on-demand cloud services, or disaster recovery solutions, we will customize a managed storage configuration to ensure business continuity in the event of a system failure.
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
4 
1
4 
4 
1
We build and operate high performance clouds for business-critical applications. Parent to @Ntirety and @HostMySite.
"
https://medium.com/mediafire/mediafires-position-in-the-cloud-storage-market-36167b7bc61a?source=search_post---------191,"There are currently no responses for this story.
Be the first to respond.
Recently there has been extensive media coverage of the United States Government’s shut down of MegaUpload.com which has drawn attention to the entire cloud storage market as a whole. While I can’t speak to the allegations against MegaUpload, I’d like to just take this opportunity to clarify some misconceptions about MediaFire that I’ve seen recently.
MediaFire offers a free, secure, and unlimited cloud storage service for users to upload and share data with their customers, colleagues, friends, and family. MediaFire’s Professional and Business plans offer uploaders additional functionality like company branding tools, analytics tools, and multi-seat employee accounts.
MediaFire’s premium services are based on a user’s ability to upload data and pay to distribute it. This storage and sharing model is similar to Akamai, Amazon S3, YouSendIt, etc. MediaFire has no caps or restrictions for downloading data and each upload is limited to a maximum size of 200MB for non-premium users.
Most importantly, people choose MediaFire because it offers high quality services that are simple to use and extremely powerful. We incorporate the latest HTML5 technologies, the most advanced user interfaces, and are constantly expanding our services with features requested by businesses and professionals. In Q2 2012, MediaFire will redefine how people access and manage their data and how they interact with the cloud by releasing long-in-development desktop and mobile applications that solve problems Microsoft, Dropbox, and Box fail to address.
A little more about MediaFire:
MediaFire was founded in 2006 and is based near Houston, Texas. MediaFire is used by people at 86% of Fortune 500 companies and is currently ranked in the top 100 websites globally by Amazon’s Alexa and Google/DoubleClick’s Ad Planner services.
Thanks again everyone, especially to our fans who we’ve heard loud and clear evangelizing the benefits of our service. It’s easy for us to get caught up in building our technology and services and we haven’t spent enough time espousing our own benefits and differentiators. The cloud is an incredibly convenient and powerful tool for many professionals, businesses and individuals. At MediaFire, we are committed to continuing to innovate on this technology and provide it to our users in new, simple, ethical and cost effective ways.
Tom Langridge — Co-Founder, Corporate Communications
This is the blog of MediaFire.com
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
Written by
MediaFire stores all your media and makes it available to you, anytime you want, anywhere you go, on any device you might have.
This is the blog of MediaFire.com — File Sharing and Storage made Simple. On this blog we occasionally post announcements about our products and service, featured stories about our users, and news about MediaFire from around the web.
"
https://medium.com/@duhroach/google-cloud-storage-on-a-shoestring-budget-55f054fad436?source=search_post---------44,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Oct 24, 2017·6 min read
A common question I get at Meetups and conferences relate to billing optimization. Things like “what’s the cheapest way to do X” or “how do I cut my costs” or “how do I not go over my free quota”.
IMHO Google Cloud Platform has some of the most transparent, cost effective pricing structures available from cloud providers. But even with that, it’s still a challenge to properly figure out how to optimize your architecture for price.
So, let’s take a look at how to run Google Cloud Storage on a Shoestring budget.
Disclaimer : These prices are accurate as of 10/5/2017. Since time continues to move forward, these prices may not be accurate in the future. Also, these are my best attempts at “math”. All standard disclaimers about me being bad at math apply here.
A quick summary of GCS’ pricing page reveals that your GCS bill is a composite of 3 main things : Data storage, Network usage, Operations (4 if you’re using near-line, but we’ll ignore that.)
Here’s what you get for free, per month:
We will need to factor this into all of our forward facing calculations, since this quota is taken into account for your monthly services. Let’s break each one down really quick and see what the costs would be.
Once you eclipse your free storage quota, you’ll see that storage costs differ slightly per area. For example, us-central1 is 0.02 per GB, where us-east1 is 0.023 (it also changes if your in AMEA or APAC).
So, what’s it cost to store 1TB of data per month in the IOWA center (us-central1)?
(1024GB-5GB) * 0.02 = $20.38
That’s pretty cheap, I suppose. That’s basically lunch for two people to store 1TB of data.
You’re charged for outbound costs, depending on the origin, and the outbound tier. For example, if you only transfer 0–1TB, from the Iowa location, it’s $0.12 per GB to anywhere in the world (except China and Australia which have diff costs) based on a tiered structure (so if you go over 1TB, you’ll get charged differently).
So, what’s it cost to transfer 1TB of data a month to to external clients (outside of GCP) in the US or EU?
(1024GB — 1GB) * 0.12 = $122.76
Operations on a bucket/object are anything which queries or changes data on it. Ops are broken into two categories : A and B, and have different costs associated:
$0.05 per 10k ops (A — insert, patching, Listing buckets, listing objects, watching & triggers)
$0.004 per 10k ops (B — mostly everything else)
So, let’s say your 1TB of data is broken across 1k objects in GCS.
Let’s say the average user looks at 500 object listings a month (you’re a shoe site, or something), in batches of 50, so 10 calls to our API per month, per user.
What’s this cost?
If we have 10k users @ 10 API calls a month, that’s 100k Class A ops. Objects.list is a Class A operation. Meaning we get 5k of those free a month. . so:
(100k -5k)*0.05 = $4.75
Prices are highly dependent on your scenarios, so let’s take a few examples.
Scenario 1 : Data in a regional bucket, transferring to the same region.
Note: moving data from a regional bucket to a service (GCE,GAE,GCF,GKE) in the same region is free. However the cost for network egress looks to be uniform between GCS, and all services. So, there’s no upside to sending data to a Frontend and serving that to a user.
Although sending GCS->Frontend->user will incur costs if the frontend is in a different region than the GCS bucket.
Scenario 2 : Data in a regional bucket, transferring to a different region.
Let’s put our data in us-west1, and have a user in us-east4. We’d be getting charged the same $122.76 cost for network egress, but our performance wouldn’t be ideal due to distance latency.
What if we copied the data to a bucket in a closer region (us-east4)?
(1024GB-5GB) * 0.02 = $20.38 and is doubled, since us-east4 and us-west1 have the same storage cost. If you assume that xfer costs remain the same, then you’d end up with a total cost of $122.76 for network, and $40.76 for storage giving $163.52. So, a little more expensive.
What if we used a multi-region bucket instead?
Storage costs are 0.026 / gig / month for multi-regional. So, (1024GB-5GB) * 0.026 = $26.494 for storage, plus $122.76 for network gives us $149.25. So it’s about ~18 dollars cheaper to use a multiregional bucket instead.
You can specify the load-balancer to fetch from a GCS bucket directly, which would allow the data to be distributed and cached via the CDN w/o needing to set up extra buckets.
LB is 0.08 per /GB for the first 10TB of cache-egress, and 0.04 /gig of cache fill.
So, what would it cost to xfer 1TB of data between regions to a client using the LB as a front end to the bucket?
So, summary:
For cross-region fetches (in the same multi-region):
If we have 10k users @ 10 API calls a month, that’s 100k Class A ops.
Objects.list is a Class A operation. Meaning we get 5k of those free a month… so: (100k -5k)*0.05 = $4.75
Does it make more sense to list that in Datastore?
Considering DataStore is 1GB of storage free (0.18 /gig over that); it costs you 0.06 per 100k entries read.
If we had 500 object listings, let’s assume 1k of metadata each; that would be 500k of metadata storage (which is within the free tier) .
Each user is looking at 500 object listings a month; at 10k users, that’s 5000k entity reads / month. Factoring in the cost of 0.06 per 100k reads, we end up with ((5000–50)/100)*0.06= $2.97
So, Summary:
It’s cheaper to store your object metadata in data store, and fetch it there, rather than doing a GCS bucket listing.
Custom metatdata on an object is charged per character for the object. If you assume 1k metadata per object, and 100k objects, you’re looking at an additional 1k of data in your GCS bucket charges. That’s still relatively low for storage, ($9.5^e-7 cents total);
Fetching that metadata falls as a classB operation, which you get 50k of free / month. So, if you were doing 100k metadata fetches a month, that’s ((100k-50k)/10k)*.004 = $0.02.
Likewise, DataStore gives you 1GB free storage a day; so your 100k would be in the free tier, storage wise, daily.
You get 50k entity reads / day free. If we assume 100k / month, then 100k / 30 = 3k reads / day.
All within the free tier.
In order for Datastore to not make sense, you’d have to be doing >50k entity reads a day, at which point, you’re pro-rated charged $0.06 per 100k entities you read. Meaning you’d be doing 1500k+ entity reads a month before you’d jump over the free tier, but even at that point, it’s still cheaper to do the reads in Datastore rather than grabbing metadata directly from GCS.
So, Summary:
Datastore is the cheapest way to store and fetch metadata on your objects.
Absolutely! Stay tuned to my medium page for more details, and don’t forget to check out Google Cloud Performance Atlas, where we help you trim down your cloud usage and maximize your profit-to-cost structure.
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
127 
2
127 
127 
2
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
"
https://medium.com/@richardyaoipg/google-imports-hard-drive-data-to-cloud-storage-8cc00d4c3061?source=search_post---------198,"Sign in
There are currently no responses for this story.
Be the first to respond.
Richard Yao
Jun 18, 2013·1 min read
Google today added a new service to its Cloud Storage that will allow developers to send their hard drives to Google to import very large data sets that would otherwise be too expensive and time consuming to undertake alone. Google is charging a flat fee of $80 per hard drive. Google claims that this process of uploading data to a Cloud Storage bucket is both faster and less expensive than transferring data over the Internet. Amazon already has a similar service, and also charges $80 for a hard drive — but Amazon also charges a per-hour fee for uploading data.
Manager of Strategy & Content, IPG Media Lab
Manager of Strategy & Content, IPG Media Lab
"
https://medium.com/google-cloud/3-great-options-for-persistent-storage-with-cloud-run-f1581ee05164?source=search_post---------365,"There are currently no responses for this story.
Be the first to respond.
Cloud Run is a managed compute platform that automatically scales your stateless containers. Cloud Run is serverless: it abstracts away all infrastructure management, so you can focus on what matters most — building great applications.
However, most applications are stateful. As a developer, how can you permanently store images, database records, and other pieces of data?
In this article, we’ll discuss 3 example ways in which you can store data on Google Cloud Platform with Cloud Run.
There are a variety of ways to store data on Google Cloud, depending on your application’s needs. In this imaginary application, let’s create a e-commerce shoe store. Here are three storage options we’ll use for storing different kinds of data:
For storing files/objects such as images and video, or archival backups, Google Cloud Storage is an excellent option for this type of data. Here’s an example code snippet in Node that can be used with Cloud Run:
For storing unstructured data, or creating a flexible, NoSQL, cloud database, Cloud Firestore is a great solution for mobile, web, and server development. In this sample, we want to store flexible JSON objects for shoe data:
For storing structured data in a RDBMS, such as MySQL, PostgreSQL, and SQL Server, Cloud SQL can connect to a nice interface for storing and querying your data. Here created a customer table where we can use familiar SQL to create powerful queries:
Note: See Connecting to Cloud SQL from Cloud Run (fully managed) for full documentation on how to connect and use these services together.
Those are 3 storage options for storing data in GCP. Other products include Cloud Filestore, Cloud Storage for Firebase, and Cloud Memorystore among other databases.
Sometimes your data storage could use more configuration. Here are two tips:
Thanks for reading. If you are looking for more reading about storing data with GCP, are some links:
Google Cloud community articles and blogs
57 
3
57 claps
57 
3
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Google • Explorer
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://medium.com/hackernoon/crypto-review-siacoin-sc-b1d0f0a5c78f?source=search_post---------207,"There are currently no responses for this story.
Be the first to respond.
For my first review I will be going over Sia — one of the players in the decentralized storage space. The cloud storage industry is massive and companies like Sia are hoping to put a dent in the dominance Amazon, Microsoft, and Google have enjoyed over the past decade.
sia.tech
It took a while but I am finally picking back up on my crypto review series. If you haven't read my previous articles, please check out my intro to bitcoin.
medium.com
And my intro to the crypto review series
medium.com
I will write about coins I am interested in, most likely I will own some (I own ~40 different coins now so that doesnt make them particularly special) but in no way is this investment advice — it is generally a way for me to align my thoughts — do your own research before investing!!
What is Sia? Basically storage — think dropbox, google drive, etc. The shtick is that it is decentralized, encrypted, peer to peer storage — although, no one party is holding the information you are storing — it is encrypted, multiplied, chopped into little bits, and then all those little bits are sent around to a bunch of different hosts. When you want access to your file, the appropriate bits are recompiled and you get your file. Think of your file as being in a T-1000 type state.
By building the framework and outsourcing the storage to anyone with a hard drive and an internet connection Sia has reduced overhead dramatically and can charge a much lower fee. They multiply the bits in case a node holding a piece of your file happens to be offline (even though they are incentivized not to be).
Now for some due diligence
Sia was created by David Vorick and Luke Champine of Nebulous Inc, a VC-funded startup in Boston.The idea of Sia was originally conceived at HackMIT 2013.
There really isnt much out there on David Vorick but not in the way that some of these scam coins have shady founders but more in the way where I am assuming he is more head down, focusing on the product. The VC firm seems legit but there isnt really any other crypto (when i checked) in their portfolio. It seems like they could use a marketing/evangelist type.
David’s online presence basically boils down to his medium posts and some youtube videos. This one also had the added benefit of talking through the different blockchain storage protocols. David jumps in at min 13. I like his tone for as much as thats worth.
They have a product & users!!
There are two components that set Sia apart from other decentralized storage (STORJ, IPFS) on the pricing side. I have a reasonable understanding of STORJ, still looking into IPFS so these assumptions may not be 100%.
Product details can be picked up from their website but my take is that they are taking the whole decentralized thing to the max. This is great for decentralization maximalists but I do think it may limit the extent of their adoption.
They also tout the redundancy they get from storing file bits across multiple machines and incentivizing hosts to stay online.
I haven't stored anything yet but but this shed some interesting light on the structure. You need to lock some funds up that you can then use to secure storage at whatever rate you are comfortable with. As adoption picks up a lot of the coins will be off the exchanges and locked up. Could lead to volatility of those outstanding or it may just find a nice equilibrium. Something to keep an eye on.
Biggest question with all these projects is — Do they reeeeeeeeally need their own token? Sia is issued on the blockchain where they issue the smart contracts. Im sure there are some more nitty gritty details that lay it out but it seems like a reasonable enough reason for a separate currency.
Heres what they are touting, directly from their front page
Now, this is really only a projection because the host(those with HD space to rent) set their own price (because its decentralized!!!)
It looks like from a list of active hosts that the going rate is ~2k SC per TB per month which at current rates (.00000570 BTC @ BTC = 2700 USD) comes out to a bit over $30. I think this is mostly because of the massive increase in price SC has seen over the past month (roughly 6x increase against USD, 4x against bitcoin) and my assumption that these are longer term contracts. I imagine the longer its out the more stable the price will become but as you can see it is truly global in scope so some volatility against USD will remain. I do think their claim in the chart above is a little misleading though since they are assuming hosts will be willing to be paid at that rate.
There are 26 billion or so SC now, and there will eventually by around 48 billion in 2023. The faster minting is a bit of a concern now but they seem to be gaining adoption at a decent clip. At the current price of 2k SC per terabyte per month when all 48,460,000,000 coins are mined they will have a capacity of 24.23 exabytes.
I couldn’t find information on exactly how much Amazon is storing total these days but I don’t think this is that crazy of a number. Driverless car, Image recognition, VR, AI, machine learning, other cool buzzwords — what do they all have in common? a need for massive amounts of data.
Love this — a public Trello board with their roadmap
trello.com
There is a lot of useful information on their Pulse Site like the distribution of hosts.
and the current prices (I believe those are monthly)
They also seem very active in slack answering questions. It seems like David is making the rounds more now but to make the jump they will likely need to bring in some marketing folks. They are VERY open with the project which the community appreciates so if they deliver Im sure we will continue to hear more from them.
The next step for Sia in my opinion is landing a big partner or some applications built on it to gain some notoriety. My guess is STORJ will be more consumer focused, IPFS is going to land the enterprise projects, and Sia will fall somewhere in between but be popular with the projects striving for true full stack decentralization.
I really think some of the “Next 6 Months” items could trigger a jump — video streaming gets people excited, automatic host pricing will make it less manual, and supporting over 10 TB at a time should allow for more applications to be built on it.
Feel free to recommend a coin to dive into next, and as always, please do your own research and only invest as much as you can afford to lose.
Happy Investing!
Hacker Noon is how hackers start their afternoons. We’re a part of the @AMI family. We are now accepting submissions and happy to discuss advertising & sponsorship opportunities.
If you enjoyed this story, we recommend reading our latest tech stories and trending tech stories. Until next time, don’t take the realities of the world for granted!
#BlackLivesMatter
717 
9
how hackers start their afternoons. the real shit is on hackernoon.com. Take a look.

By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.
Medium sent you an email at  to complete your subscription.
717 claps
717 
9
Written by
Founder @ Atomica, Crypto Stuff, Dad Stuff.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Written by
Founder @ Atomica, Crypto Stuff, Dad Stuff.
Elijah McClain, George Floyd, Eric Garner, Breonna Taylor, Ahmaud Arbery, Michael Brown, Oscar Grant, Atatiana Jefferson, Tamir Rice, Bettie Jones, Botham Jean
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://blog.sia.tech/the-state-of-cryptocurrency-mining-538004a37f9b?source=search_post---------203,"For those new to the blog, I am the lead developer of Sia, a blockchain based cloud storage platform. About a year ago, myself and some members of the Sia team started Obelisk, a cryptocurrency ASIC manufacturing company. Our first ASICs are going to ship in about 8 weeks, and our journey with Obelisk has given us a lot of insight into the world of cryptocurrency mining.
One of the reasons we started Obelisk was because we felt that coin devs in general had a very poor view into the mining world, and that the best way to understand it would be to get our hands dirty and bring a miner to market.
Since starting Obelisk, we’ve learned a lot about the mining space, as relevant to GPUs, to ASICs, to FPGAs, to ASIC resistance, mining farms, electricity, and to a whole host of other subjects that coin developers should be more aware of. We aren’t able to share everything that we know, but we’ve pulled together information on a set of key topics that I think will be helpful to cryptocurrency designers and other members of the cryptocurrency community.
We’ve been pessimistic on ASIC resistance for a long time, and our journey into the hardware world solidly confirmed our position. Hardware is extremely flexible. General purpose computational devices like CPUs, GPUs, and even DRAM all make substantial compromises to their true potential in order to be useful for general computation. For basic hardware development, most algorithms can see substantial optimization just by taking away all of that generality and focusing on one specific thing.
The vast majority of ASIC-resistant algorithms were designed by software engineers making assumptions about the limitations of custom hardware. These assumptions tend to be incorrect.
Equihash is perhaps the easiest target, as a lot of people were quite confident in the equihash algorithm, and we’ve been saying for close to a year that we know how to make very effective equihash ASICs.
The key is to make sorting memory. A lot of algorithm designers don’t seem to realize that in an ASIC, you can merge the computational and storage pieces of a chip. When a GPU does equihash computations, it has to go all the way out to off-chip memory, bring data to the computational cores, manipulate the data, and then send the altered data all the way back out to the off-chip memory.
For equihash, the manipulations that you need to make to the data are simple enough that you can just merge the memory and computation together, meaning that you can do most of your manipulating in-place, substantially reducing the amount of energy used to move data back and forth, and also substantially decreasing the amount of time between adjustments to the data. This greatly increases efficiency and speed.
Needless to say, we weren’t the least bit surprised when Bitmain released powerful ASICs for equihash. The Bitmain ASICs are actually substantially less performant (5x to 10x) than our own internal study suggested they would be. There could be many reasons for this, but overall we think that it’s pretty reasonable to assume that more powerful equihash ASICs will be released in the coming months.
We also had loose designs for ethash (Ethereum’s algorithm). Admittedly, ethash was not as easily amenable to ASICs as equihash, but as we’ve seen from products on the market today, you can still do well enough to obsolete GPUs. Ethash is by far the most ASIC resistant algorithm we’ve looked at, most of the others have shortcuts that are even more significant than the shortcuts you can take with equihash.
At the end of the day, you will always be able to create custom hardware that can outperform general purpose hardware. I can’t stress enough that everyone I’ve talked to in favor of ASIC resistance has consistently and substantially underestimated the flexibility that hardware engineers have to design around specific problems, even under a constrained budget. For any algorithm, there will always be a path that custom hardware engineers can take to beat out general purpose hardware. It’s a fundamental limitation of general purpose hardware.
A lot of people believe that computing is broken up into 3 categories: CPU, GPU, and ASIC. While those are the categories that are generally visible to the public, in the chip world there’s really only one type of chip: an ASIC. Internally, Nvidia, Intel, and other companies refer to their products as ASICs. The categories as known to the public are really a statement about how flexible the ASIC is.
I would like to use a 1 to 10 scale to measure flexibility. At one side, a ‘1’, we’ll put an Intel CPU. And at the other side, a ‘10’, we’ll put a bitcoin ASIC. Designers have the ability to create chips that fall anywhere on this scale. As you move from a ‘1’ to a ‘10’, you lose substantial flexibility, but gain substantial performance. You also decrease the amount of design and development effort required as you sacrifice flexibility. On this scale, a GPU is a ‘2’.
Generally speaking, we don’t see products developed that fall anywhere between a GPU and a fully inflexibile ASIC because typically by the time you’ve given up enough flexibility to move away from a GPU, you’ve only got a very specific application in mind and you are willing to sacrifice every last bit of flexibility to maximize performance. It’s also a lot less costly to design fully inflexible ASICs, which is another reason you don’t see too many things in the middle.
Two examples of products between a GPU and an ASIC would be the Baikal miners and the Google TPU. These are chips which can cover a flexible range of applications at performances which are substantially better than a GPU. The Baikal case specifically is interesting, because it’s good enough to obsolete GPUs for a large number of coins, all using the same basic chip. These chips appear to be flexible enough to follow hardforks as well.
The strategy of hardforking ASICs off of a network is going to lose potency the more it happens, because chip designers do have the ability to make chips that are flexible, anywhere from slightly flexible to highly flexible, with each piece of flexibility costing only a bit of performance. The Monero devs have committed to keeping the same general structure for the PoW algorithm, and because of that commitment we believe that you could make a Monero miner capable of surviving hard forks with less than a 5x hit to performance.
Equihash is an algorithm that has three parameters. Zcash mining happens with one particular choice for these parameters, and any naive hardfork from Zcash to drop ASICs would likely involve changing one or more of these parameters. We were able to come up with a basic architecture for equiahsh ASICs that would be able to successfully follow a hardfork that chose any set of parameters. Meaning, a basic hardfork tweaking the algorithm parameters would not be enough to disrupt our chip, a more fundamental change would be needed. Despite this flexibility, we believe our ASIC would be able to see massive speedups and efficiency gains over GPUs. We never found funding for the equihash ASICs, and as a result our designs ended up on the shelf.
The ultimate conclusion here once again wraps back to the capabilities of ASICs. I think there are a lot of people out there who do not realize that flexible ASICs are possible, and expected that routinely doing small hardforks to disrupt any ASICs on the network would be sufficient. It may be sufficient sometimes, but just as algorithms can attempt to be ASIC resistant, ASICs can attempt to be hardfork resistant, especially when the changes are more minor.
A few months ago, it was publicly exposed that ASICs had been developed in secret to mine Monero. My sources say that they had been mining on these secret ASICs since early 2017, and got almost a full year of secret mining in before discovery. The ROI on those secret ASICs was massive, and gave the group more than enough money to try again with other ASIC resistant coins.
It’s estimated that Monero’s secret ASICs made up more than 50% of the hashrate for almost a full year before discovery, and during that time, nobody noticed. During that time, a huge fraction of the Monero issuance was centralizing into the hands of a small group, and a 51% attack could have been executed at any time.
Monero’s hardfork appears to have been successful in shaking the ASICs. I don’t believe that the ASIC designers attempted to build flexibility into their ASICs, but now that Monero has announced a twice-annual PoW change, we may see another round of secret ASICs with more flexibility. The block reward for Monero is high enough that even if you think you have only a 30% chance of your ASIC surviving the PoW hardfork, it’s more than worthwhile to pursue a hardfork resistant ASIC.
My strong guess is that Monero is going to have another round of secret ASICs built, and that these ASICs will be more conservative and flexible, attempting to follow the hard forks that Monero puts out every 6 months.
We’ve heard rumors of plenty of other secret ASICs. People who own secret ASICs tend not to talk about them very much, but as of March 2018, we had heard of secret ASIC rumors specifically for both Equihash and Ethash, and then for many other smaller coins that don’t have any ASICs on them yet. We believe a full 3 different groups were actively mining on Zcash with different ASICs prior to the Bitmain Z9 announcement.
We know of mining farms that are willing to pay millions of dollars for exclusive access to designs for specific cryptocurrencies. Even low ranking cryptocurrencies have the potential to make millions in profits for someone with exclusive access to secret ASICs. As a result, an informal underground industry has been set up around secret mining. The heavy amount of secrecy involved means it’s disconnected and mostly operates off of rumor and previous relationships. But it’s nonetheless a very lucrative industry, and even when things happen like the Vertcoin hardfork, the setback to secret miners is dwarfed by the returns of the successes.
At this point, I think it’s safe to assume that every Proof-of-Work coin with a block reward of more than $20 million in the past year has at least one group of secret ASICs currently mining on it, or will have secret ASICs mining on it within a few months. The easiest way to detect this is GPU returns, however as ASICs continue to infiltrate every coin on the market, that will cease to be a reliable metric as there will be no GPU-only coin to use as a baseline, at least not one that is large enough to sustain all of the massive GPU farms that are out there.
The ASIC game has become such an advanced game because there is so much money on the table. Even small coins can be worth tens of millions of dollars, which is more than enough to justify a high-risk production run.
Manufacturers that sell ASICs to the public, like Bitmain, tend to be less exposed than consumers to things like ASIC hardforks. Using Sia as an example, we estimate it cost Bitmain less than $10 million to bring the A3 to market. Within 8 minutes of announcing the A3, Bitmain already had more than $20 million in sales for the hardware they spent $10 million designing and manufacturing. Before any of the miners had made any returns for customers, Bitmain had recovered their full initial investment and more.
In this case, a hardfork doesn’t hurt Bitmain. Bitmain made a profit off of Sia, and there’s nothing the developers can do about that. And it seems that was the case for the Monero miners that Bitmain announced as well. Bitmain didn’t even get to announce the miners until after Monero announced their hardfork, and still it seems that they sold enough obsolete hardware to customers to make back their costs and turn a hearty profit.
The mining game is weighted heavily in favor of the manufacturers. They get to control the hardware production, the supply, and they know more about the state of the industry than anyone else. The profitability of a miner largely depends on variables that the manufacturer controls without disclosure to anyone else.
In the case of Halong’s Decred miner, we saw them “sell out” of an unknown batch size of $10,000 miners. After that, it was observed that more than 50% of the mining rewards were collecting into a single address that was known to be associated with Halong, meaning that they did keep the majority of the hashrate and profits to themselves. Our investigation into the mining equipment strongly suggests to us that the total manufacturing cost of the equipment is less than $1,000, meaning that anyone who paid $10,000 for it was paying a massive profit premium to the manufacturer, giving them the ability to make 9 more units for themselves. Beyond this, the buyer has no idea how many were sold nor where the difficulty would be when the units shipped. The manufacturer does know whether or not the buyer is going to be able to make a return, but the buyer does not. The buyer is trusting the manufacturer entirely.
If a cryptocurrency like Sia has a monthly block reward of $10 million, and a batch of miners is expected to have a shelf life of $120 million, the most you would expect a company could make off of building miners is $120 million. But, manufacturers actually have a way to make substantially more than that.
In the case of Bitmain’s A3, a small batch of miners were sold to the public with a very fast shipping time, less than 10 days. Shortly afterwards, YouTube videos started circulating of people who had bought the miners and were legitimately making $800 per day off of their miner. This created a lot of mania around the A3, setting Bitmain up for a very successful batch 2.
While we don’t know exactly how many A3 units got sold, we suspect that the profit margins they made on their batch 2 sales are greater than the potential block reward from mining using the A3 units. That is to say, Bitmain sold over a hundred million dollars in mining rigs knowing that the block reward was not large enough for their customers to make back that money, even assuming free electricity. And this isn’t the first time, they pulled something similar with the Dash miners. We call it flooding, and it’s another example of the dangerous asymmetry that exists between manufacturers and customers.
At the end of the day, cryptocurrency miner manufacturers are selling money printing machines. A well-funded profit maximizing entity is only going to sell a money printing machine for more money than they expect they could get it to print themselves. The buyer needs to understand why the manufacturer is selling the units instead of keeping them for themselves.
There are a few reasons it would make sense for a manufacturer to sell a money printing machine rather than keep it. The first is capital — manufacturing is an expensive process with a lot of lead times. If the manufacturer doesn’t have enough money to build their own units, then it makes sense to sell the units instead, and use the money from sales for production. It boils down to the manufacturer selling future revenue to get revenue today, which is a very common transaction in the financial world.
Another reason the manufacturer may sell money printing machines instead of running them is the electricity costs of running them. If the manufacturer can only get a certain deal on electricity, then there may be someone else with cheaper electricity or better datacenters who would be willing to buy the units at a price that’s higher than what the manufacturer values them at. Most manufacturers however have access to good electricity deals, and unless you have some deal with free electricity or are otherwise running a cutting edge professional operation, you are not likely to do better than the manufacturer.
And finally, the manufacturer may have some other reason that they want money quickly instead of making a longer term investment into the hardware themselves. This is likely not the case in cryptocurrency mining though, because the shelf life of miners tends to be under two years, and to a business that’s not a long time at all to wait for healthy returns.
In the traditional chip development world, it takes about 2 years to go from launching a development effort to getting a chip out out the door. In the case of the Sia and Decred miners we built, it looks like we’re going to be at about 13 months total from project launch to product delivery. If we had to do the same thing again, I think we could do it in about 9 months.
A huge portion of the time spent is on full-custom routing for the chip. There’s a much faster development process called place-and-route which trims about 3 months off of the chip development time, but produces chips that are 2x-5x slower than what a full-custom team can produce. We think that if we used a place-and-route design methodology, we could get our product delivery timeline close to 6 months.
We believe it took Bitmain about 5 months to create the A3 miner, and we believe it took Halong about 9 months to create the B52 miner. We suspect both of these were completed using place-and-route methodologies, especially given the relatively poor performance of each.
Those are timelines for creating a chip from scratch. If the goal is to chase a hardfork, the timelines are a lot shorter. If you know in advance that you are going to need to redesign your chip, there are a lot of shortcuts you can take to reduce the overall time required to get to market. Changing a design to meet a tweak is going to take much less time than starting from scratch, a good team with a well-planned base architecture can probably complete designs in about 2 weeks. From there, with some help from a hot-lot, you can get a new set of chips in about 40 days. These then need to be packaged, which is going to take around a week, and then sent to the manufacturer for assembly. Finally, you have to get the units to a datacenter and start mining.
If you had all the wafers, parts, and everything reserved ahead of time, we believe that you could upgrade a chip to adapt to a hardfork and have miners mining on the new hashing algorithm in about 70 days, at least in theory. In practice, Bitmain would probably require 3–4 months to adapt an existing chip to a hard fork, and if they hadn’t reserved wafers in advance they’d be looking at 4–5 months. Any company that is not Bitmain can probably add another 30–60 days to these numbers.
Some people already understand the situation with economies of scale well. The more money you spend, the more effective each dollar is. This effect is maintained throughout every level of scale that I’ve been able to peer into, including scales where you are going from billions of dollars to tens of billions of dollars.
The most simple way that this manifests is in volume orders. If you order one hundred thousand heatsinks, you can get one price. If you order one million heatsinks, you get a better price. As you continue to scale up, the price falls off. This effect is in place for almost all parts in the hardware industry, and it happens because manufacturers get to the point where they can buy and dedicate equipment to your order, and then keep that equipment at 100% efficiency. As you scale up, you gain greater customization and specialization in addition to cost savings, meaning your products become more effective as well as cheaper.
At some point, it makes sense just to buy out all the capacity at the manufacturer. A huge component of manufacturing price is paying for equipment. Equipment that is idle 50% of the time is going to have 2x the effective per-part price than equipment that is in use 100% of the time. As you increase lead times and order volumes, you can start getting fully dedicated equipment running nonstop for you, which again pulls the price down substantially.
In a similar sort of turn, someone has to manufacture that equipment. If you scale up to the point where you are continuously ordering specific equipment, the manufacturer can start to dedicate pipelines to you and keep their own equipment operating 100% of the time, and so the equipment you use for manufacturing is getting cheaper now on top of being in use all the time.
And that’s just the beginning. At every step, each provider, manufacturer, etc. is going to be taking margins, typically somewhere around 30% depending on how commodity your orders are. If you have enough money, you can start to engage in vertical integration, cutting out the margins of your manufacturers by either buying them out or creating your own margin-free entity.
Hardware goes through a lot of steps. There’s the acquisition of raw materials like iron and oil, the refinement of these materials, and then they get manufactured into base parts that can be sold for more general products. Those base parts often have lead times that are 6+ months, which means that suppliers typically keep a huge number of them in stock, so that they can provide customers with parts on faster timelines. Every step typically introduces both a middleman and inefficiency, especially because each step is targeted towards general use parts instead of a specific product. If you have a specific product that has enough volume / scale to justify dedicated supply chain elements, you will shave costs, you will shave lead times, you will improve product quality and performance, and you will get ahead of what anyone without that type of scale can achieve.
To present a very rough number, it seems to me that every time you 10x the amount of money you are spending, you can save about 30% per part. That is to say, if you are spending $100 million on mining units, you might get units for $500 each. And if you are spending $1 billion on mining units, you can squeeze that price down to $350 per miner just by having more money to throw around. And then if you jump to $10 billion, your per-miner price might drop to $245. Your mining machines aren’t just getting cheaper though, they are also becoming more customized and higher performance. You don’t just build a dollar moat with scale, you also build a quality moat.
When we started Obelisk, we had numerous separate sources reach out and warn us that Bitmain plays dirty, and that if we try to manufacture in China, we will be stopped.
With that in mind, we brought up the issue to everyone we worked with, and proceeded cautiously with an American manufacturer that owned a facility in China. This was attractive because the prices were close to half of what we would have paid to manufacture in America, and manufacturing was going to be one of our largest expenses by far.
We did everything we could to keep the entity disconnected from Obelisk, and we hid the name of the manufacturer from our website or any public data, and we were very careful with who we gave the name of our manufacturer privately. We had a separate entity put in parts orders where we could.
After any reasonable timeframe to reach out to another manufacturer, after-hours on a Friday night, our manufacturer reached out to us and said with little warning or reasonable explanation that they would be unable to manufacture for us. Just as we had been warned, our attempt to manufacture in China had fallen flat on its face. This setback is estimated to have cost us somewhere north of $2 million.
We have absolutely no evidence that Bitmain was involved in any way. We’ve had other companies reach out to us and confirm that they’ve experienced similar things, but they too had no concrete evidence that Bitmain was involved in any way. I honestly was not sure whether to include this section in the blog post, because unlike most of the other things I’ve been saying, we really have nothing more than a bunch of warnings that ended up being correct to inform us.
But it’s well established in the industry that Bitmain plays dirty, and it’s been suggested to us from all sides that they have been and will continue making moves within our supply chain to ensure we can’t succeed, and that they do the same with all of their competitors.
Mining farms are perhaps the one area where manufacturers and economies of scale are not dominant. Good electricity deals tend to come in smaller packages, tend to be distributed around the world, and tend to be difficult to find and each involve unique circumstances. As such, it’s been difficult for larger companies to create a system for scooping up low cost electricity worldwide. Instead, the cheapest electricity and datacenters in the world tend to be held by smaller parties that don’t individually own all that much electricity or hashrate.
From what I’ve been able to dig up, the average professional mining farm is paying somewhere between 4 and 6 cents for electricity, and then another 3 to 6 cents for management and maintenance. A total cost of $50 per kilowatt per month is probably somewhere close to the median for large scale mining farms. As techniques improve and the industry grows, we expect this number to fall closer to $35 per kilowatt per month (including maintenance, land, taxes, etc.) throughout 2019 and 2020. We don’t believe that anyone paying more than $80 per kilowatt per month will be able to remain competitive unless the price of cryptocurrency continues to rise rapidly over the next year.
The top 20% of miners all seem to be below $35 per kilowatt per month already, from what we’ve been able to glean, and the top 5% seem to be below $20 per kilowatt per month. By my estimation, if the price of Bitcoin were to fall substantially, these mining operations would be able to stay in business and everyone paying $50 or more would be forced to shut down their facilities.
It’s really hard to know where Bitmain is at, but based on everything we’ve been seeing we estimate that Bitmain is somewhere around the $30 per kilowatt per month mark. That is, they are doing better than the median mining operation, but by no means are they in the elite tier.
Most mining startups seem incredibly focused around the chip itself. From what we’ve seen, the chip is really less than half the story. So, the chip is important (apologies for the title), but if all you’ve got is the best chip in the world, you aren’t going to be a competitive manufacturer.
As a miner, the goal at the end of the day is to do as many hashes as possible for as little money as possible. A faster chip means that you need to spend less money on chips to get hashrate. And a more energy efficient chip means you need to spend less money on electricity to get hashrate. But you aren’t just spending money on chips and electricity. You spend money on PCB, on controllers, on ports like ethernet ports, on power supplies and power management, on fans, on enclosures, on shelves in your datacenter, etc.
At the end of the day the chip is only a portion of the equation for mining successfully. If you aren’t thinking about the whole picture, you are going to end up with a chip that will lose you money. This is actually one of the things that killed Butterfly Labs (among many) — they designed a high performance chip that produced hundreds of watts of heat. By comparison, Bitmain chips are typically about six watts each. Where Bitmain is able to throw on a forest of fat heatsinks, Butterfly Labs had to struggle with expensive, cutting edge, unreliable cooling systems, and that ultimately meant their powerhouse chip was late to market and too expensive to operate.
People tend to under-estimate Bitmain. Yes, they have the most money, and yes, they dominate because of their economies of scale. But they also dominate because they’ve got the fastest to-market time of any company. They dominate because they’ve got the best chip developers in cryptocurrency. They dominate because they’ve innovated in dozens of places to squeeze costs and inefficiencies out of corners that most people aren’t aware exist. They hire the best people and pay them well. And they work hard to make sure that at every iteration, they are the ones on top.
There’s not a whole lot more to say here. I feel that a lot of people under-estimate Bitmain or assume that because they play dirty they wouldn’t be able to keep up without playing dirty. But that’s not true. They play dirty because it’s yet another place they can optimize their business, and because they know they can get away with it. Everything else they do is highly optimized as well. If we want to understand mining, we need to appreciate that the entity that controls most of mining today is an impressive, highly skilled, well refined entity.
The biggest takeaway from all of this is that mining is for big players. The more money you spend, the more of an advantage you have, and there’s not an easy way to change that equation. At least with traditional Nakamoto style consensus, a large entity that produces and controls most of the hashrate seems to be more or less the outcome, and at the very best you get into a situation where there are 2 or 3 major players that are all on similar footing. But I don’t think at any point in the next few decades will we see a situation where many manufacturing companies are all producing relatively competitive miners. Manufacturing just inherently leads to centralization, and it happens across many different vectors.
Though that’s discouraging news, it’s not the end of the world for Bitcoin or other Proof of Work based cryptocurrencies. Decentralization of hashrate is a good-to-have, but there are a large number of other incentives and mechanisms at play that keep monopoly manufacturers in line. A great example of this is the Bitcoin / Segwit2x situation. More than 80% of the hashrate was openly in support of activating Segwit2x, and yet the motion as a whole failed.
There are plenty of other tools available to cryptocurrency developers and communities as well to deal with a hostile hashrate base, including hardforks and community splits. The hashrate owners know this, and as a result they are careful not to do anything that would cause a revolt or threaten their healthy profit streams. And now that we know to expect a largely centralized hashrate, we can continue as developers and inventors to work on structures and schemes which are secure even when the hashrate is all pooled into a small number of places.
Decentralized storage — Sia, Skynet, and cryptocurrency.
24K 
117
24K claps
24K 
117
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by

Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://medium.com/crustnetwork/crust-and-subsocial-announce-partnership-4b0f66aca873?source=search_post---------282,
https://medium.com/net-core/using-google-cloud-storage-in-asp-net-core-74f9c5ee55f5?source=search_post---------26,"There are currently no responses for this story.
Be the first to respond.
In this post, I will show how to use Google Cloud Storage (GCS) to store and serve images of an ASP.NET Core web application.
The sections of this post will be as follows:
The application that we will modify manages a database of TV Shows and its main page looks like below:
The app is an ASP.NET Core MVC web application and it uses EF Core to perform CRUD operations on a SQL Server database. You can download the source code from this Github repository.
If you are interested in the development process of this application, you can check this post.
In the current version, the image of the TV show needs to exist in the wwwroot/images folder of the project and is entered as shown below:
In the modified version, the user will select the image file to be uploaded and the file will be uploaded to Google Cloud Storage in the create action.
Let’s start by performing the necessary actions in the Google Cloud Platform (GCP).
If you don’t have a Google Cloud account, go to Google Cloud web site and click Get started for free button. Sign up with your Gmail account and get $300 (for free) to spend on Google Cloud Platform over the next 12 months.
In this section, we will do the necessary operations to use Google Cloud Storage service of the GCP for our application.
Cloud Storage provides worldwide, highly durable object storage that scales to exabytes of data. You can access data instantly from any storage class, integrate storage into your applications with a single unified API, and easily optimize price and performance.
Cloud Storage is typically used to store unstructured data.
First, we will create a new GCP project in the Google Cloud Console. (You can use an existing project too). Select the dropdown box shown in the red box and then click New Project.
Give your project a name and then click Create.
Next, we will create a bucket to store our images.
Buckets are the basic containers that hold your data in Cloud Storage.
Write Storage in the search box and then click Storage.
Click Create Bucket on the next screen. Then,
After the creation is completed, we will see our bucket as follows:
Next, we will make the bucket publicly readable so it can serve files:
Now, we will set up a service account and download credentials to run our application locally.
Enter APIs & Services in the search box of the cloud console. Then,
In the next dialog, create the new service account as shown below:
After this operation, a new JSON file is created and we download this file to our computer to use in our project in the next section.
In this section, we will perform the modifications listed below in our application so that we will be able to upload the image of the TV show that will be used in the application:
Modify the model
Change the TvShow model as seen below:
We added two new properties to the model:
Next, we will run the following commands in the Package Manager Console to update the database.
Modify the views
In the Create.cshtml, add the parts shown in red boxes below and delete the part related to ImageUrl :
Modify the Edit.cshtml as shown below:
Next, we will add style attribute to the ImageUrl to make the height and width of image standard in the other views.
In the Delete.cshtml and Details.cshtml, change the part related to ImageUrl with the following:
In the Index.cshtml, change the part related to ImageUrl with the following:
Edit the application settings
Add the following key-value pairs to appsettings.json and change the values with the values of the credential file path and GCS bucket that you created in the previous section:
Install the Google Cloud Storage package
Right-click on the project and select Manage Nuget Packages… and then install the following package:
Create a new class for GCS operations
Now, we will create a new class that will manage the file upload and delete operations in our GCS bucket using the client library that we installed above.
Right-click on the project and create a new folder called CloudStorage. Then add the following ICloudStorage interface to this folder.
Next, add a class called GoogleCloudStorage which implements the ICloudStorage under this folder:
This class gets the credential JSON file name and the GCS bucket name from the application settings that we defined previously.
As seen in the implementation, it has two methods:
We will use these methods in the Create, Edit and Delete action methods of the TvShowsController.
Edit Startup.cs
Before using the GoogleCloudStorage class in the TvShowsController, we need to register it as a service in Startup.cs to the built-in IoC container as it will be injected to the controller via constructor injection.
Add the following line to the end of the ConfigureServices method:
Modify the controller
Now, we will modify Create, Edit and Delete action methods of the TvShowsController.
First, change the following part for the dependency injection:
Second, add the following new methods to the controller class:
FormFileName method creates a new object name in the format below:
Then, this new name is passed as a parameter to the UploadFile method and it is used as the object name in the GCS bucket. This object name will also be stored in the TvShow.ImageStorageName field and will be used to delete the object from the GCS bucket if needed.
UploadFile method calls the UploadFileAsync method that we defined in the GoogleCloudStorage class previously. Notice that after the file is uploaded to GCS, the public URL to this file is returned and we will use this URL to serve the file directly from GCS. So we store this value in the TvShow.ImageUrl field to show the poster in the views of the application.
Next, change the Create method as shown below:
As seen above, when we create a new TV Show, we also upload the image file (if the user chooses one) to the GCS using the UploadFile method.
Next, change Edit method as shown in the below image:
In the Edit method, we first check if there is a related poster for this TV Show and if it exists we delete that from the GCS. Then, we upload the new image to the GCS.
Lastly, add the following code to the DeleteConfirmed method:
You can find the last version of the project in this GitHub repository.
Now, we will test the application and see if everything works as expected.
First, let’s create a new TV Show:
As you see above, we can now choose the image file from our computer. After clicking the Create button, the new record including the fields related to the image is created in the database as shown below:
We can also check that the object is created in our bucket in the GCS:
Notice that the ImageStorageName in the table and the Name field in the GCS bucket match.
In the Index view of the application, the poster of the image is served from the GCS using the ImageUrl field in the table.
Next, we will test the Delete view:
After clicking the Delete button, if we refresh the GCS bucket we see that the image object is deleted here as well.
You can check the Edit view and see that everything works as expected.
To avoid incurring charges, we can delete our GCP project to stop billing for all the resources used within the project.
In the Google Cloud Console, search for Manage Resources. In this menu, select your project and click Delete.
That’s the end of the post. I hope you found this post helpful and easy to follow. If you have any questions and/or comments, please let me know in the responses section below.
And if you liked this post, please clap your hands 👏👏👏
Bye!
https://cloud.google.com/storage/
https://cloud.google.com/storage/docs/quickstart-console
https://cloud.google.com/appengine/docs/flexible/dotnet/using-cloud-storage
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
200 
3
200 claps
200 
3
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Written by
A software developer who loves learning new things and sharing these..
Posts related to .NET Core (ASP.NET Core, MVC, Web API…)
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@alibaba-cloud/cloud-storage-picks-with-alibaba-cloud-11c82bc73d97?source=search_post---------123,"Sign in
There are currently no responses for this story.
Be the first to respond.
Alibaba Cloud
Aug 4, 2020·7 min read
Storage is an essential cloud service replacing traditional on-premises storage hardware devices. Alibaba Cloud supplies out-of-the-box Cloud Disk storage options for products like Elastic Compute Service (ECS) and Container Service for Kubernetes (ACK). However, there is a wider range of Alibaba Cloud storage offerings to consider. In this blog, we are going to examine and demystify the products in the Alibaba Cloud Storage & CDN range.
First, we’ll look at cloud essential storage products such as Cloud Disk, Object Storage Service (OSS), and Apsara File Storage NAS. Then we’ll look at Alibaba Cloud’s enterprise specific products such as Hybrid Cloud Storage Array, Cloud Storage Gateway, Hybrid Backup Recovery, and petabyte level data migration service Data Transport which assists businesses migrating their data to the cloud.
Then we’ll check out the Content Delivery Network (CDN) product for efficient file serving across the global Internet. And the Dynamic route for CDN (DCDN) product, which is an add-on to CDN providing super-spec acceleration for dynamic content where necessary.
First, let’s look at Alibaba Cloud’s three standard data storage services: Block Storage (or Cloud Disk), Object Storage Service (OSS), and Apsara File Storage NAS.[1]
Block Storage (also known as Cloud Disk) ships with products like ECS instances and container services, and supports random and sequential read/write operations. Block Storage is available as a system or data disk depending on requirements and offers high-performance and low-latency, scalable, block-level storage.
Block Storage has two flavors: scalable cloud-distributed storage disks and local disks that attach onto the physical machine hosting the ECS instance. Both options support low latency, high availability, and durability but the local option services high performance business scenarios. The following table details the various disk storage options available with Block Storage.
Block Storage meets the data storage needs of most business requirements. However, if you need to store massive amounts of unstructured data such as images, audios, and videos, you should consider Alibaba Cloud’s Object Storage Service or Apsara File Service NAS depending on your requirements.
Alibaba Cloud’s Object Storage Service (OSS)[2] is a secure storage, backup, and archive service for storing massive amounts of unstructured data. OSS eliminates the need for user created server storage and is simple to use with easy development tools. OSS takes care of data storage for small ecommerce websites and vast enterprise infrastructures.
With a guaranteed durability of 99.999999999% and availability of 99.95%, its RESTful multi-language APIs offer storage and accessibility anywhere on the Internet. OSS has elastic scaling of both capacity and processing power, and has a wide variety of storage classes to optimize cost.[3] OSS plugs into many Alibaba Cloud products and services. For example, ECS, CDN, Elastic MapReduce, and the Alibaba Cloud Apsara range of products feature OSS as a corner stone in their designs.
The following diagram shows an example architecture using Alibaba Cloud OSS storage with the aim of optimizing data delivery depending on data popularity.
Optimizing data delivery with OSS
Alibaba Cloud Apsara Network Attached Storage (NAS) is a file storage service used with ECS, E-HPC, and container service products.[4] Like OSS, NAS stores massive amounts of unstructured data but also offers standard file access protocols such as the Network File System (NFS) protocol for Linux, and the Server Message Block (SMB) protocol for Windows.[5]
With 99.999999999% data reliability, no Single-Point-Of-Failure, and a guaranteed SLA of 99.9% availability, Apsara File Storage NAS is elastically scalable to petabytes. Supporting end-to-end permission management, NAS is suitable for file sharing tasks in industries such as radio, television, high-performance computing, and containerization. It is simple to use and cost effective.
The following diagram shows an example of a NAS distributed file storage environment that processes DNA sequencing.
DNA Sequencing with NAS
Alibaba Cloud’s HCSA is a managed global enterprise data product for businesses that have requirements for on-premises physical data centers but still wish to benefit from cloud services. HCSA integrates on-premises enterprise data centers with dedicated devices that hook up with Alibaba Cloud’s Storage Gateway for storage management.[6]
HCSA’s dedicated devices offer enterprise customers a simple deployment for scalable, secure, and compliant data configurations that support backup, archiving, disaster recovery, replication, deduplication, and compression ensuring cost-efficiency, reliability, and durability.[7] HCSA features include intelligent high-performance caching, cross-platform support, high-efficiency of data transport and validation, and robust logging and monitoring.
The following diagram shows the Hybrid Cloud Storage Array device connecting an on-premises, physical datacenter to Alibaba Cloud’s Storage Gateway offering a seamless plugin to the Alibaba Cloud ecosphere and beyond.
Enable Alibaba Cloud with HCSA and Storage Gateway
Alibaba Cloud Storage Gateway, mentioned above, deploys in enterprise data centers and on the cloud.[8] It uses OSS buckets for backend storage but also offers the same file protocols as NAS and including iSCSI.[9] Required by the Hybrid Cloud Storage Array product just mentioned, it provides seamless connectivity to on-premises enterprise data centers.
Businesses moving to Alibaba Cloud often need help migrating data. Alibaba Cloud’s Data Transport product offers both self-service and managed set ups for transferring petabyte levels of data from on-premises data centers to Alibaba Cloud storage services such as OSS.[10]
Supporting NAS, HDFS, and FastDFS, Data Transport is fully supported by Alibaba Cloud’s Resource Access Management (RAM) and uses specialized devices for massive encrypted data transfers that ensure maximum compression and deduplication efficiency with small file merge technology.
Transfer massive amounts of data with Data Transport
Alibaba Cloud’s Hybrid Backup Recovery service offers a fully managed data backup service for Block Storage, OSS, NAS, and on-premises data as well as data stored in VMWare virtual machines, SAP HANA instances, and SQL Server databases.[11]
With full and incremental backup, deduplication, and encryption features, the Hybrid Backup Recovery integrated platform is an easy to use, low cost, and highly efficient option for secure backup and disaster recovery across on-premises and cloud infrastructures.
Hybrid Backup Recovery to Alibaba Cloud
For content delivery, Alibaba Cloud offers clients a horizontally scalable, HTPPS secured, Content Delivery Network (CDN) product that eliminates network congestion across the Internet with more than 2800 nodes worldwide and 120Tbps bandwidth. With a background network of 10 Gigabyte NICs and SSD storage, response rate is less than a few milliseconds for super-powered low latencies.[12]
Common scenarios include live and on-demand streaming, mobile content delivery, downloading, and website acceleration.
Alibaba Cloud CDN for website data acceleration
Alibaba Cloud CDN is available with another product, Data Transfer Plan, which may significantly reduce costs for high traffic usage.
When your enterprise serves dynamic content to interactive users, Alibaba Cloud’s Dynamic Content Delivery Network (DCDN) is a good option.[13] An enhanced CDN service, DCDN ensures low latency, minimal packet loss, and insignificant service instability while accelerating static and dynamic content delivery across your sites.
The main differences between these two CDN services are shown in the table below.
We have examined the full Alibaba Cloud Storage & CDN range of products, with something for everyone; from basic in-house set ups to vast globally-distributed architectures.
Fully supporting clients throughout these challenging times,[14] and with a super reliable global infrastructure that is proving itself up to the task in international emergencies, is now the time to move your data storage and delivery requirements to Alibaba Cloud?
[1] https://partners-intl.aliyun.com/help/doc-detail/63136.htm[2] https://www.alibabacloud.com/product/oss[3] https://www.alibabacloud.com/help/doc-detail/31817.html[4] https://www.alibabacloud.com/product/nas[5] https://partners-intl.aliyun.com/help/doc-detail/63136.htm[6] https://www.alibabacloud.com/product/storage-array[7] https://www.alibabacloud.com/solutions/backup_archive[8] https://www.alibabacloud.com/products/cloud-storage-gateway[9] https://www.alibabacloud.com/help/doc-detail/53972.htm[10] https://www.alibabacloud.com/product/data-transport[11] https://www.alibabacloud.com/products/hybrid-backup-recovery[12] https://www.alibabacloud.com/product/cdn[13] https://www.alibabacloud.com/products/dcdn[14] https://www.alibabacloud.com/campaign/fight-coronavirus-covid-19
www.alibabacloud.com
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
See all (24)
2 
2 claps
2 
Follow me to keep abreast with the latest technology news, industry insights, and developer trends.
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@marckohlbrugge/how-to-backup-your-dvd-archive-to-the-cloud-6aa59cae66eb?source=search_post---------246,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marc Köhlbrugge
Apr 18, 2016·8 min read
I‘ve always been pretty obsessive about backing up my files and back in the day when HDD’s were expensive and cloud storage wasn’t a suitable option yet, burning my files to CD’s and later DVD’s was my strategy of choice.
Now, 10–15 years later, I still have this massive archive of DVD’s around and none of my computers actually have a DVD player anymore. I figured if I wanted long-term access to all these files locked away in that ancient medium I shouldn’t wait much longer re-digitizing them. I figured the best place to store them was in the cloud, but first I’d need to copy all the files to my computer. With 100+ DVD’s that might take a while, right?
Well, yes. It did take a while, but I was able to automate most of the process.
Eventually I got my workflow set up so that I simply inserted a DVD, my script would do its thing, and when every file was copied to my HDD the drive would eject itself and I’d get a desktop notification telling me whether the copy was successful or not. I then inserted the next DVD, and so forth.
Finally, I uploaded my files to cloud using Arq and Amazon Cloud Drive which is pretty straightforward. Just configure Arq to watch your folder of DVD files and it will do its thing all by itself.
There’s two parts to this workflow which I’ll share below.
The first part is actually copying the files to your HDD in a way that preserves things like the creation dates, file permissions, etc. Ideally it would also be quite flexible so for example if we accidentally run out of HDD space and need to re-import the same DVD we don’t want to start from scratch nor do we want to end up with duplicate files. We take care of all this using a relatively simple shell script.
The second part is automating the process of running that shell script everytime we insert a new DVD. The goal here is to do as little work as possible so we just want to take out a DVD and insert the next one without needing to click any buttons or manually run any scripts. Luckily Mac OS X has something built-in called launchd which is a magical thing that automates all sorts of things on your Mac.
I have a iMac and Macbook Pro, neither which have a built-in DVD player so I needed to buy an external one. Apple’s USB SuperDrive is $79 which is a bit steep considering it would be obsolete after I imported all my DVD’s.
After doing a little bit of research I realised there wasn’t much difference between all the USB DVD players on the market. They all got the same reading speed (“8x”) and cost around $30. I went with the LG GP57ES40. It’s actually a DVD writer, not just a reader. I don’t think they sell readers anymore.
I didn’t have much experience working with shell scripts, so I googled around and found an old Metafilter thread where people shared some of their scripts. I based my script on one created by ‘mrzarquon’ so credits to him! 🙌
Here’s what my script ended up looking like:
You can download it here.
To execute it on your Mac you’ll need to download the file, and then make it ‘executable’. This a security measure to make it just a little harder for someone to mess with your system. To make the script executable follow these steps:
In case you were wondering chmod stands for change mode and x means eXecutable. The plus signs simply means “whatever the current mode is, also make it executable.”
Now we’re almost ready to run the script, but you first need to make a few changes. You’ll need to specify the location of your DVD drive, and where you want to copy the files to. For this you’ll need to edit the script with a text editor. TextEdit works, just be careful not to make any mistakes editing or the script won’t work anymore.
Here’s what you’ll need to change:
Save you changes and return to the Terminal. You can simply drag the file to your Terminal and press Enter to execute it.
Of course, first make sure you have inserted a DVD 😁
You should now see your files being copied…
This might take a while depending on the amount and size of the files you’re importing. When it’s finished your DVD tray will eject and you should see a notification like this:
If the notification automatically disappears and you’d like for it to stay until manually closed you can go to System Preferences > Notifications > Script Editor and set it to Alerts.
If you’re curious to how the script works here’s what’s going on:
Okay sweet, so the script takes care of copying all the files and folders and even if you break off the process halfway through it will simply continue where it left off the next time you enter the same DVD. Nifty!
We don’t want to have to run the script everytime we insert a DVD though, do we? Nope, we don’t. So let’s automate this as well.
For this we’ll use launchctl which interacts with launchd.
We want to tell launchd to watch the /Volumes folder and run our script whenever something changes (like a DVD being mounted!). Since launchd doesn’t understand English yet we need to tell it using something it does understand called a Launch Agent. Here’s what it looks like:
The plist format is a little strange, but other than that it’s pretty straightforward. we set /Volumes as our WatchPaths which means launchd will run the script found in ProgramArguments whenever something changes in /Volumes
You’ll want to change all the paths to match yours!
You’ll also notice we set two log locations so we can still keep track of what’s happening as we won’t be opening the Terminal anymore and thus don’t get to see the script’s output. To watch these logs you can run tail -f /location/to/log in the Terminal or use the Console.app that comes with Mac OS X which basically does the same thing, but with a nicer interface.
To activate this launch agent follow these steps:
We use launchctl load to load the launch agent. You can use launchctl unload ~/Library/LaunchAgents/com.marckohlbrugge.batchImportDvds.plist to deactivate it again which you’ll want to do when you’re done importing all files.
Now whenever you insert a DVD it will run the script, which will copy all the files and eject the drive when done.
Finally, you probably want to move all these files off your hard drive to the cloud. I personally use Amazon Cloud Drive together with Arq which makes uploading and encrypting a breeze. You could also use something like Dropbox, but I like the combination of Amazon Cloud Drive and Arq because my files will be encrypted with Arq and there’s no storage limit with Amazon Cloud Drive.
If you have any questions or suggestions to improve the workflow please let me know. If you made it this far I’m guessing you enjoyed the article, or you just have way too much time on your hands, either way a 💚 would be appreciate it as it helps more people find this article.
@BetaList Founder
5 
2
5 
5 
2
@BetaList Founder
"
https://medium.com/@BradJonson/why-i-stopped-using-dropbox-and-google-drive-after-i-found-out-about-this-dc9029cde30e?source=search_post---------338,"Sign in
There are currently no responses for this story.
Be the first to respond.
Brad Johnson
Aug 23, 2019·5 min read
This easy-to-use tool uses military-grade encryption technology that makes it impossible for anyone to steal your most private information
According to a recent Gallup Poll, 71% of Americans fear losing their private information from a cyber-crime more than they are about getting their vehicle stolen, getting murdered, or being sexually assaulted… combined.
Many people who aren’t “tech savvy” feel an enormous amount of anxiety because they believe they are being watched, listened, and recorded on a 24-hour basis.
However, many Americans have made it their mission to keep every part of their life private and secure.
This includes videos of family, pictures of their children and special moments, and documents with lots of important information like tax records.
“I had trouble sleeping at night because I thought, ‘what if someone stole my private information and sold it on the ‘dark web’…?”
Everyone knows about storing their information on a “cloud” of some kind, but what they don’t know is how those “clouds” aren’t as secure as they had hoped…In 2012, Dropbox was notified that they’re system had been “hacked.” But they didn’t know how bad it was until 4 years later!
Turns out over 68 million users had their login data stolen and was being sold to the highest bidder on the “dark web.”
Can you imagine a stranger in another part of the world selling your most private information, such as login data or images of your family?
Anyway, you’re about to discover a new highly-secured cloud storage and file sharing application made from a Swiss company that is disrupting the data storage marketplace and has the potential of stopping bad guys in their tracks.
They created the only cloud storage and file sharing system that acts like a fortress for your private information.
Plus, pCloud has way more storage than leading competitors, is super easy to use (even a 9 year old can navigate it), and is more secure than Dropbox and Google Drive.
Discover why the world’s most “unhackable” cloud storage system is making people run from Dropbox and Google Drive. Click here to find out now!
Two Major Reasons:
Reason Number One: pCloud is the only cloud storage system that allows you to purchase a “lifetime plan” for one low price.
No monthly or yearly charges.
This allows you to automatically back-up all of your valuable information, images, music, videos, you name it… without taking up space on an external hard drive. (This is a massive savings.)
Reason Number Two: In today’s world where personal data is more valuable than the price of oil, pCloud takes their privacy seriously.
They are held accountable by Swiss laws — the most strict when it comes to personal data — and operate on the only “unhackable” encryption system in the world.
Normally, most “cloud” systems can still be accessed by the service provider, and are limited on their encryption.
Yes, even the most well-recognized cloud storage software can still be “decrypted” and private information shared in the blink of an eye…
pCloud uses a highly-secured private encryption called, “pCloud Crypto.” (not to be confused with “cryptocurrency”)
There are 3 parts to this state-of-the-art encryption system, which are:
1.) Client-Side Encryption. Only the client has access to his/her information. If a device gets stolen, the suspect won’t be able to access their information on the pCloud.
2.) Zero-Knowledge Privacy. No one can access your information, and that includes any authority or even the owners of pCloud.
3.) Multi-Layered Algorithm. Your information, including your password key, is buried underneath a large stack of algorithms that are impossible to hack.
Matter of fact, to prove their system can’t be hacked, pCloud offered $100,000 to anyone that can hack their system.
Over 2,800 participated in this challenge, including the brightest minds from Berkeley, Boston, and MIT.
And after 6 months… no one was able to succeed!
Like anyone claiming to be the “most secure” cloud out there, we questioned it…
So, we did what anyone that cares about the privacy would do, we tested the new cloud storage for ourselves…
A few of our production team members signed up, downloaded the app, and received their initial 10GB storage for free.
“Finally! This is the cloud storage for me because I’m not that great with computers.”
Step-by-step, pCloud walks you through how to upload and sync all of your information. And it back-ups your information every day automatically. It’s having security and efficiency without me thinking about it.
“I can access my files from anywhere.”
I’ve been using Google Drive for a long time. But I think I might be changing to pCloud. It’s way easier to navigate and it’s ridiculously fast! I’ll get lost sometimes in my Google drive and I don’t know how secure it is. But pCloud makes it easy and is very clear on their security. And it comes with a music player which is a bonus.
“I hated sharing files because it took me forever to figure it out. pCloud solves this for me…”
I sometimes like to record videos of my dogs and send it to my family. But every time I tried to send it through text, it wouldn’t work. So I had to figure out how to send a file with a link. It took forever and it would be more frustrating than it was worth.
pCloud makes it way easier. Now I can film a video and send it almost immediately. I love it.
“One payment up-front and that’s it.”
It’s nice that I don’t have to worry about an automatic monthly or yearly withdrawal. pCloud gives you up to 2TB of storage at one cost. Such a no-brainer.
If you’re ready to finally back-up your most precious moments on the only cloud that understands the real definition of privacy, then you can click here right now.
For a limited time, pCloud is giving 2TB of storage when they sign up for the lifetime plan (which saves you the most amount of money).
But this offer is very very limited…
So if you wait and think about it, you might be too late.
Hurry. Get the security you’ve been searching for and click the link below right now.
[Get Real Secure Cloud Storage Now]
Sources:
https://www.ntia.doc.gov/blog/2018/most-americans-continue-have-privacy-and-security-concerns-ntia-survey-finds
https://dma.org.uk/uploads/misc/5b0522b113a23-global-data-privacy-report---final-2_5b0522b11396e.pdf
https://news.gallup.com/poll/244676/cybercrimes-remain-worrisome-americans.aspx
https://www.washingtonpost.com/news/the-switch/wp/2016/09/07/hacked-dropbox-data-of-68-million-users-is-now-or-sale-on-the-dark-web/
https://secureswissdata.com/switzerland-privacy-data-protection-laws/
4
4
"
https://medium.com/obelisk-blog/the-state-of-cryptocurrency-mining-2d8521bd754a?source=search_post---------234,"There are currently no responses for this story.
Be the first to respond.
For those new to the blog, I am the lead developer of Sia, a blockchain based cloud storage platform. About a year ago, myself and some members of the Sia team started Obelisk, a cryptocurrency ASIC manufacturing company. Our first ASICs are going to ship in about 8 weeks, and our journey with Obelisk has given us a lot of insight into the world of cryptocurrency mining.
One of the reasons we started Obelisk was because we felt that coin devs in general had a very poor view into the mining world, and that the best way to understand it would be to get our hands dirty and bring a miner to market.
Since starting Obelisk, we’ve learned a lot about the mining space, as relevant to GPUs, to ASICs, to FPGAs, to ASIC resistance, mining farms, electricity, and to a whole host of other subjects that coin developers should be more aware of. We aren’t able to share everything that we know, but we’ve pulled together information on a set of key topics that I think will be helpful to cryptocurrency designers and other members of the cryptocurrency community.
We’ve been pessimistic on ASIC resistance for a long time, and our journey into the hardware world solidly confirmed our position. Hardware is extremely flexible. General purpose computational devices like CPUs, GPUs, and even DRAM all make substantial compromises to their true potential in order to be useful for general computation. For basic hardware development, most algorithms can see substantial optimization just by taking away all of that generality and focusing on one specific thing.
The vast majority of ASIC-resistant algorithms were designed by software engineers making assumptions about the limitations of custom hardware. These assumptions tend to be incorrect.
Equihash is perhaps the easiest target, as a lot of people were quite confident in the equihash algorithm, and we’ve been saying for close to a year that we know how to make very effective equihash ASICs.
The key is to make sorting memory. A lot of algorithm designers don’t seem to realize that in an ASIC, you can merge the computational and storage pieces of a chip. When a GPU does equihash computations, it has to go all the way out to off-chip memory, bring data to the computational cores, manipulate the data, and then send the altered data all the way back out to the off-chip memory.
For equihash, the manipulations that you need to make to the data are simple enough that you can just merge the memory and computation together, meaning that you can do most of your manipulating in-place, substantially reducing the amount of energy used to move data back and forth, and also substantially decreasing the amount of time between adjustments to the data. This greatly increases efficiency and speed.
Needless to say, we weren’t the least bit surprised when Bitmain released powerful ASICs for equihash. The Bitmain ASICs are actually substantially less performant (5x to 10x) than our own internal study suggested they would be. There could be many reasons for this, but overall we think that it’s pretty reasonable to assume that more powerful equihash ASICs will be released in the coming months.
We also had loose designs for ethash (Ethereum’s algorithm). Admittedly, ethash was not as easily amenable to ASICs as equihash, but as we’ve seen from products on the market today, you can still do well enough to obsolete GPUs. Ethash is by far the most ASIC resistant algorithm we’ve looked at, most of the others have shortcuts that are even more significant than the shortcuts you can take with equihash.
At the end of the day, you will always be able to create custom hardware that can outperform general purpose hardware. I can’t stress enough that everyone I’ve talked to in favor of ASIC resistance has consistently and substantially underestimated the flexibility that hardware engineers have to design around specific problems, even under a constrained budget. For any algorithm, there will always be a path that custom hardware engineers can take to beat out general purpose hardware. It’s a fundamental limitation of general purpose hardware.
A lot of people believe that computing is broken up into 3 categories: CPU, GPU, and ASIC. While those are the categories that are generally visible to the public, in the chip world there’s really only one type of chip: an ASIC. Internally, Nvidia, Intel, and other companies refer to their products as ASICs. The categories as known to the public are really a statement about how flexible the ASIC is.
I would like to use a 1 to 10 scale to measure flexibility. At one side, a ‘1’, we’ll put an Intel CPU. And at the other side, a ‘10’, we’ll put a bitcoin ASIC. Designers have the ability to create chips that fall anywhere on this scale. As you move from a ‘1’ to a ‘10’, you lose substantial flexibility, but gain substantial performance. You also decrease the amount of design and development effort required as you sacrifice flexibility. On this scale, a GPU is a ‘2’.
Generally speaking, we don’t see products developed that fall anywhere between a GPU and a fully inflexibile ASIC because typically by the time you’ve given up enough flexibility to move away from a GPU, you’ve only got a very specific application in mind and you are willing to sacrifice every last bit of flexibility to maximize performance. It’s also a lot less costly to design fully inflexible ASICs, which is another reason you don’t see too many things in the middle.
Two examples of products between a GPU and an ASIC would be the Baikal miners and the Google TPU. These are chips which can cover a flexible range of applications at performances which are substantially better than a GPU. The Baikal case specifically is interesting, because it’s good enough to obsolete GPUs for a large number of coins, all using the same basic chip. These chips appear to be flexible enough to follow hardforks as well.
The strategy of hardforking ASICs off of a network is going to lose potency the more it happens, because chip designers do have the ability to make chips that are flexible, anywhere from slightly flexible to highly flexible, with each piece of flexibility costing only a bit of performance. The Monero devs have committed to keeping the same general structure for the PoW algorithm, and because of that commitment we believe that you could make a Monero miner capable of surviving hard forks with less than a 5x hit to performance.
Equihash is an algorithm that has three parameters. Zcash mining happens with one particular choice for these parameters, and any naive hardfork from Zcash to drop ASICs would likely involve changing one or more of these parameters. We were able to come up with a basic architecture for equiahsh ASICs that would be able to successfully follow a hardfork that chose any set of parameters. Meaning, a basic hardfork tweaking the algorithm parameters would not be enough to disrupt our chip, a more fundamental change would be needed. Despite this flexibility, we believe our ASIC would be able to see massive speedups and efficiency gains over GPUs. We never found funding for the equihash ASICs, and as a result our designs ended up on the shelf.
The ultimate conclusion here once again wraps back to the capabilities of ASICs. I think there are a lot of people out there who do not realize that flexible ASICs are possible, and expected that routinely doing small hardforks to disrupt any ASICs on the network would be sufficient. It may be sufficient sometimes, but just as algorithms can attempt to be ASIC resistant, ASICs can attempt to be hardfork resistant, especially when the changes are more minor.
A few months ago, it was publicly exposed that ASICs had been developed in secret to mine Monero. My sources say that they had been mining on these secret ASICs since early 2017, and got almost a full year of secret mining in before discovery. The ROI on those secret ASICs was massive, and gave the group more than enough money to try again with other ASIC resistant coins.
It’s estimated that Monero’s secret ASICs made up more than 50% of the hashrate for almost a full year before discovery, and during that time, nobody noticed. During that time, a huge fraction of the Monero issuance was centralizing into the hands of a small group, and a 51% attack could have been executed at any time.
Monero’s hardfork appears to have been successful in shaking the ASICs. I don’t believe that the ASIC designers attempted to build flexibility into their ASICs, but now that Monero has announced a twice-annual PoW change, we may see another round of secret ASICs with more flexibility. The block reward for Monero is high enough that even if you think you have only a 30% chance of your ASIC surviving the PoW hardfork, it’s more than worthwhile to pursue a hardfork resistant ASIC.
My strong guess is that Monero is going to have another round of secret ASICs built, and that these ASICs will be more conservative and flexible, attempting to follow the hard forks that Monero puts out every 6 months.
We’ve heard rumors of plenty of other secret ASICs. People who own secret ASICs tend not to talk about them very much, but as of March 2018, we had heard of secret ASIC rumors specifically for both Equihash and Ethash, and then for many other smaller coins that don’t have any ASICs on them yet. We believe a full 3 different groups were actively mining on Zcash with different ASICs prior to the Bitmain Z9 announcement.
We know of mining farms that are willing to pay millions of dollars for exclusive access to designs for specific cryptocurrencies. Even low ranking cryptocurrencies have the potential to make millions in profits for someone with exclusive access to secret ASICs. As a result, an informal underground industry has been set up around secret mining. The heavy amount of secrecy involved means it’s disconnected and mostly operates off of rumor and previous relationships. But it’s nonetheless a very lucrative industry, and even when things happen like the Vertcoin hardfork, the setback to secret miners is dwarfed by the returns of the successes.
At this point, I think it’s safe to assume that every Proof-of-Work coin with a block reward of more than $20 million in the past year has at least one group of secret ASICs currently mining on it, or will have secret ASICs mining on it within a few months. The easiest way to detect this is GPU returns, however as ASICs continue to infiltrate every coin on the market, that will cease to be a reliable metric as there will be no GPU-only coin to use as a baseline, at least not one that is large enough to sustain all of the massive GPU farms that are out there.
The ASIC game has become such an advanced game because there is so much money on the table. Even small coins can be worth tens of millions of dollars, which is more than enough to justify a high-risk production run.
Manufacturers that sell ASICs to the public, like Bitmain, tend to be less exposed than consumers to things like ASIC hardforks. Using Sia as an example, we estimate it cost Bitmain less than $10 million to bring the A3 to market. Within 8 minutes of announcing the A3, Bitmain already had more than $20 million in sales for the hardware they spent $10 million designing and manufacturing. Before any of the miners had made any returns for customers, Bitmain had recovered their full initial investment and more.
In this case, a hardfork doesn’t hurt Bitmain. Bitmain made a profit off of Sia, and there’s nothing the developers can do about that. And it seems that was the case for the Monero miners that Bitmain announced as well. Bitmain didn’t even get to announce the miners until after Monero announced their hardfork, and still it seems that they sold enough obsolete hardware to customers to make back their costs and turn a hearty profit.
The mining game is weighted heavily in favor of the manufacturers. They get to control the hardware production, the supply, and they know more about the state of the industry than anyone else. The profitability of a miner largely depends on variables that the manufacturer controls without disclosure to anyone else.
In the case of Halong’s Decred miner, we saw them “sell out” of an unknown batch size of $10,000 miners. After that, it was observed that more than 50% of the mining rewards were collecting into a single address that was known to be associated with Halong, meaning that they did keep the majority of the hashrate and profits to themselves. Our investigation into the mining equipment strongly suggests to us that the total manufacturing cost of the equipment is less than $1,000, meaning that anyone who paid $10,000 for it was paying a massive profit premium to the manufacturer, giving them the ability to make 9 more units for themselves. Beyond this, the buyer has no idea how many were sold nor where the difficulty would be when the units shipped. The manufacturer does know whether or not the buyer is going to be able to make a return, but the buyer does not. The buyer is trusting the manufacturer entirely.
If a cryptocurrency like Sia has a monthly block reward of $10 million, and a batch of miners is expected to have a shelf life of $120 million, the most you would expect a company could make off of building miners is $120 million. But, manufacturers actually have a way to make substantially more than that.
In the case of Bitmain’s A3, a small batch of miners were sold to the public with a very fast shipping time, less than 10 days. Shortly afterwards, YouTube videos started circulating of people who had bought the miners and were legitimately making $800 per day off of their miner. This created a lot of mania around the A3, setting Bitmain up for a very successful batch 2.
While we don’t know exactly how many A3 units got sold, we suspect that the profit margins they made on their batch 2 sales are greater than the potential block reward from mining using the A3 units. That is to say, Bitmain sold over a hundred million dollars in mining rigs knowing that the block reward was not large enough for their customers to make back that money, even assuming free electricity. And this isn’t the first time, they pulled something similar with the Dash miners. We call it flooding, and it’s another example of the dangerous asymmetry that exists between manufacturers and customers.
At the end of the day, cryptocurrency miner manufacturers are selling money printing machines. A well-funded profit maximizing entity is only going to sell a money printing machine for more money than they expect they could get it to print themselves. The buyer needs to understand why the manufacturer is selling the units instead of keeping them for themselves.
There are a few reasons it would make sense for a manufacturer to sell a money printing machine rather than keep it. The first is capital — manufacturing is an expensive process with a lot of lead times. If the manufacturer doesn’t have enough money to build their own units, then it makes sense to sell the units instead, and use the money from sales for production. It boils down to the manufacturer selling future revenue to get revenue today, which is a very common transaction in the financial world.
Another reason the manufacturer may sell money printing machines instead of running them is the electricity costs of running them. If the manufacturer can only get a certain deal on electricity, then there may be someone else with cheaper electricity or better datacenters who would be willing to buy the units at a price that’s higher than what the manufacturer values them at. Most manufacturers however have access to good electricity deals, and unless you have some deal with free electricity or are otherwise running a cutting edge professional operation, you are not likely to do better than the manufacturer.
And finally, the manufacturer may have some other reason that they want money quickly instead of making a longer term investment into the hardware themselves. This is likely not the case in cryptocurrency mining though, because the shelf life of miners tends to be under two years, and to a business that’s not a long time at all to wait for healthy returns.
In the traditional chip development world, it takes about 2 years to go from launching a development effort to getting a chip out out the door. In the case of the Sia and Decred miners we built, it looks like we’re going to be at about 13 months total from project launch to product delivery. If we had to do the same thing again, I think we could do it in about 9 months.
A huge portion of the time spent is on full-custom routing for the chip. There’s a much faster development process called place-and-route which trims about 3 months off of the chip development time, but produces chips that are 2x-5x slower than what a full-custom team can produce. We think that if we used a place-and-route design methodology, we could get our product delivery timeline close to 6 months.
We believe it took Bitmain about 5 months to create the A3 miner, and we believe it took Halong about 9 months to create the B52 miner. We suspect both of these were completed using place-and-route methodologies, especially given the relatively poor performance of each.
Those are timelines for creating a chip from scratch. If the goal is to chase a hardfork, the timelines are a lot shorter. If you know in advance that you are going to need to redesign your chip, there are a lot of shortcuts you can take to reduce the overall time required to get to market. Changing a design to meet a tweak is going to take much less time than starting from scratch, a good team with a well-planned base architecture can probably complete designs in about 2 weeks. From there, with some help from a hot-lot, you can get a new set of chips in about 40 days. These then need to be packaged, which is going to take around a week, and then sent to the manufacturer for assembly. Finally, you have to get the units to a datacenter and start mining.
If you had all the wafers, parts, and everything reserved ahead of time, we believe that you could upgrade a chip to adapt to a hardfork and have miners mining on the new hashing algorithm in about 70 days, at least in theory. In practice, Bitmain would probably require 3–4 months to adapt an existing chip to a hard fork, and if they hadn’t reserved wafers in advance they’d be looking at 4–5 months. Any company that is not Bitmain can probably add another 30–60 days to these numbers.
Some people already understand the situation with economies of scale well. The more money you spend, the more effective each dollar is. This effect is maintained throughout every level of scale that I’ve been able to peer into, including scales where you are going from billions of dollars to tens of billions of dollars.
The most simple way that this manifests is in volume orders. If you order one hundred thousand heatsinks, you can get one price. If you order one million heatsinks, you get a better price. As you continue to scale up, the price falls off. This effect is in place for almost all parts in the hardware industry, and it happens because manufacturers get to the point where they can buy and dedicate equipment to your order, and then keep that equipment at 100% efficiency. As you scale up, you gain greater customization and specialization in addition to cost savings, meaning your products become more effective as well as cheaper.
At some point, it makes sense just to buy out all the capacity at the manufacturer. A huge component of manufacturing price is paying for equipment. Equipment that is idle 50% of the time is going to have 2x the effective per-part price than equipment that is in use 100% of the time. As you increase lead times and order volumes, you can start getting fully dedicated equipment running nonstop for you, which again pulls the price down substantially.
In a similar sort of turn, someone has to manufacture that equipment. If you scale up to the point where you are continuously ordering specific equipment, the manufacturer can start to dedicate pipelines to you and keep their own equipment operating 100% of the time, and so the equipment you use for manufacturing is getting cheaper now on top of being in use all the time.
And that’s just the beginning. At every step, each provider, manufacturer, etc. is going to be taking margins, typically somewhere around 30% depending on how commodity your orders are. If you have enough money, you can start to engage in vertical integration, cutting out the margins of your manufacturers by either buying them out or creating your own margin-free entity.
Hardware goes through a lot of steps. There’s the acquisition of raw materials like iron and oil, the refinement of these materials, and then they get manufactured into base parts that can be sold for more general products. Those base parts often have lead times that are 6+ months, which means that suppliers typically keep a huge number of them in stock, so that they can provide customers with parts on faster timelines. Every step typically introduces both a middleman and inefficiency, especially because each step is targeted towards general use parts instead of a specific product. If you have a specific product that has enough volume / scale to justify dedicated supply chain elements, you will shave costs, you will shave lead times, you will improve product quality and performance, and you will get ahead of what anyone without that type of scale can achieve.
To present a very rough number, it seems to me that every time you 10x the amount of money you are spending, you can save about 30% per part. That is to say, if you are spending $100 million on mining units, you might get units for $500 each. And if you are spending $1 billion on mining units, you can squeeze that price down to $350 per miner just by having more money to throw around. And then if you jump to $10 billion, your per-miner price might drop to $245. Your mining machines aren’t just getting cheaper though, they are also becoming more customized and higher performance. You don’t just build a dollar moat with scale, you also build a quality moat.
When we started Obelisk, we had numerous separate sources reach out and warn us that Bitmain plays dirty, and that if we try to manufacture in China, we will be stopped.
With that in mind, we brought up the issue to everyone we worked with, and proceeded cautiously with an American manufacturer that owned a facility in China. This was attractive because the prices were close to half of what we would have paid to manufacture in America, and manufacturing was going to be one of our largest expenses by far.
We did everything we could to keep the entity disconnected from Obelisk, and we hid the name of the manufacturer from our website or any public data, and we were very careful with who we gave the name of our manufacturer privately. We had a separate entity put in parts orders where we could.
After any reasonable timeframe to reach out to another manufacturer, after-hours on a Friday night, our manufacturer reached out to us and said with little warning or reasonable explanation that they would be unable to manufacture for us. Just as we had been warned, our attempt to manufacture in China had fallen flat on its face. This setback is estimated to have cost us somewhere north of $2 million.
We have absolutely no evidence that Bitmain was involved in any way. We’ve had other companies reach out to us and confirm that they’ve experienced similar things, but they too had no concrete evidence that Bitmain was involved in any way. I honestly was not sure whether to include this section in the blog post, because unlike most of the other things I’ve been saying, we really have nothing more than a bunch of warnings that ended up being correct to inform us.
But it’s well established in the industry that Bitmain plays dirty, and it’s been suggested to us from all sides that they have been and will continue making moves within our supply chain to ensure we can’t succeed, and that they do the same with all of their competitors.
Mining farms are perhaps the one area where manufacturers and economies of scale are not dominant. Good electricity deals tend to come in smaller packages, tend to be distributed around the world, and tend to be difficult to find and each involve unique circumstances. As such, it’s been difficult for larger companies to create a system for scooping up low cost electricity worldwide. Instead, the cheapest electricity and datacenters in the world tend to be held by smaller parties that don’t individually own all that much electricity or hashrate.
From what I’ve been able to dig up, the average professional mining farm is paying somewhere between 4 and 6 cents for electricity, and then another 3 to 6 cents for management and maintenance. A total cost of $50 per kilowatt per month is probably somewhere close to the median for large scale mining farms. As techniques improve and the industry grows, we expect this number to fall closer to $35 per kilowatt per month (including maintenance, land, taxes, etc.) throughout 2019 and 2020. We don’t believe that anyone paying more than $80 per kilowatt per month will be able to remain competitive unless the price of cryptocurrency continues to rise rapidly over the next year.
The top 20% of miners all seem to be below $35 per kilowatt per month already, from what we’ve been able to glean, and the top 5% seem to be below $20 per kilowatt per month. By my estimation, if the price of Bitcoin were to fall substantially, these mining operations would be able to stay in business and everyone paying $50 or more would be forced to shut down their facilities.
It’s really hard to know where Bitmain is at, but based on everything we’ve been seeing we estimate that Bitmain is somewhere around the $30 per kilowatt per month mark. That is, they are doing better than the median mining operation, but by no means are they in the elite tier.
Most mining startups seem incredibly focused around the chip itself. From what we’ve seen, the chip is really less than half the story. So, the chip is important (apologies for the title), but if all you’ve got is the best chip in the world, you aren’t going to be a competitive manufacturer.
As a miner, the goal at the end of the day is to do as many hashes as possible for as little money as possible. A faster chip means that you need to spend less money on chips to get hashrate. And a more energy efficient chip means you need to spend less money on electricity to get hashrate. But you aren’t just spending money on chips and electricity. You spend money on PCB, on controllers, on ports like ethernet ports, on power supplies and power management, on fans, on enclosures, on shelves in your datacenter, etc.
At the end of the day the chip is only a portion of the equation for mining successfully. If you aren’t thinking about the whole picture, you are going to end up with a chip that will lose you money. This is actually one of the things that killed Butterfly Labs (among many) — they designed a high performance chip that produced hundreds of watts of heat. By comparison, Bitmain chips are typically about six watts each. Where Bitmain is able to throw on a forest of fat heatsinks, Butterfly Labs had to struggle with expensive, cutting edge, unreliable cooling systems, and that ultimately meant their powerhouse chip was late to market and too expensive to operate.
People tend to under-estimate Bitmain. Yes, they have the most money, and yes, they dominate because of their economies of scale. But they also dominate because they’ve got the fastest to-market time of any company. They dominate because they’ve got the best chip developers in cryptocurrency. They dominate because they’ve innovated in dozens of places to squeeze costs and inefficiencies out of corners that most people aren’t aware exist. They hire the best people and pay them well. And they work hard to make sure that at every iteration, they are the ones on top.
There’s not a whole lot more to say here. I feel that a lot of people under-estimate Bitmain or assume that because they play dirty they wouldn’t be able to keep up without playing dirty. But that’s not true. They play dirty because it’s yet another place they can optimize their business, and because they know they can get away with it. Everything else they do is highly optimized as well. If we want to understand mining, we need to appreciate that the entity that controls most of mining today is an impressive, highly skilled, well refined entity.
The biggest takeaway from all of this is that mining is for big players. The more money you spend, the more of an advantage you have, and there’s not an easy way to change that equation. At least with traditional Nakamoto style consensus, a large entity that produces and controls most of the hashrate seems to be more or less the outcome, and at the very best you get into a situation where there are 2 or 3 major players that are all on similar footing. But I don’t think at any point in the next few decades will we see a situation where many manufacturing companies are all producing relatively competitive miners. Manufacturing just inherently leads to centralization, and it happens across many different vectors.
Though that’s discouraging news, it’s not the end of the world for Bitcoin or other Proof of Work based cryptocurrencies. Decentralization of hashrate is a good-to-have, but there are a large number of other incentives and mechanisms at play that keep monopoly manufacturers in line. A great example of this is the Bitcoin / Segwit2x situation. More than 80% of the hashrate was openly in support of activating Segwit2x, and yet the motion as a whole failed.
There are plenty of other tools available to cryptocurrency developers and communities as well to deal with a hostile hashrate base, including hardforks and community splits. The hashrate owners know this, and as a result they are careful not to do anything that would cause a revolt or threaten their healthy profit streams. And now that we know to expect a largely centralized hashrate, we can continue as developers and inventors to work on structures and schemes which are secure even when the hashrate is all pooled into a small number of places.
Originally published at blog.sia.tech on May 13, 2018.
Blog from the Obelisk team, producers of high-performance…
213 
213 claps
213 
Written by

Blog from the Obelisk team, producers of high-performance ASIC miners for cryptocurrency.
Written by

Blog from the Obelisk team, producers of high-performance ASIC miners for cryptocurrency.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://towardsdatascience.com/how-to-upload-data-to-google-bigquery-989dc7b92583?source=search_post---------226,"Sign in
There are currently no responses for this story.
Be the first to respond.
Marie Sharapa
Jul 23, 2020·7 min read
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@satinderp/mustbin-blends-the-best-of-messaging-with-secure-cloud-storage-d64eac8e175b?source=search_post---------135,"Sign in
There are currently no responses for this story.
Be the first to respond.
Satinder S. Panesar
Dec 23, 2015·3 min read
Mustbin has come a long way since it started off as a document and camera screen grab app. It has scaled up to include mobile messaging as part of its service offerings. What makes the news even more interesting is the slew of features that it has packed into the new app — Mustbin 2.0. It is promoting itself as encrypted messenger with a host of factors that make it difficult to turn away our attention from the app.
Mustbin 2.0’s Value Proposition
Given the sores of messenger apps already around, Mustbin 2.0 is positioning itself in the market a bit differently. It is dedicated solely to provide users a way to manage private message and photos. It not only offers total privacy but also has enhanced security in form of cloud services integration. It provides the below list of compelling factors to let you decide in favor of downloading this app –
While messaging started as one of Mustbin’s offerings only a year back, it has taken long strides in a short amount of time to incorporate these features that will be appealing to privacy conscious smartphone users. At that stage it has managed to raise $1.5 million in funding. Till date, it has raised $6 million in angel funding to help its endeavor to provide newer, better and creative solutions to the mobile app segment.
Interested in trying out the app? Download it from the below app store link and let us know your thoughts on what benefits you liked the best.
https://itunes.apple.com/us/app/mustbin/id725880116?ls=1&mt=8
play.google.com
Originally published at: MobDevApp
Entrepreneur, Web Development, Mobile app development, IoT, Augmented Reality, Virtual Reality
1 
1 
1 
Entrepreneur, Web Development, Mobile app development, IoT, Augmented Reality, Virtual Reality
"
https://medium.com/bb-tutorials-and-thoughts/how-to-host-a-vue-js-static-website-with-gcp-cloud-storage-b8e87b89e4de?source=search_post---------89,"There are currently no responses for this story.
Be the first to respond.
There are a number of ways you can build a website with Vue.js such as Java with Vue, NodeJS with Vue, NGINX serving Vue, etc. For the single-page applications, all you need to do is to load the initial index.html. Once you load the index.html the Vue library kicks in and do the rest of the…
"
https://medium.com/@apollofintech/apollo-2020-decentralized-cloud-a1e3ce293e41?source=search_post---------249,"Sign in
There are currently no responses for this story.
Be the first to respond.
Apollo Fintech
Aug 18, 2019·2 min read
Apollo 2020 is the foundation’s new roadmap. You can learn more via links below:
Apollo Cloud will be one of the first decentralized, near-limitless cloud storage systems. It will be affordable and practical.
The decentralized file storage system will allow anyone to earn fees by partitioning off hard drive space with one click. Users can safely store files of nearly unlimited size with the security and privacy of decentralization.
With traditional cloud storage such as Dropbox, users are forced to rely on a centralized company with strict terms and agreements, worldwide law enforcement compliance, KYC requirements, as well as centralized servers that are vulnerable to manipulation and access by staff and hackers.
Users’ personal information and sensitive files are in the hands of the company. Moreover, current decentralized options are severely limited by their corresponding blockchain. Size is limited and storage is inconsistent because as user-run nodes are removed, information is also removed. Additionally, cost is astronomically high compared to traditional methods, and impractical for multiple or large files.
Here are Apollo Cloud’s features:
Apollo (APL) all-in-one privacy currency combines features of mainstream cryptocurrencies in an unregulatable platform. With two-second block speed, APL is one of the fastest cryptos on Earth. “Apollonauts” use features such as Encrypted Messaging, Smart Contracts, Decentralized Exchange, Dapps, and Decentralized File Storage.
Learn more at www.apollocurrency.com
World-Shaping solutions for a global economy www.aplfintech.com
204 
204 
204 
World-Shaping solutions for a global economy www.aplfintech.com
"
https://medium.com/analytics-vidhya/spring-boot-with-firebase-storage-73f574af8c4?source=search_post---------240,"There are currently no responses for this story.
Be the first to respond.
Firebase provides a powerful, simple, and cost-effective object cloud storage service. Also, the Firebase SDK adds Google security to file uploads and downloads. The SDK can be used to store images, files, videos, and other user-generated content.
This article seeks to achieve the following:
"
https://medium.com/@Kontainers/the-days-of-doing-business-without-freighttech-are-numbered-6c12e0f5fb3a?source=search_post---------308,"Sign in
There are currently no responses for this story.
Be the first to respond.
Kontainers
Oct 4, 2018·3 min read
Aaron Levie, the Co-Founder and CEO of cloud storage specialists Box, sent out a very interesting tweet yesterday:
It’s interesting in two ways.
First, even the best can forget how to count (look at the tweet again).
Second, and much more importantly, his tweet calls out a pattern familiar to all of us in the business of transforming the way companies do business — larger companies cannot cope with the pace of change and constantly have to pay a premium to catch up.
At Kontainers, we are seeing this pattern repeated again in the Freight Industry — an industry facing once of the largest changes in decades to how business is done. This change is being led by the larger companies in the industry and, importantly, small and nimble digital startups.
In 2018 so far, four companies alone have raised $374 million, bringing their their combined value to over $759 million:
Flexport — $100m ($304m total)
Convoy — $185m ($265m total)
Freightos — $44m ($100m total)
Project 44 — $45m ($90m total)
Obviously there has been dozens more sizeable investments in the FreightTech space and these can be broken into two general categories: ‘Disruptors’ or ‘Enablers’.
Both businesses are focused on a technical, customer centric experience, but on a very high level, ‘Disruptors’ want to eat some of your market share and ‘Enablers’ are here to help guard it.
So, to reiterate from the graph above, to add this digital layer the modern customer is expecting, shipping brands now have three options:
The third option, to build in-house, is by far the most difficult in any industry.
A recent World Economic Forum Report showed that managers will spend up to $1.2 trillion in 2018 alone on efforts to digitally transform their companies, only about 1% will achieve transformation at scale.
Now, look at your company and think about where it fits into Aaron’s tweet.
Are you at stage 2? “Politely No, that technology would never work here”
In that case you may already be too late.
How long do you think it will get to stage 7? “Well this is compelling”
2019? 2020?
The longer you take to address the threat from the multi-billion dollars of investment going into digital transformations and FreighTech startups, the less likely you business will be able compete, adapt, or even survive.
Don’t be a number 3, don’t get left behind.
Kontainers is a customisable, client-facing, execution platform for Freight Forwarders, deployed in an unbeatable timeframe, and with a proven track record of multiple on-time deliveries for 4 of the top20 Shipping Brands
Click here to schedule a call with a member of the Kontainers Team
Making container shipping simple
See all (519)
Making container shipping simple
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/shecodeafrica/understanding-the-cloud-286d5630d2f?source=search_post---------369,"There are currently no responses for this story.
Be the first to respond.
While surfing the Internet or having a discussion with a technical colleague or friend, you may have at some point or the other heard about the word “Cloud / Cloud computing” and it’s association to storage, In your curious state, you try to find out it’s meaning but really all that Technical grammar in the explanation never gets to you
What Exactly Is The Cloud?
Your typical technical blog will describe it as “ a computing-infrastructure and software model for enabling ubiquitous access to shared pools of configurable resources (such as computer networks, servers, storage, applications and services),which can be rapidly provisioned with minimal management effort, often over the Internet.”
No, Really, What Exactly Is The Cloud?
To break all that down, you first need to understand the Cloud isn’t a physical thing (neither is it the cloud up in the sky) you get to use like a safe.
“The cloud” is simply all of the things you can access remotely on the internet as an individual. The saying “something’s being stored in the cloud” simply means it’s being stored on a server or multiple servers built by another individual in a particular location rather than your normal PC’s hard drive or Phone’s storage.
We make use of the cloud in some of our daily activities but have little or no idea we’re really doing that. Here are a few examples of how we use the cloud:
Some of the popular cloud storage services available used to host data online include: Google Cloud, Amazon web services(AWS) owned by Amazon which has been around since 2006 and is used by a host of popular sites including Instagram and Pinterest
How Big Is The Cloud?
Unlike our local storage on our PC’s or Phones which have a storage limit, the storage limit for a particular cloud service is limitless, no one knows the amount of space that can be created but according to an infographic, the cloud can store up to 1 Exabyte
How Safe Is The Cloud?
With growing concerns in personal information being compromised, many individuals get worried about making use of them, but Some Companies like Google are tackling that with their data encryption for paid cloud storage users.
Benefits Of working with The Cloud?
There are immense benefits associated with working with the cloud, the biggest being how it allows you store and access data(large or small) remotely from any location without having to worry about storage space. It also allows large companies run efficiently and cost-effective while scaling up or down on data size:
We celebrate and empower women in Technology across Africa…
95 
95 claps
95 
We celebrate and empower women in Technology across Africa by telling their stories, empowering them through technical programs & activities while also helping them share their knowledge and ideas through articles.
Written by
Software Developer | Developer Relations and community Expert | DEI Consultant
We celebrate and empower women in Technology across Africa by telling their stories, empowering them through technical programs & activities while also helping them share their knowledge and ideas through articles.
"
https://medium.com/crustnetwork/crust-network-partners-with-automata-for-decentralized-cloud-storage-345558485d7e?source=search_post---------114,"There are currently no responses for this story.
Be the first to respond.
We’re excited to announce a partnership between Automata and Crust Network, the decentralized storage network for Web3. The collaboration will support the storage solutions and introduce Witness to the network.
Witness is an off-chain governance tooling solution for uncompromised privacy, at the same time also offering on-chain execution via Chainhook. The service has recently been launched on Ethereum and Binance Smart Chain, with Chainhook ready to use as a preview feature on the respective Testnets.
“We’ve been encouraged by the recent momentum that Witness has been gaining, which buoys expectation around how privacy is once again coming to the forefront for many blockchain projects. ”
- Deli Gong, Co-Founder of Automata Network
The open-source nature of blockchain has created collaborative communities who come together to discuss, debate and ultimately decide on a future they believe in together. With the opening up of community interest and engagement in increasingly granular ways, blockchain technologies will have to not only empower but protect users in the toss up between a transparent and private experience.
Automata Network is a decentralized service protocol that provides middleware-like services for dApps on Ethereum and Polkadot to achieve privacy, high assurance and frictionless computation.
Website | Telegram | Discord | Twitter | Github
Crust implements the incentive layer protocol for decentralized storage. It is adaptable to multiple storage layer protocols such as IPFS, and provides support for the application layer. Crust’s architecture also has the capability of supporting a decentralized computing layer and building a decentralized cloud ecosystem.
Website | Telegram | Discord | Twitter | Github
Decentralized Cloud Blockchain Technology
17 
17 claps
17 
Decentralized Cloud Blockchain Technology
Written by
CRUST implements the incentive layer protocol for decentralized storage, and vision to building a decentralized cloud ecosystem.
Decentralized Cloud Blockchain Technology
"
https://medium.com/meet-lima/the-3-2-1-backup-rule-d62b78f94238?source=search_post---------376,"There are currently no responses for this story.
Be the first to respond.
Losing data is one of the worst nightmare. We know! And it doesn’t take much to happen. Here is a hint on the best practices to manage data the best way possible.
At least. You should have the original files along with two copies of those files.
If you keep two copies of your data on the same storage device, you’ll lose everything at once if something happens to this device. For example, if you store your photos on your laptop, get an external hard drive to store a copy of your precious photos. It could be a Cloud storage as well: your photos will then be stored on a server. Your choice about the combination!
If your original files are stored on your computer at home, one solution could be to store one copie on an external hard drive at home, and the other on a Cloud service. If you do not feel confortable with having your data stored on a Cloud service, you could also store the third copy on another external hard drive that you’ll keep at your workplace or at a friend’s place. Offsite storage is life saving if something like a fire or flooding happens in your home.
Your data is precious and we, at Lima, want to make sure you keep it stored and secure. With the backup plan, your data will sync automatically between two or three (or more) Lima devices. You can set up one Lima at work or at a friend’s place to have one copy of your file offsite. With Lima, you get the 3–2–1 backup rule right!
The first Cloud that respects your privacy.
35 
35 claps
35 
Written by
Access your files from all your devices. Keep those files safe at home. Just the way it should be.
The first Cloud that respects your privacy.
Written by
Access your files from all your devices. Keep those files safe at home. Just the way it should be.
The first Cloud that respects your privacy.
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@mkasanm/microsoft-adds-third-party-cloud-storage-options-for-office-online-and-office-mobile-77b2d4bc494c?source=search_post---------136,"Sign in
There are currently no responses for this story.
Be the first to respond.
Markus Kasanmascheff
Jan 28, 2016·1 min read
Microsoft allows third-party cloud storage vendors such as Dropbox, Box and others to integrate their services directly with Office Online, Office for iOS and the new Outlook.
http://www.winbuzzer.com/2016/01/28/microsoft-adds-third-party-cloud-storage-options-for-office-online-and-office-mobile-xcxwbn/
Windows Expert Covering the #Microsoft-universe, #BJJ-player and M.A. in Economics. @HootsuiteES Ambassador Personal account, my opinions only.
1 
1 
1 
Windows Expert Covering the #Microsoft-universe, #BJJ-player and M.A. in Economics. @HootsuiteES Ambassador Personal account, my opinions only.
"
https://medium.com/google-cloud/automating-cloud-storage-data-classification-setup-cloud-storage-and-pub-sub-8cacfcf8ba14?source=search_post---------62,"There are currently no responses for this story.
Be the first to respond.
Authors: Priyanka Vergadia, Jenny Brown
“Get Cooking in Cloud” is a blog and video series to help enterprises and developers build business solutions on Google Cloud. In this series we plan on identifying specific topics that developers are looking to architect on Google cloud. Once identified we create a mini series on that topic.
In this miniseries, we will go over the automation of data classification in Google Cloud Storage, for security and organizational purposes.
In this article we will dive deeper into creating the different Cloud Storage Buckets we need and set up Pub/Sub.
How to automate the upload and classification of data with Google Cloud Storage.
Dinner Winner is an application that collects recipe submissions from users all over the world, and posts regular winning recipes for everyone to enjoy.
The recipe submissions need to be assessed for clarity and scrubbed for any identifiable information, before they are sent on to judging. And after the anonymous judging takes place, winners are contacted, and their recipes are posted to the application!
Based on their architecture, we’ve identified the primary issues as security and cross-contamination related, as well as huge efficiency gaps. The current, manual process will inevitably lead to system overwhelm and lapses in quality, so we’ve decided to seek automation as our main fix.
In this blog, we’ll go over steps 1 and 2.
Before we can get started on creating buckets, we need to set up our environment:
If you’re looking to classify data in an automated fashion, now you know how to do it. Stay tuned for more articles in the Get Cooking in Cloud series and checkout the references below for more details.
Google Cloud community articles and blogs
124 
124 claps
124 
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
Written by
Developer Advocate @Google, Artist & Traveler! Twitter @pvergadia
A collection of technical articles and blogs published or curated by Google Cloud Developer Advocates. The views expressed are those of the authors and don't necessarily reflect those of Google.
"
https://blog.sia.tech/what-sia-might-become-b033464a7a5a?source=search_post---------256,"For those who are new, Sia is an attempt at a decentralized cloud storage platform. By using encryption, cryptographic contracts, and redundancy, Sia enables a collection of untrusted and unknown machines to be combined into a single logical cloud storage host that is faster, cheaper, and more reliable than traditional cloud storage platforms. And because the untrusted nodes are located all over the world, Sia can become an effective content distribution network at no additional cost. The uploader is at liberty to select the nodes they use, which means they can avoid nodes inside of certain legal jurisdictions, or exclusively use nodes that have public reputation. The decentralization portion comes from splitting the file into many erasure coded pieces and giving each piece to a different host. Out of the hosts holding the file, only a small percentage actually need to be trustworthy for the file to be both safe and accessible.
Files on the Sia network are represented as ‘.sia’ objects, small objects which contain a list of encryption keys and hosts. A ‘.sia’ object is essentially a pointer to a larger file on the network. The current ‘.sia’ objects are between 2kb and 20kb, but there is room for substantial improvement. There are two tricks that can be played to reduce the size of a ‘.sia’ object. The first is that you can create a smaller ‘.sia’ object which points to a larger (or even multiple larger) ‘.sia’ object(s). You essentially end up storing large ‘.sia’ objects on the Sia network, and then do a two-stage lookup when fetching files. The second trick involves formalizing a protocol for deterministically generating the smaller ‘.sia’ files when starting from a random seed. By doing this, we can actually reduce the size of an initial ‘.sia’ object to that of a single hash, and then that hash could potentially point to any object or collection of objects. It becomes the ultimate way to store objects on the Internet. Sharing a file with a friend is as simple as uploading the file to Sia, and then giving your friend the 32-byte (or even a minified version at 16 or even 8 bytes) ‘.sia’ object that will allow your friend to locate the object on the Sia network. In a more interesting example, you could store something like a VM image. You could provide the tiny ‘.sia’ input to the VM host and it could use that to load an entire operating system snapshot from the Sia network.
One of the greatest advancements of Sia is the introduction of a free market for cloud storage. Today, being a cloud storage provider means establishing a brand, establishing trust, having customer service and support, and often requires establishing an entire ecosystem. Sia eliminates all of that overhead for storage and bandwidth providers. If you have a hard drive and an internet connection, you just plug your computer in and you can start accepting storage contracts. People don’t need to know who you are or trust you, and you don’t have to deal with any PR or customer services issues. Similar to Bitcoin mining, you simply plug in your machine and it starts generating income. People with access to cheap resources can turn large profits by providing those cheap resources to the rest of the world. There is no vendor lock-in on Sia, no privacy policies (privacy is complete and automatic), just a pure market for storage and bandwidth. We at Nebulous believe that this will cause the price of each resource to plummet. This is already visible on the beta network, where storage is virtually free. (The current price, even at the standard 8x redundancy, is about 3% of the cost of equivalent traditional cloud storage services).
Content distribution services (think Netflix, Spotify, or even YouTube) could benefit by using Sia as a content distribution network. Today’s content distribution networks are expensive and heavily redundant, and are run on many servers that all need to be controlled by a single central party. Sia is able to outsource nodes by paying untrusted entities and then using cryptographic contracts to enforce storage and distribution agreements. When using Sia, the content distribution network is actually built in. This can also provide a layer of protection to controversial services. For a while, Comcast was throttling Netflix traffic. All Sia traffic would look the same, and Comcast would have great difficulties singling out a service for throttling. Sia’s free market model also means that distribution costs are likely to decrease substantially for services that are bandwidth intensive, as cheaper nodes will be heavily prioritized when streaming, meaning more expensive nodes may not get any traffic at all (thus applying downwards price pressure).
The promise of gigabit internet adds some interesting possibilities. A gigabit internet connection is as fast as an SSD. With Google Fiber or some equivalent service, your computer could be just as fast without a hard drive (but downloading everything from the Internet) as it would be with a hard drive. You could potentially store your entire computer on Sia and nowhere else at all, without seeing an impact to performance. This means that you could boot your personal OS and settings from any computer in the world, and you wouldn’t even need a USB stick, CD, or anything more than a simple Internet connection. This is not likely to work on a traditional operating system because of extreme latencies, but if you set up a ramdisk OS (the whole OS stays in ram) that occasionally synchronizes to a cloud storage service, speeds would not suffer without a hard drive.
One of the potential promises of Bitcoin is the elimination of advertising from the Internet, replacing everything instead with content paywalls. Bitcoin enables paywall infrastructure that is simpler and smoother than any previously existing paywall infrastrucutre, enabling people to visit arbitrary websites and pay tenths of a cent for their time there, then never visiting the website again. Using Bitcoin, this can be done in a decentralized way such that neither the website nor the visitor need to be a part of the same payment service — they just each need to be part of the same decentralized payment service network. The setup time is potentially very minimal. A huge problem here though is transitioning from something that used to be free to something that now costs money. Though you have gotten rid of ads, it means shelling out actual dollars, and this causes a huge psychological barrier even if the payments only amount to a few cents per month. Human psychology does not like making lots of small payments.
The Sia ecosystem is actually forced into this model from the getgo. There is no free storage on the decentralized network, and no ‘trial download’ you can make that will give you 5GB for free, regardless of how financially inexpensive it may be. Small pockets of free storage are simply incompatible with the protocol. Storage on Sia is a metered resource. However, there are great examples of successful metered resources in our society today. The first major example is a use-now pay-later model. This is the model followed by your utility company. Every time you turn on the lights, every time you shower, every time you run the dishwasher, your utility companies are charging you small amounts of money. Over the course of a month, this adds up to a significant sum of money but you are used to it, and in general you know when the bill is going to be expensive. You happily pay your utility bills at the end of the month. The second example is pay-now use-later. Filling up a car with gasoline is a great example of this. Every time you get in the car, you know that you are going to be using some gasoline and that you’ll eventually need a refill, and that the refill is going to be a large sum of money. But you happily drive around anyway, because you know that the expense of spending money on gasoline is worth driving around.
The key in each of these scenarios is that you aren’t actually paying except every once in a while. You are aware that its costing you money each time it happens, but you only have to pay at the end of the month. When resources are tight, you can limit your expenses and avoid surprise heavy bills. These models of paying for things work well with human psychology. Sia has chosen to follow the gasoline model of resource consumption. When you start using Sia, you’re going to ‘fill up’ your account with some resources, and you’ll be given an estimation of how far these resources are going to stretch. When you start to run out of resources, you’ll get a ‘low fuel’ warning indicating you need to fill up again. Because of the payment channel network that will be set up on Sia, downloading from unknown servers and parties does not require any setup time. As long as both the uploader and the host are a part of the same global payment channel network (composed of many mutually distrusting parties), they can do instant and fully secure monetary transactions with eachother. This facilitates both uploading and downloading.
It’s entirely possible that this could also facilitate web browsing as a greater experience. Sia’s payment channel network is not limited to decentralized storage, and content downloads don’t need to come directly from Sia’s decentralized network. Using Sia, you could visit and manage paywalls on centralized websites, eliminate the need for ads and giving web hosts a more wholesome source of income.
Today, Sia’s core focus is decentralized storage, and more specifically a decentralized object store. We are focused on turning Sia into a simple, efficient, and secure platform where you can place and fetch data of any kind, and store a lookup value that’s nothing more than a hash. Uploading should be simple, keeping files safe should be simple, and moving files to other computers (either your own, or sharing with friends) should be simple. Finding files that you need should be simple, and most importantly, there should never be any question whether a file you put onto the Sia network still exists. The platform is still in beta, but is making great strides forward. We’re excited for the future, and we hope that you are too.
Sia, by Nebulous Inc., is a blockchain-based decentralized cloud storage platform.
Decentralized storage — Sia, Skynet, and cryptocurrency.
26 
26 claps
26 
Written by
Decentralized cloud storage. Store your data securely in the cloud without the need to trust any central service. Download at: https://sia.tech/
Decentralized storage — Sia, Skynet, and cryptocurrency.
Written by
Decentralized cloud storage. Store your data securely in the cloud without the need to trust any central service. Download at: https://sia.tech/
Decentralized storage — Sia, Skynet, and cryptocurrency.
"
https://blog.wetrust.io/an-interview-with-shawn-wilkison-ceo-cto-of-storj-e7dbab8fb7f5?source=search_post---------238,"Shawn Wilkinson is the creator of Storj, an open source cloud storage platform that can’t be censored, monitored, or have downtime. It is the first decentralized, end-to-end encrypted cloud storage that uses blockchain technology and cryptography to secure data. He is also the Founder, CEO and CTO of Storj Labs Inc, a corporate entity aiming to bring decentralized cloud storage to the average business and consumer.
Hi Shawn, thanks for chatting with us! How did you first become involved with blockchain?
I started getting involved with blockchain around 2012. A friend of mine was mining, and I thought hey, I’ll try it out. I mined about $50 worth of Bitcoin, and was around for the first large Bitcoin bubble. I thought “whoa, this is pretty cool,” started digging into it more and fell in love with the technology, started seeing uses for it outside of just being a currency.
Can you tell us a bit about how Storj got started?
I started Storj to provide crowdsourced p2p data storage. Most data out there is centralized, and by decentralizing it, I wanted to provide a similar service that was faster, cheaper and more secure — without forcing the users to change their habits too much. Storj was started as an open source project in 2014, and we launched with a crowdsale. The Storj tokens we released allow people to use space on the network. We raised half a million dollars through the token crowdsale, and in 2015 we started Storj Labs Inc, a company built on top of the Storj platform that provides additional support and services. You can think of Storj as being like Wordpress.org, and Storj Labs as being like Wordpress.com, providing additional services that make it easy for people to use the platform.
How have token crowdsales evolved since you started Storj?
Crowdsales, were definitely rougher in the early days! We launched our crowdsale at the same time as Ethereum — in fact, I contributed to the Ethereum crowdsale and Vitalik contributed to Storj. These days, crowdsales are more established, and there’s a better setup around them. We’re seeing crowdsales raising 5 million dollars in 30 minutes, which really hasn’t been done before.
What are your thoughts on the current state of the blockchain space? Aside from Storj, which projects are particularly exciting to you?
The blockchain space is still very experimental, but it’s a rapidly growing space. I like that there is a virtuous cycle and relationship between the growth of the ecosystem, the value of the tokens, incentives for new participants, and how the value goes right back into the ecosystem building new and exciting things. Projects such as WeTrust, Ethereum, Golem, and others are promising in the positive impact they can make.
How do you think the people working with blockchain can drive greater acceptance of cryptocurrencies among the “general public”, people who aren’t intimately familiar with blockchain technology?
We at Storj believe that blockchain and cryptocurrencies can only achieve acceptable among the “general public” though abstraction. You don’t need to know about HTTP/TCP/UDP or any of the foundational internet protocols to use Facebook or Twitter. Same thing should go for blockchain and cryptocurrencies. You should be able to use the technology to store data (what we offer), or transfer money without having to have a deep understanding of the technology.
How did you learn about WeTrust? Anything in particular that excites you about the project?
I met George at a fintech conference in Atlanta, but even before that I had stumbled across it at some point. WeTrust is an interesting concept, and it seems well polished compared to some other projects. I like that there’s already something there you can use.
Any thoughts on how the blockchain community can incentivize people to create more applications / dApps targeting social good?
I think the current ecosystem of tokens and coins already provide a method for incentives. Tokens like Zcash promote privacy, which I personal believe is a form of social good. Others, like Ethereum, provide a platform on which you can build projects more focused on your more traditional social good like WeTrust.
Thanks for chatting with us, Shawn!
To learn more about the WeTrust project, join our Slack!
To learn more about Storj’s innovative approach to decentralizing the cloud, click here!
WeTrust is on a mission to build and enable decentralized…
9 
9 claps
9 
Written by
Product Marketing @ WeTrust
WeTrust is on a mission to build and enable decentralized financial technologies that have a positive financial and social impact on the world.
Written by
Product Marketing @ WeTrust
WeTrust is on a mission to build and enable decentralized financial technologies that have a positive financial and social impact on the world.
"
https://medium.com/@duhroach/google-cloud-storage-performance-4cfcec8bad72?source=search_post---------51,"Sign in
There are currently no responses for this story.
Be the first to respond.
Colt McAnlis
Oct 5, 2017·4 min read
Google cloud storage is listed as a “a unified object storage solution” … or in layman’s terms, it’s a place in the cloud to host and serve files.
The uses for GCS are pretty large. For example, some people host their static websites there, some ship game content from there, others have used it to power their back-up services, and let’s not forget, it can host and serve content just like a CDN.
That’s all fine and dandy, but what I’m concerned with is : how would you test performance of it?
As we looked at in previous posts, it’s really easy to just throw a curl command around to see how GCS performs when fetching an asset.
First, place an asset on a server, say, in North America, (maybe near a BBQ restaurant) and then fetch that asset from a bunch of machines located around the world.
for ((i=0;i<500;i++)); do (time curl http://examplecdn.cdn/asset.png) 2>&1; done | grep real
The graph below shows our asset being fetched from a machine in Asia, Australia and Europe.
Note : Caching was not enabled for these assets, so their numbers are higher than what you’d see from the CDN article. AND this bucket was intentionally set to Regional (instead of multiregional) We can talk about the perf differences, but that a topic for another article.
Now this bucket was set up to be Regional, which means it doesn’t scale properly to being accessed across regions, so you can expect that the farther away the request is, the slower the response can be.
But we’ve seen read performance be really strong with GCS in the past; Write performance, however, is much trickier to get correct. In some following articles, I’ll highlight a few gotchas that some developer friends ran into, but for now, let’s just upload the same file, under different names, to the US-CENTRAL-1 bucket from various places in the world.
Now, if you run this test yourself, be careful about how you do your timings. There’s two main ways to do upload data to GCS: via `gsutil cp` and via a language-specific API. I’ve charted the performance of both below.
Note : These tests were done on a bucket was intentionally set to Regional (instead of multiregional) We can talk about the perf differences, but that’s another article.
We can see that write performance, for a 600k file is really good over time, however we see a significant difference between the GSUTIL and Python versions. The reason for this is that our scripting process for GSUTIL will cause a new process to be created for each uploaded asset, which isn’t ideal in terms of processor usage, and may not directly be GSUTIL’s fault.
Note : There’s also a multithreaded upload flag, but we’ll talk about that in a future article.
Direct read/write performance is easy to test. But in honesty, there’s lots of variables here that influence performance, that’s a pain to write proper benchmarking for. As we will talk about later, performance can greatly vary depending on the size of your objects, or if you’re creating, deleting, uploading or downloading. Thankfully, you don’t have to write all those tests yourself. The gsutil perfdiag command runs a suite of diagnostic tests for a given Google Storage bucket:
Simply running `gsutil perfdiag -o test.json gs://<your bucket name>` can get you something that looks like this :
The perfdiag command is provided so that customers can run a known measurement suite when troubleshooting performance problems.
Now that we have a clear picture of the proper way to test GCS read/write performance, we can get on with helping some of our developers and their performance problems. Stay tuned to Google Cloud Performance Atlas (YT Video, Medium Blog), and subscribe / follow to get more great content!
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
See all (78)
103 
103 claps
103 
DA @ Google; http://goo.gl/bbPefY | http://goo.gl/xZ4fE7 | https://goo.gl/RGsQlF | http://goo.gl/4ZJkY1 | http://goo.gl/qR5WI1
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/chainsafe-systems/chainsafe-files-encrypted-sharing-ff2967edbb01?source=search_post---------254,"There are currently no responses for this story.
Be the first to respond.
ChainSafe Files provides highly private, highly secure cloud storage that allows users to maintain full sovereignty over their data. Our product leverages IPFS and Filecoin to give users all the benefits of distributed storage and blockchain technology. Files abstracts away the complexities of these systems to deliver a simple, user-friendly experience. In this way, users can enjoy easy access to our data storage solution that is more private, more resilient, and more decentralized than traditional cloud storage options. Try it today here!
ChainSafe Files’ latest feature release is sharing! Through hours of shadowy super coding, the Files team has developed and enabled end-to-end encrypted file sharing between existing users. This means you can share any of your uploaded files either by username, sharing key or anonymously via a Web 3.0 wallet! This opens up a lot of powerful use cases for the application, without compromising on our core promises of privacy, security and data sovereignty.
In addition to sharing files with one another, users can now control permissions settings for their shared folders. The new feature includes options to give view-only permission and edit permissions for shared folders, all through an intuitive UI.
Users can create a shared folder with any other existing users. In order to maintain end-to-end encryption, a unique encryption key is generated for each shared folder. To create a shared folder, head over to the “Shared” menu on the left hand bar and click “Create Shared Folder.” From here, you can name the folder and add other users to it. Users can be added to the share with either “edit” or “read only” permissions. Editors can upload and reorganize folder contents, while view-only users can view the folder’s contents but cannot alter them.
In the “Create Shared Folder” modal, users can be searched for and added using either their username, Ethereum address or sharing key. Your sharing key can be found in the settings tab, where you can also set your public username. If you would like to share via Ethereum address, it will be the same as the one you use to sign in.
Adding someone to a shared folder will allow them to decrypt the entire contents of that folder. Make sure you are willing to share all the files in a folder before sharing it with someone else. If you would like to share various files with different individuals, simply create specific folders for each distinct share. The folder structure is what allows us to maintain private and distinct encryption keys so if you are sharing a single file, it should be done by creating a shared folder.
Sharing on ChainSafe Files comes with full access control. If you would like to revoke someone’s access to a shared folder, click “Manage Access” and remove the desired user. You can change a user’s access level by removing them from one access list, and adding them to another. As the owner, you can also delete files from a shared folder. Deleted files will no longer be accessible to anyone else with access to that folder unless they have a copy of that file in their home folder.
Our next step will be link sharing. This will allow you to share a secure link with anyone on the internet through any medium you choose. This way, you can invite people to your shared folders, even if they don’t already have a Files account!
Beyond sharing, our next feature release will be billing. This will allow users to sign up for paid plans and enjoy access to additional storage. Payments will be accepted via crypto or credit card.
After billing, the team will focus on enhanced collaboration between users. This “teams” feature, will include enhanced permissions setting, and a suite of other tools to make it easy for groups of people to share and collaborate without compromising their security, privacy or data sovereignty.
While this work is underway, the Files team will deliver ongoing upgrades to the design and UX of the product as well as it’s performance and efficiency.
We rely on user feedback to help make files better so we would love to hear your thoughts! What do you like about Files? Is there anything you would change? What feature would you like to see in the future? Tell us what you think in our Files power user Telegram group, or on GitHub. Also feel free to reach out on twitter Twitter and head over to our Discord channel for product support and Q&A.
Want to work with ChainSafe? Come join us! Check out the careers section of our website with our open positions, and get in touch with us at careers@chainsafe.io if you are interested!
building the infrastructure for web 3.0
91 
91 claps
91 
Written by
Helping to build Web3 for a better world. Project Manager and Technical Writer at ChainSafe.
building the infrastructure for web 3.0
Written by
Helping to build Web3 for a better world. Project Manager and Technical Writer at ChainSafe.
building the infrastructure for web 3.0
Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more
Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore
If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Start a blog
About
Write
Help
Legal
Get the Medium app
"
https://medium.com/@spencerrascoff/when-inexperience-is-your-biggest-advantage-8140c61e8002?source=search_post---------260,"Sign in
There are currently no responses for this story.
Be the first to respond.
Spencer Rascoff
Mar 25, 2017·4 min read
I recently connected with Aaron Levie, CEO of Box, an enterprise-focused cloud storage company used by big names like GE and AstraZeneca. Aaron and a couple of his high school friends founded Box in 2005 around the same time we started Zillow in Seattle. Interestingly, our team considered doing something similar before deciding to focus on real estate. Had we gone into cloud storage as well, we wouldn’t have Zillow… and Aaron and I probably wouldn’t have had such a candid and friendly conversation.
Aaron and I have a lot in common. At 10 years my junior, Aaron knows what it’s like to be a young CEO; he cut his teeth on the C-suite at the same age I was when co-founding Hotwire. Being a young executive has huge implications on your leadership style; in fact, it almost dictates how you behave. As a 23-year-old leading 40- and 50-year-olds, you absolutely cannot tell people what to do. You’ll be met with a, “Punk, what do you know?” response. But you still need to lead these people, and I found as a young leader that I could only do so through two-way communication, transparent decision-making and always, always communicating the “why” behind any effort.
This leadership style isn’t just important for young leaders; it’s important for all leaders as the future of work continues to evolve. In addition to being a young CEO, Aaron leads a company that is all about changing the way information is shared across organizations. He believes how companies communicate internally will be what makes or breaks their future: “No industry is exempt from disruption,” says Aaron, “which means companies need to have the shortest distance between the person with an idea and the person who can make a decision.” That exchange needs great technology like Box, but it also needs the open communication channels and an environment where new ideas, perspective and even dissent can come from all levels.
Beyond leadership, inexperience brings another huge advantage on the ideas front. I mentioned that our founding team explored the idea of doing a file-sharing startup, but what I failed to mention was why we didn’t. Our team, having originated from Microsoft and Expedia and being in the Seattle area, discarded the idea because we thought the behemoths like Microsoft and Amazon would eventually create the same service and give it away for free, and we wouldn’t be able to compete. We had played in this industry; we were jaded and cautious. Aaron wasn’t. He was a junior in college who, along with a few of his friends, saw an opportunity to solve a problem and dove right in. The founders of Box didn’t think of what Microsoft would or could do, and that was their advantage. Aaron put this well in our conversation: “Silicon Valley — and tech innovation in general — thrives on a set of people every couple years who know nothing about history coming in and saying, ‘What’s Microsoft? What’s Google? We are trying to solve a problem.’” When you have a lot of experience, those questions are unavoidable.
Lastly, inexperience pretty much requires you to take big swings if you want to make a mark quickly — whether that means disrupting an industry or just bringing something new to a company. Aaron recalled that one of the most valuable pieces of advice he’s ever received was from Box’s first investor, Mark Cuban; he told them not to hedge their bets, that they couldn’t “kind of disrupt” if they wanted to be successful. They had to go all in.
We also had the advantage of inexperience when we founded Zillow. None of us knew anything about real estate beyond how frustrating it was to find information when trying to buy or sell a home. We had a very clear problem we were trying to solve, and so we didn’t just “kind of” disrupt access to real estate information; we turned on the lights — giant floodlights. We believed it was best for the consumer to know and access as much information as the professional when it came to making one of the biggest financial investments of their lives, and we didn’t consider that it hadn’t been done before or might be met with industry pushback. By going all in, we created something that truly addressed a problem and opened the doors for a whole set of innovation around the entire lifecycle of homeownership, and the industry has since evolved to embrace this transparency because it is ultimately best for the consumer.
“Inexperience” isn’t typically seen as desirable in the workplace — but I think organizations should make sure they are introducing enough inexperience, whether by bringing in executives from different industries, reorganizing team structures or ensuring their summer interns feel they can contribute. New voices and new perspectives may sometimes be rooted in naïveté, but that’s not necessarily a bad thing. Sometimes, like in the case of Box, inexperience has huge payoffs.
Press play below to listen to our conversation.
You can read a transcript of the full interview with Aaron, as well as others in the Office Hours series, here. If you like what we’re doing, please write a review on iTunes.
Co-founder and former CEO @ZillowGroup. Co-author of Zillow Talk. Host of Office Hours podcast. Passionate about all things culture, leadership and management.
6 
6 
6 
Co-founder and former CEO @ZillowGroup. Co-author of Zillow Talk. Host of Office Hours podcast. Passionate about all things culture, leadership and management.
"
https://medium.com/fortknoxster/online-privacy-101-encryption-and-zero-knowledge-principle-1eebce038f71?source=search_post---------225,"There are currently no responses for this story.
Be the first to respond.
The basics of privacy online go hand in hand with encryption. If you want privacy, encrypt your activity, your inbox, your cloud storage, your hard drive, your chats, and VoIP calls and video conferences.
Since encryption is the ultimate source of frustration for hackers, tech giants and spy agencies alike, it’s no wonder security experts and privacy advocates from Snowden to Assange, Bruce Schneider to Brian Krebs, keep reiterating — encrypt.
But encryption in different online services is not made equal, unfortunately. Providers offering end-to-end encryption in chat, cloud storage or inbox often store their customers’ keys. The apparent reason is to let you reset your password, should you lose it. However, if the provider holds your keys, it can access your data — unencrypt it, view it, hand it over to law enforcement, or trade on the black market.
It’s a question of trust. Can you trust Google, Facebook, and WhatsApp to not snoop on you? Some call it fear mongering and paranoia, but a reasonable degree of suspicion can never hurt. With that in mind, how do you choose a service that offers end-to-end encryption, but does not leave a backdoor open?
Due diligence. There is no way you can trust ads and marketing slogans — verify every single claim. Read Terms of Service, Privacy Policy, check out the provider’s location and the data retention laws applicable in the country.
Contact provider’s support and see if they are responsive, helpful and professional. Read their White Paper, if possible, and question their encryption technology. Namely, inquire if the end-to-end encryption is zero-knowledge.
Now, let’s see why we advocate zero-knowledge encryption for communication apps and storage services.
Encryption is, as you probably know, data scrambling that turns your data into ones and zeroes for anyone without a key. You hold the key — you can decrypt the data back into plain text.
With strong encryption, and presently it’s at least AES-256, your data would take ten years for modern computers to brute-force.
Encryption keeps you safe. Encryption protects your financial details and passwords when you bank online. It protects your cell phone conversations from eavesdroppers. If you encrypt your laptop — and I hope you do — it protects your data if your computer is stolen. It protects our money and our privacy. Bruce Schneier
When choosing encryption-enabled services, opt for end-to-end encryption. End-to-end encryption covers all of your traffic and stored data top to bottom:
That way, even if your data travels through servers located in countries that engage in snooping (14 Eyes, or other repressive regimes like China, Russia, Arab Emirates, Iran), the hackers or spy agencies won’t be able to read it or decrypt it.
However, as we mentioned, most mainstream providers still hold your keys, so your privacy and security of your data are not in your hands alone. That’s where the principle of zero-knowledge helps providers ensure only the customer holds the encryption keys and helps the customer rest assured the provider’s claims of 100% privacy aren’t false or opportunist, at best.
Zero-knowledge encryption locks the service provider out, restricting access to your data only to you and those, with whom you share that data.
The provider is responsible for encrypting the data, for transmitting it encrypted, for storing it encrypted. Thanks to the privacy by design principle, the provider can’t access your data.
All the provider can see on its servers is blocks of encrypted ones and zeroes.
Zero-knowledge platforms aren’t ideal — they are complex and difficult to implement. That’s why you won’t see free services offering zero-knowledge encryption. Such networks require ongoing maintenance, so can’t be free, not with the current state of technology.
On the other hand, zero-knowledge encryption is beneficial for the end user because it enables secure communication, or data storage and file sharing. It is also quite straightforward for the customer since there is no particular prerequisite or a technical skill that you need to master.
The only tangible downside to zero-knowledge encryption is if you lose a password, you lose your data since the provider won’t be able to help you restore your keys.
There is a widespread misconception that strong encryption and security come at the cost of usability. That only tech-advanced users can sort out the complex applications. This misconception takes its roots in history, as encryption and secure communication were difficult, indeed, when PGP first came around.
Currently, the usability barrier has been successfully overcome, and you can enjoy end-to-end, zero-knowledge encryption and two-factor authentication without compromising your productivity or usability.
Encryption has been a buzz word in mainstream media ever since Snowden spilled the beans about mass surveillance. So tech giants jumped on the bandwagon and started enabling end-to-end encryption for services that have a long track record of user tracking and profiling.
It’s as if encryption is an absolution of all privacy-invasive practices these companies have been engaging in the past, present, and future.
For average users, perhaps, but for the reasonable paranoid who’ve jumped ship and quit using social media for private communication, it was a weak statement.
Mainstream providers still want you to put your blind trust in their good intentions. Even with end-to-end encryption where they hold the keys and implement “design features” that trigger “backdoor” flags among security experts.
Suppose you trust them, that’s your choice. But there’s another issue with encryption-enabled services that aren’t zero-knowledge — they get hacked, too.
Assuming your Dropbox encrypted file storage is protected by a strong cryptographic key, which would take years to brute-force. In which case, persistent individuals can still gain access to your data by hacking the provider’s master key. In which case, it doesn’t matter if your password was good.
Likewise, persistent individuals could just gain a warrant and a gag order, and the non-zero-knowledge provider would hand over your key and your data without ever notifying you.
What is it that makes zero-knowledge so secure? The concept of zero-knowledge proof is the foundation of any zero-knowledge architecture.
Without zero-knowledge:
In zero-knowledge architecture, user authorization is possible when the server doesn’t have your password. This is the Zero-Knowledge Proof principle developed by MIT researchers. Rackoff, McMicali, and Goldwasser.
In this case, you need to prove to the provider’s server that you have the authentic password without disclosing the password. The requisites that make such a transaction are difficult to implement, but yet possible and actively deployed by security providers:
Your password consists of mathematically interconnected keys -private and public. You have the private key. The provider has the public key. As you log in, the server matches this mathematical link between your private key and its public key and authenticates you as the legitimate account holder.
Importantly, the service provider, in this case, can not generate your private key based on its knowledge about this mathematical link.
In this case, hackers or spy agencies have no business in targeting your service provider to obtain your private key.
In layman terms, zero-knowledge encryption relies on the following:
Therefore, the server only stores the 64-character cryptographic hash of your private key. The server uses the PBKDF2 algorithm, SHA-256, and 5000 rounds of hashing to store the hash of your private password.
The good news is the end user does not have to go through extra steps to authenticate — it’s the usual login process, only zero-knowledge. You can add two-factor authentication for an even better security.
Wrapping up the above, if you are looking for a secure provider that doesn’t make false claims about complete encryption and privacy, check the following:
Check Chapter 1 of our Online Privacy 101 for more information on data retention laws, 14 Eyes, mass and corporate surveillance.
Please also join our Telegram group and visit our Facebook page and Twitter page for more inspiration.
Originally published at fortknoxster.com on September 25, 2017.
FortKnoxster is a cyber-security company offering secure and private communications for all.
824 
824 claps
824 
ABOUT US FortKnoxster is founded by skilled entrepreneurs and cyber-security experts, with an extensive experience in the field of online security and cyberdefence.
Written by
FortKnoxster is a cyber-security company offering secure and private communications for all.
ABOUT US FortKnoxster is founded by skilled entrepreneurs and cyber-security experts, with an extensive experience in the field of online security and cyberdefence.
"
https://blog.bluzelle.com/bluzelle-partnership-with-0chain-dda901d61fe2?source=search_post---------392,"Bluzelle is pleased to announce their partnership with 0chain to help grow its database service platform, using 0chain’s fast, free, guaranteed decentralized storage.
0chain & Bluzelle met at Token2049 in Hong Kong, March 2018. With many mutual areas of benefit between the two leading solutions this is an exciting partnership. Together the two solutions can bring world class decentralized storage solutions while growing the entire ecosystem.
0chain is developing several groundbreaking protocols, including 2D-BPoS consensus, self-forking, storage, and inflation protocols to achieve massive scalability, a fast finality, and a 0-cost dCloud.
Neeraj Murarka, CTO of Bluzelle says,
“Bluzelle and 0chain’s innovative data storage mechanisms combine to provide a completely decentralized solution for the needs of not just dApp developers but even traditional applications that require a reliable and scalable storage platform.”
Saswata Basu, Founder of 0chain says:
“We are excited to work with Bluzelle and its flagship database platform to help grow the decentralized storage and database ecosystem with our fast, free, and flexible computing and storage platform. ”
0chain plans to release their testnet Q3 2018, and mainnet in Q4 2018.
More info on 0chain:
Website: 0chain.net
Twitter: @0costcloud
Telegram: https://t.me/0chain
Bluzelle is a decentralized storage network for the creator economy.
217 
217 claps
217 
Written by
Bluzelle is a decentralized data network for dapps to manage data in a secure, tamper-proof, and highly scalable manner.
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
Written by
Bluzelle is a decentralized data network for dapps to manage data in a secure, tamper-proof, and highly scalable manner.
Bluzelle delivers high security, unmatched availability, and is censorship-resistant. Whether you are an artist, musician, scientist, publisher, or developer, Bluzelle protects the intellectual property of all creators.
"
https://medium.com/@memority/who-are-memoritys-beneficiaries-58ecdb453d0e?source=search_post---------324,"Sign in
There are currently no responses for this story.
Be the first to respond.
Memority
Jun 15, 2018·2 min read
Memority platform is a place where two groups of customers meet: data owners and hosters. But it’s not just the storage, it’s the self-sufficient ecosystem that includes many applications to meet the needs of business, government organizations and individuals in the super-secure storage of all kinds of valuable data.
For data owners Memority solves the problem of information storage with maximum level of security due to the storage algorithm that uses its own private blockchain network based on Ethereum. Data owner can store data in an encrypted form in a decentralized and completely secure manner, paying for the storage with MMR tokens. System automatically stores files in 10 copies at 10 random hosters, and if hosters shut down, the files are automatically copied to a new hoster. So there’s no way you’ll lose the files. Hosters can receive MMR tokens for providing their disk space to users of the Memority platform to store their data.
Above all Memority platform is the interface for creating applications by third-party developers and percentage for miners. Third-party developers will be able to implement their ideas and create their own applications, using the infrastructure of Memority, and receiving MMR tokens for this. Miners receive rewards in the form of MMR tokens for supporting the working capacity of blockchain (only 10,000 or more token owners can become miners). Memority use Proof of Authority mining, so there is no need for large computing resources 5% of payments for data storage are distributed among miners.
The most interesting group of beneficiaries from marketing and business point of view is data owners. According to Statista, during the May 2017 survey, 61% of respondents aged 18 to 29 years stated that they were using cloud storage services for their data, while 42% of respondents aged 30 to 59 years and 18% of respondents aged 60 years and older said the same. This statistics shows that the most active group of users by age is students and graduates. We can suppose that this age interval is also true for Memority’s potential hosters.
A blockchain-based platform for a completely decentralized, super-secure storage of valuable data.
19.1K 
19.1K 
19.1K 
A blockchain-based platform for a completely decentralized, super-secure storage of valuable data.
"
https://heartbeat.comet.ml/working-with-firebase-storage-in-android-part-1-a789f9eea037?source=search_post---------242,"Sign in
Harshit Dwivedi
Mar 23, 2020·6 min read
If you follow my writing (here or here), then you know I’ve been working on a new Android app called AfterShoot — it’s an AI-powered Android app that helps users take better pictures while also managing their digital waste.
aftershoot.co
One of the important features that I had to implement in this app was the ability to handle feedback provided by users — meaning, if my trained model predicted an incorrect result, I should be able to take that feedback from the user about what the correct output should be and use that to retrain my model for better results.
If you are new to Firebase, you might want to check out my earlier posts on Firebase here:
heartbeat.comet.ml
heartbeat.comet.ml
While implementing a custom data warehouse to store all this user data might be a mammoth task to undertake manually, thankfully Firebase provides us with an easy way to handle such scenarios. To achieve this functionality, I’ll be using Firebase Storage to upload and save the images that were incorrectly predicted by my model.
Here’s a screenshot showcasing what my Firebase Storage directory structure will look like:
For instance, if my model predicted a blurred picture as a good picture, the user now has an option to submit this feedback back to me. Upon doing so, that image will be uploaded into the blurred folder.
I can probably run a weekly training schedule for my model based on the feedback I’ve received. Doing so will result in a better model over time.
As the use case for using Firebase Storage is clear, in this blog post I’ll be outlining how I implemented it in AfterShoot. You can use the learnings to implement a similar feature in your app, as well!
Before we go ahead with Android-specific configurations, it’s crucial that you’ve configured a Firebase Project with your app. To do so, you can find a good tutorial here:
firebase.google.com
Once done, you might also want to set up Firebase Storage from your Firebase Console. To do so, go to your app’s Firebase Console and navigate to the Storage option. From there, click on “Get Started” and follow the on-screen instructions.
To use Firebase Storage, you need to add the following dependency to your app’s build.gradle file:
Once done, sync your changes with Gradle and let it download the required dependencies.
Once the Firebase dependencies are added, the next step is to get an instance of the storage bucket/folder to which you want to upload your files.
For instance, the code below shows how to get access to the instance of the feedback folders from the screenshot earlier.
As you can see, the getReference() method gives me access to the root folder, from which I can navigate to any nested subfolder with relative ease.
Once the folder path is configured, you might want to upload a particular file to that folder. To do so, we use the helper method child() along with putBytes(), available inside the FirebaseStorage class.
The child() method will determine the name of the file that’s to be created, while the putBytes() method will determine the content of that file. Here’s what the code for doing this looks like:
As you can see in the code above, as soon as the uploadImage() method is called, Firebase will initiate an upload task for the provided image into the storage bucket. We also have an option to monitor its progress as it’s being uploaded.
You can also specify the file’s metadata while you’re uploading it. For example, to let the storage know that you’re uploading a .jpeg image, you can set its content type accordingly while uploading the image:
Note: By default, Firebase Storage will only accept upload operations from users who are logged into your app. Users who are not logged in won’t be able to upload files to your storage bucket. To modify this, you can tweak your storage rules to allow unauthenticated users to modify your data.
Since the upload process requires internet connectivity and, in practical scenarios, it’s not guaranteed that the device will always be connected to the internet, you need to be able to handle disconnections or limited connectivity gracefully.
For example, if the device is connected to mobile data, you’d want to pause the upload process and wait for the device to be connected to a WiFi network to resume the process. Firebase allows us to do this as well, and the code snippet below shows how to do so:
You can also attach the corresponding listeners to your task, in addition to the 3 listeners we attached above:
Uploading the image from the app might not pose any issues whatsoever since Firebase handles this task in a background thread. However, it makes more sense to offload it into a foreground thread so that the upload task continues even if the user exits your app while the file is being uploaded.
You can do so by creating a foreground service that contains the code to upload the image that we’ve written earlier, and instead of calling the uploadImage() method directly, start the service instead. You can learn more about how to do this in a blog I wrote earlier:
heartbeat.comet.ml
And that’s it! As you saw in this blog, it’s extremely easy to work with Firebase Storage, and in no time you can set up a robust and scalable data warehouse for your app!
In the next part of this blog post, we’ll look into working with the uploaded files, including how you can fetch, move, delete and modify them from an Android app.
If you want to look at the source code for the contents covered in this blog post, you can find it here:
github.com
Thanks for reading! If you enjoyed this story, please click the 👏 button and share it to help others find it! Feel free to leave a comment 💬 below.
Have feedback? Let’s connect on Twitter.
Editor’s Note: Heartbeat is a contributor-driven online publication and community dedicated to providing premier educational resources for data science, machine learning, and deep learning practitioners. We’re committed to supporting and inspiring developers and engineers from all walks of life.
Editorially independent, Heartbeat is sponsored and published by Comet, an MLOps platform that enables data scientists & ML teams to track, compare, explain, & optimize their experiments. We pay our contributors, and we don’t sell ads.
If you’d like to contribute, head on over to our call for contributors. You can also sign up to receive our weekly newsletters (Deep Learning Weekly and the Comet Newsletter), join us on Slack, and follow Comet on Twitter and LinkedIn for resources, events, and much more that will help you build better ML models, faster.
Has an *approximate* knowledge of many things. https://aftershoot.co
235 
235 claps
235 
Helping data scientists, ML engineers, and deep learning engineers build better models faster
"
